{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2102.04130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_Y/0/1/0/all/0/1\">Yennie Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_H/0/1/0/all/0/1\">Haider Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benussi_E/0/1/0/all/0/1\">Elias Benussi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volpin_F/0/1/0/all/0/1\">Filippo Volpin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dreyer_F/0/1/0/all/0/1\">Frederic A. Dreyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1\">Aleksandar Shtedritski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1\">Yuki M. Asano</a>",
          "description": "The capabilities of natural language models trained on large-scale data have\nincreased immensely over the past few years. Open source libraries such as\nHuggingFace have made these models easily available and accessible. While prior\nresearch has identified biases in large language models, this paper considers\nbiases contained in the most popular versions of these models when applied\n`out-of-the-box' for downstream tasks. We focus on generative language models\nas they are well-suited for extracting biases inherited from training data.\nSpecifically, we conduct an in-depth analysis of GPT-2, which is the most\ndownloaded text generation model on HuggingFace, with over half a million\ndownloads in the past month alone. We assess biases related to occupational\nassociations for different protected categories by intersecting gender with\nreligion, sexuality, ethnicity, political affiliation, and continental name\norigin. Using a template-based data collection pipeline, we collect 396K\nsentence completions made by GPT-2 and find: (i) The machine-predicted jobs are\nless diverse and more stereotypical for women than for men, especially for\nintersections; (ii) Intersectional interactions are highly relevant for\noccupational associations, which we quantify by fitting 262 logistic models;\n(iii) For most occupations, GPT-2 reflects the skewed gender and ethnicity\ndistribution found in US Labour Bureau data, and even pulls the\nsocietally-skewed distribution towards gender parity in cases where its\npredictions deviate from real labor market observations. This raises the\nnormative question of what language models _should_ learn - whether they should\nreflect or correct for existing inequalities.",
          "link": "http://arxiv.org/abs/2102.04130",
          "publishedOn": "2021-08-09T00:49:26.852Z",
          "wordCount": 733,
          "title": "Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models. (arXiv:2102.04130v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jie_C/0/1/0/all/0/1\">Cheng Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Da Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zigeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>",
          "description": "With the increasing scale of search engine marketing, designing an efficient\nbidding system is becoming paramount for the success of e-commerce companies.\nThe critical challenges faced by a modern industrial-level bidding system\ninclude: 1. the catalog is enormous, and the relevant bidding features are of\nhigh sparsity; 2. the large volume of bidding requests induces significant\ncomputation burden to both the offline and online serving. Leveraging\nextraneous user-item information proves essential to mitigate the sparsity\nissue, for which we exploit the natural language signals from the users' query\nand the contextual knowledge from the products. In particular, we extract the\nvector representations of ads via the Transformer model and leverage their\ngeometric relation to building collaborative bidding predictions via\nclustering. The two-step procedure also significantly reduces the computation\nstress of bid evaluation and optimization. In this paper, we introduce the\nend-to-end structure of the bidding system for search engine marketing for\nWalmart e-commerce, which successfully handles tens of millions of bids each\nday. We analyze the online and offline performances of our approach and discuss\nhow we find it as a production-efficient solution.",
          "link": "http://arxiv.org/abs/2106.12700",
          "publishedOn": "2021-08-09T00:49:26.734Z",
          "wordCount": 666,
          "title": "An Efficient Group-based Search Engine Marketing System for E-Commerce. (arXiv:2106.12700v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.07942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zanella_Beguelin_S/0/1/0/all/0/1\">Santiago Zanella-B&#xe9;guelin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1\">Lukas Wutschitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1\">Shruti Tople</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruhle_V/0/1/0/all/0/1\">Victor R&#xfc;hle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paverd_A/0/1/0/all/0/1\">Andrew Paverd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohrimenko_O/0/1/0/all/0/1\">Olga Ohrimenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopf_B/0/1/0/all/0/1\">Boris K&#xf6;pf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1\">Marc Brockschmidt</a>",
          "description": "To continuously improve quality and reflect changes in data, machine learning\napplications have to regularly retrain and update their core models. We show\nthat a differential analysis of language model snapshots before and after an\nupdate can reveal a surprising amount of detailed information about changes in\nthe training data. We propose two new metrics---\\emph{differential score} and\n\\emph{differential rank}---for analyzing the leakage due to updates of natural\nlanguage models. We perform leakage analysis using these metrics across models\ntrained on several different datasets using different methods and\nconfigurations. We discuss the privacy implications of our findings, propose\nmitigation strategies and evaluate their effect.",
          "link": "http://arxiv.org/abs/1912.07942",
          "publishedOn": "2021-08-09T00:49:26.673Z",
          "wordCount": 609,
          "title": "Analyzing Information Leakage of Updates to Natural Language Models. (arXiv:1912.07942v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1\">Aiqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaohan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>",
          "description": "Online sexism has become an increasing concern in social media platforms as\nit has affected the healthy development of the Internet and can have negative\neffects in society. While research in the sexism detection domain is growing,\nmost of this research focuses on English as the language and on Twitter as the\nplatform. Our objective here is to broaden the scope of this research by\nconsidering the Chinese language on Sina Weibo. We propose the first Chinese\nsexism dataset -- Sina Weibo Sexism Review (SWSR) dataset --, as well as a\nlarge Chinese lexicon SexHateLex made of abusive and gender-related terms. We\nintroduce our data collection and annotation process, and provide an\nexploratory analysis of the dataset characteristics to validate its quality and\nto show how sexism is manifested in Chinese. The SWSR dataset provides labels\nat different levels of granularity including (i) sexism or non-sexism, (ii)\nsexism category and (iii) target type, which can be exploited, among others,\nfor building computational methods to identify and investigate finer-grained\ngender-related abusive language. We conduct experiments for the three sexism\nclassification tasks making use of state-of-the-art machine learning models.\nOur results show competitive performance, providing a benchmark for sexism\ndetection in the Chinese language, as well as an error analysis highlighting\nopen challenges needing more research in Chinese NLP. The SWSR dataset and\nSexHateLex lexicon are publicly available.",
          "link": "http://arxiv.org/abs/2108.03070",
          "publishedOn": "2021-08-09T00:49:26.638Z",
          "wordCount": 668,
          "title": "SWSR: A Chinese Dataset and Lexicon for Online Sexism Detection. (arXiv:2108.03070v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1\">Aiqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>",
          "description": "Most hate speech detection research focuses on a single language, generally\nEnglish, which limits their generalisability to other languages. In this paper\nwe investigate the cross-lingual hate speech detection task, tackling the\nproblem by adapting the hate speech resources from one language to another. We\npropose a cross-lingual capsule network learning model coupled with extra\ndomain-specific lexical semantics for hate speech (CCNL-Ex). Our model achieves\nstate-of-the-art performance on benchmark datasets from AMI@Evalita2018 and\nAMI@Ibereval2018 involving three languages: English, Spanish and Italian,\noutperforming state-of-the-art baselines on all six language pairs.",
          "link": "http://arxiv.org/abs/2108.03089",
          "publishedOn": "2021-08-09T00:49:26.629Z",
          "wordCount": 529,
          "title": "Cross-lingual Capsule Network for Hate Speech Detection in Social Media. (arXiv:2108.03089v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1\">Guangyi Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Le Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>",
          "description": "Sentence semantic matching requires an agent to determine the semantic\nrelation between two sentences, which is widely used in various natural\nlanguage tasks, such as Natural Language Inference (NLI), Paraphrase\nIdentification (PI), and so on. Much recent progress has been made in this\narea, especially attention-based methods and pre-trained language model based\nmethods. However, most of these methods focus on all the important parts in\nsentences in a static way and only emphasize how important the words are to the\nquery, inhibiting the ability of attention mechanism. In order to overcome this\nproblem and boost the performance of attention mechanism, we propose a novel\ndynamic re-read attention, which can pay close attention to one small region of\nsentences at each step and re-read the important parts for better sentence\nrepresentations. Based on this attention variation, we develop a novel Dynamic\nRe-read Network (DRr-Net) for sentence semantic matching. Moreover, selecting\none small region in dynamic re-read attention seems insufficient for sentence\nsemantics, and employing pre-trained language models as input encoders will\nintroduce incomplete and fragile representation problems. To this end, we\nextend DRrNet to Locally-Aware Dynamic Re-read Attention Net (LadRa-Net), in\nwhich local structure of sentences is employed to alleviate the shortcoming of\nByte-Pair Encoding (BPE) in pre-trained language models and boost the\nperformance of dynamic reread attention. Extensive experiments on two popular\nsentence semantic matching tasks demonstrate that DRr-Net can significantly\nimprove the performance of sentence semantic matching. Meanwhile, LadRa-Net is\nable to achieve better performance by considering the local structures of\nsentences. In addition, it is exceedingly interesting that some discoveries in\nour experiments are consistent with some findings of psychological research.",
          "link": "http://arxiv.org/abs/2108.02915",
          "publishedOn": "2021-08-09T00:49:26.493Z",
          "wordCount": 723,
          "title": "LadRa-Net: Locally-Aware Dynamic Re-read Attention Net for Sentence Semantic Matching. (arXiv:2108.02915v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1\">Aotao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stellar_J/0/1/0/all/0/1\">Jennifer E. Stellar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>",
          "description": "Humans possess the unique ability to communicate emotions through language.\nAlthough concepts like anger or awe are abstract, there is a shared consensus\nabout what these English emotion words mean. This consensus may give the\nimpression that their meaning is static, but we propose this is not the case.\nWe cannot travel back to earlier periods to study emotion concepts directly,\nbut we can examine text corpora, which have partially preserved the meaning of\nemotion words. Using natural language processing of historical text, we found\nevidence for semantic change in emotion words over the past century and that\nvarying rates of change were predicted in part by an emotion concept's\nprototypicality - how representative it is of the broader category of\n\"emotion\". Prototypicality negatively correlated with historical rates of\nemotion semantic change obtained from text-based word embeddings, beyond more\nestablished variables including usage frequency in English and a second\ncomparison language, French. This effect for prototypicality did not\nconsistently extend to the semantic category of birds, suggesting its relevance\nfor predicting semantic change may be category-dependent. Our results suggest\nemotion semantics are evolving over time, with prototypical emotion words\nremaining semantically stable, while other emotion words evolve more freely.",
          "link": "http://arxiv.org/abs/2108.02887",
          "publishedOn": "2021-08-09T00:49:26.483Z",
          "wordCount": 621,
          "title": "Evolution of emotion semantics. (arXiv:2108.02887v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1\">Markus Dreyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_F/0/1/0/all/0/1\">Feng Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atluri_S/0/1/0/all/0/1\">Sandeep Atluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Sujith Ravi</a>",
          "description": "We analyze the tradeoff between factuality and abstractiveness of summaries.\nWe introduce abstractiveness constraints to control the degree of\nabstractiveness at decoding time, and we apply this technique to characterize\nthe abstractiveness-factuality tradeoff across multiple widely-studied\ndatasets, using extensive human evaluations. We train a neural summarization\nmodel on each dataset and visualize the rates of change in factuality as we\ngradually increase abstractiveness using our abstractiveness constraints. We\nobserve that, while factuality generally drops with increased abstractiveness,\ndifferent datasets lead to different rates of factuality decay. We propose new\nmeasures to quantify the tradeoff between factuality and abstractiveness, incl.\nmuQAGS, which balances factuality with abstractiveness. We also quantify this\ntradeoff in previous works, aiming to establish baselines for the\nabstractiveness-factuality tradeoff that future publications can compare\nagainst.",
          "link": "http://arxiv.org/abs/2108.02859",
          "publishedOn": "2021-08-09T00:49:26.458Z",
          "wordCount": 561,
          "title": "Analyzing the Abstractiveness-Factuality Tradeoff With Nonlinear Abstractiveness Constraints. (arXiv:2108.02859v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Alexander Hanbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1\">Patrick Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiguo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>",
          "description": "The current state-of-the-art generative models for open-domain question\nanswering (ODQA) have focused on generating direct answers from unstructured\ntextual information. However, a large amount of world's knowledge is stored in\nstructured databases, and need to be accessed using query languages such as\nSQL. Furthermore, query languages can answer questions that require complex\nreasoning, as well as offering full explainability. In this paper, we propose a\nhybrid framework that takes both textual and tabular evidence as input and\ngenerates either direct answers or SQL queries depending on which form could\nbetter answer the question. The generated SQL queries can then be executed on\nthe associated databases to obtain the final answers. To the best of our\nknowledge, this is the first paper that applies Text2SQL to ODQA tasks.\nEmpirically, we demonstrate that on several ODQA datasets, the hybrid methods\nconsistently outperforms the baseline models that only take homogeneous input\nby a large margin. Specifically we achieve state-of-the-art performance on\nOpenSQuAD dataset using a T5-base model. In a detailed analysis, we demonstrate\nthat the being able to generate structural SQL queries can always bring gains,\nespecially for those questions that requires complex reasoning.",
          "link": "http://arxiv.org/abs/2108.02866",
          "publishedOn": "2021-08-09T00:49:26.438Z",
          "wordCount": 642,
          "title": "Dual Reader-Parser on Hybrid Textual and Tabular Evidence for Open Domain Question Answering. (arXiv:2108.02866v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tuxworth_D/0/1/0/all/0/1\">David Tuxworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antypas_D/0/1/0/all/0/1\">Dimosthenis Antypas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1\">Luis Espinosa-Anke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preece_A/0/1/0/all/0/1\">Alun Preece</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_D/0/1/0/all/0/1\">David Rogers</a>",
          "description": "This paper demonstrates a two-stage method for deriving insights from social\nmedia data relating to disinformation by applying a combination of geospatial\nclassification and embedding-based language modelling across multiple\nlanguages. In particular, the analysis in centered on Twitter and\ndisinformation for three European languages: English, French and Spanish.\nFirstly, Twitter data is classified into European and non-European sets using\nBERT. Secondly, Word2vec is applied to the classified texts resulting in\nEurocentric, non-Eurocentric and global representations of the data for the\nthree target languages. This comparative analysis demonstrates not only the\nefficacy of the classification method but also highlights geographic, temporal\nand linguistic differences in the disinformation-related media. Thus, the\ncontributions of the work are threefold: (i) a novel language-independent\ntransformer-based geolocation method; (ii) an analytical approach that exploits\nlexical specificity and word embeddings to interrogate user-generated content;\nand (iii) a dataset of 36 million disinformation related tweets in English,\nFrench and Spanish.",
          "link": "http://arxiv.org/abs/2108.03067",
          "publishedOn": "2021-08-09T00:49:26.427Z",
          "wordCount": 604,
          "title": "Deriving Disinformation Insights from Geolocalized Twitter Callouts. (arXiv:2108.03067v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03016",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petruzzellis_L/0/1/0/all/0/1\">L Petruzzellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1\">A Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visentin_M/0/1/0/all/0/1\">M Visentin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chebat_J/0/1/0/all/0/1\">J.-C. Chebat</a>",
          "description": "In this paper we investigate the verbal expression of shopping experience\nobtained by a sample of customers asked to freely verbalize how they felt when\nentering a store. Using novel tools of Text Mining and Social Network Analysis,\nwe analyzed the interviews to understand the connection between the emotions\naroused during the shopping experience, satisfaction and the way participants\nlink these concepts to self-satisfaction and self-identity. The results show a\nprominent role of emotions in the discourse about the shopping experience\nbefore purchasing and an inward-looking connection to the self. Our results\nalso suggest that modern retail environment should enhance the hedonic shopping\nexperience in terms of fun, fantasy, moods, and emotions.",
          "link": "http://arxiv.org/abs/2108.03016",
          "publishedOn": "2021-08-09T00:49:26.413Z",
          "wordCount": 585,
          "title": "Tell me a story about yourself: The words of shopping experience and self-satisfaction. (arXiv:2108.03016v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupte_A/0/1/0/all/0/1\">Amit Gupte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romanov_A/0/1/0/all/0/1\">Alexey Romanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mantravadi_S/0/1/0/all/0/1\">Sahitya Mantravadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banda_D/0/1/0/all/0/1\">Dalitso Banda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1\">Raza Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meenal_L/0/1/0/all/0/1\">Lakshmanan Ramu Meenal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Benjamin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Soundar Srinivasan</a>",
          "description": "Document digitization is essential for the digital transformation of our\nsocieties, yet a crucial step in the process, Optical Character Recognition\n(OCR), is still not perfect. Even commercial OCR systems can produce\nquestionable output depending on the fidelity of the scanned documents. In this\npaper, we demonstrate an effective framework for mitigating OCR errors for any\ndownstream NLP task, using Named Entity Recognition (NER) as an example. We\nfirst address the data scarcity problem for model training by constructing a\ndocument synthesis pipeline, generating realistic but degraded data with NER\nlabels. We measure the NER accuracy drop at various degradation levels and show\nthat a text restoration model, trained on the degraded data, significantly\ncloses the NER accuracy gaps caused by OCR errors, including on an\nout-of-domain dataset. For the benefit of the community, we have made the\ndocument synthesis pipeline available as an open-source project.",
          "link": "http://arxiv.org/abs/2108.02899",
          "publishedOn": "2021-08-09T00:49:26.398Z",
          "wordCount": 621,
          "title": "Lights, Camera, Action! A Framework to Improve NLP Accuracy over OCR documents. (arXiv:2108.02899v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02923",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yulin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuxi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yuchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiameng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Kun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>",
          "description": "Structured text understanding on Visually Rich Documents (VRDs) is a crucial\npart of Document Intelligence. Due to the complexity of content and layout in\nVRDs, structured text understanding has been a challenging task. Most existing\nstudies decoupled this problem into two sub-tasks: entity labeling and entity\nlinking, which require an entire understanding of the context of documents at\nboth token and segment levels. However, little work has been concerned with the\nsolutions that efficiently extract the structured data from different levels.\nThis paper proposes a unified framework named StrucTexT, which is flexible and\neffective for handling both sub-tasks. Specifically, based on the transformer,\nwe introduce a segment-token aligned encoder to deal with the entity labeling\nand entity linking tasks at different levels of granularity. Moreover, we\ndesign a novel pre-training strategy with three self-supervised tasks to learn\na richer representation. StrucTexT uses the existing Masked Visual Language\nModeling task and the new Sentence Length Prediction and Paired Boxes Direction\ntasks to incorporate the multi-modal information across text, image, and\nlayout. We evaluate our method for structured text understanding at\nsegment-level and token-level and show it outperforms the state-of-the-art\ncounterparts with significantly superior performance on the FUNSD, SROIE, and\nEPHOIE datasets.",
          "link": "http://arxiv.org/abs/2108.02923",
          "publishedOn": "2021-08-09T00:49:26.382Z",
          "wordCount": 654,
          "title": "StrucTexT: Structured Text Understanding with Multi-Modal Transformers. (arXiv:2108.02923v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wet_H/0/1/0/all/0/1\">Harm de Wet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>",
          "description": "Disinformation through fake news is an ongoing problem in our society and has\nbecome easily spread through social media. The most cost and time effective way\nto filter these large amounts of data is to use a combination of human and\ntechnical interventions to identify it. From a technical perspective, Natural\nLanguage Processing (NLP) is widely used in detecting fake news. Social media\ncompanies use NLP techniques to identify the fake news and warn their users,\nbut fake news may still slip through undetected. It is especially a problem in\nmore localised contexts (outside the United States of America). How do we\nadjust fake news detection systems to work better for local contexts such as in\nSouth Africa. In this work we investigate fake news detection on South African\nwebsites. We curate a dataset of South African fake news and then train\ndetection models. We contrast this with using widely available fake news\ndatasets (from mostly USA website). We also explore making the datasets more\ndiverse by combining them and observe the differences in behaviour in writing\nbetween nations' fake news using interpretable machine learning.",
          "link": "http://arxiv.org/abs/2108.02941",
          "publishedOn": "2021-08-09T00:49:26.371Z",
          "wordCount": 640,
          "title": "Is it Fake? News Disinformation Detection on South African News Websites. (arXiv:2108.02941v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>",
          "description": "Recall the classical text generation works, the generation framework can be\nbriefly divided into two phases: \\textbf{idea reasoning} and \\textbf{surface\nrealization}. The target of idea reasoning is to figure out the main idea which\nwill be presented in the following talking/writing periods. Surface realization\naims to arrange the most appropriate sentence to depict and convey the\ninformation distilled from the main idea. However, the current popular\ntoken-by-token text generation methods ignore this crucial process and suffer\nfrom many serious issues, such as idea/topic drift. To tackle the problems and\nrealize this two-phase paradigm, we propose a new framework named Sentence\nSemantic Regression (\\textbf{SSR}) based on sentence-level language modeling.\nFor idea reasoning, two architectures \\textbf{SSR-AR} and \\textbf{SSR-NonAR}\nare designed to conduct sentence semantic regression autoregressively (like\nGPT2/3) and bidirectionally (like BERT). In the phase of surface realization, a\nmixed-granularity sentence decoder is designed to generate text with better\nconsistency by jointly incorporating the predicted sentence-level main idea as\nwell as the preceding contextual token-level information. We conduct\nexperiments on four tasks of story ending prediction, story ending generation,\ndialogue generation, and sentence infilling. The results show that SSR can\nobtain better performance in terms of automatic metrics and human evaluation.",
          "link": "http://arxiv.org/abs/2108.02984",
          "publishedOn": "2021-08-09T00:49:26.323Z",
          "wordCount": 625,
          "title": "Sentence Semantic Regression for Text Generation. (arXiv:2108.02984v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vanmassenhove_E/0/1/0/all/0/1\">Eva Vanmassenhove</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monti_J/0/1/0/all/0/1\">Johanna Monti</a>",
          "description": "Languages differ in terms of the absence or presence of gender features, the\nnumber of gender classes and whether and where gender features are explicitly\nmarked. These cross-linguistic differences can lead to ambiguities that are\ndifficult to resolve, especially for sentence-level MT systems. The\nidentification of ambiguity and its subsequent resolution is a challenging task\nfor which currently there aren't any specific resources or challenge sets\navailable. In this paper, we introduce gENder-IT, an English--Italian challenge\nset focusing on the resolution of natural gender phenomena by providing\nword-level gender tags on the English source side and multiple gender\nalternative translations, where needed, on the Italian target side.",
          "link": "http://arxiv.org/abs/2108.02854",
          "publishedOn": "2021-08-09T00:49:26.127Z",
          "wordCount": 552,
          "title": "GENder-IT: An Annotated English-Italian Parallel Challenge Set for Cross-Linguistic Natural Gender Phenomena. (arXiv:2108.02854v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Moin Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahzad_K/0/1/0/all/0/1\">Khurram Shahzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1\">Kamran Malik</a>",
          "description": "Hate speech is a specific type of controversial content that is widely\nlegislated as a crime that must be identified and blocked. However, due to the\nsheer volume and velocity of the Twitter data stream, hate speech detection\ncannot be performed manually. To address this issue, several studies have been\nconducted for hate speech detection in European languages, whereas little\nattention has been paid to low-resource South Asian languages, making the\nsocial media vulnerable for millions of users. In particular, to the best of\nour knowledge, no study has been conducted for hate speech detection in Roman\nUrdu text, which is widely used in the sub-continent. In this study, we have\nscrapped more than 90,000 tweets and manually parsed them to identify 5,000\nRoman Urdu tweets. Subsequently, we have employed an iterative approach to\ndevelop guidelines and used them for generating the Hate Speech Roman Urdu 2020\ncorpus. The tweets in the this corpus are classified at three levels:\nNeutral-Hostile, Simple-Complex, and Offensive-Hate speech. As another\ncontribution, we have used five supervised learning techniques, including a\ndeep learning technique, to evaluate and compare their effectiveness for hate\nspeech detection. The results show that Logistic Regression outperformed all\nother techniques, including deep learning techniques for the two levels of\nclassification, by achieved an F1 score of 0.906 for distinguishing between\nNeutral-Hostile tweets, and 0.756 for distinguishing between Offensive-Hate\nspeech tweets.",
          "link": "http://arxiv.org/abs/2108.02830",
          "publishedOn": "2021-08-09T00:49:26.085Z",
          "wordCount": 719,
          "title": "Hate Speech Detection in Roman Urdu. (arXiv:2108.02830v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+ElFadeel_H/0/1/0/all/0/1\">Haytham ElFadeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peshterliev_S/0/1/0/all/0/1\">Stan Peshterliev</a>",
          "description": "Large transformer models, such as BERT, achieve state-of-the-art results in\nmachine reading comprehension (MRC) for open-domain question answering (QA).\nHowever, transformers have a high computational cost for inference which makes\nthem hard to apply to online QA systems for applications like voice assistants.\nTo reduce computational cost and latency, we propose decoupling the transformer\nMRC model into input-component and cross-component. The decoupling allows for\npart of the representation computation to be performed offline and cached for\nonline use. To retain the decoupled transformer accuracy, we devised a\nknowledge distillation objective from a standard transformer model. Moreover,\nwe introduce learned representation compression layers which help reduce by\nfour times the storage requirement for the cache. In experiments on the SQUAD\n2.0 dataset, a decoupled transformer reduces the computational cost and latency\nof open-domain MRC by 30-40% with only 1.2 points worse F1-score compared to a\nstandard transformer.",
          "link": "http://arxiv.org/abs/2108.02765",
          "publishedOn": "2021-08-06T00:51:45.090Z",
          "wordCount": 579,
          "title": "Decoupled Transformer for Scalable Inference in Open-domain Question Answering. (arXiv:2108.02765v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Popov_V/0/1/0/all/0/1\">Vadim Popov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vovk_I/0/1/0/all/0/1\">Ivan Vovk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gogoryan_V/0/1/0/all/0/1\">Vladimir Gogoryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadekova_T/0/1/0/all/0/1\">Tasnima Sadekova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kudinov_M/0/1/0/all/0/1\">Mikhail Kudinov</a>",
          "description": "Recently, denoising diffusion probabilistic models and generative score\nmatching have shown high potential in modelling complex data distributions\nwhile stochastic calculus has provided a unified point of view on these\ntechniques allowing for flexible inference schemes. In this paper we introduce\nGrad-TTS, a novel text-to-speech model with score-based decoder producing\nmel-spectrograms by gradually transforming noise predicted by encoder and\naligned with text input by means of Monotonic Alignment Search. The framework\nof stochastic differential equations helps us to generalize conventional\ndiffusion probabilistic models to the case of reconstructing data from noise\nwith different parameters and allows to make this reconstruction flexible by\nexplicitly controlling trade-off between sound quality and inference speed.\nSubjective human evaluation shows that Grad-TTS is competitive with\nstate-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We\nwill make the code publicly available shortly.",
          "link": "http://arxiv.org/abs/2105.06337",
          "publishedOn": "2021-08-06T00:51:45.076Z",
          "wordCount": 602,
          "title": "Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech. (arXiv:2105.06337v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seongmin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihwa Lee</a>",
          "description": "Text variational autoencoders (VAEs) are notorious for posterior collapse, a\nphenomenon where the model's decoder learns to ignore signals from the encoder.\nBecause posterior collapse is known to be exacerbated by expressive decoders,\nTransformers have seen limited adoption as components of text VAEs. Existing\nstudies that incorporate Transformers into text VAEs (Li et al., 2020; Fang et\nal., 2021) mitigate posterior collapse using massive pretraining, a technique\nunavailable to most of the research community without extensive computing\nresources. We present a simple two-phase training scheme to convert a\nsequence-to-sequence Transformer into a VAE with just finetuning. The resulting\nlanguage model is competitive with massively pretrained Transformer-based VAEs\nin some internal metrics while falling short on others. To facilitate training\nwe comprehensively explore the impact of common posterior collapse alleviation\ntechniques in the literature. We release our code for reproducability.",
          "link": "http://arxiv.org/abs/2108.02446",
          "publishedOn": "2021-08-06T00:51:45.066Z",
          "wordCount": 567,
          "title": "Finetuning Pretrained Transformers into Variational Autoencoders. (arXiv:2108.02446v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02725",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shaunak Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_M/0/1/0/all/0/1\">Mikhail Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_G/0/1/0/all/0/1\">Gaurav Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sviridenko_M/0/1/0/all/0/1\">Maxim Sviridenko</a>",
          "description": "Numerous online stock image libraries offer high quality yet copyright free\nimages for use in marketing campaigns. To assist advertisers in navigating such\nthird party libraries, we study the problem of automatically fetching relevant\nad images given the ad text (via a short textual query for images). Motivated\nby our observations in logged data on ad image search queries (given ad text),\nwe formulate a keyword extraction problem, where a keyword extracted from the\nad text (or its augmented version) serves as the ad image query. In this\ncontext, we propose VisualTextRank: an unsupervised method to (i) augment input\nad text using semantically similar ads, and (ii) extract the image query from\nthe augmented ad text. VisualTextRank builds on prior work on graph based\ncontext extraction (biased TextRank in particular) by leveraging both the text\nand image of similar ads for better keyword extraction, and using advertiser\ncategory specific biasing with sentence-BERT embeddings. Using data collected\nfrom the Verizon Media Native (Yahoo Gemini) ad platform's stock image search\nfeature for onboarding advertisers, we demonstrate the superiority of\nVisualTextRank compared to competitive keyword extraction baselines (including\nan $11\\%$ accuracy lift over biased TextRank). For the case when the stock\nimage library is restricted to English queries, we show the effectiveness of\nVisualTextRank on multilingual ads (translated to English) while leveraging\nsemantically similar English ads. Online tests with a simplified version of\nVisualTextRank led to a 28.7% increase in the usage of stock image search, and\na 41.6% increase in the advertiser onboarding rate in the Verizon Media Native\nad platform.",
          "link": "http://arxiv.org/abs/2108.02725",
          "publishedOn": "2021-08-06T00:51:45.047Z",
          "wordCount": 714,
          "title": "VisualTextRank: Unsupervised Graph-based Content Extraction for Automating Ad Text to Image Search. (arXiv:2108.02725v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkhasov_A/0/1/0/all/0/1\">Alex Malkhasov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manoshin_A/0/1/0/all/0/1\">Andrey Manoshin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dima_G/0/1/0/all/0/1\">George Dima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cserhati_R/0/1/0/all/0/1\">R&#xe9;ka Cserh&#xe1;ti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">Md.Sadek Hossain Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sardi_M/0/1/0/all/0/1\">Matt S&#xe1;rdi</a>",
          "description": "This report presents the results of the EENLP project, done as a part of EEML\n2021 summer school.\n\nIt presents a broad index of NLP resources for Eastern European languages,\nwhich, we hope, could be helpful for the NLP community; several new\nhand-crafted cross-lingual datasets focused on Eastern European languages, and\na sketch evaluation of cross-lingual transfer learning abilities of several\nmodern multilingual Transformer-based models.",
          "link": "http://arxiv.org/abs/2108.02605",
          "publishedOn": "2021-08-06T00:51:45.020Z",
          "wordCount": 520,
          "title": "EENLP: Cross-lingual Eastern European NLP Index. (arXiv:2108.02605v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuxin Ding</a>",
          "description": "Single online handwritten Chinese character recognition~(single OLHCCR) has\nachieved prominent performance. However, in real application scenarios, users\nalways write multiple Chinese characters to form one complete sentence and the\ncontextual information within these characters holds the significant potential\nto improve the accuracy, robustness and efficiency of sentence-level OLHCCR. In\nthis work, we first propose a simple and straightforward end-to-end network,\nnamely vanilla compositional network~(VCN) to tackle the sentence-level OLHCCR.\nIt couples convolutional neural network with sequence modeling architecture to\nexploit the handwritten character's previous contextual information. Although\nVCN performs much better than the state-of-the-art single OLHCCR model, it\nexposes high fragility when confronting with not well written characters such\nas sloppy writing, missing or broken strokes. To improve the robustness of\nsentence-level OLHCCR, we further propose a novel deep spatial-temporal fusion\nnetwork~(DSTFN). It utilizes a pre-trained autoregresssive framework as the\nbackbone component, which projects each Chinese character into word embeddings,\nand integrates the spatial glyph features of handwritten characters and their\ncontextual information multiple times at multi-layer fusion module. We also\nconstruct a large-scale sentence-level handwriting dataset, named as CSOHD to\nevaluate models. Extensive experiment results demonstrate that DSTFN achieves\nthe state-of-the-art performance, which presents strong robustness compared\nwith VCN and exiting single OLHCCR models. The in-depth empirical analysis and\ncase studies indicate that DSTFN can significantly improve the efficiency of\nhandwriting input, with the handwritten Chinese character with incomplete\nstrokes being recognized precisely.",
          "link": "http://arxiv.org/abs/2108.02561",
          "publishedOn": "2021-08-06T00:51:45.013Z",
          "wordCount": 682,
          "title": "Sentence-level Online Handwritten Chinese Character Recognition. (arXiv:2108.02561v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Larson_S/0/1/0/all/0/1\">Stefan Larson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Navtej Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_S/0/1/0/all/0/1\">Saarthak Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stewart_S/0/1/0/all/0/1\">Shanti Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_U/0/1/0/all/0/1\">Uma Krishnaswamy</a>",
          "description": "To be robust enough for widespread adoption, document analysis systems\ninvolving machine learning models must be able to respond correctly to inputs\nthat fall outside of the data distribution that was used to generate the data\non which the models were trained. This paper explores the ability of text\nclassifiers trained on standard document classification datasets to generalize\nto out-of-distribution documents at inference time. We take the Tobacco-3482\nand RVL-CDIP datasets as a starting point and generate new out-of-distribution\nevaluation datasets in order to analyze the generalization performance of\nmodels trained on these standard datasets. We find that models trained on the\nsmaller Tobacco-3482 dataset perform poorly on our new out-of-distribution\ndata, while text classification models trained on the larger RVL-CDIP exhibit\nsmaller performance drops.",
          "link": "http://arxiv.org/abs/2108.02684",
          "publishedOn": "2021-08-06T00:51:44.999Z",
          "wordCount": 570,
          "title": "Exploring Out-of-Distribution Generalization in Text Classifiers Trained on Tobacco-3482 and RVL-CDIP. (arXiv:2108.02684v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_B/0/1/0/all/0/1\">Bowen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor W. Tsang</a>",
          "description": "Aspect-level sentiment classification (ASC) aims to predict the fine-grained\nsentiment polarity towards a given aspect mentioned in a review. Despite recent\nadvances in ASC, enabling machines to preciously infer aspect sentiments is\nstill challenging. This paper tackles two challenges in ASC: (1) due to lack of\naspect knowledge, aspect representation derived in prior works is inadequate to\nrepresent aspect's exact meaning and property information; (2) prior works only\ncapture either local syntactic information or global relational information,\nthus missing either one of them leads to insufficient syntactic information. To\ntackle these challenges, we propose a novel ASC model which not only end-to-end\nembeds and leverages aspect knowledge but also marries the two kinds of\nsyntactic information and lets them compensate for each other. Our model\nincludes three key components: (1) a knowledge-aware gated recurrent memory\nnetwork recurrently integrates dynamically summarized aspect knowledge; (2) a\ndual syntax graph network combines both kinds of syntactic information to\ncomprehensively capture sufficient syntactic information; (3) a knowledge\nintegrating gate re-enhances the final representation with further needed\naspect knowledge; (4) an aspect-to-context attention mechanism aggregates the\naspect-related semantics from all hidden states into the final representation.\nExperimental results on several benchmark datasets demonstrate the\neffectiveness of our model, which overpass previous state-of-the-art models by\nlarge margins in terms of both Accuracy and Macro-F1.",
          "link": "http://arxiv.org/abs/2108.02352",
          "publishedOn": "2021-08-06T00:51:44.953Z",
          "wordCount": 673,
          "title": "Understand me, if you refer to Aspect Knowledge: Knowledge-aware Gated Recurrent Memory Network. (arXiv:2108.02352v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>",
          "description": "Probing classifiers have emerged as one of the prominent methodologies for\ninterpreting and analyzing deep neural network models of natural language\nprocessing. The basic idea is simple---a classifier is trained to predict some\nlinguistic property from a model's representations---and has been used to\nexamine a wide variety of models and properties. However, recent studies have\ndemonstrated various methodological weaknesses of this approach. This article\ncritically reviews the probing classifiers framework, highlighting their\npromises, shortcomings, and advances.",
          "link": "http://arxiv.org/abs/2102.12452",
          "publishedOn": "2021-08-06T00:51:44.821Z",
          "wordCount": 543,
          "title": "Probing Classifiers: Promises, Shortcomings, and Advances. (arXiv:2102.12452v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Having numerous potential applications and great impact, end-to-end speech\ntranslation (ST) has long been treated as an independent task, failing to fully\ndraw strength from the rapid advances of its sibling - text machine translation\n(MT). With text and audio inputs represented differently, the modality gap has\nrendered MT data and its end-to-end models incompatible with their ST\ncounterparts. In observation of this obstacle, we propose to bridge this\nrepresentation gap with Chimera. By projecting audio and text features to a\ncommon semantic representation, Chimera unifies MT and ST tasks and boosts the\nperformance on ST benchmarks, MuST-C and Augmented Librispeech, to a new\nstate-of-the-art. Specifically, Chimera obtains 27.1 BLEU on MuST-C EN-DE,\nimproving the SOTA by a +1.9 BLEU margin. Further experimental analyses\ndemonstrate that the shared semantic space indeed conveys common knowledge\nbetween these two tasks and thus paves a new way for augmenting training\nresources across modalities. Code, data, and resources are available at\nhttps://github.com/Glaciohound/Chimera-ST.",
          "link": "http://arxiv.org/abs/2105.03095",
          "publishedOn": "2021-08-06T00:51:44.807Z",
          "wordCount": 638,
          "title": "Learning Shared Semantic Space for Speech-to-Text Translation. (arXiv:2105.03095v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tjandra_A/0/1/0/all/0/1\">Andros Tjandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_D/0/1/0/all/0/1\">Diptanu Gon Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Frank Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kritika Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1\">Alexis Conneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sela_A/0/1/0/all/0/1\">Assaf Sela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saraf_Y/0/1/0/all/0/1\">Yatharth Saraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>",
          "description": "Language identification greatly impacts the success of downstream tasks such\nas automatic speech recognition. Recently, self-supervised speech\nrepresentations learned by wav2vec 2.0 have been shown to be very effective for\na range of speech tasks. We extend previous self-supervised work on language\nidentification by experimenting with pre-trained models which were learned on\nreal-world unconstrained speech in multiple languages and not just on English.\nWe show that models pre-trained on many languages perform better and enable\nlanguage identification systems that require very little labeled data to\nperform well. Results on a 25 languages setup show that with only 10 minutes of\nlabeled data per language, a cross-lingually pre-trained model can achieve over\n93% accuracy.",
          "link": "http://arxiv.org/abs/2107.04082",
          "publishedOn": "2021-08-06T00:51:44.765Z",
          "wordCount": 606,
          "title": "Improved Language Identification Through Cross-Lingual Self-Supervised Learning. (arXiv:2107.04082v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02776",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hono_Y/0/1/0/all/0/1\">Yukiya Hono</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kei Hashimoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oura_K/0/1/0/all/0/1\">Keiichiro Oura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nankaku_Y/0/1/0/all/0/1\">Yoshihiko Nankaku</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tokuda_K/0/1/0/all/0/1\">Keiichi Tokuda</a>",
          "description": "This paper presents Sinsy, a deep neural network (DNN)-based singing voice\nsynthesis (SVS) system. In recent years, DNNs have been utilized in statistical\nparametric SVS systems, and DNN-based SVS systems have demonstrated better\nperformance than conventional hidden Markov model-based ones. SVS systems are\nrequired to synthesize a singing voice with pitch and timing that strictly\nfollow a given musical score. Additionally, singing expressions that are not\ndescribed on the musical score, such as vibrato and timing fluctuations, should\nbe reproduced. The proposed system is composed of four modules: a time-lag\nmodel, a duration model, an acoustic model, and a vocoder, and singing voices\ncan be synthesized taking these characteristics of singing voices into account.\nTo better model a singing voice, the proposed system incorporates improved\napproaches to modeling pitch and vibrato and better training criteria into the\nacoustic model. In addition, we incorporated PeriodNet, a non-autoregressive\nneural vocoder with robustness for the pitch, into our systems to generate a\nhigh-fidelity singing voice waveform. Moreover, we propose automatic pitch\ncorrection techniques for DNN-based SVS to synthesize singing voices with\ncorrect pitch even if the training data has out-of-tune phrases. Experimental\nresults show our system can synthesize a singing voice with better timing, more\nnatural vibrato, and correct pitch, and it can achieve better mean opinion\nscores in subjective evaluation tests.",
          "link": "http://arxiv.org/abs/2108.02776",
          "publishedOn": "2021-08-06T00:51:44.726Z",
          "wordCount": 690,
          "title": "Sinsy: A Deep Neural Network-Based Singing Voice Synthesis System. (arXiv:2108.02776v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2010.07079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_D/0/1/0/all/0/1\">Da Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Margaret Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1\">Y-Lan Boureau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinan_E/0/1/0/all/0/1\">Emily Dinan</a>",
          "description": "Models trained on large unlabeled corpora of human interactions will learn\npatterns and mimic behaviors therein, which include offensive or otherwise\ntoxic behavior and unwanted biases. We investigate a variety of methods to\nmitigate these issues in the context of open-domain generative dialogue models.\nWe introduce a new human-and-model-in-the-loop framework for both training\nsafer models and for evaluating them, as well as a novel method to distill\nsafety considerations inside generative models without the use of an external\nclassifier at deployment time. We conduct experiments comparing these methods\nand find our new techniques are (i) safer than existing models as measured by\nautomatic and human evaluations while (ii) maintaining usability metrics such\nas engagingness relative to the state of the art. We then discuss the\nlimitations of this work by analyzing failure cases of our models.",
          "link": "http://arxiv.org/abs/2010.07079",
          "publishedOn": "2021-08-06T00:51:44.713Z",
          "wordCount": 612,
          "title": "Recipes for Safety in Open-domain Chatbots. (arXiv:2010.07079v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longyin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guodong Zhou</a>",
          "description": "Various neural-based methods have been proposed so far for joint mention\ndetection and coreference resolution. However, existing works on coreference\nresolution are mainly dependent on filtered mention representation, while other\nspans are largely neglected. In this paper, we aim at increasing the\nutilization rate of data and investigating whether those eliminated spans are\ntotally useless, or to what extent they can improve the performance of\ncoreference resolution. To achieve this, we propose a mention representation\nrefining strategy where spans highly related to mentions are well leveraged\nusing a pointer network for representation enhancing. Notably, we utilize an\nadditional loss term in this work to encourage the diversity between entity\nclusters. Experimental results on the document-level CoNLL-2012 Shared Task\nEnglish dataset show that eliminated spans are indeed much effective and our\napproach can achieve competitive results when compared with previous\nstate-of-the-art in coreference resolution.",
          "link": "http://arxiv.org/abs/2101.00737",
          "publishedOn": "2021-08-06T00:51:44.672Z",
          "wordCount": 619,
          "title": "Coreference Resolution: Are the eliminated spans totally worthless?. (arXiv:2101.00737v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yidi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_B/0/1/0/all/0/1\">Bidisha Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhavi_M/0/1/0/all/0/1\">Maulik Madhavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>",
          "description": "End-to-end intent classification using speech has numerous advantages\ncompared to the conventional pipeline approach using automatic speech\nrecognition (ASR), followed by natural language processing modules. It attempts\nto predict intent from speech without using an intermediate ASR module.\nHowever, such end-to-end framework suffers from the unavailability of large\nspeech resources with higher acoustic variation in spoken language\nunderstanding. In this work, we exploit the scope of the transformer\ndistillation method that is specifically designed for knowledge distillation\nfrom a transformer based language model to a transformer based speech model. In\nthis regard, we leverage the reliable and widely used bidirectional encoder\nrepresentations from transformers (BERT) model as a language model and transfer\nthe knowledge to build an acoustic model for intent classification using the\nspeech. In particular, a multilevel transformer based teacher-student model is\ndesigned, and knowledge distillation is performed across attention and hidden\nsub-layers of different transformer layers of the student and teacher models.\nWe achieve an intent classification accuracy of 99.10% and 88.79% for Fluent\nspeech corpus and ATIS database, respectively. Further, the proposed method\ndemonstrates better performance and robustness in acoustically degraded\ncondition compared to the baseline method.",
          "link": "http://arxiv.org/abs/2108.02598",
          "publishedOn": "2021-08-06T00:51:44.641Z",
          "wordCount": 635,
          "title": "Knowledge Distillation from BERT Transformer to Speech Transformer for Intent Classification. (arXiv:2108.02598v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demirel_E/0/1/0/all/0/1\">Emir Demirel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahlback_S/0/1/0/all/0/1\">Sven Ahlb&#xe4;ck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1\">Simon Dixon</a>",
          "description": "This paper makes several contributions to automatic lyrics transcription\n(ALT) research. Our main contribution is a novel variant of the Multistreaming\nTime-Delay Neural Network (MTDNN) architecture, called MSTRE-Net, which\nprocesses the temporal information using multiple streams in parallel with\nvarying resolutions keeping the network more compact, and thus with a faster\ninference and an improved recognition rate than having identical TDNN streams.\nIn addition, two novel preprocessing steps prior to training the acoustic model\nare proposed. First, we suggest using recordings from both monophonic and\npolyphonic domains during training the acoustic model. Second, we tag\nmonophonic and polyphonic recordings with distinct labels for discriminating\nnon-vocal silence and music instances during alignment. Moreover, we present a\nnew test set with a considerably larger size and a higher musical variability\ncompared to the existing datasets used in ALT literature, while maintaining the\ngender balance of the singers. Our best performing model sets the\nstate-of-the-art in lyrics transcription by a large margin. For\nreproducibility, we publicly share the identifiers to retrieve the data used in\nthis paper.",
          "link": "http://arxiv.org/abs/2108.02625",
          "publishedOn": "2021-08-06T00:51:44.625Z",
          "wordCount": 615,
          "title": "MSTRE-Net: Multistreaming Acoustic Modeling for Automatic Lyrics Transcription. (arXiv:2108.02625v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weinzierl_M/0/1/0/all/0/1\">Maxwell A. Weinzierl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harabagiu_S/0/1/0/all/0/1\">Sanda M. Harabagiu</a>",
          "description": "Enormous hope in the efficacy of vaccines became recently a successful\nreality in the fight against the COVID-19 pandemic. However, vaccine hesitancy,\nfueled by exposure to social media misinformation about COVID-19 vaccines\nbecame a major hurdle. Therefore, it is essential to automatically detect where\nmisinformation about COVID-19 vaccines on social media is spread and what kind\nof misinformation is discussed, such that inoculation interventions can be\ndelivered at the right time and in the right place, in addition to\ninterventions designed to address vaccine hesitancy. This paper is addressing\nthe first step in tackling hesitancy against COVID-19 vaccines, namely the\nautomatic detection of misinformation about the vaccines on Twitter, the social\nmedia platform that has the highest volume of conversations about COVID-19 and\nits vaccines. We present CoVaxLies, a new dataset of tweets judged relevant to\nseveral misinformation targets about COVID-19 vaccines on which a novel method\nof detecting misinformation was developed. Our method organizes CoVaxLies in a\nMisinformation Knowledge Graph as it casts misinformation detection as a graph\nlink prediction problem. The misinformation detection method detailed in this\npaper takes advantage of the link scoring functions provided by several\nknowledge embedding methods. The experimental results demonstrate the\nsuperiority of this method when compared with classification-based methods,\nwidely used currently.",
          "link": "http://arxiv.org/abs/2108.02314",
          "publishedOn": "2021-08-06T00:51:44.610Z",
          "wordCount": 688,
          "title": "Automatic Detection of COVID-19 Vaccine Misinformation with Graph Link Prediction. (arXiv:2108.02314v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.13838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhiying Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Raphael Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Ji Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>",
          "description": "Pretrained transformers achieve the state of the art across tasks in natural\nlanguage processing, motivating researchers to investigate their inner\nmechanisms. One common direction is to understand what features are important\nfor prediction. In this paper, we apply information bottlenecks to analyze the\nattribution of each feature for prediction on a black-box model. We use BERT as\nthe example and evaluate our approach both quantitatively and qualitatively. We\nshow the effectiveness of our method in terms of attribution and the ability to\nprovide insight into how information flows through layers. We demonstrate that\nour technique outperforms two competitive methods in degradation tests on four\ndatasets. Code is available at https://github.com/bazingagin/IBA.",
          "link": "http://arxiv.org/abs/2012.13838",
          "publishedOn": "2021-08-06T00:51:44.601Z",
          "wordCount": 594,
          "title": "Inserting Information Bottlenecks for Attribution in Transformers. (arXiv:2012.13838v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khorrami_K/0/1/0/all/0/1\">Khazar Khorrami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>",
          "description": "Systems that can find correspondences between multiple modalities, such as\nbetween speech and images, have great potential to solve different recognition\nand data analysis tasks in an unsupervised manner. This work studies multimodal\nlearning in the context of visually grounded speech (VGS) models, and focuses\non their recently demonstrated capability to extract spatiotemporal alignments\nbetween spoken words and the corresponding visual objects without ever been\nexplicitly trained for object localization or word recognition. As the main\ncontributions, we formalize the alignment problem in terms of an audiovisual\nalignment tensor that is based on earlier VGS work, introduce systematic\nmetrics for evaluating model performance in aligning visual objects and spoken\nwords, and propose a new VGS model variant for the alignment task utilizing\ncross-modal attention layer. We test our model and a previously proposed model\nin the alignment task using SPEECH-COCO captions coupled with MSCOCO images. We\ncompare the alignment performance using our proposed evaluation metrics to the\nsemantic retrieval task commonly used to evaluate VGS models. We show that\ncross-modal attention layer not only helps the model to achieve higher semantic\ncross-modal retrieval performance, but also leads to substantial improvements\nin the alignment performance between image object and spoken words.",
          "link": "http://arxiv.org/abs/2108.02562",
          "publishedOn": "2021-08-06T00:51:44.588Z",
          "wordCount": 652,
          "title": "Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models. (arXiv:2108.02562v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diallo_M/0/1/0/all/0/1\">Mountaga Diallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fourati_C/0/1/0/all/0/1\">Chayma Fourati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddad_H/0/1/0/all/0/1\">Hatem Haddad</a>",
          "description": "For easier communication, posting, or commenting on each others posts, people\nuse their dialects. In Africa, various languages and dialects exist. However,\nthey are still underrepresented and not fully exploited for analytical studies\nand research purposes. In order to perform approaches like Machine Learning and\nDeep Learning, datasets are required. One of the African languages is Bambara,\nused by citizens in different countries. However, no previous work on datasets\nfor this language was performed for Sentiment Analysis. In this paper, we\npresent the first common-crawl-based Bambara dialectal dataset dedicated for\nSentiment Analysis, available freely for Natural Language Processing research\npurposes.",
          "link": "http://arxiv.org/abs/2108.02524",
          "publishedOn": "2021-08-06T00:51:44.560Z",
          "wordCount": 550,
          "title": "Bambara Language Dataset for Sentiment Analysis. (arXiv:2108.02524v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xianfeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Ernan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_Q/0/1/0/all/0/1\">Qiu Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "This paper introduces WeChat AI's participation in WMT 2021 shared news\ntranslation task on English->Chinese, English->Japanese, Japanese->English and\nEnglish->German. Our systems are based on the Transformer (Vaswani et al.,\n2017) with several novel and effective variants. In our experiments, we employ\ndata filtering, large-scale synthetic data generation (i.e., back-translation,\nknowledge distillation, forward-translation, iterative in-domain knowledge\ntransfer), advanced finetuning approaches, and boosted Self-BLEU based model\nensemble. Our constrained systems achieve 36.9, 46.9, 27.8 and 31.3\ncase-sensitive BLEU scores on English->Chinese, English->Japanese,\nJapanese->English and English->German, respectively. The BLEU scores of\nEnglish->Chinese, English->Japanese and Japanese->English are the highest among\nall submissions, and that of English->German is the highest among all\nconstrained submissions.",
          "link": "http://arxiv.org/abs/2108.02401",
          "publishedOn": "2021-08-06T00:51:44.470Z",
          "wordCount": 560,
          "title": "WeChat Neural Machine Translation Systems for WMT21. (arXiv:2108.02401v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weijiang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Lei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuejian Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1\">Nong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>",
          "description": "The task of video-based commonsense captioning aims to generate event-wise\ncaptions and meanwhile provide multiple commonsense descriptions (e.g.,\nattribute, effect and intention) about the underlying event in the video. Prior\nworks explore the commonsense captions by using separate networks for different\ncommonsense types, which is time-consuming and lacks mining the interaction of\ndifferent commonsense. In this paper, we propose a Hybrid Reasoning Network\n(HybridNet) to endow the neural networks with the capability of semantic-level\nreasoning and word-level reasoning. Firstly, we develop multi-commonsense\nlearning for semantic-level reasoning by jointly training different commonsense\ntypes in a unified network, which encourages the interaction between the clues\nof multiple commonsense descriptions, event-wise captions and videos. Then,\nthere are two steps to achieve the word-level reasoning: (1) a memory module\nrecords the history predicted sequence from the previous generation processes;\n(2) a memory-routed multi-head attention (MMHA) module updates the word-level\nattention maps by incorporating the history information from the memory module\ninto the transformer decoder for word-level reasoning. Moreover, the multimodal\nfeatures are used to make full use of diverse knowledge for commonsense\nreasoning. Experiments and abundant analysis on the large-scale\nVideo-to-Commonsense benchmark show that our HybridNet achieves\nstate-of-the-art performance compared with other methods.",
          "link": "http://arxiv.org/abs/2108.02365",
          "publishedOn": "2021-08-06T00:51:44.457Z",
          "wordCount": 651,
          "title": "Hybrid Reasoning Network for Video-based Commonsense Captioning. (arXiv:2108.02365v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silverman_G/0/1/0/all/0/1\">Greg M. Silverman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finzel_R/0/1/0/all/0/1\">Raymond L. Finzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinz_M/0/1/0/all/0/1\">Michael V. Heinz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilakes_J/0/1/0/all/0/1\">Jake Vasilakes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solinsky_J/0/1/0/all/0/1\">Jacob C. Solinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McEwan_R/0/1/0/all/0/1\">Reed McEwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_B/0/1/0/all/0/1\">Benjamin C. Knoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tignanelli_C/0/1/0/all/0/1\">Christopher J. Tignanelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melton_G/0/1/0/all/0/1\">Genevieve B. Melton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pakhomov_S/0/1/0/all/0/1\">Serguei VS Pakhomov</a>",
          "description": "Our objective in this study is to investigate the behavior of Boolean\noperators on combining annotation output from multiple Natural Language\nProcessing (NLP) systems across multiple corpora and to assess how filtering by\naggregation of Unified Medical Language System (UMLS) Metathesaurus concepts\naffects system performance for Named Entity Recognition (NER) of UMLS concepts.\nWe used three corpora annotated for UMLS concepts: 2010 i2b2 VA challenge set\n(31,161 annotations), Multi-source Integrated Platform for Answering Clinical\nQuestions (MiPACQ) corpus (17,457 annotations including UMLS concept unique\nidentifiers), and Fairview Health Services corpus (44,530 annotations). Our\nresults showed that for UMLS concept matching, Boolean ensembling of the MiPACQ\ncorpus trended towards higher performance over individual systems. Use of an\napproximate grid-search can help optimize the precision-recall tradeoff and can\nprovide a set of heuristics for choosing an optimal set of ensembles.",
          "link": "http://arxiv.org/abs/2108.02255",
          "publishedOn": "2021-08-06T00:51:44.405Z",
          "wordCount": 605,
          "title": "An Empirical Study of UMLS Concept Extraction from Clinical Notes using Boolean Combination Ensembles. (arXiv:2108.02255v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wenjuan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yingnian Wu</a>",
          "description": "Transfer learning with large pretrained transformer-based language models\nlike BERT has become a dominating approach for most NLP tasks. Simply\nfine-tuning those large language models on downstream tasks or combining it\nwith task-specific pretraining is often not robust. In particular, the\nperformance considerably varies as the random seed changes or the number of\npretraining and/or fine-tuning iterations varies, and the fine-tuned model is\nvulnerable to adversarial attack. We propose a simple yet effective\nadapter-based approach to mitigate these issues. Specifically, we insert small\nbottleneck layers (i.e., adapter) within each layer of a pretrained model, then\nfix the pretrained layers and train the adapter layers on the downstream task\ndata, with (1) task-specific unsupervised pretraining and then (2)\ntask-specific supervised training (e.g., classification, sequence labeling).\nOur experiments demonstrate that such a training scheme leads to improved\nstability and adversarial robustness in transfer learning to various downstream\ntasks.",
          "link": "http://arxiv.org/abs/2108.02340",
          "publishedOn": "2021-08-06T00:51:44.369Z",
          "wordCount": 582,
          "title": "Robust Transfer Learning with Pretrained Language Models through Adapters. (arXiv:2108.02340v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>",
          "description": "Video captioning combines video understanding and language generation.\nDifferent from image captioning that describes a static image with details of\nalmost every object, video captioning usually considers a sequence of frames\nand biases towards focused objects, e.g., the objects that stay in focus\nregardless of the changing background. Therefore, detecting and properly\naccommodating focused objects is critical in video captioning. To enforce the\ndescription of focused objects and achieve controllable video captioning, we\npropose an Object-Oriented Non-Autoregressive approach (O2NA), which performs\ncaption generation in three steps: 1) identify the focused objects and predict\ntheir locations in the target caption; 2) generate the related attribute words\nand relation words of these focused objects to form a draft caption; and 3)\ncombine video information to refine the draft caption to a fluent final\ncaption. Since the focused objects are generated and located ahead of other\nwords, it is difficult to apply the word-by-word autoregressive generation\nprocess; instead, we adopt a non-autoregressive approach. The experiments on\ntwo benchmark datasets, i.e., MSR-VTT and MSVD, demonstrate the effectiveness\nof O2NA, which achieves results competitive with the state-of-the-arts but with\nboth higher diversity and higher inference speed.",
          "link": "http://arxiv.org/abs/2108.02359",
          "publishedOn": "2021-08-06T00:51:44.356Z",
          "wordCount": 642,
          "title": "O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable Video Captioning. (arXiv:2108.02359v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>",
          "description": "Language Models like ELMo and BERT have provided robust representations of\nnatural language, which serve as the language understanding component for a\ndiverse range of downstream tasks.Curriculum learning is a method that employs\na structured training regime instead, which has been leveraged in computer\nvision and machine translation to improve model training speed and model\nperformance. While language models have proven transformational for the natural\nlanguage processing community, these models have proven expensive,\nenergy-intensive, and challenging to train. In this work, we explore the effect\nof curriculum learning on language model pretraining using various\nlinguistically motivated curricula and evaluate transfer performance on the\nGLUE Benchmark. Despite a broad variety of training methodologies and\nexperiments we do not find compelling evidence that curriculum learning methods\nimprove language model training.",
          "link": "http://arxiv.org/abs/2108.02170",
          "publishedOn": "2021-08-05T01:56:19.793Z",
          "wordCount": 551,
          "title": "Curriculum learning for language modeling. (arXiv:2108.02170v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tucker_M/0/1/0/all/0/1\">Mycal Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Siddharth Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_D/0/1/0/all/0/1\">Dana Hughes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1\">Katia Sycara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Michael Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1\">Julie Shah</a>",
          "description": "Neural agents trained in reinforcement learning settings can learn to\ncommunicate among themselves via discrete tokens, accomplishing as a team what\nagents would be unable to do alone. However, the current standard of using\none-hot vectors as discrete communication tokens prevents agents from acquiring\nmore desirable aspects of communication such as zero-shot understanding.\nInspired by word embedding techniques from natural language processing, we\npropose neural agent architectures that enables them to communicate via\ndiscrete tokens derived from a learned, continuous space. We show in a decision\ntheoretic framework that our technique optimizes communication over a wide\nrange of scenarios, whereas one-hot tokens are only optimal under restrictive\nassumptions. In self-play experiments, we validate that our trained agents\nlearn to cluster tokens in semantically-meaningful ways, allowing them\ncommunicate in noisy environments where other techniques fail. Lastly, we\ndemonstrate both that agents using our method can effectively respond to novel\nhuman communication and that humans can understand unlabeled emergent agent\ncommunication, outperforming the use of one-hot communication.",
          "link": "http://arxiv.org/abs/2108.01828",
          "publishedOn": "2021-08-05T01:56:19.685Z",
          "wordCount": 606,
          "title": "Emergent Discrete Communication in SemanticSpaces. (arXiv:2108.01828v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02035",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengding Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huadong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>",
          "description": "Tuning pre-trained language models (PLMs) with task-specific prompts has been\na promising approach for text classification. Particularly, previous studies\nsuggest that prompt-tuning has remarkable superiority in the low-data scenario\nover the generic fine-tuning methods with extra classifiers. The core idea of\nprompt-tuning is to insert text pieces, i.e., template, to the input and\ntransform a classification problem into a masked language modeling problem,\nwhere a crucial step is to construct a projection, i.e., verbalizer, between a\nlabel space and a label word space. A verbalizer is usually handcrafted or\nsearched by gradient descent, which may lack coverage and bring considerable\nbias and high variances to the results. In this work, we focus on incorporating\nexternal knowledge into the verbalizer, forming a knowledgeable prompt-tuning\n(KPT), to improve and stabilize prompt-tuning. Specifically, we expand the\nlabel word space of the verbalizer using external knowledge bases (KBs) and\nrefine the expanded label word space with the PLM itself before predicting with\nthe expanded label word space. Extensive experiments on zero and few-shot text\nclassification tasks demonstrate the effectiveness of knowledgeable\nprompt-tuning.",
          "link": "http://arxiv.org/abs/2108.02035",
          "publishedOn": "2021-08-05T01:56:19.664Z",
          "wordCount": 617,
          "title": "Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification. (arXiv:2108.02035v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>",
          "description": "Assessing an AI agent that can converse in human language and understand\nvisual content is challenging. Generation metrics, such as BLEU scores favor\ncorrect syntax over semantics. Hence a discriminative approach is often used,\nwhere an agent ranks a set of candidate options. The mean reciprocal rank (MRR)\nmetric evaluates the model performance by taking into account the rank of a\nsingle human-derived answer. This approach, however, raises a new challenge:\nthe ambiguity and synonymy of answers, for instance, semantic equivalence\n(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative\ngain (NDCG) metric has been used to capture the relevance of all the correct\nanswers via dense annotations. However, the NDCG metric favors the usually\napplicable uncertain answers such as `I don't know. Crafting a model that\nexcels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should\nanswer a human-like reply and validate the correctness of any answer. To\naddress this issue, we describe a two-step non-parametric ranking approach that\ncan merge strong MRR and NDCG models. Using our approach, we manage to keep\nmost MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG\nstate-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won\nthe recent Visual Dialog 2020 challenge. Source code is available at\nhttps://github.com/idansc/mrr-ndcg.",
          "link": "http://arxiv.org/abs/2104.07511",
          "publishedOn": "2021-08-05T01:56:19.642Z",
          "wordCount": 697,
          "title": "Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09332",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seongsik Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Harksoo Kim</a>",
          "description": "Sentence-level relation extraction mainly aims to classify the relation\nbetween two entities in a sentence. The sentence-level relation extraction\ncorpus often contains data that are difficult for the model to infer or noise\ndata. In this paper, we propose a curriculum learning-based relation extraction\nmodel that splits data by difficulty and utilizes them for learning. In the\nexperiments with the representative sentence-level relation extraction\ndatasets, TACRED and Re-TACRED, the proposed method obtained an F1-score of\n75.0% and 91.4% respectively, which are the state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2107.09332",
          "publishedOn": "2021-08-05T01:56:19.624Z",
          "wordCount": 539,
          "title": "Improving Sentence-Level Relation Extraction through Curriculum Learning. (arXiv:2107.09332v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.10582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yining Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciao_D/0/1/0/all/0/1\">Daniel Ciao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>",
          "description": "Previous neural solvers of math word problems (MWPs) are learned with full\nsupervision and fail to generate diverse solutions. In this paper, we address\nthis issue by introducing a \\textit{weakly-supervised} paradigm for learning\nMWPs. Our method only requires the annotations of the final answers and can\ngenerate various solutions for a single problem. To boost weakly-supervised\nlearning, we propose a novel \\textit{learning-by-fixing} (LBF) framework, which\ncorrects the misperceptions of the neural network via symbolic reasoning.\nSpecifically, for an incorrect solution tree generated by the neural network,\nthe \\textit{fixing} mechanism propagates the error from the root node to the\nleaf nodes and infers the most probable fix that can be executed to get the\ndesired answer. To generate more diverse solutions, \\textit{tree\nregularization} is applied to guide the efficient shrinkage and exploration of\nthe solution space, and a \\textit{memory buffer} is designed to track and save\nthe discovered various fixes for each problem. Experimental results on the\nMath23K dataset show the proposed LBF framework significantly outperforms\nreinforcement learning baselines in weakly-supervised learning. Furthermore, it\nachieves comparable top-1 and much better top-3/5 answer accuracies than\nfully-supervised methods, demonstrating its strength in producing diverse\nsolutions.",
          "link": "http://arxiv.org/abs/2012.10582",
          "publishedOn": "2021-08-05T01:56:19.617Z",
          "wordCount": 668,
          "title": "Learning by Fixing: Solving Math Word Problems with Weak Supervision. (arXiv:2012.10582v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ormazabal_A/0/1/0/all/0/1\">Aitor Ormazabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1\">Aitor Soroa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labaka_G/0/1/0/all/0/1\">Gorka Labaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>",
          "description": "Recent research on cross-lingual word embeddings has been dominated by\nunsupervised mapping approaches that align monolingual embeddings. Such methods\ncritically rely on those embeddings having a similar structure, but it was\nrecently shown that the separate training in different languages causes\ndepartures from this assumption. In this paper, we propose an alternative\napproach that does not have this limitation, while requiring a weak seed\ndictionary (e.g., a list of identical words) as the only form of supervision.\nRather than aligning two fixed embedding spaces, our method works by fixing the\ntarget language embeddings, and learning a new set of embeddings for the source\nlanguage that are aligned with them. To that end, we use an extension of\nskip-gram that leverages translated context words as anchor points, and\nincorporates self-learning and iterative restarts to reduce the dependency on\nthe initial dictionary. Our approach outperforms conventional mapping methods\non bilingual lexicon induction, and obtains competitive results in the\ndownstream XNLI task.",
          "link": "http://arxiv.org/abs/2012.15715",
          "publishedOn": "2021-08-05T01:56:19.610Z",
          "wordCount": 640,
          "title": "Beyond Offline Mapping: Learning Cross Lingual Word Embeddings through Context Anchoring. (arXiv:2012.15715v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Braude_T/0/1/0/all/0/1\">Tom Braude</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander Schwing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1\">Ariel Shamir</a>",
          "description": "We address the problem of visual storytelling, i.e., generating a story for a\ngiven sequence of images. While each sentence of the story should describe a\ncorresponding image, a coherent story also needs to be consistent and relate to\nboth future and past images. To achieve this we develop ordered image attention\n(OIA). OIA models interactions between the sentence-corresponding image and\nimportant regions in other images of the sequence. To highlight the important\nobjects, a message-passing-like algorithm collects representations of those\nobjects in an order-aware manner. To generate the story's sentences, we then\nhighlight important image attention vectors with an Image-Sentence Attention\n(ISA). Further, to alleviate common linguistic mistakes like repetitiveness, we\nintroduce an adaptive prior. The obtained results improve the METEOR score on\nthe VIST dataset by 1%. In addition, an extensive human study verifies\ncoherency improvements and shows that OIA and ISA generated stories are more\nfocused, shareable, and image-grounded.",
          "link": "http://arxiv.org/abs/2108.02180",
          "publishedOn": "2021-08-05T01:56:19.584Z",
          "wordCount": 597,
          "title": "Towards Coherent Visual Storytelling with Ordered Image Attention. (arXiv:2108.02180v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shih-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>",
          "description": "We present ReadOnce Transformers, an approach to convert a transformer-based\nmodel into one that can build an information-capturing, task-independent, and\ncompressed representation of text. The resulting representation is reusable\nacross different examples and tasks, thereby requiring a document shared across\nmany examples or tasks to only be \\emph{read once}. This leads to faster\ntraining and evaluation of models. Additionally, we extend standard\ntext-to-text transformer models to Representation+Text-to-text models, and\nevaluate on multiple downstream tasks: multi-hop QA, abstractive QA, and\nlong-document summarization. Our one-time computed representation results in a\n2x-5x speedup compared to standard text-to-text models, while the compression\nalso allows existing language models to handle longer documents without the\nneed for designing new pre-trained models.",
          "link": "http://arxiv.org/abs/2010.12854",
          "publishedOn": "2021-08-05T01:56:19.538Z",
          "wordCount": 583,
          "title": "ReadOnce Transformers: Reusable Representations of Text for Transformers. (arXiv:2010.12854v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.06028",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_A/0/1/0/all/0/1\">Andy T. Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>",
          "description": "We introduce a self-supervised speech pre-training method called TERA, which\nstands for Transformer Encoder Representations from Alteration. Recent\napproaches often learn by using a single auxiliary task like contrastive\nprediction, autoregressive prediction, or masked reconstruction. Unlike\nprevious methods, we use alteration along three orthogonal axes to pre-train\nTransformer Encoders on a large amount of unlabeled speech. The model learns\nthrough the reconstruction of acoustic frames from their altered counterpart,\nwhere we use a stochastic policy to alter along various dimensions: time,\nfrequency, and magnitude. TERA can be used for speech representations\nextraction or fine-tuning with downstream models. We evaluate TERA on several\ndownstream tasks, including phoneme classification, keyword spotting, speaker\nrecognition, and speech recognition. We present a large-scale comparison of\nvarious self-supervised models. TERA achieves strong performance in the\ncomparison by improving upon surface features and outperforming previous\nmodels. In our experiments, we study the effect of applying different\nalteration techniques, pre-training on more data, and pre-training on various\nfeatures. We analyze different model sizes and find that smaller models are\nstrong representation learners than larger models, while larger models are more\neffective for downstream fine-tuning than smaller models. Furthermore, we show\nthe proposed method is transferable to downstream datasets not used in\npre-training.",
          "link": "http://arxiv.org/abs/2007.06028",
          "publishedOn": "2021-08-05T01:56:19.521Z",
          "wordCount": 707,
          "title": "TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech. (arXiv:2007.06028v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuebler_J/0/1/0/all/0/1\">Joseph Kuebler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_L/0/1/0/all/0/1\">Lingbo Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>",
          "description": "Information extraction (IE) in scientific literature has facilitated many\ndown-stream tasks. OpenIE, which does not require any relation schema but\nidentifies a relational phrase to describe the relationship between a subject\nand an object, is being a trending topic of IE in sciences. The subjects,\nobjects, and relations are often multiword expressions, which brings challenges\nfor methods to identify the boundaries of the expressions given very limited or\neven no training data. In this work, we present a set of rules for extracting\nstructured information based on dependency parsing that can be applied to any\nscientific dataset requiring no expert's annotation. Results on novel datasets\nshow the effectiveness of the proposed method. We discuss negative results as\nwell.",
          "link": "http://arxiv.org/abs/2108.02074",
          "publishedOn": "2021-08-05T01:56:19.502Z",
          "wordCount": 545,
          "title": "Multi-Round Parsing-based Multiword Rules for Scientific OpenIE. (arXiv:2108.02074v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.05150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tsung-Yen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Michael Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chow_Y/0/1/0/all/0/1\">Yinlam Chow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramadge_P/0/1/0/all/0/1\">Peter J. Ramadge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>",
          "description": "While safe reinforcement learning (RL) holds great promise for many practical\napplications like robotics or autonomous cars, current approaches require\nspecifying constraints in mathematical form. Such specifications demand domain\nexpertise, limiting the adoption of safe RL. In this paper, we propose learning\nto interpret natural language constraints for safe RL. To this end, we first\nintroduce HazardWorld, a new multi-task benchmark that requires an agent to\noptimize reward while not violating constraints specified in free-form text. We\nthen develop an agent with a modular architecture that can interpret and adhere\nto such textual constraints while learning new tasks. Our model consists of (1)\na constraint interpreter that encodes textual constraints into spatial and\ntemporal representations of forbidden states, and (2) a policy network that\nuses these representations to produce a policy achieving minimal constraint\nviolations during training. Across different domains in HazardWorld, we show\nthat our method achieves higher rewards (up to11x) and fewer constraint\nviolations (by 1.8x) compared to existing approaches. However, in terms of\nabsolute performance, HazardWorld still poses significant challenges for agents\nto learn efficiently, motivating the need for future work.",
          "link": "http://arxiv.org/abs/2010.05150",
          "publishedOn": "2021-08-05T01:56:19.494Z",
          "wordCount": 664,
          "title": "Safe Reinforcement Learning with Natural Language Constraints. (arXiv:2010.05150v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bouscarrat_L/0/1/0/all/0/1\">L&#xe9;o Bouscarrat</a> (LIS, TALEP, QARMA), <a href=\"http://arxiv.org/find/cs/1/au:+Bonnefoy_A/0/1/0/all/0/1\">Antoine Bonnefoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capponi_C/0/1/0/all/0/1\">C&#xe9;cile Capponi</a> (LIS, QARMA), <a href=\"http://arxiv.org/find/cs/1/au:+Ramisch_C/0/1/0/all/0/1\">Carlos Ramisch</a> (LIS, TALEP)",
          "description": "This paper explains our participation in task 1 of the CASE 2021 shared task.\nThis task is about multilingual event extraction from news. We focused on\nsub-task 4, event information extraction. This sub-task has a small training\ndataset and we fine-tuned a multilingual BERT to solve this sub-task. We\nstudied the instability problem on the dataset and tried to mitigate it.",
          "link": "http://arxiv.org/abs/2106.14625",
          "publishedOn": "2021-08-05T01:56:19.485Z",
          "wordCount": 569,
          "title": "AMU-EURANOVA at CASE 2021 Task 1: Assessing the stability of multilingual BERT. (arXiv:2106.14625v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barylska_K/0/1/0/all/0/1\">Kamila Barylska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gogolinska_A/0/1/0/all/0/1\">Anna Gogoli&#x144;ska</a>",
          "description": "Reversible computations constitute an unconventional form of computing where\nany sequence of performed operations can be undone by executing in reverse\norder at any point during a computation. It has been attracting increasing\nattention as it provides opportunities for low-power computation, being at the\nsame time essential or eligible in various applications. In recent work, we\nhave proposed a structural way of translating Reversing Petri Nets (RPNs) - a\ntype of Petri nets that embeds reversible computation, to bounded Coloured\nPetri Nets (CPNs) - an extension of traditional Petri Nets, where tokens carry\ndata values. Three reversing semantics are possible in RPNs: backtracking\n(reversing of the lately executed action), causal reversing (action can be\nreversed only when all its effects have been undone) and out of causal\nreversing (any previously performed action can be reversed). In this paper, we\nextend the RPN to CPN translation with formal proofs of correctness. Moreover,\nthe possibility of introduction of cycles to RPNs is discussed. We analyze\nwhich type of cycles could be allowed in RPNs to ensure consistency with the\ncurrent semantics. It emerged that the most interesting case related to cycles\nin RPNs occurs in causal semantics, where various interpretations of dependency\nresult in different net's behaviour during reversing. Three definitions of\ndependence are presented and discussed.",
          "link": "http://arxiv.org/abs/2108.02167",
          "publishedOn": "2021-08-05T01:56:19.404Z",
          "wordCount": 643,
          "title": "Acyclic and Cyclic Reversing Computations in Petri Nets. (arXiv:2108.02167v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitropolsky_D/0/1/0/all/0/1\">Daniel Mitropolsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1\">Michael J. Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadimitriou_C/0/1/0/all/0/1\">Christos H. Papadimitriou</a>",
          "description": "We describe a parser of English effectuated by biologically plausible neurons\nand synapses, and implemented through the Assembly Calculus, a recently\nproposed computational framework for cognitive function. We demonstrate that\nthis device is capable of correctly parsing reasonably nontrivial sentences.\nWhile our experiments entail rather simple sentences in English, our results\nsuggest that the parser can be extended beyond what we have implemented, to\nseveral directions encompassing much of language. For example, we present a\nsimple Russian version of the parser, and discuss how to handle recursion,\nembedding, and polysemy.",
          "link": "http://arxiv.org/abs/2108.02189",
          "publishedOn": "2021-08-05T01:56:19.396Z",
          "wordCount": 513,
          "title": "A Biologically Plausible Parser. (arXiv:2108.02189v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hori_C/0/1/0/all/0/1\">Chiori Hori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hori_T/0/1/0/all/0/1\">Takaaki Hori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1\">Jonathan Le Roux</a>",
          "description": "Video captioning is an essential technology to understand scenes and describe\nevents in natural language. To apply it to real-time monitoring, a system needs\nnot only to describe events accurately but also to produce the captions as soon\nas possible. Low-latency captioning is needed to realize such functionality,\nbut this research area for online video captioning has not been pursued yet.\nThis paper proposes a novel approach to optimize each caption's output timing\nbased on a trade-off between latency and caption quality. An audio-visual\nTrans-former is trained to generate ground-truth captions using only a small\nportion of all video frames, and to mimic outputs of a pre-trained Transformer\nto which all the frames are given. A CNN-based timing detector is also trained\nto detect a proper output timing, where the captions generated by the two\nTrans-formers become sufficiently close to each other. With the jointly trained\nTransformer and timing detector, a caption can be generated in the early stages\nof an event-triggered video clip, as soon as an event happens or when it can be\nforecasted. Experiments with the ActivityNet Captions dataset show that our\napproach achieves 94% of the caption quality of the upper bound given by the\npre-trained Transformer using the entire video clips, using only 28% of frames\nfrom the beginning.",
          "link": "http://arxiv.org/abs/2108.02147",
          "publishedOn": "2021-08-05T01:56:19.253Z",
          "wordCount": 659,
          "title": "Optimizing Latency for Online Video CaptioningUsing Audio-Visual Transformers. (arXiv:2108.02147v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1\">Leonard Adolphs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1\">Shehzaad Dhuliawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>",
          "description": "Large pre-trained language models (LMs) are capable of not only recovering\nlinguistic but also factual and commonsense knowledge. To access the knowledge\nstored in mask-based LMs, we can use cloze-style questions and let the model\nfill in the blank. The flexibility advantage over structured knowledge bases\ncomes with the drawback of finding the right query for a certain information\nneed. Inspired by human behavior to disambiguate a question, we propose to\nquery LMs by example. To clarify the ambivalent question \"Who does Neuer play\nfor?\", a successful strategy is to demonstrate the relation using another\nsubject, e.g., \"Ronaldo plays for Portugal. Who does Neuer play for?\". We apply\nthis approach of querying by example to the LAMA probe and obtain substantial\nimprovements of up to 37.8% for BERT-large on the T-REx data when providing\nonly 10 demonstrations--even outperforming a baseline that queries the model\nwith up to 40 paraphrases of the question. The examples are provided through\nthe model's context and thus require neither fine-tuning nor an additional\nforward pass. This suggests that LMs contain more factual and commonsense\nknowledge than previously assumed--if we query the model in the right way.",
          "link": "http://arxiv.org/abs/2108.01928",
          "publishedOn": "2021-08-05T01:56:19.202Z",
          "wordCount": 622,
          "title": "How to Query Language Models?. (arXiv:2108.01928v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01879",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1\">Shahbaz Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousef_T/0/1/0/all/0/1\">Tariq Yousef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Khatib_K/0/1/0/all/0/1\">Khalid Al-Khatib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janicke_S/0/1/0/all/0/1\">Stefan J&#xe4;nicke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>",
          "description": "This paper introduces Summary Explorer, a new tool to support the manual\ninspection of text summarization systems by compiling the outputs of\n55~state-of-the-art single document summarization approaches on three benchmark\ndatasets, and visually exploring them during a qualitative assessment. The\nunderlying design of the tool considers three well-known summary quality\ncriteria (coverage, faithfulness, and position bias), encapsulated in a guided\nassessment based on tailored visualizations. The tool complements existing\napproaches for locally debugging summarization models and improves upon them.\nThe tool is available at https://tldr.webis.de/",
          "link": "http://arxiv.org/abs/2108.01879",
          "publishedOn": "2021-08-05T01:56:19.186Z",
          "wordCount": 526,
          "title": "Summary Explorer: Visualizing the State of the Art in Text Summarization. (arXiv:2108.01879v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>",
          "description": "Despite the success of multilingual sequence-to-sequence pretraining, most\nexisting approaches rely on monolingual corpora, and do not make use of the\nstrong cross-lingual signal contained in parallel data. In this paper, we\npresent PARADISE (PARAllel & Denoising Integration in SEquence-to-sequence\nmodels), which extends the conventional denoising objective used to train these\nmodels by (i) replacing words in the noised sequence according to a\nmultilingual dictionary, and (ii) predicting the reference translation\naccording to a parallel corpus instead of recovering the original sequence. Our\nexperiments on machine translation and cross-lingual natural language inference\nshow an average improvement of 2.0 BLEU points and 6.7 accuracy points from\nintegrating parallel data into pretraining, respectively, obtaining results\nthat are competitive with several popular models at a fraction of their\ncomputational cost.",
          "link": "http://arxiv.org/abs/2108.01887",
          "publishedOn": "2021-08-05T01:56:19.125Z",
          "wordCount": 561,
          "title": "PARADISE: Exploiting Parallel Data for Multilingual Sequence-to-Sequence Pretraining. (arXiv:2108.01887v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Loge_C/0/1/0/all/0/1\">C&#xe9;cile Log&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_E/0/1/0/all/0/1\">Emily Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dadey_D/0/1/0/all/0/1\">David Yaw Amoah Dadey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>",
          "description": "Recent advances in Natural Language Processing (NLP), and specifically\nautomated Question Answering (QA) systems, have demonstrated both impressive\nlinguistic fluency and a pernicious tendency to reflect social biases. In this\nstudy, we introduce Q-Pain, a dataset for assessing bias in medical QA in the\ncontext of pain management, one of the most challenging forms of clinical\ndecision-making. Along with the dataset, we propose a new, rigorous framework,\nincluding a sample experimental design, to measure the potential biases present\nwhen making treatment decisions. We demonstrate its use by assessing two\nreference Question-Answering systems, GPT-2 and GPT-3, and find statistically\nsignificant differences in treatment between intersectional race-gender\nsubgroups, thus reaffirming the risks posed by AI in medical settings, and the\nneed for datasets like ours to ensure safety before medical AI applications are\ndeployed.",
          "link": "http://arxiv.org/abs/2108.01764",
          "publishedOn": "2021-08-05T01:56:19.054Z",
          "wordCount": 602,
          "title": "Q-Pain: A Question Answering Dataset to Measure Social Bias in Pain Management. (arXiv:2108.01764v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seongmin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1\">Dongchan Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paik_S/0/1/0/all/0/1\">Sangyoun Paik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Subong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazakova_A/0/1/0/all/0/1\">Alena Kazakova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihwa Lee</a>",
          "description": "Fine-tuning pretrained language models (LMs) is a popular approach to\nautomatic speech recognition (ASR) error detection during post-processing.\nWhile error detection systems often take advantage of statistical language\narchetypes captured by LMs, at times the pretrained knowledge can hinder error\ndetection performance. For instance, presence of speech disfluencies might\nconfuse the post-processing system into tagging disfluent but accurate\ntranscriptions as ASR errors. Such confusion occurs because both error\ndetection and disfluency detection tasks attempt to identify tokens at\nstatistically unlikely positions. This paper proposes a scheme to improve\nexisting LM-based ASR error detection systems, both in terms of detection\nscores and resilience to such distracting auxiliary tasks. Our approach adopts\nthe popular mixup method in text feature space and can be utilized with any\nblack-box ASR output. To demonstrate the effectiveness of our method, we\nconduct post-processing experiments with both traditional and end-to-end ASR\nsystems (both for English and Korean languages) with 5 different speech\ncorpora. We find that our method improves both ASR error detection F 1 scores\nand reduces the number of correctly transcribed disfluencies wrongly detected\nas ASR errors. Finally, we suggest methods to utilize resulting LMs directly in\nsemi-supervised ASR training.",
          "link": "http://arxiv.org/abs/2108.01812",
          "publishedOn": "2021-08-05T01:56:19.038Z",
          "wordCount": 649,
          "title": "Improving Distinction between ASR Errors and Speech Disfluencies with Feature Space Interpolation. (arXiv:2108.01812v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sachin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malmi_E/0/1/0/all/0/1\">Eric Malmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Severyn_A/0/1/0/all/0/1\">Aliaksei Severyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>",
          "description": "As large-scale language model pretraining pushes the state-of-the-art in text\ngeneration, recent work has turned to controlling attributes of the text such\nmodels generate. While modifying the pretrained models via fine-tuning remains\nthe popular approach, it incurs a significant computational cost and can be\ninfeasible due to lack of appropriate data. As an alternative, we propose\nMuCoCO -- a flexible and modular algorithm for controllable inference from\npretrained models. We formulate the decoding process as an optimization problem\nwhich allows for multiple attributes we aim to control to be easily\nincorporated as differentiable constraints to the optimization. By relaxing\nthis discrete optimization to a continuous one, we make use of Lagrangian\nmultipliers and gradient-descent based techniques to generate the desired text.\nWe evaluate our approach on controllable machine translation and style transfer\nwith multiple sentence-level attributes and observe significant improvements\nover baselines.",
          "link": "http://arxiv.org/abs/2108.01850",
          "publishedOn": "2021-08-05T01:56:19.015Z",
          "wordCount": 576,
          "title": "Controlled Text Generation as Continuous Optimization with Multiple Constraints. (arXiv:2108.01850v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1\">Vivek Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>",
          "description": "In this shared task, we seek the participating teams to investigate the\nfactors influencing the quality of the code-mixed text generation systems. We\nsynthetically generate code-mixed Hinglish sentences using two distinct\napproaches and employ human annotators to rate the generation quality. We\npropose two subtasks, quality rating prediction and annotators' disagreement\nprediction of the synthetic Hinglish dataset. The proposed subtasks will put\nforward the reasoning and explanation of the factors influencing the quality\nand human perception of the code-mixed text.",
          "link": "http://arxiv.org/abs/2108.01861",
          "publishedOn": "2021-08-05T01:56:18.987Z",
          "wordCount": 519,
          "title": "Quality Evaluation of the Low-Resource Synthetically Generated Code-Mixed Hinglish Text. (arXiv:2108.01861v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1\">Zaid Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>",
          "description": "Multimodal target/aspect sentiment classification combines multimodal\nsentiment analysis and aspect/target sentiment classification. The goal of the\ntask is to combine vision and language to understand the sentiment towards a\ntarget entity in a sentence. Twitter is an ideal setting for the task because\nit is inherently multimodal, highly emotional, and affects real world events.\nHowever, multimodal tweets are short and accompanied by complex, possibly\nirrelevant images. We introduce a two-stream model that translates images in\ninput space using an object-aware transformer followed by a single-pass\nnon-autoregressive text generation approach. We then leverage the translation\nto construct an auxiliary sentence that provides multimodal information to a\nlanguage model. Our approach increases the amount of text available to the\nlanguage model and distills the object-level information in complex images. We\nachieve state-of-the-art performance on two multimodal Twitter datasets without\nmodifying the internals of the language model to accept multimodal data,\ndemonstrating the effectiveness of our translation. In addition, we explain a\nfailure mode of a popular approach for aspect sentiment analysis when applied\nto tweets. Our code is available at\n\\textcolor{blue}{\\url{https://github.com/codezakh/exploiting-BERT-thru-translation}}.",
          "link": "http://arxiv.org/abs/2108.01682",
          "publishedOn": "2021-08-05T01:56:18.976Z",
          "wordCount": 623,
          "title": "Exploiting BERT For Multimodal Target SentimentClassification Through Input Space Translation. (arXiv:2108.01682v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Davani_A/0/1/0/all/0/1\">Aida Mostafazadeh Davani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omrani_A/0/1/0/all/0/1\">Ali Omrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_B/0/1/0/all/0/1\">Brendan Kennedy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atari_M/0/1/0/all/0/1\">Mohammad Atari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Morteza Dehghani</a>",
          "description": "Bias mitigation approaches reduce models' dependence on sensitive features of\ndata, such as social group tokens (SGTs), resulting in equal predictions across\nthe sensitive features. In hate speech detection, however, equalizing model\npredictions may ignore important differences among targeted social groups, as\nhate speech can contain stereotypical language specific to each SGT. Here, to\ntake the specific language about each SGT into account, we rely on\ncounterfactual fairness and equalize predictions among counterfactuals,\ngenerated by changing the SGTs. Our method evaluates the similarity in sentence\nlikelihoods (via pre-trained language models) among counterfactuals, to treat\nSGTs equally only within interchangeable contexts. By applying logit pairing to\nequalize outcomes on the restricted set of counterfactuals for each instance,\nwe improve fairness metrics while preserving model performance on hate speech\ndetection.",
          "link": "http://arxiv.org/abs/2108.01721",
          "publishedOn": "2021-08-05T01:56:18.964Z",
          "wordCount": 582,
          "title": "Improving Counterfactual Generation for Fair Hate Speech Detection. (arXiv:2108.01721v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alshubaily_I/0/1/0/all/0/1\">Ibrahim Alshubaily</a>",
          "description": "The vast majority of textual content is unstructured, making automated\nclassification an important task for many applications. The goal of text\nclassification is to automatically classify text documents into one or more\npredefined categories. Recently proposed simple architectures for text\nclassification such as Convolutional Neural Networks for Sentence\nClassification by Kim, Yoon showed promising results. In this paper, we propose\nincorporating an attention mechanism into the network to boost its performance,\nwe also propose WordRank for vocabulary selection to reduce the network\nembedding parameters and speed up training with minimum accuracy loss. By\nadopting the proposed ideas TextCNN accuracy on 20News increased from 94.79 to\n96.88, moreover, the number of parameters for the embedding layer can be\nreduced substantially with little accuracy loss by using WordRank. By using\nWordRank for vocabulary selection we can reduce the number of parameters by\nmore than 5x from 7.9M to 1.5M, and the accuracy will only decrease by 1.2%.",
          "link": "http://arxiv.org/abs/2108.01921",
          "publishedOn": "2021-08-05T01:56:18.943Z",
          "wordCount": 575,
          "title": "TextCNN with Attention for Text Classification. (arXiv:2108.01921v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghangam_S/0/1/0/all/0/1\">Sangeeta Ghangam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitenack_D/0/1/0/all/0/1\">Daniel Whitenack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemecek_J/0/1/0/all/0/1\">Joshua Nemecek</a>",
          "description": "Running automatic speech recognition (ASR) on edge devices is non-trivial due\nto resource constraints, especially in scenarios that require supporting\nmultiple languages. We propose a new approach to enable multilingual speech\nrecognition on edge devices. This approach uses both language identification\nand accent identification to select one of multiple monolingual ASR models\non-the-fly, each fine-tuned for a particular accent. Initial results for both\nrecognition performance and resource usage are promising with our approach\nusing less than 1/12th of the memory consumed by other solutions.",
          "link": "http://arxiv.org/abs/2108.02034",
          "publishedOn": "2021-08-05T01:56:18.924Z",
          "wordCount": 535,
          "title": "Dyn-ASR: Compact, Multilingual Speech Recognition via Spoken Language and Accent Identification. (arXiv:2108.02034v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balazy_K/0/1/0/all/0/1\">Klaudia Ba&#x142;azy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banaei_M/0/1/0/all/0/1\">Mohammadreza Banaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lebret_R/0/1/0/all/0/1\">R&#xe9;mi Lebret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1\">Jacek Tabor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aberer_K/0/1/0/all/0/1\">Karl Aberer</a>",
          "description": "The adoption of Transformer-based models in natural language processing (NLP)\nhas led to great success using a massive number of parameters. However, due to\ndeployment constraints in edge devices, there has been a rising interest in the\ncompression of these models to improve their inference time and memory\nfootprint. This paper presents a novel loss objective to compress token\nembeddings in the Transformer-based models by leveraging an AutoEncoder\narchitecture. More specifically, we emphasize the importance of the direction\nof compressed embeddings with respect to original uncompressed embeddings. The\nproposed method is task-agnostic and does not require further language modeling\npre-training. Our method significantly outperforms the commonly used SVD-based\nmatrix-factorization approach in terms of initial language model Perplexity.\nMoreover, we evaluate our proposed approach over SQuAD v1.1 dataset and several\ndownstream tasks from the GLUE benchmark, where we also outperform the baseline\nin most scenarios. Our code is public.",
          "link": "http://arxiv.org/abs/2106.08181",
          "publishedOn": "2021-08-04T01:59:20.836Z",
          "wordCount": 622,
          "title": "Direction is what you need: Improving Word Embedding Compression in Large Language Models. (arXiv:2106.08181v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_S/0/1/0/all/0/1\">Shih-Hsuan Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_T/0/1/0/all/0/1\">Tien-Hong Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fu-An Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Berlin Chen</a>",
          "description": "How to effectively incorporate cross-utterance information cues into a neural\nlanguage model (LM) has emerged as one of the intriguing issues for automatic\nspeech recognition (ASR). Existing research efforts on improving\ncontextualization of an LM typically regard previous utterances as a sequence\nof additional input and may fail to capture complex global structural\ndependencies among these utterances. In view of this, we in this paper seek to\nrepresent the historical context information of an utterance as\ngraph-structured data so as to distill cross-utterances, global word\ninteraction relationships. To this end, we apply a graph convolutional network\n(GCN) on the resulting graph to obtain the corresponding GCN embeddings of\nhistorical words. GCN has recently found its versatile applications on\nsocial-network analysis, text summarization, and among others due mainly to its\nability of effectively capturing rich relational information among elements.\nHowever, GCN remains largely underexplored in the context of ASR, especially\nfor dealing with conversational speech. In addition, we frame ASR N-best\nreranking as a prediction problem, leveraging bidirectional encoder\nrepresentations from transformers (BERT) as the vehicle to not only seize the\nlocal intrinsic word regularity patterns inherent in a candidate hypothesis but\nalso incorporate the cross-utterance, historical word interaction cues\ndistilled by GCN for promoting performance. Extensive experiments conducted on\nthe AMI benchmark dataset seem to confirm the pragmatic utility of our methods,\nin relation to some current top-of-the-line methods.",
          "link": "http://arxiv.org/abs/2106.06922",
          "publishedOn": "2021-08-04T01:59:20.791Z",
          "wordCount": 731,
          "title": "Cross-utterance Reranking Models with BERT and Graph Convolutional Networks for Conversational Speech Recognition. (arXiv:2106.06922v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1\">Lei Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>",
          "description": "Controlling the style of natural language by disentangling the latent space\nis an important step towards interpretable machine learning. After the latent\nspace is disentangled, the style of a sentence can be transformed by tuning the\nstyle representation without affecting other features of the sentence. Previous\nworks usually use adversarial training to guarantee that disentangled vectors\ndo not affect each other. However, adversarial methods are difficult to train.\nEspecially when there are multiple features (e.g., sentiment, or tense, which\nwe call style types in this paper), each feature requires a separate\ndiscriminator for extracting a disentangled style vector corresponding to that\nfeature. In this paper, we propose a unified distribution-controlling method,\nwhich provides each specific style value (the value of style types, e.g.,\npositive sentiment, or past tense) with a unique representation. This method\ncontributes a solid theoretical basis to avoid adversarial training in\nmulti-type disentanglement. We also propose multiple loss functions to achieve\na style-content disentanglement as well as a disentanglement among multiple\nstyle types. In addition, we observe that if two different style types always\nhave some specific style values that occur together in the dataset, they will\naffect each other when transferring the style values. We call this phenomenon\ntraining bias, and we propose a loss function to alleviate such training bias\nwhile disentangling multiple types. We conduct experiments on two datasets\n(Yelp service reviews and Amazon product reviews) to evaluate the\nstyle-disentangling effect and the unsupervised style transfer performance on\ntwo style types: sentiment and tense. The experimental results show the\neffectiveness of our model.",
          "link": "http://arxiv.org/abs/2012.08883",
          "publishedOn": "2021-08-04T01:59:20.743Z",
          "wordCount": 724,
          "title": "Multi-type Disentanglement without Adversarial Training. (arXiv:2012.08883v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.",
          "link": "http://arxiv.org/abs/2107.12651",
          "publishedOn": "2021-08-04T01:59:20.736Z",
          "wordCount": 633,
          "title": "Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.05906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Lianhui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>",
          "description": "Abductive and counterfactual reasoning, core abilities of everyday human\ncognition, require reasoning about what might have happened at time t, while\nconditioning on multiple contexts from the relative past and future. However,\nsimultaneous incorporation of past and future contexts using generative\nlanguage models (LMs) can be challenging, as they are trained either to\ncondition only on the past context or to perform narrowly scoped\ntext-infilling. In this paper, we propose DeLorean, a new unsupervised decoding\nalgorithm that can flexibly incorporate both the past and future contexts using\nonly off-the-shelf, left-to-right language models and no supervision. The key\nintuition of our algorithm is incorporating the future through\nback-propagation, during which, we only update the internal representation of\nthe output while fixing the model parameters. By alternating between forward\nand backward propagation, DeLorean can decode the output representation that\nreflects both the left and right contexts. We demonstrate that our approach is\ngeneral and applicable to two nonmonotonic reasoning tasks: abductive text\ngeneration and counterfactual story revision, where DeLorean outperforms a\nrange of unsupervised and some supervised methods, based on automatic and human\nevaluation.",
          "link": "http://arxiv.org/abs/2010.05906",
          "publishedOn": "2021-08-04T01:59:20.729Z",
          "wordCount": 695,
          "title": "Back to the Future: Unsupervised Backprop-based Decoding for Counterfactual and Abductive Commonsense Reasoning. (arXiv:2010.05906v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuedong Huang</a>",
          "description": "Commonsense question answering (QA) requires a model to grasp commonsense and\nfactual knowledge to answer questions about world events. Many prior methods\ncouple language modeling with knowledge graphs (KG). However, although a KG\ncontains rich structural information, it lacks the context to provide a more\nprecise understanding of the concepts. This creates a gap when fusing knowledge\ngraphs into language modeling, especially when there is insufficient labeled\ndata. Thus, we propose to employ external entity descriptions to provide\ncontextual information for knowledge understanding. We retrieve descriptions of\nrelated concepts from Wiktionary and feed them as additional input to\npre-trained language models. The resulting model achieves state-of-the-art\nresult in the CommonsenseQA dataset and the best result among non-generative\nmodels in OpenBookQA.",
          "link": "http://arxiv.org/abs/2012.04808",
          "publishedOn": "2021-08-04T01:59:20.708Z",
          "wordCount": 609,
          "title": "Fusing Context Into Knowledge Graph for Commonsense Question Answering. (arXiv:2012.04808v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01624",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1\">Rohan Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazi_B/0/1/0/all/0/1\">Badih Ghazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vineet Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Ravi Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manurangsi_P/0/1/0/all/0/1\">Pasin Manurangsi</a>",
          "description": "In this work, we study the large-scale pretraining of BERT-Large with\ndifferentially private SGD (DP-SGD). We show that combined with a careful\nimplementation, scaling up the batch size to millions (i.e., mega-batches)\nimproves the utility of the DP-SGD step for BERT; we also enhance its\nefficiency by using an increasing batch size schedule. Our implementation\nbuilds on the recent work of [SVK20], who demonstrated that the overhead of a\nDP-SGD step is minimized with effective use of JAX [BFH+18, FJL18] primitives\nin conjunction with the XLA compiler [XLA17]. Our implementation achieves a\nmasked language model accuracy of 60.5% at a batch size of 2M, for $\\epsilon =\n5.36$. To put this number in perspective, non-private BERT models achieve an\naccuracy of $\\sim$70%.",
          "link": "http://arxiv.org/abs/2108.01624",
          "publishedOn": "2021-08-04T01:59:20.701Z",
          "wordCount": 561,
          "title": "Large-Scale Differentially Private BERT. (arXiv:2108.01624v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gajbhiye_A/0/1/0/all/0/1\">Amit Gajbhiye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1\">Noura Al Moubayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradley_S/0/1/0/all/0/1\">Steven Bradley</a>",
          "description": "Neural language representation models such as BERT, pre-trained on\nlarge-scale unstructured corpora lack explicit grounding to real-world\ncommonsense knowledge and are often unable to remember facts required for\nreasoning and inference. Natural Language Inference (NLI) is a challenging\nreasoning task that relies on common human understanding of language and\nreal-world commonsense knowledge. We introduce a new model for NLI called\nExternal Knowledge Enhanced BERT (ExBERT), to enrich the contextual\nrepresentation with real-world commonsense knowledge from external knowledge\nsources and enhance BERT's language understanding and reasoning capabilities.\nExBERT takes full advantage of contextual word representations obtained from\nBERT and employs them to retrieve relevant external knowledge from knowledge\ngraphs and to encode the retrieved external knowledge. Our model adaptively\nincorporates the external knowledge context required for reasoning over the\ninputs. Extensive experiments on the challenging SciTail and SNLI benchmarks\ndemonstrate the effectiveness of ExBERT: in comparison to the previous\nstate-of-the-art, we obtain an accuracy of 95.9% on SciTail and 91.5% on SNLI.",
          "link": "http://arxiv.org/abs/2108.01589",
          "publishedOn": "2021-08-04T01:59:20.689Z",
          "wordCount": 595,
          "title": "ExBERT: An External Knowledge Enhanced BERT for Natural Language Inference. (arXiv:2108.01589v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_K/0/1/0/all/0/1\">Kuang Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Ming Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aoying Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>",
          "description": "We present InferWiki, a Knowledge Graph Completion (KGC) dataset that\nimproves upon existing benchmarks in inferential ability, assumptions, and\npatterns. First, each testing sample is predictable with supportive data in the\ntraining set. To ensure it, we propose to utilize rule-guided train/test\ngeneration, instead of conventional random split. Second, InferWiki initiates\nthe evaluation following the open-world assumption and improves the inferential\ndifficulty of the closed-world assumption, by providing manually annotated\nnegative and unknown triples. Third, we include various inference patterns\n(e.g., reasoning path length and types) for comprehensive evaluation. In\nexperiments, we curate two settings of InferWiki varying in sizes and\nstructures, and apply the construction process on CoDEx as comparative\ndatasets. The results and empirical analyses demonstrate the necessity and\nhigh-quality of InferWiki. Nevertheless, the performance gap among various\ninferential assumptions and patterns presents the difficulty and inspires\nfuture research direction. Our datasets can be found in\nhttps://github.com/TaoMiner/inferwiki",
          "link": "http://arxiv.org/abs/2108.01387",
          "publishedOn": "2021-08-04T01:59:20.682Z",
          "wordCount": 598,
          "title": "Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion. (arXiv:2108.01387v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1\">Xiaoxue Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lijuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Maria Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jindong Chen</a>",
          "description": "We present a new human-human dialogue dataset - PhotoChat, the first dataset\nthat casts light on the photo sharing behavior in onlin emessaging. PhotoChat\ncontains 12k dialogues, each of which is paired with a user photo that is\nshared during the conversation. Based on this dataset, we propose two tasks to\nfacilitate research on image-text modeling: a photo-sharing intent prediction\ntask that predicts whether one intends to share a photo in the next\nconversation turn, and a photo retrieval task that retrieves the most relevant\nphoto according to the dialogue context. In addition, for both tasks, we\nprovide baseline models using the state-of-the-art models and report their\nbenchmark performances. The best image retrieval model achieves 10.4% recall@1\n(out of 1000 candidates) and the best photo intent prediction model achieves\n58.1% F1 score, indicating that the dataset presents interesting yet\nchallenging real-world problems. We are releasing PhotoChat to facilitate\nfuture research work among the community.",
          "link": "http://arxiv.org/abs/2108.01453",
          "publishedOn": "2021-08-04T01:59:20.676Z",
          "wordCount": 606,
          "title": "PhotoChat: A Human-Human Dialogue Dataset with Photo Sharing Behavior for Joint Image-Text Modeling. (arXiv:2108.01453v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Anant Khandelwal</a>",
          "description": "An intelligent dialogue system in a multi-turn setting should not only\ngenerate the responses which are of good quality, but it should also generate\nthe responses which can lead to long-term success of the dialogue. Although,\nthe current approaches improved the response quality, but they over-look the\ntraining signals present in the dialogue data. We can leverage these signals to\ngenerate the weakly supervised training data for learning dialog policy and\nreward estimator, and make the policy take actions (generates responses) which\ncan foresee the future direction for a successful (rewarding) conversation. We\nsimulate the dialogue between an agent and a user (modelled similar to an agent\nwith supervised learning objective) to interact with each other. The agent uses\ndynamic blocking to generate ranked diverse responses and\nexploration-exploitation to select among the Top-K responses. Each simulated\nstate-action pair is evaluated (works as a weak annotation) with three quality\nmodules: Semantic Relevant, Semantic Coherence and Consistent Flow. Empirical\nstudies with two benchmarks indicate that our model can significantly\nout-perform the response quality and lead to a successful conversation on both\nautomatic evaluation and human judgement.",
          "link": "http://arxiv.org/abs/2108.01487",
          "publishedOn": "2021-08-04T01:59:20.611Z",
          "wordCount": 630,
          "title": "$\\textrm{WeaSuL}^{\\pi}$: Weakly Supervised Dialogue Policy Learning: Reward Estimation for Multi-turn Dialogue. (arXiv:2108.01487v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01547",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1\">Pei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuxian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yida Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Henry Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaocong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bosi Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoyan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>",
          "description": "Although pre-trained language models have remarkably enhanced the generation\nability of dialogue systems, open-domain Chinese dialogue systems are still\nlimited by the dialogue data and the model size compared with English ones. In\nthis paper, we propose EVA, a Chinese dialogue system that contains the largest\nChinese pre-trained dialogue model with 2.8B parameters. To build this model,\nwe collect the largest Chinese dialogue dataset named WDC-Dialogue from various\npublic social media. This dataset contains 1.4B context-response pairs and is\nused as the pre-training corpus of EVA. Extensive experiments on automatic and\nhuman evaluation show that EVA outperforms other Chinese pre-trained dialogue\nmodels especially in the multi-turn interaction of human-bot conversations.",
          "link": "http://arxiv.org/abs/2108.01547",
          "publishedOn": "2021-08-04T01:59:20.592Z",
          "wordCount": 576,
          "title": "EVA: An Open-Domain Chinese Dialogue System with Large-Scale Generative Pre-Training. (arXiv:2108.01547v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Souvik Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sougata Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_R/0/1/0/all/0/1\">Rohini K. Srihari</a>",
          "description": "The Covid-19 pandemic has caused a spur in the medical research literature.\nWith new research advances in understanding the virus, there is a need for\nrobust text mining tools which can process, extract and present answers from\nthe literature in a concise and consumable way. With a DialoGPT based\nmulti-turn conversation generation module, and BM-25 \\& neural embeddings based\nensemble information retrieval module, in this paper we present a\nconversational system, which can retrieve and answer coronavirus-related\nqueries from the rich medical literature, and present it in a conversational\nsetting with the user. We further perform experiments to compare neural\nembedding-based document retrieval and the traditional BM25 retrieval algorithm\nand report the results.",
          "link": "http://arxiv.org/abs/2108.01436",
          "publishedOn": "2021-08-04T01:59:20.585Z",
          "wordCount": 599,
          "title": "Medical Literature Mining and Retrieval in a Conversational Setting. (arXiv:2108.01436v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goindani_A/0/1/0/all/0/1\">Akshay Goindani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1\">Manish Shrivastava</a>",
          "description": "Multiple parallel attention mechanisms that use multiple attention heads\nfacilitate greater performance of the Transformer model for various\napplications e.g., Neural Machine Translation (NMT), text classification. In\nmulti-head attention mechanism, different heads attend to different parts of\nthe input. However, the limitation is that multiple heads might attend to the\nsame part of the input, resulting in multiple heads being redundant. Thus, the\nmodel resources are under-utilized. One approach to avoid this is to prune\nleast important heads based on certain importance score. In this work, we focus\non designing a Dynamic Head Importance Computation Mechanism (DHICM) to\ndynamically calculate the importance of a head with respect to the input. Our\ninsight is to design an additional attention layer together with multi-head\nattention, and utilize the outputs of the multi-head attention along with the\ninput, to compute the importance for each head. Additionally, we add an extra\nloss function to prevent the model from assigning same score to all heads, to\nidentify more important heads and improvise performance. We analyzed\nperformance of DHICM for NMT with different languages. Experiments on different\ndatasets show that DHICM outperforms traditional Transformer-based approach by\nlarge margin, especially, when less training data is available.",
          "link": "http://arxiv.org/abs/2108.01377",
          "publishedOn": "2021-08-04T01:59:20.553Z",
          "wordCount": 630,
          "title": "A Dynamic Head Importance Computation Mechanism for Neural Machine Translation. (arXiv:2108.01377v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01280",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mussakhojayeva_S/0/1/0/all/0/1\">Saida Mussakhojayeva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khassanov_Y/0/1/0/all/0/1\">Yerbolat Khassanov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varol_H/0/1/0/all/0/1\">Huseyin Atakan Varol</a>",
          "description": "We study training a single end-to-end (E2E) automatic speech recognition\n(ASR) model for three languages used in Kazakhstan: Kazakh, Russian, and\nEnglish. We first describe the development of multilingual E2E ASR based on\nTransformer networks and then perform an extensive assessment on the\naforementioned languages. We also compare two variants of output grapheme set\nconstruction: combined and independent. Furthermore, we evaluate the impact of\nLMs and data augmentation techniques on the recognition performance of the\nmultilingual E2E ASR. In addition, we present several datasets for training and\nevaluation purposes. Experiment results show that the multilingual models\nachieve comparable performances to the monolingual baselines with a similar\nnumber of parameters. Our best monolingual and multilingual models achieved\n20.9% and 20.5% average word error rates on the combined test set,\nrespectively. To ensure the reproducibility of our experiments and results, we\nshare our training recipes, datasets, and pre-trained models.",
          "link": "http://arxiv.org/abs/2108.01280",
          "publishedOn": "2021-08-04T01:59:20.490Z",
          "wordCount": 606,
          "title": "A Study of Multilingual End-to-End Speech Recognition for Kazakh, Russian, and English. (arXiv:2108.01280v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_D/0/1/0/all/0/1\">Dushyant Singh Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gopendra Vikram Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadeh_A/0/1/0/all/0/1\">Amir Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>",
          "description": "Humor recognition in conversations is a challenging task that has recently\ngained popularity due to its importance in dialogue understanding, including in\nmultimodal settings (i.e., text, acoustics, and visual). The few existing\ndatasets for humor are mostly in English. However, due to the tremendous growth\nin multilingual content, there is a great demand to build models and systems\nthat support multilingual information access. To this end, we propose a dataset\nfor Multimodal Multiparty Hindi Humor (M2H2) recognition in conversations\ncontaining 6,191 utterances from 13 episodes of a very popular TV series\n\"Shrimaan Shrimati Phir Se\". Each utterance is annotated with humor/non-humor\nlabels and encompasses acoustic, visual, and textual modalities. We propose\nseveral strong multimodal baselines and show the importance of contextual and\nmultimodal information for humor recognition in conversations. The empirical\nresults on M2H2 dataset demonstrate that multimodal information complements\nunimodal information for humor recognition. The dataset and the baselines are\navailable at this http URL and\nhttps://github.com/declare-lab/M2H2-dataset.",
          "link": "http://arxiv.org/abs/2108.01260",
          "publishedOn": "2021-08-04T01:59:20.475Z",
          "wordCount": 610,
          "title": "M2H2: A Multimodal Multiparty Hindi Dataset For Humor Recognition in Conversations. (arXiv:2108.01260v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01425",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Talafha_B/0/1/0/all/0/1\">Bashar Talafha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zater_M/0/1/0/all/0/1\">Muhy Eddin Za&#x27;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suleiman_S/0/1/0/all/0/1\">Samer Suleiman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Ayyoub_M/0/1/0/all/0/1\">Mahmoud Al-Ayyoub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Kabi_M/0/1/0/all/0/1\">Mohammed N. Al-Kabi</a>",
          "description": "The role of predicting sarcasm in the text is known as automatic sarcasm\ndetection. Given the prevalence and challenges of sarcasm in sentiment-bearing\ntext, this is a critical phase in most sentiment analysis tasks. With the\nincreasing popularity and usage of different social media platforms among users\naround the world, people are using sarcasm more and more in their day-to-day\nconversations, social media posts and tweets, and it is considered as a way for\npeople to express their sentiment about some certain topics or issues. As a\nresult of the increasing popularity, researchers started to focus their\nresearch endeavors on detecting sarcasm from a text in different languages\nespecially the English language. However, the task of sarcasm detection is a\nchallenging task due to the nature of sarcastic texts; which can be relative\nand significantly differs from one person to another depending on the topic,\nregion, the user's mentality and other factors. In addition to these\nchallenges, sarcasm detection in the Arabic language has its own challenges due\nto the complexity of the Arabic language, such as being morphologically rich,\nwith many dialects that significantly vary between each other, while also being\nlowly resourced. In recent years, only few research attempts started tackling\nthe task of sarcasm detection in Arabic, including creating and collecting\ncorpora, organizing workshops and establishing baseline models. This paper\nintends to create a new humanly annotated Arabic corpus for sarcasm detection\ncollected from tweets, and implementing a new approach for sarcasm detection\nand quantification in Arabic tweets. The annotation technique followed in this\npaper is unique in sarcasm detection and the proposed approach tackles the\nproblem as a regression problem instead of classification; i.e., the model\nattempts to predict the level of sarcasm instead of binary classification.",
          "link": "http://arxiv.org/abs/2108.01425",
          "publishedOn": "2021-08-04T01:59:20.468Z",
          "wordCount": 730,
          "title": "sarcasm detection and quantification in arabic tweets. (arXiv:2108.01425v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinmeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wansen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Long Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1\">Quanjun Yin</a>",
          "description": "Evaluating the quality of a dialogue system is an understudied problem. The\nrecent evolution of evaluation method motivated this survey, in which an\nexplicit and comprehensive analysis of the existing methods is sought. We are\nfirst to divide the evaluation methods into three classes, i.e., automatic\nevaluation, human-involved evaluation and user simulator based evaluation.\nThen, each class is covered with main features and the related evaluation\nmetrics. The existence of benchmarks, suitable for the evaluation of dialogue\ntechniques are also discussed in detail. Finally, some open issues are pointed\nout to bring the evaluation method into a new frontier.",
          "link": "http://arxiv.org/abs/2108.01369",
          "publishedOn": "2021-08-04T01:59:20.461Z",
          "wordCount": 535,
          "title": "How to Evaluate Your Dialogue Models: A Review of Approaches. (arXiv:2108.01369v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Encheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongru Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yongping Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Meiling Hu</a>",
          "description": "Medical Dialogue Generation (MDG) is intended to build a medical dialogue\nsystem for intelligent consultation, which can communicate with patients in\nreal-time, thereby improving the efficiency of clinical diagnosis with broad\napplication prospects. This paper presents our proposed framework for the\nChinese MDG organized by the 2021 China conference on knowledge graph and\nsemantic computing (CCKS) competition, which requires generating\ncontext-consistent and medically meaningful responses conditioned on the\ndialogue history. In our framework, we propose a pipeline system composed of\nentity prediction and entity-aware dialogue generation, by adding predicted\nentities to the dialogue model with a fusion mechanism, thereby utilizing\ninformation from different sources. At the decoding stage, we propose a new\ndecoding mechanism named Entity-revised Diverse Beam Search (EDBS) to improve\nentity correctness and promote the length and quality of the final response.\nThe proposed method wins both the CCKS and the International Conference on\nLearning Representations (ICLR) 2021 Workshop Machine Learning for Preventing\nand Combating Pandemics (MLPCP) Track 1 Entity-aware MED competitions, which\ndemonstrate the practicality and effectiveness of our method.",
          "link": "http://arxiv.org/abs/2108.01266",
          "publishedOn": "2021-08-04T01:59:20.384Z",
          "wordCount": 629,
          "title": "More but Correct: Generating Diversified and Entity-revised Medical Response. (arXiv:2108.01266v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>",
          "description": "Dialogue summarization aims to generate a summary that indicates the key\npoints of a given dialogue. In this work, we propose an end-to-end neural model\nfor dialogue summarization with two novel modules, namely, the \\emph{supporting\nutterance flow modeling module} and the \\emph{fact regularization module}. The\nsupporting utterance flow modeling helps to generate a coherent summary by\nsmoothly shifting the focus from the former utterances to the later ones. The\nfact regularization encourages the generated summary to be factually consistent\nwith the ground-truth summary during model training, which helps to improve the\nfactual correctness of the generated summary in inference time. Furthermore, we\nalso introduce a new benchmark dataset for dialogue summarization. Extensive\nexperiments on both existing and newly-introduced datasets demonstrate the\neffectiveness of our model.",
          "link": "http://arxiv.org/abs/2108.01268",
          "publishedOn": "2021-08-04T01:59:20.329Z",
          "wordCount": 565,
          "title": "Dialogue Summarization with Supporting Utterance Flow Modeling and Fact Regularization. (arXiv:2108.01268v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hoang Long Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renkens_V/0/1/0/all/0/1\">Vincent Renkens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelemans_J/0/1/0/all/0/1\">Joris Pelemans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potharaju_S/0/1/0/all/0/1\">Srividya Pranavi Potharaju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nalamalapu_A/0/1/0/all/0/1\">Anil Kumar Nalamalapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbacak_M/0/1/0/all/0/1\">Murat Akbacak</a>",
          "description": "Recognition errors are common in human communication. Similar errors often\nlead to unwanted behaviour in dialogue systems or virtual assistants. In human\ncommunication, we can recover from them by repeating misrecognized words or\nphrases; however in human-machine communication this recovery mechanism is not\navailable. In this paper, we attempt to bridge this gap and present a system\nthat allows a user to correct speech recognition errors in a virtual assistant\nby repeating misunderstood words. When a user repeats part of the phrase the\nsystem rewrites the original query to incorporate the correction. This rewrite\nallows the virtual assistant to understand the original query successfully. We\npresent an end-to-end 2-step attention pointer network that can generate the\nthe rewritten query by merging together the incorrectly understood utterance\nwith the correction follow-up. We evaluate the model on data collected for this\ntask and compare the proposed model to a rule-based baseline and a standard\npointer network. We show that rewriting the original query is an effective way\nto handle repetition-based recovery and that the proposed model outperforms the\nrule based baseline, reducing Word Error Rate by 19% relative at 2% False Alarm\nRate on annotated data.",
          "link": "http://arxiv.org/abs/2108.01208",
          "publishedOn": "2021-08-04T01:59:20.288Z",
          "wordCount": 649,
          "title": "User-Initiated Repetition-Based Recovery in Multi-Utterance Dialogue Systems. (arXiv:2108.01208v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bradley He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radfar_M/0/1/0/all/0/1\">Martin Radfar</a>",
          "description": "In order to evaluate the performance of the attention based neural ASR under\nnoisy conditions, the current trend is to present hours of various noisy speech\ndata to the model and measure the overall word/phoneme error rate (W/PER). In\ngeneral, it is unclear how these models perform when exposed to a cocktail\nparty setup in which two or more speakers are active. In this paper, we present\nthe mixtures of speech signals to a popular attention-based neural ASR, known\nas Listen, Attend, and Spell (LAS), at different target-to-interference ratio\n(TIR) and measure the phoneme error rate. In particular, we investigate in\ndetails when two phonemes are mixed what will be the predicted phoneme; in this\nfashion we build a model in which the most probable predictions for a phoneme\nare given. We found a 65% relative increase in PER when LAS was presented with\nmixed speech signals at TIR = 0 dB and the performance approaches the unmixed\nscenario at TIR = 30 dB. Our results show the model, when presented with mixed\nphonemes signals, tend to predict those that have higher accuracies during\nevaluation of original phoneme signals.",
          "link": "http://arxiv.org/abs/2108.01245",
          "publishedOn": "2021-08-04T01:59:20.235Z",
          "wordCount": 635,
          "title": "The Performance Evaluation of Attention-Based Neural ASR under Mixed Speech Input. (arXiv:2108.01245v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_deMiguel_C/0/1/0/all/0/1\">Claudia Mart&#xed;nez-deMiguel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segura_Bedmar_I/0/1/0/all/0/1\">Isabel Segura-Bedmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chacon_Solano_E/0/1/0/all/0/1\">Esteban Chac&#xf3;n-Solano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_Aspizua_S/0/1/0/all/0/1\">Sara Guerrero-Aspizua</a>",
          "description": "The RareDis corpus contains more than 5,000 rare diseases and almost 6,000\nclinical manifestations are annotated. Moreover, the Inter Annotator Agreement\nevaluation shows a relatively high agreement (F1-measure equal to 83.5% under\nexact match criteria for the entities and equal to 81.3% for the relations).\nBased on these results, this corpus is of high quality, supposing a significant\nstep for the field since there is a scarcity of available corpus annotated with\nrare diseases. This could open the door to further NLP applications, which\nwould facilitate the diagnosis and treatment of these rare diseases and,\ntherefore, would improve dramatically the quality of life of these patients.",
          "link": "http://arxiv.org/abs/2108.01204",
          "publishedOn": "2021-08-04T01:59:20.176Z",
          "wordCount": 551,
          "title": "The RareDis corpus: a corpus annotated with rare diseases, their signs and symptoms. (arXiv:2108.01204v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1\">Ioana Baldini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dennis Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamurthy_K/0/1/0/all/0/1\">Karthikeyan Natesan Ramamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yurochkin_M/0/1/0/all/0/1\">Mikhail Yurochkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Moninder Singh</a>",
          "description": "We study the performance-fairness trade-off in more than a dozen fine-tuned\nLMs for toxic text classification. We empirically show that no blanket\nstatement can be made with respect to the bias of large versus regular versus\ncompressed models. Moreover, we find that focusing on fairness-agnostic\nperformance metrics can lead to models with varied fairness characteristics.",
          "link": "http://arxiv.org/abs/2108.01250",
          "publishedOn": "2021-08-04T01:59:20.162Z",
          "wordCount": 506,
          "title": "Your fairness may vary: Group fairness of pretrained language models in toxic text classification. (arXiv:2108.01250v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01141",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abandah_G/0/1/0/all/0/1\">Gheith A. Abandah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suyyagh_A/0/1/0/all/0/1\">Ashraf Suyyagh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khedher_M/0/1/0/all/0/1\">Mohammed Z. Khedher</a>",
          "description": "Soft spelling errors are a class of spelling mistakes that is widespread\namong native Arabic speakers and foreign learners alike. Some of these errors\nare typographical in nature. They occur due to orthographic variations of some\nArabic letters and the complex rules that dictate their correct usage. Many\npeople forgo these rules, and given the identical phonetic sounds, they often\nconfuse such letters. In this paper, we propose a bidirectional long short-term\nmemory network that corrects this class of errors. We develop, train, evaluate,\nand compare a set of BiLSTM networks. We approach the spelling correction\nproblem at the character level. We handle Arabic texts from both classical and\nmodern standard Arabic. We treat the problem as a one-to-one sequence\ntranscription problem. Since the soft Arabic errors class encompasses omission\nand addition mistakes, to preserve the one-to-one sequence transcription, we\npropose a simple low-resource yet effective technique that maintains the\none-to-one sequencing and avoids using a costly encoder-decoder architecture.\nWe train the BiLSTM models to correct the spelling mistakes using transformed\ninput and stochastic error injection approaches. We recommend a configuration\nthat has two BiLSTM layers, uses the dropout regularization, and is trained\nusing the latter training approach with error injection rate of 40%. The best\nmodel corrects 96.4% of the injected errors and achieves a low character error\nrate of 1.28% on a real test set of soft spelling mistakes.",
          "link": "http://arxiv.org/abs/2108.01141",
          "publishedOn": "2021-08-04T01:59:20.151Z",
          "wordCount": 698,
          "title": "Correcting Arabic Soft Spelling Mistakes using BiLSTM-based Machine Learning. (arXiv:2108.01141v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miltenburg_E/0/1/0/all/0/1\">Emiel van Miltenburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinciu_M/0/1/0/all/0/1\">Miruna-Adriana Clinciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkatzia_D/0/1/0/all/0/1\">Dimitra Gkatzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inglis_S/0/1/0/all/0/1\">Stephanie Inglis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leppanen_L/0/1/0/all/0/1\">Leo Lepp&#xe4;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahamood_S/0/1/0/all/0/1\">Saad Mahamood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_E/0/1/0/all/0/1\">Emma Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoch_S/0/1/0/all/0/1\">Stephanie Schoch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomson_C/0/1/0/all/0/1\">Craig Thomson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Luou Wen</a>",
          "description": "We observe a severe under-reporting of the different kinds of errors that\nNatural Language Generation systems make. This is a problem, because mistakes\nare an important indicator of where systems should still be improved. If\nauthors only report overall performance metrics, the research community is left\nin the dark about the specific weaknesses that are exhibited by\n`state-of-the-art' research. Next to quantifying the extent of error\nunder-reporting, this position paper provides recommendations for error\nidentification, analysis and reporting.",
          "link": "http://arxiv.org/abs/2108.01182",
          "publishedOn": "2021-08-04T01:59:20.125Z",
          "wordCount": 556,
          "title": "Underreporting of errors in NLG output, and what to do about it. (arXiv:2108.01182v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1\">Andrei-Marius Avram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1\">Vasile Pais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1\">Dan Tufis</a>",
          "description": "EuroVoc is a multilingual thesaurus that was built for organizing the\nlegislative documentary of the European Union institutions. It contains\nthousands of categories at different levels of specificity and its descriptors\nare targeted by legal texts in almost thirty languages. In this work we propose\na unified framework for EuroVoc classification on 22 languages by fine-tuning\nmodern Transformer-based pretrained language models. We study extensively the\nperformance of our trained models and show that they significantly improve the\nresults obtained by a similar tool - JEX - on the same dataset. The code and\nthe fine-tuned models were open sourced, together with a programmatic interface\nthat eases the process of loading the weights of a trained model and of\nclassifying a new document.",
          "link": "http://arxiv.org/abs/2108.01139",
          "publishedOn": "2021-08-04T01:59:20.114Z",
          "wordCount": 568,
          "title": "PyEuroVoc: A Tool for Multilingual Legal Document Classification with EuroVoc Descriptors. (arXiv:2108.01139v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1\">Manas Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faldu_K/0/1/0/all/0/1\">Keyur Faldu</a>",
          "description": "AI systems have seen significant adoption in various domains. At the same\ntime, further adoption in some domains is hindered by inability to fully trust\nan AI system that it will not harm a human. Besides the concerns for fairness,\nprivacy, transparency, and explainability are key to developing trusts in AI\nsystems. As stated in describing trustworthy AI \"Trust comes through\nunderstanding. How AI-led decisions are made and what determining factors were\nincluded are crucial to understand.\" The subarea of explaining AI systems has\ncome to be known as XAI. Multiple aspects of an AI system can be explained;\nthese include biases that the data might have, lack of data points in a\nparticular region of the example space, fairness of gathering the data, feature\nimportances, etc. However, besides these, it is critical to have human-centered\nexplanations that are directly related to decision-making similar to how a\ndomain expert makes decisions based on \"domain knowledge,\" that also include\nwell-established, peer-validated explicit guidelines. To understand and\nvalidate an AI system's outcomes (such as classification, recommendations,\npredictions), that lead to developing trust in the AI system, it is necessary\nto involve explicit domain knowledge that humans understand and use.",
          "link": "http://arxiv.org/abs/2108.01174",
          "publishedOn": "2021-08-04T01:59:20.098Z",
          "wordCount": 637,
          "title": "Knowledge-intensive Language Understanding for Explainable AI. (arXiv:2108.01174v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiahong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xingyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Renjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Church_K/0/1/0/all/0/1\">Kenneth Church</a>",
          "description": "We propose a method for emotion recognition through emotiondependent speech\nrecognition using Wav2vec 2.0. Our method achieved a significant improvement\nover most previously reported results on IEMOCAP, a benchmark emotion dataset.\nDifferent types of phonetic units are employed and compared in terms of\naccuracy and robustness of emotion recognition within and across datasets and\nlanguages. Models of phonemes, broad phonetic classes, and syllables all\nsignificantly outperform the utterance model, demonstrating that phonetic units\nare helpful and should be incorporated in speech emotion recognition. The best\nperformance is from using broad phonetic classes. Further research is needed to\ninvestigate the optimal set of broad phonetic classes for the task of emotion\nrecognition. Finally, we found that Wav2vec 2.0 can be fine-tuned to recognize\ncoarser-grained or larger phonetic units than phonemes, such as broad phonetic\nclasses and syllables.",
          "link": "http://arxiv.org/abs/2108.01132",
          "publishedOn": "2021-08-04T01:59:20.061Z",
          "wordCount": 571,
          "title": "The Role of Phonetic Units in Speech Emotion Recognition. (arXiv:2108.01132v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiahong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xingyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dongji Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Renjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Church_K/0/1/0/all/0/1\">Kenneth Church</a>",
          "description": "Much of the recent literature on automatic speech recognition (ASR) is taking\nan end-to-end approach. Unlike English where the writing system is closely\nrelated to sound, Chinese characters (Hanzi) represent meaning, not sound. We\npropose factoring audio -> Hanzi into two sub-tasks: (1) audio -> Pinyin and\n(2) Pinyin -> Hanzi, where Pinyin is a system of phonetic transcription of\nstandard Chinese. Factoring the audio -> Hanzi task in this way achieves 3.9%\nCER (character error rate) on the Aishell-1 corpus, the best result reported on\nthis dataset so far.",
          "link": "http://arxiv.org/abs/2108.01129",
          "publishedOn": "2021-08-04T01:59:20.011Z",
          "wordCount": 538,
          "title": "Decoupling recognition and transcription in Mandarin ASR. (arXiv:2108.01129v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiahong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryant_N/0/1/0/all/0/1\">Neville Ryant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xingyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Church_K/0/1/0/all/0/1\">Kenneth Church</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liberman_M/0/1/0/all/0/1\">Mark Liberman</a>",
          "description": "This study reports our efforts to improve automatic recognition of\nsuprasegmentals by fine-tuning wav2vec 2.0 with CTC, a method that has been\nsuccessful in automatic speech recognition. We demonstrate that the method can\nimprove the state-of-the-art on automatic recognition of syllables, tones, and\npitch accents. Utilizing segmental information, by employing tonal finals or\ntonal syllables as recognition units, can significantly improve Mandarin tone\nrecognition. Language models are helpful when tonal syllables are used as\nrecognition units, but not helpful when tones are recognition units. Finally,\nMandarin tone recognition can benefit from English phoneme recognition by\ncombing the two tasks in fine-tuning wav2vec 2.0.",
          "link": "http://arxiv.org/abs/2108.01122",
          "publishedOn": "2021-08-04T01:59:20.000Z",
          "wordCount": 537,
          "title": "Automatic recognition of suprasegmentals in speech. (arXiv:2108.01122v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.",
          "link": "http://arxiv.org/abs/2107.08264",
          "publishedOn": "2021-08-03T02:06:30.405Z",
          "wordCount": 730,
          "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1\">Brandon Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1\">Sophie Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>",
          "description": "Citation context analysis (CCA) is an important task in natural language\nprocessing that studies how and why scholars discuss each others' work. Despite\ndecades of study, traditional frameworks for CCA have largely relied on\noverly-simplistic assumptions of how authors cite, which ignore several\nimportant phenomena. For instance, scholarly papers often contain rich\ndiscussions of cited work that span multiple sentences and express multiple\nintents concurrently. Yet, CCA is typically approached as a single-sentence,\nsingle-label classification task, and thus existing datasets fail to capture\nthis interesting discourse. In our work, we address this research gap by\nproposing a novel framework for CCA as a document-level context extraction and\nlabeling task. We release MultiCite, a new dataset of 12,653 citation contexts\nfrom over 1,200 computational linguistics papers. Not only is it the largest\ncollection of expert-annotated citation contexts to-date, MultiCite contains\nmulti-sentence, multi-label citation contexts within full paper texts. Finally,\nwe demonstrate how our dataset, while still usable for training classic CCA\nmodels, also supports the development of new types of models for CCA beyond\nfixed-width text classification. We release our code and dataset at\nhttps://github.com/allenai/multicite.",
          "link": "http://arxiv.org/abs/2107.00414",
          "publishedOn": "2021-08-03T02:06:30.299Z",
          "wordCount": 658,
          "title": "MultiCite: Modeling realistic citations requires moving beyond the single-sentence single-label setting. (arXiv:2107.00414v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1\">Devamanyu Hazarika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>",
          "description": "The majority of existing methods for empathetic response generation rely on\nthe emotion of the context to generate empathetic responses. However, empathy\nis much more than generating responses with an appropriate emotion. It also\noften entails subtle expressions of understanding and personal resonance with\nthe situation of the other interlocutor. Unfortunately, such qualities are\ndifficult to quantify and the datasets lack the relevant annotations. To\naddress this issue, in this paper we propose an approach that relies on\nexemplars to cue the generative model on fine stylistic properties that signal\nempathy to the interlocutor. To this end, we employ dense passage retrieval to\nextract relevant exemplary responses from the training set. Three elements of\nhuman communication -- emotional presence, interpretation, and exploration, and\nsentiment are additionally introduced using synthetic labels to guide the\ngeneration towards empathy. The human evaluation is also extended by these\nelements of human communication. We empirically show that these approaches\nyield significant improvements in empathetic response quality in terms of both\nautomated and human-evaluated metrics. The implementation is available at\nhttps://github.com/declare-lab/exemplary-empathy.",
          "link": "http://arxiv.org/abs/2106.11791",
          "publishedOn": "2021-08-03T02:06:30.292Z",
          "wordCount": 654,
          "title": "Exemplars-guided Empathetic Response Generation Controlled by the Elements of Human Communication. (arXiv:2106.11791v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_R/0/1/0/all/0/1\">Rachit Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiwelekar_A/0/1/0/all/0/1\">Arvind W Kiwelekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netak_L/0/1/0/all/0/1\">Laxman D Netak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodake_A/0/1/0/all/0/1\">Akshay Ghodake</a>",
          "description": "Although most logistics and freight forwarding organizations, in one way or\nanother, claim to have core values. The engagement of employees is a vast\nstructure that affects almost every part of the company's core environmental\nvalues. There is little theoretical knowledge about the relationship between\nfirms and the engagement of employees. Based on research literature, this paper\naims to provide a novel approach for insight around employee engagement in a\nlogistics organization by implementing deep natural language processing\nconcepts. The artificial intelligence-enabled solution named Intelligent Pulse\n(I-Pulse) can evaluate hundreds and thousands of pulse survey comments and\nprovides the actionable insights and gist of employee feedback. I-Pulse allows\nthe stakeholders to think in new ways in their organization, helping them to\nhave a powerful influence on employee engagement, retention, and efficiency.\nThis study is of corresponding interest to researchers and practitioners.",
          "link": "http://arxiv.org/abs/2106.07341",
          "publishedOn": "2021-08-03T02:06:30.172Z",
          "wordCount": 618,
          "title": "i-Pulse: A NLP based novel approach for employee engagement in logistics organization. (arXiv:2106.07341v1 [cs.SI] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Palliser_Sans_R/0/1/0/all/0/1\">Rafel Palliser-Sans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rial_Farras_A/0/1/0/all/0/1\">Albert Rial-Farr&#xe0;s</a>",
          "description": "This paper presents our submission to SemEval-2021 Task 5: Toxic Spans\nDetection. The purpose of this task is to detect the spans that make a text\ntoxic, which is a complex labour for several reasons. Firstly, because of the\nintrinsic subjectivity of toxicity, and secondly, due to toxicity not always\ncoming from single words like insults or offends, but sometimes from whole\nexpressions formed by words that may not be toxic individually. Following this\nidea of focusing on both single words and multi-word expressions, we study the\nimpact of using a multi-depth DistilBERT model, which uses embeddings from\ndifferent layers to estimate the final per-token toxicity. Our quantitative\nresults show that using information from multiple depths boosts the performance\nof the model. Finally, we also analyze our best model qualitatively.",
          "link": "http://arxiv.org/abs/2104.00639",
          "publishedOn": "2021-08-03T02:06:30.151Z",
          "wordCount": 617,
          "title": "HLE-UPC at SemEval-2021 Task 5: Multi-Depth DistilBERT for Toxic Spans Detection. (arXiv:2104.00639v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parry_H/0/1/0/all/0/1\">Hishan Parry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xun_L/0/1/0/all/0/1\">Lei Xun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_A/0/1/0/all/0/1\">Amin Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jia Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrett_G/0/1/0/all/0/1\">Geoff V. Merrett</a>",
          "description": "The Transformer architecture is widely used for machine translation tasks.\nHowever, its resource-intensive nature makes it challenging to implement on\nconstrained embedded devices, particularly where available hardware resources\ncan vary at run-time. We propose a dynamic machine translation model that\nscales the Transformer architecture based on the available resources at any\nparticular time. The proposed approach, 'Dynamic-HAT', uses a HAT\nSuperTransformer as the backbone to search for SubTransformers with different\naccuracy-latency trade-offs at design time. The optimal SubTransformers are\nsampled from the SuperTransformer at run-time, depending on latency\nconstraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses\ninherited SubTransformers sampled directly from the SuperTransformer with a\nswitching time of <1s. Using inherited SubTransformers results in a BLEU score\nloss of <1.5% because the SubTransformer configuration is not retrained from\nscratch after sampling. However, to recover this loss in performance, the\ndimensions of the design space can be reduced to tailor it to a family of\ntarget hardware. The new reduced design space results in a BLEU score increase\nof approximately 1% for sub-optimal models from the original design space, with\na wide range for performance scaling between 0.356s - 1.526s for the GPU and\n2.9s - 7.31s for the CPU.",
          "link": "http://arxiv.org/abs/2107.08199",
          "publishedOn": "2021-08-03T02:06:30.134Z",
          "wordCount": 680,
          "title": "Dynamic Transformer for Efficient Machine Translation on Embedded Devices. (arXiv:2107.08199v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kaiyu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayazit_D/0/1/0/all/0/1\">Deniz Bayazit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_R/0/1/0/all/0/1\">Rebecca Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1\">Stefanie Tellex</a>",
          "description": "Humans use spatial language to naturally describe object locations and their\nrelations. Interpreting spatial language not only adds a perceptual modality\nfor robots, but also reduces the barrier of interfacing with humans. Previous\nwork primarily considers spatial language as goal specification for instruction\nfollowing tasks in fully observable domains, often paired with reference paths\nfor reward-based learning. However, spatial language is inherently subjective\nand potentially ambiguous or misleading. Hence, in this paper, we consider\nspatial language as a form of stochastic observation. We propose SLOOP (Spatial\nLanguage Object-Oriented POMDP), a new framework for partially observable\ndecision making with a probabilistic observation model for spatial language. We\napply SLOOP to object search in city-scale environments. To interpret\nambiguous, context-dependent prepositions (e.g. front), we design a simple\nconvolutional neural network that predicts the language provider's latent frame\nof reference (FoR) given the environment context. Search strategies are\ncomputed via an online POMDP planner based on Monte Carlo Tree Search.\nEvaluation based on crowdsourced language data, collected over areas of five\ncities in OpenStreetMap, shows that our approach achieves faster search and\nhigher success rate compared to baselines, with a wider margin as the spatial\nlanguage becomes more complex. Finally, we demonstrate the proposed method in\nAirSim, a realistic simulator where a drone is tasked to find cars in a\nneighborhood environment.",
          "link": "http://arxiv.org/abs/2012.02705",
          "publishedOn": "2021-08-03T02:06:30.126Z",
          "wordCount": 717,
          "title": "Spatial Language Understanding for Object Search in Partially Observed City-scale Environments. (arXiv:2012.02705v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balali_A/0/1/0/all/0/1\">Ali Balali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadpour_M/0/1/0/all/0/1\">Masoud Asadpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_S/0/1/0/all/0/1\">Seyed Hossein Jafari</a>",
          "description": "Data is published on the web over time in great volumes, but majority of the\ndata is unstructured, making it hard to understand and difficult to interpret.\nInformation Extraction (IE) methods extract structured information from\nunstructured data. One of the challenging IE tasks is Event Extraction (EE)\nwhich seeks to derive information about specific incidents and their actors\nfrom the text. EE is useful in many domains such as building a knowledge base,\ninformation retrieval, summarization and online monitoring systems. In the past\ndecades, some event ontologies like ACE, CAMEO and ICEWS were developed to\ndefine event forms, actors and dimensions of events observed in the text. These\nevent ontologies still have some shortcomings such as covering only a few\ntopics like political events, having inflexible structure in defining argument\nroles, lack of analytical dimensions, and complexity in choosing event\nsub-types. To address these concerns, we propose an event ontology, namely\nCOfEE, that incorporates both expert domain knowledge, previous ontologies and\na data-driven approach for identifying events from text. COfEE consists of two\nhierarchy levels (event types and event sub-types) that include new categories\nrelating to environmental issues, cyberspace, criminal activity and natural\ndisasters which need to be monitored instantly. Also, dynamic roles according\nto each event sub-type are defined to capture various dimensions of events. In\na follow-up experiment, the proposed ontology is evaluated on Wikipedia events,\nand it is shown to be general and comprehensive. Moreover, in order to\nfacilitate the preparation of gold-standard data for event extraction, a\nlanguage-independent online tool is presented based on COfEE.",
          "link": "http://arxiv.org/abs/2107.10326",
          "publishedOn": "2021-08-03T02:06:30.107Z",
          "wordCount": 733,
          "title": "COfEE: A Comprehensive Ontology for Event Extraction from text, with an online annotation tool. (arXiv:2107.10326v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>",
          "description": "Weak supervision has shown promising results in many natural language\nprocessing tasks, such as Named Entity Recognition (NER). Existing work mainly\nfocuses on learning deep NER models only with weak supervision, i.e., without\nany human annotation, and shows that by merely using weakly labeled data, one\ncan achieve good performance, though still underperforms fully supervised NER\nwith manually/strongly labeled data. In this paper, we consider a more\npractical scenario, where we have both a small amount of strongly labeled data\nand a large amount of weakly labeled data. Unfortunately, we observe that\nweakly labeled data does not necessarily improve, or even deteriorate the model\nperformance (due to the extensive noise in the weak labels) when we train deep\nNER models over a simple or weighted combination of the strongly labeled and\nweakly labeled data. To address this issue, we propose a new multi-stage\ncomputational framework -- NEEDLE with three essential ingredients: (1) weak\nlabel completion, (2) noise-aware loss function, and (3) final fine-tuning over\nthe strongly labeled data. Through experiments on E-commerce query NER and\nBiomedical NER, we demonstrate that NEEDLE can effectively suppress the noise\nof the weak labels and outperforms existing methods. In particular, we achieve\nnew SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74,\nBC5CDR-disease 90.69, NCBI-disease 92.28.",
          "link": "http://arxiv.org/abs/2106.08977",
          "publishedOn": "2021-08-03T02:06:30.097Z",
          "wordCount": 705,
          "title": "Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data. (arXiv:2106.08977v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1\">Henning Wachsmuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>",
          "description": "Despite extensive research efforts in the recent years, computational\nmodeling of argumentation remains one of the most challenging areas of natural\nlanguage processing (NLP). This is primarily due to inherent complexity of the\ncognitive processes behind human argumentation, which commonly combine and\nintegrate plethora of different types of knowledge, requiring from\ncomputational models capabilities that are far beyond what is needed for most\nother (i.e., simpler) natural language understanding tasks. The existing large\nbody of work on mining, assessing, generating, and reasoning over arguments\nlargely acknowledges that much more common sense and world knowledge needs to\nbe integrated into computational models that would accurately model\nargumentation. A systematic overview and organization of the types of knowledge\nintroduced in existing models of computational argumentation (CA) is, however,\nmissing and this hinders targeted progress in the field. In this survey paper,\nwe fill this gap by (1) proposing a pyramid of types of knowledge required in\nCA tasks, (2) analysing the state of the art with respect to the reliance and\nexploitation of these types of knowledge, for each of the for main research\nareas in CA, and (3) outlining and discussing directions for future research\nefforts in CA.",
          "link": "http://arxiv.org/abs/2107.00281",
          "publishedOn": "2021-08-03T02:06:30.076Z",
          "wordCount": 665,
          "title": "Scientia Potentia Est -- On the Role of Knowledge in Computational Argumentation. (arXiv:2107.00281v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Weidong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mingjun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lusheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jinwen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jianbo Tang</a>",
          "description": "Language model pre-training based on large corpora has achieved tremendous\nsuccess in terms of constructing enriched contextual representations and has\nled to significant performance gains on a diverse range of Natural Language\nUnderstanding (NLU) tasks. Despite the success, most current pre-trained\nlanguage models, such as BERT, are trained based on single-grained\ntokenization, usually with fine-grained characters or sub-words, making it hard\nfor them to learn the precise meaning of coarse-grained words and phrases. In\nthis paper, we propose a simple yet effective pre-training method named LICHEE\nto efficiently incorporate multi-grained information of input text. Our method\ncan be applied to various pre-trained language models and improve their\nrepresentation capability. Extensive experiments conducted on CLUE and\nSuperGLUE demonstrate that our method achieves comprehensive improvements on a\nwide variety of NLU tasks in both Chinese and English with little extra\ninference cost incurred, and that our best ensemble model achieves the\nstate-of-the-art performance on CLUE benchmark competition.",
          "link": "http://arxiv.org/abs/2108.00801",
          "publishedOn": "2021-08-03T02:06:30.069Z",
          "wordCount": 604,
          "title": "LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization. (arXiv:2108.00801v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>",
          "description": "Pre-trained language models (PrLM) has been shown powerful in enhancing a\nbroad range of downstream tasks including various dialogue related ones.\nHowever, PrLMs are usually trained on general plain text with common language\nmodel (LM) training objectives, which cannot sufficiently capture dialogue\nexclusive features due to the limitation of such training setting, so that\nthere is an immediate need to fill the gap between a specific dialogue task and\nthe LM task. As it is unlikely to collect huge dialogue data for\ndialogue-oriented pre-training, in this paper, we propose three strategies to\nsimulate the conversation features on general plain text. Our proposed method\ndiffers from existing post-training methods that it may yield a general-purpose\nPrLM and does not individualize to any detailed task while keeping the\ncapability of learning dialogue related features including speaker awareness,\ncontinuity and consistency. The resulted Dialog-PrLM is fine-tuned on three\npublic multi-turn dialogue datasets and helps achieve significant and\nconsistent improvement over the plain PrLMs.",
          "link": "http://arxiv.org/abs/2106.00420",
          "publishedOn": "2021-08-03T02:06:30.051Z",
          "wordCount": 603,
          "title": "Dialogue-oriented Pre-training. (arXiv:2106.00420v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tianming Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1\">Gaurav Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Maozu Guo</a>",
          "description": "Label noise and long-tailed distributions are two major challenges in\ndistantly supervised relation extraction. Recent studies have shown great\nprogress on denoising, but pay little attention to the problem of long-tailed\nrelations. In this paper, we introduce constraint graphs to model the\ndependencies between relation labels. On top of that, we further propose a\nnovel constraint graph-based relation extraction framework(CGRE) to handle the\ntwo challenges simultaneously. CGRE employs graph convolution networks (GCNs)\nto propagate information from data-rich relation nodes to data-poor relation\nnodes, and thus boosts the representation learning of long-tailed relations. To\nfurther improve the noise immunity, a constraint-aware attention module is\ndesigned in CGRE to integrate the constraint information. Experimental results\non a widely-used benchmark dataset indicate that our approach achieves\nsignificant improvements over the previous methods for both denoising and\nlong-tailed relation extraction. Our dataset and codes are available at\nhttps://github.com/tmliang/CGRE.",
          "link": "http://arxiv.org/abs/2105.11225",
          "publishedOn": "2021-08-03T02:06:30.021Z",
          "wordCount": 615,
          "title": "Distantly-Supervised Long-Tailed Relation Extraction Using Constraint Graphs. (arXiv:2105.11225v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Amish Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1\">Sourav Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datar_A/0/1/0/all/0/1\">Arnhav Datar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadiwala_J/0/1/0/all/0/1\">Juned Kadiwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1\">Hrithwik Shalu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1\">Jimson Mathew</a>",
          "description": "Reliable detection of the prodromal stages of Alzheimer's disease (AD)\nremains difficult even today because, unlike other neurocognitive impairments,\nthere is no definitive diagnosis of AD in vivo. In this context, existing\nresearch has shown that patients often develop language impairment even in mild\nAD conditions. We propose a multimodal deep learning method that utilizes\nspeech and the corresponding transcript simultaneously to detect AD. For audio\nsignals, the proposed audio-based network, a convolutional neural network (CNN)\nbased model, predicts the diagnosis for multiple speech segments, which are\ncombined for the final prediction. Similarly, we use contextual embedding\nextracted from BERT concatenated with a CNN-generated embedding for classifying\nthe transcript. The individual predictions of the two models are then combined\nto make the final classification. We also perform experiments to analyze the\nmodel performance when Automated Speech Recognition (ASR) system generated\ntranscripts are used instead of manual transcription in the text-based model.\nThe proposed method achieves 85.3% 10-fold cross-validation accuracy when\ntrained and evaluated on the Dementiabank Pitt corpus.",
          "link": "http://arxiv.org/abs/2012.00096",
          "publishedOn": "2021-08-03T02:06:30.014Z",
          "wordCount": 651,
          "title": "Multi-Modal Detection of Alzheimer's Disease from Speech and Text. (arXiv:2012.00096v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vassilev_A/0/1/0/all/0/1\">Apostol Vassilev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Munawar Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Honglan Jin</a>",
          "description": "When people try to understand nuanced language they typically process\nmultiple input sensor modalities to complete this cognitive task. It turns out\nthe human brain has even a specialized neuron formation, called sagittal\nstratum, to help us understand sarcasm. We use this biological formation as the\ninspiration for designing a neural network architecture that combines\npredictions of different models on the same text to construct robust, accurate\nand computationally efficient classifiers for sentiment analysis and study\nseveral different realizations. Among them, we propose a systematic new\napproach to combining multiple predictions based on a dedicated neural network\nand develop mathematical analysis of it along with state-of-the-art\nexperimental results. We also propose a heuristic-hybrid technique for\ncombining models and back it up with experimental results on a representative\nbenchmark dataset and comparisons to other methods to show the advantages of\nthe new approaches.",
          "link": "http://arxiv.org/abs/2006.12958",
          "publishedOn": "2021-08-03T02:06:30.007Z",
          "wordCount": 640,
          "title": "Can you tell? SSNet -- a Sagittal Stratum-inspired Neural Network Framework for Sentiment Analysis. (arXiv:2006.12958v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>",
          "description": "Natural language to SQL (NL2SQL) aims to parse a natural language with a\ngiven database into a SQL query, which widely appears in practical Internet\napplications. Jointly encode database schema and question utterance is a\ndifficult but important task in NL2SQL. One solution is to treat the input as a\nheterogeneous graph. However, it failed to learn good word representation in\nquestion utterance. Learning better word representation is important for\nconstructing a well-designed NL2SQL system. To solve the challenging task, we\npresent a Relation aware Semi-autogressive Semantic Parsing (\\MODN) ~framework,\nwhich is more adaptable for NL2SQL. It first learns relation embedding over the\nschema entities and question words with predefined schema relations with\nELECTRA and relation aware transformer layer as backbone. Then we decode the\nquery SQL with a semi-autoregressive parser and predefined SQL syntax. From\nempirical results and case study, our model shows its effectiveness in learning\nbetter word representation in NL2SQL.",
          "link": "http://arxiv.org/abs/2108.00804",
          "publishedOn": "2021-08-03T02:06:29.997Z",
          "wordCount": 588,
          "title": "Relation Aware Semi-autoregressive Semantic Parsing for NL2SQL. (arXiv:2108.00804v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stergiadis_E/0/1/0/all/0/1\">Emmanouil Stergiadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Satendra Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovalev_F/0/1/0/all/0/1\">Fedor Kovalev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levin_P/0/1/0/all/0/1\">Pavel Levin</a>",
          "description": "While NMT has achieved remarkable results in the last 5 years, production\nsystems come with strict quality requirements in arbitrarily niche domains that\nare not always adequately covered by readily available parallel corpora. This\nis typically addressed by training domain specific models, using fine-tuning\nmethods and some variation of back-translation on top of in-domain monolingual\ncorpora. However, industrial practitioners can rarely afford to focus on a\nsingle domain. A far more typical scenario includes a set of closely related,\nyet succinctly different sub-domains. At Booking.com, we need to translate\nproperty descriptions, user reviews, as well as messages, (for example those\nsent between a customer and an agent or property manager). An editor might need\nto translate articles across a set of different topics. An e-commerce platform\nwould typically need to translate both the description of each item and the\nuser generated content related to them. To this end, we propose MDT: a novel\nmethod to simultaneously fine-tune on several sub-domains by passing\nmultidimensional sentence-level information to the model during training and\ninference. We show that MDT achieves results competitive to N specialist models\neach fine-tuned on a single constituent domain, while effectively serving all N\nsub-domains, therefore cutting development and maintenance costs by the same\nfactor. Besides BLEU (industry standard automatic evaluation metric known to\nonly weakly correlate with human judgement) we also report rigorous human\nevaluation results for all models and sub-domains as well as specific examples\nthat better contextualise the performance of each model in terms of adequacy\nand fluency. To facilitate further research, we plan to make the code available\nupon acceptance.",
          "link": "http://arxiv.org/abs/2102.10160",
          "publishedOn": "2021-08-03T02:06:29.968Z",
          "wordCount": 729,
          "title": "Multi-Domain Adaptation in Neural Machine Translation Through Multidimensional Tagging. (arXiv:2102.10160v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1\">Omar Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>",
          "description": "Systems for Open-Domain Question Answering (OpenQA) generally depend on a\nretriever for finding candidate passages in a large corpus and a reader for\nextracting answers from those passages. In much recent work, the retriever is a\nlearned component that uses coarse-grained vector representations of questions\nand passages. We argue that this modeling choice is insufficiently expressive\nfor dealing with the complexity of natural language questions. To address this,\nwe define ColBERT-QA, which adapts the scalable neural retrieval model ColBERT\nto OpenQA. ColBERT creates fine-grained interactions between questions and\npassages. We propose an efficient weak supervision strategy that iteratively\nuses ColBERT to create its own training data. This greatly improves OpenQA\nretrieval on Natural Questions, SQuAD, and TriviaQA, and the resulting system\nattains state-of-the-art extractive OpenQA performance on all three datasets.",
          "link": "http://arxiv.org/abs/2007.00814",
          "publishedOn": "2021-08-03T02:06:29.947Z",
          "wordCount": 609,
          "title": "Relevance-guided Supervision for OpenQA with ColBERT. (arXiv:2007.00814v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grundmann_P/0/1/0/all/0/1\">Paul Grundmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_S/0/1/0/all/0/1\">Sebastian Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loser_A/0/1/0/all/0/1\">Alexander L&#xf6;ser</a>",
          "description": "Retrieving answer passages from long documents is a complex task requiring\nsemantic understanding of both discourse and document context. We approach this\nchallenge specifically in a clinical scenario, where doctors retrieve cohorts\nof patients based on diagnoses and other latent medical aspects. We introduce\nCAPR, a rule-based self-supervision objective for training Transformer language\nmodels for domain-specific passage matching. In addition, we contribute a novel\nretrieval dataset based on clinical notes to simulate this scenario on a large\ncorpus of clinical notes. We apply our objective in four Transformer-based\narchitectures: Contextual Document Vectors, Bi-, Poly- and Cross-encoders. From\nour extensive evaluation on MIMIC-III and three other healthcare datasets, we\nreport that CAPR outperforms strong baselines in the retrieval of\ndomain-specific passages and effectively generalizes across rule-based and\nhuman-labeled passages. This makes the model powerful especially in zero-shot\nscenarios where only limited training data is available.",
          "link": "http://arxiv.org/abs/2108.00775",
          "publishedOn": "2021-08-03T02:06:29.941Z",
          "wordCount": 572,
          "title": "Self-supervised Answer Retrieval on Clinical Notes. (arXiv:2108.00775v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2001.01037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiamei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1\">Sebastian Lapuschkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1\">Wojciech Samek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1\">Alexander Binder</a>",
          "description": "This paper analyzes the predictions of image captioning models with attention\nmechanisms beyond visualizing the attention itself. We develop variants of\nlayer-wise relevance propagation (LRP) and gradient-based explanation methods,\ntailored to image captioning models with attention mechanisms. We compare the\ninterpretability of attention heatmaps systematically against the explanations\nprovided by explanation methods such as LRP, Grad-CAM, and Guided Grad-CAM. We\nshow that explanation methods provide simultaneously pixel-wise image\nexplanations (supporting and opposing pixels of the input image) and linguistic\nexplanations (supporting and opposing words of the preceding sequence) for each\nword in the predicted captions. We demonstrate with extensive experiments that\nexplanation methods 1) can reveal additional evidence used by the model to make\ndecisions compared to attention; 2) correlate to object locations with high\nprecision; 3) are helpful to \"debug\" the model, e.g. by analyzing the reasons\nfor hallucinated object words. With the observed properties of explanations, we\nfurther design an LRP-inference fine-tuning strategy that reduces the issue of\nobject hallucination in image captioning models, and meanwhile, maintains the\nsentence fluency. We conduct experiments with two widely used attention\nmechanisms: the adaptive attention mechanism calculated with the additive\nattention and the multi-head attention mechanism calculated with the scaled dot\nproduct.",
          "link": "http://arxiv.org/abs/2001.01037",
          "publishedOn": "2021-08-03T02:06:29.934Z",
          "wordCount": 703,
          "title": "Explain and Improve: LRP-Inference Fine-Tuning for Image Captioning Models. (arXiv:2001.01037v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Nupur Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_A/0/1/0/all/0/1\">Anshul Rai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Gaurav Kumar</a>",
          "description": "To truly grasp reasoning ability, a Natural Language Inference model should\nbe evaluated on counterfactual data. TabPert facilitates this by assisting in\nthe generation of such counterfactual data for assessing model tabular\nreasoning issues. TabPert allows a user to update a table, change its\nassociated hypotheses, change their labels, and highlight rows that are\nimportant for hypothesis classification. TabPert also captures information\nabout the techniques used to automatically produce the table, as well as the\nstrategies employed to generate the challenging hypotheses. These\ncounterfactual tables and hypotheses, as well as the metadata, can then be used\nto explore an existing model's shortcomings methodically and quantitatively.",
          "link": "http://arxiv.org/abs/2108.00603",
          "publishedOn": "2021-08-03T02:06:29.926Z",
          "wordCount": 553,
          "title": "TabPert: An Effective Platform for Tabular Perturbation. (arXiv:2108.00603v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhongkun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>",
          "description": "Complex reasoning aims to draw a correct inference based on complex rules. As\na hallmark of human intelligence, it involves a degree of explicit reading\ncomprehension, interpretation of logical knowledge and complex rule\napplication. In this paper, we take a step forward in complex reasoning by\nsystematically studying the three challenging and domain-general tasks of the\nLaw School Admission Test (LSAT), including analytical reasoning, logical\nreasoning and reading comprehension. We propose a hybrid reasoning system to\nintegrate these three tasks and achieve impressive overall performance on the\nLSAT tests. The experimental results demonstrate that our system endows itself\na certain complex reasoning ability, especially the fundamental reading\ncomprehension and challenging logical reasoning capacities. Further analysis\nalso shows the effectiveness of combining the pre-trained models with the\ntask-specific reasoning module, and integrating symbolic knowledge into\ndiscrete interpretable reasoning steps in complex reasoning. We further shed a\nlight on the potential future directions, like unsupervised symbolic knowledge\nextraction, model interpretability, few-shot learning and comprehensive\nbenchmark for complex reasoning.",
          "link": "http://arxiv.org/abs/2108.00648",
          "publishedOn": "2021-08-03T02:06:29.896Z",
          "wordCount": 626,
          "title": "From LSAT: The Progress and Challenges of Complex Reasoning. (arXiv:2108.00648v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kellert_O/0/1/0/all/0/1\">Olga Kellert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matlis_N/0/1/0/all/0/1\">Nicholas H. Matlis</a>",
          "description": "The explosion in the availability of natural language data in the era of\nsocial media has given rise to a host of applications such as sentiment\nanalysis and opinion mining. Simultaneously, the growing availability of\nprecise geolocation information is enabling visualization of global phenomena\nsuch as environmental changes and disease propagation. Opportunities for\ntracking spatial variations in language use, however, have largely been\noverlooked, especially on small spatial scales. Here we explore the use of\nTwitter data with precise geolocation information to resolve spatial variations\nin language use on an urban scale down to single city blocks. We identify\nseveral categories of language tokens likely to show distinctive patterns of\nuse and develop quantitative methods to visualize the spatial distributions\nassociated with these patterns. Our analysis concentrates on comparison of\ncontrasting pairs of Tweet distributions from the same category, each defined\nby a set of tokens. Our work shows that analysis of small-scale variations can\nprovide unique information on correlations between language use and social\ncontext which are highly valuable to a wide range of fields from linguistic\nscience and commercial advertising to social services.",
          "link": "http://arxiv.org/abs/2108.00533",
          "publishedOn": "2021-08-03T02:06:29.874Z",
          "wordCount": 618,
          "title": "Geolocation differences of language use in urban areas. (arXiv:2108.00533v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongkyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhiliang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Lanqing Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nevin L. Zhang</a>",
          "description": "Text style transfer aims to alter the style (e.g., sentiment) of a sentence\nwhile preserving its content. A common approach is to map a given sentence to\ncontent representation that is free of style, and the content representation is\nfed to a decoder with a target style. Previous methods in filtering style\ncompletely remove tokens with style at the token level, which incurs the loss\nof content information. In this paper, we propose to enhance content\npreservation by implicitly removing the style information of each token with\nreverse attention, and thereby retain the content. Furthermore, we fuse content\ninformation when building the target style representation, making it dynamic\nwith respect to the content. Our method creates not only style-independent\ncontent representation, but also content-dependent style representation in\ntransferring style. Empirical results show that our method outperforms the\nstate-of-the-art baselines by a large margin in terms of content preservation.\nIn addition, it is also competitive in terms of style transfer accuracy and\nfluency.",
          "link": "http://arxiv.org/abs/2108.00449",
          "publishedOn": "2021-08-03T02:06:29.866Z",
          "wordCount": 615,
          "title": "Enhancing Content Preservation in Text Style Transfer Using Reverse Attention and Conditional Layer Normalization. (arXiv:2108.00449v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1\">Mithun Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1\">Punyajoy Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutt_R/0/1/0/all/0/1\">Ritam Dutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_B/0/1/0/all/0/1\">Binny Mathew</a>",
          "description": "Hate speech is regarded as one of the crucial issues plaguing the online\nsocial media. The current literature on hate speech detection leverages\nprimarily the textual content to find hateful posts and subsequently identify\nhateful users. However, this methodology disregards the social connections\nbetween users. In this paper, we run a detailed exploration of the problem\nspace and investigate an array of models ranging from purely textual to graph\nbased to finally semi-supervised techniques using Graph Neural Networks (GNN)\nthat utilize both textual and graph-based features. We run exhaustive\nexperiments on two datasets -- Gab, which is loosely moderated and Twitter,\nwhich is strictly moderated. Overall the AGNN model achieves 0.791 macro\nF1-score on the Gab dataset and 0.780 macro F1-score on the Twitter dataset\nusing only 5% of the labeled instances, considerably outperforming all the\nother models including the fully supervised ones. We perform detailed error\nanalysis on the best performing text and graph based models and observe that\nhateful users have unique network neighborhood signatures and the AGNN model\nbenefits by paying attention to these signatures. This property, as we observe,\nalso allows the model to generalize well across platforms in a zero-shot\nsetting. Lastly, we utilize the best performing GNN model to analyze the\nevolution of hateful users and their targets over time in Gab.",
          "link": "http://arxiv.org/abs/2108.00524",
          "publishedOn": "2021-08-03T02:06:29.855Z",
          "wordCount": 694,
          "title": "You too Brutus! Trapping Hateful Users in Social Media: Challenges, Solutions & Insights. (arXiv:2108.00524v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henao_P/0/1/0/all/0/1\">Pablo Restrepo Henao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischbach_J/0/1/0/all/0/1\">Jannik Fischbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spies_D/0/1/0/all/0/1\">Dominik Spies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frattini_J/0/1/0/all/0/1\">Julian Frattini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelsang_A/0/1/0/all/0/1\">Andreas Vogelsang</a>",
          "description": "Identifying feature requests and bug reports in user comments holds great\npotential for development teams. However, automated mining of RE-related\ninformation from social media and app stores is challenging since (1) about 70%\nof user comments contain noisy, irrelevant information, (2) the amount of user\ncomments grows daily making manual analysis unfeasible, and (3) user comments\nare written in different languages. Existing approaches build on traditional\nmachine learning (ML) and deep learning (DL), but fail to detect feature\nrequests and bug reports with high Recall and acceptable Precision which is\nnecessary for this task. In this paper, we investigate the potential of\ntransfer learning (TL) for the classification of user comments. Specifically,\nwe train both monolingual and multilingual BERT models and compare the\nperformance with state-of-the-art methods. We found that monolingual BERT\nmodels outperform existing baseline methods in the classification of English\nApp Reviews as well as English and Italian Tweets. However, we also observed\nthat the application of heavyweight TL models does not necessarily lead to\nbetter performance. In fact, our multilingual BERT models perform worse than\ntraditional ML methods.",
          "link": "http://arxiv.org/abs/2108.00663",
          "publishedOn": "2021-08-03T02:06:29.849Z",
          "wordCount": 634,
          "title": "Transfer Learning for Mining Feature Requests and Bug Reports from Tweets and App Store Reviews. (arXiv:2108.00663v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stent_A/0/1/0/all/0/1\">Amanda Stent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1\">Mark Dredze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>",
          "description": "Commonly-used transformer language models depend on a tokenization schema\nwhich sets an unchangeable subword vocabulary prior to pre-training, destined\nto be applied to all downstream tasks regardless of domain shift, novel word\nformations, or other sources of vocabulary mismatch. Recent work has shown that\n\"token-free\" models can be trained directly on characters or bytes, but\ntraining these models from scratch requires substantial computational\nresources, and this implies discarding the many domain-specific models that\nwere trained on tokens. In this paper, we present XRayEmb, a method for\nretrofitting existing token-based models with character-level information.\nXRayEmb is composed of a character-level \"encoder\" that computes vector\nrepresentations of character sequences, and a generative component that decodes\nfrom the internal representation to a character sequence. We show that\nincorporating XRayEmb's learned vectors into sequences of pre-trained token\nembeddings helps performance on both autoregressive and masked pre-trained\ntransformer architectures and on both sequence-level and sequence tagging\ntasks, particularly on non-standard English text.",
          "link": "http://arxiv.org/abs/2108.00391",
          "publishedOn": "2021-08-03T02:06:29.842Z",
          "wordCount": 592,
          "title": "Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information. (arXiv:2108.00391v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1\">Harsh Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>",
          "description": "To build challenging multi-hop question answering datasets, we propose a\nbottom-up semi-automatic process of constructing multi-hop question via\ncomposition of single-hop questions. Constructing multi-hop questions as\ncomposition of single-hop questions allows us to exercise greater control over\nthe quality of the resulting multi-hop questions. This process allows building\na dataset with (i) connected reasoning where each step needs the answer from a\nprevious step; (ii) minimal train-test leakage by eliminating even partial\noverlap of reasoning steps; (iii) variable number of hops and composition\nstructures; and (iv) contrasting unanswerable questions by modifying the\ncontext. We use this process to construct a new multihop QA dataset:\nMuSiQue-Ans with ~25K 2-4 hop questions using seed questions from 5 existing\nsingle-hop datasets. Our experiments demonstrate that MuSique is challenging\nfor state-of-the-art QA models (e.g., human-machine gap of $~$30 F1 pts),\nsignificantly harder than existing datasets (2x human-machine gap), and\nsubstantially less cheatable (e.g., a single-hop model is worse by 30 F1 pts).\nWe also build an even more challenging dataset, MuSiQue-Full, consisting of\nanswerable and unanswerable contrast question pairs, where model performance\ndrops further by 13+ F1 pts. For data and code, see\n\\url{https://github.com/stonybrooknlp/musique}.",
          "link": "http://arxiv.org/abs/2108.00573",
          "publishedOn": "2021-08-03T02:06:29.835Z",
          "wordCount": 626,
          "title": "MuSiQue: Multi-hop Questions via Single-hop Question Composition. (arXiv:2108.00573v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1\">Chang Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiangyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>",
          "description": "Text generation from semantic parses is to generate textual descriptions for\nformal representation inputs such as logic forms and SQL queries. This is\nchallenging due to two reasons: (1) the complex and intensive inner logic with\nthe data scarcity constraint, (2) the lack of automatic evaluation metrics for\nlogic consistency. To address these two challenges, this paper first proposes\nSNOWBALL, a framework for logic consistent text generation from semantic parses\nthat employs an iterative training procedure by recursively augmenting the\ntraining set with quality control. Second, we propose a novel automatic metric,\nBLEC, for evaluating the logical consistency between the semantic parses and\ngenerated texts. The experimental results on two benchmark datasets, Logic2Text\nand Spider, demonstrate the SNOWBALL framework enhances the logic consistency\non both BLEC and human evaluation. Furthermore, our statistical analysis\nreveals that BLEC is more logically consistent with human evaluation than\ngeneral-purpose automatic metrics including BLEU, ROUGE and, BLEURT. Our data\nand code are available at https://github.com/Ciaranshu/relogic.",
          "link": "http://arxiv.org/abs/2108.00577",
          "publishedOn": "2021-08-03T02:06:29.733Z",
          "wordCount": 597,
          "title": "Logic-Consistency Text Generation from Semantic Parses. (arXiv:2108.00577v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1\">Riyaz A. Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_A/0/1/0/all/0/1\">Atreya Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1\">Manish Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Maneesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>",
          "description": "While neural models routinely report state-of-the-art performance across NLP\ntasks involving reasoning, their outputs are often observed to not properly use\nand reason on the evidence presented to them in the inputs. A model that\nreasons properly is expected to attend to the right parts of the input, be\nself-consistent in its predictions across examples, avoid spurious patterns in\ninputs, and to ignore biasing from its underlying pre-trained language model in\na nuanced, context-sensitive fashion (e.g. handling counterfactuals). Do\ntoday's models do so? In this paper, we study this question using the problem\nof reasoning on tabular data. The tabular nature of the input is particularly\nsuited for the study as it admits systematic probes targeting the properties\nlisted above. Our experiments demonstrate that a BERT-based model\nrepresentative of today's state-of-the-art fails to properly reason on the\nfollowing counts: it often (a) misses the relevant evidence, (b) suffers from\nhypothesis and knowledge biases, and, (c) relies on annotation artifacts and\nknowledge from pre-trained language models as primary evidence rather than\nrelying on reasoning on the premises in the tabular input.",
          "link": "http://arxiv.org/abs/2108.00578",
          "publishedOn": "2021-08-03T02:06:29.726Z",
          "wordCount": 639,
          "title": "Is My Model Using The Right Evidence? Systematic Probes for Examining Evidence-Based Tabular Reasoning. (arXiv:2108.00578v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anshul Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahiya_K/0/1/0/all/0/1\">Kunal Dahiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sheshansh Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saini_D/0/1/0/all/0/1\">Deepak Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sumeet Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1\">Purushottam Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Manik Varma</a>",
          "description": "Extreme multi-label classification (XML) involves tagging a data point with\nits most relevant subset of labels from an extremely large label set, with\nseveral applications such as product-to-product recommendation with millions of\nproducts. Although leading XML algorithms scale to millions of labels, they\nlargely ignore label meta-data such as textual descriptions of the labels. On\nthe other hand, classical techniques that can utilize label metadata via\nrepresentation learning using deep networks struggle in extreme settings. This\npaper develops the DECAF algorithm that addresses these challenges by learning\nmodels enriched by label metadata that jointly learn model parameters and\nfeature representations using deep networks and offer accurate classification\nat the scale of millions of labels. DECAF makes specific contributions to model\narchitecture design, initialization, and training, enabling it to offer up to\n2-6% more accurate prediction than leading extreme classifiers on publicly\navailable benchmark product-to-product recommendation datasets, such as\nLF-AmazonTitles-1.3M. At the same time, DECAF was found to be up to 22x faster\nat inference than leading deep extreme classifiers, which makes it suitable for\nreal-time applications that require predictions within a few milliseconds. The\ncode for DECAF is available at the following URL\nhttps://github.com/Extreme-classification/DECAF.",
          "link": "http://arxiv.org/abs/2108.00368",
          "publishedOn": "2021-08-03T02:06:29.720Z",
          "wordCount": 654,
          "title": "DECAF: Deep Extreme Classification with Label Features. (arXiv:2108.00368v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00480",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Rahimikia_E/0/1/0/all/0/1\">Eghbal Rahimikia</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zohren_S/0/1/0/all/0/1\">Stefan Zohren</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Poon_S/0/1/0/all/0/1\">Ser-Huang Poon</a>",
          "description": "We develop FinText, a novel, state-of-the-art, financial word embedding from\nDow Jones Newswires Text News Feed Database. Incorporating this word embedding\nin a machine learning model produces a substantial increase in volatility\nforecasting performance on days with volatility jumps for 23 NASDAQ stocks from\n27 July 2007 to 18 November 2016. A simple ensemble model, combining our word\nembedding and another machine learning model that uses limit order book data,\nprovides the best forecasting performance for both normal and jump volatility\ndays. Finally, we use Integrated Gradients and SHAP (SHapley Additive\nexPlanations) to make the results more 'explainable' and the model comparisons\nmore transparent.",
          "link": "http://arxiv.org/abs/2108.00480",
          "publishedOn": "2021-08-03T02:06:29.702Z",
          "wordCount": 549,
          "title": "Realised Volatility Forecasting: Machine Learning via Financial Word Embedding. (arXiv:2108.00480v1 [q-fin.CP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bruyn_M/0/1/0/all/0/1\">Maxime De Bruyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotfi_E/0/1/0/all/0/1\">Ehsan Lotfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhmann_J/0/1/0/all/0/1\">Jeska Buhmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daelemans_W/0/1/0/all/0/1\">Walter Daelemans</a>",
          "description": "Knowledgeable FAQ chatbots are a valuable resource to any organization.\nUnlike traditional call centers or FAQ web pages, they provide instant\nresponses and are always available. Our experience running a COVID19 chatbot\nrevealed the lack of resources available for FAQ answering in non-English\nlanguages. While powerful and efficient retrieval-based models exist for\nEnglish, it is rarely the case for other languages which do not have the same\namount of training data available. In this work, we propose a novel pretaining\nprocedure to adapt ConveRT, an English SOTA conversational agent, to other\nlanguages with less training data available. We apply it for the first time to\nthe task of Dutch FAQ answering related to the COVID19 vaccine. We show it\nperforms better than an open-source alternative in a low-data regime and\nhigh-data regime.",
          "link": "http://arxiv.org/abs/2108.00719",
          "publishedOn": "2021-08-03T02:06:29.688Z",
          "wordCount": 606,
          "title": "ConveRT, an Application to FAQ Answering. (arXiv:2108.00719v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Joy T. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agu_N/0/1/0/all/0/1\">Nkechinyere N. Agu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1\">Ismini Lourentzou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Arjun Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paguio_J/0/1/0/all/0/1\">Joseph A. Paguio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jasper S. Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dee_E/0/1/0/all/0/1\">Edward C. Dee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_W/0/1/0/all/0/1\">William Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashyap_S/0/1/0/all/0/1\">Satyananda Kashyap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giovannini_A/0/1/0/all/0/1\">Andrea Giovannini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1\">Leo A. Celi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Mehdi Moradi</a>",
          "description": "Despite the progress in automatic detection of radiologic findings from chest\nX-ray (CXR) images in recent years, a quantitative evaluation of the\nexplainability of these models is hampered by the lack of locally labeled\ndatasets for different findings. With the exception of a few expert-labeled\nsmall-scale datasets for specific findings, such as pneumonia and pneumothorax,\nmost of the CXR deep learning models to date are trained on global \"weak\"\nlabels extracted from text reports, or trained via a joint image and\nunstructured text learning strategy. Inspired by the Visual Genome effort in\nthe computer vision community, we constructed the first Chest ImaGenome dataset\nwith a scene graph data structure to describe $242,072$ images. Local\nannotations are automatically produced using a joint rule-based natural\nlanguage processing (NLP) and atlas-based bounding box detection pipeline.\nThrough a radiologist constructed CXR ontology, the annotations for each CXR\nare connected as an anatomy-centered scene graph, useful for image-level\nreasoning and multimodal fusion applications. Overall, we provide: i) $1,256$\ncombinations of relation annotations between $29$ CXR anatomical locations\n(objects with bounding box coordinates) and their attributes, structured as a\nscene graph per image, ii) over $670,000$ localized comparison relations (for\nimproved, worsened, or no change) between the anatomical locations across\nsequential exams, as well as ii) a manually annotated gold standard scene graph\ndataset from $500$ unique patients.",
          "link": "http://arxiv.org/abs/2108.00316",
          "publishedOn": "2021-08-03T02:06:29.681Z",
          "wordCount": 697,
          "title": "Chest ImaGenome Dataset for Clinical Reasoning. (arXiv:2108.00316v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00513",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Tian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_K/0/1/0/all/0/1\">Khushbu Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1\">Sutanay Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chandan K. Reddy</a>",
          "description": "Question Answering (QA) in clinical notes has gained a lot of attention in\nthe past few years. Existing machine reading comprehension approaches in\nclinical domain can only handle questions about a single block of clinical\ntexts and fail to retrieve information about different patients and clinical\nnotes. To handle more complex questions, we aim at creating knowledge base from\nclinical notes to link different patients and clinical notes, and performing\nknowledge base question answering (KBQA). Based on the expert annotations in\nn2c2, we first created the ClinicalKBQA dataset that includes 8,952 QA pairs\nand covers questions about seven medical topics through 322 question templates.\nThen, we proposed an attention-based aspect reasoning (AAR) method for KBQA and\ninvestigated the impact of different aspects of answers (e.g., entity, type,\npath, and context) for prediction. The AAR method achieves better performance\ndue to the well-designed encoder and attention mechanism. In the experiments,\nwe find that both aspects, type and path, enable the model to identify answers\nsatisfying the general conditions and produce lower precision and higher\nrecall. On the other hand, the aspects, entity and context, limit the answers\nby node-specific information and lead to higher precision and lower recall.",
          "link": "http://arxiv.org/abs/2108.00513",
          "publishedOn": "2021-08-03T02:06:29.652Z",
          "wordCount": 638,
          "title": "Attention-based Aspect Reasoning for Knowledge Base Question Answering on Clinical Notes. (arXiv:2108.00513v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binlong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>",
          "description": "Chinese sentiment analysis (CSA) has always been one of the challenges in\nnatural language processing due to its complexity and uncertainty. Transformer\nhas succeeded in capturing semantic features, but it uses position encoding to\ncapture sequence features, which has great shortcomings compared with the\nrecurrent model. In this paper, we propose T-E-GRU for Chinese sentiment\nanalysis, which combine transformer encoder and GRU. We conducted experiments\non three Chinese comment datasets. In view of the confusion of punctuation\nmarks in Chinese comment texts, we selectively retain some punctuation marks\nwith sentence segmentation ability. The experimental results show that T-E-GRU\noutperforms classic recurrent model and recurrent model with attention.",
          "link": "http://arxiv.org/abs/2108.00400",
          "publishedOn": "2021-08-03T02:06:29.607Z",
          "wordCount": 542,
          "title": "Transformer-Encoder-GRU (T-E-GRU) for Chinese Sentiment Analysis on Chinese Comment Text. (arXiv:2108.00400v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gadde_R/0/1/0/all/0/1\">Ravi Teja Gadde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1\">Ivan Bulyko</a>",
          "description": "Neural language models (LM) trained on diverse corpora are known to work well\non previously seen entities, however, updating these models with dynamically\nchanging entities such as place names, song titles and shopping items requires\nre-training from scratch and collecting full sentences containing these\nentities. We aim to address this issue, by introducing entity-aware language\nmodels (EALM), where we integrate entity models trained on catalogues of\nentities into the pre-trained LMs. Our combined language model adaptively adds\ninformation from the entity models into the pre-trained LM depending on the\nsentence context. Our entity models can be updated independently of the\npre-trained LM, enabling us to influence the distribution of entities output by\nthe final LM, without any further training of the pre-trained LM. We show\nsignificant perplexity improvements on task-oriented dialogue datasets,\nespecially on long-tailed utterances, with an ability to continually adapt to\nnew entities (to an extent).",
          "link": "http://arxiv.org/abs/2108.00082",
          "publishedOn": "2021-08-03T02:06:29.557Z",
          "wordCount": 595,
          "title": "Towards Continual Entity Learning in Language Models for Conversational Agents. (arXiv:2108.00082v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Podina_I/0/1/0/all/0/1\">Ioana R. Podin&#x103;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1\">Liviu P. Dinu</a>",
          "description": "In this work, we provide an extensive part-of-speech analysis of the\ndiscourse of social media users with depression. Research in psychology\nrevealed that depressed users tend to be self-focused, more preoccupied with\nthemselves and ruminate more about their lives and emotions. Our work aims to\nmake use of large-scale datasets and computational methods for a quantitative\nexploration of discourse. We use the publicly available depression dataset from\nthe Early Risk Prediction on the Internet Workshop (eRisk) 2018 and extract\npart-of-speech features and several indices based on them. Our results reveal\nstatistically significant differences between the depressed and non-depressed\nindividuals confirming findings from the existing psychology literature. Our\nwork provides insights regarding the way in which depressed individuals are\nexpressing themselves on social media platforms, allowing for better-informed\ncomputational models to help monitor and prevent mental illnesses.",
          "link": "http://arxiv.org/abs/2108.00279",
          "publishedOn": "2021-08-03T02:06:29.550Z",
          "wordCount": 576,
          "title": "A Psychologically Informed Part-of-Speech Analysis of Depression in Social Media. (arXiv:2108.00279v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anshul Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_N/0/1/0/all/0/1\">Noveen Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sheshansh Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sumeet Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1\">Purushottam Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Manik Varma</a>",
          "description": "Deep extreme classification (XC) seeks to train deep architectures that can\ntag a data point with its most relevant subset of labels from an extremely\nlarge label set. The core utility of XC comes from predicting labels that are\nrarely seen during training. Such rare labels hold the key to personalized\nrecommendations that can delight and surprise a user. However, the large number\nof rare labels and small amount of training data per rare label offer\nsignificant statistical and computational challenges. State-of-the-art deep XC\nmethods attempt to remedy this by incorporating textual descriptions of labels\nbut do not adequately address the problem. This paper presents ECLARE, a\nscalable deep learning architecture that incorporates not only label text, but\nalso label correlations, to offer accurate real-time predictions within a few\nmilliseconds. Core contributions of ECLARE include a frugal architecture and\nscalable techniques to train deep models along with label correlation graphs at\nthe scale of millions of labels. In particular, ECLARE offers predictions that\nare 2 to 14% more accurate on both publicly available benchmark datasets as\nwell as proprietary datasets for a related products recommendation task sourced\nfrom the Bing search engine. Code for ECLARE is available at\nhttps://github.com/Extreme-classification/ECLARE.",
          "link": "http://arxiv.org/abs/2108.00261",
          "publishedOn": "2021-08-03T02:06:29.542Z",
          "wordCount": 655,
          "title": "ECLARE: Extreme Classification with Label Graph Correlations. (arXiv:2108.00261v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suresh_V/0/1/0/all/0/1\">Varsha Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_D/0/1/0/all/0/1\">Desmond C. Ong</a>",
          "description": "Modern emotion recognition systems are trained to recognize only a small set\nof emotions, and hence fail to capture the broad spectrum of emotions people\nexperience and express in daily life. In order to engage in more empathetic\ninteractions, future AI has to perform \\textit{fine-grained} emotion\nrecognition, distinguishing between many more varied emotions. Here, we focus\non improving fine-grained emotion recognition by introducing external knowledge\ninto a pre-trained self-attention model. We propose Knowledge-Embedded\nAttention (KEA) to use knowledge from emotion lexicons to augment the\ncontextual representations from pre-trained ELECTRA and BERT models. Our\nresults and error analyses outperform previous models on several datasets, and\nis better able to differentiate closely-confusable emotions, such as afraid and\nterrified.",
          "link": "http://arxiv.org/abs/2108.00194",
          "publishedOn": "2021-08-03T02:06:29.510Z",
          "wordCount": 567,
          "title": "Using Knowledge-Embedded Attention to Augment Pre-trained Language Models for Fine-Grained Emotion Recognition. (arXiv:2108.00194v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1\">AbdelRahim Elmadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1\">El Moatez Billah Nagoudi</a>",
          "description": "Masked language models (MLMs) are pretrained with a denoising objective that,\nwhile useful, is in a mismatch with the objective of downstream fine-tuning. We\npropose pragmatic masking and surrogate fine-tuning as two strategies that\nexploit social cues to drive pre-trained representations toward a broad set of\nconcepts useful for a wide class of social meaning tasks. To test our methods,\nwe introduce a new benchmark of 15 different Twitter datasets for social\nmeaning detection. Our methods achieve 2.34% F1 over a competitive baseline,\nwhile outperforming other transfer learning methods such as multi-task learning\nand domain-specific language models pretrained on large datasets. With only 5%\nof training data (severely few-shot), our methods enable an impressive 68.74%\naverage F1, and we observe promising results in a zero-shot setting involving\nsix datasets from three different languages.",
          "link": "http://arxiv.org/abs/2108.00356",
          "publishedOn": "2021-08-03T02:06:29.447Z",
          "wordCount": 577,
          "title": "Improving Social Meaning Detection with Pragmatic Masking and Surrogate Fine-Tuning. (arXiv:2108.00356v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00104",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1\">Peng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseem_T/0/1/0/all/0/1\">Tahira Naseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1\">Ram&#xf3;n Fernandez Astudillo</a>",
          "description": "Transformer-based language models pre-trained on large amounts of text data\nhave proven remarkably successful in learning generic transferable linguistic\nrepresentations. Here we study whether structural guidance leads to more\nhuman-like systematic linguistic generalization in Transformer language models\nwithout resorting to pre-training on very large amounts of data. We explore two\ngeneral ideas. The \"Generative Parsing\" idea jointly models the incremental\nparse and word sequence as part of the same sequence modeling task. The\n\"Structural Scaffold\" idea guides the language model's representation via\nadditional structure loss that separately predicts the incremental constituency\nparse. We train the proposed models along with a vanilla Transformer language\nmodel baseline on a 14 million-token and a 46 million-token subset of the BLLIP\ndataset, and evaluate models' syntactic generalization performances on SG Test\nSuites and sized BLiMP. Experiment results across two benchmarks suggest\nconverging evidence that generative structural supervisions can induce more\nrobust and humanlike linguistic generalization in Transformer language models\nwithout the need for data intensive pre-training.",
          "link": "http://arxiv.org/abs/2108.00104",
          "publishedOn": "2021-08-03T02:06:29.439Z",
          "wordCount": 601,
          "title": "Structural Guidance for Transformer Language Models. (arXiv:2108.00104v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara L. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "We introduce mTVR, a large-scale multilingual video moment retrieval dataset,\ncontaining 218K English and Chinese queries from 21.8K TV show video clips. The\ndataset is collected by extending the popular TVR dataset (in English) with\npaired Chinese queries and subtitles. Compared to existing moment retrieval\ndatasets, mTVR is multilingual, larger, and comes with diverse annotations. We\nfurther propose mXML, a multilingual moment retrieval model that learns and\noperates on data from both languages, via encoder parameter sharing and\nlanguage neighborhood constraints. We demonstrate the effectiveness of mXML on\nthe newly collected MTVR dataset, where mXML outperforms strong monolingual\nbaselines while using fewer parameters. In addition, we also provide detailed\ndataset analyses and model ablations. Data and code are publicly available at\nhttps://github.com/jayleicn/mTVRetrieval",
          "link": "http://arxiv.org/abs/2108.00061",
          "publishedOn": "2021-08-03T02:06:29.423Z",
          "wordCount": 569,
          "title": "MTVR: Multilingual Moment Retrieval in Videos. (arXiv:2108.00061v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tumarada_K/0/1/0/all/0/1\">Kishore Tumarada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dr. Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragut_D/0/1/0/all/0/1\">Dr. Eduard Dragut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gnawali_D/0/1/0/all/0/1\">Dr. Omprakash Gnawali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_D/0/1/0/all/0/1\">Dr. Arjun Mukherjee</a>",
          "description": "Opinion prediction is an emerging research area with diverse real-world\napplications, such as market research and situational awareness. We identify\ntwo lines of approaches to the problem of opinion prediction. One uses\ntopic-based sentiment analysis with time-series modeling, while the other uses\nstatic embedding of text. The latter approaches seek user-specific solutions by\ngenerating user fingerprints. Such approaches are useful in predicting user's\nreactions to unseen content. In this work, we propose a novel dynamic\nfingerprinting method that leverages contextual embedding of user's comments\nconditioned on relevant user's reading history. We integrate BERT variants with\na recurrent neural network to generate predictions. The results show up to 13\\%\nimprovement in micro F1-score compared to previous approaches. Experimental\nresults show novel insights that were previously unknown such as better\npredictions for an increase in dynamic history length, the impact of the nature\nof the article on performance, thereby laying the foundation for further\nresearch.",
          "link": "http://arxiv.org/abs/2108.00270",
          "publishedOn": "2021-08-03T02:06:29.400Z",
          "wordCount": 599,
          "title": "Opinion Prediction with User Fingerprinting. (arXiv:2108.00270v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1\">Mika H&#xe4;m&#xe4;l&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1\">Khalid Alnajjar</a>",
          "description": "We survey human evaluation in papers presenting work on creative natural\nlanguage generation that have been published in INLG 2020 and ICCC 2020. The\nmost typical human evaluation method is a scaled survey, typically on a 5 point\nscale, while many other less common methods exist. The most commonly evaluated\nparameters are meaning, syntactic correctness, novelty, relevance and emotional\nvalue, among many others. Our guidelines for future evaluation include clearly\ndefining the goal of the generative system, asking questions as concrete as\npossible, testing the evaluation setup, using multiple different evaluation\nsetups, reporting the entire evaluation process and potential biases clearly,\nand finally analyzing the evaluation results in a more profound way than merely\nreporting the most typical statistics.",
          "link": "http://arxiv.org/abs/2108.00308",
          "publishedOn": "2021-08-03T02:06:29.385Z",
          "wordCount": 570,
          "title": "Human Evaluation of Creative NLG Systems: An Interdisciplinary Survey on Recent Papers. (arXiv:2108.00308v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1\">Ethel Ong</a>",
          "description": "In order to ensure quality and effective learning, fluency, and\ncomprehension, the proper identification of the difficulty levels of reading\nmaterials should be observed. In this paper, we describe the development of\nautomatic machine learning-based readability assessment models for educational\nFilipino texts using the most diverse set of linguistic features for the\nlanguage. Results show that using a Random Forest model obtained a high\nperformance of 62.7% in terms of accuracy, and 66.1% when using the optimal\ncombination of feature sets consisting of traditional and syllable\npattern-based predictors.",
          "link": "http://arxiv.org/abs/2108.00241",
          "publishedOn": "2021-08-03T02:06:29.368Z",
          "wordCount": 531,
          "title": "Diverse Linguistic Features for Assessing Reading Difficulty of Educational Filipino Texts. (arXiv:2108.00241v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Heng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1\">Yew-Soon Ong</a>",
          "description": "Current one-stage methods for visual grounding encode the language query as\none holistic sentence embedding before fusion with visual feature. Such a\nformulation does not treat each word of a query sentence on par when modeling\nlanguage to visual attention, therefore prone to neglect words which are less\nimportant for sentence embedding but critical for visual grounding. In this\npaper we propose Word2Pix: a one-stage visual grounding network based on\nencoder-decoder transformer architecture that enables learning for textual to\nvisual feature correspondence via word to pixel attention. The embedding of\neach word from the query sentence is treated alike by attending to visual\npixels individually instead of single holistic sentence embedding. In this way,\neach word is given equivalent opportunity to adjust the language to vision\nattention towards the referent target through multiple stacks of transformer\ndecoder layers. We conduct the experiments on RefCOCO, RefCOCO+ and RefCOCOg\ndatasets and the proposed Word2Pix outperforms existing one-stage methods by a\nnotable margin. The results obtained also show that Word2Pix surpasses\ntwo-stage visual grounding models, while at the same time keeping the merits of\none-stage paradigm namely end-to-end training and real-time inference speed\nintact.",
          "link": "http://arxiv.org/abs/2108.00205",
          "publishedOn": "2021-08-03T02:06:29.360Z",
          "wordCount": 639,
          "title": "Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding. (arXiv:2108.00205v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hannun_A/0/1/0/all/0/1\">Awni Hannun</a>",
          "description": "The decade from 2010 to 2020 saw remarkable improvements in automatic speech\nrecognition. Many people now use speech recognition on a daily basis, for\nexample to perform voice search queries, send text messages, and interact with\nvoice assistants like Amazon Alexa and Siri by Apple. Before 2010 most people\nrarely used speech recognition. Given the remarkable changes in the state of\nspeech recognition over the previous decade, what can we expect over the coming\ndecade? I attempt to forecast the state of speech recognition research and\napplications by the year 2030. While the changes to general speech recognition\naccuracy will not be as dramatic as in the previous decade, I suggest we have\nan exciting decade of progress in speech technology ahead of us.",
          "link": "http://arxiv.org/abs/2108.00084",
          "publishedOn": "2021-08-03T02:06:29.238Z",
          "wordCount": 551,
          "title": "The History of Speech Recognition to the Year 2030. (arXiv:2108.00084v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morgan_S/0/1/0/all/0/1\">Skye Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>",
          "description": "This paper addresses the identification of toxic, engaging, and fact-claiming\ncomments on social media. We used the dataset made available by the organizers\nof the GermEval-2021 shared task containing over 3,000 manually annotated\nFacebook comments in German. Considering the relatedness of the three tasks, we\napproached the problem using large pre-trained transformer models and multitask\nlearning. Our results indicate that multitask learning achieves performance\nsuperior to the more common single task learning approach in all three tasks.\nWe submit our best systems to GermEval-2021 under the team name WLV-RIT.",
          "link": "http://arxiv.org/abs/2108.00057",
          "publishedOn": "2021-08-03T02:06:29.145Z",
          "wordCount": 553,
          "title": "WLV-RIT at GermEval 2021: Multitask Learning with Transformers to Detect Toxic, Engaging, and Fact-Claiming Comments. (arXiv:2108.00057v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2103.06523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaekeol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_E/0/1/0/all/0/1\">Euna Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suh_J/0/1/0/all/0/1\">Jangwon Suh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1\">Wonjong Rhee</a>",
          "description": "BERT-based Neural Ranking Models (NRMs) can be classified according to how\nthe query and document are encoded through BERT's self-attention layers -\nbi-encoder versus cross-encoder. Bi-encoder models are highly efficient because\nall the documents can be pre-processed before the query time, but their\nperformance is inferior compared to cross-encoder models. Both models utilize a\nranker that receives BERT representations as the input and generates a\nrelevance score as the output. In this work, we propose a method where\nmulti-teacher distillation is applied to a cross-encoder NRM and a bi-encoder\nNRM to produce a bi-encoder NRM with two rankers. The resulting student\nbi-encoder achieves an improved performance by simultaneously learning from a\ncross-encoder teacher and a bi-encoder teacher and also by combining relevance\nscores from the two rankers. We call this method TRMD (Two Rankers and\nMulti-teacher Distillation). In the experiments, TwinBERT and ColBERT are\nconsidered as baseline bi-encoders. When monoBERT is used as the cross-encoder\nteacher, together with either TwinBERT or ColBERT as the bi-encoder teacher,\nTRMD produces a student bi-encoder that performs better than the corresponding\nbaseline bi-encoder. For P@20, the maximum improvement was 11.4%, and the\naverage improvement was 6.8%. As an additional experiment, we considered\nproducing cross-encoder students with TRMD, and found that it could also\nimprove the cross-encoders.",
          "link": "http://arxiv.org/abs/2103.06523",
          "publishedOn": "2021-08-09T00:49:26.829Z",
          "wordCount": 688,
          "title": "Improving Bi-encoder Document Ranking Models with Two Rankers and Multi-teacher Distillation. (arXiv:2103.06523v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Knittel_J/0/1/0/all/0/1\">Johannes Knittel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_S/0/1/0/all/0/1\">Steffen Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yingcai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shixia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ertl_T/0/1/0/all/0/1\">Thomas Ertl</a>",
          "description": "Breaking news and first-hand reports often trend on social media platforms\nbefore traditional news outlets cover them. The real-time analysis of posts on\nsuch platforms can reveal valuable and timely insights for journalists,\npoliticians, business analysts, and first responders, but the high number and\ndiversity of new posts pose a challenge. In this work, we present an\ninteractive system that enables the visual analysis of streaming social media\ndata on a large scale in real-time. We propose an efficient and explainable\ndynamic clustering algorithm that powers a continuously updated visualization\nof the current thematic landscape as well as detailed visual summaries of\nspecific topics of interest. Our parallel clustering strategy provides an\nadaptive stream with a digestible but diverse selection of recent posts related\nto relevant topics. We also integrate familiar visual metaphors that are highly\ninterlinked for enabling both explorative and more focused monitoring tasks.\nAnalysts can gradually increase the resolution to dive deeper into particular\ntopics. In contrast to previous work, our system also works with non-geolocated\nposts and avoids extensive preprocessing such as detecting events. We evaluated\nour dynamic clustering algorithm and discuss several use cases that show the\nutility of our system.",
          "link": "http://arxiv.org/abs/2108.03052",
          "publishedOn": "2021-08-09T00:49:26.725Z",
          "wordCount": 650,
          "title": "Real-Time Visual Analysis of High-Volume Social Media Posts. (arXiv:2108.03052v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaochen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>",
          "description": "Hybrid recommendations have recently attracted a lot of attention where user\nfeatures are utilized as auxiliary information to address the sparsity problem\ncaused by insufficient user-item interactions. However, extracted user features\ngenerally contain rich multimodal information, and most of them are irrelevant\nto the recommendation purpose. Therefore, excessive reliance on these features\nwill make the model overfit on noise and difficult to generalize. In this\narticle, we propose a variational bandwidth auto-encoder (VBAE) for\nrecommendations, aiming to address the sparsity and noise problems\nsimultaneously. VBAE first encodes user collaborative and feature information\ninto Gaussian latent variables via deep neural networks to capture non-linear\nuser similarities. Moreover, by considering the fusion of collaborative and\nfeature variables as a virtual communication channel from an\ninformation-theoretic perspective, we introduce a user-dependent channel to\ndynamically control the information allowed to be accessed from the feature\nembeddings. A quantum-inspired uncertainty measurement of the hidden rating\nembeddings is proposed accordingly to infer the channel bandwidth by\ndisentangling the uncertainty information in the ratings from the semantic\ninformation. Through this mechanism, VBAE incorporates adequate auxiliary\ninformation from user features if collaborative information is insufficient,\nwhile avoiding excessive reliance on noisy user features to improve its\ngeneralization ability to new users. Extensive experiments conducted on three\nreal-world datasets demonstrate the effectiveness of the proposed method. Codes\nand datasets are released at https://github.com/yaochenzhu/vbae.",
          "link": "http://arxiv.org/abs/2105.07597",
          "publishedOn": "2021-08-09T00:49:26.713Z",
          "wordCount": 678,
          "title": "Variational Bandwidth Auto-encoder for Hybrid Recommender Systems. (arXiv:2105.07597v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>",
          "description": "Recommender system recommends interesting items to users based on users' past\ninformation history. Researchers have been paying attention to improvement of\nalgorithmic performance such as MAE and precision@K. Major techniques such as\nmatrix factorization and learning to rank are optimized based on such\nevaluation metrics. However, the intrinsic Matthew Effect problem poses great\nthreat to the fairness of the recommender system, and the unfairness problem\ncannot be resolved by optimization of traditional metrics. In this paper, we\npropose a novel algorithm that incorporates Matthew Effect reduction with the\nmatrix factorization framework. We demonstrate that our approach can boost the\nfairness of the algorithm and enhances performance evaluated by traditional\nmetrics.",
          "link": "http://arxiv.org/abs/2106.07347",
          "publishedOn": "2021-08-06T00:51:44.435Z",
          "wordCount": 566,
          "title": "Zipf Matrix Factorization : Matrix Factorization with Matthew Effect Reduction. (arXiv:2106.07347v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06416",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petrescu_D/0/1/0/all/0/1\">Diana Petrescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>",
          "description": "Recommendations with personalized explanations have been shown to increase\nuser trust and perceived quality and help users make better decisions.\nMoreover, such explanations allow users to provide feedback by critiquing them.\nSeveral algorithms for recommender systems with multi-step critiquing have\ntherefore been developed. However, providing a user-friendly interface based on\npersonalized explanations and critiquing has not been addressed in the last\ndecade. In this paper, we introduce four different web interfaces (available\nunder https://lia.epfl.ch/critiquing/) helping users making decisions and\nfinding their ideal item. We have chosen the hotel recommendation domain as a\nuse case even though our approach is trivially adaptable for other domains.\nMoreover, our system is model-agnostic (for both recommender systems and\ncritiquing models) allowing a great flexibility and further extensions. Our\ninterfaces are above all a useful tool to help research in recommendation with\ncritiquing. They allow to test such systems on a real use case and also to\nhighlight some limitations of these approaches to find solutions to overcome\nthem.",
          "link": "http://arxiv.org/abs/2107.06416",
          "publishedOn": "2021-08-06T00:51:44.129Z",
          "wordCount": 629,
          "title": "Multi-Step Critiquing User Interface for Recommender Systems. (arXiv:2107.06416v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rafee_S/0/1/0/all/0/1\">Syed Rifat Mahmud Rafee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gyorgy Fazekas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiggins_G/0/1/0/all/0/1\">Geraint A.~Wiggins</a>",
          "description": "Music Performers have their own idiosyncratic way of interpreting a musical\npiece. A group of skilled performers playing the same piece of music would\nlikely to inject their unique artistic styles in their performances. The\nvariations of the tempo, timing, dynamics, articulation etc. from the actual\nnotated music are what make the performers unique in their performances. This\nstudy presents a dataset consisting of four movements of Schubert's ``Sonata in\nB-flat major, D.960\" performed by nine virtuoso pianists individually. We\nproposed and extracted a set of expressive features that are able to capture\nthe characteristics of an individual performer's style. We then present a\nperformer identification method based on the similarity of feature\ndistribution, given a set of piano performances. The identification is done\nconsidering each feature individually as well as a fusion of the features.\nResults show that the proposed method achieved a precision of 0.903 using\nfusion features. Moreover, the onset time deviation feature shows promising\nresult when considered individually.",
          "link": "http://arxiv.org/abs/2108.02576",
          "publishedOn": "2021-08-06T00:51:44.089Z",
          "wordCount": 610,
          "title": "Performer Identification From Symbolic Representation of Music Using Statistical Models. (arXiv:2108.02576v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02634",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuyue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yashen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haiyong Xie</a>",
          "description": "Reasoning on knowledge graph (KG) has been studied for explainable\nrecommendation due to it's ability of providing explicit explanations. However,\ncurrent KG-based explainable recommendation methods unfortunately ignore the\ntemporal information (such as purchase time, recommend time, etc.), which may\nresult in unsuitable explanations. In this work, we propose a novel Time-aware\nPath reasoning for Recommendation (TPRec for short) method, which leverages the\npotential of temporal information to offer better recommendation with plausible\nexplanations. First, we present an efficient time-aware interaction relation\nextraction component to construct collaborative knowledge graph with time-aware\ninteractions (TCKG for short), and then introduce a novel time-aware path\nreasoning method for recommendation. We conduct extensive experiments on three\nreal-world datasets. The results demonstrate that the proposed TPRec could\nsuccessfully employ TCKG to achieve substantial gains and improve the quality\nof explainable recommendation.",
          "link": "http://arxiv.org/abs/2108.02634",
          "publishedOn": "2021-08-06T00:51:44.076Z",
          "wordCount": 585,
          "title": "Time-aware Path Reasoning on Knowledge Graph for Recommendation. (arXiv:2108.02634v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demirel_E/0/1/0/all/0/1\">Emir Demirel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahlback_S/0/1/0/all/0/1\">Sven Ahlb&#xe4;ck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1\">Simon Dixon</a>",
          "description": "This paper makes several contributions to automatic lyrics transcription\n(ALT) research. Our main contribution is a novel variant of the Multistreaming\nTime-Delay Neural Network (MTDNN) architecture, called MSTRE-Net, which\nprocesses the temporal information using multiple streams in parallel with\nvarying resolutions keeping the network more compact, and thus with a faster\ninference and an improved recognition rate than having identical TDNN streams.\nIn addition, two novel preprocessing steps prior to training the acoustic model\nare proposed. First, we suggest using recordings from both monophonic and\npolyphonic domains during training the acoustic model. Second, we tag\nmonophonic and polyphonic recordings with distinct labels for discriminating\nnon-vocal silence and music instances during alignment. Moreover, we present a\nnew test set with a considerably larger size and a higher musical variability\ncompared to the existing datasets used in ALT literature, while maintaining the\ngender balance of the singers. Our best performing model sets the\nstate-of-the-art in lyrics transcription by a large margin. For\nreproducibility, we publicly share the identifiers to retrieve the data used in\nthis paper.",
          "link": "http://arxiv.org/abs/2108.02625",
          "publishedOn": "2021-08-06T00:51:44.047Z",
          "wordCount": 615,
          "title": "MSTRE-Net: Multistreaming Acoustic Modeling for Automatic Lyrics Transcription. (arXiv:2108.02625v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_D/0/1/0/all/0/1\">Detao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chuanfei Xu</a>",
          "description": "Matching items for a user from a travel item pool of large cardinality have\nbeen the most important technology for increasing the business at Fliggy, one\nof the most popular online travel platforms (OTPs) in China. There are three\nmajor challenges facing OTPs: sparsity, diversity, and implicitness. In this\npaper, we present a novel Fliggy ITinerary-aware deep matching NETwork (FitNET)\nto address these three challenges. FitNET is designed based on the popular deep\nmatching network, which has been successfully employed in many industrial\nrecommendation systems, due to its effectiveness. The concept itinerary is\nfirstly proposed under the context of recommendation systems for OTPs, which is\ndefined as the list of unconsumed orders of a user. All orders in a user\nitinerary are learned as a whole, based on which the implicit travel intention\nof each user can be more accurately inferred. To alleviate the sparsity\nproblem, users' profiles are incorporated into FitNET. Meanwhile, a series of\nitinerary-aware attention mechanisms that capture the vital interactions\nbetween user's itinerary and other input categories are carefully designed.\nThese mechanisms are very helpful in inferring a user's travel intention or\npreference, and handling the diversity in a user's need. Further, two training\nobjectives, i.e., prediction accuracy of user's travel intention and prediction\naccuracy of user's click behavior, are utilized by FitNET, so that these two\nobjectives can be optimized simultaneously. An offline experiment on Fliggy\nproduction dataset with over 0.27 million users and 1.55 million travel items,\nand an online A/B test both show that FitNET effectively learns users' travel\nintentions, preferences, and diverse needs, based on their itineraries and\ngains superior performance compared with state-of-the-art methods. FitNET now\nhas been successfully deployed at Fliggy, serving major online traffic.",
          "link": "http://arxiv.org/abs/2108.02343",
          "publishedOn": "2021-08-06T00:51:44.001Z",
          "wordCount": 719,
          "title": "Itinerary-aware Personalized Deep Matching at Fliggy. (arXiv:2108.02343v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wendong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhi Jiang</a>",
          "description": "Most current recommender systems used the historical behaviour data of user\nto predict user' preference. However, it is difficult to recommend items to new\nusers accurately. To alleviate this problem, existing user cold start methods\neither apply deep learning to build a cross-domain recommender system or map\nuser attributes into the space of user behaviour. These methods are more\nchallenging when applied to online travel platform (e.g., Fliggy), because it\nis hard to find a cross-domain that user has similar behaviour with travel\nscenarios and the Location Based Services (LBS) information of users have not\nbeen paid sufficient attention. In this work, we propose a LBS-based\nHeterogeneous Relations Model (LHRM) for user cold start recommendation, which\nutilizes user's LBS information and behaviour information in related domains\nand user's behaviour information in travel platforms (e.g., Fliggy) to\nconstruct the heterogeneous relations between users and items. Moreover, an\nattention-based multi-layer perceptron is applied to extract latent factors of\nusers and items. Through this way, LHRM has better generalization performance\nthan existing methods. Experimental results on real data from Fliggy's offline\nlog illustrate the effectiveness of LHRM.",
          "link": "http://arxiv.org/abs/2108.02344",
          "publishedOn": "2021-08-06T00:51:43.933Z",
          "wordCount": 633,
          "title": "LHRM: A LBS based Heterogeneous Relations Model for User Cold Start Recommendation in Online Travel Platform. (arXiv:2108.02344v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>",
          "description": "Assessing an AI agent that can converse in human language and understand\nvisual content is challenging. Generation metrics, such as BLEU scores favor\ncorrect syntax over semantics. Hence a discriminative approach is often used,\nwhere an agent ranks a set of candidate options. The mean reciprocal rank (MRR)\nmetric evaluates the model performance by taking into account the rank of a\nsingle human-derived answer. This approach, however, raises a new challenge:\nthe ambiguity and synonymy of answers, for instance, semantic equivalence\n(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative\ngain (NDCG) metric has been used to capture the relevance of all the correct\nanswers via dense annotations. However, the NDCG metric favors the usually\napplicable uncertain answers such as `I don't know. Crafting a model that\nexcels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should\nanswer a human-like reply and validate the correctness of any answer. To\naddress this issue, we describe a two-step non-parametric ranking approach that\ncan merge strong MRR and NDCG models. Using our approach, we manage to keep\nmost MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG\nstate-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won\nthe recent Visual Dialog 2020 challenge. Source code is available at\nhttps://github.com/idansc/mrr-ndcg.",
          "link": "http://arxiv.org/abs/2104.07511",
          "publishedOn": "2021-08-05T01:56:19.592Z",
          "wordCount": 697,
          "title": "Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Desai_A/0/1/0/all/0/1\">Aditya Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_L/0/1/0/all/0/1\">Li Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Anshumali Shrivastava</a>",
          "description": "Deep learning for recommendation data is the one of the most pervasive and\nchallenging AI workload in recent times. State-of-the-art recommendation models\nare one of the largest models rivalling the likes of GPT-3 and Switch\nTransformer. Challenges in deep learning recommendation models (DLRM) stem from\nlearning dense embeddings for each of the categorical values. These embedding\ntables in industrial scale models can be as large as hundreds of terabytes.\nSuch large models lead to a plethora of engineering challenges, not to mention\nprohibitive communication overheads, and slower training and inference times.\nOf these, slower inference time directly impacts user experience. Model\ncompression for DLRM is gaining traction and the community has recently shown\nimpressive compression results. In this paper, we present Random Offset Block\nEmbedding Array (ROBE) as a low memory alternative to embedding tables which\nprovide orders of magnitude reduction in memory usage while maintaining\naccuracy and boosting execution speed. ROBE is a simple fundamental approach in\nimproving both cache performance and the variance of randomized hashing, which\ncould be of independent interest in itself. We demonstrate that we can\nsuccessfully train DLRM models with same accuracy while using $1000 \\times$\nless memory. A $1000\\times$ compressed model directly results in faster\ninference without any engineering. In particular, we show that we can train\nDLRM model using ROBE Array of size 100MB on a single GPU to achieve AUC of\n0.8025 or higher as required by official MLPerf CriteoTB benchmark DLRM model\nof 100GB while achieving about $2.7\\times$ (170\\%) improvement in inference\nthroughput.",
          "link": "http://arxiv.org/abs/2108.02191",
          "publishedOn": "2021-08-05T01:56:19.246Z",
          "wordCount": 711,
          "title": "Random Offset Block Embedding Array (ROBE) for CriteoTB Benchmark MLPerf DLRM Model : 1000$\\times$ Compression and 2.7$\\times$ Faster Inference. (arXiv:2108.02191v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2009.08978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Filipovic_M/0/1/0/all/0/1\">Milena Filipovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitrevski_B/0/1/0/all/0/1\">Blagoj Mitrevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1\">Emma Lejal Glaude</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>",
          "description": "Recommender systems research tends to evaluate model performance offline and\non randomly sampled targets, yet the same systems are later used to predict\nuser behavior sequentially from a fixed point in time. Simulating online\nrecommender system performance is notoriously difficult and the discrepancy\nbetween online and offline behaviors is typically not accounted for in offline\nevaluations. This disparity permits weaknesses to go unnoticed until the model\nis deployed in a production setting. In this paper, we first demonstrate how\nomitting temporal context when evaluating recommender system performance leads\nto false confidence. To overcome this, we postulate that offline evaluation\nprotocols can only model real-life use-cases if they account for temporal\ncontext. Next, we propose a training procedure to further embed the temporal\ncontext in existing models: we introduce it in a multi-objective approach to\ntraditionally time-unaware recommender systems and confirm its advantage via\nthe proposed evaluation protocol. Finally, we validate that the Pareto Fronts\nobtained with the added objective dominate those produced by state-of-the-art\nmodels that are only optimized for accuracy on three real-world publicly\navailable datasets. The results show that including our temporal objective can\nimprove recall@20 by up to 20%.",
          "link": "http://arxiv.org/abs/2009.08978",
          "publishedOn": "2021-08-05T01:56:19.213Z",
          "wordCount": 683,
          "title": "Modeling Online Behavior in Recommender Systems: The Importance of Temporal Context. (arXiv:2009.08978v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1\">Leonard Adolphs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1\">Shehzaad Dhuliawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>",
          "description": "Large pre-trained language models (LMs) are capable of not only recovering\nlinguistic but also factual and commonsense knowledge. To access the knowledge\nstored in mask-based LMs, we can use cloze-style questions and let the model\nfill in the blank. The flexibility advantage over structured knowledge bases\ncomes with the drawback of finding the right query for a certain information\nneed. Inspired by human behavior to disambiguate a question, we propose to\nquery LMs by example. To clarify the ambivalent question \"Who does Neuer play\nfor?\", a successful strategy is to demonstrate the relation using another\nsubject, e.g., \"Ronaldo plays for Portugal. Who does Neuer play for?\". We apply\nthis approach of querying by example to the LAMA probe and obtain substantial\nimprovements of up to 37.8% for BERT-large on the T-REx data when providing\nonly 10 demonstrations--even outperforming a baseline that queries the model\nwith up to 40 paraphrases of the question. The examples are provided through\nthe model's context and thus require neither fine-tuning nor an additional\nforward pass. This suggests that LMs contain more factual and commonsense\nknowledge than previously assumed--if we query the model in the right way.",
          "link": "http://arxiv.org/abs/2108.01928",
          "publishedOn": "2021-08-05T01:56:19.194Z",
          "wordCount": 622,
          "title": "How to Query Language Models?. (arXiv:2108.01928v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milenkoski_M/0/1/0/all/0/1\">Martin Milenkoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>",
          "description": "In this paper, we describe a method to tackle data sparsity and create\nrecommendations in domains with limited knowledge about user preferences. We\nexpand the variational autoencoder collaborative filtering from a single-domain\nto a multi-domain setting. The intuition is that user-item interactions in a\nsource domain can augment the recommendation quality in a target domain. The\nintuition can be taken to its extreme, where, in a cross-domain setup, the user\nhistory in a source domain is enough to generate high-quality recommendations\nin a target one. We thus create a Product-of-Experts (POE) architecture for\nrecommendations that jointly models user-item interactions across multiple\ndomains. The method is resilient to missing data for one or more of the\ndomains, which is a situation often found in real life. We present results on\ntwo widely-used datasets - Amazon and Yelp, which support the claim that\nholistic user preference knowledge leads to better recommendations.\nSurprisingly, we find that in some cases, a POE recommender that does not\naccess the target domain user representation can surpass a strong VAE\nrecommender baseline trained on the target domain.",
          "link": "http://arxiv.org/abs/2104.12822",
          "publishedOn": "2021-08-05T01:56:19.168Z",
          "wordCount": 660,
          "title": "Recommending Burgers based on Pizza Preferences: Addressing Data Sparsity with a Product of Experts. (arXiv:2104.12822v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02138",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reiter_Haas_M/0/1/0/all/0/1\">Markus Reiter-Haas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parada_Cabaleiro_E/0/1/0/all/0/1\">Emilia Parada-Cabaleiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1\">Markus Schedl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motamedi_E/0/1/0/all/0/1\">Elham Motamedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tkalcic_M/0/1/0/all/0/1\">Marko Tkalcic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lex_E/0/1/0/all/0/1\">Elisabeth Lex</a>",
          "description": "Providing suitable recommendations is of vital importance to improve the user\nsatisfaction of music recommender systems. Here, users often listen to the same\ntrack repeatedly and appreciate recommendations of the same song multiple\ntimes. Thus, accounting for users' relistening behavior is critical for music\nrecommender systems. In this paper, we describe a psychology-informed approach\nto model and predict music relistening behavior that is inspired by studies in\nmusic psychology, which relate music preferences to human memory. We adopt a\nwell-established psychological theory of human cognition that models the\noperations of human memory, i.e., Adaptive Control of Thought-Rational (ACT-R).\nIn contrast to prior work, which uses only the base-level component of ACT-R,\nwe utilize five components of ACT-R, i.e., base-level, spreading, partial\nmatching, valuation, and noise, to investigate the effect of five factors on\nmusic relistening behavior: (i) recency and frequency of prior exposure to\ntracks, (ii) co-occurrence of tracks, (iii) the similarity between tracks, (iv)\nfamiliarity with tracks, and (v) randomness in behavior. On a dataset of 1.7\nmillion listening events from Last.fm, we evaluate the performance of our\napproach by sequentially predicting the next track(s) in user sessions. We find\nthat recency and frequency of prior exposure to tracks is an effective\npredictor of relistening behavior. Besides, considering the co-occurrence of\ntracks and familiarity with tracks further improves performance in terms of\nR-precision. We hope that our work inspires future research on the merits of\nconsidering cognitive aspects of memory retrieval to model and predict complex\nuser behavior.",
          "link": "http://arxiv.org/abs/2108.02138",
          "publishedOn": "2021-08-05T01:56:19.159Z",
          "wordCount": 693,
          "title": "Predicting Music Relistening Behavior Using the ACT-R Framework. (arXiv:2108.02138v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carlton_J/0/1/0/all/0/1\">Jonathan Carlton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1\">Andy Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jay_C/0/1/0/all/0/1\">Caroline Jay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keane_J/0/1/0/all/0/1\">John Keane</a>",
          "description": "Media is evolving from traditional linear narratives to personalised\nexperiences, where control over information (or how it is presented) is given\nto individual audience members. Measuring and understanding audience engagement\nwith this media is important in at least two ways: (1) a post-hoc understanding\nof how engaged audiences are with the content will help production teams learn\nfrom experience and improve future productions; (2), this type of media has\npotential for real-time measures of engagement to be used to enhance the user\nexperience by adapting content on-the-fly. Engagement is typically measured by\nasking samples of users to self-report, which is time consuming and expensive.\nIn some domains, however, interaction data have been used to infer engagement.\nFortuitously, the nature of interactive media facilitates a much richer set of\ninteraction data than traditional media; our research aims to understand if\nthese data can be used to infer audience engagement. In this paper, we report a\nstudy using data captured from audience interactions with an interactive TV\nshow to model and predict engagement. We find that temporal metrics, including\noverall time spent on the experience and the interval between events, are\npredictive of engagement. The results demonstrate that interaction data can be\nused to infer users' engagement during and after an experience, and the\nproposed techniques are relevant to better understand audience preference and\nresponses.",
          "link": "http://arxiv.org/abs/2108.01949",
          "publishedOn": "2021-08-05T01:56:19.146Z",
          "wordCount": 685,
          "title": "Using Interaction Data to Predict Engagement with Interactive Media. (arXiv:2108.01949v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_B/0/1/0/all/0/1\">Bidyarthi Dutta</a>",
          "description": "This paper analysed author-assigned and title keywords into constituent words\ncollected from 769 articles published in the journal Low Temperature Physics\nsince the year 2006 to 2010. The total number of distinct keywords over the\nsaid time span has been found as 1155, which have been analyzed into 869\nnumbers of single words having total frequency of occurrence of 2287. The\nsingle words obtained from keywords have been categorized in four broad\nclasses, viz. eponymous word, form word, acronym and semantic word. A semantic\nword bears several contexts and thus may be considered as relevant in several\nother subject areas. These probable relevant subject areas have been found with\nthe aid of two popular online reference tools. The semantic words are further\ncategorized in twelve classes according to their contexts. Some parameters have\nbeen defined on the basis of associations among the words and formation of\nkeywords in consequence, i.e. Word Association Density, Word Association\nCoefficient and Keyword Formation Density. The values of these parameters have\nbeen observed for different word categories. The statistics of word association\ntending keyword formation would be known from this study. The allied subject\ndomains also become predictable from this study.",
          "link": "http://arxiv.org/abs/2108.01915",
          "publishedOn": "2021-08-05T01:56:19.116Z",
          "wordCount": 637,
          "title": "An analytical study of content and contexts of keywords on physics. (arXiv:2108.01915v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Springstein_M/0/1/0/all/0/1\">Matthias Springstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1\">Stefanie Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahnama_J/0/1/0/all/0/1\">Javad Rahnama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohle_H/0/1/0/all/0/1\">Hubertus Kohle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>",
          "description": "In this paper, we introduce iART: an open Web platform for art-historical\nresearch that facilitates the process of comparative vision. The system\nintegrates various machine learning techniques for keyword- and content-based\nimage retrieval as well as category formation via clustering. An intuitive GUI\nsupports users to define queries and explore results. By using a\nstate-of-the-art cross-modal deep learning approach, it is possible to search\nfor concepts that were not previously detected by trained classification\nmodels. Art-historical objects from large, openly licensed collections such as\nAmsterdam Rijksmuseum and Wikidata are made available to users.",
          "link": "http://arxiv.org/abs/2108.01542",
          "publishedOn": "2021-08-04T01:59:20.070Z",
          "wordCount": 537,
          "title": "iART: A Search Engine for Art-Historical Images to Support Research in the Humanities. (arXiv:2108.01542v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gabbolini_G/0/1/0/all/0/1\">Giovanni Gabbolini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bridge_D/0/1/0/all/0/1\">Derek Bridge</a>",
          "description": "We introduce a novel and interpretable path-based music similarity measure.\nOur similarity measure assumes that items, such as songs and artists, and\ninformation about those items are represented in a knowledge graph. We find\npaths in the graph between a seed and a target item; we score those paths based\non their interestingness; and we aggregate those scores to determine the\nsimilarity between the seed and the target. A distinguishing feature of our\nsimilarity measure is its interpretability. In particular, we can translate the\nmost interesting paths into natural language, so that the causes of the\nsimilarity judgements can be readily understood by humans. We compare the\naccuracy of our similarity measure with other competitive path-based similarity\nbaselines in two experimental settings and with four datasets. %\\sout{The\nresults show that our measure has highest accuracy in general.} The results\nhighlight the validity of our approach to music similarity, and demonstrate\nthat path interestingness scores can be the basis of an accurate and\ninterpretable similarity measure.",
          "link": "http://arxiv.org/abs/2108.01632",
          "publishedOn": "2021-08-04T01:59:20.053Z",
          "wordCount": 593,
          "title": "An Interpretable Music Similarity Measure Based on Path Interestingness. (arXiv:2108.01632v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Veeramachaneni_S/0/1/0/all/0/1\">Sowmini Devi Veeramachaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujari_A/0/1/0/all/0/1\">Arun K Pujari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmanabhan_V/0/1/0/all/0/1\">Vineet Padmanabhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vikas Kumar</a>",
          "description": "Recommender systems(RS), especially collaborative filtering(CF) based RS, has\nbeen playing an important role in many e-commerce applications. As the\ninformation being searched over the internet is rapidly increasing, users often\nface the difficulty of finding items of his/her own interest and RS often\nprovides help in such tasks. Recent studies show that, as the item space\nincreases, and the number of items rated by the users become very less, issues\nlike sparsity arise. To mitigate the sparsity problem, transfer learning\ntechniques are being used wherein the data from dense domain(source) is\nconsidered in order to predict the missing entries in the sparse\ndomain(target). In this paper, we propose a transfer learning approach for\ncross-domain recommendation when both domains have no overlap of users and\nitems. In our approach the transferring of knowledge from source to target\ndomain is done in a novel way. We make use of co-clustering technique to obtain\nthe codebook (cluster-level rating pattern) of source domain. By making use of\nhinge loss function we transfer the learnt codebook of the source domain to\ntarget. The use of hinge loss as a loss function is novel and has not been\ntried before in transfer learning. We demonstrate that our technique improves\nthe approximation of the target matrix on benchmark datasets.",
          "link": "http://arxiv.org/abs/2108.01473",
          "publishedOn": "2021-08-04T01:59:20.045Z",
          "wordCount": 654,
          "title": "A Hinge-Loss based Codebook Transfer for Cross-Domain Recommendation with Nonoverlapping Data. (arXiv:2108.01473v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tingtian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zixun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoruo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Hui Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yipeng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hengcan Shi</a>",
          "description": "Recently, the witness of the rapidly growing popularity of short videos on\ndifferent Internet platforms has intensified the need for a background music\n(BGM) retrieval system. However, existing video-music retrieval methods only\nbased on the visual modality cannot show promising performance regarding videos\nwith fine-grained virtual contents. In this paper, we also investigate the\nwidely added voice-overs in short videos and propose a novel framework to\nretrieve BGM for fine-grained short videos. In our framework, we use the\nself-attention (SA) and the cross-modal attention (CMA) modules to explore the\nintra- and the inter-relationships of different modalities respectively. For\nbalancing the modalities, we dynamically assign different weights to the modal\nfeatures via a fusion gate. For paring the query and the BGM embeddings, we\nintroduce a triplet pseudo-label loss to constrain the semantics of the modal\nembeddings. As there are no existing virtual-content video-BGM retrieval\ndatasets, we build and release two virtual-content video datasets HoK400 and\nCFM400. Experimental results show that our method achieves superior performance\nand outperforms other state-of-the-art methods with large margins.",
          "link": "http://arxiv.org/abs/2104.10557",
          "publishedOn": "2021-08-04T01:59:20.028Z",
          "wordCount": 653,
          "title": "Deep Music Retrieval for Fine-Grained Videos by Exploiting Cross-Modal-Encoded Voice-Overs. (arXiv:2104.10557v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_D/0/1/0/all/0/1\">Ding Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_B/0/1/0/all/0/1\">Becky West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiquan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jinzhou Huang</a>",
          "description": "E-commerce sites strive to provide users the most timely relevant information\nin order to reduce shopping frictions and increase customer satisfaction. Multi\narmed bandit models (MAB) as a type of adaptive optimization algorithms provide\npossible approaches for such purposes. In this paper, we analyze using three\nclassic MAB algorithms, epsilon-greedy, Thompson sampling (TS), and upper\nconfidence bound 1 (UCB1) for dynamic content recommendations, and walk through\nthe process of developing these algorithms internally to solve a real world\ne-commerce use case. First, we analyze the three MAB algorithms using simulated\npurchasing datasets with non-stationary reward distributions to simulate the\npossible time-varying customer preferences, where the traffic allocation\ndynamics and the accumulative rewards of different algorithms are studied.\nSecond, we compare the accumulative rewards of the three MAB algorithms with\nmore than 1,000 trials using actual historical A/B test datasets. We find that\nthe larger difference between the success rates of competing recommendations\nthe more accumulative rewards the MAB algorithms can achieve. In addition, we\nfind that TS shows the highest average accumulative rewards under different\ntesting scenarios. Third, we develop a batch-updated MAB algorithm to overcome\nthe delayed reward issue in e-commerce and enable an online content\noptimization on our App homepage. For a state-of-the-art comparison, a real A/B\ntest among our batch-updated MAB algorithm, a third-party MAB solution, and the\ndefault business logic are conducted. The result shows that our batch-updated\nMAB algorithm outperforms the counterparts and achieves 6.13% relative\nclick-through rate (CTR) increase and 16.1% relative conversion rate (CVR)\nincrease compared to the default experience, and 2.9% relative CTR increase and\n1.4% relative CVR increase compared to the external MAB service.",
          "link": "http://arxiv.org/abs/2108.01440",
          "publishedOn": "2021-08-04T01:59:19.925Z",
          "wordCount": 719,
          "title": "Adaptively Optimize Content Recommendation Using Multi Armed Bandit Algorithms in E-commerce. (arXiv:2108.01440v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01532",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Liu_C/0/1/0/all/0/1\">Chuang Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yu_S/0/1/0/all/0/1\">Shimin Yu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Huang_Y/0/1/0/all/0/1\">Ying Huang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_Z/0/1/0/all/0/1\">Zi-Ke Zhang</a>",
          "description": "Link and sign prediction in complex networks bring great help to\ndecision-making and recommender systems, such as in predicting potential\nrelationships or relative status levels. Many previous studies focused on\ndesigning the special algorithms to perform either link prediction or sign\nprediction. In this work, we propose an effective model integration algorithm\nconsisting of network embedding, network feature engineering, and an integrated\nclassifier, which can perform the link and sign prediction in the same\nframework. Network embedding can accurately represent the characteristics of\ntopological structures and cooperate with the powerful network feature\nengineering and integrated classifier can achieve better prediction.\nExperiments on several datasets show that the proposed model can achieve\nstate-of-the-art or competitive performance for both link and sign prediction\nin spite of its generality. Interestingly, we find that using only very low\nnetwork embedding dimension can generate high prediction performance, which can\nsignificantly reduce the computational overhead during training and prediction.\nThis study offers a powerful methodology for multi-task prediction in complex\nnetworks.",
          "link": "http://arxiv.org/abs/2108.01532",
          "publishedOn": "2021-08-04T01:59:19.917Z",
          "wordCount": 618,
          "title": "Effective Model Integration Algorithm for Improving Link and Sign Prediction in Complex Networks. (arXiv:2108.01532v1 [physics.soc-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01454",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weichselbraun_A/0/1/0/all/0/1\">Albert Weichselbraun</a>",
          "description": "Inscriptis provides a library, command line client and Web service for\nconverting HTML to plain text. Its development has been triggered by the need\nto obtain accurate text representations for knowledge extraction tasks that\npreserve the spatial alignment of text without drawing upon heavyweight,\nbrowser-based solutions such as Selenium. In contrast to existing software\npackages such as HTML2text, jusText and Lynx, Inscriptis (i) provides a\nlayout-aware conversion of HTML that more closely resembles the rendering\nobtained from standard Web browsers and, therefore, better preserves the\nspatial arrangement of text elements. Inscriptis excels in terms of conversion\nquality, since it correctly converts complex HTML constructs such as nested\ntables and also interprets a subset of HTML attributes that determine the text\nalignment. In addition, it (ii) supports annotation rules, i.e., user-provided\nmappings that allow for annotating the extracted text based on structural and\nsemantic information encoded in HTML tags and attributes used for controlling\nstructure and layout in the original HTML document. These unique features\nensure that downstream knowledge extraction components can operate on accurate\ntext representations, and may even use information on the semantics and\nstructure of the original HTML document, if annotation support has been\nenabled.",
          "link": "http://arxiv.org/abs/2108.01454",
          "publishedOn": "2021-08-04T01:59:19.908Z",
          "wordCount": 640,
          "title": "Inscriptis -- A Python-based HTML to text conversion library optimized for knowledge extraction from the Web. (arXiv:2108.01454v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lechiakh_M/0/1/0/all/0/1\">Mohamed Lechiakh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurer_A/0/1/0/all/0/1\">Alexandre Maurer</a>",
          "description": "So far, most research on recommender systems focused on maintaining long-term\nuser engagement and satisfaction, by promoting relevant and personalized\ncontent. However, it is still very challenging to evaluate the quality and the\nreliability of this content. In this paper, we propose FEBR (Expert-Based\nRecommendation Framework), an apprenticeship learning framework to assess the\nquality of the recommended content on online platforms. The framework exploits\nthe demonstrated trajectories of an expert (assumed to be reliable) in a\nrecommendation evaluation environment, to recover an unknown utility function.\nThis function is used to learn an optimal policy describing the expert's\nbehavior, which is then used in the framework to provide high-quality and\npersonalized recommendations. We evaluate the performance of our solution\nthrough a user interest simulation environment (using RecSim). We simulate\ninteractions under the aforementioned expert policy for videos recommendation,\nand compare its efficiency with standard recommendation methods. The results\nshow that our approach provides a significant gain in terms of content quality,\nevaluated by experts and watched by users, while maintaining almost the same\nwatch time as the baseline approaches.",
          "link": "http://arxiv.org/abs/2108.01455",
          "publishedOn": "2021-08-04T01:59:19.773Z",
          "wordCount": 614,
          "title": "FEBR: Expert-Based Recommendation Framework for beneficial and personalized content. (arXiv:2108.01455v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antaris_S/0/1/0/all/0/1\">Stefanos Antaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafailidis_D/0/1/0/all/0/1\">Dimitrios Rafailidis</a>",
          "description": "Accounting for the fact that users have different sequential patterns, the\nmain drawback of state-of-the-art recommendation strategies is that a fixed\nsequence length of user-item interactions is required as input to train the\nmodels. This might limit the recommendation accuracy, as in practice users\nfollow different trends on the sequential recommendations. Hence, baseline\nstrategies might ignore important sequential interactions or add noise to the\nmodels with redundant interactions, depending on the variety of users'\nsequential behaviours. To overcome this problem, in this study we propose the\nSAR model, which not only learns the sequential patterns but also adjusts the\nsequence length of user-item interactions in a personalized manner. We first\ndesign an actor-critic framework, where the RL agent tries to compute the\noptimal sequence length as an action, given the user's state representation at\na certain time step. In addition, we optimize a joint loss function to align\nthe accuracy of the sequential recommendations with the expected cumulative\nrewards of the critic network, while at the same time we adapt the sequence\nlength with the actor network in a personalized manner. Our experimental\nevaluation on four real-world datasets demonstrates the superiority of our\nproposed model over several baseline approaches. Finally, we make our\nimplementation publicly available at https://github.com/stefanosantaris/sar.",
          "link": "http://arxiv.org/abs/2108.01442",
          "publishedOn": "2021-08-04T01:59:19.622Z",
          "wordCount": 643,
          "title": "Sequence Adaptation via Reinforcement Learning in Recommender Systems. (arXiv:2108.01442v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01441",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1\">Quanye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianying Lin</a>",
          "description": "Manifold ranking has been successfully applied in query-oriented\nmulti-document summarization. It not only makes use of the relationships among\nthe sentences, but also the relationships between the given query and the\nsentences. However, the information of original query is often insufficient. So\nwe present a query expansion method, which is combined in the manifold ranking\nto resolve this problem. Our method not only utilizes the information of the\nquery term itself and the knowledge base WordNet to expand it by synonyms, but\nalso uses the information of the document set itself to expand the query in\nvarious ways (mean expansion, variance expansion and TextRank expansion).\nCompared with the previous query expansion methods, our method combines\nmultiple query expansion methods to better represent query information, and at\nthe same time, it makes a useful attempt on manifold ranking. In addition, we\nuse the degree of word overlap and the proximity between words to calculate the\nsimilarity between sentences. We performed experiments on the datasets of DUC\n2006 and DUC2007, and the evaluation results show that the proposed query\nexpansion method can significantly improve the system performance and make our\nsystem comparable to the state-of-the-art systems.",
          "link": "http://arxiv.org/abs/2108.01441",
          "publishedOn": "2021-08-04T01:59:19.612Z",
          "wordCount": 643,
          "title": "Using Query Expansion in Manifold Ranking for Query-Oriented Multi-Document Summarization. (arXiv:2108.01441v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1\">Xiaoxue Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lijuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Maria Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jindong Chen</a>",
          "description": "We present a new human-human dialogue dataset - PhotoChat, the first dataset\nthat casts light on the photo sharing behavior in onlin emessaging. PhotoChat\ncontains 12k dialogues, each of which is paired with a user photo that is\nshared during the conversation. Based on this dataset, we propose two tasks to\nfacilitate research on image-text modeling: a photo-sharing intent prediction\ntask that predicts whether one intends to share a photo in the next\nconversation turn, and a photo retrieval task that retrieves the most relevant\nphoto according to the dialogue context. In addition, for both tasks, we\nprovide baseline models using the state-of-the-art models and report their\nbenchmark performances. The best image retrieval model achieves 10.4% recall@1\n(out of 1000 candidates) and the best photo intent prediction model achieves\n58.1% F1 score, indicating that the dataset presents interesting yet\nchallenging real-world problems. We are releasing PhotoChat to facilitate\nfuture research work among the community.",
          "link": "http://arxiv.org/abs/2108.01453",
          "publishedOn": "2021-08-04T01:59:19.597Z",
          "wordCount": 606,
          "title": "PhotoChat: A Human-Human Dialogue Dataset with Photo Sharing Behavior for Joint Image-Text Modeling. (arXiv:2108.01453v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Ziyi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaohui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiqun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuesong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shaoping Ma</a>",
          "description": "Reading comprehension is a complex cognitive process involving many human\nbrain activities. Plenty of works have studied the reading patterns and\nattention allocation mechanisms in the reading process. However, little is\nknown about what happens in human brain during reading comprehension and how we\ncan utilize this information as implicit feedback to facilitate information\nacquisition performance. With the advances in brain imaging techniques such as\nEEG, it is possible to collect high-precision brain signals in almost real\ntime. With neuroimaging techniques, we carefully design a lab-based user study\nto investigate brain activities during reading comprehension. Our findings show\nthat neural responses vary with different types of contents, i.e., contents\nthat can satisfy users' information needs and contents that cannot. We suggest\nthat various cognitive activities, e.g., cognitive loading, semantic-thematic\nunderstanding, and inferential processing, at the micro-time scale during\nreading comprehension underpin these neural responses. Inspired by these\ndetectable differences in cognitive activities, we construct supervised\nlearning models based on EEG features for two reading comprehension tasks:\nanswer sentence classification and answer extraction. Results show that it is\nfeasible to improve their performance with brain signals. These findings imply\nthat brain signals are valuable feedback for enhancing human-computer\ninteractions during reading comprehension.",
          "link": "http://arxiv.org/abs/2108.01360",
          "publishedOn": "2021-08-04T01:59:19.581Z",
          "wordCount": 645,
          "title": "Understanding Human Reading Comprehension with brain signals. (arXiv:2108.01360v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_F/0/1/0/all/0/1\">Fuyuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xing Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Huifeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruiming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiuqiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xue Liu</a>",
          "description": "Click-through rate prediction is one of the core tasks in commercial\nrecommender systems. It aims to predict the probability of a user clicking a\nparticular item given user and item features. As feature interactions bring in\nnon-linearity, they are widely adopted to improve the performance of CTR\nprediction models. Therefore, effectively modelling feature interactions has\nattracted much attention in both the research and industry field. The current\napproaches can generally be categorized into three classes: (1) na\\\"ive\nmethods, which do not model feature interactions and only use original\nfeatures; (2) memorized methods, which memorize feature interactions by\nexplicitly viewing them as new features and assigning trainable embeddings; (3)\nfactorized methods, which learn latent vectors for original features and\nimplicitly model feature interactions through factorization functions. Studies\nhave shown that modelling feature interactions by one of these methods alone\nare suboptimal due to the unique characteristics of different feature\ninteractions. To address this issue, we first propose a general framework\ncalled OptInter which finds the most suitable modelling method for each feature\ninteraction. Different state-of-the-art deep CTR models can be viewed as\ninstances of OptInter. To realize the functionality of OptInter, we also\nintroduce a learning algorithm that automatically searches for the optimal\nmodelling method. We conduct extensive experiments on four large datasets. Our\nexperiments show that OptInter improves the best performed state-of-the-art\nbaseline deep CTR models by up to 2.21%. Compared to the memorized method,\nwhich also outperforms baselines, we reduce up to 91% parameters. In addition,\nwe conduct several ablation studies to investigate the influence of different\ncomponents of OptInter. Finally, we provide interpretable discussions on the\nresults of OptInter.",
          "link": "http://arxiv.org/abs/2108.01265",
          "publishedOn": "2021-08-04T01:59:19.519Z",
          "wordCount": 725,
          "title": "Memorize, Factorize, or be Na\\\"ive: Learning Optimal Feature Interaction Methods for CTR Prediction. (arXiv:2108.01265v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Souvik Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sougata Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_R/0/1/0/all/0/1\">Rohini K. Srihari</a>",
          "description": "The Covid-19 pandemic has caused a spur in the medical research literature.\nWith new research advances in understanding the virus, there is a need for\nrobust text mining tools which can process, extract and present answers from\nthe literature in a concise and consumable way. With a DialoGPT based\nmulti-turn conversation generation module, and BM-25 \\& neural embeddings based\nensemble information retrieval module, in this paper we present a\nconversational system, which can retrieve and answer coronavirus-related\nqueries from the rich medical literature, and present it in a conversational\nsetting with the user. We further perform experiments to compare neural\nembedding-based document retrieval and the traditional BM25 retrieval algorithm\nand report the results.",
          "link": "http://arxiv.org/abs/2108.01436",
          "publishedOn": "2021-08-04T01:59:19.503Z",
          "wordCount": 599,
          "title": "Medical Literature Mining and Retrieval in a Conversational Setting. (arXiv:2108.01436v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soprano_M/0/1/0/all/0/1\">Michael Soprano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roitero_K/0/1/0/all/0/1\">Kevin Roitero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbera_D/0/1/0/all/0/1\">David La Barbera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceolin_D/0/1/0/all/0/1\">Davide Ceolin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spina_D/0/1/0/all/0/1\">Damiano Spina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mizzaro_S/0/1/0/all/0/1\">Stefano Mizzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demartini_G/0/1/0/all/0/1\">Gianluca Demartini</a>",
          "description": "Recent work has demonstrated the viability of using crowdsourcing as a tool\nfor evaluating the truthfulness of public statements. Under certain conditions\nsuch as: (1) having a balanced set of workers with different backgrounds and\ncognitive abilities; (2) using an adequate set of mechanisms to control the\nquality of the collected data; and (3) using a coarse grained assessment scale,\nthe crowd can provide reliable identification of fake news. However, fake news\nare a subtle matter: statements can be just biased (\"cherrypicked\"), imprecise,\nwrong, etc. and the unidimensional truth scale used in existing work cannot\naccount for such differences. In this paper we propose a multidimensional\nnotion of truthfulness and we ask the crowd workers to assess seven different\ndimensions of truthfulness selected based on existing literature: Correctness,\nNeutrality, Comprehensibility, Precision, Completeness, Speaker's\nTrustworthiness, and Informativeness. We deploy a set of quality control\nmechanisms to ensure that the thousands of assessments collected on 180\npublicly available fact-checked statements distributed over two datasets are of\nadequate quality, including a custom search engine used by the crowd workers to\nfind web pages supporting their truthfulness assessments. A comprehensive\nanalysis of crowdsourced judgments shows that: (1) the crowdsourced assessments\nare reliable when compared to an expert-provided gold standard; (2) the\nproposed dimensions of truthfulness capture independent pieces of information;\n(3) the crowdsourcing task can be easily learned by the workers; and (4) the\nresulting assessments provide a useful basis for a more complete estimation of\nstatement truthfulness.",
          "link": "http://arxiv.org/abs/2108.01222",
          "publishedOn": "2021-08-04T01:59:19.428Z",
          "wordCount": 718,
          "title": "The Many Dimensions of Truthfulness: Crowdsourcing Misinformation Assessments on a Multidimensional Scale. (arXiv:2108.01222v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pathak_M/0/1/0/all/0/1\">Manish Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aditya Jain</a>",
          "description": "Recommendation engines are integral to the modern e-commerce experience, both\nfor the seller and the end user. Accurate recommendations lead to higher\nrevenue and better user experience. In this paper, we are presenting our\nsolution to ECML PKDD Farfetch Fashion Recommendation Challenge.The goal of\nthis challenge is to maximize the chances of a click when the users are\npresented with set of fashion items. We have approached this problem as a\nbinary classification problem. Our winning solution utilizes Catboost as the\nclassifier and Bayesian Optimization for hyper parameter tuning. Our baseline\nmodel achieved MRR of 0.5153 on the validation set. Bayesian optimization of\nhyper parameters improved the MRR to 0.5240 on the validation set. Our final\nsubmission on the test set achieved a MRR of 0.5257.",
          "link": "http://arxiv.org/abs/2108.01314",
          "publishedOn": "2021-08-04T01:59:19.411Z",
          "wordCount": 564,
          "title": "Solving Fashion Recommendation -- The Farfetch Challenge. (arXiv:2108.01314v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00735",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Swijsen_L/0/1/0/all/0/1\">Lars Swijsen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Veken_J/0/1/0/all/0/1\">Joeri Van der Veken</a>, <a href=\"http://arxiv.org/find/math/1/au:+Vannieuwenhoven_N/0/1/0/all/0/1\">Nick Vannieuwenhoven</a>",
          "description": "We propose a Riemannian conjugate gradient (CG) optimization method for\nfinding low rank approximations of incomplete tensors. Our main contribution\nconsists of an explicit expression of the geodesics on the Segre manifold.\nThese are exploited in our algorithm to perform the retractions. We apply our\nmethod to movie rating predictions in a recommender system for the MovieLens\ndataset, and identification of pure fluorophores via fluorescent spectroscopy\nwith missing data. In this last application, we recover the tensor\ndecomposition from less than $10\\%$ of the data.",
          "link": "http://arxiv.org/abs/2108.00735",
          "publishedOn": "2021-08-03T02:06:29.491Z",
          "wordCount": 530,
          "title": "Tensor completion using geodesics on Segre manifolds. (arXiv:2108.00735v1 [math.DG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.09060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noufi_C/0/1/0/all/0/1\">Camille Noufi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Prateek Verma</a>",
          "description": "In music and speech, meaning is derived at multiple levels of context.\nAffect, for example, can be inferred both by a short sound token and by sonic\npatterns over a longer temporal window such as an entire recording. In this\nletter, we focus on inferring meaning from this dichotomy of contexts. We show\nhow contextual representations of short sung vocal lines can be implicitly\nlearned from fundamental frequency ($F_0$) and thus be used as a meaningful\nfeature space for downstream Music Information Retrieval (MIR) tasks. We\npropose three self-supervised deep learning paradigms which leverage pseudotask\nlearning of these two levels of context to produce latent representation\nspaces. We evaluate the usefulness of these representations by embedding unseen\npitch contours into each space and conducting downstream classification tasks.\nOur results show that contextual representation can enhance downstream\nclassification by as much as 15\\% as compared to using traditional statistical\ncontour features.",
          "link": "http://arxiv.org/abs/2007.09060",
          "publishedOn": "2021-08-03T02:06:29.478Z",
          "wordCount": 632,
          "title": "Self-Supervised Learning of Context-Aware Pitch Prosody Representations. (arXiv:2007.09060v4 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Harin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoeger_F/0/1/0/all/0/1\">Frank Hoeger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoenwiesner_M/0/1/0/all/0/1\">Marc Schoenwiesner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1\">Minsu Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1\">Nori Jacoby</a>",
          "description": "Do people from different cultural backgrounds perceive the mood in music the\nsame way? How closely do human ratings across different cultures approximate\nautomatic mood detection algorithms that are often trained on corpora of\npredominantly Western popular music? Analyzing 166 participants responses from\nBrazil, South Korea, and the US, we examined the similarity between the ratings\nof nine categories of perceived moods in music and estimated their alignment\nwith four popular mood detection algorithms. We created a dataset of 360 recent\npop songs drawn from major music charts of the countries and constructed\nsemantically identical mood descriptors across English, Korean, and Portuguese\nlanguages. Multiple participants from the three countries rated their\nfamiliarity, preference, and perceived moods for a given song. Ratings were\nhighly similar within and across cultures for basic mood attributes such as\nsad, cheerful, and energetic. However, we found significant cross-cultural\ndifferences for more complex characteristics such as dreamy and love. To our\nsurprise, the results of mood detection algorithms were uniformly correlated\nacross human ratings from all three countries and did not show a detectable\nbias towards any particular culture. Our study thus suggests that the mood\ndetection algorithms can be considered as an objective measure at least within\nthe popular music context.",
          "link": "http://arxiv.org/abs/2108.00768",
          "publishedOn": "2021-08-03T02:06:29.432Z",
          "wordCount": 676,
          "title": "Cross-cultural Mood Perception in Pop Songs and its Alignment with Mood Detection Algorithms. (arXiv:2108.00768v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2007.00814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1\">Omar Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>",
          "description": "Systems for Open-Domain Question Answering (OpenQA) generally depend on a\nretriever for finding candidate passages in a large corpus and a reader for\nextracting answers from those passages. In much recent work, the retriever is a\nlearned component that uses coarse-grained vector representations of questions\nand passages. We argue that this modeling choice is insufficiently expressive\nfor dealing with the complexity of natural language questions. To address this,\nwe define ColBERT-QA, which adapts the scalable neural retrieval model ColBERT\nto OpenQA. ColBERT creates fine-grained interactions between questions and\npassages. We propose an efficient weak supervision strategy that iteratively\nuses ColBERT to create its own training data. This greatly improves OpenQA\nretrieval on Natural Questions, SQuAD, and TriviaQA, and the resulting system\nattains state-of-the-art extractive OpenQA performance on all three datasets.",
          "link": "http://arxiv.org/abs/2007.00814",
          "publishedOn": "2021-08-03T02:06:29.392Z",
          "wordCount": 609,
          "title": "Relevance-guided Supervision for OpenQA with ColBERT. (arXiv:2007.00814v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>",
          "description": "It is widely acknowledged that learning joint embeddings of recipes with\nimages is challenging due to the diverse composition and deformation of\ningredients in cooking procedures. We present a Multi-modal Semantics enhanced\nJoint Embedding approach (MSJE) for learning a common feature space between the\ntwo modalities (text and image), with the ultimate goal of providing\nhigh-performance cross-modal retrieval services. Our MSJE approach has three\nunique features. First, we extract the TFIDF feature from the title,\ningredients and cooking instructions of recipes. By determining the\nsignificance of word sequences through combining LSTM learned features with\ntheir TFIDF features, we encode a recipe into a TFIDF weighted vector for\ncapturing significant key terms and how such key terms are used in the\ncorresponding cooking instructions. Second, we combine the recipe TFIDF feature\nwith the recipe sequence feature extracted through two-stage LSTM networks,\nwhich is effective in capturing the unique relationship between a recipe and\nits associated image(s). Third, we further incorporate TFIDF enhanced category\nsemantics to improve the mapping of image modality and to regulate the\nsimilarity loss function during the iterative learning of cross-modal joint\nembedding. Experiments on the benchmark dataset Recipe1M show the proposed\napproach outperforms the state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.00724",
          "publishedOn": "2021-08-03T02:06:29.377Z",
          "wordCount": 655,
          "title": "Learning TFIDF Enhanced Joint Embedding for Recipe-Image Cross-Modal Retrieval Service. (arXiv:2108.00724v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hansi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhichao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingyao Ai</a>",
          "description": "User and item reviews are valuable for the construction of recommender\nsystems. In general, existing review-based methods for recommendation can be\nbroadly categorized into two groups: the siamese models that build static user\nand item representations from their reviews respectively, and the\ninteraction-based models that encode user and item dynamically according to the\nsimilarity or relationships of their reviews. Although the interaction-based\nmodels have more model capacity and fit human purchasing behavior better,\nseveral problematic model designs and assumptions of the existing\ninteraction-based models lead to its suboptimal performance compared to\nexisting siamese models. In this paper, we identify three problems of the\nexisting interaction-based recommendation models and propose a couple of\nsolutions as well as a new interaction-based model to incorporate review data\nfor rating prediction. Our model implements a relevance matching model with\nregularized training losses to discover user relevant information from long\nitem reviews, and it also adapts a zero attention strategy to dynamically\nbalance the item-dependent and item-independent information extracted from user\nreviews. Empirical experiments and case studies on Amazon Product Benchmark\ndatasets show that our model can extract effective and interpretable user/item\nrepresentations from their reviews and outperforms multiple types of\nstate-of-the-art review-based recommendation models.",
          "link": "http://arxiv.org/abs/2101.06387",
          "publishedOn": "2021-08-03T02:06:29.266Z",
          "wordCount": 660,
          "title": "A Zero Attentive Relevance Matching Networkfor Review Modeling in Recommendation System. (arXiv:2101.06387v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.04695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitrevski_B/0/1/0/all/0/1\">Blagoj Mitrevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filipovic_M/0/1/0/all/0/1\">Milena Filipovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1\">Emma Lejal Glaude</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>",
          "description": "Multi-objective gradient methods are becoming the standard for solving\nmulti-objective problems. Among others, they show promising results in\ndeveloping multi-objective recommender systems with both correlated and\nconflicting objectives. Classic multi-gradient descent usually relies on the\ncombination of the gradients, not including the computation of first and second\nmoments of the gradients. This leads to a brittle behavior and misses important\nareas in the solution space. In this work, we create a multi-objective\nmodel-agnostic Adamize method that leverages the benefits of the Adam optimizer\nin single-objective problems. This corrects and stabilizes the gradients of\nevery objective before calculating a common gradient descent vector that\noptimizes all the objectives simultaneously. We evaluate the benefits of\nmulti-objective Adamize on two multi-objective recommender systems and for\nthree different objective combinations, both correlated or conflicting. We\nreport significant improvements, measured with three different Pareto front\nmetrics: hypervolume, coverage, and spacing. Finally, we show that the Adamized\nPareto front strictly dominates the previous one on multiple objective pairs.",
          "link": "http://arxiv.org/abs/2009.04695",
          "publishedOn": "2021-08-03T02:06:29.253Z",
          "wordCount": 643,
          "title": "Momentum-based Gradient Methods in Multi-Objective Recommendation. (arXiv:2009.04695v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grundmann_P/0/1/0/all/0/1\">Paul Grundmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_S/0/1/0/all/0/1\">Sebastian Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loser_A/0/1/0/all/0/1\">Alexander L&#xf6;ser</a>",
          "description": "Retrieving answer passages from long documents is a complex task requiring\nsemantic understanding of both discourse and document context. We approach this\nchallenge specifically in a clinical scenario, where doctors retrieve cohorts\nof patients based on diagnoses and other latent medical aspects. We introduce\nCAPR, a rule-based self-supervision objective for training Transformer language\nmodels for domain-specific passage matching. In addition, we contribute a novel\nretrieval dataset based on clinical notes to simulate this scenario on a large\ncorpus of clinical notes. We apply our objective in four Transformer-based\narchitectures: Contextual Document Vectors, Bi-, Poly- and Cross-encoders. From\nour extensive evaluation on MIMIC-III and three other healthcare datasets, we\nreport that CAPR outperforms strong baselines in the retrieval of\ndomain-specific passages and effectively generalizes across rule-based and\nhuman-labeled passages. This makes the model powerful especially in zero-shot\nscenarios where only limited training data is available.",
          "link": "http://arxiv.org/abs/2108.00775",
          "publishedOn": "2021-08-03T02:06:29.231Z",
          "wordCount": 572,
          "title": "Self-supervised Answer Retrieval on Clinical Notes. (arXiv:2108.00775v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1\">Jingtao Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiaxin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiqun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shaoping Ma</a>",
          "description": "Recently, Information Retrieval community has witnessed fast-paced advances\nin Dense Retrieval (DR), which performs first-stage retrieval by encoding\ndocuments in a low-dimensional embedding space and querying them with\nembedding-based search. Despite the impressive ranking performance, previous\nstudies usually adopt brute-force search to acquire candidates, which is\nprohibitive in practical Web search scenarios due to its tremendous memory\nusage and time cost. To overcome these problems, vector compression methods, a\nbranch of Approximate Nearest Neighbor Search (ANNS), have been adopted in many\npractical embedding-based retrieval applications. One of the most popular\nmethods is Product Quantization (PQ). However, although existing vector\ncompression methods including PQ can help improve the efficiency of DR, they\nincur severely decayed retrieval performance due to the separation between\nencoding and compression. To tackle this problem, we present JPQ, which stands\nfor Joint optimization of query encoding and Product Quantization. It trains\nthe query encoder and PQ index jointly in an end-to-end manner based on three\noptimization strategies, namely ranking-oriented loss, PQ centroid\noptimization, and end-to-end negative sampling. We evaluate JPQ on two publicly\navailable retrieval benchmarks. Experimental results show that JPQ\nsignificantly outperforms existing popular vector compression methods in terms\nof different trade-off settings. Compared with previous DR models that use\nbrute-force search, JPQ almost matches the best retrieval performance with 30x\ncompression on index size. The compressed index further brings 10x speedup on\nCPU and 2x speedup on GPU in query latency.",
          "link": "http://arxiv.org/abs/2108.00644",
          "publishedOn": "2021-08-03T02:06:29.167Z",
          "wordCount": 681,
          "title": "Jointly Optimizing Query Encoder and Product Quantization to Improve Retrieval Performance. (arXiv:2108.00644v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2011.07734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1\">Qihao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun Chen</a>",
          "description": "Recommendation from implicit feedback is a highly challenging task due to the\nlack of reliable negative feedback data. Existing methods address this\nchallenge by treating all the un-observed data as negative (dislike) but\ndownweight the confidence of these data. However, this treatment causes two\nproblems: (1) Confidence weights of the unobserved data are usually assigned\nmanually, which lack flexibility and may create empirical bias on evaluating\nuser's preference. (2) To handle massive volume of the unobserved feedback\ndata, most of the existing methods rely on stochastic inference and data\nsampling strategies. However, since a user is only aware of a very small\nfraction of items in a large dataset, it is difficult for existing samplers to\nselect informative training instances in which the user really dislikes the\nitem rather than does not know it.\n\nTo address the above two problems, we propose two novel recommendation\nmethods SamWalker and SamWalker++ that support both adaptive confidence\nassignment and efficient model learning. SamWalker models data confidence with\na social network-aware function, which can adaptively specify different weights\nto different data according to users' social contexts. However, the social\nnetwork information may not be available in many recommender systems, which\nhinders application of SamWalker. Thus, we further propose SamWalker++, which\ndoes not require any side information and models data confidence with a\nconstructed pseudo-social network. We also develop fast random-walk-based\nsampling strategies for our SamWalker and SamWalker++ to adaptively draw\ninformative training instances, which can speed up gradient estimation and\nreduce sampling variance. Extensive experiments on five real-world datasets\ndemonstrate the superiority of the proposed SamWalker and SamWalker++.",
          "link": "http://arxiv.org/abs/2011.07734",
          "publishedOn": "2021-08-03T02:06:29.135Z",
          "wordCount": 728,
          "title": "SamWalker++: recommendation with informative sampling strategy. (arXiv:2011.07734v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kihwan Kim</a>",
          "description": "All learning algorithms for recommendations face inevitable and critical\ntrade-off between exploiting partial knowledge of a user's preferences for\nshort-term satisfaction and exploring additional user preferences for long-term\ncoverage. Although exploration is indispensable for long success of a\nrecommender system, the exploration has been considered as the risk to decrease\nuser satisfaction. The reason for the risk is that items chosen for exploration\nfrequently mismatch with the user's interests. To mitigate this risk,\nrecommender systems have mixed items chosen for exploration into a\nrecommendation list, disguising the items as recommendations to elicit feedback\non the items to discover the user's additional tastes. This mix-in approach has\nbeen widely used in many recommenders, but there is rare research, evaluating\nthe effectiveness of the mix-in approach or proposing a new approach for\neliciting user feedback without deceiving users. In this work, we aim to\npropose a new approach for feedback elicitation without any deception and\ncompare our approach to the conventional mix-in approach for evaluation. To\nthis end, we designed a recommender interface that reveals which items are for\nexploration and conducted a within-subject study with 94 MTurk workers. Our\nresults indicated that users left significantly more feedback on items chosen\nfor exploration with our interface. Besides, users evaluated that our new\ninterface is better than the conventional mix-in interface in terms of novelty,\ndiversity, transparency, trust, and satisfaction. Finally, path analysis show\nthat, in only our new interface, exploration caused to increase user-centric\nevaluation metrics. Our work paves the way for how to design an interface,\nwhich utilizes learning algorithm based on users' feedback signals, giving\nbetter user experience and gathering more feedback data.",
          "link": "http://arxiv.org/abs/2108.00151",
          "publishedOn": "2021-08-03T02:06:28.832Z",
          "wordCount": 708,
          "title": "An Empirical analysis on Transparent Algorithmic Exploration in Recommender Systems. (arXiv:2108.00151v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moradi_E/0/1/0/all/0/1\">Ehsan Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_D/0/1/0/all/0/1\">Debajyoti Mondal</a>",
          "description": "Graph layouts are key to exploring massive graphs. An enormous number of\nnodes and edges do not allow network analysis software to produce meaningful\nvisualization of the pervasive networks. Long computation time, memory and\ndisplay limitations encircle the software's ability to explore massive graphs.\nThis paper introduces BigGraphVis, a new parallel graph visualization method\nthat uses GPU parallel processing and community detection algorithm to\nvisualize graph communities. We combine parallelized streaming community\ndetection algorithm and probabilistic data structure to leverage parallel\nprocessing of Graphics Processing Unit (GPU). To the best of our knowledge,\nthis is the first attempt to combine the power of streaming algorithms coupled\nwith GPU computing to tackle big graph visualization challenges. Our method\nextracts community information in a few passes on the edge list, and renders\nthe community structures using the ForceAtlas2 algorithm. Our experiment with\nmassive real-life graphs indicates that about 70 to 95 percent speedup can be\nachieved by visualizing graph communities, and the visualization appears to be\nmeaningful and reliable. The biggest graph that we examined contains above 3\nmillion nodes and 34 million edges, and the layout computation took about five\nminutes. We also observed that the BigGraphVis coloring strategy can be\nsuccessfully applied to produce a more informative ForceAtlas2 layout.",
          "link": "http://arxiv.org/abs/2108.00529",
          "publishedOn": "2021-08-03T02:06:28.800Z",
          "wordCount": 667,
          "title": "BigGraphVis: Leveraging Streaming Algorithms and GPU Acceleration for Visualizing Big Graphs. (arXiv:2108.00529v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anshul Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_N/0/1/0/all/0/1\">Noveen Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sheshansh Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sumeet Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1\">Purushottam Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Manik Varma</a>",
          "description": "Deep extreme classification (XC) seeks to train deep architectures that can\ntag a data point with its most relevant subset of labels from an extremely\nlarge label set. The core utility of XC comes from predicting labels that are\nrarely seen during training. Such rare labels hold the key to personalized\nrecommendations that can delight and surprise a user. However, the large number\nof rare labels and small amount of training data per rare label offer\nsignificant statistical and computational challenges. State-of-the-art deep XC\nmethods attempt to remedy this by incorporating textual descriptions of labels\nbut do not adequately address the problem. This paper presents ECLARE, a\nscalable deep learning architecture that incorporates not only label text, but\nalso label correlations, to offer accurate real-time predictions within a few\nmilliseconds. Core contributions of ECLARE include a frugal architecture and\nscalable techniques to train deep models along with label correlation graphs at\nthe scale of millions of labels. In particular, ECLARE offers predictions that\nare 2 to 14% more accurate on both publicly available benchmark datasets as\nwell as proprietary datasets for a related products recommendation task sourced\nfrom the Bing search engine. Code for ECLARE is available at\nhttps://github.com/Extreme-classification/ECLARE.",
          "link": "http://arxiv.org/abs/2108.00261",
          "publishedOn": "2021-08-03T02:06:28.775Z",
          "wordCount": 655,
          "title": "ECLARE: Extreme Classification with Label Graph Correlations. (arXiv:2108.00261v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Veretennikov_A/0/1/0/all/0/1\">Alexander B. Veretennikov</a>",
          "description": "The problem of proximity full-text search is considered. If a search query\ncontains high-frequently occurring words, then multi-component key indexes\ndeliver an improvement in the search speed compared with ordinary inverted\nindexes. It was shown that we can increase the search speed by up to 130 times\nin cases when queries consist of high-frequently occurring words. In this\npaper, we investigate how the multi-component key index architecture affects\nthe quality of the search. We consider several well-known methods of relevance\nranking, where these methods are of different authors. Using these methods, we\nperform the search in the ordinary inverted index and then in an index enhanced\nwith multi-component key indexes. The results show that with multi-component\nkey indexes we obtain search results that are very close, in terms of relevance\nranking, to the search results that are obtained by means of ordinary inverted\nindexes.",
          "link": "http://arxiv.org/abs/2108.00410",
          "publishedOn": "2021-08-03T02:06:28.765Z",
          "wordCount": 608,
          "title": "Relevance ranking for proximity full-text search based on additional indexes with multi-component keys. (arXiv:2108.00410v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anshul Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahiya_K/0/1/0/all/0/1\">Kunal Dahiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sheshansh Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saini_D/0/1/0/all/0/1\">Deepak Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sumeet Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1\">Purushottam Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Manik Varma</a>",
          "description": "Extreme multi-label classification (XML) involves tagging a data point with\nits most relevant subset of labels from an extremely large label set, with\nseveral applications such as product-to-product recommendation with millions of\nproducts. Although leading XML algorithms scale to millions of labels, they\nlargely ignore label meta-data such as textual descriptions of the labels. On\nthe other hand, classical techniques that can utilize label metadata via\nrepresentation learning using deep networks struggle in extreme settings. This\npaper develops the DECAF algorithm that addresses these challenges by learning\nmodels enriched by label metadata that jointly learn model parameters and\nfeature representations using deep networks and offer accurate classification\nat the scale of millions of labels. DECAF makes specific contributions to model\narchitecture design, initialization, and training, enabling it to offer up to\n2-6% more accurate prediction than leading extreme classifiers on publicly\navailable benchmark product-to-product recommendation datasets, such as\nLF-AmazonTitles-1.3M. At the same time, DECAF was found to be up to 22x faster\nat inference than leading deep extreme classifiers, which makes it suitable for\nreal-time applications that require predictions within a few milliseconds. The\ncode for DECAF is available at the following URL\nhttps://github.com/Extreme-classification/DECAF.",
          "link": "http://arxiv.org/abs/2108.00368",
          "publishedOn": "2021-08-03T02:06:28.738Z",
          "wordCount": 654,
          "title": "DECAF: Deep Extreme Classification with Label Features. (arXiv:2108.00368v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ulukus_S/0/1/0/all/0/1\">Sennur Ulukus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gastpar_M/0/1/0/all/0/1\">Michael Gastpar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafar_S/0/1/0/all/0/1\">Syed Jafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_R/0/1/0/all/0/1\">Ravi Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1\">Chao Tian</a>",
          "description": "Most of our lives are conducted in the cyberspace. The human notion of\nprivacy translates into a cyber notion of privacy on many functions that take\nplace in the cyberspace. This article focuses on three such functions: how to\nprivately retrieve information from cyberspace (privacy in information\nretrieval), how to privately leverage large-scale distributed/parallel\nprocessing (privacy in distributed computing), and how to learn/train machine\nlearning models from private data spread across multiple users (privacy in\ndistributed (federated) learning). The article motivates each privacy setting,\ndescribes the problem formulation, summarizes breakthrough results in the\nhistory of each problem, and gives recent results and discusses some of the\nmajor ideas that emerged in each field. In addition, the cross-cutting\ntechniques and interconnections between the three topics are discussed along\nwith a set of open problems and challenges.",
          "link": "http://arxiv.org/abs/2108.00026",
          "publishedOn": "2021-08-03T02:06:28.542Z",
          "wordCount": 591,
          "title": "Private Retrieval, Computing and Learning: Recent Progress and Future Challenges. (arXiv:2108.00026v1 [cs.CR])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2108.02953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wanqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming Yang</a>",
          "description": "This paper investigates a valuable setting called few-shot unsupervised\ndomain adaptation (FS-UDA), which has not been sufficiently studied in the\nliterature. In this setting, the source domain data are labelled, but with\nfew-shot per category, while the target domain data are unlabelled. To address\nthe FS-UDA setting, we develop a general UDA model to solve the following two\nkey issues: the few-shot labeled data per category and the domain adaptation\nbetween support and query sets. Our model is general in that once trained it\nwill be able to be applied to various FS-UDA tasks from the same source and\ntarget domains. Inspired by the recent local descriptor based few-shot learning\n(FSL), our general UDA model is fully built upon local descriptors (LDs) for\nimage classification and domain adaptation. By proposing a novel concept called\nsimilarity patterns (SPs), our model not only effectively considers the spatial\nrelationship of LDs that was ignored in previous FSL methods, but also makes\nthe learned image similarity better serve the required domain alignment.\nSpecifically, we propose a novel IMage-to-class sparse Similarity Encoding\n(IMSE) method. It learns SPs to extract the local discriminative information\nfor classification and meanwhile aligns the covariance matrix of the SPs for\ndomain adaptation. Also, domain adversarial training and multi-scale local\nfeature matching are performed upon LDs. Extensive experiments conducted on a\nmulti-domain benchmark dataset DomainNet demonstrates the state-of-the-art\nperformance of our IMSE for the novel setting of FS-UDA. In addition, for FSL,\nour IMSE can also show better performance than most of recent FSL methods on\nminiImageNet.",
          "link": "http://arxiv.org/abs/2108.02953",
          "publishedOn": "2021-08-09T00:49:26.032Z",
          "wordCount": 713,
          "title": "Few-shot Unsupervised Domain Adaptation with Image-to-class Sparse Similarity Encoding. (arXiv:2108.02953v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02607",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Susan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilin Chen</a>",
          "description": "We introduce a new efficient framework, the Unified Context Network (UniCon),\nfor robust active speaker detection (ASD). Traditional methods for ASD usually\noperate on each candidate's pre-cropped face track separately and do not\nsufficiently consider the relationships among the candidates. This potentially\nlimits performance, especially in challenging scenarios with low-resolution\nfaces, multiple candidates, etc. Our solution is a novel, unified framework\nthat focuses on jointly modeling multiple types of contextual information:\nspatial context to indicate the position and scale of each candidate's face,\nrelational context to capture the visual relationships among the candidates and\ncontrast audio-visual affinities with each other, and temporal context to\naggregate long-term information and smooth out local uncertainties. Based on\nsuch information, our model optimizes all candidates in a unified process for\nrobust and reliable ASD. A thorough ablation study is performed on several\nchallenging ASD benchmarks under different settings. In particular, our method\noutperforms the state-of-the-art by a large margin of about 15% mean Average\nPrecision (mAP) absolute on two challenging subsets: one with three candidate\nspeakers, and the other with faces smaller than 64 pixels. Together, our UniCon\nachieves 92.0% mAP on the AVA-ActiveSpeaker validation set, surpassing 90% for\nthe first time on this challenging dataset at the time of submission. Project\nwebsite: https://unicon-asd.github.io/.",
          "link": "http://arxiv.org/abs/2108.02607",
          "publishedOn": "2021-08-06T00:51:44.662Z",
          "wordCount": 688,
          "title": "UniCon: Unified Context Network for Robust Active Speaker Detection. (arXiv:2108.02607v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02515",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verde_S/0/1/0/all/0/1\">Sebastiano Verde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasquini_C/0/1/0/all/0/1\">Cecilia Pasquini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lago_F/0/1/0/all/0/1\">Federica Lago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goller_A/0/1/0/all/0/1\">Alessandro Goller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natale_F/0/1/0/all/0/1\">Francesco GB De Natale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piva_A/0/1/0/all/0/1\">Alessandro Piva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boato_G/0/1/0/all/0/1\">Giulia Boato</a>",
          "description": "The amount of multimedia content shared everyday, combined with the level of\nrealism reached by recent fake-generating technologies, threatens to impair the\ntrustworthiness of online information sources. The process of uploading and\nsharing data tends to hinder standard media forensic analyses, since multiple\nre-sharing steps progressively hide the traces of past manipulations. At the\nsame time though, new traces are introduced by the platforms themselves,\nenabling the reconstruction of the sharing history of digital objects, with\npossible applications in information flow monitoring and source identification.\nIn this work, we propose a supervised framework for the reconstruction of image\nsharing chains on social media platforms. The system is structured as a cascade\nof backtracking blocks, each of them tracing back one step of the sharing chain\nat a time. Blocks are designed as ensembles of classifiers trained to analyse\nthe input image independently from one another by leveraging different feature\nrepresentations that describe both content and container of the media object.\nIndividual decisions are then properly combined by a late fusion strategy.\nResults highlight the advantages of employing multiple clues, which allow\naccurately tracing back up to three steps along the sharing chain.",
          "link": "http://arxiv.org/abs/2108.02515",
          "publishedOn": "2021-08-06T00:51:44.632Z",
          "wordCount": 629,
          "title": "Multi-clue reconstruction of sharing chains for social media images. (arXiv:2108.02515v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1\">Shahroz Tariq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Simon S. Woo</a>",
          "description": "Over the last few decades, artificial intelligence research has made\ntremendous strides, but it still heavily relies on fixed datasets in stationary\nenvironments. Continual learning is a growing field of research that examines\nhow AI systems can learn sequentially from a continuous stream of linked data\nin the same way that biological systems do. Simultaneously, fake media such as\ndeepfakes and synthetic face images have emerged as significant to current\nmultimedia technologies. Recently, numerous method has been proposed which can\ndetect deepfakes with high accuracy. However, they suffer significantly due to\ntheir reliance on fixed datasets in limited evaluation settings. Therefore, in\nthis work, we apply continuous learning to neural networks' learning dynamics,\nemphasizing its potential to increase data efficiency significantly. We propose\nContinual Representation using Distillation (CoReD) method that employs the\nconcept of Continual Learning (CL), Representation Learning (RL), and Knowledge\nDistillation (KD). We design CoReD to perform sequential domain adaptation\ntasks on new deepfake and GAN-generated synthetic face datasets, while\neffectively minimizing the catastrophic forgetting in a teacher-student model\nsetting. Our extensive experimental results demonstrate that our method is\nefficient at domain adaptation to detect low-quality deepfakes videos and\nGAN-generated images from several datasets, outperforming the-state-of-art\nbaseline methods.",
          "link": "http://arxiv.org/abs/2107.02408",
          "publishedOn": "2021-08-06T00:51:44.416Z",
          "wordCount": 710,
          "title": "CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation. (arXiv:2107.02408v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02481",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Javaheri_A/0/1/0/all/0/1\">Alireza Javaheri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brites_C/0/1/0/all/0/1\">Catarina Brites</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pereira_F/0/1/0/all/0/1\">Fernando Pereira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ascenso_J/0/1/0/all/0/1\">Jo&#xe3;o Ascenso</a>",
          "description": "Point cloud coding solutions have been recently standardized to address the\nneeds of multiple application scenarios. The design and assessment of point\ncloud coding methods require reliable objective quality metrics to evaluate the\nlevel of degradation introduced by compression or any other type of processing.\nSeveral point cloud objective quality metrics has been recently proposed to\nreliable estimate human perceived quality, including the so-called\nprojection-based metrics. In this context, this paper proposes a joint geometry\nand color projection-based point cloud objective quality metric which solves\nthe critical weakness of this type of quality metrics, i.e., the misalignment\nbetween the reference and degraded projected images. Moreover, the proposed\npoint cloud quality metric exploits the best performing 2D quality metrics in\nthe literature to assess the quality of the projected images. The experimental\nresults show that the proposed projection-based quality metric offers the best\nsubjective-objective correlation performance in comparison with other metrics\nin the literature. The Pearson correlation gains regarding D1-PSNR and D2-PSNR\nmetrics are 17% and 14.2 when data with all coding degradations is considered.",
          "link": "http://arxiv.org/abs/2108.02481",
          "publishedOn": "2021-08-06T00:51:44.395Z",
          "wordCount": 627,
          "title": "Joint Geometry and Color Projection-based Point Cloud Quality Metric. (arXiv:2108.02481v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yanbin Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>",
          "description": "Transformer achieves remarkable successes in understanding 1 and\n2-dimensional signals (e.g., NLP and Image Content Understanding). As a\npotential alternative to convolutional neural networks, it shares merits of\nstrong interpretability, high discriminative power on hyper-scale data, and\nflexibility in processing varying length inputs. However, its encoders\nnaturally contain computational intensive operations such as pair-wise\nself-attention, incurring heavy computational burden when being applied on the\ncomplex 3-dimensional video signals.\n\nThis paper presents Token Shift Module (i.e., TokShift), a novel,\nzero-parameter, zero-FLOPs operator, for modeling temporal relations within\neach transformer encoder. Specifically, the TokShift barely temporally shifts\npartial [Class] token features back-and-forth across adjacent frames. Then, we\ndensely plug the module into each encoder of a plain 2D vision transformer for\nlearning 3D video representation. It is worth noticing that our TokShift\ntransformer is a pure convolutional-free video transformer pilot with\ncomputational efficiency for video understanding. Experiments on standard\nbenchmarks verify its robustness, effectiveness, and efficiency. Particularly,\nwith input clips of 8/12 frames, the TokShift transformer achieves SOTA\nprecision: 79.83%/80.40% on the Kinetics-400, 66.56% on EGTEA-Gaze+, and 96.80%\non UCF-101 datasets, comparable or better than existing SOTA convolutional\ncounterparts. Our code is open-sourced in:\nhttps://github.com/VideoNetworks/TokShift-Transformer.",
          "link": "http://arxiv.org/abs/2108.02432",
          "publishedOn": "2021-08-06T00:51:43.978Z",
          "wordCount": 640,
          "title": "Token Shift Transformer for Video Classification. (arXiv:2108.02432v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1810.01248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xutan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Faqiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yidan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>",
          "description": "Deep learning researches on the transformation problems for image and text\nhave raised great attention. However, present methods for music feature\ntransfer using neural networks are far from practical application. In this\npaper, we initiate a novel system for transferring the texture of music, and\nrelease it as an open source project. Its core algorithm is composed of a\nconverter which represents sounds as texture spectra, a corresponding\nreconstructor and a feed-forward transfer network. We evaluate this system from\nmultiple perspectives, and experimental results reveal that it achieves\nconvincing results in both sound effects and computational performance.",
          "link": "http://arxiv.org/abs/1810.01248",
          "publishedOn": "2021-08-05T01:56:19.261Z",
          "wordCount": 601,
          "title": "A Lightweight Music Texture Transfer System. (arXiv:1810.01248v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Donglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1\">Zhenqiu Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>",
          "description": "Hashing plays an important role in information retrieval, due to its low\nstorage and high speed of processing. Among the techniques available in the\nliterature, multi-modal hashing, which can encode heterogeneous multi-modal\nfeatures into compact hash codes, has received particular attention. Most of\nthe existing multi-modal hashing methods adopt the fixed weighting factors to\nfuse multiple modalities for any query data, which cannot capture the variation\nof different queries. Besides, many methods introduce hyper-parameters to\nbalance many regularization terms that make the optimization harder. Meanwhile,\nit is time-consuming and labor-intensive to set proper parameter values. The\nlimitations may significantly hinder their promotion in real applications. In\nthis paper, we propose a simple, yet effective method that is inspired by the\nHadamard matrix. The proposed method captures the multi-modal feature\ninformation in an adaptive manner and preserves the discriminative semantic\ninformation in the hash codes. Our framework is flexible and involves a very\nfew hyper-parameters. Extensive experimental results show the method is\neffective and achieves superior performance compared to state-of-the-art\nalgorithms.",
          "link": "http://arxiv.org/abs/2009.12148",
          "publishedOn": "2021-08-05T01:56:19.133Z",
          "wordCount": 652,
          "title": "Adaptive Multi-modal Fusion Hashing via Hadamard Matrix. (arXiv:2009.12148v4 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>",
          "description": "For an image with multiple scene texts, different people may be interested in\ndifferent text information. Current text-aware image captioning models are not\nable to generate distinctive captions according to various information needs.\nTo explore how to generate personalized text-aware captions, we define a new\nchallenging task, namely Question-controlled Text-aware Image Captioning\n(Qc-TextCap). With questions as control signals, this task requires models to\nunderstand questions, find related scene texts and describe them together with\nobjects fluently in human language. Based on two existing text-aware captioning\ndatasets, we automatically construct two datasets, ControlTextCaps and\nControlVizWiz to support the task. We propose a novel Geometry and Question\nAware Model (GQAM). GQAM first applies a Geometry-informed Visual Encoder to\nfuse region-level object features and region-level scene text features with\nconsidering spatial relationships. Then, we design a Question-guided Encoder to\nselect the most relevant visual features for each question. Finally, GQAM\ngenerates a personalized text-aware caption with a Multimodal Decoder. Our\nmodel achieves better captioning performance and question answering ability\nthan carefully designed baselines on both two datasets. With questions as\ncontrol signals, our model generates more informative and diverse captions than\nthe state-of-the-art text-aware captioning model. Our code and datasets are\npublicly available at https://github.com/HAWLYQ/Qc-TextCap.",
          "link": "http://arxiv.org/abs/2108.02059",
          "publishedOn": "2021-08-05T01:56:19.076Z",
          "wordCount": 644,
          "title": "Question-controlled Text-aware Image Captioning. (arXiv:2108.02059v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03088",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peidong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zibin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xiyu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shutao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Maowei Hu</a>",
          "description": "Compared with tedious per-pixel mask annotating, it is much easier to\nannotate data by clicks, which costs only several seconds for an image.\nHowever, applying clicks to learn video semantic segmentation model has not\nbeen explored before. In this work, we propose an effective weakly-supervised\nvideo semantic segmentation pipeline with click annotations, called WeClick,\nfor saving laborious annotating effort by segmenting an instance of the\nsemantic class with only a single click. Since detailed semantic information is\nnot captured by clicks, directly training with click labels leads to poor\nsegmentation predictions. To mitigate this problem, we design a novel memory\nflow knowledge distillation strategy to exploit temporal information (named\nmemory flow) in abundant unlabeled video frames, by distilling the neighboring\npredictions to the target frame via estimated motion. Moreover, we adopt\nvanilla knowledge distillation for model compression. In this case, WeClick\nlearns compact video semantic segmentation models with the low-cost click\nannotations during the training phase yet achieves real-time and accurate\nmodels during the inference period. Experimental results on Cityscapes and\nCamvid show that WeClick outperforms the state-of-the-art methods, increases\nperformance by 10.24% mIoU than baseline, and achieves real-time execution.",
          "link": "http://arxiv.org/abs/2107.03088",
          "publishedOn": "2021-08-05T01:56:19.065Z",
          "wordCount": 674,
          "title": "WeClick: Weakly-Supervised Video Semantic Segmentation with Click Annotations. (arXiv:2107.03088v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>",
          "description": "Most current image captioning systems focus on describing general image\ncontent, and lack background knowledge to deeply understand the image, such as\nexact named entities or concrete events. In this work, we focus on the\nentity-aware news image captioning task which aims to generate informative\ncaptions by leveraging the associated news articles to provide background\nknowledge about the target image. However, due to the length of news articles,\nprevious works only employ news articles at the coarse article or sentence\nlevel, which are not fine-grained enough to refine relevant events and choose\nnamed entities accurately. To overcome these limitations, we propose an\nInformation Concentrated Entity-aware news image CAPtioning (ICECAP) model,\nwhich progressively concentrates on relevant textual information within the\ncorresponding news article from the sentence level to the word level. Our model\nfirst creates coarse concentration on relevant sentences using a cross-modality\nretrieval model and then generates captions by further concentrating on\nrelevant words within the sentences. Extensive experiments on both BreakingNews\nand GoodNews datasets demonstrate the effectiveness of our proposed method,\nwhich outperforms other state-of-the-arts. The code of ICECAP is publicly\navailable at https://github.com/HAWLYQ/ICECAP.",
          "link": "http://arxiv.org/abs/2108.02050",
          "publishedOn": "2021-08-05T01:56:19.003Z",
          "wordCount": 630,
          "title": "ICECAP: Information Concentrated Entity-aware Image Captioning. (arXiv:2108.02050v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengpei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wenjing Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_T/0/1/0/all/0/1\">Tingcheng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruomei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan-fang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangjian He</a>",
          "description": "The latest trend in the bottom-up perspective for arbitrary-shape scene text\ndetection is to reason the links between text segments using Graph\nConvolutional Network (GCN). Notwithstanding, the performance of the best\nperforming bottom-up method is still inferior to that of the best performing\ntop-down method even with the help of GCN. We argue that this is not mainly\ncaused by the limited feature capturing ability of the text proposal backbone\nor GCN, but by their failure to make a full use of visual-relational features\nfor suppressing false detection, as well as the sub-optimal route-finding\nmechanism used for grouping text segments. In this paper, we revitalize the\nclassic text detection frameworks by aggregating the visual-relational features\nof text with two effective false positive/negative suppression mechanisms.\nFirst, dense overlapping text segments depicting the `characterness' and\n`streamline' of text are generated for further relational reasoning and weakly\nsupervised segment classification. Here, relational graph features are used for\nsuppressing false positives/negatives. Then, to fuse the relational features\nwith visual features, a Location-Aware Transfer (LAT) module is designed to\ntransfer text's relational features into visual compatible features with a Fuse\nDecoding (FD) module to enhance the representation of text regions for the\nsecond step suppression. Finally, a novel multiple-text-map-aware\ncontour-approximation strategy is developed, instead of the widely-used\nroute-finding process. Experiments conducted on five benchmark datasets, i.e.,\nCTW1500, Total-Text, ICDAR2015, MSRA-TD500, and MLT2017 demonstrate that our\nmethod outperforms the state-of-the-art performance when being embedded in a\nclassic text detection framework, which revitalises the superb strength of the\nbottom-up methods.",
          "link": "http://arxiv.org/abs/2108.01809",
          "publishedOn": "2021-08-05T01:56:18.908Z",
          "wordCount": 690,
          "title": "What's Wrong with the Bottom-up Methods in Arbitrary-shape Scene Text Detection. (arXiv:2108.01809v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02119",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Coelho_D/0/1/0/all/0/1\">D. F. G. Coelho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cintra_R/0/1/0/all/0/1\">R. J. Cintra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Madanayake_A/0/1/0/all/0/1\">A. Madanayake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Perera_S/0/1/0/all/0/1\">S. Perera</a>",
          "description": "This paper introduces a collection of scaling methods for generating\n$2N$-point DCT-II approximations based on $N$-point low-complexity\ntransformations. Such scaling is based on the Hou recursive matrix\nfactorization of the exact $2N$-point DCT-II matrix. Encompassing the widely\nemployed Jridi-Alfalou-Meher scaling method, the proposed techniques are shown\nto produce DCT-II approximations that outperform the transforms resulting from\nthe JAM scaling method according to total error energy and mean squared error.\nOrthogonality conditions are derived and an extensive error analysis based on\nstatistical simulation demonstrates the good performance of the introduced\nscaling methods. A hardware implementation is also provided demonstrating the\ncompetitiveness of the proposed methods when compared to the JAM scaling\nmethod.",
          "link": "http://arxiv.org/abs/2108.02119",
          "publishedOn": "2021-08-05T01:56:18.869Z",
          "wordCount": 572,
          "title": "Low-complexity Scaling Methods for DCT-II Approximations. (arXiv:2108.02119v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01374",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hung_H/0/1/0/all/0/1\">Hsiao-Tzu Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ching_J/0/1/0/all/0/1\">Joann Ching</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doh_S/0/1/0/all/0/1\">Seungheon Doh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Nabin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1\">Juhan Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Hsuan Yang</a>",
          "description": "While there are many music datasets with emotion labels in the literature,\nthey cannot be used for research on symbolic-domain music analysis or\ngeneration, as there are usually audio files only. In this paper, we present\nthe EMOPIA (pronounced `yee-m\\`{o}-pi-uh') dataset, a shared multi-modal (audio\nand MIDI) database focusing on perceived emotion in pop piano music, to\nfacilitate research on various tasks related to music emotion. The dataset\ncontains 1,087 music clips from 387 songs and clip-level emotion labels\nannotated by four dedicated annotators. Since the clips are not restricted to\none clip per song, they can also be used for song-level analysis. We present\nthe methodology for building the dataset, covering the song list curation, clip\nselection, and emotion annotation processes. Moreover, we prototype use cases\non clip-level music emotion classification and emotion-based symbolic music\ngeneration by training and evaluating corresponding models using the dataset.\nThe result demonstrates the potential of EMOPIA for being used in future\nexploration on piano emotion-related MIR tasks.",
          "link": "http://arxiv.org/abs/2108.01374",
          "publishedOn": "2021-08-04T01:59:19.479Z",
          "wordCount": 626,
          "title": "EMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation. (arXiv:2108.01374v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01612",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nabipour_S/0/1/0/all/0/1\">Saeideh Nabipour</a>",
          "description": "This paper investigates a novel approach of digital image watermarking based\non BCH error correction code in Discrete Cosine Transformation (DCT) domain. In\nthe proposed technique, the watermark is encoded through BCH error correction\ncode before embedding process, then it is embedded into the Discrete Cosine\nTransformation (DCT) coefficients of cover image. The proposed algorithm also\nemploys lookup table method to find the best positions in the frequency domains\nfor watermark insertion. The significant feature of this method is the\nreduction of time required in the process embedding of information, security\nand ability to correct the error caused by different attacks. Experimental\nresults show the superiority of the proposed approach against the existing\napproaches.",
          "link": "http://arxiv.org/abs/2108.01612",
          "publishedOn": "2021-08-04T01:59:19.455Z",
          "wordCount": 551,
          "title": "Fast BCH Coding for Optimal Robust Image Watermarking in DCT Domain. (arXiv:2108.01612v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tingtian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zixun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoruo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Hui Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yipeng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hengcan Shi</a>",
          "description": "Recently, the witness of the rapidly growing popularity of short videos on\ndifferent Internet platforms has intensified the need for a background music\n(BGM) retrieval system. However, existing video-music retrieval methods only\nbased on the visual modality cannot show promising performance regarding videos\nwith fine-grained virtual contents. In this paper, we also investigate the\nwidely added voice-overs in short videos and propose a novel framework to\nretrieve BGM for fine-grained short videos. In our framework, we use the\nself-attention (SA) and the cross-modal attention (CMA) modules to explore the\nintra- and the inter-relationships of different modalities respectively. For\nbalancing the modalities, we dynamically assign different weights to the modal\nfeatures via a fusion gate. For paring the query and the BGM embeddings, we\nintroduce a triplet pseudo-label loss to constrain the semantics of the modal\nembeddings. As there are no existing virtual-content video-BGM retrieval\ndatasets, we build and release two virtual-content video datasets HoK400 and\nCFM400. Experimental results show that our method achieves superior performance\nand outperforms other state-of-the-art methods with large margins.",
          "link": "http://arxiv.org/abs/2104.10557",
          "publishedOn": "2021-08-04T01:59:19.386Z",
          "wordCount": 653,
          "title": "Deep Music Retrieval for Fine-Grained Videos by Exploiting Cross-Modal-Encoded Voice-Overs. (arXiv:2104.10557v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.",
          "link": "http://arxiv.org/abs/2107.08264",
          "publishedOn": "2021-08-03T02:06:28.916Z",
          "wordCount": 730,
          "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Hsing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>",
          "description": "The surprisingness of a song is an essential and seemingly subjective factor\nin determining whether the listener likes it. With the help of information\ntheory, it can be described as the transition probability of a music sequence\nmodeled as a Markov chain. In this study, we introduce the concept of deriving\nentropy variations over time, so that the surprise contour of each chord\nsequence can be extracted. Based on this, we propose a user-controllable\nframework that uses a conditional variational autoencoder (CVAE) to harmonize\nthe melody based on the given chord surprise indication. Through explicit\nconditions, the model can randomly generate various and harmonic chord\nprogressions for a melody, and the Spearman's correlation and p-value\nsignificance show that the resulting chord progressions match the given\nsurprise contour quite well. The vanilla CVAE model was evaluated in a basic\nmelody harmonization task (no surprise control) in terms of six objective\nmetrics. The results of experiments on the Hooktheory Lead Sheet Dataset show\nthat our model achieves performance comparable to the state-of-the-art melody\nharmonization model.",
          "link": "http://arxiv.org/abs/2108.00378",
          "publishedOn": "2021-08-03T02:06:28.809Z",
          "wordCount": 610,
          "title": "SurpriseNet: Melody Harmonization Conditioning on User-controlled Surprise Contours. (arXiv:2108.00378v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1\">Uttaran Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Childs_E/0/1/0/all/0/1\">Elizabeth Childs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rewkowski_N/0/1/0/all/0/1\">Nicholas Rewkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>",
          "description": "We present a generative adversarial network to synthesize 3D pose sequences\nof co-speech upper-body gestures with appropriate affective expressions. Our\nnetwork consists of two components: a generator to synthesize gestures from a\njoint embedding space of features encoded from the input speech and the seed\nposes, and a discriminator to distinguish between the synthesized pose\nsequences and real 3D pose sequences. We leverage the Mel-frequency cepstral\ncoefficients and the text transcript computed from the input speech in separate\nencoders in our generator to learn the desired sentiments and the associated\naffective cues. We design an affective encoder using multi-scale\nspatial-temporal graph convolutions to transform 3D pose sequences into latent,\npose-based affective features. We use our affective encoder in both our\ngenerator, where it learns affective features from the seed poses to guide the\ngesture synthesis, and our discriminator, where it enforces the synthesized\ngestures to contain the appropriate affective expressions. We perform extensive\nevaluations on two benchmark datasets for gesture synthesis from the speech,\nthe TED Gesture Dataset and the GENEA Challenge 2020 Dataset. Compared to the\nbest baselines, we improve the mean absolute joint error by 10--33%, the mean\nacceleration difference by 8--58%, and the Fr\\'echet Gesture Distance by\n21--34%. We also conduct a user study and observe that compared to the best\ncurrent baselines, around 15.28% of participants indicated our synthesized\ngestures appear more plausible, and around 16.32% of participants felt the\ngestures had more appropriate affective expressions aligned with the speech.",
          "link": "http://arxiv.org/abs/2108.00262",
          "publishedOn": "2021-08-03T02:06:28.790Z",
          "wordCount": 703,
          "title": "Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective Expression Learning. (arXiv:2108.00262v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>",
          "description": "Occluded person re-identification (ReID) aims to match person images with\nocclusion. It is fundamentally challenging because of the serious occlusion\nwhich aggravates the misalignment problem between images. At the cost of\nincorporating a pose estimator, many works introduce pose information to\nalleviate the misalignment in both training and testing. To achieve high\naccuracy while preserving low inference complexity, we propose a network named\nPose-Guided Feature Learning with Knowledge Distillation (PGFL-KD), where the\npose information is exploited to regularize the learning of semantics aligned\nfeatures but is discarded in testing. PGFL-KD consists of a main branch (MB),\nand two pose-guided branches, \\ieno, a foreground-enhanced branch (FEB), and a\nbody part semantics aligned branch (SAB). The FEB intends to emphasise the\nfeatures of visible body parts while excluding the interference of obstructions\nand background (\\ieno, foreground feature alignment). The SAB encourages\ndifferent channel groups to focus on different body parts to have body part\nsemantics aligned representation. To get rid of the dependency on pose\ninformation when testing, we regularize the MB to learn the merits of the FEB\nand SAB through knowledge distillation and interaction-based training.\nExtensive experiments on occluded, partial, and holistic ReID tasks show the\neffectiveness of our proposed network.",
          "link": "http://arxiv.org/abs/2108.00139",
          "publishedOn": "2021-08-03T02:06:28.717Z",
          "wordCount": 652,
          "title": "Pose-Guided Feature Learning with Knowledge Distillation for Occluded Person Re-Identification. (arXiv:2108.00139v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>",
          "description": "This paper introduces a two-phase deep feature calibration framework for\nefficient learning of semantics enhanced text-image cross-modal joint\nembedding, which clearly separates the deep feature calibration in data\npreprocessing from training the joint embedding model. We use the Recipe1M\ndataset for the technical description and empirical validation. In\npreprocessing, we perform deep feature calibration by combining deep feature\nengineering with semantic context features derived from raw text-image input\ndata. We leverage LSTM to identify key terms, NLP methods to produce ranking\nscores for key terms before generating the key term feature. We leverage\nwideResNet50 to extract and encode the image category semantics to help\nsemantic alignment of the learned recipe and image embeddings in the joint\nlatent space. In joint embedding learning, we perform deep feature calibration\nby optimizing the batch-hard triplet loss function with soft-margin and double\nnegative sampling, also utilizing the category-based alignment loss and\ndiscriminator-based alignment loss. Extensive experiments demonstrate that our\nSEJE approach with the deep feature calibration significantly outperforms the\nstate-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.00705",
          "publishedOn": "2021-08-03T02:06:28.695Z",
          "wordCount": 615,
          "title": "Efficient Deep Feature Calibration for Cross-Modal Joint Embedding Learning. (arXiv:2108.00705v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Javaheri_A/0/1/0/all/0/1\">Alireza Javaheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brites_C/0/1/0/all/0/1\">Catarina Brites</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_F/0/1/0/all/0/1\">Fernando Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ascenso_J/0/1/0/all/0/1\">Jo&#xe3;o Ascenso</a>",
          "description": "Point clouds (PCs) are a powerful 3D visual representation paradigm for many\nemerging application domains, especially virtual and augmented reality, and\nautonomous vehicles. However, the large amount of PC data required for highly\nimmersive and realistic experiences requires the availability of efficient,\nlossy PC coding solutions are critical. Recently, two MPEG PC coding standards\nhave been developed to address the relevant application requirements and\nfurther developments are expected in the future. In this context, the\nassessment of PC quality, notably for decoded PCs, is critical and asks for the\ndesign of efficient objective PC quality metrics. In this paper, a novel\npoint-to-distribution metric is proposed for PC quality assessment considering\nboth the geometry and texture. This new quality metric exploits the\nscale-invariance property of the Mahalanobis distance to assess first the\ngeometry and color point-to-distribution distortions, which are after fused to\nobtain a joint geometry and color quality metric. The proposed quality metric\nsignificantly outperforms the best PC quality assessment metrics in the\nliterature.",
          "link": "http://arxiv.org/abs/2108.00054",
          "publishedOn": "2021-08-03T02:06:28.684Z",
          "wordCount": 620,
          "title": "A Point-to-Distribution Joint Geometry and Color Metric for Point Cloud Quality Assessment. (arXiv:2108.00054v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_P/0/1/0/all/0/1\">Prithwiraj Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raju_R/0/1/0/all/0/1\">Rajan Saha Raju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1\">Arif Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">M. Shahidur Rahman</a>",
          "description": "Text-to-Speech (TTS) system is a system where speech is synthesized from a\ngiven text following any particular approach. Concatenative synthesis, Hidden\nMarkov Model (HMM) based synthesis, Deep Learning (DL) based synthesis with\nmultiple building blocks, etc. are the main approaches for implementing a TTS\nsystem. Here, we are presenting our deep learning-based end-to-end Bangla\nspeech synthesis system. It has been implemented with minimal human annotation\nusing only 3 major components (Encoder, Decoder, Post-processing net including\nwaveform synthesis). It does not require any frontend preprocessor and\nGrapheme-to-Phoneme (G2P) converter. Our model has been trained with\nphonetically balanced 20 hours of single speaker speech data. It has obtained a\n3.79 Mean Opinion Score (MOS) on a scale of 5.0 as subjective evaluation and a\n0.77 Perceptual Evaluation of Speech Quality(PESQ) score on a scale of [-0.5,\n4.5] as objective evaluation. It is outperforming all existing non-commercial\nstate-of-the-art Bangla TTS systems based on naturalness.",
          "link": "http://arxiv.org/abs/2108.00500",
          "publishedOn": "2021-08-03T02:06:28.669Z",
          "wordCount": 587,
          "title": "End to End Bangla Speech Synthesis. (arXiv:2108.00500v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+K_P/0/1/0/all/0/1\">Priyadarshini K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Siddhartha Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borkar_V/0/1/0/all/0/1\">Vivek Borkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Subhasis Chaudhuri</a>",
          "description": "Active metric learning is the problem of incrementally selecting high-utility\nbatches of training data (typically, ordered triplets) to annotate, in order to\nprogressively improve a learned model of a metric over some input domain as\nrapidly as possible. Standard approaches, which independently assess the\ninformativeness of each triplet in a batch, are susceptible to highly\ncorrelated batches with many redundant triplets and hence low overall utility.\nWhile a recent work \\cite{kumari2020batch} proposes batch-decorrelation\nstrategies for metric learning, they rely on ad hoc heuristics to estimate the\ncorrelation between two triplets at a time. We present a novel batch active\nmetric learning method that leverages the Maximum Entropy Principle to learn\nthe least biased estimate of triplet distribution for a given set of prior\nconstraints. To avoid redundancy between triplets, our method collectively\nselects batches with maximum joint entropy, which simultaneously captures both\ninformativeness and diversity. We take advantage of the submodularity of the\njoint entropy function to construct a tractable solution using an efficient\ngreedy algorithm based on Gram-Schmidt orthogonalization that is provably\n$\\left( 1 - \\frac{1}{e} \\right)$-optimal. Our approach is the first batch\nactive metric learning method to define a unified score that balances\ninformativeness and diversity for an entire batch of triplets. Experiments with\nseveral real-world datasets demonstrate that our algorithm is robust,\ngeneralizes well to different applications and input modalities, and\nconsistently outperforms the state-of-the-art.",
          "link": "http://arxiv.org/abs/2102.07365",
          "publishedOn": "2021-08-03T02:06:28.648Z",
          "wordCount": 736,
          "title": "A Unified Batch Selection Policy for Active Metric Learning. (arXiv:2102.07365v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruilong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1\">David A. Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1\">Angjoo Kanazawa</a>",
          "description": "We present AIST++, a new multi-modal dataset of 3D dance motion and music,\nalong with FACT, a Full-Attention Cross-modal Transformer network for\ngenerating 3D dance motion conditioned on music. The proposed AIST++ dataset\ncontains 5.2 hours of 3D dance motion in 1408 sequences, covering 10 dance\ngenres with multi-view videos with known camera poses -- the largest dataset of\nthis kind to our knowledge. We show that naively applying sequence models such\nas transformers to this dataset for the task of music conditioned 3D motion\ngeneration does not produce satisfactory 3D motion that is well correlated with\nthe input music. We overcome these shortcomings by introducing key changes in\nits architecture design and supervision: FACT model involves a deep cross-modal\ntransformer block with full-attention that is trained to predict $N$ future\nmotions. We empirically show that these changes are key factors in generating\nlong sequences of realistic dance motion that are well-attuned to the input\nmusic. We conduct extensive experiments on AIST++ with user studies, where our\nmethod outperforms recent state-of-the-art methods both qualitatively and\nquantitatively.",
          "link": "http://arxiv.org/abs/2101.08779",
          "publishedOn": "2021-08-03T02:06:28.610Z",
          "wordCount": 670,
          "title": "AI Choreographer: Music Conditioned 3D Dance Generation with AIST++. (arXiv:2101.08779v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingsong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hai Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhimin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>",
          "description": "Automated tagging of video advertisements has been a critical yet challenging\nproblem, and it has drawn increasing interests in last years as its\napplications seem to be evident in many fields. Despite sustainable efforts\nhave been made, the tagging task is still suffered from several challenges,\nsuch as, efficiently feature fusion approach is desirable, but under-explored\nin previous studies. In this paper, we present our approach for Multimodal\nVideo Ads Tagging in the 2021 Tencent Advertising Algorithm Competition.\nSpecifically, we propose a novel multi-modal feature fusion framework, with the\ngoal to combine complementary information from multiple modalities. This\nframework introduces stacking-based ensembling approach to reduce the influence\nof varying levels of noise and conflicts between different modalities. Thus,\nour framework can boost the performance of the tagging task, compared to\nprevious methods. To empirically investigate the effectiveness and robustness\nof the proposed framework, we conduct extensive experiments on the challenge\ndatasets. The obtained results suggest that our framework can significantly\noutperform related approaches and our method ranks as the 1st place on the\nfinal leaderboard, with a Global Average Precision (GAP) of 82.63%. To better\npromote the research in this field, we will release our code in the final\nversion.",
          "link": "http://arxiv.org/abs/2108.00679",
          "publishedOn": "2021-08-03T02:06:28.590Z",
          "wordCount": 658,
          "title": "Multimodal Feature Fusion for Video Advertisements Tagging Via Stacking Ensemble. (arXiv:2108.00679v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.09018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengde Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>",
          "description": "This paper presents an end-to-end semi-supervised object detection approach,\nin contrast to previous more complex multi-stage methods. The end-to-end\ntraining gradually improves pseudo label qualities during the curriculum, and\nthe more and more accurate pseudo labels in turn benefit object detection\ntraining. We also propose two simple yet effective techniques within this\nframework: a soft teacher mechanism where the classification loss of each\nunlabeled bounding box is weighed by the classification score produced by the\nteacher network; a box jittering approach to select reliable pseudo boxes for\nthe learning of box regression. On the COCO benchmark, the proposed approach\noutperforms previous methods by a large margin under various labeling ratios,\ni.e. 1\\%, 5\\% and 10\\%. Moreover, our approach proves to perform also well when\nthe amount of labeled data is relatively large. For example, it can improve a\n40.9 mAP baseline detector trained using the full COCO training set by +3.6\nmAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the\nstate-of-the-art Swin Transformer based object detector (58.9 mAP on test-dev),\nit can still significantly improve the detection accuracy by +1.5 mAP, reaching\n60.4 mAP, and improve the instance segmentation accuracy by +1.2 mAP, reaching\n52.4 mAP. Further incorporating with the Object365 pre-trained model, the\ndetection accuracy reaches 61.3 mAP and the instance segmentation accuracy\nreaches 53.0 mAP, pushing the new state-of-the-art.",
          "link": "http://arxiv.org/abs/2106.09018",
          "publishedOn": "2021-08-09T00:49:28.100Z",
          "wordCount": 715,
          "title": "End-to-End Semi-Supervised Object Detection with Soft Teacher. (arXiv:2106.09018v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alonso_I/0/1/0/all/0/1\">Inigo Alonso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1\">Alberto Sabater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferstl_D/0/1/0/all/0/1\">David Ferstl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1\">Luis Montesano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1\">Ana C. Murillo</a>",
          "description": "This work presents a novel approach for semi-supervised semantic\nsegmentation. The key element of this approach is our contrastive learning\nmodule that enforces the segmentation network to yield similar pixel-level\nfeature representations for same-class samples across the whole dataset. To\nachieve this, we maintain a memory bank continuously updated with relevant and\nhigh-quality feature vectors from labeled data. In an end-to-end training, the\nfeatures from both labeled and unlabeled data are optimized to be similar to\nsame-class samples from the memory bank. Our approach outperforms the current\nstate-of-the-art for semi-supervised semantic segmentation and semi-supervised\ndomain adaptation on well-known public benchmarks, with larger improvements on\nthe most challenging scenarios, i.e., less available labeled data.\nhttps://github.com/Shathe/SemiSeg-Contrastive",
          "link": "http://arxiv.org/abs/2104.13415",
          "publishedOn": "2021-08-09T00:49:28.028Z",
          "wordCount": 610,
          "title": "Semi-Supervised Semantic Segmentation with Pixel-Level Contrastive Learning from a Class-wise Memory Bank. (arXiv:2104.13415v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.01034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kuangyan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yiming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Liao Zhu</a>",
          "description": "This paper has proposed a new baseline deep learning model of more benefits\nfor image classification. Different from the convolutional neural network(CNN)\npractice where filters are trained by back propagation to represent different\npatterns of an image, we are inspired by a method called \"PCANet\" in \"PCANet: A\nSimple Deep Learning Baseline for Image Classification?\" to choose filter\nvectors from basis vectors in frequency domain like Fourier coefficients or\nwavelets without back propagation. Researchers have demonstrated that those\nbasis in frequency domain can usually provide physical insights, which adds to\nthe interpretability of the model by analyzing the frequencies selected.\nBesides, the training process will also be more time efficient, mathematically\nclear and interpretable compared with the \"black-box\" training process of CNN.",
          "link": "http://arxiv.org/abs/2001.01034",
          "publishedOn": "2021-08-09T00:49:27.986Z",
          "wordCount": 606,
          "title": "FrequentNet : A New Interpretable Deep Learning Baseline for Image Classification. (arXiv:2001.01034v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08637",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Rajpal_S/0/1/0/all/0/1\">Sheetal Rajpal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_M/0/1/0/all/0/1\">Manoj Agarwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpal_A/0/1/0/all/0/1\">Ankit Rajpal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lakhyani_N/0/1/0/all/0/1\">Navin Lakhyani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saggar_A/0/1/0/all/0/1\">Arpita Saggar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_N/0/1/0/all/0/1\">Naveen Kumar</a>",
          "description": "Coronaviruses constitute a family of viruses that gives rise to respiratory\ndiseases. As COVID-19 is highly contagious, early diagnosis of COVID-19 is\ncrucial for an effective treatment strategy. However, the RT-PCR test which is\nconsidered to be a gold standard in the diagnosis of COVID-19 suffers from a\nhigh false-negative rate. Chest X-ray (CXR) image analysis has emerged as a\nfeasible and effective diagnostic technique towards this objective. In this\nwork, we propose the COVID-19 classification problem as a three-class\nclassification problem to distinguish between COVID-19, normal, and pneumonia\nclasses. We propose a three-stage framework, named COV-ELM. Stage one deals\nwith preprocessing and transformation while stage two deals with feature\nextraction. These extracted features are passed as an input to the ELM at the\nthird stage, resulting in the identification of COVID-19. The choice of ELM in\nthis work has been motivated by its faster convergence, better generalization\ncapability, and shorter training time in comparison to the conventional\ngradient-based learning algorithms. As bigger and diverse datasets become\navailable, ELM can be quickly retrained as compared to its gradient-based\ncompetitor models. The proposed model achieved a macro average F1-score of 0.95\nand the overall sensitivity of ${0.94 \\pm 0.02} at a 95% confidence interval.\nWhen compared to state-of-the-art machine learning algorithms, the COV-ELM is\nfound to outperform its competitors in this three-class classification\nscenario. Further, LIME has been integrated with the proposed COV-ELM model to\ngenerate annotated CXR images. The annotations are based on the superpixels\nthat have contributed to distinguish between the different classes. It was\nobserved that the superpixels correspond to the regions of the human lungs that\nare clinically observed in COVID-19 and Pneumonia cases.",
          "link": "http://arxiv.org/abs/2007.08637",
          "publishedOn": "2021-08-09T00:49:27.980Z",
          "wordCount": 831,
          "title": "COV-ELM classifier: An Extreme Learning Machine based identification of COVID-19 using Chest X-Ray Images. (arXiv:2007.08637v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1\">Chaitanya Devaguptapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Devansh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_G/0/1/0/all/0/1\">Gaurav Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>",
          "description": "Adversarial robustness of deep learning models has gained much traction in\nthe last few years. Various attacks and defenses are proposed to improve the\nadversarial robustness of modern-day deep learning architectures. While all\nthese approaches help improve the robustness, one promising direction for\nimproving adversarial robustness is un-explored, i.e., the complex topology of\nthe neural network architecture. In this work, we answer the following\nquestion: \"Can the complex topology of a neural network give adversarial\nrobustness without any form of adversarial training?\" empirically by\nexperimenting with different hand-crafted and NAS based architectures. Our\nfindings show that, for small-scale attacks, NAS-based architectures are more\nrobust for small-scale datasets and simple tasks than hand-crafted\narchitectures. However, as the dataset's size or the task's complexity\nincrease, hand-crafted architectures are more robust than NAS-based\narchitectures. We perform the first large scale study to understand adversarial\nrobustness purely from an architectural perspective. Our results show that\nrandom sampling in the search space of DARTS (a popular NAS method) with simple\nensembling can improve the robustness to PGD attack by nearly ~12\\%. We show\nthat NAS, which is popular for SoTA accuracy, can provide adversarial accuracy\nas a free add-on without any form of adversarial training. Our results show\nthat leveraging the power of neural network topology with methods like\nensembles can be an excellent way to achieve adversarial robustness without any\nform of adversarial training. We also introduce a metric that can be used to\ncalculate the trade-off between clean accuracy and adversarial robustness.",
          "link": "http://arxiv.org/abs/2007.08428",
          "publishedOn": "2021-08-09T00:49:27.964Z",
          "wordCount": 753,
          "title": "On Adversarial Robustness: A Neural Architecture Search perspective. (arXiv:2007.08428v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garibaldi_J/0/1/0/all/0/1\">Jonathan M Garibaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>",
          "description": "The goal of representation learning is different from the ultimate objective\nof machine learning such as decision making, it is therefore very difficult to\nestablish clear and direct objectives for training representation learning\nmodels. It has been argued that a good representation should disentangle the\nunderlying variation factors, yet how to translate this into training\nobjectives remains unknown. This paper presents an attempt to establish direct\ntraining criterions and design principles for developing good representation\nlearning models. We propose that a good representation learning model should be\nmaximally expressive, i.e., capable of distinguishing the maximum number of\ninput configurations. We formally define expressiveness and introduce the\nmaximum expressiveness (MEXS) theorem of a general learning model. We propose\nto train a model by maximizing its expressiveness while at the same time\nincorporating general priors such as model smoothness. We present a conscience\ncompetitive learning algorithm which encourages the model to reach its MEXS\nwhilst at the same time adheres to model smoothness prior. We also introduce a\nlabel consistent training (LCT) technique to boost model smoothness by\nencouraging it to assign consistent labels to similar samples. We present\nextensive experimental results to show that our method can indeed design\nrepresentation learning models capable of developing representations that are\nas good as or better than state of the art. We also show that our technique is\ncomputationally efficient, robust against different parameter settings and can\nwork effectively on a variety of datasets. Code available at\nhttps://github.com/qlilx/odgrlm.git",
          "link": "http://arxiv.org/abs/2107.05948",
          "publishedOn": "2021-08-09T00:49:27.890Z",
          "wordCount": 716,
          "title": "On Designing Good Representation Learning Models. (arXiv:2107.05948v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muralikrishnan_S/0/1/0/all/0/1\">Sanjeev Muralikrishnan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Siddhartha Chaudhuri</a> (2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1\">Noam Aigerman</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1\">Vladimir Kim</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1\">Matthew Fisher</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy Mitra</a> (1 and 2) ((1) University College London, (2) Adobe Research, (3) IIT Bombay)",
          "description": "We investigate the problem of training generative models on a very sparse\ncollection of 3D models. We use geometrically motivated energies to augment and\nthus boost a sparse collection of example (training) models. We analyze the\nHessian of the as-rigid-as-possible (ARAP) energy to sample from and project to\nthe underlying (local) shape space, and use the augmented dataset to train a\nvariational autoencoder (VAE). We iterate the process of building latent spaces\nof VAE and augmenting the associated dataset, to progressively reveal a richer\nand more expressive generative space for creating geometrically and\nsemantically valid samples. Our framework allows us to train generative 3D\nmodels even with a small set of good quality 3D models, which are typically\nhard to curate. We extensively evaluate our method against a set of strong\nbaselines, provide ablation studies and demonstrate application towards\nestablishing shape correspondences. We present multiple examples of interesting\nand meaningful shape variations even when starting from as few as 3-10 training\nshapes.",
          "link": "http://arxiv.org/abs/2108.03225",
          "publishedOn": "2021-08-09T00:49:27.882Z",
          "wordCount": 620,
          "title": "GLASS: Geometric Latent Augmentation for Shape Spaces. (arXiv:2108.03225v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1904.02601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1\">Anqi Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xui_L/0/1/0/all/0/1\">Lan Xui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>",
          "description": "In this paper, we present TightCap, a data-driven scheme to capture both the\nhuman shape and dressed garments accurately with only a single 3D human scan,\nwhich enables numerous applications such as virtual try-on, biometrics and body\nevaluation. To break the severe variations of the human poses and garments, we\npropose to model the clothing tightness - the displacements from the garments\nto the human shape implicitly in the global UV texturing domain. To this end,\nwe utilize an enhanced statistical human template and an effective multi-stage\nalignment scheme to map the 3D scan into a hybrid 2D geometry image. Based on\nthis 2D representation, we propose a novel framework to predicted clothing\ntightness via a novel tightness formulation, as well as an effective\noptimization scheme to further reconstruct multi-layer human shape and garments\nunder various clothing categories and human postures. We further propose a new\nclothing tightness dataset (CTD) of human scans with a large variety of\nclothing styles, poses and corresponding ground-truth human shapes to stimulate\nfurther research. Extensive experiments demonstrate the effectiveness of our\nTightCap to achieve high-quality human shape and dressed garments\nreconstruction, as well as the further applications for clothing segmentation,\nretargeting and animation.",
          "link": "http://arxiv.org/abs/1904.02601",
          "publishedOn": "2021-08-09T00:49:27.874Z",
          "wordCount": 693,
          "title": "TightCap: 3D Human Shape Capture with Clothing Tightness Field. (arXiv:1904.02601v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1\">Prashant Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pai_A/0/1/0/all/0/1\">Ajey Pai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1\">Nisarg Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Prasenjit Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makharia_G/0/1/0/all/0/1\">Govind Makharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1\">Prathosh AP</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>",
          "description": "Contrastive Learning (CL) is a recent representation learning approach, which\nencourages inter-class separability and intra-class compactness in learned\nimage representations. Since medical images often contain multiple semantic\nclasses in an image, using CL to learn representations of local features (as\nopposed to global) is important. In this work, we present a novel\nsemi-supervised 2D medical segmentation solution that applies CL on image\npatches, instead of full images. These patches are meaningfully constructed\nusing the semantic information of different classes obtained via pseudo\nlabeling. We also propose a novel consistency regularization (CR) scheme, which\nworks in synergy with CL. It addresses the problem of confirmation bias, and\nencourages better clustering in the feature space. We evaluate our method on\nfour public medical segmentation datasets and a novel histopathology dataset\nthat we introduce. Our method obtains consistent improvements over\nstate-of-the-art semi-supervised segmentation approaches for all datasets.",
          "link": "http://arxiv.org/abs/2106.06801",
          "publishedOn": "2021-08-09T00:49:27.859Z",
          "wordCount": 683,
          "title": "Contrastive Semi-Supervised Learning for 2D Medical Image Segmentation. (arXiv:2106.06801v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songtao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>",
          "description": "In this report, we present some experienced improvements to YOLO series,\nforming a new high-performance detector -- YOLOX. We switch the YOLO detector\nto an anchor-free manner and conduct other advanced detection techniques, i.e.,\na decoupled head and the leading label assignment strategy SimOTA to achieve\nstate-of-the-art results across a large scale range of models: For YOLO-Nano\nwith only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing\nNanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in\nindustry, we boost it to 47.3% AP on COCO, outperforming the current best\npractice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as\nYOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on\nTesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on\nStreaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021)\nusing a single YOLOX-L model. We hope this report can provide useful experience\nfor developers and researchers in practical scenes, and we also provide deploy\nversions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at\nhttps://github.com/Megvii-BaseDetection/YOLOX.",
          "link": "http://arxiv.org/abs/2107.08430",
          "publishedOn": "2021-08-09T00:49:27.834Z",
          "wordCount": 654,
          "title": "YOLOX: Exceeding YOLO Series in 2021. (arXiv:2107.08430v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+VS_V/0/1/0/all/0/1\">Vibashan VS</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oza_P/0/1/0/all/0/1\">Poojan Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "In image fusion, images obtained from different sensors are fused to generate\na single image with enhanced information. In recent years, state-of-the-art\nmethods have adopted Convolution Neural Networks (CNNs) to encode meaningful\nfeatures for image fusion. Specifically, CNN-based methods perform image fusion\nby fusing local features. However, they do not consider long-range dependencies\nthat are present in the image. Transformer-based models are designed to\novercome this by modeling the long-range dependencies with the help of\nself-attention mechanism. This motivates us to propose a novel Image Fusion\nTransformer (IFT) where we develop a transformer-based multi-scale fusion\nstrategy that attends to both local and long-range information (or global\ncontext). The proposed method follows a two-stage training approach. In the\nfirst stage, we train an auto-encoder to extract deep features at multiple\nscales. In the second stage, multi-scale features are fused using a\nSpatio-Transformer (ST) fusion strategy. The ST fusion blocks are comprised of\na CNN and a transformer branch which capture local and long-range features,\nrespectively. Extensive experiments on multiple benchmark datasets show that\nthe proposed method performs better than many competitive fusion algorithms.\nFurthermore, we show the effectiveness of the proposed ST fusion strategy with\nan ablation analysis. The source code is available at:\nhttps://github.com/Vibashan/Image-Fusion-Transformer.",
          "link": "http://arxiv.org/abs/2107.09011",
          "publishedOn": "2021-08-09T00:49:27.815Z",
          "wordCount": 673,
          "title": "Image Fusion Transformer. (arXiv:2107.09011v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lakshika_J/0/1/0/all/0/1\">Jayani P. G. Lakshika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talagala_T/0/1/0/all/0/1\">Thiyanga S. Talagala</a>",
          "description": "Plant species identification is time consuming, costly, and requires lots of\nefforts, and expertise knowledge. In recent, many researchers use deep learning\nmethods to classify plants directly using plant images. While deep learning\nmodels have achieved a great success, the lack of interpretability limit their\nwidespread application. To overcome this, we explore the use of interpretable,\nmeasurable and computer-aided features extracted from plant leaf images. Image\nprocessing is one of the most challenging, and crucial steps in\nfeature-extraction. The purpose of image processing is to improve the leaf\nimage by removing undesired distortion. The main image processing steps of our\nalgorithm involves: i) Convert original image to RGB (Red-Green-Blue) image,\nii) Gray scaling, iii) Gaussian smoothing, iv) Binary thresholding, v) Remove\nstalk, vi) Closing holes, and vii) Resize image. The next step after image\nprocessing is to extract features from plant leaf images. We introduced 52\ncomputationally efficient features to classify plant species. These features\nare mainly classified into four groups as: i) shape-based features, ii)\ncolor-based features, iii) texture-based features, and iv) scagnostic features.\nLength, width, area, texture correlation, monotonicity and scagnostics are to\nname few of them. We explore the ability of features to discriminate the\nclasses of interest under supervised learning and unsupervised learning\nsettings. For that, supervised dimensionality reduction technique, Linear\nDiscriminant Analysis (LDA), and unsupervised dimensionality reduction\ntechnique, Principal Component Analysis (PCA) are used to convert and visualize\nthe images from digital-image space to feature space. The results show that the\nfeatures are sufficient to discriminate the classes of interest under both\nsupervised and unsupervised learning settings.",
          "link": "http://arxiv.org/abs/2106.08077",
          "publishedOn": "2021-08-09T00:49:27.797Z",
          "wordCount": 730,
          "title": "Computer-aided Interpretable Features for Leaf Image Classification. (arXiv:2106.08077v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltanian_Zadeh_S/0/1/0/all/0/1\">Somayyeh Soltanian-Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farsiu_S/0/1/0/all/0/1\">Sina Farsiu</a>",
          "description": "Salient object detection (SOD) is viewed as a pixel-wise saliency modeling\ntask by traditional deep learning-based methods. A limitation of current SOD\nmodels is insufficient utilization of inter-pixel information, which usually\nresults in imperfect segmentation near edge regions and low spatial coherence.\nAs we demonstrate, using a saliency mask as the only label is suboptimal. To\naddress this limitation, we propose a connectivity-based approach called\nbilateral connectivity network (BiconNet), which uses connectivity masks\ntogether with saliency masks as labels for effective modeling of inter-pixel\nrelationships and object saliency. Moreover, we propose a bilateral voting\nmodule to enhance the output connectivity map, and a novel edge feature\nenhancement method that efficiently utilizes edge-specific features. Through\ncomprehensive experiments on five benchmark datasets, we demonstrate that our\nproposed method can be plugged into any existing state-of-the-art\nsaliency-based SOD framework to improve its performance with negligible\nparameter increase.",
          "link": "http://arxiv.org/abs/2103.00334",
          "publishedOn": "2021-08-09T00:49:27.768Z",
          "wordCount": 656,
          "title": "BiconNet: An Edge-preserved Connectivity-based Approach for Salient Object Detection. (arXiv:2103.00334v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gosala_N/0/1/0/all/0/1\">Nikhil Gosala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>",
          "description": "Bird's-Eye-View (BEV) maps have emerged as one of the most powerful\nrepresentations for scene understanding due to their ability to provide rich\nspatial context while being easy to interpret and process. However, generating\nBEV maps requires complex multi-stage paradigms that encapsulate a series of\ndistinct tasks such as depth estimation, ground plane estimation, and semantic\nsegmentation. These sub-tasks are often learned in a disjoint manner which\nprevents the model from holistic reasoning and results in erroneous BEV maps.\nMoreover, existing algorithms only predict the semantics in the BEV space,\nwhich limits their use in applications where the notion of object instances is\ncritical. In this work, we present the first end-to-end learning approach for\ndirectly predicting dense panoptic segmentation maps in the BEV, given a single\nmonocular image in the frontal view (FV). Our architecture follows the top-down\nparadigm and incorporates a novel dense transformer module consisting of two\ndistinct transformers that learn to independently map vertical and flat regions\nin the input image from the FV to the BEV. Additionally, we derive a\nmathematical formulation for the sensitivity of the FV-BEV transformation which\nallows us to intelligently weight pixels in the BEV space to account for the\nvarying descriptiveness across the FV image. Extensive evaluations on the\nKITTI-360 and nuScenes datasets demonstrate that our approach exceeds the\nstate-of-the-art in the PQ metric by 3.61 pp and 4.93 pp respectively.",
          "link": "http://arxiv.org/abs/2108.03227",
          "publishedOn": "2021-08-09T00:49:27.753Z",
          "wordCount": 671,
          "title": "Bird's-Eye-View Panoptic Segmentation Using Monocular Frontal View Images. (arXiv:2108.03227v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Athey_T/0/1/0/all/0/1\">Thomas L. Athey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tward_D/0/1/0/all/0/1\">Daniel J. Tward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_U/0/1/0/all/0/1\">Ulrich Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_M/0/1/0/all/0/1\">Michael I. Miller</a>",
          "description": "Recent advances in brain clearing and imaging have made it possible to image\nentire mammalian brains at sub-micron resolution. These images offer the\npotential to assemble brain-wide atlases of projection neuron morphology, but\nmanual neuron reconstruction remains a bottleneck. In this paper we present a\nprobabilistic method which combines a hidden Markov state process that encodes\nneuron geometric properties with a random field appearance model of the\nflourescence process. Our method utilizes dynamic programming to efficiently\ncompute the global maximizers of what we call the \"most probable\" neuron path.\nWe applied our algorithm to the output of image segmentation models where false\nnegatives severed neuronal processes, and showed that it can follow axons in\nthe presence of noise or nearby neurons. Our method has the potential to be\nintegrated into a semi or fully automated reconstruction pipeline.\nAdditionally, it creates a framework for conditioning the probability to fixed\nstart and endpoints through which users can intervene with hard constraints to,\nfor example, rule out certain reconstructions, or assign axons to particular\ncell bodies.",
          "link": "http://arxiv.org/abs/2106.02701",
          "publishedOn": "2021-08-09T00:49:27.746Z",
          "wordCount": 638,
          "title": "Hidden Markov Modeling for Maximum Likelihood Neuron Reconstruction. (arXiv:2106.02701v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kaidong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathan_M/0/1/0/all/0/1\">Mohammad I. Fathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1\">Krushi Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Cuncong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Ajay Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Amit Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jean S. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>",
          "description": "Colorectal cancer (CRC) is one of the most common types of cancer with a high\nmortality rate. Colonoscopy is the preferred procedure for CRC screening and\nhas proven to be effective in reducing CRC mortality. Thus, a reliable\ncomputer-aided polyp detection and classification system can significantly\nincrease the effectiveness of colonoscopy. In this paper, we create an\nendoscopic dataset collected from various sources and annotate the ground truth\nof polyp location and classification results with the help of experienced\ngastroenterologists. The dataset can serve as a benchmark platform to train and\nevaluate the machine learning models for polyp classification. We have also\ncompared the performance of eight state-of-the-art deep learning-based object\ndetection models. The results demonstrate that deep CNN models are promising in\nCRC screening. This work can serve as a baseline for future research in polyp\ndetection and classification.",
          "link": "http://arxiv.org/abs/2104.10824",
          "publishedOn": "2021-08-09T00:49:27.739Z",
          "wordCount": 622,
          "title": "Colonoscopy Polyp Detection and Classification: Dataset Creation and Comparative Evaluations. (arXiv:2104.10824v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Chengxiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhengping Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1\">Zheng Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Gangyi Ding</a>",
          "description": "Deep generative models have made great progress in synthesizing images with\narbitrary human poses and transferring poses of one person to others. However,\nmost existing approaches explicitly leverage the pose information extracted\nfrom the source images as a conditional input for the generative networks.\nMeanwhile, they usually focus on the visual fidelity of the synthesized images\nbut neglect the inherent consistency, which further confines their performance\nof pose transfer. To alleviate the current limitations and improve the quality\nof the synthesized images, we propose a pose transfer network with Disentangled\nFeature Consistency (DFC-Net) to facilitate human pose transfer. Given a pair\nof images containing the source and target person, DFC-Net extracts pose and\nstatic information from the source and target respectively, then synthesizes an\nimage of the target person with the desired pose from the source. Moreover,\nDFC-Net leverages disentangled feature consistency losses in the adversarial\ntraining to strengthen the transfer coherence and integrates the keypoint\namplifier to enhance the pose feature extraction. Additionally, an unpaired\nsupport dataset Mixamo-Sup providing more extra pose information has been\nfurther utilized during the training to improve the generality and robustness\nof DFC-Net. Extensive experimental results on Mixamo-Pose and EDN-10k have\ndemonstrated DFC-Net achieves state-of-the-art performance on pose transfer.",
          "link": "http://arxiv.org/abs/2107.10984",
          "publishedOn": "2021-08-09T00:49:27.720Z",
          "wordCount": 680,
          "title": "Human Pose Transfer with Disentangled Feature Consistency. (arXiv:2107.10984v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yajun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arai_S/0/1/0/all/0/1\">Shogo Arai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Diyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fangzhou Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosuge_K/0/1/0/all/0/1\">Kazuhiro Kosuge</a>",
          "description": "Instance segmentation is an important pre-processing task in numerous\nreal-world applications, such as robotics, autonomous vehicles, and\nhuman-computer interaction. Compared with the rapid development of deep\nlearning for two-dimensional (2D) image tasks, deep learning-based instance\nsegmentation of 3D point cloud still has a lot of room for development. In\nparticular, distinguishing a large number of occluded objects of the same class\nis a highly challenging problem, which is seen in a robotic bin-picking. In a\nusual bin-picking scene, many indentical objects are stacked together and the\nmodel of the objects is known. Thus, the semantic information can be ignored;\ninstead, the focus in the bin-picking is put on the segmentation of instances.\nBased on this task requirement, we propose a Fast Point Cloud Clustering (FPCC)\nfor instance segmentation of bin-picking scene. FPCC includes a network named\nFPCC-Net and a fast clustering algorithm. FPCC-net has two subnets, one for\ninferring the geometric centers for clustering and the other for describing\nfeatures of each point. FPCC-Net extracts features of each point and infers\ngeometric center points of each instance simultaneously. After that, the\nproposed clustering algorithm clusters the remaining points to the closest\ngeometric center in feature embedding space. Experiments show that FPCC also\nsurpasses the existing works in bin-picking scenes and is more computationally\nefficient. Our code and data are available at https://github.com/xyjbaal/FPCC.",
          "link": "http://arxiv.org/abs/2012.14618",
          "publishedOn": "2021-08-09T00:49:27.712Z",
          "wordCount": 708,
          "title": "FPCC: Fast Point Cloud Clustering for Instance Segmentation. (arXiv:2012.14618v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Windsor_R/0/1/0/all/0/1\">Rhydian Windsor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamaludin_A/0/1/0/all/0/1\">Amir Jamaludin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadir_T/0/1/0/all/0/1\">Timor Kadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>",
          "description": "This paper explores the use of self-supervised deep learning in medical\nimaging in cases where two scan modalities are available for the same subject.\nSpecifically, we use a large publicly-available dataset of over 20,000 subjects\nfrom the UK Biobank with both whole body Dixon technique magnetic resonance\n(MR) scans and also dual-energy x-ray absorptiometry (DXA) scans. We make three\ncontributions: (i) We introduce a multi-modal image-matching contrastive\nframework, that is able to learn to match different-modality scans of the same\nsubject with high accuracy. (ii) Without any adaption, we show that the\ncorrespondences learnt during this contrastive training step can be used to\nperform automatic cross-modal scan registration in a completely unsupervised\nmanner. (iii) Finally, we use these registrations to transfer segmentation maps\nfrom the DXA scans to the MR scans where they are used to train a network to\nsegment anatomical regions without requiring ground-truth MR examples. To aid\nfurther research, our code will be made publicly available.",
          "link": "http://arxiv.org/abs/2107.06652",
          "publishedOn": "2021-08-09T00:49:27.704Z",
          "wordCount": 641,
          "title": "Self-Supervised Multi-Modal Alignment for Whole Body Medical Imaging. (arXiv:2107.06652v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1\">Nathanael L. Baisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zheheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_R/0/1/0/all/0/1\">Ritesh Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Bryan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelov_P/0/1/0/all/0/1\">Plamen Angelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1\">Sue Black</a>",
          "description": "In cases of serious crime, including sexual abuse, often the only available\ninformation with demonstrated potential for identification is images of the\nhands. Since this evidence is captured in uncontrolled situations, it is\ndifficult to analyse. As global approaches to feature comparison are limited in\nthis case, it is important to extend to consider local information. In this\nwork, we propose hand-based person identification by learning both global and\nlocal deep feature representation. Our proposed method, Global and Part-Aware\nNetwork (GPA-Net), creates global and local branches on the conv-layer for\nlearning robust discriminative global and part-level features. For learning the\nlocal (part-level) features, we perform uniform partitioning on the conv-layer\nin both horizontal and vertical directions. We retrieve the parts by conducting\na soft partition without explicitly partitioning the images or requiring\nexternal cues such as pose estimation. We make extensive evaluations on two\nlarge multi-ethnic and publicly available hand datasets, demonstrating that our\nproposed method significantly outperforms competing approaches.",
          "link": "http://arxiv.org/abs/2101.05260",
          "publishedOn": "2021-08-09T00:49:27.687Z",
          "wordCount": 666,
          "title": "Hand-Based Person Identification using Global and Part-Aware Deep Feature Representation Learning. (arXiv:2101.05260v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1\">Bolin Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arridge_S/0/1/0/all/0/1\">Simon R. Arridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucka_F/0/1/0/all/0/1\">Felix Lucka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_B/0/1/0/all/0/1\">Ben T. Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_N/0/1/0/all/0/1\">Nam Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beard_P/0/1/0/all/0/1\">Paul C. Beard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1\">Edward Z. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Betcke_M/0/1/0/all/0/1\">Marta M. Betcke</a>",
          "description": "Curvelet frame is of special significance for photoacoustic tomography (PAT)\ndue to its sparsifying and microlocalisation properties. We derive a one-to-one\nmap between wavefront directions in image and data spaces in PAT which suggests\nnear equivalence between the recovery of the initial pressure and PAT data from\ncompressed/subsampled measurements when assuming sparsity in Curvelet frame. As\nthe latter is computationally more tractable, investigation to which extent\nthis equivalence holds conducted in this paper is of immediate practical\nsignificance. To this end we formulate and compare DR, a two step approach\nbased on the recovery of the complete volume of the photoacoustic data from the\nsubsampled data followed by the acoustic inversion, and p0R, a one step\napproach where the photoacoustic image (the initial pressure, p0) is directly\nrecovered from the subsampled data. Effective representation of the\nphotoacoustic data requires basis defined on the range of the photoacoustic\nforward operator. To this end we propose a novel wedge-restriction of Curvelet\ntransform which enables us to construct such basis. Both recovery problems are\nformulated in a variational framework. As the Curvelet frame is heavily\noverdetermined, we use reweighted l1 norm penalties to enhance the sparsity of\nthe solution. The data reconstruction problem DR is a standard compressed\nsensing recovery problem, which we solve using an ADMMtype algorithm, SALSA.\nSubsequently, the initial pressure is recovered using time reversal as\nimplemented in the k-Wave Toolbox. The p0 reconstruction problem, p0R, aims to\nrecover the photoacoustic image directly via FISTA, or ADMM when in addition\nincluding a non-negativity constraint. We compare and discuss the relative\nmerits of the two approaches and illustrate them on 2D simulated and 3D real\ndata in a fair and rigorous manner.",
          "link": "http://arxiv.org/abs/2011.13080",
          "publishedOn": "2021-08-09T00:49:27.679Z",
          "wordCount": 782,
          "title": "Photoacoustic Reconstruction Using Sparsity in Curvelet Frame: Image versus Data Domain. (arXiv:2011.13080v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rajvinder Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verjans_J/0/1/0/all/0/1\">Johan W. Verjans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>",
          "description": "Anomaly detection with weakly supervised video-level labels is typically\nformulated as a multiple instance learning (MIL) problem, in which we aim to\nidentify snippets containing abnormal events, with each video represented as a\nbag of video snippets. Although current methods show effective detection\nperformance, their recognition of the positive instances, i.e., rare abnormal\nsnippets in the abnormal videos, is largely biased by the dominant negative\ninstances, especially when the abnormal events are subtle anomalies that\nexhibit only small differences compared with normal events. This issue is\nexacerbated in many methods that ignore important video temporal dependencies.\nTo address this issue, we introduce a novel and theoretically sound method,\nnamed Robust Temporal Feature Magnitude learning (RTFM), which trains a feature\nmagnitude learning function to effectively recognise the positive instances,\nsubstantially improving the robustness of the MIL approach to the negative\ninstances from abnormal videos. RTFM also adapts dilated convolutions and\nself-attention mechanisms to capture long- and short-range temporal\ndependencies to learn the feature magnitude more faithfully. Extensive\nexperiments show that the RTFM-enabled MIL model (i) outperforms several\nstate-of-the-art methods by a large margin on four benchmark data sets\n(ShanghaiTech, UCF-Crime, XD-Violence and UCSD-Peds) and (ii) achieves\nsignificantly improved subtle anomaly discriminability and sample efficiency.\nCode is available at https://github.com/tianyu0207/RTFM.",
          "link": "http://arxiv.org/abs/2101.10030",
          "publishedOn": "2021-08-09T00:49:27.654Z",
          "wordCount": 697,
          "title": "Weakly-supervised Video Anomaly Detection with Robust Temporal Feature Magnitude Learning. (arXiv:2101.10030v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.03780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anpei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Ling Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>",
          "description": "Recently, Generative Adversarial Networks (GANs)} have been widely used for\nportrait image generation. However, in the latent space learned by GANs,\ndifferent attributes, such as pose, shape, and texture style, are generally\nentangled, making the explicit control of specific attributes difficult. To\naddress this issue, we propose a SofGAN image generator to decouple the latent\nspace of portraits into two subspaces: a geometry space and a texture space.\nThe latent codes sampled from the two subspaces are fed to two network branches\nseparately, one to generate the 3D geometry of portraits with canonical pose,\nand the other to generate textures. The aligned 3D geometries also come with\nsemantic part segmentation, encoded as a semantic occupancy field (SOF). The\nSOF allows the rendering of consistent 2D semantic segmentation maps at\narbitrary views, which are then fused with the generated texture maps and\nstylized to a portrait photo using our semantic instance-wise (SIW) module.\nThrough extensive experiments, we show that our system can generate high\nquality portrait images with independently controllable geometry and texture\nattributes. The method also generalizes well in various applications such as\nappearance-consistent facial animation and dynamic styling.",
          "link": "http://arxiv.org/abs/2007.03780",
          "publishedOn": "2021-08-09T00:49:27.635Z",
          "wordCount": 672,
          "title": "SofGAN: A Portrait Image Generator with Dynamic Styling. (arXiv:2007.03780v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gojcic_Z/0/1/0/all/0/1\">Zan Gojcic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usvyatsov_M/0/1/0/all/0/1\">Mikhail Usvyatsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieser_A/0/1/0/all/0/1\">Andreas Wieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>",
          "description": "We introduce PREDATOR, a model for pairwise point-cloud registration with\ndeep attention to the overlap region. Different from previous work, our model\nis specifically designed to handle (also) point-cloud pairs with low overlap.\nIts key novelty is an overlap-attention block for early information exchange\nbetween the latent encodings of the two point clouds. In this way the\nsubsequent decoding of the latent representations into per-point features is\nconditioned on the respective other point cloud, and thus can predict which\npoints are not only salient, but also lie in the overlap region between the two\npoint clouds. The ability to focus on points that are relevant for matching\ngreatly improves performance: PREDATOR raises the rate of successful\nregistrations by more than 20% in the low-overlap scenario, and also sets a new\nstate of the art for the 3DMatch benchmark with 89% registration recall.",
          "link": "http://arxiv.org/abs/2011.13005",
          "publishedOn": "2021-08-09T00:49:27.628Z",
          "wordCount": 637,
          "title": "PREDATOR: Registration of 3D Point Clouds with Low Overlap. (arXiv:2011.13005v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.09872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kisuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Ran Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luther_K/0/1/0/all/0/1\">Kyle Luther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seung_H/0/1/0/all/0/1\">H. Sebastian Seung</a>",
          "description": "We show dense voxel embeddings learned via deep metric learning can be\nemployed to produce a highly accurate segmentation of neurons from 3D electron\nmicroscopy images. A \"metric graph\" on a set of edges between voxels is\nconstructed from the dense voxel embeddings generated by a convolutional\nnetwork. Partitioning the metric graph with long-range edges as repulsive\nconstraints yields an initial segmentation with high precision, with\nsubstantial accuracy gain for very thin objects. The convolutional embedding\nnet is reused without any modification to agglomerate the systematic splits\ncaused by complex \"self-contact\" motifs. Our proposed method achieves\nstate-of-the-art accuracy on the challenging problem of 3D neuron\nreconstruction from the brain images acquired by serial section electron\nmicroscopy. Our alternative, object-centered representation could be more\ngenerally useful for other computational tasks in automated neural circuit\nreconstruction.",
          "link": "http://arxiv.org/abs/1909.09872",
          "publishedOn": "2021-08-09T00:49:27.585Z",
          "wordCount": 605,
          "title": "Learning and Segmenting Dense Voxel Embeddings for 3D Neuron Reconstruction. (arXiv:1909.09872v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1901.10233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Volkhonskiy_D/0/1/0/all/0/1\">Denis Volkhonskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muravleva_E/0/1/0/all/0/1\">Ekaterina Muravleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudakov_O/0/1/0/all/0/1\">Oleg Sudakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orlov_D/0/1/0/all/0/1\">Denis Orlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belozerov_B/0/1/0/all/0/1\">Boris Belozerov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koroteev_D/0/1/0/all/0/1\">Dmitry Koroteev</a>",
          "description": "In many branches of earth sciences, the problem of rock study on the\nmicro-level arises. However, a significant number of representative samples is\nnot always feasible. Thus the problem of the generation of samples with similar\nproperties becomes actual. In this paper, we propose a novel deep learning\narchitecture for three-dimensional porous media reconstruction from\ntwo-dimensional slices. We fit a distribution on all possible three-dimensional\nstructures of a specific type based on the given dataset of samples. Then,\ngiven partial information (central slices), we recover the three-dimensional\nstructure around such slices as the most probable one according to that\nconstructed distribution. Technically, we implement this in the form of a deep\nneural network with encoder, generator and discriminator modules. Numerical\nexperiments show that this method provides a good reconstruction in terms of\nMinkowski functionals.",
          "link": "http://arxiv.org/abs/1901.10233",
          "publishedOn": "2021-08-09T00:49:27.551Z",
          "wordCount": 626,
          "title": "Reconstruction of 3D Porous Media From 2D Slices. (arXiv:1901.10233v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03168",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sribhashyam_S/0/1/0/all/0/1\">Sidharth Srivatsav Sribhashyam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salekin_M/0/1/0/all/0/1\">Md Sirajus Salekin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goldgof_D/0/1/0/all/0/1\">Dmitry Goldgof</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>",
          "description": "Spectrograms visualize the frequency components of a given signal which may\nbe an audio signal or even a time-series signal. Audio signals have higher\nsampling rate and high variability of frequency with time. Spectrograms can\ncapture such variations well. But, vital signs which are time-series signals\nhave less sampling frequency and low-frequency variability due to which,\nspectrograms fail to express variations and patterns. In this paper, we propose\na novel solution to introduce frequency variability using frequency modulation\non vital signs. Then we apply spectrograms on frequency modulated signals to\ncapture the patterns. The proposed approach has been evaluated on 4 different\nmedical datasets across both prediction and classification tasks. Significant\nresults are found showing the efficacy of the approach for vital sign signals.\nThe results from the proposed approach are promising with an accuracy of 91.55%\nand 91.67% in prediction and classification tasks respectively.",
          "link": "http://arxiv.org/abs/2108.03168",
          "publishedOn": "2021-08-09T00:49:27.474Z",
          "wordCount": 612,
          "title": "Pattern Recognition in Vital Signs Using Spectrograms. (arXiv:2108.03168v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Unnikrishnan_A/0/1/0/all/0/1\">Aishwarya Unnikrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_J/0/1/0/all/0/1\">Joseph Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1\">Lu Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capodieci_A/0/1/0/all/0/1\">Andrew Capodieci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_P/0/1/0/all/0/1\">Paramsothy Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barton_K/0/1/0/all/0/1\">Kira Barton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1\">Maani Ghaffari</a>",
          "description": "This paper reports on a dynamic semantic mapping framework that incorporates\n3D scene flow measurements into a closed-form Bayesian inference model.\nExistence of dynamic objects in the environment cause artifacts and traces in\ncurrent mapping algorithms, leading to an inconsistent map posterior. We\nleverage state-of-the-art semantic segmentation and 3D flow estimation using\ndeep learning to provide measurements for map inference. We develop a\ncontinuous (i.e., can be queried at arbitrary resolution) Bayesian model that\npropagates the scene with flow and infers a 3D semantic occupancy map with\nbetter performance than its static counterpart. Experimental results using\npublicly available data sets show that the proposed framework generalizes its\npredecessors and improves over direct measurements from deep neural networks\nconsistently.",
          "link": "http://arxiv.org/abs/2108.03180",
          "publishedOn": "2021-08-09T00:49:27.297Z",
          "wordCount": 567,
          "title": "Dynamic Semantic Occupancy Mapping using 3D Scene Flow and Closed-Form Bayesian Inference. (arXiv:2108.03180v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kudisthalert_W/0/1/0/all/0/1\">Wasu Kudisthalert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasupa_K/0/1/0/all/0/1\">Kitsuchart Pasupa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>",
          "description": "Extreme Learning Machine is a powerful classification method very competitive\nexisting classification methods. It is extremely fast at training.\nNevertheless, it cannot perform face verification tasks properly because face\nverification tasks require comparison of facial images of two individuals at\nthe same time and decide whether the two faces identify the same person. The\nstructure of Extreme Leaning Machine was not designed to feed two input data\nstreams simultaneously, thus, in 2-input scenarios Extreme Learning Machine\nmethods are normally applied using concatenated inputs. However, this setup\nconsumes two times more computational resources and it is not optimized for\nrecognition tasks where learning a separable distance metric is critical. For\nthese reasons, we propose and develop a Siamese Extreme Learning Machine\n(SELM). SELM was designed to be fed with two data streams in parallel\nsimultaneously. It utilizes a dual-stream Siamese condition in the extra\nSiamese layer to transform the data before passing it along to the hidden\nlayer. Moreover, we propose a Gender-Ethnicity-Dependent triplet feature\nexclusively trained on a variety of specific demographic groups. This feature\nenables learning and extracting of useful facial features of each group.\nExperiments were conducted to evaluate and compare the performances of SELM,\nExtreme Learning Machine, and DCNN. The experimental results showed that the\nproposed feature was able to perform correct classification at 97.87% accuracy\nand 99.45% AUC. They also showed that using SELM in conjunction with the\nproposed feature provided 98.31% accuracy and 99.72% AUC. They outperformed the\nwell-known DCNN and Extreme Leaning Machine methods by a wide margin.",
          "link": "http://arxiv.org/abs/2108.03140",
          "publishedOn": "2021-08-09T00:49:27.273Z",
          "wordCount": 703,
          "title": "SELM: Siamese Extreme Learning Machine with Application to Face Biometrics. (arXiv:2108.03140v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chartsias_A/0/1/0/all/0/1\">Agisilaos Chartsias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mumith_A/0/1/0/all/0/1\">Angela Mumith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_J/0/1/0/all/0/1\">Jorge Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_K/0/1/0/all/0/1\">Kanwal Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beqiri_A/0/1/0/all/0/1\">Arian Beqiri</a>",
          "description": "Analysis of cardiac ultrasound images is commonly performed in routine\nclinical practice for quantification of cardiac function. Its increasing\nautomation frequently employs deep learning networks that are trained to\npredict disease or detect image features. However, such models are extremely\ndata-hungry and training requires labelling of many thousands of images by\nexperienced clinicians. Here we propose the use of contrastive learning to\nmitigate the labelling bottleneck. We train view classification models for\nimbalanced cardiac ultrasound datasets and show improved performance for\nviews/classes for which minimal labelled data is available. Compared to a naive\nbaseline model, we achieve an improvement in F1 score of up to 26% in those\nviews while maintaining state-of-the-art performance for the views with\nsufficiently many labelled training observations.",
          "link": "http://arxiv.org/abs/2108.03124",
          "publishedOn": "2021-08-09T00:49:27.253Z",
          "wordCount": 570,
          "title": "Contrastive Learning for View Classification of Echocardiograms. (arXiv:2108.03124v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bateson_M/0/1/0/all/0/1\">Mathilde Bateson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kervadec_H/0/1/0/all/0/1\">Hoel Kervadec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lombaert_H/0/1/0/all/0/1\">Herv&#xe9; Lombaert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>",
          "description": "Domain adaptation (DA) has drawn high interest for its capacity to adapt a\nmodel trained on labeled source data to perform well on unlabeled or weakly\nlabeled target data from a different domain. Most common DA techniques require\nconcurrent access to the input images of both the source and target domains.\nHowever, in practice, privacy concerns often impede the availability of source\nimages in the adaptation phase. This is a very frequent DA scenario in medical\nimaging, where, for instance, the source and target images could come from\ndifferent clinical sites. We introduce a source-free domain adaptation for\nimage segmentation. Our formulation is based on minimizing a label-free entropy\nloss defined over target-domain data, which we further guide with a\ndomain-invariant prior on the segmentation regions. Many priors can be derived\nfrom anatomical information. Here, a class ratio prior is estimated from\nanatomical knowledge and integrated in the form of a Kullback Leibler (KL)\ndivergence in our overall loss function. Furthermore, we motivate our overall\nloss with an interesting link to maximizing the mutual information between the\ntarget images and their label predictions. We show the effectiveness of our\nprior aware entropy minimization in a variety of domain-adaptation scenarios,\nwith different modalities and applications, including spine, prostate, and\ncardiac segmentation. Our method yields comparable results to several state of\nthe art adaptation techniques, despite having access to much less information,\nas the source images are entirely absent in our adaptation phase. Our\nstraightforward adaptation strategy uses only one network, contrary to popular\nadversarial techniques, which are not applicable to a source-free DA setting.\nOur framework can be readily used in a breadth of segmentation problems, and\nour code is publicly available: https://github.com/mathilde-b/SFDA",
          "link": "http://arxiv.org/abs/2108.03152",
          "publishedOn": "2021-08-09T00:49:27.245Z",
          "wordCount": 729,
          "title": "Source-Free Domain Adaptation for Image Segmentation. (arXiv:2108.03152v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jooyoung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yonghyun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwon_Y/0/1/0/all/0/1\">Youngjune Gwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "Denoising diffusion probabilistic models (DDPM) have shown remarkable\nperformance in unconditional image generation. However, due to the\nstochasticity of the generative process in DDPM, it is challenging to generate\nimages with the desired semantics. In this work, we propose Iterative Latent\nVariable Refinement (ILVR), a method to guide the generative process in DDPM to\ngenerate high-quality images based on a given reference image. Here, the\nrefinement of the generative process in DDPM enables a single DDPM to sample\nimages from various sets directed by the reference image. The proposed ILVR\nmethod generates high-quality images while controlling the generation. The\ncontrollability of our method allows adaptation of a single DDPM without any\nadditional learning in various image generation tasks, such as generation from\nvarious downsampling factors, multi-domain image translation, paint-to-image,\nand editing with scribbles.",
          "link": "http://arxiv.org/abs/2108.02938",
          "publishedOn": "2021-08-09T00:49:27.238Z",
          "wordCount": 574,
          "title": "ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models. (arXiv:2108.02938v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suarez_I/0/1/0/all/0/1\">Iago Su&#xe1;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buenaposada_J/0/1/0/all/0/1\">Jos&#xe9; M. Buenaposada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumela_L/0/1/0/all/0/1\">Luis Baumela</a>",
          "description": "Detecting local features, such as corners, segments or blobs, is the first\nstep in the pipeline of many Computer Vision applications. Its speed is crucial\nfor real time applications. In this paper we present ELSED, the fastest line\nsegment detector in the literature. The key for its efficiency is a local\nsegment growing algorithm that connects gradient aligned pixels in presence of\nsmall discontinuities. The proposed algorithm not only runs in devices with\nvery low end hardware, but may also be parametrized to foster the detection of\nshort or longer segments, depending on the task at hand. We also introduce new\nmetrics to evaluate the accuracy and repeatability of segment detectors. In our\nexperiments with different public benchmarks we prove that our method is the\nmost efficient in the literature and quantify the accuracy traded for such\ngain.",
          "link": "http://arxiv.org/abs/2108.03144",
          "publishedOn": "2021-08-09T00:49:27.231Z",
          "wordCount": 566,
          "title": "ELSED: Enhanced Line SEgment Drawing. (arXiv:2108.03144v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03131",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+MacLean_A/0/1/0/all/0/1\">Alexander MacLean</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abbasi_S/0/1/0/all/0/1\">Saad Abbasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebadi_A/0/1/0/all/0/1\">Ashkan Ebadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_A/0/1/0/all/0/1\">Andy Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pavlova_M/0/1/0/all/0/1\">Maya Pavlova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gunraj_H/0/1/0/all/0/1\">Hayden Gunraj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xi_P/0/1/0/all/0/1\">Pengcheng Xi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kohli_S/0/1/0/all/0/1\">Sonny Kohli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>",
          "description": "The Coronavirus Disease 2019 (COVID-19) pandemic has impacted many aspects of\nlife globally, and a critical factor in mitigating its effects is screening\nindividuals for infections, thereby allowing for both proper treatment for\nthose individuals as well as action to be taken to prevent further spread of\nthe virus. Point-of-care ultrasound (POCUS) imaging has been proposed as a\nscreening tool as it is a much cheaper and easier to apply imaging modality\nthan others that are traditionally used for pulmonary examinations, namely\nchest x-ray and computed tomography. Given the scarcity of expert radiologists\nfor interpreting POCUS examinations in many highly affected regions around the\nworld, low-cost deep learning-driven clinical decision support solutions can\nhave a large impact during the on-going pandemic. Motivated by this, we\nintroduce COVID-Net US, a highly efficient, self-attention deep convolutional\nneural network design tailored for COVID-19 screening from lung POCUS images.\nExperimental results show that the proposed COVID-Net US can achieve an AUC of\nover 0.98 while achieving 353X lower architectural complexity, 62X lower\ncomputational complexity, and 14.3X faster inference times on a Raspberry Pi.\nClinical validation was also conducted, where select cases were reviewed and\nreported on by a practicing clinician (20 years of clinical practice)\nspecializing in intensive care (ICU) and 15 years of expertise in POCUS\ninterpretation. To advocate affordable healthcare and artificial intelligence\nfor resource-constrained environments, we have made COVID-Net US open source\nand publicly available as part of the COVID-Net open source initiative.",
          "link": "http://arxiv.org/abs/2108.03131",
          "publishedOn": "2021-08-09T00:49:27.223Z",
          "wordCount": 781,
          "title": "COVID-Net US: A Tailored, Highly Efficient, Self-Attention Deep Convolutional Neural Network Design for Detection of COVID-19 Patient Cases from Point-of-care Ultrasound Imaging. (arXiv:2108.03131v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Keren Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianbing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Appearance and motion are two important sources of information in video\nobject segmentation (VOS). Previous methods mainly focus on using simplex\nsolutions, lowering the upper bound of feature collaboration among and across\nthese two cues. In this paper, we study a novel framework, termed the FSNet\n(Full-duplex Strategy Network), which designs a relational cross-attention\nmodule (RCAM) to achieve the bidirectional message propagation across embedding\nsubspaces. Furthermore, the bidirectional purification module (BPM) is\nintroduced to update the inconsistent features between the spatial-temporal\nembeddings, effectively improving the model robustness. By considering the\nmutual restraint within the full-duplex strategy, our FSNet performs the\ncross-modal feature-passing (i.e., transmission and receiving) simultaneously\nbefore the fusion and decoding stage, making it robust to various challenging\nscenarios (e.g., motion blur, occlusion) in VOS. Extensive experiments on five\npopular benchmarks (i.e., DAVIS$_{16}$, FBMS, MCL, SegTrack-V2, and\nDAVSOD$_{19}$) show that our FSNet outperforms other state-of-the-arts for both\nthe VOS and video salient object detection tasks.",
          "link": "http://arxiv.org/abs/2108.03151",
          "publishedOn": "2021-08-09T00:49:27.203Z",
          "wordCount": 600,
          "title": "Full-Duplex Strategy for Video Object Segmentation. (arXiv:2108.03151v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhiqing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fengkai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuo Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yangyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huici Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhiyong Feng</a>",
          "description": "With autonomous driving developing in a booming stage, accurate object\ndetection in complex scenarios attract wide attention to ensure the safety of\nautonomous driving. Millimeter wave (mmWave) radar and vision fusion is a\nmainstream solution for accurate obstacle detection. This article presents a\ndetailed survey on mmWave radar and vision fusion based obstacle detection\nmethods. Firstly, we introduce the tasks, evaluation criteria and datasets of\nobject detection for autonomous driving. Then, the process of mmWave radar and\nvision fusion is divided into three parts: sensor deployment, sensor\ncalibration and sensor fusion, which are reviewed comprehensively. Especially,\nwe classify the fusion methods into data level, decision level and feature\nlevel fusion methods. Besides, we introduce the fusion of lidar and vision in\nautonomous driving in the aspects of obstacle detection, object classification\nand road segmentation, which is promising in the future. Finally, we summarize\nthis article.",
          "link": "http://arxiv.org/abs/2108.03004",
          "publishedOn": "2021-08-09T00:49:27.196Z",
          "wordCount": 595,
          "title": "MmWave Radar and Vision Fusion based Object Detection for Autonomous Driving: A Survey. (arXiv:2108.03004v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenglong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Wen Chen</a>",
          "description": "Contrastive learning, which aims at minimizing the distance between positive\npairs while maximizing that of negative ones, has been widely and successfully\napplied in unsupervised feature learning, where the design of positive and\nnegative (pos/neg) pairs is one of its keys. In this paper, we attempt to\ndevise a feature-level data manipulation, differing from data augmentation, to\nenhance the generic contrastive self-supervised learning. To this end, we first\ndesign a visualization scheme for pos/neg score (Pos/neg score indicates cosine\nsimilarity of pos/neg pair.) distribution, which enables us to analyze,\ninterpret and understand the learning process. To our knowledge, this is the\nfirst attempt of its kind. More importantly, leveraging this tool, we gain some\nsignificant observations, which inspire our novel Feature Transformation\nproposals including the extrapolation of positives. This operation creates\nharder positives to boost the learning because hard positives enable the model\nto be more view-invariant. Besides, we propose the interpolation among\nnegatives, which provides diversified negatives and makes the model more\ndiscriminative. It is the first attempt to deal with both challenges\nsimultaneously. Experiment results show that our proposed Feature\nTransformation can improve at least 6.0% accuracy on ImageNet-100 over MoCo\nbaseline, and about 2.0% accuracy on ImageNet-1K over the MoCoV2 baseline.\nTransferring to the downstream tasks successfully demonstrate our model is less\ntask-bias. Visualization tools and codes\nhttps://github.com/DTennant/CL-Visualizing-Feature-Transformation .",
          "link": "http://arxiv.org/abs/2108.02982",
          "publishedOn": "2021-08-09T00:49:27.189Z",
          "wordCount": 674,
          "title": "Improving Contrastive Learning by Visualizing Feature Transformation. (arXiv:2108.02982v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1\">Antonio Pepe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1\">Christina Gsaxner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fen-hua Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1\">Jens Kleesiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F. Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1\">Jan Egger</a>",
          "description": "The aortic vessel tree is composed of the aorta and its branching arteries,\nand plays a key role in supplying the whole body with blood. Aortic diseases,\nlike aneurysms or dissections, can lead to an aortic rupture, whose treatment\nwith open surgery is highly risky. Therefore, patients commonly undergo drug\ntreatment under constant monitoring, which requires regular inspections of the\nvessels through imaging. The standard imaging modality for diagnosis and\nmonitoring is computed tomography (CT), which can provide a detailed picture of\nthe aorta and its branching vessels if combined with a contrast agent,\nresulting in a CT angiography (CTA). Optimally, the whole aortic vessel tree\ngeometry from consecutive CTAs, are overlaid and compared. This allows to not\nonly detect changes in the aorta, but also more peripheral vessel tree changes,\ncaused by the primary pathology or newly developed. When performed manually,\nthis reconstruction requires slice by slice contouring, which could easily take\na whole day for a single aortic vessel tree and, hence, is not feasible in\nclinical practice. Automatic or semi-automatic vessel tree segmentation\nalgorithms, on the other hand, can complete this task in a fraction of the\nmanual execution time and run in parallel to the clinical routine of the\nclinicians. In this paper, we systematically review computing techniques for\nthe automatic and semi-automatic segmentation of the aortic vessel tree. The\nreview concludes with an in-depth discussion on how close these\nstate-of-the-art approaches are to an application in clinical practice and how\nactive this research field is, taking into account the number of publications,\ndatasets and challenges.",
          "link": "http://arxiv.org/abs/2108.02998",
          "publishedOn": "2021-08-09T00:49:27.181Z",
          "wordCount": 744,
          "title": "AI-based Aortic Vessel Tree Segmentation for Cardiovascular Diseases Treatment: Status Quo. (arXiv:2108.02998v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03116",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1\">Kai Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weixing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Feng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Dongdong Zheng</a>",
          "description": "Rotating object detection has wide applications in aerial photographs, remote\nsensing images, UAVs, etc. At present, most of the rotating object detection\ndatasets focus on the field of remote sensing, and these images are usually\nshot in high-altitude scenes. However, image datasets captured at low-altitude\nareas also should be concerned, such as drone-based datasets. So we present a\nlow-altitude dronebased dataset, named UAV-ROD, aiming to promote the research\nand development in rotating object detection and UAV applications. The UAV-ROD\nconsists of 1577 images and 30,090 instances of car category annotated by\noriented bounding boxes. In particular, The UAV-ROD can be utilized for the\nrotating object detection, vehicle orientation recognition and object counting\ntasks. Compared with horizontal object detection, the regression stage of the\nrotation detection is a tricky problem. In this paper, we propose a rotating\nobject detector TS4Net, which contains anchor refinement module (ARM) and\ntwo-stage sample selective strategy (TS4). The ARM can convert preseted\nhorizontal anchors into high-quality rotated anchors through twostage anchor\nrefinement. The TS4 module utilizes different constrained sample selective\nstrategies to allocate positive and negative samples, which is adaptive to the\nregression task in different stages. Benefiting from the ARM and TS4, the\nTS4Net can achieve superior performance for rotating object detection solely\nwith one preseted horizontal anchor. Extensive experimental results on UAV-ROD\ndataset and three remote sensing datasets DOTA, HRSC2016 and UCAS-AOD\ndemonstrate that our method achieves competitive performance against most\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.03116",
          "publishedOn": "2021-08-09T00:49:27.173Z",
          "wordCount": 683,
          "title": "TS4Net: Two-Stage Sample Selective Strategy for Rotating Object Detection. (arXiv:2108.03116v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demir_U/0/1/0/all/0/1\">Ufuk Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozer_A/0/1/0/all/0/1\">Atahan Ozer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahin_Y/0/1/0/all/0/1\">Yusuf H. Sahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_G/0/1/0/all/0/1\">Gozde Unal</a>",
          "description": "In recent years, deep learning based methods have shown success in essential\nmedical image analysis tasks such as segmentation. Post-processing and refining\nthe results of segmentation is a common practice to decrease the\nmisclassifications originating from the segmentation network. In addition to\nwidely used methods like Conditional Random Fields (CRFs) which focus on the\nstructure of the segmented volume/area, a graph-based recent approach makes use\nof certain and uncertain points in a graph and refines the segmentation\naccording to a small graph convolutional network (GCN). However, there are two\ndrawbacks of the approach: most of the edges in the graph are assigned randomly\nand the GCN is trained independently from the segmentation network. To address\nthese issues, we define a new neighbor-selection mechanism according to feature\ndistances and combine the two networks in the training procedure. According to\nthe experimental results on pancreas segmentation from Computed Tomography (CT)\nimages, we demonstrate improvement in the quantitative measures. Also,\nexamining the dynamic neighbors created by our method, edges between\nsemantically similar image parts are observed. The proposed method also shows\nqualitative enhancements in the segmentation maps, as demonstrated in the\nvisual results.",
          "link": "http://arxiv.org/abs/2108.03117",
          "publishedOn": "2021-08-09T00:49:27.153Z",
          "wordCount": 651,
          "title": "Uncertainty-Based Dynamic Graph Neighborhoods For Medical Segmentation. (arXiv:2108.03117v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boxer_L/0/1/0/all/0/1\">Laurence Boxer</a>",
          "description": "Two objects may be close in the Hausdor? metric, yet have very different\ngeometric and topological properties. We examine other methods of comparing\ndigital images such that objects close in each of these measures have some\nsimilar geometric or topological property. Such measures may be combined with\nthe Hausdorff metric to yield a metric in which close images are similar with\nrespect to multiple properties.",
          "link": "http://arxiv.org/abs/2108.03114",
          "publishedOn": "2021-08-09T00:49:27.144Z",
          "wordCount": 502,
          "title": "Beyond the Hausdorff Metric in Digital Topology. (arXiv:2108.03114v1 [cs.CG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03002",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhang_H/0/1/0/all/0/1\">HongBing Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1\">XinYi Liu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fan_H/0/1/0/all/0/1\">HongTao Fan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1\">YaJing Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ye_Y/0/1/0/all/0/1\">Yinlin Ye</a>",
          "description": "More recently, an Approximate SVD Based on Qatar Riyal (QR) Decomposition\n(CSVD-QR) method for matrix complete problem is presented, whose computational\ncomplexity is $O(r^2(m+n))$, which is mainly due to that $r$ is far less than\n$\\min\\{m,n\\}$, where $r$ represents the largest number of singular values of\nmatrix $X$. What is particularly interesting is that after replacing the\nnuclear norm with the $L_{2,1}$ norm proposed based on this decomposition, as\nthe upper bound of the nuclear norm, when the intermediate matrix $D$ in its\ndecomposition is close to the diagonal matrix, it will converge to the nuclear\nnorm, and is exactly equal, when the $D$ matrix is equal to the diagonal\nmatrix, to the nuclear norm, which ingeniously avoids the calculation of the\nsingular value of the matrix. To the best of our knowledge, there is no\nliterature to generalize and apply it to solve tensor complete problems.\nInspired by this, in this paper we propose a class of tensor minimization model\nbased on $L_{2,1}$ norm and CSVD-QR method for the tensor complete problem,\nwhich is convex and therefore has a global minimum solution.",
          "link": "http://arxiv.org/abs/2108.03002",
          "publishedOn": "2021-08-09T00:49:27.136Z",
          "wordCount": 640,
          "title": "Fast and Accurate Low-Rank Tensor Completion Methods Based on QR Decomposition and $L_{2,1}$ Norm Minimization. (arXiv:2108.03002v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shuvendu Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>",
          "description": "We propose a self-supervised contrastive learning approach for facial\nexpression recognition (FER) in videos. We propose a novel temporal\nsampling-based augmentation scheme to be utilized in addition to standard\nspatial augmentations used for contrastive learning. Our proposed temporal\naugmentation scheme randomly picks from one of three temporal sampling\ntechniques: (1) pure random sampling, (2) uniform sampling, and (3) sequential\nsampling. This is followed by a combination of up to three standard spatial\naugmentations. We then use a deep R(2+1)D network for FER, which we train in a\nself-supervised fashion based on the augmentations and subsequently fine-tune.\nExperiments are performed on the Oulu-CASIA dataset and the performance is\ncompared to other works in FER. The results indicate that our method achieves\nan accuracy of 89.4%, setting a new state-of-the-art by outperforming other\nworks. Additional experiments and analysis confirm the considerable\ncontribution of the proposed temporal augmentation versus the existing spatial\nones.",
          "link": "http://arxiv.org/abs/2108.03064",
          "publishedOn": "2021-08-09T00:49:27.128Z",
          "wordCount": 599,
          "title": "Spatiotemporal Contrastive Learning of Facial Expressions in Videos. (arXiv:2108.03064v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03138",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mason_H/0/1/0/all/0/1\">Harry Mason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristoni_L/0/1/0/all/0/1\">Lorenzo Cristoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walden_A/0/1/0/all/0/1\">Andrew Walden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazzari_R/0/1/0/all/0/1\">Roberto Lazzari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulimood_T/0/1/0/all/0/1\">Thomas Pulimood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandjean_L/0/1/0/all/0/1\">Louis Grandjean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wheeler_Kingshott_C/0/1/0/all/0/1\">Claudia AM Gandini Wheeler-Kingshott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baum_Z/0/1/0/all/0/1\">Zachary MC Baum</a>",
          "description": "Lung ultrasound imaging has been shown effective in detecting typical\npatterns for interstitial pneumonia, as a point-of-care tool for both patients\nwith COVID-19 and other community-acquired pneumonia (CAP). In this work, we\nfocus on the hyperechoic B-line segmentation task. Using deep neural networks,\nwe automatically outline the regions that are indicative of pathology-sensitive\nartifacts and their associated sonographic patterns. With a real-world\ndata-scarce scenario, we investigate approaches to utilize both COVID-19 and\nCAP lung ultrasound data to train the networks; comparing fine-tuning and\nunsupervised domain adaptation. Segmenting either type of lung condition at\ninference may support a range of clinical applications during evolving epidemic\nstages, but also demonstrates value in resource-constrained clinical scenarios.\nAdapting real clinical data acquired from COVID-19 patients to those from CAP\npatients significantly improved Dice scores from 0.60 to 0.87 (p < 0.001) and\nfrom 0.43 to 0.71 (p < 0.001), on independent COVID-19 and CAP test cases,\nrespectively. It is of practical value that the improvement was demonstrated\nwith only a small amount of data in both training and adaptation data sets, a\ncommon constraint for deploying machine learning models in clinical practice.\nInterestingly, we also report that the inverse adaptation, from labelled CAP\ndata to unlabeled COVID-19 data, did not demonstrate an improvement when tested\non either condition. Furthermore, we offer a possible explanation that\ncorrelates the segmentation performance to label consistency and data domain\ndiversity in this point-of-care lung ultrasound application.",
          "link": "http://arxiv.org/abs/2108.03138",
          "publishedOn": "2021-08-09T00:49:27.121Z",
          "wordCount": 740,
          "title": "Lung Ultrasound Segmentation and Adaptation between COVID-19 and Community-Acquired Pneumonia. (arXiv:2108.03138v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiangxi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+lin_G/0/1/0/all/0/1\">Guosheng lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>",
          "description": "Currently, the state-of-the-art methods treat few-shot semantic segmentation\ntask as a conditional foreground-background segmentation problem, assuming each\nclass is independent. In this paper, we introduce the concept of meta-class,\nwhich is the meta information (e.g. certain middle-level features) shareable\namong all classes. To explicitly learn meta-class representations in few-shot\nsegmentation task, we propose a novel Meta-class Memory based few-shot\nsegmentation method (MM-Net), where we introduce a set of learnable memory\nembeddings to memorize the meta-class information during the base class\ntraining and transfer to novel classes during the inference stage. Moreover,\nfor the $k$-shot scenario, we propose a novel image quality measurement module\nto select images from the set of support images. A high-quality class prototype\ncould be obtained with the weighted sum of support image features based on the\nquality measure. Experiments on both PASCAL-$5^i$ and COCO dataset shows that\nour proposed method is able to achieve state-of-the-art results in both 1-shot\nand 5-shot settings. Particularly, our proposed MM-Net achieves 37.5\\% mIoU on\nthe COCO dataset in 1-shot setting, which is 5.1\\% higher than the previous\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2108.02958",
          "publishedOn": "2021-08-09T00:49:27.112Z",
          "wordCount": 615,
          "title": "Learning Meta-class Memory for Few-Shot Semantic Segmentation. (arXiv:2108.02958v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1\">Shunda Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1\">Liang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>",
          "description": "Single image dehazing is a challenging task, for which the domain shift\nbetween synthetic training data and real-world testing images usually leads to\ndegradation of existing methods. To address this issue, we propose a novel\nimage dehazing framework collaborating with unlabeled real data. First, we\ndevelop a disentangled image dehazing network (DID-Net), which disentangles the\nfeature representations into three component maps, i.e. the latent haze-free\nimage, the transmission map, and the global atmospheric light estimate,\nrespecting the physical model of a haze process. Our DID-Net predicts the three\ncomponent maps by progressively integrating features across scales, and refines\neach map by passing an independent refinement network. Then a\ndisentangled-consistency mean-teacher network (DMT-Net) is employed to\ncollaborate unlabeled real data for boosting single image dehazing.\nSpecifically, we encourage the coarse predictions and refinements of each\ndisentangled component to be consistent between the student and teacher\nnetworks by using a consistency loss on unlabeled real data. We make comparison\nwith 13 state-of-the-art dehazing methods on a new collected dataset (Haze4K)\nand two widely-used dehazing datasets (i.e., SOTS and HazeRD), as well as on\nreal-world hazy images. Experimental results demonstrate that our method has\nobvious quantitative and qualitative improvements over the existing methods.",
          "link": "http://arxiv.org/abs/2108.02934",
          "publishedOn": "2021-08-09T00:49:27.086Z",
          "wordCount": 657,
          "title": "From Synthetic to Real: Image Dehazing Collaborating with Unlabeled Real Data. (arXiv:2108.02934v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuejiao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_K/0/1/0/all/0/1\">Kea Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1\">Tyler Derr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1\">Eirini Ntoutsi</a>",
          "description": "While image understanding on recognition-level has achieved remarkable\nadvancements, reliable visual scene understanding requires comprehensive image\nunderstanding on recognition-level but also cognition-level, which calls for\nexploiting the multi-source information as well as learning different levels of\nunderstanding and extensive commonsense knowledge. In this paper, we propose a\nnovel Cognitive Attention Network (CAN) for visual commonsense reasoning to\nachieve interpretable visual understanding. Specifically, we first introduce an\nimage-text fusion module to fuse information from images and text collectively.\nSecond, a novel inference module is designed to encode commonsense among image,\nquery and response. Extensive experiments on large-scale Visual Commonsense\nReasoning (VCR) benchmark dataset demonstrate the effectiveness of our\napproach. The implementation is publicly available at\nhttps://github.com/tanjatang/CAN",
          "link": "http://arxiv.org/abs/2108.02924",
          "publishedOn": "2021-08-09T00:49:27.078Z",
          "wordCount": 562,
          "title": "Interpretable Visual Understanding with Cognitive Attention Network. (arXiv:2108.02924v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1\">Liangyu Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanjie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>",
          "description": "Labeling is onerous for crowd counting as it should annotate each individual\nin crowd images. Recently, several methods have been proposed for\nsemi-supervised crowd counting to reduce the labeling efforts. Given a limited\nlabeling budget, they typically select a few crowd images and densely label all\nindividuals in each of them. Despite the promising results, we argue the\nNone-or-All labeling strategy is suboptimal as the densely labeled individuals\nin each crowd image usually appear similar while the massive unlabeled crowd\nimages may contain entirely diverse individuals. To this end, we propose to\nbreak the labeling chain of previous methods and make the first attempt to\nreduce spatial labeling redundancy for semi-supervised crowd counting. First,\ninstead of annotating all the regions in each crowd image, we propose to\nannotate the representative ones only. We analyze the region representativeness\nfrom both vertical and horizontal directions, and formulate them as cluster\ncenters of Gaussian Mixture Models. Additionally, to leverage the rich\nunlabeled regions, we exploit the similarities among individuals in each crowd\nimage to directly supervise the unlabeled regions via feature propagation\ninstead of the error-prone label propagation employed in the previous methods.\nIn this way, we can transfer the original spatial labeling redundancy caused by\nindividual similarities to effective supervision signals on the unlabeled\nregions. Extensive experiments on the widely-used benchmarks demonstrate that\nour method can outperform previous best approaches by a large margin.",
          "link": "http://arxiv.org/abs/2108.02970",
          "publishedOn": "2021-08-09T00:49:27.071Z",
          "wordCount": 678,
          "title": "Reducing Spatial Labeling Redundancy for Semi-supervised Crowd Counting. (arXiv:2108.02970v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wen-Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Min-Chun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chu-Song Chen</a>",
          "description": "Geometry-aware modules are widely applied in recent deep learning\narchitectures for scene representation and rendering. However, these modules\nrequire intrinsic camera information that might not be obtained accurately. In\nthis paper, we propose a Spatial Transformation Routing (STR) mechanism to\nmodel the spatial properties without applying any geometric prior. The STR\nmechanism treats the spatial transformation as the message passing process, and\nthe relation between the view poses and the routing weights is modeled by an\nend-to-end trainable neural network. Besides, an Occupancy Concept Mapping\n(OCM) framework is proposed to provide explainable rationals for scene-fusion\nprocesses. We conducted experiments on several datasets and show that the\nproposed STR mechanism improves the performance of the Generative Query Network\n(GQN). The visualization results reveal that the routing process can pass the\nobserved information from one location of some view to the associated location\nin the other view, which demonstrates the advantage of the proposed model in\nterms of spatial cognition.",
          "link": "http://arxiv.org/abs/2108.03072",
          "publishedOn": "2021-08-09T00:49:27.063Z",
          "wordCount": 603,
          "title": "STR-GQN: Scene Representation and Rendering for Unknown Cameras Based on Spatial Transformation Routing. (arXiv:2108.03072v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+lu_Z/0/1/0/all/0/1\">Zhihe lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Sen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>",
          "description": "A few-shot semantic segmentation model is typically composed of a CNN\nencoder, a CNN decoder and a simple classifier (separating foreground and\nbackground pixels). Most existing methods meta-learn all three model components\nfor fast adaptation to a new class. However, given that as few as a single\nsupport set image is available, effective model adaption of all three\ncomponents to the new class is extremely challenging. In this work we propose\nto simplify the meta-learning task by focusing solely on the simplest\ncomponent, the classifier, whilst leaving the encoder and decoder to\npre-training. We hypothesize that if we pre-train an off-the-shelf segmentation\nmodel over a set of diverse training classes with sufficient annotations, the\nencoder and decoder can capture rich discriminative features applicable for any\nunseen classes, rendering the subsequent meta-learning stage unnecessary. For\nthe classifier meta-learning, we introduce a Classifier Weight Transformer\n(CWT) designed to dynamically adapt the supportset trained classifier's weights\nto each query image in an inductive way. Extensive experiments on two standard\nbenchmarks show that despite its simplicity, our method outperforms the\nstate-of-the-art alternatives, often by a large margin.Code is available on\nhttps://github.com/zhiheLu/CWTfor-FSS.",
          "link": "http://arxiv.org/abs/2108.03032",
          "publishedOn": "2021-08-09T00:49:27.056Z",
          "wordCount": 637,
          "title": "Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer. (arXiv:2108.03032v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03001",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chenqian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanlu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Lyna Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiaotian Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuqing Yang</a>",
          "description": "Architecture performance predictors have been widely used in neural\narchitecture search (NAS). Although they are shown to be simple and effective,\nthe optimization objectives in previous arts (e.g., precise accuracy estimation\nor perfect ranking of all architectures in the space) did not capture the\nranking nature of NAS. In addition, a large number of ground-truth\narchitecture-accuracy pairs are usually required to build a reliable predictor,\nmaking the process too computationally expensive. To overcome these, in this\npaper, we look at NAS from a novel point of view and introduce Learning to Rank\n(LTR) methods to select the best (ace) architectures from a space.\nSpecifically, we propose to use Normalized Discounted Cumulative Gain (NDCG) as\nthe target metric and LambdaRank as the training algorithm. We also propose to\nleverage weak supervision from weight sharing by pretraining architecture\nrepresentation on weak labels obtained from the super-net and then finetuning\nthe ranking model using a small number of architectures trained from scratch.\nExtensive experiments on NAS benchmarks and large-scale search spaces\ndemonstrate that our approach outperforms SOTA with a significantly reduced\nsearch cost.",
          "link": "http://arxiv.org/abs/2108.03001",
          "publishedOn": "2021-08-09T00:49:27.038Z",
          "wordCount": 642,
          "title": "AceNAS: Learning to Rank Ace Neural Architectures with Weak Supervision of Weight Sharing. (arXiv:2108.03001v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbato_F/0/1/0/all/0/1\">Francesco Barbato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1\">Umberto Michieli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toldo_M/0/1/0/all/0/1\">Marco Toldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1\">Pietro Zanuttigh</a>",
          "description": "Deep learning models achieve outstanding accuracy in semantic segmentation,\nhowever they require a huge amount of labeled data for their optimization.\nHence, domain adaptation approaches have come into play to transfer knowledge\nacquired on a label-abundant source domain to a related label-scarce target\ndomain. However, such models do not generalize well to data with statistical\nproperties not perfectly matching the ones of the training samples. In this\nwork, we design and carefully analyze multiple latent space-shaping\nregularization strategies that work in conjunction to reduce the domain\ndiscrepancy in semantic segmentation. In particular, we devise a feature\nclustering strategy to increase domain alignment, a feature perpendicularity\nconstraint to space apart feature belonging to different semantic classes,\nincluding those not present in the current batch, and a feature norm alignment\nstrategy to separate active and inactive channels. Additionally, we propose a\nnovel performance metric to capture the relative efficacy of an adaptation\nstrategy compared to supervised training. We verify the effectiveness of our\nframework in synthetic-to-real and real-to-real adaptation scenarios,\noutperforming previous state-of-the-art methods on multiple road scenes\nbenchmarks and using different backbones.",
          "link": "http://arxiv.org/abs/2108.03021",
          "publishedOn": "2021-08-09T00:49:27.029Z",
          "wordCount": 637,
          "title": "Adapting Segmentation Networks to New Domains by Disentangling Latent Representations. (arXiv:2108.03021v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1\">Weiqing Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haisheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuqiang Jiang</a>",
          "description": "Background: Maintaining a healthy diet is vital to avoid health-related\nissues, e.g., undernutrition, obesity and many non-communicable diseases. An\nindispensable part of the health diet is dietary assessment. Traditional manual\nrecording methods are burdensome and contain substantial biases and errors.\nRecent advances in Artificial Intelligence, especially computer vision\ntechnologies, have made it possible to develop automatic dietary assessment\nsolutions, which are more convenient, less time-consuming and even more\naccurate to monitor daily food intake.\n\nScope and approach: This review presents one unified Vision-Based Dietary\nAssessment (VBDA) framework, which generally consists of three stages: food\nimage analysis, volume estimation and nutrient derivation. Vision-based food\nanalysis methods, including food recognition, detection and segmentation, are\nsystematically summarized, and methods of volume estimation and nutrient\nderivation are also given. The prosperity of deep learning makes VBDA gradually\nmove to an end-to-end implementation, which applies food images to a single\nnetwork to directly estimate the nutrition. The recently proposed end-to-end\nmethods are also discussed. We further analyze existing dietary assessment\ndatasets, indicating that one large-scale benchmark is urgently needed, and\nfinally highlight key challenges and future trends for VBDA.\n\nKey findings and conclusions: After thorough exploration, we find that\nmulti-task end-to-end deep learning approaches are one important trend of VBDA.\nDespite considerable research progress, many challenges remain for VBDA due to\nthe meal complexity. We also provide the latest ideas for future development of\nVBDA, e.g., fine-grained food analysis and accurate volume estimation. This\nsurvey aims to encourage researchers to propose more practical solutions for\nVBDA.",
          "link": "http://arxiv.org/abs/2108.02947",
          "publishedOn": "2021-08-09T00:49:27.021Z",
          "wordCount": 689,
          "title": "Vision-Based Food Analysis for Automatic Dietary Assessment. (arXiv:2108.02947v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Che_K/0/1/0/all/0/1\">Kaiwei Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chengwei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yibing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1\">Nachuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiankun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1\">Max Q.-H. Meng</a>",
          "description": "Colonoscopy is a standard imaging tool for visualizing the entire\ngastrointestinal (GI) tract of patients to capture lesion areas. However, it\ntakes the clinicians excessive time to review a large number of images\nextracted from colonoscopy videos. Thus, automatic detection of biological\nanatomical landmarks within the colon is highly demanded, which can help reduce\nthe burden of clinicians by providing guidance information for the locations of\nlesion areas. In this article, we propose a novel deep learning-based approach\nto detect biological anatomical landmarks in colonoscopy videos. First, raw\ncolonoscopy video sequences are pre-processed to reject interference frames.\nSecond, a ResNet-101 based network is used to detect three biological\nanatomical landmarks separately to obtain the intermediate detection results.\nThird, to achieve more reliable localization of the landmark periods within the\nwhole video period, we propose to post-process the intermediate detection\nresults by identifying the incorrectly predicted frames based on their temporal\ndistribution and reassigning them back to the correct class. Finally, the\naverage detection accuracy reaches 99.75\\%. Meanwhile, the average IoU of 0.91\nshows a high degree of similarity between our predicted landmark periods and\nground truth. The experimental results demonstrate that our proposed model is\ncapable of accurately detecting and localizing biological anatomical landmarks\nfrom colonoscopy videos.",
          "link": "http://arxiv.org/abs/2108.02948",
          "publishedOn": "2021-08-09T00:49:27.013Z",
          "wordCount": 657,
          "title": "Deep Learning-based Biological Anatomical Landmark Detection in Colonoscopy Videos. (arXiv:2108.02948v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1\">Rashmi Bakshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courtney_J/0/1/0/all/0/1\">Jane Courtney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berry_D/0/1/0/all/0/1\">Damon Berry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavin_G/0/1/0/all/0/1\">Graham Gavin</a>",
          "description": "The process of hand washing involves complex hand movements. There are six\nprincipal sequential steps for washing hands as per the World Health\nOrganisation (WHO) guidelines. In this work, a detailed description of an\naluminium rig construction for creating a robust hand-washing dataset is\ndiscussed. The preliminary results with the help of image processing and\ncomputer vision algorithms for hand pose extraction and feature detection such\nas Harris detector, Shi-Tomasi and SIFT are demonstrated. The hand hygiene\npose- Rub hands palm to palm was captured as an input image for running all the\nexperiments. The future work will focus upon processing the video recordings of\nhand movements captured and applying deep-learning solutions for the\nclassification of hand-hygiene stages.",
          "link": "http://arxiv.org/abs/2108.03015",
          "publishedOn": "2021-08-09T00:49:27.004Z",
          "wordCount": 548,
          "title": "Feature Detection for Hand Hygiene Stages. (arXiv:2108.03015v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tokieda_K/0/1/0/all/0/1\">Kodai Tokieda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwaguchi_T/0/1/0/all/0/1\">Takafumi Iwaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawasaki_H/0/1/0/all/0/1\">Hiroshi Kawasaki</a>",
          "description": "Importance of structured-light based one-shot scanning technique is\nincreasing because of its simple system configuration and ability of capturing\nmoving objects. One severe limitation of the technique is that it can capture\nonly sparse shape, but not high frequency shapes, because certain area of\nprojection pattern is required to encode spatial information. In this paper, we\npropose a technique to recover high-frequency shapes by using shading\ninformation, which is captured by one-shot RGB-D sensor based on structured\nlight with single camera. Since color image comprises shading information of\nobject surface, high-frequency shapes can be recovered by shape from shading\ntechniques. Although multiple images with different lighting positions are\nrequired for shape from shading techniques, we propose a learning based\napproach to recover shape from a single image. In addition, to overcome the\nproblem of preparing sufficient amount of data for training, we propose a new\ndata augmentation method for high-frequency shapes using synthetic data and\ndomain adaptation. Experimental results are shown to confirm the effectiveness\nof the proposed method.",
          "link": "http://arxiv.org/abs/2108.02937",
          "publishedOn": "2021-08-09T00:49:26.997Z",
          "wordCount": 607,
          "title": "High-frequency shape recovery from shading by CNN and domain adaptation. (arXiv:2108.02937v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosinol_A/0/1/0/all/0/1\">Antoni Rosinol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>",
          "description": "Meshes are commonly used as 3D maps since they encode the topology of the\nscene while being lightweight.\n\nUnfortunately, 3D meshes are mathematically difficult to handle directly\nbecause of their combinatorial and discrete nature.\n\nTherefore, most approaches generate 3D meshes of a scene after fusing depth\ndata using volumetric or other representations.\n\nNevertheless, volumetric fusion remains computationally expensive both in\nterms of speed and memory.\n\nIn this paper, we leapfrog these intermediate representations and build a 3D\nmesh directly from a depth map and the sparse landmarks triangulated with\nvisual odometry.\n\nTo this end, we formulate a non-smooth convex optimization problem that we\nsolve using a primal-dual method.\n\nOur approach generates a smooth and accurate 3D mesh that substantially\nimproves the state-of-the-art on direct mesh reconstruction while running in\nreal-time.",
          "link": "http://arxiv.org/abs/2108.02957",
          "publishedOn": "2021-08-09T00:49:26.978Z",
          "wordCount": 571,
          "title": "Smooth Mesh Estimation from Depth Data using Non-Smooth Convex Optimization. (arXiv:2108.02957v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xinxin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haotian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruigang Yang</a>",
          "description": "This paper presents a novel framework to recover \\emph{detailed} avatar from\na single image. It is a challenging task due to factors such as variations in\nhuman shapes, body poses, texture, and viewpoints. Prior methods typically\nattempt to recover the human body shape using a parametric-based template that\nlacks the surface details. As such resulting body shape appears to be without\nclothing. In this paper, we propose a novel learning-based framework that\ncombines the robustness of the parametric model with the flexibility of\nfree-form 3D deformation. We use the deep neural networks to refine the 3D\nshape in a Hierarchical Mesh Deformation (HMD) framework, utilizing the\nconstraints from body joints, silhouettes, and per-pixel shading information.\nOur method can restore detailed human body shapes with complete textures beyond\nskinned models. Experiments demonstrate that our method has outperformed\nprevious state-of-the-art approaches, achieving better accuracy in terms of\nboth 2D IoU number and 3D metric distance.",
          "link": "http://arxiv.org/abs/2108.02931",
          "publishedOn": "2021-08-09T00:49:26.970Z",
          "wordCount": 593,
          "title": "Detailed Avatar Recovery from Single Image. (arXiv:2108.02931v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yue Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhuoming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honggang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Deqiang Xu</a>",
          "description": "mmWave radar has been shown as an effective sensing technique in low\nvisibility, smoke, dusty, and dense fog environment. However tapping the\npotential of radar sensing to reconstruct 3D object shapes remains a great\nchallenge, due to the characteristics of radar data such as sparsity, low\nresolution, specularity, high noise, and multi-path induced shadow reflections\nand artifacts. In this paper we propose 3D Reconstruction and Imaging via\nmmWave Radar (3DRIMR), a deep learning based architecture that reconstructs 3D\nshape of an object in dense detailed point cloud format, based on sparse raw\nmmWave radar intensity data. The architecture consists of two back-to-back\nconditional GAN deep neural networks: the first generator network generates 2D\ndepth images based on raw radar intensity data, and the second generator\nnetwork outputs 3D point clouds based on the results of the first generator.\nThe architecture exploits both convolutional neural network's convolutional\noperation (that extracts local structure neighborhood information) and the\nefficiency and detailed geometry capture capability of point clouds (other than\ncostly voxelization of 3D space or distance fields). Our experiments have\ndemonstrated 3DRIMR's effectiveness in reconstructing 3D objects, and its\nperformance improvement over standard techniques.",
          "link": "http://arxiv.org/abs/2108.02858",
          "publishedOn": "2021-08-09T00:49:26.962Z",
          "wordCount": 642,
          "title": "3DRIMR: 3D Reconstruction and Imaging via mmWave Radar based on Deep Learning. (arXiv:2108.02858v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fontinele_J/0/1/0/all/0/1\">Jefferson Fontinele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lefundes_G/0/1/0/all/0/1\">Gabriel Lefundes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_L/0/1/0/all/0/1\">Luciano Oliveira</a>",
          "description": "This paper introduces a method for image semantic segmentation grounded on a\nnovel fusion scheme, which takes place inside a deep convolutional neural\nnetwork. The main goal of our proposal is to explore object boundary\ninformation to improve the overall segmentation performance. Unlike previous\nworks that combine boundary and segmentation features, or those that use\nboundary information to regularize semantic segmentation, we instead propose a\nnovel approach that embodies boundary information onto segmentation. For that,\nour semantic segmentation method uses two streams, which are combined through\nan attention gate, forming an end-to-end Y-model. To the best of our knowledge,\nours is the first work to show that boundary detection can improve semantic\nsegmentation when fused through a semantic fusion gate (attention model). We\nperformed an extensive evaluation of our method over public data sets. We found\ncompetitive results on all data sets after comparing our proposed model with\nother twelve state-of-the-art segmenters, considering the same training\nconditions. Our proposed model achieved the best mIoU on the CityScapes,\nCamVid, and Pascal Context data sets, and the second best on Mapillary Vistas.",
          "link": "http://arxiv.org/abs/2108.02840",
          "publishedOn": "2021-08-09T00:49:26.954Z",
          "wordCount": 621,
          "title": "Attention-based fusion of semantic boundary and non-boundary information to improve semantic segmentation. (arXiv:2108.02840v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jile Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengsen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yihang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xuetao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Ling-Yu Duan</a>",
          "description": "Visual retrieval system faces frequent model update and deployment. It is a\nheavy workload to re-extract features of the whole database every time.Feature\ncompatibility enables the learned new visual features to be directly compared\nwith the old features stored in the database. In this way, when updating the\ndeployed model, we can bypass the inflexible and time-consuming feature\nre-extraction process. However, the old feature space that needs to be\ncompatible is not ideal and faces the distribution discrepancy problem with the\nnew space caused by different supervision losses. In this work, we propose a\nglobal optimization Dual-Tuning method to obtain feature compatibility against\ndifferent networks and losses. A feature-level prototype loss is proposed to\nexplicitly align two types of embedding features, by transferring global\nprototype information. Furthermore, we design a component-level mutual\nstructural regularization to implicitly optimize the feature intrinsic\nstructure. Experimental results on million-scale datasets demonstrate that our\nDual-Tuning is able to obtain feature compatibility without sacrificing\nperformance. (Our code will be avaliable at\nhttps://github.com/yanbai1993/Dual-Tuning)",
          "link": "http://arxiv.org/abs/2108.02959",
          "publishedOn": "2021-08-09T00:49:26.935Z",
          "wordCount": 616,
          "title": "Dual-Tuning: Joint Prototype Transfer and Structure Regularization for Compatible Feature Learning. (arXiv:2108.02959v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wanqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming Yang</a>",
          "description": "This paper investigates a valuable setting called few-shot unsupervised\ndomain adaptation (FS-UDA), which has not been sufficiently studied in the\nliterature. In this setting, the source domain data are labelled, but with\nfew-shot per category, while the target domain data are unlabelled. To address\nthe FS-UDA setting, we develop a general UDA model to solve the following two\nkey issues: the few-shot labeled data per category and the domain adaptation\nbetween support and query sets. Our model is general in that once trained it\nwill be able to be applied to various FS-UDA tasks from the same source and\ntarget domains. Inspired by the recent local descriptor based few-shot learning\n(FSL), our general UDA model is fully built upon local descriptors (LDs) for\nimage classification and domain adaptation. By proposing a novel concept called\nsimilarity patterns (SPs), our model not only effectively considers the spatial\nrelationship of LDs that was ignored in previous FSL methods, but also makes\nthe learned image similarity better serve the required domain alignment.\nSpecifically, we propose a novel IMage-to-class sparse Similarity Encoding\n(IMSE) method. It learns SPs to extract the local discriminative information\nfor classification and meanwhile aligns the covariance matrix of the SPs for\ndomain adaptation. Also, domain adversarial training and multi-scale local\nfeature matching are performed upon LDs. Extensive experiments conducted on a\nmulti-domain benchmark dataset DomainNet demonstrates the state-of-the-art\nperformance of our IMSE for the novel setting of FS-UDA. In addition, for FSL,\nour IMSE can also show better performance than most of recent FSL methods on\nminiImageNet.",
          "link": "http://arxiv.org/abs/2108.02953",
          "publishedOn": "2021-08-09T00:49:26.927Z",
          "wordCount": 713,
          "title": "Few-shot Unsupervised Domain Adaptation with Image-to-class Sparse Similarity Encoding. (arXiv:2108.02953v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanjie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongmin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>",
          "description": "Existing domain adaptation methods for crowd counting view each crowd image\nas a whole and reduce domain discrepancies on crowds and backgrounds\nsimultaneously. However, we argue that these methods are suboptimal, as crowds\nand backgrounds have quite different characteristics and backgrounds may vary\ndramatically in different crowd scenes (see Fig.~\\ref{teaser}). This makes\ncrowds not well aligned across domains together with backgrounds in a holistic\nmanner. To this end, we propose to untangle crowds and backgrounds from crowd\nimages and design fine-grained domain adaption methods for crowd counting.\nDifferent from other tasks which possess region-based fine-grained annotations\n(e.g., segments or bounding boxes), crowd counting only annotates one point on\neach human head, which impedes the implementation of fine-grained adaptation\nmethods. To tackle this issue, we propose a novel and effective schema to learn\ncrowd segmentation from point-level crowd counting annotations in the context\nof Multiple Instance Learning. We further leverage the derived segments to\npropose a crowd-aware fine-grained domain adaptation framework for crowd\ncounting, which consists of two novel adaptation modules, i.e., Crowd Region\nTransfer (CRT) and Crowd Density Alignment (CDA). Specifically, the CRT module\nis designed to guide crowd features transfer across domains beyond background\ndistractions, and the CDA module dedicates to constraining the target-domain\ncrowd density distributions. Extensive experiments on multiple cross-domain\nsettings (i.e., Synthetic $\\rightarrow$ Real, Fixed $\\rightarrow$ Fickle,\nNormal $\\rightarrow$ BadWeather) demonstrate the superiority of the proposed\nmethod compared with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.02980",
          "publishedOn": "2021-08-09T00:49:26.920Z",
          "wordCount": 679,
          "title": "Fine-grained Domain Adaptive Crowd Counting via Point-derived Segmentation. (arXiv:2108.02980v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sambaturu_B/0/1/0/all/0/1\">Bhavani Sambaturu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ashutosh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C.V. Jawahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1\">Chetan Arora</a>",
          "description": "Semantic segmentation of medical images is an essential first step in\ncomputer-aided diagnosis systems for many applications. However, given many\ndisparate imaging modalities and inherent variations in the patient data, it is\ndifficult to consistently achieve high accuracy using modern deep neural\nnetworks (DNNs). This has led researchers to propose interactive image\nsegmentation techniques where a medical expert can interactively correct the\noutput of a DNN to the desired accuracy. However, these techniques often need\nseparate training data with the associated human interactions, and do not\ngeneralize to various diseases, and types of medical images. In this paper, we\nsuggest a novel conditional inference technique for DNNs which takes the\nintervention by a medical expert as test time constraints and performs\ninference conditioned upon these constraints. Our technique is generic can be\nused for medical images from any modality. Unlike other methods, our approach\ncan correct multiple structures simultaneously and add structures missed at\ninitial segmentation. We report an improvement of 13.3, 12.5, 17.8, 10.2, and\n12.4 times in user annotation time than full human annotation for the nucleus,\nmultiple cells, liver and tumor, organ, and brain segmentation respectively. We\nreport a time saving of 2.8, 3.0, 1.9, 4.4, and 8.6 fold compared to other\ninteractive segmentation techniques. Our method can be useful to clinicians for\ndiagnosis and post-surgical follow-up with minimal intervention from the\nmedical expert. The source-code and the detailed results are available here\n[1].",
          "link": "http://arxiv.org/abs/2108.02996",
          "publishedOn": "2021-08-09T00:49:26.910Z",
          "wordCount": 705,
          "title": "Efficient and Generic Interactive Segmentation Framework to Correct Mispredictions during Clinical Evaluation of Medical Images. (arXiv:2108.02996v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sandhini Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krueger_G/0/1/0/all/0/1\">Gretchen Krueger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jack Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radford_A/0/1/0/all/0/1\">Alec Radford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong Wook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brundage_M/0/1/0/all/0/1\">Miles Brundage</a>",
          "description": "Recently, there have been breakthroughs in computer vision (\"CV\") models that\nare more generalizable with the advent of models such as CLIP and ALIGN. In\nthis paper, we analyze CLIP and highlight some of the challenges such models\npose. CLIP reduces the need for task specific training data, potentially\nopening up many niche tasks to automation. CLIP also allows its users to\nflexibly specify image classification classes in natural language, which we\nfind can shift how biases manifest. Additionally, through some preliminary\nprobes we find that CLIP can inherit biases found in prior computer vision\nsystems. Given the wide and unpredictable domain of uses for such models, this\nraises questions regarding what sufficiently safe behaviour for such systems\nmay look like. These results add evidence to the growing body of work calling\nfor a change in the notion of a 'better' model--to move beyond simply looking\nat higher accuracy at task-oriented capability evaluations, and towards a\nbroader 'better' that takes into account deployment-critical features such as\ndifferent use contexts, and people who interact with the model when thinking\nabout model deployment.",
          "link": "http://arxiv.org/abs/2108.02818",
          "publishedOn": "2021-08-09T00:49:26.901Z",
          "wordCount": 643,
          "title": "Evaluating CLIP: Towards Characterization of Broader Capabilities and Downstream Implications. (arXiv:2108.02818v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thuan Trong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thuan Q. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1\">Dung Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Ngoc Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_N/0/1/0/all/0/1\">Nguyen D. Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khang Nguyen</a>",
          "description": "Vietnam is such an attractive tourist destination with its stunning and\npristine landscapes and its top-rated unique food and drink. Among thousands of\nVietnamese dishes, foreigners and native people are interested in easy-to-eat\ntastes and easy-to-do recipes, along with reasonable prices, mouthwatering\nflavors, and popularity. Due to the diversity and almost all the dishes have\nsignificant similarities and the lack of quality Vietnamese food datasets, it\nis hard to implement an auto system to classify Vietnamese food, therefore,\nmake people easier to discover Vietnamese food. This paper introduces a new\nVietnamese food dataset named VinaFood21, which consists of 13,950 images\ncorresponding to 21 dishes. We use 10,044 images for model training and 6,682\ntest images to classify each food in the VinaFood21 dataset and achieved an\naverage accuracy of 74.81% when fine-tuning CNN EfficientNet-B0.\n(https://github.com/nguyenvd-uit/uit-together-dataset)",
          "link": "http://arxiv.org/abs/2108.02929",
          "publishedOn": "2021-08-09T00:49:26.874Z",
          "wordCount": 586,
          "title": "VinaFood21: A Novel Dataset for Evaluating Vietnamese Food Recognition. (arXiv:2108.02929v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02923",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yulin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuxi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yuchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiameng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Kun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>",
          "description": "Structured text understanding on Visually Rich Documents (VRDs) is a crucial\npart of Document Intelligence. Due to the complexity of content and layout in\nVRDs, structured text understanding has been a challenging task. Most existing\nstudies decoupled this problem into two sub-tasks: entity labeling and entity\nlinking, which require an entire understanding of the context of documents at\nboth token and segment levels. However, little work has been concerned with the\nsolutions that efficiently extract the structured data from different levels.\nThis paper proposes a unified framework named StrucTexT, which is flexible and\neffective for handling both sub-tasks. Specifically, based on the transformer,\nwe introduce a segment-token aligned encoder to deal with the entity labeling\nand entity linking tasks at different levels of granularity. Moreover, we\ndesign a novel pre-training strategy with three self-supervised tasks to learn\na richer representation. StrucTexT uses the existing Masked Visual Language\nModeling task and the new Sentence Length Prediction and Paired Boxes Direction\ntasks to incorporate the multi-modal information across text, image, and\nlayout. We evaluate our method for structured text understanding at\nsegment-level and token-level and show it outperforms the state-of-the-art\ncounterparts with significantly superior performance on the FUNSD, SROIE, and\nEPHOIE datasets.",
          "link": "http://arxiv.org/abs/2108.02923",
          "publishedOn": "2021-08-09T00:49:26.844Z",
          "wordCount": 654,
          "title": "StrucTexT: Structured Text Understanding with Multi-Modal Transformers. (arXiv:2108.02923v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>",
          "description": "The growing number of action classes has posed a new challenge for video\nunderstanding, making Zero-Shot Action Recognition (ZSAR) a thriving direction.\nThe ZSAR task aims to recognize target (unseen) actions without training\nexamples by leveraging semantic representations to bridge seen and unseen\nactions. However, due to the complexity and diversity of actions, it remains\nchallenging to semantically represent action classes and transfer knowledge\nfrom seen data. In this work, we propose an ER-enhanced ZSAR model inspired by\nan effective human memory technique Elaborative Rehearsal (ER), which involves\nelaborating a new concept and relating it to known concepts. Specifically, we\nexpand each action class as an Elaborative Description (ED) sentence, which is\nmore discriminative than a class name and less costly than manual-defined\nattributes. Besides directly aligning class semantics with videos, we\nincorporate objects from the video as Elaborative Concepts (EC) to improve\nvideo semantics and generalization from seen actions to unseen actions. Our\nER-enhanced ZSAR model achieves state-of-the-art results on three existing\nbenchmarks. Moreover, we propose a new ZSAR evaluation protocol on the Kinetics\ndataset to overcome limitations of current benchmarks and demonstrate the first\ncase where ZSAR performance is comparable to few-shot learning baselines on\nthis more realistic setting. We will release our codes and collected EDs at\nhttps://github.com/DeLightCMU/ElaborativeRehearsal.",
          "link": "http://arxiv.org/abs/2108.02833",
          "publishedOn": "2021-08-09T00:49:26.836Z",
          "wordCount": 645,
          "title": "Elaborative Rehearsal for Zero-shot Action Recognition. (arXiv:2108.02833v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jindi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kejie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaohua Jia</a>",
          "description": "In recent years, many deep learning models have been adopted in autonomous\ndriving. At the same time, these models introduce new vulnerabilities that may\ncompromise the safety of autonomous vehicles. Specifically, recent studies have\ndemonstrated that adversarial attacks can cause a significant decline in\ndetection precision of deep learning-based 3D object detection models. Although\ndriving safety is the ultimate concern for autonomous driving, there is no\ncomprehensive study on the linkage between the performance of deep learning\nmodels and the driving safety of autonomous vehicles under adversarial attacks.\nIn this paper, we investigate the impact of two primary types of adversarial\nattacks, perturbation attacks and patch attacks, on the driving safety of\nvision-based autonomous vehicles rather than the detection precision of deep\nlearning models. In particular, we consider two state-of-the-art models in\nvision-based 3D object detection, Stereo R-CNN and DSGN. To evaluate driving\nsafety, we propose an end-to-end evaluation framework with a set of driving\nsafety performance metrics. By analyzing the results of our extensive\nevaluation experiments, we find that (1) the attack's impact on the driving\nsafety of autonomous vehicles and the attack's impact on the precision of 3D\nobject detectors are decoupled, and (2) the DSGN model demonstrates stronger\nrobustness to adversarial attacks than the Stereo R-CNN model. In addition, we\nfurther investigate the causes behind the two findings with an ablation study.\nThe findings of this paper provide a new perspective to evaluate adversarial\nattacks and guide the selection of deep learning models in autonomous driving.",
          "link": "http://arxiv.org/abs/2108.02940",
          "publishedOn": "2021-08-09T00:49:26.809Z",
          "wordCount": 695,
          "title": "Evaluating Adversarial Attacks on Driving Safety in Vision-Based Autonomous Vehicles. (arXiv:2108.02940v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_F/0/1/0/all/0/1\">Fengchun Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>",
          "description": "We study a worst-case scenario in generalization: Out-of-domain\ngeneralization from a single source. The goal is to learn a robust model from a\nsingle source and expect it to generalize over many unknown distributions. This\nchallenging problem has been seldom investigated while existing solutions\nsuffer from various limitations such as the ignorance of uncertainty assessment\nand label augmentation. In this paper, we propose uncertainty-guided domain\ngeneralization to tackle the aforementioned limitations. The key idea is to\naugment the source capacity in both feature and label spaces, while the\naugmentation is guided by uncertainty assessment. To the best of our knowledge,\nthis is the first work to (1) quantify the generalization uncertainty from a\nsingle source and (2) leverage it to guide both feature and label augmentation\nfor robust generalization. The model training and deployment are effectively\norganized in a Bayesian meta-learning framework. We conduct extensive\ncomparisons and ablation study to validate our approach. The results prove our\nsuperior performance in a wide scope of tasks including image classification,\nsemantic segmentation, text classification, and speech recognition.",
          "link": "http://arxiv.org/abs/2108.02888",
          "publishedOn": "2021-08-09T00:49:26.797Z",
          "wordCount": 629,
          "title": "Out-of-domain Generalization from a Single Source: A Uncertainty Quantification Approach. (arXiv:2108.02888v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Ken C. L. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashyap_S/0/1/0/all/0/1\">Satyananda Kashyap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Mehdi Moradi</a>",
          "description": "Transfer learning allows the reuse of deep learning features on new datasets\nwith limited data. However, the resulting models could be unnecessarily large\nand thus inefficient. Although network pruning can be applied to improve\ninference efficiency, existing algorithms usually require fine-tuning and may\nnot be suitable for small datasets. In this paper, we propose an algorithm that\ntransforms the convolutional weights into the subspaces of orthonormal bases\nwhere a model is pruned. Using singular value decomposition, we decompose a\nconvolutional layer into two layers: a convolutional layer with the orthonormal\nbasis vectors as the filters, and a layer that we name \"BasisScalingConv\",\nwhich is responsible for rescaling the features and transforming them back to\nthe original space. As the filters in each transformed layer are linearly\nindependent with known relative importance, pruning can be more effective and\nstable, and fine tuning individual weights is unnecessary. Furthermore, as the\nnumbers of input and output channels of the original convolutional layer remain\nunchanged, basis pruning is applicable to virtually all network architectures.\nBasis pruning can also be combined with existing pruning algorithms for double\npruning to further increase the pruning capability. With less than 1% reduction\nin the classification accuracy, we can achieve pruning ratios up to 98.9% in\nparameters and 98.6% in FLOPs.",
          "link": "http://arxiv.org/abs/2108.02893",
          "publishedOn": "2021-08-09T00:49:26.790Z",
          "wordCount": 652,
          "title": "Basis Scaling and Double Pruning for Efficient Transfer Learning. (arXiv:2108.02893v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1\">Miao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Baorong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xuetong Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jizhou Huang</a>",
          "description": "Image Retrieval is a fundamental task of obtaining images similar to the\nquery one from a database. A common image retrieval practice is to firstly\nretrieve candidate images via similarity search using global image features and\nthen re-rank the candidates by leveraging their local features. Previous\nlearning-based studies mainly focus on either global or local image\nrepresentation learning to tackle the retrieval task. In this paper, we abandon\nthe two-stage paradigm and seek to design an effective single-stage solution by\nintegrating local and global information inside images into compact image\nrepresentations. Specifically, we propose a Deep Orthogonal Local and Global\n(DOLG) information fusion framework for end-to-end image retrieval. It\nattentively extracts representative local information with multi-atrous\nconvolutions and self-attention at first. Components orthogonal to the global\nimage representation are then extracted from the local information. At last,\nthe orthogonal components are concatenated with the global representation as a\ncomplementary, and then aggregation is performed to generate the final\nrepresentation. The whole framework is end-to-end differentiable and can be\ntrained with image-level labels. Extensive experimental results validate the\neffectiveness of our solution and show that our model achieves state-of-the-art\nimage retrieval performances on Revisited Oxford and Paris datasets.",
          "link": "http://arxiv.org/abs/2108.02927",
          "publishedOn": "2021-08-09T00:49:26.782Z",
          "wordCount": 653,
          "title": "DOLG: Single-Stage Image Retrieval with Deep Orthogonal Fusion of Local and Global Features. (arXiv:2108.02927v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02832",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1\">Akash Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jonnalagedda_P/0/1/0/all/0/1\">Padmaja Jonnalagedda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>",
          "description": "Most of the existing works in supervised spatio-temporal video\nsuper-resolution (STVSR) heavily rely on a large-scale external dataset\nconsisting of paired low-resolution low-frame rate (LR-LFR)and high-resolution\nhigh-frame-rate (HR-HFR) videos. Despite their remarkable performance, these\nmethods make a prior assumption that the low-resolution video is obtained by\ndown-scaling the high-resolution video using a known degradation kernel, which\ndoes not hold in practical settings. Another problem with these methods is that\nthey cannot exploit instance-specific internal information of video at testing\ntime. Recently, deep internal learning approaches have gained attention due to\ntheir ability to utilize the instance-specific statistics of a video. However,\nthese methods have a large inference time as they require thousands of gradient\nupdates to learn the intrinsic structure of the data. In this work, we\npresentAdaptiveVideoSuper-Resolution (Ada-VSR) which leverages external, as\nwell as internal, information through meta-transfer learning and internal\nlearning, respectively. Specifically, meta-learning is employed to obtain\nadaptive parameters, using a large-scale external dataset, that can adapt\nquickly to the novel condition (degradation model) of the given test video\nduring the internal learning task, thereby exploiting external and internal\ninformation of a video for super-resolution. The model trained using our\napproach can quickly adapt to a specific video condition with only a few\ngradient updates, which reduces the inference time significantly. Extensive\nexperiments on standard datasets demonstrate that our method performs favorably\nagainst various state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.02832",
          "publishedOn": "2021-08-09T00:49:26.762Z",
          "wordCount": 670,
          "title": "Ada-VSR: Adaptive Video Super-Resolution with Meta-Learning. (arXiv:2108.02832v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Sen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wentong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Michael Ying Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>",
          "description": "A lifespan face synthesis (LFS) model aims to generate a set of\nphoto-realistic face images of a person's whole life, given only one snapshot\nas reference. The generated face image given a target age code is expected to\nbe age-sensitive reflected by bio-plausible transformations of shape and\ntexture, while being identity preserving. This is extremely challenging because\nthe shape and texture characteristics of a face undergo separate and highly\nnonlinear transformations w.r.t. age. Most recent LFS models are based on\ngenerative adversarial networks (GANs) whereby age code conditional\ntransformations are applied to a latent face representation. They benefit\ngreatly from the recent advancements of GANs. However, without explicitly\ndisentangling their latent representations into the texture, shape and identity\nfactors, they are fundamentally limited in modeling the nonlinear age-related\ntransformation on texture and shape whilst preserving identity. In this work, a\nnovel LFS model is proposed to disentangle the key face characteristics\nincluding shape, texture and identity so that the unique shape and texture age\ntransformations can be modeled effectively. This is achieved by extracting\nshape, texture and identity features separately from an encoder. Critically,\ntwo transformation modules, one conditional convolution based and the other\nchannel attention based, are designed for modeling the nonlinear shape and\ntexture feature transformations respectively. This is to accommodate their\nrather distinct aging processes and ensure that our synthesized images are both\nage-sensitive and identity preserving. Extensive experiments show that our LFS\nmodel is clearly superior to the state-of-the-art alternatives. Codes and demo\nare available on our project website:\n\\url{https://senhe.github.io/projects/iccv_2021_lifespan_face}.",
          "link": "http://arxiv.org/abs/2108.02874",
          "publishedOn": "2021-08-09T00:49:26.755Z",
          "wordCount": 694,
          "title": "Disentangled Lifespan Face Synthesis. (arXiv:2108.02874v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1\">Shagufta Henna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reji_A/0/1/0/all/0/1\">Aparna Reji</a>",
          "description": "Covid-19 detection at an early stage can aid in an effective treatment and\nisolation plan to prevent its spread. Recently, transfer learning has been used\nfor Covid-19 detection using X-ray, ultrasound, and CT scans. One of the major\nlimitations inherent to these proposed methods is limited labeled dataset size\nthat affects the reliability of Covid-19 diagnosis and disease progression. In\nthis work, we demonstrate that how we can augment limited X-ray images data by\nusing Contrast limited adaptive histogram equalization (CLAHE) to train the\nlast layer of the pre-trained deep learning models to mitigate the bias of\ntransfer learning for Covid-19 detection. We transfer learned various\npre-trained deep learning models including AlexNet, ZFNet, VGG-16, ResNet-18,\nand GoogLeNet, and fine-tune the last layer by using CLAHE-augmented dataset.\nThe experiment results reveal that the CLAHE-based augmentation to various\npre-trained deep learning models significantly improves the model efficiency.\nThe pre-trained VCG-16 model with CLAHEbased augmented images achieves a\nsensitivity of 95% using 15 epochs. AlexNet works show good sensitivity when\ntrained on non-augmented data. Other models demonstrate a value of less than\n60% when trained on non-augmented data. Our results reveal that the sample bias\ncan negatively impact the performance of transfer learning which is\nsignificantly improved by using CLAHE-based augmentation.",
          "link": "http://arxiv.org/abs/2108.02870",
          "publishedOn": "2021-08-09T00:49:26.704Z",
          "wordCount": 692,
          "title": "A Data Augmented Approach to Transfer Learning for Covid-19 Detection. (arXiv:2108.02870v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ningli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Debao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1\">Xiao Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strasbaugh_C/0/1/0/all/0/1\">Chris Strasbaugh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_A/0/1/0/all/0/1\">Alper Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sezen_H/0/1/0/all/0/1\">Halil Sezen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>",
          "description": "In this paper, we present a case study that performs an unmanned aerial\nvehicle (UAV) based fine-scale 3D change detection and monitoring of\nprogressive collapse performance of a building during a demolition event.\nMulti-temporal oblique photogrammetry images are collected with 3D point clouds\ngenerated at different stages of the demolition. The geometric accuracy of the\ngenerated point clouds has been evaluated against both airborne and terrestrial\nLiDAR point clouds, achieving an average distance of 12 cm and 16 cm for roof\nand facade respectively. We propose a hierarchical volumetric change detection\nframework that unifies multi-temporal UAV images for pose estimation (free of\nground control points), reconstruction, and a coarse-to-fine 3D density change\nanalysis. This work has provided a solution capable of addressing change\ndetection on full 3D time-series datasets where dramatic scene content changes\nare presented progressively. Our change detection results on the building\ndemolition event have been evaluated against the manually marked ground-truth\nchanges and have achieved an F-1 score varying from 0.78 to 0.92, with\nconsistently high precision (0.92 - 0.99). Volumetric changes through the\ndemolition progress are derived from change detection and have shown to\nfavorably reflect the qualitative and quantitative building demolition\nprogression.",
          "link": "http://arxiv.org/abs/2108.02800",
          "publishedOn": "2021-08-09T00:49:26.684Z",
          "wordCount": 669,
          "title": "A volumetric change detection framework using UAV oblique photogrammetry - A case study of ultra-high-resolution monitoring of progressive building collapse. (arXiv:2108.02800v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kukacka_J/0/1/0/all/0/1\">Jan Kuka&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zenz_A/0/1/0/all/0/1\">Anja Zenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kollovieh_M/0/1/0/all/0/1\">Marcel Kollovieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Justel_D/0/1/0/all/0/1\">Dominik J&#xfc;stel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntziachristos_V/0/1/0/all/0/1\">Vasilis Ntziachristos</a>",
          "description": "Fundus photography is the primary method for retinal imaging and essential\nfor diabetic retinopathy prevention. Automated segmentation of fundus\nphotographs would improve the quality, capacity, and cost-effectiveness of eye\ncare screening programs. However, current segmentation methods are not robust\ntowards the diversity in imaging conditions and pathologies typical for\nreal-world clinical applications. To overcome these limitations, we utilized\ncontrastive self-supervised learning to exploit the large variety of unlabeled\nfundus images in the publicly available EyePACS dataset. We pre-trained an\nencoder of a U-Net, which we later fine-tuned on several retinal vessel and\nlesion segmentation datasets. We demonstrate for the first time that by using\ncontrastive self-supervised learning, the pre-trained network can recognize\nblood vessels, optic disc, fovea, and various lesions without being provided\nany labels. Furthermore, when fine-tuned on a downstream blood vessel\nsegmentation task, such pre-trained networks achieve state-of-the-art\nperformance on images from different datasets. Additionally, the pre-training\nalso leads to shorter training times and an improved few-shot performance on\nboth blood vessel and lesion segmentation tasks. Altogether, our results\nshowcase the benefits of contrastive self-supervised pre-training which can\nplay a crucial role in real-world clinical applications requiring robust models\nable to adapt to new devices with only a few annotated samples.",
          "link": "http://arxiv.org/abs/2108.02798",
          "publishedOn": "2021-08-09T00:49:26.664Z",
          "wordCount": 650,
          "title": "Self-Supervised Learning from Unlabeled Fundus Photographs Improves Segmentation of the Retina. (arXiv:2108.02798v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cheng-Ju Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>",
          "description": "Human-robot collaboration is an essential research topic in artificial\nintelligence (AI), enabling researchers to devise cognitive AI systems and\naffords an intuitive means for users to interact with the robot. Of note,\ncommunication plays a central role. To date, prior studies in embodied agent\nnavigation have only demonstrated that human languages facilitate communication\nby instructions in natural languages. Nevertheless, a plethora of other forms\nof communication is left unexplored. In fact, human communication originated in\ngestures and oftentimes is delivered through multimodal cues, e.g. \"go there\"\nwith a pointing gesture. To bridge the gap and fill in the missing dimension of\ncommunication in embodied agent navigation, we propose investigating the\neffects of using gestures as the communicative interface instead of verbal\ncues. Specifically, we develop a VR-based 3D simulation environment, named\nGes-THOR, based on AI2-THOR platform. In this virtual environment, a human\nplayer is placed in the same virtual scene and shepherds the artificial agent\nusing only gestures. The agent is tasked to solve the navigation problem guided\nby natural gestures with unknown semantics; we do not use any predefined\ngestures due to the diversity and versatile nature of human gestures. We argue\nthat learning the semantics of natural gestures is mutually beneficial to\nlearning the navigation task--learn to communicate and communicate to learn. In\na series of experiments, we demonstrate that human gesture cues, even without\npredefined semantics, improve the object-goal navigation for an embodied agent,\noutperforming various state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.02846",
          "publishedOn": "2021-08-09T00:49:26.621Z",
          "wordCount": 701,
          "title": "Communicative Learning with Natural Gestures for Embodied Navigation Agents with Human-in-the-Scene. (arXiv:2108.02846v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zohourianshahzadi_Z/0/1/0/all/0/1\">Zanyar Zohourianshahzadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1\">Jugal K. Kalita</a>",
          "description": "Inspired by how the human brain employs a higher number of neural pathways\nwhen describing a highly focused subject, we show that deep attentive models\nused for the main vision-language task of image captioning, could be extended\nto achieve better performance. Image captioning bridges a gap between computer\nvision and natural language processing. Automated image captioning is used as a\ntool to eliminate the need for human agent for creating descriptive captions\nfor unseen images.Automated image captioning is challenging and yet\ninteresting. One reason is that AI based systems capable of generating\nsentences that describe an input image could be used in a wide variety of tasks\nbeyond generating captions for unseen images found on web or uploaded to social\nmedia. For example, in biology and medical sciences, these systems could\nprovide researchers and physicians with a brief linguistic description of\nrelevant images, potentially expediting their work.",
          "link": "http://arxiv.org/abs/2108.02807",
          "publishedOn": "2021-08-09T00:49:26.609Z",
          "wordCount": 628,
          "title": "Neural Twins Talk & Alternative Calculations. (arXiv:2108.02807v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Shuquan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Songfang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>",
          "description": "Point cloud segmentation is a fundamental task in 3D. Despite recent progress\non point cloud segmentation with the power of deep networks, current deep\nlearning methods based on the clean label assumptions may fail with noisy\nlabels. Yet, object class labels are often mislabeled in real-world point cloud\ndatasets. In this work, we take the lead in solving this issue by proposing a\nnovel Point Noise-Adaptive Learning (PNAL) framework. Compared to existing\nnoise-robust methods on image tasks, our PNAL is noise-rate blind, to cope with\nthe spatially variant noise rate problem specific to point clouds.\nSpecifically, we propose a novel point-wise confidence selection to obtain\nreliable labels based on the historical predictions of each point. A novel\ncluster-wise label correction is proposed with a voting strategy to generate\nthe best possible label taking the neighbor point correlations into\nconsideration. We conduct extensive experiments to demonstrate the\neffectiveness of PNAL on both synthetic and real-world noisy datasets. In\nparticular, even with $60\\%$ symmetric noisy labels, our proposed method\nproduces much better results than its baseline counterpart without PNAL and is\ncomparable to the ideal upper bound trained on a completely clean dataset.\nMoreover, we fully re-labeled the validation set of a popular but noisy\nreal-world scene dataset ScanNetV2 to make it clean, for rigorous experiment\nand future research. Our code and data are available at\n\\url{https://shuquanye.com/PNAL_website/}.",
          "link": "http://arxiv.org/abs/2107.14230",
          "publishedOn": "2021-08-06T01:58:38.210Z",
          "wordCount": 701,
          "title": "Learning with Noisy Labels for Robust Point Cloud Segmentation. (arXiv:2107.14230v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02602",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Condat_L/0/1/0/all/0/1\">Laurent Condat</a>",
          "description": "It is common to have to process signals or images whose values are cyclic and\ncan be represented as points on the complex circle, like wrapped phases,\nangles, orientations, or color hues. We consider a Tikhonov-type regularization\nmodel to smoothen or interpolate circle-valued signals defined on arbitrary\ngraphs. We propose a convex relaxation of this nonconvex problem as a\nsemidefinite program, and an efficient algorithm to solve it.",
          "link": "http://arxiv.org/abs/2108.02602",
          "publishedOn": "2021-08-06T00:51:47.662Z",
          "wordCount": 501,
          "title": "Tikhonov Regularization of Circle-Valued Signals. (arXiv:2108.02602v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1\">Shahroz Tariq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Simon S. Woo</a>",
          "description": "Over the last few decades, artificial intelligence research has made\ntremendous strides, but it still heavily relies on fixed datasets in stationary\nenvironments. Continual learning is a growing field of research that examines\nhow AI systems can learn sequentially from a continuous stream of linked data\nin the same way that biological systems do. Simultaneously, fake media such as\ndeepfakes and synthetic face images have emerged as significant to current\nmultimedia technologies. Recently, numerous method has been proposed which can\ndetect deepfakes with high accuracy. However, they suffer significantly due to\ntheir reliance on fixed datasets in limited evaluation settings. Therefore, in\nthis work, we apply continuous learning to neural networks' learning dynamics,\nemphasizing its potential to increase data efficiency significantly. We propose\nContinual Representation using Distillation (CoReD) method that employs the\nconcept of Continual Learning (CL), Representation Learning (RL), and Knowledge\nDistillation (KD). We design CoReD to perform sequential domain adaptation\ntasks on new deepfake and GAN-generated synthetic face datasets, while\neffectively minimizing the catastrophic forgetting in a teacher-student model\nsetting. Our extensive experimental results demonstrate that our method is\nefficient at domain adaptation to detect low-quality deepfakes videos and\nGAN-generated images from several datasets, outperforming the-state-of-art\nbaseline methods.",
          "link": "http://arxiv.org/abs/2107.02408",
          "publishedOn": "2021-08-06T00:51:47.654Z",
          "wordCount": 710,
          "title": "CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation. (arXiv:2107.02408v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1\">Juan P. Vigueras-Guill&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1\">Frank Seeliger</a>",
          "description": "Regularization in convolutional neural networks (CNNs) is usually addressed\nwith dropout layers. However, dropout is sometimes detrimental in the\nconvolutional part of a CNN as it simply sets to zero a percentage of pixels in\nthe feature maps, adding unrepresentative examples during training. Here, we\npropose a CNN layer that performs regularization by applying random rotations\nof reflections to a small percentage of feature maps after every convolutional\nlayer. We prove how this concept is beneficial for images with orientational\nsymmetries, such as in medical images, as it provides a certain degree of\nrotational invariance. We tested this method in two datasets, a patch-based set\nof histopathology images (PatchCamelyon) to perform classification using a\ngeneric DenseNet, and a set of specular microscopy images of the corneal\nendothelium to perform segmentation using a tailored U-net, improving the\nperformance in both cases.",
          "link": "http://arxiv.org/abs/2108.02704",
          "publishedOn": "2021-08-06T00:51:47.648Z",
          "wordCount": 615,
          "title": "Rotaflip: A New CNN Layer for Regularization and Rotational Invariance in Medical Images. (arXiv:2108.02704v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.13611",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Mu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Huijuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Q/0/1/0/all/0/1\">Qichuan Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>",
          "description": "Image-to-image translation has been revolutionized with GAN-based methods.\nHowever, existing methods lack the ability to preserve the identity of the\nsource domain. As a result, synthesized images can often over-adapt to the\nreference domain, losing important structural characteristics and suffering\nfrom suboptimal visual quality. To solve these challenges, we propose a novel\nfrequency domain image translation (FDIT) framework, exploiting frequency\ninformation for enhancing the image generation process. Our key idea is to\ndecompose the image into low-frequency and high-frequency components, where the\nhigh-frequency feature captures object structure akin to the identity. Our\ntraining objective facilitates the preservation of frequency information in\nboth pixel space and Fourier spectral space. We broadly evaluate FDIT across\nfive large-scale datasets and multiple tasks including image translation and\nGAN inversion. Extensive experiments and ablations show that FDIT effectively\npreserves the identity of the source image, and produces photo-realistic\nimages. FDIT establishes state-of-the-art performance, reducing the average FID\nscore by 5.6% compared to the previous best method.",
          "link": "http://arxiv.org/abs/2011.13611",
          "publishedOn": "2021-08-06T00:51:47.641Z",
          "wordCount": 641,
          "title": "Frequency Domain Image Translation: More Photo-realistic, Better Identity-preserving. (arXiv:2011.13611v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hornauer_J/0/1/0/all/0/1\">Julia Hornauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nalpantidis_L/0/1/0/all/0/1\">Lazaros Nalpantidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>",
          "description": "Real-world perception systems in many cases build on hardware with limited\nresources to adhere to cost and power limitations of their carrying system.\nDeploying deep neural networks on resource-constrained hardware became possible\nwith model compression techniques, as well as efficient and hardware-aware\narchitecture design. However, model adaptation is additionally required due to\nthe diverse operation environments. In this work, we address the problem of\ntraining deep neural networks on resource-constrained hardware in the context\nof visual domain adaptation. We select the task of monocular depth estimation\nwhere our goal is to transform a pre-trained model to the target's domain data.\nWhile the source domain includes labels, we assume an unlabelled target domain,\nas it happens in real-world applications. Then, we present an adversarial\nlearning approach that is adapted for training on the device with limited\nresources. Since visual domain adaptation, i.e. neural network training, has\nnot been previously explored for resource-constrained hardware, we present the\nfirst feasibility study for image-based depth estimation. Our experiments show\nthat visual domain adaptation is relevant only for efficient network\narchitectures and training sets at the order of a few hundred samples. Models\nand code are publicly available.",
          "link": "http://arxiv.org/abs/2108.02671",
          "publishedOn": "2021-08-06T00:51:47.635Z",
          "wordCount": 650,
          "title": "Visual Domain Adaptation for Monocular Depth Estimation on Resource-Constrained Hardware. (arXiv:2108.02671v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Rongchang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>",
          "description": "Semi-supervised learning aims to boost the accuracy of a model by exploring\nunlabeled images. The state-of-the-art methods are consistency-based which\nlearn about unlabeled images by encouraging the model to give consistent\npredictions for images under different augmentations. However, when applied to\npose estimation, the methods degenerate and predict every pixel in unlabeled\nimages as background. This is because contradictory predictions are gradually\npushed to the background class due to highly imbalanced class distribution. But\nthis is not an issue in supervised learning because it has accurate labels.\nThis inspires us to stabilize the training by obtaining reliable pseudo labels.\nSpecifically, we learn two networks to mutually teach each other. In\nparticular, for each image, we compose an easy-hard pair by applying different\naugmentations and feed them to both networks. The more reliable predictions on\neasy images in each network are used to teach the other network to learn about\nthe corresponding hard images. The approach successfully avoids degeneration\nand achieves promising results on public datasets. The source code will be\nreleased.",
          "link": "http://arxiv.org/abs/2011.12498",
          "publishedOn": "2021-08-06T00:51:47.538Z",
          "wordCount": 646,
          "title": "An Empirical Study of the Collapsing Problem in Semi-Supervised 2D Human Pose Estimation. (arXiv:2011.12498v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samak_T/0/1/0/all/0/1\">Tanmay Vilas Samak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samak_C/0/1/0/all/0/1\">Chinmay Vilas Samak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandhasamy_S/0/1/0/all/0/1\">Sivanathan Kandhasamy</a>",
          "description": "In this work, we present a lightweight pipeline for robust behavioral cloning\nof a human driver using end-to-end imitation learning. The proposed pipeline\nwas employed to train and deploy three distinct driving behavior models onto a\nsimulated vehicle. The training phase comprised of data collection, balancing,\naugmentation, preprocessing and training a neural network, following which, the\ntrained model was deployed onto the ego vehicle to predict steering commands\nbased on the feed from an onboard camera. A novel coupled control law was\nformulated to generate longitudinal control commands on-the-go based on the\npredicted steering angle and other parameters such as actual speed of the ego\nvehicle and the prescribed constraints for speed and steering. We analyzed\ncomputational efficiency of the pipeline and evaluated robustness of the\ntrained models through exhaustive experimentation during the deployment phase.\nWe also compared our approach against state-of-the-art implementation in order\nto comment on its validity.",
          "link": "http://arxiv.org/abs/2010.04767",
          "publishedOn": "2021-08-06T00:51:47.406Z",
          "wordCount": 651,
          "title": "Robust Behavioral Cloning for Autonomous Vehicles using End-to-End Imitation Learning. (arXiv:2010.04767v4 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sanguesa_M/0/1/0/all/0/1\">Marta Girones Sanguesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutnar_D/0/1/0/all/0/1\">Denis Kutnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velden_B/0/1/0/all/0/1\">Bas H.M. van der Velden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijf_H/0/1/0/all/0/1\">Hugo J. Kuijf</a>",
          "description": "Cerebral microbleeds are small, dark, round lesions that can be visualised on\nT2*-weighted MRI or other sequences sensitive to susceptibility effects. In\nthis work, we propose a multi-stage approach to both microbleed detection and\nsegmentation. First, possible microbleed locations are detected with a Mask\nR-CNN technique. Second, at each possible microbleed location, a simple U-Net\nperforms the final segmentation. This work used the 72 subjects as training\ndata provided by the \"Where is VALDO?\" challenge of MICCAI 2021.",
          "link": "http://arxiv.org/abs/2108.02482",
          "publishedOn": "2021-08-06T00:51:47.386Z",
          "wordCount": 528,
          "title": "MixMicrobleed: Multi-stage detection and segmentation of cerebral microbleeds. (arXiv:2108.02482v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jingyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1\">Qi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1\">Hongyang Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "Unsupervised learning is just at a tipping point where it could really take\noff. Among these approaches, contrastive learning has seen tremendous progress\nand led to state-of-the-art performance. In this paper, we construct a novel\nprobabilistic graphical model that effectively incorporates the low rank\npromoting prior into the framework of contrastive learning, referred to as\nLORAC. In contrast to the existing conventional self-supervised approaches that\nonly considers independent learning, our hypothesis explicitly requires that\nall the samples belonging to the same instance class lie on the same subspace\nwith small dimension. This heuristic poses particular joint learning\nconstraints to reduce the degree of freedom of the problem during the search of\nthe optimal network parameterization. Most importantly, we argue that the low\nrank prior employed here is not unique, and many different priors can be\ninvoked in a similar probabilistic way, corresponding to different hypotheses\nabout underlying truth behind the contrastive features. Empirical evidences\nshow that the proposed algorithm clearly surpasses the state-of-the-art\napproaches on multiple benchmarks, including image classification, object\ndetection, instance segmentation and keypoint detection.",
          "link": "http://arxiv.org/abs/2108.02696",
          "publishedOn": "2021-08-06T00:51:47.359Z",
          "wordCount": 629,
          "title": "A Low Rank Promoting Prior for Unsupervised Contrastive Learning. (arXiv:2108.02696v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.02443",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pomponi_J/0/1/0/all/0/1\">Jary Pomponi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scardapane_S/0/1/0/all/0/1\">Simone Scardapane</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Uncini_A/0/1/0/all/0/1\">Aurelio Uncini</a>",
          "description": "Catastrophic forgetting (CF) happens whenever a neural network overwrites\npast knowledge while being trained on new tasks. Common techniques to handle CF\ninclude regularization of the weights (using, e.g., their importance on past\ntasks), and rehearsal strategies, where the network is constantly re-trained on\npast data. Generative models have also been applied for the latter, in order to\nhave endless sources of data. In this paper, we propose a novel method that\ncombines the strengths of regularization and generative-based rehearsal\napproaches. Our generative model consists of a normalizing flow (NF), a\nprobabilistic and invertible neural network, trained on the internal embeddings\nof the network. By keeping a single NF conditioned on the task, we show that\nour memory overhead remains constant. In addition, exploiting the invertibility\nof the NF, we propose a simple approach to regularize the network's embeddings\nwith respect to past tasks. We show that our method performs favorably with\nrespect to state-of-the-art approaches in the literature, with bounded\ncomputational power and memory overheads.",
          "link": "http://arxiv.org/abs/2007.02443",
          "publishedOn": "2021-08-06T00:51:47.351Z",
          "wordCount": 655,
          "title": "Pseudo-Rehearsal for Continual Learning with Normalizing Flows. (arXiv:2007.02443v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.03948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Q/0/1/0/all/0/1\">Qi Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_F/0/1/0/all/0/1\">Fuhong Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>",
          "description": "Recently, convolutional neural networks (CNNs) have set latest\nstate-of-the-art on various human activity recognition (HAR) datasets. However,\ndeep CNNs often require more computing resources, which limits their\napplications in embedded HAR. Although many successful methods have been\nproposed to reduce memory and FLOPs of CNNs, they often involve special network\narchitectures designed for visual tasks, which are not suitable for deep HAR\ntasks with time series sensor signals, due to remarkable discrepancy.\nTherefore, it is necessary to develop lightweight deep models to perform HAR.\nAs filter is the basic unit in constructing CNNs, it deserves further research\nwhether re-designing smaller filters is applicable for deep HAR. In the paper,\ninspired by the idea, we proposed a lightweight CNN using Lego filters for HAR.\nA set of lower-dimensional filters is used as Lego bricks to be stacked for\nconventional filters, which does not rely on any special network structure. The\nlocal loss function is used to train model. To our knowledge, this is the first\npaper that proposes lightweight CNN for HAR in ubiquitous and wearable\ncomputing arena. The experiment results on five public HAR datasets, UCI-HAR\ndataset, OPPORTUNITY dataset, UNIMIB-SHAR dataset, PAMAP2 dataset, and WISDM\ndataset collected from either smartphones or multiple sensor nodes, indicate\nthat our novel Lego CNN with local loss can greatly reduce memory and\ncomputation cost over CNN, while achieving higher accuracy. That is to say, the\nproposed model is smaller, faster and more accurate. Finally, we evaluate the\nactual performance on an Android smartphone.",
          "link": "http://arxiv.org/abs/2005.03948",
          "publishedOn": "2021-08-06T00:51:47.336Z",
          "wordCount": 743,
          "title": "Layer-wise training convolutional neural networks with smaller filters for human activity recognition using wearable sensors. (arXiv:2005.03948v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Ke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>",
          "description": "Multi-label image recognition is a challenging computer vision task of\npractical use. Progresses in this area, however, are often characterized by\ncomplicated methods, heavy computations, and lack of intuitive explanations. To\neffectively capture different spatial regions occupied by objects from\ndifferent categories, we propose an embarrassingly simple module, named\nclass-specific residual attention (CSRA). CSRA generates class-specific\nfeatures for every category by proposing a simple spatial attention score, and\nthen combines it with the class-agnostic average pooling feature. CSRA achieves\nstate-of-the-art results on multilabel recognition, and at the same time is\nmuch simpler than them. Furthermore, with only 4 lines of code, CSRA also leads\nto consistent improvement across many diverse pretrained models and datasets\nwithout any extra training. CSRA is both easy to implement and light in\ncomputations, which also enjoys intuitive explanations and visualizations.",
          "link": "http://arxiv.org/abs/2108.02456",
          "publishedOn": "2021-08-06T00:51:47.323Z",
          "wordCount": 574,
          "title": "Residual Attention: A Simple but Effective Method for Multi-Label Recognition. (arXiv:2108.02456v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoman Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shixiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>",
          "description": "Automatic and accurate tumor segmentation on medical images is in high demand\nto assist physicians with diagnosis and treatment. However, it is difficult to\nobtain massive amounts of annotated training data required by the deep-learning\nmodels as the manual delineation process is often tedious and expertise\nrequired. Although self-supervised learning (SSL) scheme has been widely\nadopted to address this problem, most SSL methods focus only on global\nstructure information, ignoring the key distinguishing features of tumor\nregions: local intensity variation and large size distribution. In this paper,\nwe propose Scale-Aware Restoration (SAR), a SSL method for 3D tumor\nsegmentation. Specifically, a novel proxy task, i.e. scale discrimination, is\nformulated to pre-train the 3D neural network combined with the\nself-restoration task. Thus, the pre-trained model learns multi-level local\nrepresentations through multi-scale inputs. Moreover, an adversarial learning\nmodule is further introduced to learn modality invariant representations from\nmultiple unlabeled source datasets. We demonstrate the effectiveness of our\nmethods on two downstream tasks: i) Brain tumor segmentation, ii) Pancreas\ntumor segmentation. Compared with the state-of-the-art 3D SSL methods, our\nproposed approach can significantly improve the segmentation accuracy. Besides,\nwe analyze its advantages from multiple perspectives such as data efficiency,\nperformance, and convergence speed.",
          "link": "http://arxiv.org/abs/2010.06107",
          "publishedOn": "2021-08-06T00:51:47.316Z",
          "wordCount": 672,
          "title": "SAR: Scale-Aware Restoration Learning for 3D Tumor Segmentation. (arXiv:2010.06107v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>",
          "description": "We present VoxelTrack for multi-person 3D pose estimation and tracking from a\nfew cameras which are separated by wide baselines. It employs a multi-branch\nnetwork to jointly estimate 3D poses and re-identification (Re-ID) features for\nall people in the environment. In contrast to previous efforts which require to\nestablish cross-view correspondence based on noisy 2D pose estimates, it\ndirectly estimates and tracks 3D poses from a 3D voxel-based representation\nconstructed from multi-view images. We first discretize the 3D space by regular\nvoxels and compute a feature vector for each voxel by averaging the body joint\nheatmaps that are inversely projected from all views. We estimate 3D poses from\nthe voxel representation by predicting whether each voxel contains a particular\nbody joint. Similarly, a Re-ID feature is computed for each voxel which is used\nto track the estimated 3D poses over time. The main advantage of the approach\nis that it avoids making any hard decisions based on individual images. The\napproach can robustly estimate and track 3D poses even when people are severely\noccluded in some cameras. It outperforms the state-of-the-art methods by a\nlarge margin on three public datasets including Shelf, Campus and CMU Panoptic.",
          "link": "http://arxiv.org/abs/2108.02452",
          "publishedOn": "2021-08-06T00:51:47.276Z",
          "wordCount": 640,
          "title": "VoxelTrack: Multi-Person 3D Human Pose Estimation and Tracking in the Wild. (arXiv:2108.02452v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1912.05949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1\">Nathanael L. Baisa</a>",
          "description": "We propose a novel online multi-object visual tracker using a Gaussian\nmixture Probability Hypothesis Density (GM-PHD) filter and deep appearance\nlearning. The GM-PHD filter has a linear complexity with the number of objects\nand observations while estimating the states and cardinality of time-varying\nnumber of objects, however, it is susceptible to miss-detections and does not\ninclude the identity of objects. We use visual-spatio-temporal information\nobtained from object bounding boxes and deeply learned appearance\nrepresentations to perform estimates-to-tracks data association for target\nlabeling as well as formulate an augmented likelihood and then integrate into\nthe update step of the GM-PHD filter. We also employ additional unassigned\ntracks prediction after the data association step to overcome the\nsusceptibility of the GM-PHD filter towards miss-detections caused by\nocclusion. Extensive evaluations on MOT16, MOT17 and HiEve benchmark datasets\nshow that our tracker significantly outperforms several state-of-the-art\ntrackers in terms of tracking accuracy and identification.",
          "link": "http://arxiv.org/abs/1912.05949",
          "publishedOn": "2021-08-06T00:51:47.265Z",
          "wordCount": 661,
          "title": "Occlusion-Robust Online Multi-Object Visual Tracking using a GM-PHD Filter with CNN-Based Re-Identification. (arXiv:1912.05949v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02667",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shubao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke-Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Taiping Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_M/0/1/0/all/0/1\">Mingwei Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>",
          "description": "With various face presentation attacks arising under unseen scenarios, face\nanti-spoofing (FAS) based on domain generalization (DG) has drawn growing\nattention due to its robustness. Most existing methods utilize DG frameworks to\nalign the features to seek a compact and generalized feature space. However,\nlittle attention has been paid to the feature extraction process for the FAS\ntask, especially the influence of normalization, which also has a great impact\non the generalization of the learned representation. To address this issue, we\npropose a novel perspective of face anti-spoofing that focuses on the\nnormalization selection in the feature extraction process. Concretely, an\nAdaptive Normalized Representation Learning (ANRL) framework is devised, which\nadaptively selects feature normalization methods according to the inputs,\naiming to learn domain-agnostic and discriminative representation. Moreover, to\nfacilitate the representation learning, Dual Calibration Constraints are\ndesigned, including Inter-Domain Compatible loss and Inter-Class Separable\nloss, which provide a better optimization direction for generalizable\nrepresentation. Extensive experiments and visualizations are presented to\ndemonstrate the effectiveness of our method against the SOTA competitors.",
          "link": "http://arxiv.org/abs/2108.02667",
          "publishedOn": "2021-08-06T00:51:47.250Z",
          "wordCount": 621,
          "title": "Adaptive Normalized Representation Learning for Generalizable Face Anti-Spoofing. (arXiv:2108.02667v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>",
          "description": "Ocean fronts can cause the accumulation of nutrients and affect the\npropagation of underwater sound, so high-precision ocean front detection is of\ngreat significance to the marine fishery and national defense fields. However,\nthe current ocean front detection methods either have low detection accuracy or\nmost can only detect the occurrence of ocean front by binary classification,\nrarely considering the differences of the characteristics of multiple ocean\nfronts in different sea areas. In order to solve the above problems, we propose\na semantic segmentation network called location and seasonality enhanced\nnetwork (LSENet) for multi-class ocean fronts detection at pixel level. In this\nnetwork, we first design a channel supervision unit structure, which integrates\nthe seasonal characteristics of the ocean front itself and the contextual\ninformation to improve the detection accuracy. We also introduce a location\nattention mechanism to adaptively assign attention weights to the fronts\naccording to their frequently occurred sea area, which can further improve the\naccuracy of multi-class ocean front detection. Compared with other semantic\nsegmentation methods and current representative ocean front detection method,\nthe experimental results demonstrate convincingly that our method is more\neffective.",
          "link": "http://arxiv.org/abs/2108.02455",
          "publishedOn": "2021-08-06T00:51:47.189Z",
          "wordCount": 630,
          "title": "LSENet: Location and Seasonality Enhanced Network for Multi-Class Ocean Front Detection. (arXiv:2108.02455v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Wen Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongfang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1\">Chang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yu-Ling Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yun Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xueli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yanhong Tai</a>",
          "description": "Objective: We develop a computer-aided diagnosis (CAD) system using deep\nlearning approaches for lesion detection and classification on whole-slide\nimages (WSIs) with breast cancer. The deep features being distinguishing in\nclassification from the convolutional neural networks (CNN) are demonstrated in\nthis study to provide comprehensive interpretability for the proposed CAD\nsystem using pathological knowledge. Methods: In the experiment, a total of 186\nslides of WSIs were collected and classified into three categories:\nNon-Carcinoma, Ductal Carcinoma in Situ (DCIS), and Invasive Ductal Carcinoma\n(IDC). Instead of conducting pixel-wise classification into three classes\ndirectly, we designed a hierarchical framework with the multi-view scheme that\nperforms lesion detection for region proposal at higher magnification first and\nthen conducts lesion classification at lower magnification for each detected\nlesion. Results: The slide-level accuracy rate for three-category\nclassification reaches 90.8% (99/109) through 5-fold cross-validation and\nachieves 94.8% (73/77) on the testing set. The experimental results show that\nthe morphological characteristics and co-occurrence properties learned by the\ndeep learning models for lesion classification are accordant with the clinical\nrules in diagnosis. Conclusion: The pathological interpretability of the deep\nfeatures not only enhances the reliability of the proposed CAD system to gain\nacceptance from medical specialists, but also facilitates the development of\ndeep learning frameworks for various tasks in pathology. Significance: This\npaper presents a CAD system for pathological image analysis, which fills the\nclinical requirements and can be accepted by medical specialists with providing\nits interpretability from the pathological perspective.",
          "link": "http://arxiv.org/abs/2108.02656",
          "publishedOn": "2021-08-06T00:51:47.169Z",
          "wordCount": 710,
          "title": "A Computer-Aided Diagnosis System for Breast Pathology: A Deep Learning Approach with Model Interpretability from Pathological Perspective. (arXiv:2108.02656v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1\">Juan P. Vigueras-Guill&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1\">Frank Seeliger</a>",
          "description": "The automated segmentation of cancer tissue in histopathology images can help\nclinicians to detect, diagnose, and analyze such disease. Different from other\nnatural images used in many convolutional networks for benchmark,\nhistopathology images can be extremely large, and the cancerous patterns can\nreach beyond 1000 pixels. Therefore, the well-known networks in the literature\nwere never conceived to handle these peculiarities. In this work, we propose a\nFully Convolutional DenseUNet that is particularly designed to solve\nhistopathology problems. We evaluated our network in two public pathology\ndatasets published as challenges in the recent MICCAI 2019: binary segmentation\nin colon cancer images (DigestPath2019), and multi-class segmentation in\nprostate cancer images (Gleason2019), achieving similar and better results than\nthe winners of the challenges, respectively. Furthermore, we discussed some\ngood practices in the training setup to yield the best performance and the main\nchallenges in these histopathology datasets.",
          "link": "http://arxiv.org/abs/2108.02676",
          "publishedOn": "2021-08-06T00:51:47.121Z",
          "wordCount": 609,
          "title": "Redesigning Fully Convolutional DenseUNets for Large Histopathology Images. (arXiv:2108.02676v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yanbin Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>",
          "description": "Transformer achieves remarkable successes in understanding 1 and\n2-dimensional signals (e.g., NLP and Image Content Understanding). As a\npotential alternative to convolutional neural networks, it shares merits of\nstrong interpretability, high discriminative power on hyper-scale data, and\nflexibility in processing varying length inputs. However, its encoders\nnaturally contain computational intensive operations such as pair-wise\nself-attention, incurring heavy computational burden when being applied on the\ncomplex 3-dimensional video signals.\n\nThis paper presents Token Shift Module (i.e., TokShift), a novel,\nzero-parameter, zero-FLOPs operator, for modeling temporal relations within\neach transformer encoder. Specifically, the TokShift barely temporally shifts\npartial [Class] token features back-and-forth across adjacent frames. Then, we\ndensely plug the module into each encoder of a plain 2D vision transformer for\nlearning 3D video representation. It is worth noticing that our TokShift\ntransformer is a pure convolutional-free video transformer pilot with\ncomputational efficiency for video understanding. Experiments on standard\nbenchmarks verify its robustness, effectiveness, and efficiency. Particularly,\nwith input clips of 8/12 frames, the TokShift transformer achieves SOTA\nprecision: 79.83%/80.40% on the Kinetics-400, 66.56% on EGTEA-Gaze+, and 96.80%\non UCF-101 datasets, comparable or better than existing SOTA convolutional\ncounterparts. Our code is open-sourced in:\nhttps://github.com/VideoNetworks/TokShift-Transformer.",
          "link": "http://arxiv.org/abs/2108.02432",
          "publishedOn": "2021-08-06T00:51:47.114Z",
          "wordCount": 640,
          "title": "Token Shift Transformer for Video Classification. (arXiv:2108.02432v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bennequin_E/0/1/0/all/0/1\">Etienne Bennequin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouvier_V/0/1/0/all/0/1\">Victor Bouvier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tami_M/0/1/0/all/0/1\">Myriam Tami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toubhans_A/0/1/0/all/0/1\">Antoine Toubhans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1\">C&#xe9;line Hudelot</a>",
          "description": "Few-Shot Learning (FSL) algorithms have made substantial progress in learning\nnovel concepts with just a handful of labelled data. To classify query\ninstances from novel classes encountered at test-time, they only require a\nsupport set composed of a few labelled samples. FSL benchmarks commonly assume\nthat those queries come from the same distribution as instances in the support\nset. However, in a realistic set-ting, data distribution is plausibly subject\nto change, a situation referred to as Distribution Shift (DS). The present work\naddresses the new and challenging problem of Few-Shot Learning under\nSupport/Query Shift (FSQS) i.e., when support and query instances are sampled\nfrom related but different distributions. Our contributions are the following.\nFirst, we release a testbed for FSQS, including datasets, relevant baselines\nand a protocol for a rigorous and reproducible evaluation. Second, we observe\nthat well-established FSL algorithms unsurprisingly suffer from a considerable\ndrop in accuracy when facing FSQS, stressing the significance of our study.\nFinally, we show that transductive algorithms can limit the inopportune effect\nof DS. In particular, we study both the role of Batch-Normalization and Optimal\nTransport (OT) in aligning distributions, bridging Unsupervised Domain\nAdaptation with FSL. This results in a new method that efficiently combines OT\nwith the celebrated Prototypical Networks. We bring compelling experiments\ndemonstrating the advantage of our method. Our work opens an exciting line of\nresearch by providing a testbed and strong baselines. Our code is available at\nhttps://github.com/ebennequin/meta-domain-shift.",
          "link": "http://arxiv.org/abs/2105.11804",
          "publishedOn": "2021-08-06T00:51:46.615Z",
          "wordCount": 713,
          "title": "Bridging Few-Shot Learning and Adaptation: New Challenges of Support-Query Shift. (arXiv:2105.11804v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akan_A/0/1/0/all/0/1\">Adil Kaan Akan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1\">Erkut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1\">Aykut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guney_F/0/1/0/all/0/1\">Fatma G&#xfc;ney</a>",
          "description": "Motion is an important cue for video prediction and often utilized by\nseparating video content into static and dynamic components. Most of the\nprevious work utilizing motion is deterministic but there are stochastic\nmethods that can model the inherent uncertainty of the future. Existing\nstochastic models either do not reason about motion explicitly or make limiting\nassumptions about the static part. In this paper, we reason about appearance\nand motion in the video stochastically by predicting the future based on the\nmotion history. Explicit reasoning about motion without history already reaches\nthe performance of current stochastic models. The motion history further\nimproves the results by allowing to predict consistent dynamics several frames\ninto the future. Our model performs comparably to the state-of-the-art models\non the generic video prediction datasets, however, significantly outperforms\nthem on two challenging real-world autonomous driving datasets with complex\nmotion and dynamic background.",
          "link": "http://arxiv.org/abs/2108.02760",
          "publishedOn": "2021-08-06T00:51:46.567Z",
          "wordCount": 584,
          "title": "SLAMP: Stochastic Latent Appearance and Motion Prediction. (arXiv:2108.02760v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Haotian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhengyang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>",
          "description": "This paper proposes a hypothesis for the aesthetic appreciation that\naesthetic images make a neural network strengthen salient concepts and discard\ninessential concepts. In order to verify this hypothesis, we use multi-variate\ninteractions to represent salient concepts and inessential concepts contained\nin images. Furthermore, we design a set of operations to revise images towards\nmore beautiful ones. In experiments, we find that the revised images are more\naesthetic than the original ones to some extent.",
          "link": "http://arxiv.org/abs/2108.02646",
          "publishedOn": "2021-08-06T00:51:46.527Z",
          "wordCount": 525,
          "title": "A Hypothesis for the Aesthetic Appreciation in Neural Networks. (arXiv:2108.02646v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2001.04627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>",
          "description": "In this paper, we build on a concept of self-supervision by taking RGB frames\nas input to learn to predict both action concepts and auxiliary descriptors\ne.g., object descriptors. So-called hallucination streams are trained to\npredict auxiliary cues, simultaneously fed into classification layers, and then\nhallucinated at the testing stage to aid network. We design and hallucinate two\ndescriptors, one leveraging four popular object detectors applied to training\nvideos, and the other leveraging image- and video-level saliency detectors. The\nfirst descriptor encodes the detector- and ImageNet-wise class prediction\nscores, confidence scores, and spatial locations of bounding boxes and frame\nindexes to capture the spatio-temporal distribution of features per video.\nAnother descriptor encodes spatio-angular gradient distributions of saliency\nmaps and intensity patterns. Inspired by the characteristic function of the\nprobability distribution, we capture four statistical moments on the above\nintermediate descriptors. As numbers of coefficients in the mean, covariance,\ncoskewness and cokurtotsis grow linearly, quadratically, cubically and\nquartically w.r.t. the dimension of feature vectors, we describe the covariance\nmatrix by its leading n' eigenvectors (so-called subspace) and we capture\nskewness/kurtosis rather than costly coskewness/cokurtosis. We obtain state of\nthe art on five popular datasets such as Charades and EPIC-Kitchens.",
          "link": "http://arxiv.org/abs/2001.04627",
          "publishedOn": "2021-08-06T00:51:46.237Z",
          "wordCount": 664,
          "title": "Self-supervising Action Recognition by Statistical Moment and Subspace Descriptors. (arXiv:2001.04627v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00911",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1\">Chengtao Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1\">Liying Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Huimin Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tong_R/0/1/0/all/0/1\">Ruofeng Tong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Lanfen Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jingsong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Wei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Q/0/1/0/all/0/1\">Qingqing Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1\">Hongjie Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_Z/0/1/0/all/0/1\">Zhiyi Peng</a>",
          "description": "Multi-phase computed tomography (CT) images provide crucial complementary\ninformation for accurate liver tumor segmentation (LiTS). State-of-the-art\nmulti-phase LiTS methods usually fused cross-phase features through\nphase-weighted summation or channel-attention based concatenation. However,\nthese methods ignored the spatial (pixel-wise) relationships between different\nphases, hence leading to insufficient feature integration. In addition, the\nperformance of existing methods remains subject to the uncertainty in\nsegmentation, which is particularly acute in tumor boundary regions. In this\nwork, we propose a novel LiTS method to adequately aggregate multi-phase\ninformation and refine uncertain region segmentation. To this end, we introduce\na spatial aggregation module (SAM), which encourages per-pixel interactions\nbetween different phases, to make full use of cross-phase information.\nMoreover, we devise an uncertain region inpainting module (URIM) to refine\nuncertain pixels using neighboring discriminative features. Experiments on an\nin-house multi-phase CT dataset of focal liver lesions (MPCT-FLLs) demonstrate\nthat our method achieves promising liver tumor segmentation and outperforms\nstate-of-the-arts.",
          "link": "http://arxiv.org/abs/2108.00911",
          "publishedOn": "2021-08-06T00:51:46.221Z",
          "wordCount": 642,
          "title": "Multi-phase Liver Tumor Segmentation with Spatial Aggregation and Uncertain Region Inpainting. (arXiv:2108.00911v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhongjin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Heming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>",
          "description": "Head shapes play an important role in 3D character design. In this work, we\npropose SimpModeling, a novel sketch-based system for helping users, especially\namateur users, easily model 3D animalmorphic heads - a prevalent kind of heads\nin character design. Although sketching provides an easy way to depict desired\nshapes, it is challenging to infer dense geometric information from sparse line\ndrawings. Recently, deepnet-based approaches have been taken to address this\nchallenge and try to produce rich geometric details from very few strokes.\nHowever, while such methods reduce users' workload, they would cause less\ncontrollability of target shapes. This is mainly due to the uncertainty of the\nneural prediction. Our system tackles this issue and provides good\ncontrollability from three aspects: 1) we separate coarse shape design and\ngeometric detail specification into two stages and respectively provide\ndifferent sketching means; 2) in coarse shape designing, sketches are used for\nboth shape inference and geometric constraints to determine global geometry,\nand in geometric detail crafting, sketches are used for carving surface\ndetails; 3) in both stages, we use the advanced implicit-based shape inference\nmethods, which have strong ability to handle the domain gap between freehand\nsketches and synthetic ones used for training. Experimental results confirm the\neffectiveness of our method and the usability of our interactive system. We\nalso contribute to a dataset of high-quality 3D animal heads, which are\nmanually created by artists.",
          "link": "http://arxiv.org/abs/2108.02548",
          "publishedOn": "2021-08-06T00:51:46.148Z",
          "wordCount": 685,
          "title": "SimpModeling: Sketching Implicit Field to Guide Mesh Modeling for 3D Animalmorphic Head Design. (arXiv:2108.02548v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Garcia_F/0/1/0/all/0/1\">Fernando P&#xe9;rez-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scott_C/0/1/0/all/0/1\">Catherine Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sparks_R/0/1/0/all/0/1\">Rachel Sparks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diehl_B/0/1/0/all/0/1\">Beate Diehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1\">S&#xe9;bastien Ourselin</a>",
          "description": "Detailed analysis of seizure semiology, the symptoms and signs which occur\nduring a seizure, is critical for management of epilepsy patients. Inter-rater\nreliability using qualitative visual analysis is often poor for semiological\nfeatures. Therefore, automatic and quantitative analysis of video-recorded\nseizures is needed for objective assessment.\n\nWe present GESTURES, a novel architecture combining convolutional neural\nnetworks (CNNs) and recurrent neural networks (RNNs) to learn deep\nrepresentations of arbitrarily long videos of epileptic seizures.\n\nWe use a spatiotemporal CNN (STCNN) pre-trained on large human action\nrecognition (HAR) datasets to extract features from short snippets (approx. 0.5\ns) sampled from seizure videos. We then train an RNN to learn seizure-level\nrepresentations from the sequence of features.\n\nWe curated a dataset of seizure videos from 68 patients and evaluated\nGESTURES on its ability to classify seizures into focal onset seizures (FOSs)\n(N = 106) vs. focal to bilateral tonic-clonic seizures (TCSs) (N = 77),\nobtaining an accuracy of 98.9% using bidirectional long short-term memory\n(BLSTM) units.\n\nWe demonstrate that an STCNN trained on a HAR dataset can be used in\ncombination with an RNN to accurately represent arbitrarily long videos of\nseizures. GESTURES can provide accurate seizure classification by modeling\nsequences of semiologies.",
          "link": "http://arxiv.org/abs/2106.12014",
          "publishedOn": "2021-08-06T00:51:46.021Z",
          "wordCount": 690,
          "title": "Transfer Learning of Deep Spatiotemporal Networks to Model Arbitrarily Long Videos of Seizures. (arXiv:2106.12014v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1\">Tsang Ing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1\">Cleber Zanchettin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Adriano L. I. Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "In this paper, we argue that the unsatisfactory out-of-distribution (OOD)\ndetection performance of neural networks is mainly due to the SoftMax loss\nanisotropy and propensity to produce low entropy probability distributions in\ndisagreement with the principle of maximum entropy. Current out-of-distribution\n(OOD) detection approaches usually do not directly fix the SoftMax loss\ndrawbacks, but rather build techniques to circumvent it. Unfortunately, those\nmethods usually produce undesired side effects (e.g., classification accuracy\ndrop, additional hyperparameters, slower inferences, and collecting extra\ndata). In the opposite direction, we propose replacing SoftMax loss with a\nnovel loss function that does not suffer from the mentioned weaknesses. The\nproposed IsoMax loss is isotropic (exclusively distance-based) and provides\nhigh entropy posterior probability distributions. Replacing the SoftMax loss by\nIsoMax loss requires no model or training changes. Additionally, the models\ntrained with IsoMax loss produce as fast and energy-efficient inferences as\nthose trained using SoftMax loss. Moreover, no classification accuracy drop is\nobserved. The proposed method does not rely on outlier/background data,\nhyperparameter tuning, temperature calibration, feature extraction, metric\nlearning, adversarial training, ensemble procedures, or generative models. Our\nexperiments showed that IsoMax loss works as a seamless SoftMax loss drop-in\nreplacement that significantly improves neural networks' OOD detection\nperformance. Hence, it may be used as a baseline OOD detection approach to be\ncombined with current or future OOD detection techniques to achieve even higher\nresults.",
          "link": "http://arxiv.org/abs/2006.04005",
          "publishedOn": "2021-08-06T00:51:46.014Z",
          "wordCount": 734,
          "title": "Entropic Out-of-Distribution Detection: Seamless Detection of Unknown Examples. (arXiv:2006.04005v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guanglei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Mingli Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>",
          "description": "While convolutional neural networks have shown a tremendous impact on various\ncomputer vision tasks, they generally demonstrate limitations in explicitly\nmodeling long-range dependencies due to the intrinsic locality of the\nconvolution operation. Initially designed for natural language processing\ntasks, Transformers have emerged as alternative architectures with innate\nglobal self-attention mechanisms to capture long-range dependencies. In this\npaper, we propose TransDepth, an architecture that benefits from both\nconvolutional neural networks and transformers. To avoid the network losing its\nability to capture local-level details due to the adoption of transformers, we\npropose a novel decoder that employs attention mechanisms based on gates.\nNotably, this is the first paper that applies transformers to pixel-wise\nprediction problems involving continuous labels (i.e., monocular depth\nprediction and surface normal estimation). Extensive experiments demonstrate\nthat the proposed TransDepth achieves state-of-the-art performance on three\nchallenging datasets. Our code is available at:\nhttps://github.com/ygjwd12345/TransDepth.",
          "link": "http://arxiv.org/abs/2103.12091",
          "publishedOn": "2021-08-06T00:51:45.988Z",
          "wordCount": 613,
          "title": "Transformer-Based Attention Networks for Continuous Pixel-Wise Prediction. (arXiv:2103.12091v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12636",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fengxiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Detection transformers have recently shown promising object detection results\nand attracted increasing attention. However, how to develop effective domain\nadaptation techniques to improve its cross-domain performance remains\nunexplored and unclear. In this paper, we delve into this topic and empirically\nfind that direct feature distribution alignment on the CNN backbone only brings\nlimited improvements, as it does not guarantee domain-invariant sequence\nfeatures in the transformer for prediction. To address this issue, we propose a\nnovel Sequence Feature Alignment (SFA) method that is specially designed for\nthe adaptation of detection transformers. Technically, SFA consists of a domain\nquery-based feature alignment (DQFA) module and a token-wise feature alignment\n(TDA) module. In DQFA, a novel domain query is used to aggregate and align\nglobal context from the token sequence of both domains. DQFA reduces the domain\ndiscrepancy in global feature representations and object relations when\ndeploying in the transformer encoder and decoder, respectively. Meanwhile, TDA\naligns token features in the sequence from both domains, which reduces the\ndomain gaps in local and instance-level feature representations in the\ntransformer encoder and decoder, respectively. Besides, a novel bipartite\nmatching consistency loss is proposed to enhance the feature discriminability\nfor robust object detection. Experiments on three challenging benchmarks show\nthat SFA outperforms state-of-the-art domain adaptive object detection methods.\nCode has been made available at: https://github.com/encounter1997/SFA.",
          "link": "http://arxiv.org/abs/2107.12636",
          "publishedOn": "2021-08-06T00:51:45.914Z",
          "wordCount": 726,
          "title": "Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.04696",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Perez_Garcia_F/0/1/0/all/0/1\">Fernando P&#xe9;rez-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sparks_R/0/1/0/all/0/1\">Rachel Sparks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">S&#xe9;bastien Ourselin</a>",
          "description": "Processing of medical images such as MRI or CT presents unique challenges\ncompared to RGB images typically used in computer vision. These include a lack\nof labels for large datasets, high computational costs, and metadata to\ndescribe the physical properties of voxels. Data augmentation is used to\nartificially increase the size of the training datasets. Training with image\npatches decreases the need for computational power. Spatial metadata needs to\nbe carefully taken into account in order to ensure a correct alignment of\nvolumes.\n\nWe present TorchIO, an open-source Python library to enable efficient\nloading, preprocessing, augmentation and patch-based sampling of medical images\nfor deep learning. TorchIO follows the style of PyTorch and integrates standard\nmedical image processing libraries to efficiently process images during\ntraining of neural networks. TorchIO transforms can be composed, reproduced,\ntraced and extended. We provide multiple generic preprocessing and augmentation\noperations as well as simulation of MRI-specific artifacts.\n\nSource code, comprehensive tutorials and extensive documentation for TorchIO\ncan be found at https://torchio.rtfd.io/. The package can be installed from the\nPython Package Index running 'pip install torchio'. It includes a command-line\ninterface which allows users to apply transforms to image files without using\nPython. Additionally, we provide a graphical interface within a TorchIO\nextension in 3D Slicer to visualize the effects of transforms.\n\nTorchIO was developed to help researchers standardize medical image\nprocessing pipelines and allow them to focus on the deep learning experiments.\nIt encourages open science, as it supports reproducibility and is version\ncontrolled so that the software can be cited precisely. Due to its modularity,\nthe library is compatible with other frameworks for deep learning with medical\nimages.",
          "link": "http://arxiv.org/abs/2003.04696",
          "publishedOn": "2021-08-06T00:51:45.884Z",
          "wordCount": 819,
          "title": "TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning. (arXiv:2003.04696v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1\">Emanuel Ben-Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1\">Lihi Zelnik-Manor</a>",
          "description": "ImageNet-1K serves as the primary dataset for pretraining deep learning\nmodels for computer vision tasks. ImageNet-21K dataset, which is bigger and\nmore diverse, is used less frequently for pretraining, mainly due to its\ncomplexity, low accessibility, and underestimation of its added value. This\npaper aims to close this gap, and make high-quality efficient pretraining on\nImageNet-21K available for everyone. Via a dedicated preprocessing stage,\nutilization of WordNet hierarchical structure, and a novel training scheme\ncalled semantic softmax, we show that various models significantly benefit from\nImageNet-21K pretraining on numerous datasets and tasks, including small\nmobile-oriented models. We also show that we outperform previous ImageNet-21K\npretraining schemes for prominent new models like ViT and Mixer. Our proposed\npretraining pipeline is efficient, accessible, and leads to SoTA reproducible\nresults, from a publicly available dataset. The training code and pretrained\nmodels are available at: https://github.com/Alibaba-MIIL/ImageNet21K",
          "link": "http://arxiv.org/abs/2104.10972",
          "publishedOn": "2021-08-06T00:51:45.876Z",
          "wordCount": 632,
          "title": "ImageNet-21K Pretraining for the Masses. (arXiv:2104.10972v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00531",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pourreza_R/0/1/0/all/0/1\">Reza Pourreza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_T/0/1/0/all/0/1\">Taco S Cohen</a>",
          "description": "While most neural video codecs address P-frame coding (predicting each frame\nfrom past ones), in this paper we address B-frame compression (predicting\nframes using both past and future reference frames). Our B-frame solution is\nbased on the existing P-frame methods. As a result, B-frame coding capability\ncan easily be added to an existing neural codec. The basic idea of our B-frame\ncoding method is to interpolate the two reference frames to generate a single\nreference frame and then use it together with an existing P-frame codec to\nencode the input B-frame. Our studies show that the interpolated frame is a\nmuch better reference for the P-frame codec compared to using the previous\nframe as is usually done. Our results show that using the proposed method with\nan existing P-frame codec can lead to 28.5%saving in bit-rate on the UVG\ndataset compared to the P-frame codec while generating the same video quality.",
          "link": "http://arxiv.org/abs/2104.00531",
          "publishedOn": "2021-08-06T00:51:45.858Z",
          "wordCount": 614,
          "title": "Extending Neural P-frame Codecs for B-frame Coding. (arXiv:2104.00531v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sim_H/0/1/0/all/0/1\">Hyeonjun Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jihyong Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Munchurl Kim</a>",
          "description": "In this paper, we firstly present a dataset (X4K1000FPS) of 4K videos of 1000\nfps with the extreme motion to the research community for video frame\ninterpolation (VFI), and propose an extreme VFI network, called XVFI-Net, that\nfirst handles the VFI for 4K videos with large motion. The XVFI-Net is based on\na recursive multi-scale shared structure that consists of two cascaded modules\nfor bidirectional optical flow learning between two input frames (BiOF-I) and\nfor bidirectional optical flow learning from target to input frames (BiOF-T).\nThe optical flows are stably approximated by a complementary flow reversal\n(CFR) proposed in BiOF-T module. During inference, the BiOF-I module can start\nat any scale of input while the BiOF-T module only operates at the original\ninput scale so that the inference can be accelerated while maintaining highly\naccurate VFI performance. Extensive experimental results show that our XVFI-Net\ncan successfully capture the essential information of objects with extremely\nlarge motions and complex textures while the state-of-the-art methods exhibit\npoor performance. Furthermore, our XVFI-Net framework also performs comparably\non the previous lower resolution benchmark dataset, which shows a robustness of\nour algorithm as well. All source codes, pre-trained models, and proposed\nX4K1000FPS datasets are publicly available at\nhttps://github.com/JihyongOh/XVFI.",
          "link": "http://arxiv.org/abs/2103.16206",
          "publishedOn": "2021-08-06T00:51:45.850Z",
          "wordCount": 675,
          "title": "XVFI: eXtreme Video Frame Interpolation. (arXiv:2103.16206v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Osco_L/0/1/0/all/0/1\">Lucas Prado Osco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_J/0/1/0/all/0/1\">Jos&#xe9; Marcato Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramos_A/0/1/0/all/0/1\">Ana Paula Marques Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorge_L/0/1/0/all/0/1\">L&#xfa;cio Andr&#xe9; de Castro Jorge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatholahi_S/0/1/0/all/0/1\">Sarah Narges Fatholahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1\">Jonathan de Andrade Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_E/0/1/0/all/0/1\">Edson Takashi Matsubara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pistori_H/0/1/0/all/0/1\">Hemerson Pistori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_W/0/1/0/all/0/1\">Wesley Nunes Gon&#xe7;alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jonathan Li</a>",
          "description": "Deep Neural Networks (DNNs) learn representation from data with an impressive\ncapability, and brought important breakthroughs for processing images,\ntime-series, natural language, audio, video, and many others. In the remote\nsensing field, surveys and literature revisions specifically involving DNNs\nalgorithms' applications have been conducted in an attempt to summarize the\namount of information produced in its subfields. Recently, Unmanned Aerial\nVehicles (UAV) based applications have dominated aerial sensing research.\nHowever, a literature revision that combines both \"deep learning\" and \"UAV\nremote sensing\" thematics has not yet been conducted. The motivation for our\nwork was to present a comprehensive review of the fundamentals of Deep Learning\n(DL) applied in UAV-based imagery. We focused mainly on describing\nclassification and regression techniques used in recent applications with\nUAV-acquired data. For that, a total of 232 papers published in international\nscientific journal databases was examined. We gathered the published material\nand evaluated their characteristics regarding application, sensor, and\ntechnique used. We relate how DL presents promising results and has the\npotential for processing tasks associated with UAV-based image data. Lastly, we\nproject future perspectives, commentating on prominent DL paths to be explored\nin the UAV remote sensing field. Our revision consists of a friendly-approach\nto introduce, commentate, and summarize the state-of-the-art in UAV-based image\napplications with DNNs algorithms in diverse subfields of remote sensing,\ngrouping it in the environmental, urban, and agricultural contexts.",
          "link": "http://arxiv.org/abs/2101.10861",
          "publishedOn": "2021-08-06T00:51:45.844Z",
          "wordCount": 729,
          "title": "A Review on Deep Learning in UAV Remote Sensing. (arXiv:2101.10861v2 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Canyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eschweiler_D/0/1/0/all/0/1\">Dennis Eschweiler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stegmaier_J/0/1/0/all/0/1\">Johannes Stegmaier</a>",
          "description": "Recent developments in fluorescence microscopy allow capturing\nhigh-resolution 3D images over time for living model organisms. To be able to\nimage even large specimens, techniques like multi-view light-sheet imaging\nrecord different orientations at each time point that can then be fused into a\nsingle high-quality volume. Based on measured point spread functions (PSF),\ndeconvolution and content fusion are able to largely revert the inevitable\ndegradation occurring during the imaging process. Classical multi-view\ndeconvolution and fusion methods mainly use iterative procedures and\ncontent-based averaging. Lately, Convolutional Neural Networks (CNNs) have been\ndeployed to approach 3D single-view deconvolution microscopy, but the\nmulti-view case waits to be studied. We investigated the efficacy of CNN-based\nmulti-view deconvolution and fusion with two synthetic data sets that mimic\ndeveloping embryos and involve either two or four complementary 3D views.\nCompared with classical state-of-the-art methods, the proposed semi- and\nself-supervised models achieve competitive and superior deconvolution and\nfusion quality in the two-view and quad-view cases, respectively.",
          "link": "http://arxiv.org/abs/2108.02743",
          "publishedOn": "2021-08-06T00:51:45.837Z",
          "wordCount": 620,
          "title": "Semi- and Self-Supervised Multi-View Fusion of 3D Microscopy Images using Generative Adversarial Networks. (arXiv:2108.02743v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1\">Shuyi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protter_M/0/1/0/all/0/1\">Matan Protter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimerman_G/0/1/0/all/0/1\">Gadi Zimerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghui Xu</a>",
          "description": "Recent deep generative models have achieved promising performance in image\ninpainting. However, it is still very challenging for a neural network to\ngenerate realistic image details and textures, due to its inherent spectral\nbias. By our understanding of how artists work, we suggest to adopt a\n`structure first detail next' workflow for image inpainting. To this end, we\npropose to build a Pyramid Generator by stacking several sub-generators, where\nlower-layer sub-generators focus on restoring image structures while the\nhigher-layer sub-generators emphasize image details. Given an input image, it\nwill be gradually restored by going through the entire pyramid in a bottom-up\nfashion. Particularly, our approach has a learning scheme of progressively\nincreasing hole size, which allows it to restore large-hole images. In\naddition, our method could fully exploit the benefits of learning with\nhigh-resolution images, and hence is suitable for high-resolution image\ninpainting. Extensive experimental results on benchmark datasets have validated\nthe effectiveness of our approach compared with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.08905",
          "publishedOn": "2021-08-06T00:51:45.826Z",
          "wordCount": 634,
          "title": "Structure First Detail Next: Image Inpainting with Pyramid Generator. (arXiv:2106.08905v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">HanQin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamm_K/0/1/0/all/0/1\">Keaton Hamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Longxiu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Needell_D/0/1/0/all/0/1\">Deanna Needell</a>",
          "description": "This paper considers the use of Robust PCA in a CUR decomposition framework\nand applications thereof. Our main algorithms produce a robust version of\ncolumn-row factorizations of matrices $\\mathbf{D}=\\mathbf{L}+\\mathbf{S}$ where\n$\\mathbf{L}$ is low-rank and $\\mathbf{S}$ contains sparse outliers. These\nmethods yield interpretable factorizations at low computational cost, and\nprovide new CUR decompositions that are robust to sparse outliers, in contrast\nto previous methods. We consider two key imaging applications of Robust PCA:\nvideo foreground-background separation and face modeling. This paper examines\nthe qualitative behavior of our Robust CUR decompositions on the benchmark\nvideos and face datasets, and find that our method works as well as standard\nRobust PCA while being significantly faster. Additionally, we consider hybrid\nrandomized and deterministic sampling methods which produce a compact CUR\ndecomposition of a given matrix, and apply this to video sequences to produce\ncanonical frames thereof.",
          "link": "http://arxiv.org/abs/2101.05231",
          "publishedOn": "2021-08-06T00:51:45.810Z",
          "wordCount": 624,
          "title": "Robust CUR Decomposition: Theory and Imaging Applications. (arXiv:2101.05231v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1\">Stephen Bates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1\">Anastasios Angelopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1\">Lihua Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "While improving prediction accuracy has been the focus of machine learning in\nrecent years, this alone does not suffice for reliable decision-making.\nDeploying learning systems in consequential settings also requires calibrating\nand communicating the uncertainty of predictions. To convey instance-wise\nuncertainty for prediction tasks, we show how to generate set-valued\npredictions from a black-box predictor that control the expected loss on future\ntest points at a user-specified level. Our approach provides explicit\nfinite-sample guarantees for any dataset by using a holdout set to calibrate\nthe size of the prediction sets. This framework enables simple,\ndistribution-free, rigorous error control for many tasks, and we demonstrate it\nin five large-scale machine learning problems: (1) classification problems\nwhere some mistakes are more costly than others; (2) multi-label\nclassification, where each observation has multiple associated labels; (3)\nclassification problems where the labels have a hierarchical structure; (4)\nimage segmentation, where we wish to predict a set of pixels containing an\nobject of interest; and (5) protein structure prediction. Lastly, we discuss\nextensions to uncertainty quantification for ranking, metric learning and\ndistributionally robust learning.",
          "link": "http://arxiv.org/abs/2101.02703",
          "publishedOn": "2021-08-06T00:51:45.778Z",
          "wordCount": 677,
          "title": "Distribution-Free, Risk-Controlling Prediction Sets. (arXiv:2101.02703v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1\">Olivier J. H&#xe9;naff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1\">Skanda Koppula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1\">Jean-Baptiste Alayrac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oord_A/0/1/0/all/0/1\">Aaron van den Oord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#xe3;o Carreira</a>",
          "description": "Self-supervised pretraining has been shown to yield powerful representations\nfor transfer learning. These performance gains come at a large computational\ncost however, with state-of-the-art methods requiring an order of magnitude\nmore computation than supervised pretraining. We tackle this computational\nbottleneck by introducing a new self-supervised objective, contrastive\ndetection, which tasks representations with identifying object-level features\nacross augmentations. This objective extracts a rich learning signal per image,\nleading to state-of-the-art transfer accuracy on a variety of downstream tasks,\nwhile requiring up to 10x less pretraining. In particular, our strongest\nImageNet-pretrained model performs on par with SEER, one of the largest\nself-supervised systems to date, which uses 1000x more pretraining data.\nFinally, our objective seamlessly handles pretraining on more complex images\nsuch as those in COCO, closing the gap with supervised transfer learning from\nCOCO to PASCAL.",
          "link": "http://arxiv.org/abs/2103.10957",
          "publishedOn": "2021-08-06T00:51:45.764Z",
          "wordCount": 610,
          "title": "Efficient Visual Pretraining with Contrastive Detection. (arXiv:2103.10957v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunsong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "In this paper, we propose an instance similarity learning (ISL) method for\nunsupervised feature representation. Conventional methods assign close instance\npairs in the feature space with high similarity, which usually leads to wrong\npairwise relationship for large neighborhoods because the Euclidean distance\nfails to depict the true semantic similarity on the feature manifold. On the\ncontrary, our method mines the feature manifold in an unsupervised manner,\nthrough which the semantic similarity among instances is learned in order to\nobtain discriminative representations. Specifically, we employ the Generative\nAdversarial Networks (GAN) to mine the underlying feature manifold, where the\ngenerated features are applied as the proxies to progressively explore the\nfeature manifold so that the semantic similarity among instances is acquired as\nreliable pseudo supervision. Extensive experiments on image classification\ndemonstrate the superiority of our method compared with the state-of-the-art\nmethods. The code is available at https://github.com/ZiweiWangTHU/ISL.git.",
          "link": "http://arxiv.org/abs/2108.02721",
          "publishedOn": "2021-08-06T00:51:45.757Z",
          "wordCount": 588,
          "title": "Instance Similarity Learning for Unsupervised Feature Representation. (arXiv:2108.02721v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02740",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1\">Maks Ovsjanikov</a>",
          "description": "In this work, we propose UPDesc, an unsupervised method to learn point\ndescriptors for robust point cloud registration. Our work builds upon a recent\nsupervised 3D CNN-based descriptor extraction framework, namely, 3DSmoothNet,\nwhich leverages a voxel-based representation to parameterize the surrounding\ngeometry of interest points. Instead of using a predefined fixed-size local\nsupport in voxelization, which potentially limits the access of richer local\ngeometry information, we propose to learn the support size in a data-driven\nmanner. To this end, we design a differentiable voxelization module that can\nback-propagate gradients to the support size optimization. To optimize\ndescriptor similarity, the prior 3D CNN work and other supervised methods\nrequire abundant correspondence labels or pose annotations of point clouds for\ncrafting metric learning losses. Differently, we show that unsupervised\nlearning of descriptor similarity can be achieved by performing geometric\nregistration in networks. Our learning objectives consider descriptor\nsimilarity both across and within point clouds without supervision. Through\nextensive experiments on point cloud registration benchmarks, we show that our\nlearned descriptors yield superior performance over existing unsupervised\nmethods.",
          "link": "http://arxiv.org/abs/2108.02740",
          "publishedOn": "2021-08-06T00:51:45.695Z",
          "wordCount": 612,
          "title": "UPDesc: Unsupervised Point Descriptor Learning for Robust Registration. (arXiv:2108.02740v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1\">Juan P. Vigueras-Guill&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_A/0/1/0/all/0/1\">Arijit Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engkvist_O/0/1/0/all/0/1\">Ola Engkvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1\">Frank Seeliger</a>",
          "description": "Capsule Networks (CapsNets) is a machine learning architecture proposed to\novercome some of the shortcomings of convolutional neural networks (CNNs).\nHowever, CapsNets have mainly outperformed CNNs in datasets where images are\nsmall and/or the objects to identify have minimal background noise. In this\nwork, we present a new architecture, parallel CapsNets, which exploits the\nconcept of branching the network to isolate certain capsules, allowing each\nbranch to identify different entities. We applied our concept to the two\ncurrent types of CapsNet architectures, studying the performance for networks\nwith different layers of capsules. We tested our design in a public, highly\nunbalanced dataset of acute myeloid leukaemia images (15 classes). Our\nexperiments showed that conventional CapsNets show similar performance than our\nbaseline CNN (ResNeXt-50) but depict instability problems. In contrast,\nparallel CapsNets can outperform ResNeXt-50, is more stable, and shows better\nrotational invariance than both, conventional CapsNets and ResNeXt-50.",
          "link": "http://arxiv.org/abs/2108.02644",
          "publishedOn": "2021-08-06T00:51:45.676Z",
          "wordCount": 607,
          "title": "Parallel Capsule Networks for Classification of White Blood Cells. (arXiv:2108.02644v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1\">Qiang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1\">Nanxuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guoqiang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>",
          "description": "The fully convolutional network (FCN) has dominated salient object detection\nfor a long period. However, the locality of CNN requires the model deep enough\nto have a global receptive field and such a deep model always leads to the loss\nof local details. In this paper, we introduce a new attention-based encoder,\nvision transformer, into salient object detection to ensure the globalization\nof the representations from shallow to deep layers. With the global view in\nvery shallow layers, the transformer encoder preserves more local\nrepresentations to recover the spatial details in final saliency maps. Besides,\nas each layer can capture a global view of its previous layer, adjacent layers\ncan implicitly maximize the representation differences and minimize the\nredundant features, making that every output feature of transformer layers\ncontributes uniquely for final prediction. To decode features from the\ntransformer, we propose a simple yet effective deeply-transformed decoder. The\ndecoder densely decodes and upsamples the transformer features, generating the\nfinal saliency map with less noise injection. Experimental results demonstrate\nthat our method significantly outperforms other FCN-based and transformer-based\nmethods in five benchmarks by a large margin, with an average of 12.17%\nimprovement in terms of Mean Absolute Error (MAE). Code will be available at\nhttps://github.com/OliverRensu/GLSTR.",
          "link": "http://arxiv.org/abs/2108.02759",
          "publishedOn": "2021-08-06T00:51:45.669Z",
          "wordCount": 646,
          "title": "Unifying Global-Local Representations in Salient Object Detection with Transformer. (arXiv:2108.02759v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12469",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thoreau_M/0/1/0/all/0/1\">Michael Thoreau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilson_F/0/1/0/all/0/1\">Frazer Wilson</a>",
          "description": "Access to high resolution satellite imagery has dramatically increased in\nrecent years as several new constellations have entered service. High revisit\nfrequencies as well as improved resolution has widened the use cases of\nsatellite imagery to areas such as humanitarian relief and even Search and\nRescue (SaR). We propose a novel remote sensing object detection dataset for\ndeep learning assisted SaR. This dataset contains only small objects that have\nbeen identified as potential targets as part of a live SaR response. We\nevaluate the application of popular object detection models to this dataset as\na baseline to inform further research. We also propose a novel object detection\nmetric, specifically designed to be used in a deep learning assisted SaR\nsetting.",
          "link": "http://arxiv.org/abs/2107.12469",
          "publishedOn": "2021-08-06T00:51:45.663Z",
          "wordCount": 597,
          "title": "SaRNet: A Dataset for Deep Learning Assisted Search and Rescue with Satellite Imagery. (arXiv:2107.12469v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.06610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Havasi_M/0/1/0/all/0/1\">Marton Havasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1\">Rodolphe Jenatton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jeremiah Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_J/0/1/0/all/0/1\">Jasper Snoek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Dustin Tran</a>",
          "description": "Recent approaches to efficiently ensemble neural networks have shown that\nstrong robustness and uncertainty performance can be achieved with a negligible\ngain in parameters over the original network. However, these methods still\nrequire multiple forward passes for prediction, leading to a significant\ncomputational cost. In this work, we show a surprising result: the benefits of\nusing multiple predictions can be achieved `for free' under a single model's\nforward pass. In particular, we show that, using a multi-input multi-output\n(MIMO) configuration, one can utilize a single model's capacity to train\nmultiple subnetworks that independently learn the task at hand. By ensembling\nthe predictions made by the subnetworks, we improve model robustness without\nincreasing compute. We observe a significant improvement in negative\nlog-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet,\nand their out-of-distribution variants compared to previous methods.",
          "link": "http://arxiv.org/abs/2010.06610",
          "publishedOn": "2021-08-06T00:51:45.656Z",
          "wordCount": 632,
          "title": "Training independent subnetworks for robust prediction. (arXiv:2010.06610v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02725",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shaunak Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_M/0/1/0/all/0/1\">Mikhail Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_G/0/1/0/all/0/1\">Gaurav Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sviridenko_M/0/1/0/all/0/1\">Maxim Sviridenko</a>",
          "description": "Numerous online stock image libraries offer high quality yet copyright free\nimages for use in marketing campaigns. To assist advertisers in navigating such\nthird party libraries, we study the problem of automatically fetching relevant\nad images given the ad text (via a short textual query for images). Motivated\nby our observations in logged data on ad image search queries (given ad text),\nwe formulate a keyword extraction problem, where a keyword extracted from the\nad text (or its augmented version) serves as the ad image query. In this\ncontext, we propose VisualTextRank: an unsupervised method to (i) augment input\nad text using semantically similar ads, and (ii) extract the image query from\nthe augmented ad text. VisualTextRank builds on prior work on graph based\ncontext extraction (biased TextRank in particular) by leveraging both the text\nand image of similar ads for better keyword extraction, and using advertiser\ncategory specific biasing with sentence-BERT embeddings. Using data collected\nfrom the Verizon Media Native (Yahoo Gemini) ad platform's stock image search\nfeature for onboarding advertisers, we demonstrate the superiority of\nVisualTextRank compared to competitive keyword extraction baselines (including\nan $11\\%$ accuracy lift over biased TextRank). For the case when the stock\nimage library is restricted to English queries, we show the effectiveness of\nVisualTextRank on multilingual ads (translated to English) while leveraging\nsemantically similar English ads. Online tests with a simplified version of\nVisualTextRank led to a 28.7% increase in the usage of stock image search, and\na 41.6% increase in the advertiser onboarding rate in the Verizon Media Native\nad platform.",
          "link": "http://arxiv.org/abs/2108.02725",
          "publishedOn": "2021-08-06T00:51:45.649Z",
          "wordCount": 714,
          "title": "VisualTextRank: Unsupervised Graph-based Content Extraction for Automating Ad Text to Image Search. (arXiv:2108.02725v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuang_H/0/1/0/all/0/1\">Haofei Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwertfeger_S/0/1/0/all/0/1\">S&#xf6;ren Schwertfeger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stachniss_C/0/1/0/all/0/1\">Cyrill Stachniss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>",
          "description": "Contrastive learning has revolutionized self-supervised image representation\nlearning field, and recently been adapted to video domain. One of the greatest\nadvantages of contrastive learning is that it allows us to flexibly define\npowerful loss objectives as long as we can find a reasonable way to formulate\npositive and negative samples to contrast. However, existing approaches rely\nheavily on the short-range spatiotemporal salience to form clip-level\ncontrastive signals, thus limit themselves from using global context. In this\npaper, we propose a new video-level contrastive learning method based on\nsegments to formulate positive pairs. Our formulation is able to capture global\ncontext in a video, thus robust to temporal content change. We also incorporate\na temporal order regularization term to enforce the inherent sequential\nstructure of videos. Extensive experiments show that our video-level\ncontrastive learning framework (VCLR) is able to outperform previous\nstate-of-the-arts on five video datasets for downstream action classification,\naction localization and video retrieval. Code is available at\nhttps://github.com/amazon-research/video-contrastive-learning.",
          "link": "http://arxiv.org/abs/2108.02722",
          "publishedOn": "2021-08-06T00:51:45.641Z",
          "wordCount": 618,
          "title": "Video Contrastive Learning with Global Context. (arXiv:2108.02722v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mehra_V/0/1/0/all/0/1\">Vipul Mehra</a>",
          "description": "This thesis is divided into two parts:Part I: Analysis of Fruits, Vegetables,\nCheese and Fish based on Image Processing using Computer Vision and Deep\nLearning: A Review. It consists of a comprehensive review of image processing,\ncomputer vision and deep learning techniques applied to carry out analysis of\nfruits, vegetables, cheese and fish.This part also serves as a literature\nreview for Part II.Part II: GuavaNet: A deep neural network architecture for\nautomatic sensory evaluation to predict degree of acceptability for Guava by a\nconsumer. This part introduces to an end-to-end deep neural network\narchitecture that can predict the degree of acceptability by the consumer for a\nguava based on sensory evaluation.",
          "link": "http://arxiv.org/abs/2108.02563",
          "publishedOn": "2021-08-06T00:51:45.622Z",
          "wordCount": 574,
          "title": "GuavaNet: A deep neural network architecture for automatic sensory evaluation to predict degree of acceptability for Guava by a consumer. (arXiv:2108.02563v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Palafox_P/0/1/0/all/0/1\">Pablo Palafox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozic_A/0/1/0/all/0/1\">Alja&#x17e; Bo&#x17e;i&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Justus Thies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>",
          "description": "Parametric 3D models have enabled a wide variety of tasks in computer\ngraphics and vision, such as modeling human bodies, faces, and hands. However,\nthe construction of these parametric models is often tedious, as it requires\nheavy manual tweaking, and they struggle to represent additional complexity and\ndetails such as wrinkles or clothing. To this end, we propose Neural Parametric\nModels (NPMs), a novel, learned alternative to traditional, parametric 3D\nmodels, which does not require hand-crafted, object-specific constraints. In\nparticular, we learn to disentangle 4D dynamics into latent-space\nrepresentations of shape and pose, leveraging the flexibility of recent\ndevelopments in learned implicit functions. Crucially, once learned, our neural\nparametric models of shape and pose enable optimization over the learned spaces\nto fit to new observations, similar to the fitting of a traditional parametric\nmodel, e.g., SMPL. This enables NPMs to achieve a significantly more accurate\nand detailed representation of observed deformable sequences. We show that NPMs\nimprove notably over both parametric and non-parametric state of the art in\nreconstruction and tracking of monocular depth sequences of clothed humans and\nhands. Latent-space interpolation as well as shape/pose transfer experiments\nfurther demonstrate the usefulness of NPMs. Code is publicly available at\nhttps://pablopalafox.github.io/npms.",
          "link": "http://arxiv.org/abs/2104.00702",
          "publishedOn": "2021-08-06T00:51:45.614Z",
          "wordCount": 676,
          "title": "NPMs: Neural Parametric Models for 3D Deformable Shapes. (arXiv:2104.00702v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yuhang Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xingrui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calway_A/0/1/0/all/0/1\">Andrew Calway</a>",
          "description": "We propose a novel object-augmented RGB-D SLAM system that is capable of\nconstructing a consistent object map and performing relocalisation based on\ncentroids of objects in the map. The approach aims to overcome the view\ndependence of appearance-based relocalisation methods using point features or\nimages. During the map construction, we use a pre-trained neural network to\ndetect objects and estimate 6D poses from RGB-D data. An incremental\nprobabilistic model is used to aggregate estimates over time to create the\nobject map. Then in relocalisation, we use the same network to extract\nobjects-of-interest in the `lost' frames. Pairwise geometric matching finds\ncorrespondences between map and frame objects, and probabilistic absolute\norientation followed by application of iterative closest point to dense depth\nmaps and object centroids gives relocalisation. Results of experiments in\ndesktop environments demonstrate very high success rates even for frames with\nwidely different viewpoints from those used to construct the map, significantly\noutperforming two appearance-based methods.",
          "link": "http://arxiv.org/abs/2108.02522",
          "publishedOn": "2021-08-06T00:51:45.607Z",
          "wordCount": 602,
          "title": "Object-Augmented RGB-D SLAM for Wide-Disparity Relocalisation. (arXiv:2108.02522v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haobo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jianjun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>",
          "description": "Point cloud registration is a fundamental problem in 3D computer vision. In\nthis paper, we cast point cloud registration into a planning problem in\nreinforcement learning, which can seek the transformation between the source\nand target point clouds through trial and error. By modeling the point cloud\nregistration process as a Markov decision process (MDP), we develop a latent\ndynamic model of point clouds, consisting of a transformation network and\nevaluation network. The transformation network aims to predict the new\ntransformed feature of the point cloud after performing a rigid transformation\n(i.e., action) on it while the evaluation network aims to predict the alignment\nprecision between the transformed source point cloud and target point cloud as\nthe reward signal. Once the dynamic model of the point cloud is trained, we\nemploy the cross-entropy method (CEM) to iteratively update the planning policy\nby maximizing the rewards in the point cloud registration process. Thus, the\noptimal policy, i.e., the transformation between the source and target point\nclouds, can be obtained via gradually narrowing the search space of the\ntransformation. Experimental results on ModelNet40 and 7Scene benchmark\ndatasets demonstrate that our method can yield good registration performance in\nan unsupervised manner.",
          "link": "http://arxiv.org/abs/2108.02613",
          "publishedOn": "2021-08-06T00:51:45.599Z",
          "wordCount": 646,
          "title": "Planning with Learned Dynamic Model for Unsupervised Point Cloud Registration. (arXiv:2108.02613v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_H/0/1/0/all/0/1\">Harrison Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Brian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fawaz_K/0/1/0/all/0/1\">Kassem Fawaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>",
          "description": "The proliferation of automated facial recognition in various commercial and\ngovernment sectors has caused significant privacy concerns for individuals. A\nrecent and popular approach to address these privacy concerns is to employ\nevasion attacks against the metric embedding networks powering facial\nrecognition systems. Face obfuscation systems generate imperceptible\nperturbations, when added to an image, cause the facial recognition system to\nmisidentify the user. The key to these approaches is the generation of\nperturbations using a pre-trained metric embedding network followed by their\napplication to an online system, whose model might be proprietary. This\ndependence of face obfuscation on metric embedding networks, which are known to\nbe unfair in the context of facial recognition, surfaces the question of\ndemographic fairness -- \\textit{are there demographic disparities in the\nperformance of face obfuscation systems?} To address this question, we perform\nan analytical and empirical exploration of the performance of recent face\nobfuscation systems that rely on deep embedding networks. We find that metric\nembedding networks are demographically aware; they cluster faces in the\nembedding space based on their demographic attributes. We observe that this\neffect carries through to the face obfuscation systems: faces belonging to\nminority groups incur reduced utility compared to those from majority groups.\nFor example, the disparity in average obfuscation success rate on the online\nFace++ API can reach up to 20 percentage points. Further, for some demographic\ngroups, the average perturbation size increases by up to 17\\% when choosing a\ntarget identity belonging to a different demographic group versus the same\ndemographic group. Finally, we present a simple analytical model to provide\ninsights into these phenomena.",
          "link": "http://arxiv.org/abs/2108.02707",
          "publishedOn": "2021-08-06T00:51:45.592Z",
          "wordCount": 706,
          "title": "Fairness Properties of Face Recognition and Obfuscation Systems. (arXiv:2108.02707v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.06028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_Z/0/1/0/all/0/1\">Ziqi Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhichao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naiyan Wang</a>",
          "description": "Estimating the states of surrounding traffic participants stays at the core\nof autonomous driving. In this paper, we study a novel setting of this problem:\nmodel-free single-object tracking (SOT), which takes the object state in the\nfirst frame as input, and jointly solves state estimation and tracking in\nsubsequent frames. The main purpose for this new setting is to break the strong\nlimitation of the popular \"detection and tracking\" scheme in multi-object\ntracking. Moreover, we notice that shape completion by overlaying the point\nclouds, which is a by-product of our proposed task, not only improves the\nperformance of state estimation but also has numerous applications. As no\nbenchmark for this task is available so far, we construct a new dataset\nLiDAR-SOT and corresponding evaluation protocols based on the Waymo Open\ndataset. We then propose an optimization-based algorithm called SOTracker\ninvolving point cloud registration, vehicle shapes, correspondence, and motion\npriors. Our quantitative and qualitative results prove the effectiveness of our\nSOTracker and reveal the challenging cases for SOT in point clouds, including\nthe sparsity of LiDAR data, abrupt motion variation, etc. Finally, we also\nexplore how the proposed task and algorithm may benefit other autonomous\ndriving applications, including simulating LiDAR scans, generating motion data,\nand annotating optical flow. The code and protocols for our benchmark and\nalgorithm are available at https://github.com/TuSimple/LiDAR_SOT/. A video\ndemonstration is at https://www.youtube.com/watch?v=BpHixKs91i8.",
          "link": "http://arxiv.org/abs/2103.06028",
          "publishedOn": "2021-08-06T00:51:45.576Z",
          "wordCount": 705,
          "title": "Model-free Vehicle Tracking and State Estimation in Point Cloud Sequences. (arXiv:2103.06028v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Han Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "In this paper, we propose a generalizable mixed-precision quantization (GMPQ)\nmethod for efficient inference. Conventional methods require the consistency of\ndatasets for bitwidth search and model deployment to guarantee the policy\noptimality, leading to heavy search cost on challenging largescale datasets in\nrealistic applications. On the contrary, our GMPQ searches the\nmixed-quantization policy that can be generalized to largescale datasets with\nonly a small amount of data, so that the search cost is significantly reduced\nwithout performance degradation. Specifically, we observe that locating network\nattribution correctly is general ability for accurate visual analysis across\ndifferent data distribution. Therefore, despite of pursuing higher model\naccuracy and complexity, we preserve attribution rank consistency between the\nquantized models and their full-precision counterparts via efficient\ncapacity-aware attribution imitation for generalizable mixed-precision\nquantization strategy search. Extensive experiments show that our method\nobtains competitive accuracy-complexity trade-off compared with the\nstate-of-the-art mixed-precision networks in significantly reduced search cost.\nThe code is available at https://github.com/ZiweiWangTHU/GMPQ.git.",
          "link": "http://arxiv.org/abs/2108.02720",
          "publishedOn": "2021-08-06T00:51:45.568Z",
          "wordCount": 598,
          "title": "Generalizable Mixed-Precision Quantization via Attribution Rank Preservation. (arXiv:2108.02720v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng-Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun-Yan Zhu</a>",
          "description": "Can a user create a deep generative model by sketching a single example?\nTraditionally, creating a GAN model has required the collection of a\nlarge-scale dataset of exemplars and specialized knowledge in deep learning. In\ncontrast, sketching is possibly the most universally accessible way to convey a\nvisual concept. In this work, we present a method, GAN Sketching, for rewriting\nGANs with one or more sketches, to make GANs training easier for novice users.\nIn particular, we change the weights of an original GAN model according to user\nsketches. We encourage the model's output to match the user sketches through a\ncross-domain adversarial loss. Furthermore, we explore different regularization\nmethods to preserve the original model's diversity and image quality.\nExperiments have shown that our method can mold GANs to match shapes and poses\nspecified by sketches while maintaining realism and diversity. Finally, we\ndemonstrate a few applications of the resulting GAN, including latent space\ninterpolation and image editing.",
          "link": "http://arxiv.org/abs/2108.02774",
          "publishedOn": "2021-08-06T00:51:45.561Z",
          "wordCount": 599,
          "title": "Sketch Your Own GAN. (arXiv:2108.02774v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1911.07158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fuxun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karianakis_N/0/1/0/all/0/1\">Nikolaos Karianakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Pei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lymberopoulos_D/0/1/0/all/0/1\">Dimitrios Lymberopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Sidi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weisong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>",
          "description": "Current state-of-the-art object detectors can have significant performance\ndrop when deployed in the wild due to domain gaps with training data.\nUnsupervised Domain Adaptation (UDA) is a promising approach to adapt models\nfor new domains/environments without any expensive label cost. However, without\nground truth labels, most prior works on UDA for object detection tasks can\nonly perform coarse image-level and/or feature-level adaptation by using\nadversarial learning methods. In this work, we show that such adversarial-based\nmethods can only reduce the domain style gap, but cannot address the domain\ncontent distribution gap that is shown to be important for object detectors. To\novercome this limitation, we propose the Cross-Domain Semi-Supervised Learning\n(CDSSL) framework by leveraging high-quality pseudo labels to learn better\nrepresentations from the target domain directly. To enable SSL for cross-domain\nobject detection, we propose fine-grained domain transfer,\nprogressive-confidence-based label sharpening and imbalanced sampling strategy\nto address two challenges: (i) non-identical distribution between source and\ntarget domain data, (ii) error amplification/accumulation due to noisy pseudo\nlabeling on the target domain. Experiment results show that our proposed\napproach consistently achieves new state-of-the-art performance (2.2% - 9.5%\nbetter than prior best work on mAP) under various domain gap scenarios. The\ncode will be released.",
          "link": "http://arxiv.org/abs/1911.07158",
          "publishedOn": "2021-08-06T00:51:45.555Z",
          "wordCount": 715,
          "title": "Unsupervised Domain Adaptation for Object Detection via Cross-Domain Semi-Supervised Learning. (arXiv:1911.07158v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khorrami_K/0/1/0/all/0/1\">Khazar Khorrami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>",
          "description": "Systems that can find correspondences between multiple modalities, such as\nbetween speech and images, have great potential to solve different recognition\nand data analysis tasks in an unsupervised manner. This work studies multimodal\nlearning in the context of visually grounded speech (VGS) models, and focuses\non their recently demonstrated capability to extract spatiotemporal alignments\nbetween spoken words and the corresponding visual objects without ever been\nexplicitly trained for object localization or word recognition. As the main\ncontributions, we formalize the alignment problem in terms of an audiovisual\nalignment tensor that is based on earlier VGS work, introduce systematic\nmetrics for evaluating model performance in aligning visual objects and spoken\nwords, and propose a new VGS model variant for the alignment task utilizing\ncross-modal attention layer. We test our model and a previously proposed model\nin the alignment task using SPEECH-COCO captions coupled with MSCOCO images. We\ncompare the alignment performance using our proposed evaluation metrics to the\nsemantic retrieval task commonly used to evaluate VGS models. We show that\ncross-modal attention layer not only helps the model to achieve higher semantic\ncross-modal retrieval performance, but also leads to substantial improvements\nin the alignment performance between image object and spoken words.",
          "link": "http://arxiv.org/abs/2108.02562",
          "publishedOn": "2021-08-06T00:51:45.531Z",
          "wordCount": 652,
          "title": "Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models. (arXiv:2108.02562v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aydin_A/0/1/0/all/0/1\">Ayberk Aydin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_D/0/1/0/all/0/1\">Deniz Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karli_B/0/1/0/all/0/1\">Berat Tuna Karli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanoglu_O/0/1/0/all/0/1\">Oguz Hanoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Temizel_A/0/1/0/all/0/1\">Alptekin Temizel</a>",
          "description": "Deep Neural Networks have been shown to be vulnerable to various kinds of\nadversarial perturbations. In addition to widely studied additive noise based\nperturbations, adversarial examples can also be created by applying a per pixel\nspatial drift on input images. While spatial transformation based adversarial\nexamples look more natural to human observers due to absence of additive noise,\nthey still possess visible distortions caused by spatial transformations. Since\nthe human vision is more sensitive to the distortions in the luminance compared\nto those in chrominance channels, which is one of the main ideas behind the\nlossy visual multimedia compression standards, we propose a spatial\ntransformation based perturbation method to create adversarial examples by only\nmodifying the color components of an input image. While having competitive\nfooling rates on CIFAR-10 and NIPS2017 Adversarial Learning Challenge datasets,\nexamples created with the proposed method have better scores with regards to\nvarious perceptual quality metrics. Human visual perception studies validate\nthat the examples are more natural looking and often indistinguishable from\ntheir original counterparts.",
          "link": "http://arxiv.org/abs/2108.02502",
          "publishedOn": "2021-08-06T00:51:45.519Z",
          "wordCount": 625,
          "title": "Imperceptible Adversarial Examples by Spatial Chroma-Shift. (arXiv:2108.02502v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xinxin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Ji Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhenbo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Minglun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>",
          "description": "Given a picture of a chair, could we extract the 3-D shape of the chair,\nanimate its plausible articulations and motions, and render in-situ in its\noriginal image space? The above question prompts us to devise an automated\napproach to extract and manipulate articulated objects in single images.\nComparing with previous efforts on object manipulation, our work goes beyond\n2-D manipulation and focuses on articulable objects, thus introduces greater\nflexibility for possible object deformations. The pipeline of our approach\nstarts by reconstructing and refining a 3-D mesh representation of the object\nof interest from an input image; its control joints are predicted by exploiting\nthe semantic part segmentation information; the obtained object 3-D mesh is\nthen rigged \\& animated by non-rigid deformation, and rendered to perform\nin-situ motions in its original image space. Quantitative evaluations are\ncarried out on 3-D reconstruction from single images, an established task that\nis related to our pipeline, where our results surpass those of the SOTAs by a\nnoticeable margin. Extensive visual results also demonstrate the applicability\nof our approach.",
          "link": "http://arxiv.org/abs/2108.02708",
          "publishedOn": "2021-08-06T00:51:45.501Z",
          "wordCount": 629,
          "title": "Object Wake-up: 3-D Object Reconstruction, Animation, and in-situ Rendering from a Single Image. (arXiv:2108.02708v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02607",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Susan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilin Chen</a>",
          "description": "We introduce a new efficient framework, the Unified Context Network (UniCon),\nfor robust active speaker detection (ASD). Traditional methods for ASD usually\noperate on each candidate's pre-cropped face track separately and do not\nsufficiently consider the relationships among the candidates. This potentially\nlimits performance, especially in challenging scenarios with low-resolution\nfaces, multiple candidates, etc. Our solution is a novel, unified framework\nthat focuses on jointly modeling multiple types of contextual information:\nspatial context to indicate the position and scale of each candidate's face,\nrelational context to capture the visual relationships among the candidates and\ncontrast audio-visual affinities with each other, and temporal context to\naggregate long-term information and smooth out local uncertainties. Based on\nsuch information, our model optimizes all candidates in a unified process for\nrobust and reliable ASD. A thorough ablation study is performed on several\nchallenging ASD benchmarks under different settings. In particular, our method\noutperforms the state-of-the-art by a large margin of about 15% mean Average\nPrecision (mAP) absolute on two challenging subsets: one with three candidate\nspeakers, and the other with faces smaller than 64 pixels. Together, our UniCon\nachieves 92.0% mAP on the AVA-ActiveSpeaker validation set, surpassing 90% for\nthe first time on this challenging dataset at the time of submission. Project\nwebsite: https://unicon-asd.github.io/.",
          "link": "http://arxiv.org/abs/2108.02607",
          "publishedOn": "2021-08-06T00:51:45.493Z",
          "wordCount": 688,
          "title": "UniCon: Unified Context Network for Robust Active Speaker Detection. (arXiv:2108.02607v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiemin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>",
          "description": "Instance segmentation on point clouds is a fundamental task in 3D scene\nperception. In this work, we propose a concise clustering-based framework named\nHAIS, which makes full use of spatial relation of points and point sets.\nConsidering clustering-based methods may result in over-segmentation or\nunder-segmentation, we introduce the hierarchical aggregation to progressively\ngenerate instance proposals, i.e., point aggregation for preliminarily\nclustering points to sets and set aggregation for generating complete instances\nfrom sets. Once the complete 3D instances are obtained, a sub-network of\nintra-instance prediction is adopted for noisy points filtering and mask\nquality scoring. HAIS is fast (only 410ms per frame) and does not require\nnon-maximum suppression. It ranks 1st on the ScanNet v2 benchmark, achieving\nthe highest 69.9% AP50 and surpassing previous state-of-the-art (SOTA) methods\nby a large margin. Besides, the SOTA results on the S3DIS dataset validate the\ngood generalization ability. Code will be available at\nhttps://github.com/hustvl/HAIS.",
          "link": "http://arxiv.org/abs/2108.02350",
          "publishedOn": "2021-08-06T00:51:45.483Z",
          "wordCount": 591,
          "title": "Hierarchical Aggregation for 3D Instance Segmentation. (arXiv:2108.02350v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuxin Ding</a>",
          "description": "Single online handwritten Chinese character recognition~(single OLHCCR) has\nachieved prominent performance. However, in real application scenarios, users\nalways write multiple Chinese characters to form one complete sentence and the\ncontextual information within these characters holds the significant potential\nto improve the accuracy, robustness and efficiency of sentence-level OLHCCR. In\nthis work, we first propose a simple and straightforward end-to-end network,\nnamely vanilla compositional network~(VCN) to tackle the sentence-level OLHCCR.\nIt couples convolutional neural network with sequence modeling architecture to\nexploit the handwritten character's previous contextual information. Although\nVCN performs much better than the state-of-the-art single OLHCCR model, it\nexposes high fragility when confronting with not well written characters such\nas sloppy writing, missing or broken strokes. To improve the robustness of\nsentence-level OLHCCR, we further propose a novel deep spatial-temporal fusion\nnetwork~(DSTFN). It utilizes a pre-trained autoregresssive framework as the\nbackbone component, which projects each Chinese character into word embeddings,\nand integrates the spatial glyph features of handwritten characters and their\ncontextual information multiple times at multi-layer fusion module. We also\nconstruct a large-scale sentence-level handwriting dataset, named as CSOHD to\nevaluate models. Extensive experiment results demonstrate that DSTFN achieves\nthe state-of-the-art performance, which presents strong robustness compared\nwith VCN and exiting single OLHCCR models. The in-depth empirical analysis and\ncase studies indicate that DSTFN can significantly improve the efficiency of\nhandwriting input, with the handwritten Chinese character with incomplete\nstrokes being recognized precisely.",
          "link": "http://arxiv.org/abs/2108.02561",
          "publishedOn": "2021-08-06T00:51:45.466Z",
          "wordCount": 682,
          "title": "Sentence-level Online Handwritten Chinese Character Recognition. (arXiv:2108.02561v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02476",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1\">Hui Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Weizhen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Li Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "Differentiation of colorectal polyps is an important clinical examination. A\ncomputer-aided diagnosis system is required to assist accurate diagnosis from\ncolonoscopy images. Most previous studies at-tempt to develop models for polyp\ndifferentiation using Narrow-Band Imaging (NBI) or other enhanced images.\nHowever, the wide range of these models' applications for clinical work has\nbeen limited by the lagging of imaging techniques. Thus, we propose a novel\nframework based on a teacher-student architecture for the accurate colorectal\npolyp classification (CPC) through directly using white-light (WL) colonoscopy\nimages in the examination. In practice, during training, the auxiliary NBI\nimages are utilized to train a teacher network and guide the student network to\nacquire richer feature representation from WL images. The feature transfer is\nrealized by domain alignment and contrastive learning. Eventually the final\nstudent network has the ability to extract aligned features from only WL images\nto facilitate the CPC task. Besides, we release the first public-available\npaired CPC dataset containing WL-NBI pairs for the alignment training.\nQuantitative and qualitative evaluation indicates that the proposed method\noutperforms the previous methods in CPC, improving the accuracy by 5.6%with\nvery fast speed.",
          "link": "http://arxiv.org/abs/2108.02476",
          "publishedOn": "2021-08-06T00:51:45.458Z",
          "wordCount": 636,
          "title": "Colorectal Polyp Classification from White-light Colonoscopy Images via Domain Alignment. (arXiv:2108.02476v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02557",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Barina_D/0/1/0/all/0/1\">David Barina</a>",
          "description": "In recent years, a bag with image and video compression formats has been\ntorn. However, most of them are focused on lossy compression and only\nmarginally support the lossless mode. In this paper, I will focus on lossless\nformats and the critical question: \"Which one is the most efficient?\" It turned\nout that FLIF is currently the most efficient format for lossless image\ncompression. This finding is in contrast to that FLIF developers stopped its\ndevelopment in favor of JPEG XL.",
          "link": "http://arxiv.org/abs/2108.02557",
          "publishedOn": "2021-08-06T00:51:45.451Z",
          "wordCount": 520,
          "title": "Comparison of Lossless Image Formats. (arXiv:2108.02557v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiange Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fangfang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsui_K/0/1/0/all/0/1\">Kwok-Leung Tsui</a>",
          "description": "Automatic detection of rail track and its fasteners via using continuously\ncollected railway images is important to maintenance as it can significantly\nimprove maintenance efficiency and better ensure system safety. Dominant\ncomputer vision-based detection models typically rely on convolutional neural\nnetworks that utilize local image features and cumbersome prior settings to\ngenerate candidate boxes. In this paper, we propose a deep convolutional\ntransformer network based method to detect multi-class rail components\nincluding the rail, clip, and bolt. We effectively synergize advantages of the\nconvolutional structure on extracting latent features from raw images as well\nas advantages of transformers on selectively determining valuable latent\nfeatures to achieve an efficient and accurate performance on rail component\ndetections. Our proposed method simplifies the detection pipeline by\neliminating the need of prior settings, such as anchor box, aspect ratio,\ndefault coordinates, and post-processing, such as the threshold for non-maximum\nsuppression; as well as allows users to trade off the quality and complexity of\nthe detector with limited training data. Results of a comprehensive\ncomputational study show that our proposed method outperforms a set of existing\nstate-of-art approaches with large margins",
          "link": "http://arxiv.org/abs/2108.02423",
          "publishedOn": "2021-08-06T00:51:45.444Z",
          "wordCount": 626,
          "title": "Automatic Detection of Rail Components via A Deep Convolutional Transformer Network. (arXiv:2108.02423v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delbruck_T/0/1/0/all/0/1\">Tobi Delbruck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shih-Chii Liu</a>",
          "description": "Long Short-Term Memory (LSTM) recurrent networks are frequently used for\ntasks involving time sequential data such as speech recognition. However, it is\ndifficult to deploy these networks on hardware to achieve high throughput and\nlow latency because the fully-connected structure makes LSTM networks a\nmemory-bounded algorithm. Previous work in LSTM accelerators either exploited\nweight spatial sparsity or temporal sparsity. In this paper, we present a new\naccelerator called \"Spartus\" that exploits spatio-temporal sparsity to achieve\nultra-low latency inference. The spatial sparsity was induced using our\nproposed pruning method called Column-Balanced Targeted Dropout (CBTD) that\nleads to structured sparse weight matrices benefiting workload balance. It\nachieved up to 96% weight sparsity with negligible accuracy difference for an\nLSTM network trained on a TIMIT phone recognition task. To induce temporal\nsparsity in LSTM, we create the DeltaLSTM by extending the previous DeltaGRU\nmethod to the LSTM network. This combined sparsity saves on weight memory\naccess and associated arithmetic operations simultaneously. Spartus was\nimplemented on a Xilinx Zynq-7100 FPGA. The per-sample latency for a single\nDeltaLSTM layer of 1024 neurons running on Spartus is 1 us. Spartus achieved\n9.4 TOp/s effective batch-1 throughput and 1.1 TOp/J energy efficiency, which\nare respectively 4X and 7X higher than the previous state-of-the-art.",
          "link": "http://arxiv.org/abs/2108.02297",
          "publishedOn": "2021-08-06T00:51:45.436Z",
          "wordCount": 653,
          "title": "Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-temporal Sparsity. (arXiv:2108.02297v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "The advancements in deep learning technologies have produced immense\ncontribution to biomedical image analysis applications. With breast cancer\nbeing the common deadliest disease among women, early detection is the key\nmeans to improve survivability. Medical imaging like ultrasound presents an\nexcellent visual representation of the functioning of the organs; however, for\nany radiologist analysing such scans is challenging and time consuming which\ndelays the diagnosis process. Although various deep learning based approaches\nare proposed that achieved promising results, the present article introduces an\nefficient residual cross-spatial attention guided inception U-Net (RCA-IUnet)\nmodel with minimal training parameters for tumor segmentation using breast\nultrasound imaging to further improve the segmentation performance of varying\ntumor sizes. The RCA-IUnet model follows U-Net topology with residual inception\ndepth-wise separable convolution and hybrid pooling (max pooling and spectral\npooling) layers. In addition, cross-spatial attention filters are added to\nsuppress the irrelevant features and focus on the target structure. The\nsegmentation performance of the proposed model is validated on two publicly\navailable datasets using standard segmentation evaluation metrics, where it\noutperformed the other state-of-the-art segmentation models.",
          "link": "http://arxiv.org/abs/2108.02508",
          "publishedOn": "2021-08-06T00:51:45.429Z",
          "wordCount": 628,
          "title": "RCA-IUnet: A residual cross-spatial attention guided inception U-Net model for tumor segmentation in breast ultrasound imaging. (arXiv:2108.02508v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02560",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1\">Zhenyu Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuesheng Zhu</a>",
          "description": "Online hashing methods usually learn the hash functions online, aiming to\nefficiently adapt to the data variations in the streaming environment. However,\nwhen the hash functions are updated, the binary codes for the whole database\nhave to be updated to be consistent with the hash functions, resulting in the\ninefficiency in the online image retrieval process. In this paper, we propose a\nnovel online hashing framework without updating binary codes. In the proposed\nframework, the hash functions are fixed and a parametric similarity function\nfor the binary codes is learnt online to adapt to the streaming data.\nSpecifically, a parametric similarity function that has a bilinear form is\nadopted and a metric learning algorithm is proposed to learn the similarity\nfunction online based on the characteristics of the hashing methods. The\nexperiments on two multi-label image datasets show that our method is\ncompetitive or outperforms the state-of-the-art online hashing methods in terms\nof both accuracy and efficiency for multi-label image retrieval.",
          "link": "http://arxiv.org/abs/2108.02560",
          "publishedOn": "2021-08-06T00:51:45.421Z",
          "wordCount": 590,
          "title": "Online Hashing with Similarity Learning. (arXiv:2108.02560v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Han Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zehua Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>",
          "description": "The intellectual property (IP) of Deep neural networks (DNNs) can be easily\n``stolen'' by surrogate model attack. There has been significant progress in\nsolutions to protect the IP of DNN models in classification tasks. However,\nlittle attention has been devoted to the protection of DNNs in image processing\ntasks. By utilizing consistent invisible spatial watermarks, one recent work\nfirst considered model watermarking for deep image processing networks and\ndemonstrated its efficacy in many downstream tasks. Nevertheless, it highly\ndepends on the hypothesis that the embedded watermarks in the network outputs\nare consistent. When the attacker uses some common data augmentation attacks\n(e.g., rotate, crop, and resize) during surrogate model training, it will\ntotally fail because the underlying watermark consistency is destroyed. To\nmitigate this issue, we propose a new watermarking methodology, namely\n``structure consistency'', based on which a new deep structure-aligned model\nwatermarking algorithm is designed. Specifically, the embedded watermarks are\ndesigned to be aligned with physically consistent image structures, such as\nedges or semantic regions. Experiments demonstrate that our method is much more\nrobust than the baseline method in resisting data augmentation attacks for\nmodel IP protection. Besides that, we further test the generalization ability\nand robustness of our method to a broader range of circumvention attacks.",
          "link": "http://arxiv.org/abs/2108.02360",
          "publishedOn": "2021-08-06T00:51:45.414Z",
          "wordCount": 653,
          "title": "Exploring Structure Consistency for Deep Model Watermarking. (arXiv:2108.02360v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+zhang_J/0/1/0/all/0/1\">Jie zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qidong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>",
          "description": "Recent research shows deep neural networks are vulnerable to different types\nof attacks, such as adversarial attack, data poisoning attack and backdoor\nattack. Among them, backdoor attack is the most cunning one and can occur in\nalmost every stage of deep learning pipeline. Therefore, backdoor attack has\nattracted lots of interests from both academia and industry. However, most\nexisting backdoor attack methods are either visible or fragile to some\neffortless pre-processing such as common data transformations. To address these\nlimitations, we propose a robust and invisible backdoor attack called ``Poison\nInk''. Concretely, we first leverage the image structures as target poisoning\nareas, and fill them with poison ink (information) to generate the trigger\npattern. As the image structure can keep its semantic meaning during the data\ntransformation, such trigger pattern is inherently robust to data\ntransformations. Then we leverage a deep injection network to embed such\ntrigger pattern into the cover image to achieve stealthiness. Compared to\nexisting popular backdoor attack methods, Poison Ink outperforms both in\nstealthiness and robustness. Through extensive experiments, we demonstrate\nPoison Ink is not only general to different datasets and network architectures,\nbut also flexible for different attack scenarios. Besides, it also has very\nstrong resistance against many state-of-the-art defense techniques.",
          "link": "http://arxiv.org/abs/2108.02488",
          "publishedOn": "2021-08-06T00:51:45.389Z",
          "wordCount": 649,
          "title": "Poison Ink: Robust and Invisible Backdoor Attack. (arXiv:2108.02488v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>",
          "description": "Video captioning combines video understanding and language generation.\nDifferent from image captioning that describes a static image with details of\nalmost every object, video captioning usually considers a sequence of frames\nand biases towards focused objects, e.g., the objects that stay in focus\nregardless of the changing background. Therefore, detecting and properly\naccommodating focused objects is critical in video captioning. To enforce the\ndescription of focused objects and achieve controllable video captioning, we\npropose an Object-Oriented Non-Autoregressive approach (O2NA), which performs\ncaption generation in three steps: 1) identify the focused objects and predict\ntheir locations in the target caption; 2) generate the related attribute words\nand relation words of these focused objects to form a draft caption; and 3)\ncombine video information to refine the draft caption to a fluent final\ncaption. Since the focused objects are generated and located ahead of other\nwords, it is difficult to apply the word-by-word autoregressive generation\nprocess; instead, we adopt a non-autoregressive approach. The experiments on\ntwo benchmark datasets, i.e., MSR-VTT and MSVD, demonstrate the effectiveness\nof O2NA, which achieves results competitive with the state-of-the-arts but with\nboth higher diversity and higher inference speed.",
          "link": "http://arxiv.org/abs/2108.02359",
          "publishedOn": "2021-08-06T00:51:45.370Z",
          "wordCount": 642,
          "title": "O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable Video Captioning. (arXiv:2108.02359v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02574",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_F/0/1/0/all/0/1\">Fei Wen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1\">Zeyu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ying_R/0/1/0/all/0/1\">Rendong Ying</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peilin Liu</a>",
          "description": "Recently, much progress has been made in unsupervised restoration learning.\nHowever, existing methods more or less rely on some assumptions on the signal\nand/or degradation model, which limits their practical performance. How to\nconstruct an optimal criterion for unsupervised restoration learning without\nany prior knowledge on the degradation model is still an open question. Toward\nanswering this question, this work proposes a criterion for unsupervised\nrestoration learning based on the optimal transport theory. This criterion has\nfavorable properties, e.g., approximately maximal preservation of the\ninformation of the signal, whilst achieving perceptual reconstruction.\nFurthermore, though a relaxed unconstrained formulation is used in practical\nimplementation, we show that the relaxed formulation in theory has the same\nsolution as the original constrained formulation. Experiments on synthetic and\nreal-world data, including realistic photographic, microscopy, depth, and raw\ndepth images, demonstrate that the proposed method even compares favorably with\nsupervised methods, e.g., approaching the PSNR of supervised methods while\nhaving better perceptual quality. Particularly, for spatially correlated noise\nand realistic microscopy images, the proposed method not only achieves better\nperceptual quality but also has higher PSNR than supervised methods. Besides,\nit shows remarkable superiority in harsh practical conditions with complex\nnoise, e.g., raw depth images.",
          "link": "http://arxiv.org/abs/2108.02574",
          "publishedOn": "2021-08-06T00:51:45.355Z",
          "wordCount": 652,
          "title": "Optimal Transport for Unsupervised Restoration Learning. (arXiv:2108.02574v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_K/0/1/0/all/0/1\">Keck-Voon Ling</a>",
          "description": "In this work, we aim to address the challenging task of open set recognition\n(OSR). Many recent OSR methods rely on auto-encoders to extract class-specific\nfeatures by a reconstruction strategy, requiring the network to restore the\ninput image on pixel-level. This strategy is commonly over-demanding for OSR\nsince class-specific features are generally contained in target objects, not in\nall pixels. To address this shortcoming, here we discard the pixel-level\nreconstruction strategy and pay more attention to improving the effectiveness\nof class-specific feature extraction. We propose a mutual information-based\nmethod with a streamlined architecture, Maximal Mutual Information Open Set\nRecognition (M2IOSR). The proposed M2IOSR only uses an encoder to extract\nclass-specific features by maximizing the mutual information between the given\ninput and its latent features across multiple scales. Meanwhile, to further\nreduce the open space risk, latent features are constrained to class\nconditional Gaussian distributions by a KL-divergence loss function. In this\nway, a strong function is learned to prevent the network from mapping different\nobservations to similar latent features and help the network extract\nclass-specific features with desired statistical characteristics. The proposed\nmethod significantly improves the performance of baselines and achieves new\nstate-of-the-art results on several benchmarks consistently. Source codes are\nuploaded in supplementary materials.",
          "link": "http://arxiv.org/abs/2108.02373",
          "publishedOn": "2021-08-06T00:51:45.348Z",
          "wordCount": 641,
          "title": "M2IOSR: Maximal Mutual Information Open Set Recognition. (arXiv:2108.02373v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02559",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shixiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoman Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>",
          "description": "Annotating multiple organs in 3D medical images is time-consuming and costly.\nMeanwhile, there exist many single-organ datasets with one specific organ\nannotated. This paper investigates how to learn a multi-organ segmentation\nmodel leveraging a set of binary-labeled datasets. A novel Multi-teacher\nSingle-student Knowledge Distillation (MS-KD) framework is proposed, where the\nteacher models are pre-trained single-organ segmentation networks, and the\nstudent model is a multi-organ segmentation network. Considering that each\nteacher focuses on different organs, a region-based supervision method,\nconsisting of logits-wise supervision and feature-wise supervision, is\nproposed. Each teacher supervises the student in two regions, the organ region\nwhere the teacher is considered as an expert and the background region where\nall teachers agree. Extensive experiments on three public single-organ datasets\nand a multi-organ dataset have demonstrated the effectiveness of the proposed\nMS-KD framework.",
          "link": "http://arxiv.org/abs/2108.02559",
          "publishedOn": "2021-08-06T00:51:45.325Z",
          "wordCount": 569,
          "title": "MS-KD: Multi-Organ Segmentation with Multiple Binary-Labeled Datasets. (arXiv:2108.02559v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kutnar_D/0/1/0/all/0/1\">Denis Kutnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velden_B/0/1/0/all/0/1\">Bas H.M. van der Velden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanguesa_M/0/1/0/all/0/1\">Marta Girones Sanguesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geerlings_M/0/1/0/all/0/1\">Mirjam I. Geerlings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biesbroek_J/0/1/0/all/0/1\">J. Matthijs Biesbroek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijf_H/0/1/0/all/0/1\">Hugo J. Kuijf</a>",
          "description": "Lacunes of presumed vascular origin are fluid-filled cavities of between 3 -\n15 mm in diameter, visible on T1 and FLAIR brain MRI. Quantification of lacunes\nrelies on manual annotation or semi-automatic / interactive approaches; and\nalmost no automatic methods exist for this task. In this work, we present a\ntwo-stage approach to segment lacunes of presumed vascular origin: (1)\ndetection with Mask R-CNN followed by (2) segmentation with a U-Net CNN. Data\noriginates from Task 3 of the \"Where is VALDO?\" challenge and consists of 40\ntraining subjects. We report the mean DICE on the training set of 0.83 and on\nthe validation set of 0.84. Source code is available at:\nhttps://github.com/hjkuijf/MixLacune . The docker container hjkuijf/mixlacune\ncan be pulled from https://hub.docker.com/r/hjkuijf/mixlacune .",
          "link": "http://arxiv.org/abs/2108.02483",
          "publishedOn": "2021-08-06T00:51:45.318Z",
          "wordCount": 584,
          "title": "MixLacune: Segmentation of lacunes of presumed vascular origin. (arXiv:2108.02483v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Duo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yanye Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xuejing Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>",
          "description": "The nonlocal-based blocks are designed for capturing long-range\nspatial-temporal dependencies in computer vision tasks. Although having shown\nexcellent performance, they still lack the mechanism to encode the rich,\nstructured information among elements in an image or video. In this paper, to\ntheoretically analyze the property of these nonlocal-based blocks, we provide a\nnew perspective to interpret them, where we view them as a set of graph filters\ngenerated on a fully-connected graph. Specifically, when choosing the Chebyshev\ngraph filter, a unified formulation can be derived for explaining and analyzing\nthe existing nonlocal-based blocks (e.g., nonlocal block, nonlocal stage,\ndouble attention block). Furthermore, by concerning the property of spectral,\nwe propose an efficient and robust spectral nonlocal block, which can be more\nrobust and flexible to catch long-range dependencies when inserted into deep\nneural networks than the existing nonlocal blocks. Experimental results\ndemonstrate the clear-cut improvements and practical applicabilities of our\nmethod on image classification, action recognition, semantic segmentation, and\nperson re-identification tasks.",
          "link": "http://arxiv.org/abs/2108.02451",
          "publishedOn": "2021-08-06T00:51:45.311Z",
          "wordCount": 610,
          "title": "Unifying Nonlocal Blocks for Neural Networks. (arXiv:2108.02451v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zeren Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yazhou Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiu-Shen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongshun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1\">Fumin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng-Tao Shen</a>",
          "description": "Learning from the web can ease the extreme dependence of deep learning on\nlarge-scale manually labeled datasets. Especially for fine-grained recognition,\nwhich targets at distinguishing subordinate categories, it will significantly\nreduce the labeling costs by leveraging free web data. Despite its significant\npractical and research value, the webly supervised fine-grained recognition\nproblem is not extensively studied in the computer vision community, largely\ndue to the lack of high-quality datasets. To fill this gap, in this paper we\nconstruct two new benchmark webly supervised fine-grained datasets, termed\nWebFG-496 and WebiNat-5089, respectively. In concretely, WebFG-496 consists of\nthree sub-datasets containing a total of 53,339 web training images with 200\nspecies of birds (Web-bird), 100 types of aircrafts (Web-aircraft), and 196\nmodels of cars (Web-car). For WebiNat-5089, it contains 5089 sub-categories and\nmore than 1.1 million web training images, which is the largest webly\nsupervised fine-grained dataset ever. As a minor contribution, we also propose\na novel webly supervised method (termed ``{Peer-learning}'') for benchmarking\nthese datasets.~Comprehensive experimental results and analyses on two new\nbenchmark datasets demonstrate that the proposed method achieves superior\nperformance over the competing baseline models and states-of-the-art. Our\nbenchmark datasets and the source codes of Peer-learning have been made\navailable at\n{\\url{https://github.com/NUST-Machine-Intelligence-Laboratory/weblyFG-dataset}}.",
          "link": "http://arxiv.org/abs/2108.02399",
          "publishedOn": "2021-08-06T00:51:45.264Z",
          "wordCount": 655,
          "title": "Webly Supervised Fine-Grained Recognition: Benchmark Datasets and An Approach. (arXiv:2108.02399v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weihao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Michael Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "We design a multiscopic vision system that utilizes a low-cost monocular RGB\ncamera to acquire accurate depth estimation. Unlike multi-view stereo with\nimages captured at unconstrained camera poses, the proposed system controls the\nmotion of a camera to capture a sequence of images in horizontally or\nvertically aligned positions with the same parallax. In this system, we propose\na new heuristic method and a robust learning-based method to fuse multiple cost\nvolumes between the reference image and its surrounding images. To obtain\ntraining data, we build a synthetic dataset with multiscopic images. The\nexperiments on the real-world Middlebury dataset and real robot demonstration\nshow that our multiscopic vision system outperforms traditional two-frame\nstereo matching methods in depth estimation. Our code and dataset are available\nat \\url{https://sites.google.com/view/multiscopic",
          "link": "http://arxiv.org/abs/2108.02448",
          "publishedOn": "2021-08-06T00:51:45.258Z",
          "wordCount": 589,
          "title": "MFuseNet: Robust Depth Estimation with Learned Multiscopic Fusion. (arXiv:2108.02448v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yongxing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zekun Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Ling-Yu Duan</a>",
          "description": "Unsupervised domain adaptive person re-identification (UDA re-ID) aims at\ntransferring the labeled source domain's knowledge to improve the model's\ndiscriminability on the unlabeled target domain. From a novel perspective, we\nargue that the bridging between the source and target domains can be utilized\nto tackle the UDA re-ID task, and we focus on explicitly modeling appropriate\nintermediate domains to characterize this bridging. Specifically, we propose an\nIntermediate Domain Module (IDM) to generate intermediate domains'\nrepresentations on-the-fly by mixing the source and target domains' hidden\nrepresentations using two domain factors. Based on the \"shortest geodesic path\"\ndefinition, i.e., the intermediate domains along the shortest geodesic path\nbetween the two extreme domains can play a better bridging role, we propose two\nproperties that these intermediate domains should satisfy. To ensure these two\nproperties to better characterize appropriate intermediate domains, we enforce\nthe bridge losses on intermediate domains' prediction space and feature space,\nand enforce a diversity loss on the two domain factors. The bridge losses aim\nat guiding the distribution of appropriate intermediate domains to keep the\nright distance to the source and target domains. The diversity loss serves as a\nregularization to prevent the generated intermediate domains from being\nover-fitting to either of the source and target domains. Our proposed method\noutperforms the state-of-the-arts by a large margin in all the common UDA re-ID\ntasks, and the mAP gain is up to 7.7% on the challenging MSMT17 benchmark. Code\nis available at https://github.com/SikaStar/IDM.",
          "link": "http://arxiv.org/abs/2108.02413",
          "publishedOn": "2021-08-06T00:51:45.235Z",
          "wordCount": 694,
          "title": "IDM: An Intermediate Domain Module for Domain Adaptive Person Re-ID. (arXiv:2108.02413v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunfei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>",
          "description": "Autonomous assembly has been a desired functionality of many intelligent\nrobot systems. We study a new challenging assembly task, designing and\nconstructing a bridge without a blueprint. In this task, the robot needs to\nfirst design a feasible bridge architecture for arbitrarily wide cliffs and\nthen manipulate the blocks reliably to construct a stable bridge according to\nthe proposed design. In this paper, we propose a bi-level approach to tackle\nthis task. At the high level, the system learns a bridge blueprint policy in a\nphysical simulator using deep reinforcement learning and curriculum learning. A\npolicy is represented as an attention-based neural network with object-centric\ninput, which enables generalization to different numbers of blocks and cliff\nwidths. For low-level control, we implement a motion-planning-based policy for\nreal-robot motion control, which can be directly combined with a trained\nblueprint policy for real-world bridge construction without tuning. In our\nfield study, our bi-level robot system demonstrates the capability of\nmanipulating blocks to construct a diverse set of bridges with different\narchitectures.",
          "link": "http://arxiv.org/abs/2108.02439",
          "publishedOn": "2021-08-06T00:51:45.210Z",
          "wordCount": 615,
          "title": "Learning to Design and Construct Bridge without Blueprint. (arXiv:2108.02439v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02425",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1\">Ruihang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Grasping in cluttered scenes has always been a great challenge for robots,\ndue to the requirement of the ability to well understand the scene and object\ninformation. Previous works usually assume that the geometry information of the\nobjects is available, or utilize a step-wise, multi-stage strategy to predict\nthe feasible 6-DoF grasp poses. In this work, we propose to formalize the 6-DoF\ngrasp pose estimation as a simultaneous multi-task learning problem. In a\nunified framework, we jointly predict the feasible 6-DoF grasp poses, instance\nsemantic segmentation, and collision information. The whole framework is\njointly optimized and end-to-end differentiable. Our model is evaluated on\nlarge-scale benchmarks as well as the real robot system. On the public dataset,\nour method outperforms prior state-of-the-art methods by a large margin (+4.08\nAP). We also demonstrate the implementation of our model on a real robotic\nplatform and show that the robot can accurately grasp target objects in\ncluttered scenarios with a high success rate. Project link:\nhttps://openbyterobotics.github.io/sscl",
          "link": "http://arxiv.org/abs/2108.02425",
          "publishedOn": "2021-08-06T00:51:45.203Z",
          "wordCount": 616,
          "title": "Simultaneous Semantic and Collision Learning for 6-DoF Grasp Pose Estimation. (arXiv:2108.02425v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Guang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhiping Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1\">En Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianping Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qing Liao</a>",
          "description": "Video abnormal event detection (VAD) is a vital semi-supervised task that\nrequires learning with only roughly labeled normal videos, as anomalies are\noften practically unavailable. Although deep neural networks (DNNs) enable\ngreat progress in VAD, existing solutions typically suffer from two issues: (1)\nThe precise and comprehensive localization of video events is ignored. (2) The\nvideo semantics and temporal context are under-explored. To address those\nissues, we are motivated by the prevalent cloze test in education and propose a\nnovel approach named visual cloze completion (VCC), which performs VAD by\nlearning to complete \"visual cloze tests\" (VCTs). Specifically, VCC first\nlocalizes each video event and encloses it into a spatio-temporal cube (STC).\nTo achieve both precise and comprehensive localization, appearance and motion\nare used as mutually complementary cues to mark the object region associated\nwith each video event. For each marked region, a normalized patch sequence is\nextracted from temporally adjacent frames and stacked into the STC. By\ncomparing each patch and the patch sequence of a STC to a visual \"word\" and\n\"sentence\" respectively, we can deliberately erase a certain \"word\" (patch) to\nyield a VCT. DNNs are then trained to infer the erased patch by video\nsemantics, so as to complete the VCT. To fully exploit the temporal context,\neach patch in STC is alternatively erased to create multiple VCTs, and the\nerased patch's optical flow is also inferred to integrate richer motion clues.\nMeanwhile, a new DNN architecture is designed as a model-level solution to\nutilize video semantics and temporal context. Extensive experiments demonstrate\nthat VCC achieves state-of-the-art VAD performance. Our codes and results are\nopen at \\url{https://github.com/yuguangnudt/VEC_VAD/tree/VCC}",
          "link": "http://arxiv.org/abs/2108.02356",
          "publishedOn": "2021-08-06T00:51:45.188Z",
          "wordCount": 729,
          "title": "Video Abnormal Event Detection by Learning to Complete Visual Cloze Tests. (arXiv:2108.02356v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Minghang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>",
          "description": "The recently proposed Detection Transformer (DETR) model successfully applies\nTransformer to objects detection and achieves comparable performance with\ntwo-stage object detection frameworks, such as Faster-RCNN. However, DETR\nsuffers from its slow convergence. Training DETR from scratch needs 500 epochs\nto achieve a high accuracy. To accelerate its convergence, we propose a simple\nyet effective scheme for improving the DETR framework, namely Spatially\nModulated Co-Attention (SMCA) mechanism. The core idea of SMCA is to conduct\nlocation-aware co-attention in DETR by constraining co-attention responses to\nbe high near initially estimated bounding box locations. Our proposed SMCA\nincreases DETR's convergence speed by replacing the original co-attention\nmechanism in the decoder while keeping other operations in DETR unchanged.\nFurthermore, by integrating multi-head and scale-selection attention designs\ninto SMCA, our fully-fledged SMCA can achieve better performance compared to\nDETR with a dilated convolution-based backbone (45.6 mAP at 108 epochs vs. 43.3\nmAP at 500 epochs). We perform extensive ablation studies on COCO dataset to\nvalidate SMCA. Code is released at https://github.com/gaopengcuhk/SMCA-DETR .",
          "link": "http://arxiv.org/abs/2108.02404",
          "publishedOn": "2021-08-06T00:51:45.179Z",
          "wordCount": 611,
          "title": "Fast Convergence of DETR with Spatially Modulated Co-Attention. (arXiv:2108.02404v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02348",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanhui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1\">Xiao Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaolin Wu</a>",
          "description": "For deep learning methods of image super-resolution, the most critical issue\nis whether the paired low and high resolution images for training accurately\nreflect the sampling process of real cameras. Low and high resolution\n(LR$\\sim$HR) image pairs synthesized by existing degradation models (\\eg,\nbicubic downsampling) deviate from those in reality; thus the super-resolution\nCNN trained by these synthesized LR$\\sim$HR image pairs does not perform well\nwhen being applied to real images. In this paper, we propose a novel method to\ncapture a large set of realistic LR$\\sim$HR image pairs using real cameras.The\ndata acquisition is carried out under controllable lab conditions with minimum\nhuman intervention and at high throughput (about 500 image pairs per hour). The\nhigh level of automation makes it easy to produce a set of real LR$\\sim$HR\ntraining image pairs for each camera. Our innovation is to shoot images\ndisplayed on an ultra-high quality screen at different resolutions.There are\nthree distinctive advantages with our method that allow us to collect\nhigh-quality training datasets for image super-resolution. First, as the LR and\nHR images are taken of a 3D planar surface (the screen) the registration\nproblem fits exactly to a homography model. Second, we can display special\nmarkers on the image margin to further improve the registration\nprecision.Third, the displayed digital image file can be exploited as a\nreference to optimize the high frequency content of the restored image.\nExperimental results show that training a super-resolution CNN by our\nLR$\\sim$HR dataset has superior restoration performance than training it by\nexisting datasets on real world images at the inference stage.",
          "link": "http://arxiv.org/abs/2108.02348",
          "publishedOn": "2021-08-06T00:51:45.161Z",
          "wordCount": 699,
          "title": "Dual-reference Training Data Acquisition and CNN Construction for Image Super-Resolution. (arXiv:2108.02348v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Richard J. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Ming Y. Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F. K. Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tiffany Y. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipkova_J/0/1/0/all/0/1\">Jana Lipkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaban_M/0/1/0/all/0/1\">Muhammad Shaban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shady_M/0/1/0/all/0/1\">Maha Shady</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1\">Mane Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_B/0/1/0/all/0/1\">Bumjin Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noor_Z/0/1/0/all/0/1\">Zahra Noor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1\">Faisal Mahmood</a>",
          "description": "The rapidly emerging field of deep learning-based computational pathology has\ndemonstrated promise in developing objective prognostic models from histology\nwhole slide images. However, most prognostic models are either based on\nhistology or genomics alone and do not address how histology and genomics can\nbe integrated to develop joint image-omic prognostic models. Additionally\nidentifying explainable morphological and molecular descriptors from these\nmodels that govern such prognosis is of interest. We used multimodal deep\nlearning to integrate gigapixel whole slide pathology images, RNA-seq\nabundance, copy number variation, and mutation data from 5,720 patients across\n14 major cancer types. Our interpretable, weakly-supervised, multimodal deep\nlearning algorithm is able to fuse these heterogeneous modalities for\npredicting outcomes and discover prognostic features from these modalities that\ncorroborate with poor and favorable outcomes via multimodal interpretability.\nWe compared our model with unimodal deep learning models trained on histology\nslides and molecular profiles alone, and demonstrate performance increase in\nrisk stratification on 9 out of 14 cancers. In addition, we analyze morphologic\nand molecular markers responsible for prognostic predictions across all cancer\ntypes. All analyzed data, including morphological and molecular correlates of\npatient prognosis across the 14 cancer types at a disease and patient level are\npresented in an interactive open-access database\n(this http URL) to allow for further exploration and\nprognostic biomarker discovery. To validate that these model explanations are\nprognostic, we further analyzed high attention morphological regions in WSIs,\nwhich indicates that tumor-infiltrating lymphocyte presence corroborates with\nfavorable cancer prognosis on 9 out of 14 cancer types studied.",
          "link": "http://arxiv.org/abs/2108.02278",
          "publishedOn": "2021-08-06T00:51:45.144Z",
          "wordCount": 733,
          "title": "Pan-Cancer Integrative Histology-Genomic Analysis via Interpretable Multimodal Deep Learning. (arXiv:2108.02278v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xuri Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fuhai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1\">Joemon M. Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhilong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>",
          "description": "The current state-of-the-art image-sentence retrieval methods implicitly\nalign the visual-textual fragments, like regions in images and words in\nsentences, and adopt attention modules to highlight the relevance of\ncross-modal semantic correspondences. However, the retrieval performance\nremains unsatisfactory due to a lack of consistent representation in both\nsemantics and structural spaces. In this work, we propose to address the above\nissue from two aspects: (i) constructing intrinsic structure (along with\nrelations) among the fragments of respective modalities, e.g., \"dog $\\to$ play\n$\\to$ ball\" in semantic structure for an image, and (ii) seeking explicit\ninter-modal structural and semantic correspondence between the visual and\ntextual modalities. In this paper, we propose a novel Structured Multi-modal\nFeature Embedding and Alignment (SMFEA) model for image-sentence retrieval. In\norder to jointly and explicitly learn the visual-textual embedding and the\ncross-modal alignment, SMFEA creates a novel multi-modal structured module with\na shared context-aware referral tree. In particular, the relations of the\nvisual and textual fragments are modeled by constructing Visual Context-aware\nStructured Tree encoder (VCS-Tree) and Textual Context-aware Structured Tree\nencoder (TCS-Tree) with shared labels, from which visual and textual features\ncan be jointly learned and optimized. We utilize the multi-modal tree structure\nto explicitly align the heterogeneous image-sentence data by maximizing the\nsemantic and structural similarity between corresponding inter-modal tree\nnodes. Extensive experiments on Microsoft COCO and Flickr30K benchmarks\ndemonstrate the superiority of the proposed model in comparison to the\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.02417",
          "publishedOn": "2021-08-06T00:51:45.136Z",
          "wordCount": 690,
          "title": "Structured Multi-modal Feature Embedding and Alignment for Image-Sentence Retrieval. (arXiv:2108.02417v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiange Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fangfang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsui_K/0/1/0/all/0/1\">Kwok-Leung Tsui</a>",
          "description": "Automated inspection and detection of foreign objects on railways is\nimportant for rail transportation safety as it helps prevent potential\naccidents and trains derailment. Most existing vision-based approaches focus on\nthe detection of frontal intrusion objects with prior labels, such as\ncategories and locations of the objects. In reality, foreign objects with\nunknown categories can appear anytime on railway tracks. In this paper, we\ndevelop a semi-supervised convolutional autoencoder based framework that only\nrequires railway track images without prior knowledge on the foreign objects in\nthe training process. It consists of three different modules, a bottleneck\nfeature generator as encoder, a photographic image generator as decoder, and a\nreconstruction discriminator developed via adversarial learning. In the\nproposed framework, the problem of detecting the presence, location, and shape\nof foreign objects is addressed by comparing the input and reconstructed images\nas well as setting thresholds based on reconstruction errors. The proposed\nmethod is evaluated through comprehensive studies under different performance\ncriteria. The results show that the proposed method outperforms some well-known\nbenchmarking methods. The proposed framework is useful for data analytics via\nthe train Internet-of-Things (IoT) systems",
          "link": "http://arxiv.org/abs/2108.02421",
          "publishedOn": "2021-08-06T00:51:45.129Z",
          "wordCount": 626,
          "title": "Intelligent Railway Foreign Object Detection: A Semi-supervised Convolutional Autoencoder Based Method. (arXiv:2108.02421v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weijie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang%2A_C/0/1/0/all/0/1\">Chong Wang*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haohe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shenghao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Song Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xulun Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiafei Wu</a>",
          "description": "Expensive bounding-box annotations have limited the development of object\ndetection task. Thus, it is necessary to focus on more challenging task of\nfew-shot object detection. It requires the detector to recognize objects of\nnovel classes with only a few training samples. Nowadays, many existing popular\nmethods based on meta-learning have achieved promising performance, such as\nMeta R-CNN series. However, only a single category of support data is used as\nthe attention to guide the detecting of query images each time. Their relevance\nto each other remains unexploited. Moreover, a lot of recent works treat the\nsupport data and query images as independent branch without considering the\nrelationship between them. To address this issue, we propose a dynamic\nrelevance learning model, which utilizes the relationship between all support\nimages and Region of Interest (RoI) on the query images to construct a dynamic\ngraph convolutional network (GCN). By adjusting the prediction distribution of\nthe base detector using the output of this GCN, the proposed model can guide\nthe detector to improve the class representation implicitly. Comprehensive\nexperiments have been conducted on Pascal VOC and MS-COCO dataset. The proposed\nmodel achieves the best overall performance, which shows its effectiveness of\nlearning more generalized features. Our code is available at\nhttps://github.com/liuweijie19980216/DRL-for-FSOD.",
          "link": "http://arxiv.org/abs/2108.02235",
          "publishedOn": "2021-08-06T00:51:45.120Z",
          "wordCount": 664,
          "title": "Dynamic Relevance Learning for Few-Shot Object Detection. (arXiv:2108.02235v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guolei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingyun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "Due to the fact that fully supervised semantic segmentation methods require\nsufficient fully-labeled data to work well and can not generalize to unseen\nclasses, few-shot segmentation has attracted lots of research attention.\nPrevious arts extract features from support and query images, which are\nprocessed jointly before making predictions on query images. The whole process\nis based on convolutional neural networks (CNN), leading to the problem that\nonly local information is used. In this paper, we propose a TRansformer-based\nFew-shot Semantic segmentation method (TRFS). Specifically, our model consists\nof two modules: Global Enhancement Module (GEM) and Local Enhancement Module\n(LEM). GEM adopts transformer blocks to exploit global information, while LEM\nutilizes conventional convolutions to exploit local information, across query\nand support features. Both GEM and LEM are complementary, helping to learn\nbetter feature representations for segmenting query images. Extensive\nexperiments on PASCAL-5i and COCO datasets show that our approach achieves new\nstate-of-the-art performance, demonstrating its effectiveness.",
          "link": "http://arxiv.org/abs/2108.02266",
          "publishedOn": "2021-08-06T00:51:45.083Z",
          "wordCount": 601,
          "title": "Boosting Few-shot Semantic Segmentation with Transformers. (arXiv:2108.02266v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+he_D/0/1/0/all/0/1\">Dailan he</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yusheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Junyu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_T/0/1/0/all/0/1\">Tianrui Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaofei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aixi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>",
          "description": "Recently proposed fine-grained 3D visual grounding is an essential and\nchallenging task, whose goal is to identify the 3D object referred by a natural\nlanguage sentence from other distractive objects of the same category. Existing\nworks usually adopt dynamic graph networks to indirectly model the\nintra/inter-modal interactions, making the model difficult to distinguish the\nreferred object from distractors due to the monolithic representations of\nvisual and linguistic contents. In this work, we exploit Transformer for its\nnatural suitability on permutation-invariant 3D point clouds data and propose a\nTransRefer3D network to extract entity-and-relation aware multimodal context\namong objects for more discriminative feature learning. Concretely, we devise\nan Entity-aware Attention (EA) module and a Relation-aware Attention (RA)\nmodule to conduct fine-grained cross-modal feature matching. Facilitated by\nco-attention operation, our EA module matches visual entity features with\nlinguistic entity features while RA module matches pair-wise visual relation\nfeatures with linguistic relation features, respectively. We further integrate\nEA and RA modules into an Entity-and-Relation aware Contextual Block (ERCB) and\nstack several ERCBs to form our TransRefer3D for hierarchical multimodal\ncontext modeling. Extensive experiments on both Nr3D and Sr3D datasets\ndemonstrate that our proposed model significantly outperforms existing\napproaches by up to 10.6% and claims the new state-of-the-art. To the best of\nour knowledge, this is the first work investigating Transformer architecture\nfor fine-grained 3D visual grounding task.",
          "link": "http://arxiv.org/abs/2108.02388",
          "publishedOn": "2021-08-06T00:51:45.027Z",
          "wordCount": 676,
          "title": "TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding. (arXiv:2108.02388v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_L/0/1/0/all/0/1\">Lam Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thuc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunil Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Deokjai Choi</a>",
          "description": "Gait data captured by inertial sensors have demonstrated promising results on\nuser authentication. However, most existing approaches stored the enrolled gait\npattern insecurely for matching with the validating pattern, thus, posed\ncritical security and privacy issues. In this study, we present a gait\ncryptosystem that generates from gait data the random key for user\nauthentication, meanwhile, secures the gait pattern. First, we propose a\nrevocable and random binary string extraction method using a deep neural\nnetwork followed by feature-wise binarization. A novel loss function for\nnetwork optimization is also designed, to tackle not only the intrauser\nstability but also the inter-user randomness. Second, we propose a new\nbiometric key generation scheme, namely Irreversible Error Correct and\nObfuscate (IECO), improved from the Error Correct and Obfuscate (ECO) scheme,\nto securely generate from the binary string the random and irreversible key.\nThe model was evaluated with two benchmark datasets as OU-ISIR and whuGAIT. We\nshowed that our model could generate the key of 139 bits from 5-second data\nsequence with zero False Acceptance Rate (FAR) and False Rejection Rate (FRR)\nsmaller than 5.441%. In addition, the security and user privacy analyses showed\nthat our model was secure against existing attacks on biometric template\nprotection, and fulfilled irreversibility and unlinkability.",
          "link": "http://arxiv.org/abs/2108.02400",
          "publishedOn": "2021-08-06T00:51:45.006Z",
          "wordCount": 652,
          "title": "Security and Privacy Enhanced Gait Authentication with Random Representation Learning and Digital Lockers. (arXiv:2108.02400v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weijiang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Lei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuejian Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1\">Nong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>",
          "description": "The task of video-based commonsense captioning aims to generate event-wise\ncaptions and meanwhile provide multiple commonsense descriptions (e.g.,\nattribute, effect and intention) about the underlying event in the video. Prior\nworks explore the commonsense captions by using separate networks for different\ncommonsense types, which is time-consuming and lacks mining the interaction of\ndifferent commonsense. In this paper, we propose a Hybrid Reasoning Network\n(HybridNet) to endow the neural networks with the capability of semantic-level\nreasoning and word-level reasoning. Firstly, we develop multi-commonsense\nlearning for semantic-level reasoning by jointly training different commonsense\ntypes in a unified network, which encourages the interaction between the clues\nof multiple commonsense descriptions, event-wise captions and videos. Then,\nthere are two steps to achieve the word-level reasoning: (1) a memory module\nrecords the history predicted sequence from the previous generation processes;\n(2) a memory-routed multi-head attention (MMHA) module updates the word-level\nattention maps by incorporating the history information from the memory module\ninto the transformer decoder for word-level reasoning. Moreover, the multimodal\nfeatures are used to make full use of diverse knowledge for commonsense\nreasoning. Experiments and abundant analysis on the large-scale\nVideo-to-Commonsense benchmark show that our HybridNet achieves\nstate-of-the-art performance compared with other methods.",
          "link": "http://arxiv.org/abs/2108.02365",
          "publishedOn": "2021-08-06T00:51:44.981Z",
          "wordCount": 651,
          "title": "Hybrid Reasoning Network for Video-based Commonsense Captioning. (arXiv:2108.02365v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Duo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>",
          "description": "Semantic segmentation is a crucial image understanding task, where each pixel\nof image is categorized into a corresponding label. Since the pixel-wise\nlabeling for ground-truth is tedious and labor intensive, in practical\napplications, many works exploit the synthetic images to train the model for\nreal-word image semantic segmentation, i.e., Synthetic-to-Real Semantic\nSegmentation (SRSS). However, Deep Convolutional Neural Networks (CNNs) trained\non the source synthetic data may not generalize well to the target real-world\ndata. In this work, we propose two simple yet effective texture randomization\nmechanisms, Global Texture Randomization (GTR) and Local Texture Randomization\n(LTR), for Domain Generalization based SRSS. GTR is proposed to randomize the\ntexture of source images into diverse unreal texture styles. It aims to\nalleviate the reliance of the network on texture while promoting the learning\nof the domain-invariant cues. In addition, we find the texture difference is\nnot always occurred in entire image and may only appear in some local areas.\nTherefore, we further propose a LTR mechanism to generate diverse local regions\nfor partially stylizing the source images. Finally, we implement a\nregularization of Consistency between GTR and LTR (CGL) aiming to harmonize the\ntwo proposed mechanisms during training. Extensive experiments on five publicly\navailable datasets (i.e., GTA5, SYNTHIA, Cityscapes, BDDS and Mapillary) with\nvarious SRSS settings (i.e., GTA5/SYNTHIA to Cityscapes/BDDS/Mapillary)\ndemonstrate that the proposed method is superior to the state-of-the-art\nmethods for domain generalization based SRSS.",
          "link": "http://arxiv.org/abs/2108.02376",
          "publishedOn": "2021-08-06T00:51:44.972Z",
          "wordCount": 686,
          "title": "Global and Local Texture Randomization for Synthetic-to-Real Semantic Segmentation. (arXiv:2108.02376v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiarui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>",
          "description": "One-stage long-tailed recognition methods improve the overall performance in\na \"seesaw\" manner, i.e., either sacrifice the head's accuracy for better tail\nclassification or elevate the head's accuracy even higher but ignore the tail.\nExisting algorithms bypass such trade-off by a multi-stage training process:\npre-training on imbalanced set and fine-tuning on balanced set. Though\nachieving promising performance, not only are they sensitive to the\ngeneralizability of the pre-trained model, but also not easily integrated into\nother computer vision tasks like detection and segmentation, where pre-training\nof classifiers solely is not applicable. In this paper, we propose a one-stage\nlong-tailed recognition scheme, ally complementary experts (ACE), where the\nexpert is the most knowledgeable specialist in a sub-set that dominates its\ntraining, and is complementary to other experts in the less-seen categories\nwithout being disturbed by what it has never seen. We design a\ndistribution-adaptive optimizer to adjust the learning pace of each expert to\navoid over-fitting. Without special bells and whistles, the vanilla ACE\noutperforms the current one-stage SOTA method by 3-10% on CIFAR10-LT,\nCIFAR100-LT, ImageNet-LT and iNaturalist datasets. It is also shown to be the\nfirst one to break the \"seesaw\" trade-off by improving the accuracy of the\nmajority and minority categories simultaneously in only one stage. Code and\ntrained models are at https://github.com/jrcai/ACE.",
          "link": "http://arxiv.org/abs/2108.02385",
          "publishedOn": "2021-08-06T00:51:44.960Z",
          "wordCount": 657,
          "title": "ACE: Ally Complementary Experts for Solving Long-Tailed Recognition in One-Shot. (arXiv:2108.02385v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xinzhi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenju Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chunxia Xiao</a>",
          "description": "Existing image captioning methods just focus on understanding the\nrelationship between objects or instances in a single image, without exploring\nthe contextual correlation existed among contextual image. In this paper, we\npropose Dual Graph Convolutional Networks (Dual-GCN) with transformer and\ncurriculum learning for image captioning. In particular, we not only use an\nobject-level GCN to capture the object to object spatial relation within a\nsingle image, but also adopt an image-level GCN to capture the feature\ninformation provided by similar images. With the well-designed Dual-GCN, we can\nmake the linguistic transformer better understand the relationship between\ndifferent objects in a single image and make full use of similar images as\nauxiliary information to generate a reasonable caption description for a single\nimage. Meanwhile, with a cross-review strategy introduced to determine\ndifficulty levels, we adopt curriculum learning as the training strategy to\nincrease the robustness and generalization of our proposed model. We conduct\nextensive experiments on the large-scale MS COCO dataset, and the experimental\nresults powerfully demonstrate that our proposed method outperforms recent\nstate-of-the-art approaches. It achieves a BLEU-1 score of 82.2 and a BLEU-2\nscore of 67.6. Our source code is available at {\\em\n\\color{magenta}{\\url{https://github.com/Unbear430/DGCN-for-image-captioning}}}.",
          "link": "http://arxiv.org/abs/2108.02366",
          "publishedOn": "2021-08-06T00:51:44.946Z",
          "wordCount": 662,
          "title": "Dual Graph Convolutional Networks with Transformer and Curriculum Learning for Image Captioning. (arXiv:2108.02366v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1\">Sen Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Richard Yi Da Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1\">Gaofeng Meng</a>",
          "description": "The vanilla GAN [5] suffers from mode collapse deeply, which usually\nmanifests as that the images generated by generators tend to have a high\nsimilarity amongst them, even though their corresponding latent vectors have\nbeen very different. In this paper, we introduce a pluggable block called\ndiversity penalty (dp) to alleviate mode collapse of GANs. It is used to reduce\nthe similarity of image pairs in feature space, i.e., if two latent vectors are\ndifferent, then we enforce the generator to generate two images with different\nfeatures. The normalized Gram Matrix is used to measure the similarity. We\ncompare the proposed method with Unrolled GAN [17], BourGAN [26], PacGAN [14],\nVEEGAN [23] and ALI [4] on 2D synthetic dataset, and results show that our\nproposed method can help GAN capture more modes of the data distribution.\nFurther, we apply this penalty term into image data augmentation on MNIST,\nFashion-MNIST and CIFAR-10, and the testing accuracy is improved by 0.24%,\n1.34% and 0.52% compared with WGAN GP [6], respectively. Finally, we\nquantitatively evaluate the proposed method with IS and FID on CelebA,\nCIFAR-10, MNIST and Fashion-MNIST. Results show that our method gets much\nhigher IS and lower FID compared with some current GAN architectures.",
          "link": "http://arxiv.org/abs/2108.02353",
          "publishedOn": "2021-08-06T00:51:44.896Z",
          "wordCount": 644,
          "title": "dp-GAN : Alleviating Mode Collapse in GAN via Diversity Penalty Module. (arXiv:2108.02353v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02234",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1\">Nathanael L. Baisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Bryan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelov_P/0/1/0/all/0/1\">Plamen Angelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1\">Sue Black</a>",
          "description": "In this paper, we propose a novel hand-based person recognition method for\nthe purpose of criminal investigations since the hand image is often the only\navailable information in cases of serious crime such as sexual abuse. Our\nproposed method, Multi-Branch with Attention Network (MBA-Net), incorporates\nboth channel and spatial attention modules in branches in addition to a global\n(without attention) branch to capture global structural information for\ndiscriminative feature learning. The attention modules focus on the relevant\nfeatures of the hand image while suppressing the irrelevant backgrounds. In\norder to overcome the weakness of the attention mechanisms, equivariant to\npixel shuffling, we integrate relative positional encodings into the spatial\nattention module to capture the spatial positions of pixels. Extensive\nevaluations on two large multi-ethnic and publicly available hand datasets\ndemonstrate that our proposed method achieves state-of-the-art performance,\nsurpassing the existing hand-based identification methods.",
          "link": "http://arxiv.org/abs/2108.02234",
          "publishedOn": "2021-08-06T00:51:44.857Z",
          "wordCount": 583,
          "title": "Multi-Branch with Attention Network for Hand-Based Person Recognition. (arXiv:2108.02234v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02317",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Qiu_Z/0/1/0/all/0/1\">Ziheng Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1\">Xinyi Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_T/0/1/0/all/0/1\">Tianao Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_P/0/1/0/all/0/1\">Pan Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zibang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhong_J/0/1/0/all/0/1\">Jingang Zhong</a>",
          "description": "Fourier single-pixel imaging (FSI) is a branch of single-pixel imaging\ntechniques. It uses Fourier basis patterns as structured patterns for spatial\ninformation acquisition in the Fourier domain. However, the spatial resolution\nof the image reconstructed by FSI mainly depends on the number of Fourier\ncoefficients sampled. The reconstruction of a high-resolution image typically\nrequires a number of Fourier coefficients to be sampled, and therefore takes a\nlong data acquisition time. Here we propose a new sampling strategy for FSI. It\nallows FSI to reconstruct a clear and sharp image with a reduced number of\nmeasurements. The core of the proposed sampling strategy is to perform a\nvariable density sampling in the Fourier space and, more importantly, the\ndensity with respect to the importance of Fourier coefficients is subject to a\none-dimensional Gaussian function. Combined with compressive sensing, the\nproposed sampling strategy enables better reconstruction quality than\nconventional sampling strategies, especially when the sampling ratio is low. We\nexperimentally demonstrate compressive FSI combined with the proposed sampling\nstrategy is able to reconstruct a sharp and clear image of 256-by-256 pixels\nwith a sampling ratio of 10%. The proposed method enables fast single-pixel\nimaging and provides a new approach for efficient spatial information\nacquisition.",
          "link": "http://arxiv.org/abs/2108.02317",
          "publishedOn": "2021-08-06T00:51:44.849Z",
          "wordCount": 653,
          "title": "Efficient Fourier single-pixel imaging with Gaussian random sampling. (arXiv:2108.02317v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Cong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yicheng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Judish_M/0/1/0/all/0/1\">Max Judish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1\">Russell H Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armand_M/0/1/0/all/0/1\">Mehran Armand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grupp_R/0/1/0/all/0/1\">Robert Grupp</a>",
          "description": "Image-based navigation is widely considered the next frontier of minimally\ninvasive surgery. It is believed that image-based navigation will increase the\naccess to reproducible, safe, and high-precision surgery as it may then be\nperformed at acceptable costs and effort. This is because image-based\ntechniques avoid the need of specialized equipment and seamlessly integrate\nwith contemporary workflows. Further, it is expected that image-based\nnavigation will play a major role in enabling mixed reality environments and\nautonomous, robotic workflows. A critical component of image guidance is 2D/3D\nregistration, a technique to estimate the spatial relationships between 3D\nstructures, e.g., volumetric imagery or tool models, and 2D images thereof,\nsuch as fluoroscopy or endoscopy. While image-based 2D/3D registration is a\nmature technique, its transition from the bench to the bedside has been\nrestrained by well-known challenges, including brittleness of the optimization\nobjective, hyperparameter selection, and initialization, difficulties around\ninconsistencies or multiple objects, and limited single-view performance. One\nreason these challenges persist today is that analytical solutions are likely\ninadequate considering the complexity, variability, and high-dimensionality of\ngeneric 2D/3D registration problems. The recent advent of machine\nlearning-based approaches to imaging problems that, rather than specifying the\ndesired functional mapping, approximate it using highly expressive parametric\nmodels holds promise for solving some of the notorious challenges in 2D/3D\nregistration. In this manuscript, we review the impact of machine learning on\n2D/3D registration to systematically summarize the recent advances made by\nintroduction of this novel technology. Grounded in these insights, we then\noffer our perspective on the most pressing needs, significant open problems,\nand possible next steps.",
          "link": "http://arxiv.org/abs/2108.02238",
          "publishedOn": "2021-08-06T00:51:44.842Z",
          "wordCount": 727,
          "title": "The Impact of Machine Learning on 2D/3D Registration for Image-guided Interventions: A Systematic Review and Perspective. (arXiv:2108.02238v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1\">Willy Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossinelli_D/0/1/0/all/0/1\">Diego Rossinelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_G/0/1/0/all/0/1\">Georg Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenger_R/0/1/0/all/0/1\">Roland H. Wenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hieber_S/0/1/0/all/0/1\">Simone Hieber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1\">Bert M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtcuoglu_V/0/1/0/all/0/1\">Vartan Kurtcuoglu</a>",
          "description": "The performance of machine learning algorithms used for the segmentation of\n3D biomedical images lags behind that of the algorithms employed in the\nclassification of 2D photos. This may be explained by the comparative lack of\nhigh-volume, high-quality training datasets, which require state-of-the art\nimaging facilities, domain experts for annotation and large computational and\npersonal resources to create. The HR-Kidney dataset presented in this work\nbridges this gap by providing 1.7 TB of artefact-corrected synchrotron\nradiation-based X-ray phase-contrast microtomography images of whole mouse\nkidneys and validated segmentations of 33 729 glomeruli, which represents a 1-2\norders of magnitude increase over currently available biomedical datasets. The\ndataset further contains the underlying raw data, classical segmentations of\nrenal vasculature and uriniferous tubules, as well as true 3D manual\nannotations. By removing limits currently imposed by small training datasets,\nthe provided data open up the possibility for disruptions in machine learning\nfor biomedical image analysis.",
          "link": "http://arxiv.org/abs/2108.02226",
          "publishedOn": "2021-08-06T00:51:44.814Z",
          "wordCount": 610,
          "title": "Terabyte-scale supervised 3D training and benchmarking dataset of the mouse kidney. (arXiv:2108.02226v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Quiros_A/0/1/0/all/0/1\">Adalberto Claudio Quiros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coudray_N/0/1/0/all/0/1\">Nicolas Coudray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeaton_A/0/1/0/all/0/1\">Anna Yeaton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunhem_W/0/1/0/all/0/1\">Wisuwat Sunhem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_Smith_R/0/1/0/all/0/1\">Roderick Murray-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsirigos_A/0/1/0/all/0/1\">Aristotelis Tsirigos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Ke Yuan</a>",
          "description": "Deep learning based analysis of histopathology images shows promise in\nadvancing the understanding of tumor progression, tumor micro-environment, and\ntheir underpinning biological processes. So far, these approaches have focused\non extracting information associated with annotations. In this work, we ask how\nmuch information can be learned from the tissue architecture itself.\n\nWe present an adversarial learning model to extract feature representations\nof cancer tissue, without the need for manual annotations. We show that these\nrepresentations are able to identify a variety of morphological characteristics\nacross three cancer types: Breast, colon, and lung. This is supported by 1) the\nseparation of morphologic characteristics in the latent space; 2) the ability\nto classify tissue type with logistic regression using latent representations,\nwith an AUC of 0.97 and 85% accuracy, comparable to supervised deep models; 3)\nthe ability to predict the presence of tumor in Whole Slide Images (WSIs) using\nmultiple instance learning (MIL), achieving an AUC of 0.98 and 94% accuracy.\n\nOur results show that our model captures distinct phenotypic characteristics\nof real tissue samples, paving the way for further understanding of tumor\nprogression and tumor micro-environment, and ultimately refining\nhistopathological classification for diagnosis and treatment. The code and\npretrained models are available at:\nhttps://github.com/AdalbertoCq/Adversarial-learning-of-cancer-tissue-representations",
          "link": "http://arxiv.org/abs/2108.02223",
          "publishedOn": "2021-08-06T00:51:44.800Z",
          "wordCount": 655,
          "title": "Adversarial learning of cancer tissue representations. (arXiv:2108.02223v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1\">Nitish Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prados_D/0/1/0/all/0/1\">David Ramon Prados</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodzic_N/0/1/0/all/0/1\">Nedim Hodzic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanassios_C/0/1/0/all/0/1\">Christos Karanassios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1\">H.R. Tizhoosh</a>",
          "description": "Lung nodules are commonly missed in chest radiographs. We propose and\nevaluate P-AnoGAN, an unsupervised anomaly detection approach for lung nodules\nin radiographs. P-AnoGAN modifies the fast anomaly detection generative\nadversarial network (f-AnoGAN) by utilizing a progressive GAN and a\nconvolutional encoder-decoder-encoder pipeline. Model training uses only\nunlabelled healthy lung patches extracted from the Indiana University Chest\nX-Ray Collection. External validation and testing are performed using healthy\nand unhealthy patches extracted from the ChestX-ray14 and Japanese Society for\nRadiological Technology datasets, respectively. Our model robustly identifies\npatches containing lung nodules in external validation and test data with\nROC-AUC of 91.17% and 87.89%, respectively. These results show unsupervised\nmethods may be useful in challenging tasks such as lung nodule detection in\nradiographs.",
          "link": "http://arxiv.org/abs/2108.02233",
          "publishedOn": "2021-08-06T00:51:44.792Z",
          "wordCount": 594,
          "title": "Unsupervised Detection of Lung Nodules in Chest Radiography Using Generative Adversarial Networks. (arXiv:2108.02233v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bingfeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhenjie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi Harold Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuigen Wang</a>",
          "description": "Human vision is often adversely affected by complex environmental factors,\nespecially in night vision scenarios. Thus, infrared cameras are often\nleveraged to help enhance the visual effects via detecting infrared radiation\nin the surrounding environment, but the infrared videos are undesirable due to\nthe lack of detailed semantic information. In such a case, an effective\nvideo-to-video translation method from the infrared domain to the visible light\ncounterpart is strongly needed by overcoming the intrinsic huge gap between\ninfrared and visible fields. To address this challenging problem, we propose an\ninfrared-to-visible (I2V) video translation method I2V-GAN to generate\nfine-grained and spatial-temporal consistent visible light videos by given\nunpaired infrared videos. Technically, our model capitalizes on three types of\nconstraints: 1)adversarial constraint to generate synthetic frames that are\nsimilar to the real ones, 2)cyclic consistency with the introduced perceptual\nloss for effective content conversion as well as style preservation, and\n3)similarity constraints across and within domains to enhance the content and\nmotion consistency in both spatial and temporal spaces at a fine-grained level.\nFurthermore, the current public available infrared and visible light datasets\nare mainly used for object detection or tracking, and some are composed of\ndiscontinuous images which are not suitable for video tasks. Thus, we provide a\nnew dataset for I2V video translation, which is named IRVI. Specifically, it\nhas 12 consecutive video clips of vehicle and monitoring scenes, and both\ninfrared and visible light videos could be apart into 24352 frames.\nComprehensive experiments validate that I2V-GAN is superior to the compared\nSOTA methods in the translation of I2V videos with higher fluency and finer\nsemantic details. The code and IRVI dataset are available at\nhttps://github.com/BIT-DA/I2V-GAN.",
          "link": "http://arxiv.org/abs/2108.00913",
          "publishedOn": "2021-08-05T01:56:20.655Z",
          "wordCount": 738,
          "title": "I2V-GAN: Unpaired Infrared-to-Visible Video Translation. (arXiv:2108.00913v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changgong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1\">Tien-Tsin Wong</a>",
          "description": "Graph convolutional networks have significantly improved 3D human pose\nestimation by representing the human skeleton as an undirected graph. However,\nthis representation fails to reflect the articulated characteristic of human\nskeletons as the hierarchical orders among the joints are not explicitly\npresented. In this paper, we propose to represent the human skeleton as a\ndirected graph with the joints as nodes and bones as edges that are directed\nfrom parent joints to child joints. By so doing, the directions of edges can\nexplicitly reflect the hierarchical relationships among the nodes. Based on\nthis representation, we further propose a spatial-temporal conditional directed\ngraph convolution to leverage varying non-local dependence for different poses\nby conditioning the graph topology on input poses. Altogether, we form a\nU-shaped network, named U-shaped Conditional Directed Graph Convolutional\nNetwork, for 3D human pose estimation from monocular videos. To evaluate the\neffectiveness of our method, we conducted extensive experiments on two\nchallenging large-scale benchmarks: Human3.6M and MPI-INF-3DHP. Both\nquantitative and qualitative results show that our method achieves top\nperformance. Also, ablation studies show that directed graphs can better\nexploit the hierarchy of articulated human skeletons than undirected graphs,\nand the conditional connections can yield adaptive graph topologies for\ndifferent poses.",
          "link": "http://arxiv.org/abs/2107.07797",
          "publishedOn": "2021-08-05T01:56:20.488Z",
          "wordCount": 681,
          "title": "Conditional Directed Graph Convolution for 3D Human Pose Estimation. (arXiv:2107.07797v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael Ng</a>",
          "description": "Image smoothing is a fundamental procedure in applications of both computer\nvision and graphics. The required smoothing properties can be different or even\ncontradictive among different tasks. Nevertheless, the inherent smoothing\nnature of one smoothing operator is usually fixed and thus cannot meet the\nvarious requirements of different applications. In this paper, we first\nintroduce the truncated Huber penalty function which shows strong flexibility\nunder different parameter settings. A generalized framework is then proposed\nwith the introduced truncated Huber penalty function. When combined with its\nstrong flexibility, our framework is able to achieve diverse smoothing natures\nwhere contradictive smoothing behaviors can even be achieved. It can also yield\nthe smoothing behavior that can seldom be achieved by previous methods, and\nsuperior performance is thus achieved in challenging cases. These together\nenable our framework capable of a range of applications and able to outperform\nthe state-of-the-art approaches in several tasks, such as image detail\nenhancement, clip-art compression artifacts removal, guided depth map\nrestoration, image texture removal, etc. In addition, an efficient numerical\nsolution is provided and its convergence is theoretically guaranteed even the\noptimization framework is non-convex and non-smooth. A simple yet effective\napproach is further proposed to reduce the computational cost of our method\nwhile maintaining its performance. The effectiveness and superior performance\nof our approach are validated through comprehensive experiments in a range of\napplications. Our code is available at\nhttps://github.com/wliusjtu/Generalized-Smoothing-Framework.",
          "link": "http://arxiv.org/abs/2107.07058",
          "publishedOn": "2021-08-05T01:56:20.481Z",
          "wordCount": 756,
          "title": "A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Darafsh_S/0/1/0/all/0/1\">Sahar Darafsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghidary_S/0/1/0/all/0/1\">Saeed Shiry Ghidary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_M/0/1/0/all/0/1\">Morteza Saheb Zamani</a>",
          "description": "With the rapid increase in digital technologies, most fields of study include\nrecognition of human activity and intention recognition, which are essential in\nsmart environments. In this study, we equipped the activity recognition system\nwith the ability to recognize intentions by affecting the pace of movement of\nindividuals in the representation of images. Using this technology in various\nenvironments such as elevators and automatic doors will lead to identifying\nthose who intend to pass the automatic door from those who are passing by. This\nsystem, if applied in elevators and automatic doors, will save energy and\nincrease efficiency. For this study, data preparation is applied to combine the\nspatial and temporal features with the help of digital image processing\nprinciples. Nevertheless, unlike previous studies, only one AlexNet neural\nnetwork is used instead of two-stream convolutional neural networks. Our\nembedded system was implemented with an accuracy of 98.78% on our intention\nrecognition dataset. We also examined our data representation approach on other\ndatasets, including HMDB-51, KTH, and Weizmann, and obtained accuracy of\n78.48%, 97.95%, and 100%, respectively. The image recognition and neural\nnetwork models were simulated and implemented using Xilinx simulators for the\nXilinx ZCU102 board. The operating frequency of this embedded system is 333\nMHz, and it works in real-time with 120 frames per second (fps).",
          "link": "http://arxiv.org/abs/2107.12744",
          "publishedOn": "2021-08-05T01:56:20.453Z",
          "wordCount": 699,
          "title": "Real-Time Activity Recognition and Intention Recognition Using a Vision-based Embedded System. (arXiv:2107.12744v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03088",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peidong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zibin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xiyu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shutao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Maowei Hu</a>",
          "description": "Compared with tedious per-pixel mask annotating, it is much easier to\nannotate data by clicks, which costs only several seconds for an image.\nHowever, applying clicks to learn video semantic segmentation model has not\nbeen explored before. In this work, we propose an effective weakly-supervised\nvideo semantic segmentation pipeline with click annotations, called WeClick,\nfor saving laborious annotating effort by segmenting an instance of the\nsemantic class with only a single click. Since detailed semantic information is\nnot captured by clicks, directly training with click labels leads to poor\nsegmentation predictions. To mitigate this problem, we design a novel memory\nflow knowledge distillation strategy to exploit temporal information (named\nmemory flow) in abundant unlabeled video frames, by distilling the neighboring\npredictions to the target frame via estimated motion. Moreover, we adopt\nvanilla knowledge distillation for model compression. In this case, WeClick\nlearns compact video semantic segmentation models with the low-cost click\nannotations during the training phase yet achieves real-time and accurate\nmodels during the inference period. Experimental results on Cityscapes and\nCamvid show that WeClick outperforms the state-of-the-art methods, increases\nperformance by 10.24% mIoU than baseline, and achieves real-time execution.",
          "link": "http://arxiv.org/abs/2107.03088",
          "publishedOn": "2021-08-05T01:56:20.446Z",
          "wordCount": 674,
          "title": "WeClick: Weakly-Supervised Video Semantic Segmentation with Click Annotations. (arXiv:2107.03088v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07237",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Pinaki Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadhwa_A/0/1/0/all/0/1\">Angad Wadhwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_A/0/1/0/all/0/1\">Antariksha Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_N/0/1/0/all/0/1\">Nikhil Tyagi</a>",
          "description": "This paper presents a new proposal of an efficient computational model of\nface recognition which uses cues from the distributed face recognition\nmechanism of the brain, and by gathering engineering equivalent of these cues\nfrom existing literature. Three distinct and widely used features: Histogram of\nOriented Gradients (HOG), Local Binary Patterns (LBP), and Principal components\n(PCs) extracted from target images are used in a manner which is simple, and\nyet effective. The HOG and LBP features further undergo principal component\nanalysis for dimensionality reduction. Our model uses multi-layer perceptrons\n(MLP) to classify these three features and fuse them at the decision level\nusing sum rule. A computational theory is first developed by using concepts\nfrom the information processing mechanism of the brain. Extensive experiments\nare carried out using ten publicly available datasets to validate our proposed\nmodel's performance in recognizing faces with extreme variation of\nillumination, pose angle, expression, and background. Results obtained are\nextremely promising when compared with other face recognition algorithms\nincluding CNN and deep learning-based methods. This highlights that simple\ncomputational processes, if clubbed properly, can produce competing performance\nwith best algorithms.",
          "link": "http://arxiv.org/abs/2105.07237",
          "publishedOn": "2021-08-05T01:56:20.438Z",
          "wordCount": 649,
          "title": "Brain Inspired Face Recognition: A Computational Framework. (arXiv:2105.07237v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>",
          "description": "Assessing an AI agent that can converse in human language and understand\nvisual content is challenging. Generation metrics, such as BLEU scores favor\ncorrect syntax over semantics. Hence a discriminative approach is often used,\nwhere an agent ranks a set of candidate options. The mean reciprocal rank (MRR)\nmetric evaluates the model performance by taking into account the rank of a\nsingle human-derived answer. This approach, however, raises a new challenge:\nthe ambiguity and synonymy of answers, for instance, semantic equivalence\n(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative\ngain (NDCG) metric has been used to capture the relevance of all the correct\nanswers via dense annotations. However, the NDCG metric favors the usually\napplicable uncertain answers such as `I don't know. Crafting a model that\nexcels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should\nanswer a human-like reply and validate the correctness of any answer. To\naddress this issue, we describe a two-step non-parametric ranking approach that\ncan merge strong MRR and NDCG models. Using our approach, we manage to keep\nmost MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG\nstate-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won\nthe recent Visual Dialog 2020 challenge. Source code is available at\nhttps://github.com/idansc/mrr-ndcg.",
          "link": "http://arxiv.org/abs/2104.07511",
          "publishedOn": "2021-08-05T01:56:20.427Z",
          "wordCount": 697,
          "title": "Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Celong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>",
          "description": "In this paper, we present an efficient and robust deep learning solution for\nnovel view synthesis of complex scenes. In our approach, a 3D scene is\nrepresented as a light field, i.e., a set of rays, each of which has a\ncorresponding color when reaching the image plane. For efficient novel view\nrendering, we adopt a 4D parameterization of the light field, where each ray is\ncharacterized by a 4D parameter. We then formulate the light field as a 4D\nfunction that maps 4D coordinates to corresponding color values. We train a\ndeep fully connected network to optimize this implicit function and memorize\nthe 3D scene. Then, the scene-specific model is used to synthesize novel views.\nDifferent from previous light field approaches which require dense view\nsampling to reliably render novel views, our method can render novel views by\nsampling rays and querying the color for each ray from the network directly,\nthus enabling high-quality light field rendering with a sparser set of training\nimages. Our method achieves state-of-the-art novel view synthesis results while\nmaintaining an interactive frame rate.",
          "link": "http://arxiv.org/abs/2105.07112",
          "publishedOn": "2021-08-05T01:56:20.420Z",
          "wordCount": 664,
          "title": "NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field. (arXiv:2105.07112v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yaqi Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1\">Kailang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stilla_U/0/1/0/all/0/1\">Uwe Stilla</a>",
          "description": "We tackle the problem of object completion from point clouds and propose a\nnovel point cloud completion network employing an Asymmetrical Siamese Feature\nMatching strategy, termed as ASFM-Net. Specifically, the Siamese auto-encoder\nneural network is adopted to map the partial and complete input point cloud\ninto a shared latent space, which can capture detailed shape prior. Then we\ndesign an iterative refinement unit to generate complete shapes with\nfine-grained details by integrating prior information. Experiments are\nconducted on the PCN dataset and the Completion3D benchmark, demonstrating the\nstate-of-the-art performance of the proposed ASFM-Net. Our method achieves the\n1st place in the leaderboard of Completion3D and outperforms existing methods\nwith a large margin, about 12%. The codes and trained models are released\npublicly at https://github.com/Yan-Xia/ASFM-Net.",
          "link": "http://arxiv.org/abs/2104.09587",
          "publishedOn": "2021-08-05T01:56:20.414Z",
          "wordCount": 622,
          "title": "ASFM-Net: Asymmetrical Siamese Feature Matching Network for Point Completion. (arXiv:2104.09587v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1\">Fang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingjia Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhiliang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhenjun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>",
          "description": "Weakly supervised object localization (WSOL) is a challenging problem when\ngiven image category labels but requires to learn object localization models.\nOptimizing a convolutional neural network (CNN) for classification tends to\nactivate local discriminative regions while ignoring complete object extent,\ncausing the partial activation issue. In this paper, we argue that partial\nactivation is caused by the intrinsic characteristics of CNN, where the\nconvolution operations produce local receptive fields and experience difficulty\nto capture long-range feature dependency among pixels. We introduce the token\nsemantic coupled attention map (TS-CAM) to take full advantage of the\nself-attention mechanism in visual transformer for long-range dependency\nextraction. TS-CAM first splits an image into a sequence of patch tokens for\nspatial embedding, which produce attention maps of long-range visual dependency\nto avoid partial activation. TS-CAM then re-allocates category-related\nsemantics for patch tokens, enabling each of them to be aware of object\ncategories. TS-CAM finally couples the patch tokens with the semantic-agnostic\nattention map to achieve semantic-aware localization. Experiments on the\nILSVRC/CUB-200-2011 datasets show that TS-CAM outperforms its CNN-CAM\ncounterparts by 7.1%/27.1% for WSOL, achieving state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2103.14862",
          "publishedOn": "2021-08-05T01:56:20.407Z",
          "wordCount": 694,
          "title": "TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised Object Localization. (arXiv:2103.14862v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00411",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Khakzar_A/0/1/0/all/0/1\">Ashkan Khakzar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mansour_W/0/1/0/all/0/1\">Wejdene Mansour</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_Y/0/1/0/all/0/1\">Yuezhi Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yawei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yucheng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Seong Tae Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>",
          "description": "Neural networks have demonstrated remarkable performance in classification\nand regression tasks on chest X-rays. In order to establish trust in the\nclinical routine, the networks' prediction mechanism needs to be interpretable.\nOne principal approach to interpretation is feature attribution. Feature\nattribution methods identify the importance of input features for the output\nprediction. Building on Information Bottleneck Attribution (IBA) method, for\neach prediction we identify the chest X-ray regions that have high mutual\ninformation with the network's output. Original IBA identifies input regions\nthat have sufficient predictive information. We propose Inverse IBA to identify\nall informative regions. Thus all predictive cues for pathologies are\nhighlighted on the X-rays, a desirable property for chest X-ray diagnosis.\nMoreover, we propose Regression IBA for explaining regression models. Using\nRegression IBA we observe that a model trained on cumulative severity score\nlabels implicitly learns the severity of different X-ray regions. Finally, we\npropose Multi-layer IBA to generate higher resolution and more detailed\nattribution/saliency maps. We evaluate our methods using both human-centric\n(ground-truth-based) interpretability metrics, and human-independent feature\nimportance metrics on NIH Chest X-ray8 and BrixIA datasets. The Code is\npublicly available.",
          "link": "http://arxiv.org/abs/2104.00411",
          "publishedOn": "2021-08-05T01:56:20.399Z",
          "wordCount": 733,
          "title": "Explaining COVID-19 and Thoracic Pathology Model Predictions by Identifying Informative Input Features. (arXiv:2104.00411v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roland S. Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borowski_J/0/1/0/all/0/1\">Judy Borowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1\">Robert Geirhos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallis_T/0/1/0/all/0/1\">Thomas S. A. Wallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>",
          "description": "One widely used approach towards understanding the inner workings of deep\nconvolutional neural networks is to visualize unit responses via activation\nmaximization. Feature visualizations via activation maximization are thought to\nprovide humans with precise information about the image features that cause a\nunit to be activated. If this is indeed true, these synthetic images should\nenable humans to predict the effect of an intervention, such as whether\noccluding a certain patch of the image (say, a dog's head) changes a unit's\nactivation. Here, we test this hypothesis by asking humans to predict which of\ntwo square occlusions causes a larger change to a unit's activation. Both a\nlarge-scale crowdsourced experiment and measurements with experts show that on\naverage, the extremely activating feature visualizations by Olah et al. (2017)\nindeed help humans on this task ($67 \\pm 4\\%$ accuracy; baseline performance\nwithout any visualizations is $60 \\pm 3\\%$). However, they do not provide any\nsignificant advantage over other visualizations (such as e.g. dataset samples),\nwhich yield similar performance ($66 \\pm 3\\%$ to $67 \\pm 3\\%$ accuracy). Taken\ntogether, we propose an objective psychophysical task to quantify the benefit\nof unit-level interpretability methods for humans, and find no evidence that\nfeature visualizations provide humans with better \"causal understanding\" than\nsimple alternative visualizations.",
          "link": "http://arxiv.org/abs/2106.12447",
          "publishedOn": "2021-08-05T01:56:20.379Z",
          "wordCount": 721,
          "title": "How Well do Feature Visualizations Support Causal Understanding of CNN Activations?. (arXiv:2106.12447v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Zhijian Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hanjiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>",
          "description": "In the field of large-scale SLAM for autonomous driving and mobile robotics,\n3D point cloud based place recognition has aroused significant research\ninterest due to its robustness to changing environments with drastic daytime\nand weather variance. However, it is time-consuming and effort-costly to obtain\nhigh-quality point cloud data for place recognition model training and ground\ntruth for registration in the real world. To this end, a novel\nregistration-aided 3D domain adaptation network for point cloud based place\nrecognition is proposed. A structure-aware registration network is introduced\nto help to learn features with geometric information and a 6-DoFs pose between\ntwo point clouds with partial overlap can be estimated. The model is trained\nthrough a synthetic virtual LiDAR dataset through GTA-V with diverse weather\nand daytime conditions and domain adaptation is implemented to the real-world\ndomain by aligning the global features. Our results outperform state-of-the-art\n3D place recognition baselines or achieve comparable on the real-world Oxford\nRobotCar dataset with the visualization of registration on the virtual dataset.",
          "link": "http://arxiv.org/abs/2012.05018",
          "publishedOn": "2021-08-05T01:56:20.358Z",
          "wordCount": 650,
          "title": "A Registration-aided Domain Adaptation Network for 3D Point Cloud Based Place Recognition. (arXiv:2012.05018v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lijie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonglong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1\">Dina Katabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>",
          "description": "Contrastive learning is effective at learning useful representations without\nsupervision. Yet contrastive learning is susceptible to shortcuts -- i.e., it\nmay learn shortcut features irrelevant to the downstream task and discard\nrelevant information. Past work has addressed this limitation via handcrafted\ndata augmentations that eliminate the shortcut. However, handcrafted\naugmentations are infeasible for data modalities that are not interpretable by\nhumans (e.g., radio signals). Further, even when the modality is interpretable\n(e.g., RGB), sometimes eliminating the shortcut information may be undesirable.\nFor example, in multi-attribute classification, information related to one\nattribute may act as a shortcut around other attributes. This paper presents\nreconstructive contrastive learning (RCL), a framework for learning\nunsupervised representations that are robust to shortcuts. The key idea is to\nforce the learned representation to reconstruct the input, which naturally\ncounters potential shortcuts. Extensive experiments verify that RCL is highly\nrobust to shortcuts and outperforms state-of-the-art contrastive learning\nmethods on both RGB and RF datasets for a variety of tasks.",
          "link": "http://arxiv.org/abs/2012.09962",
          "publishedOn": "2021-08-05T01:56:20.332Z",
          "wordCount": 650,
          "title": "Making Contrastive Learning Robust to Shortcuts. (arXiv:2012.09962v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuhao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1\">Junbao Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Due to the domain discrepancy in visual domain adaptation, the performance of\nsource model degrades when bumping into the high data density near decision\nboundary in target domain. A common solution is to minimize the Shannon Entropy\nto push the decision boundary away from the high density area. However, entropy\nminimization also leads to severe reduction of prediction diversity, and\nunfortunately brings harm to the domain adaptation. In this paper, we\ninvestigate the prediction discriminability and diversity by studying the\nstructure of the classification output matrix of a randomly selected data\nbatch. We find by theoretical analysis that the prediction discriminability and\ndiversity could be separately measured by the Frobenius-norm and rank of the\nbatch output matrix. The nuclear-norm is an upperbound of the former, and a\nconvex approximation of the latter. Accordingly, we propose Batch Nuclear-norm\nMaximization and Minimization, which performs nuclear-norm maximization on the\ntarget output matrix to enhance the target prediction ability, and nuclear-norm\nminimization on the source batch output matrix to increase applicability of the\nsource domain knowledge. We further approximate the nuclear-norm by\nL_{1,2}-norm, and design multi-batch optimization for stable solution on large\nnumber of categories. The fast approximation method achieves O(n^2)\ncomputational complexity and better convergence property. Experiments show that\nour method could boost the adaptation accuracy and robustness under three\ntypical domain adaptation scenarios. The code is available at\nhttps://github.com/cuishuhao/BNM.",
          "link": "http://arxiv.org/abs/2107.06154",
          "publishedOn": "2021-08-05T01:56:20.322Z",
          "wordCount": 732,
          "title": "Fast Batch Nuclear-norm Maximization and Minimization for Robust Domain Adaptation. (arXiv:2107.06154v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>",
          "description": "Existing person re-identification (Re-ID) works mostly consider a short-term\nsearch problem assuming unchanged clothes and personal appearance. However, in\nreal-world we often dress differently across locations, time, dates, seasons,\nweather, and events. As a result, the existing methods are unsuitable for\nlong-term person Re-ID with clothes change involved. Whilst there are several\nrecent long-term Re-ID attempts, a large realistic dataset with clothes change\nis lacking and indispensable for enabling extensive study as already\nexperienced in short-term Re-ID setting. In this work, we contribute a large,\nrealistic long-term person identification benchmark. It consists of 178K\nbounding boxes from 1.1K person identities, collected and constructed over 12\nmonths. Unique characteristics of this dataset include: (1) Natural/native\npersonal appearance (e.g., clothes and hair style) variations: The\nclothes-change and dressing styles all are highly diverse, with the reappearing\ngap in time ranging from minutes, hours, and days to weeks, months, seasons,\nand years. (2) Diverse walks of life: Persons across a wide range of ages and\nprofessions appear in different weather conditions (e.g., sunny, cloudy, windy,\nrainy, snowy, extremely cold) and events (e.g., working, leisure, daily\nactivities). (3) Rich camera setups: The raw videos were recorded by 17 outdoor\nsecurity cameras with various resolutions operating in a real-world\nsurveillance system for a wide and dense block. (4) Largest scale: It covers\nthe largest number of (17) cameras, (1, 121) identities, and (178, 407)\nbounding boxes, as compared to alternative datasets. Our dataset and benchmark\ncodes are available on https://github.com/PengBoXiangShang/deepchange.",
          "link": "http://arxiv.org/abs/2105.14685",
          "publishedOn": "2021-08-05T01:56:20.315Z",
          "wordCount": 702,
          "title": "DeepChange: A Long-Term Person Re-Identification Benchmark. (arXiv:2105.14685v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1\">Moritz Knolle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1\">Alexander Ziller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1\">Dmitrii Usynin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1\">Rickmer Braren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus R. Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>",
          "description": "We show that differentially private stochastic gradient descent (DP-SGD) can\nyield poorly calibrated, overconfident deep learning models. This represents a\nserious issue for safety-critical applications, e.g. in medical diagnosis. We\nhighlight and exploit parallels between stochastic gradient Langevin dynamics,\na scalable Bayesian inference technique for training deep neural networks, and\nDP-SGD, in order to train differentially private, Bayesian neural networks with\nminor adjustments to the original (DP-SGD) algorithm. Our approach provides\nconsiderably more reliable uncertainty estimates than DP-SGD, as demonstrated\nempirically by a reduction in expected calibration error (MNIST $\\sim{5}$-fold,\nPediatric Pneumonia Dataset $\\sim{2}$-fold).",
          "link": "http://arxiv.org/abs/2107.04296",
          "publishedOn": "2021-08-05T01:56:20.307Z",
          "wordCount": 593,
          "title": "Differentially private training of neural networks with Langevin dynamics for calibrated predictive uncertainty. (arXiv:2107.04296v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu-Huan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bo Ren</a>",
          "description": "Recent progress on salient object detection (SOD) mainly benefits from\nmulti-scale learning, where the high-level and low-level features collaborate\nin locating salient objects and discovering fine details, respectively.\nHowever, most efforts are devoted to low-level feature learning by fusing\nmulti-scale features or enhancing boundary representations. High-level\nfeatures, which although have long proven effective for many other tasks, yet\nhave been barely studied for SOD. In this paper, we tap into this gap and show\nthat enhancing high-level features is essential for SOD as well. To this end,\nwe introduce an Extremely-Downsampled Network (EDN), which employs an extreme\ndownsampling technique to effectively learn a global view of the whole image,\nleading to accurate salient object localization. To accomplish better\nmulti-level feature fusion, we construct the Scale-Correlated Pyramid\nConvolution (SCPC) to build an elegant decoder for recovering object details\nfrom the above extreme downsampling. Extensive experiments demonstrate that EDN\nachieves state-of-the-art performance with real-time speed. Our efficient\nEDN-Lite also achieves competitive performance with a speed of 316fps. Hence,\nthis work is expected to spark some new thinking in SOD. Full training and\ntesting code will be available at https://github.com/yuhuan-wu/EDN.",
          "link": "http://arxiv.org/abs/2012.13093",
          "publishedOn": "2021-08-05T01:56:20.299Z",
          "wordCount": 657,
          "title": "EDN: Salient Object Detection via Extremely-Downsampled Network. (arXiv:2012.13093v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02481",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Khakzar_A/0/1/0/all/0/1\">Ashkan Khakzar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Musatian_S/0/1/0/all/0/1\">Sabrina Musatian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buchberger_J/0/1/0/all/0/1\">Jonas Buchberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quiroz_I/0/1/0/all/0/1\">Icxel Valeriano Quiroz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pinger_N/0/1/0/all/0/1\">Nikolaus Pinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baselizadeh_S/0/1/0/all/0/1\">Soroosh Baselizadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Seong Tae Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>",
          "description": "Convolutional neural networks are showing promise in the automatic diagnosis\nof thoracic pathologies on chest x-rays. Their black-box nature has sparked\nmany recent works to explain the prediction via input feature attribution\nmethods (aka saliency methods). However, input feature attribution methods\nmerely identify the importance of input regions for the prediction and lack\nsemantic interpretation of model behavior. In this work, we first identify the\nsemantics associated with internal units (feature maps) of the network. We\nproceed to investigate the following questions; Does a regression model that is\nonly trained with COVID-19 severity scores implicitly learn visual patterns\nassociated with thoracic pathologies? Does a network that is trained on weakly\nlabeled data (e.g. healthy, unhealthy) implicitly learn pathologies? Moreover,\nwe investigate the effect of pretraining and data imbalance on the\ninterpretability of learned features. In addition to the analysis, we propose\nsemantic attribution to semantically explain each prediction. We present our\nfindings using publicly available chest pathologies (CheXpert, NIH ChestX-ray8)\nand COVID-19 datasets (BrixIA, and COVID-19 chest X-ray segmentation dataset).\nThe Code is publicly available.",
          "link": "http://arxiv.org/abs/2104.02481",
          "publishedOn": "2021-08-05T01:56:20.280Z",
          "wordCount": 719,
          "title": "Towards Semantic Interpretation of Thoracic Disease and COVID-19 Diagnosis Models. (arXiv:2104.02481v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tom_M/0/1/0/all/0/1\">Manu Tom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baltsavias_E/0/1/0/all/0/1\">Emmanuel Baltsavias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>",
          "description": "Depleting lake ice can serve as an indicator for climate change, just like\nsea level rise or glacial retreat. Several Lake Ice Phenological (LIP) events\nserve as sentinels to understand the regional and global climate change. Hence,\nit is useful to monitor long-term lake freezing and thawing patterns. In this\npaper we report a case study for the Oberengadin region of Switzerland, where\nthere are several small- and medium-sized mountain lakes. We observe the LIP\nevents, such as freeze-up, break-up and ice cover duration, across two decades\n(2000-2020) from optical satellite images. We analyse time-series of MODIS\nimagery by estimating spatially resolved maps of lake ice for these Alpine\nlakes with supervised machine learning (and additionally cross-check with VIIRS\ndata when available). To train the classifier we rely on reference data\nannotated manually based on webcam images. From the ice maps we derive\nlong-term LIP trends. Since the webcam data is only available for two winters,\nwe also validate our results against the operational MODIS and VIIRS snow\nproducts. We find a change in complete freeze duration of -0.76 and -0.89 days\nper annum for lakes Sils and Silvaplana, respectively. Furthermore, we observe\nplausible correlations of the LIP trends with climate data measured at nearby\nmeteorological stations. We notice that mean winter air temperature has\nnegative correlation with the freeze duration and break-up events, and positive\ncorrelation with the freeze-up events. Additionally, we observe strong negative\ncorrelation of sunshine during the winter months with the freeze duration and\nbreak-up events.",
          "link": "http://arxiv.org/abs/2103.12434",
          "publishedOn": "2021-08-05T01:56:20.273Z",
          "wordCount": 721,
          "title": "Recent Ice Trends in Swiss Mountain Lakes: 20-year Analysis of MODIS Imagery. (arXiv:2103.12434v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.06822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>",
          "description": "This paper focuses on high-transferable adversarial attacks on detectors,\nwhich are hard to attack in a black-box manner, because of their\nmultiple-output characteristics and the diversity across architectures. To\npursue a high attack transferability, one plausible way is to find a common\nproperty across detectors, which facilitates the discovery of common\nweaknesses. We are the first to suggest that the relevance map from\ninterpreters for detectors is such a property. Based on it, we design a\nRelevance Attack on Detectors (RAD), which achieves a state-of-the-art\ntransferability, exceeding existing results by above 20%. On MS COCO, the\ndetection mAPs for all 8 black-box architectures are more than halved and the\nsegmentation mAPs are also significantly influenced. Given the great\ntransferability of RAD, we generate the first adversarial dataset for object\ndetection and instance segmentation, i.e., Adversarial Objects in COntext\n(AOCO), which helps to quickly evaluate and improve the robustness of\ndetectors.",
          "link": "http://arxiv.org/abs/2008.06822",
          "publishedOn": "2021-08-05T01:56:20.267Z",
          "wordCount": 624,
          "title": "Relevance Attack on Detectors. (arXiv:2008.06822v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huabin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1\">John See</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuangrui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>",
          "description": "The crux of self-supervised video representation learning is to build general\nfeatures from unlabeled videos. However, most recent works have mainly focused\non high-level semantics and neglected lower-level representations and their\ntemporal relationship which are crucial for general video understanding. To\naddress these challenges, this paper proposes a multi-level feature\noptimization framework to improve the generalization and temporal modeling\nability of learned video representations. Concretely, high-level features\nobtained from naive and prototypical contrastive learning are utilized to build\ndistribution graphs, guiding the process of low-level and mid-level feature\nlearning. We also devise a simple temporal modeling module from multi-level\nfeatures to enhance motion pattern learning. Experiments demonstrate that\nmulti-level feature optimization with the graph constraint and temporal\nmodeling can greatly improve the representation ability in video understanding.",
          "link": "http://arxiv.org/abs/2108.02183",
          "publishedOn": "2021-08-05T01:56:20.260Z",
          "wordCount": 576,
          "title": "Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization. (arXiv:2108.02183v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinghao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Real world traffic sign recognition is an important step towards building\nautonomous vehicles, most of which highly dependent on Deep Neural Networks\n(DNNs). Recent studies demonstrated that DNNs are surprisingly susceptible to\nadversarial examples. Many attack methods have been proposed to understand and\ngenerate adversarial examples, such as gradient based attack, score based\nattack, decision based attack, and transfer based attacks. However, most of\nthese algorithms are ineffective in real-world road sign attack, because (1)\niteratively learning perturbations for each frame is not realistic for a fast\nmoving car and (2) most optimization algorithms traverse all pixels equally\nwithout considering their diverse contribution. To alleviate these problems,\nthis paper proposes the targeted attention attack (TAA) method for real world\nroad sign attack. Specifically, we have made the following contributions: (1)\nwe leverage the soft attention map to highlight those important pixels and skip\nthose zero-contributed areas - this also helps to generate natural\nperturbations, (2) we design an efficient universal attack that optimizes a\nsingle perturbation/noise based on a set of training images under the guidance\nof the pre-trained attention map, (3) we design a simple objective function\nthat can be easily optimized, (4) we evaluate the effectiveness of TAA on real\nworld data sets. Experimental results validate that the TAA method improves the\nattack successful rate (nearly 10%) and reduces the perturbation loss (about a\nquarter) compared with the popular RP2 method. Additionally, our TAA also\nprovides good properties, e.g., transferability and generalization capability.\nWe provide code and data to ensure the reproducibility:\nhttps://github.com/AdvAttack/RoadSignAttack.",
          "link": "http://arxiv.org/abs/2010.04331",
          "publishedOn": "2021-08-05T01:56:20.253Z",
          "wordCount": 745,
          "title": "Targeted Attention Attack on Deep Learning Models in Road Sign Recognition. (arXiv:2010.04331v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mertes_S/0/1/0/all/0/1\">Silvan Mertes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_T/0/1/0/all/0/1\">Tobias Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weitz_K/0/1/0/all/0/1\">Katharina Weitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heimerl_A/0/1/0/all/0/1\">Alexander Heimerl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1\">Elisabeth Andr&#xe9;</a>",
          "description": "With the ongoing rise of machine learning, the need for methods for\nexplaining decisions made by artificial intelligence systems is becoming a more\nand more important topic. Especially for image classification tasks, many\nstate-of-the-art tools to explain such classifiers rely on visual highlighting\nof important areas of the input data. Contrary, counterfactual explanation\nsystems try to enable a counterfactual reasoning by modifying the input image\nin a way such that the classifier would have made a different prediction. By\ndoing so, the users of counterfactual explanation systems are equipped with a\ncompletely different kind of explanatory information. However, methods for\ngenerating realistic counterfactual explanations for image classifiers are\nstill rare. Especially in medical contexts, where relevant information often\nconsists of textural and structural information, high-quality counterfactual\nimages have the potential to give meaningful insights into decision processes.\nIn this work, we present GANterfactual, an approach to generate such\ncounterfactual image explanations based on adversarial image-to-image\ntranslation techniques. Additionally, we conduct a user study to evaluate our\napproach in an exemplary medical use case. Our results show that, in the chosen\nmedical use-case, counterfactual explanations lead to significantly better\nresults regarding mental models, explanation satisfaction, trust, emotions, and\nself-efficacy than two state-of-the-art systems that work with saliency maps,\nnamely LIME and LRP.",
          "link": "http://arxiv.org/abs/2012.11905",
          "publishedOn": "2021-08-05T01:56:20.246Z",
          "wordCount": 695,
          "title": "GANterfactual -- Counterfactual Explanations for Medical Non-Experts using Generative Adversarial Learning. (arXiv:2012.11905v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02110",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_M/0/1/0/all/0/1\">Minyi Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">Shuigeng Zhou</a>",
          "description": "A number of deep learning based algorithms have been proposed to recover\nhigh-quality videos from low-quality compressed ones. Among them, some restore\nthe missing details of each frame via exploring the spatiotemporal information\nof neighboring frames. However, these methods usually suffer from a narrow\ntemporal scope, thus may miss some useful details from some frames outside the\nneighboring ones. In this paper, to boost artifact removal, on the one hand, we\npropose a Recursive Fusion (RF) module to model the temporal dependency within\na long temporal range. Specifically, RF utilizes both the current reference\nframes and the preceding hidden state to conduct better spatiotemporal\ncompensation. On the other hand, we design an efficient and effective\nDeformable Spatiotemporal Attention (DSTA) module such that the model can pay\nmore effort on restoring the artifact-rich areas like the boundary area of a\nmoving object. Extensive experiments show that our method outperforms the\nexisting ones on the MFQE 2.0 dataset in terms of both fidelity and perceptual\neffect. Code is available at https://github.com/zhaominyiz/RFDA-PyTorch.",
          "link": "http://arxiv.org/abs/2108.02110",
          "publishedOn": "2021-08-05T01:56:20.227Z",
          "wordCount": 624,
          "title": "Recursive Fusion and Deformable Spatiotemporal Attention for Video Compression Artifact Reduction. (arXiv:2108.02110v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01624",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_X/0/1/0/all/0/1\">Xuanye Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Feng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_X/0/1/0/all/0/1\">Xing Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1\">Jimmy Ren</a>",
          "description": "Deep neural networks have been widely used in image denoising during the past\nfew years. Even though they achieve great success on this problem, they are\ncomputationally inefficient which makes them inappropriate to be implemented in\nmobile devices. In this paper, we propose an efficient deep neural network for\nimage denoising based on pixel-wise classification. Despite using a\ncomputationally efficient network cannot effectively remove the noises from any\ncontent, it is still capable to denoise from a specific type of pattern or\ntexture. The proposed method follows such a divide and conquer scheme. We first\nuse an efficient U-net to pixel-wisely classify pixels in the noisy image based\non the local gradient statistics. Then we replace part of the convolution\nlayers in existing denoising networks by the proposed Class Specific\nConvolution layers (CSConv) which use different weights for different classes\nof pixels. Quantitative and qualitative evaluations on public datasets\ndemonstrate that the proposed method can reduce the computational costs without\nsacrificing the performance compared to state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2103.01624",
          "publishedOn": "2021-08-05T01:56:20.220Z",
          "wordCount": 640,
          "title": "Efficient Deep Image Denoising via Class Specific Convolution. (arXiv:2103.01624v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13418",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aozhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lipei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_Y/0/1/0/all/0/1\">Yaqi Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Baoqiang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zifeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhaohua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>",
          "description": "One of the challenges of the Optical Music Recognition task is to transcript\nthe symbols of the camera-captured images into digital music notations.\nPrevious end-to-end model which was developed as a Convolutional Recurrent\nNeural Network does not explore sufficient contextual information from full\nscales and there is still a large room for improvement. We propose an\ninnovative framework that combines a block of Residual Recurrent Convolutional\nNeural Network with a recurrent Encoder-Decoder network to map a sequence of\nmonophonic music symbols corresponding to the notations present in the image.\nThe Residual Recurrent Convolutional block can improve the ability of the model\nto enrich the context information. The experiment results are benchmarked\nagainst a publicly available dataset called CAMERA-PRIMUS, which demonstrates\nthat our approach surpass the state-of-the-art end-to-end method using\nConvolutional Recurrent Neural Network.",
          "link": "http://arxiv.org/abs/2010.13418",
          "publishedOn": "2021-08-05T01:56:20.213Z",
          "wordCount": 615,
          "title": "Residual Recurrent CRNN for End-to-End Optical Music Recognition on Monophonic Scores. (arXiv:2010.13418v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.13144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kassapis_E/0/1/0/all/0/1\">Elias Kassapis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dikov_G/0/1/0/all/0/1\">Georgi Dikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak K. Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nugteren_C/0/1/0/all/0/1\">Cedric Nugteren</a>",
          "description": "In semantic segmentation tasks, input images can often have more than one\nplausible interpretation, thus allowing for multiple valid labels. To capture\nsuch ambiguities, recent work has explored the use of probabilistic networks\nthat can learn a distribution over predictions. However, these do not\nnecessarily represent the empirical distribution accurately. In this work, we\npresent a strategy for learning a calibrated predictive distribution over\nsemantic maps, where the probability associated with each prediction reflects\nits ground truth correctness likelihood. To this end, we propose a novel\ntwo-stage, cascaded approach for calibrated adversarial refinement: (i) a\nstandard segmentation network is trained with categorical cross entropy to\npredict a pixelwise probability distribution over semantic classes and (ii) an\nadversarially trained stochastic network is used to model the inter-pixel\ncorrelations to refine the output of the first network into coherent samples.\nImportantly, to calibrate the refinement network and prevent mode collapse, the\nexpectation of the samples in the second stage is matched to the probabilities\npredicted in the first. We demonstrate the versatility and robustness of the\napproach by achieving state-of-the-art results on the multigrader LIDC dataset\nand on a modified Cityscapes dataset with injected ambiguities. In addition, we\nshow that the core design can be adapted to other tasks requiring learning a\ncalibrated predictive distribution by experimenting on a toy regression\ndataset. We provide an open source implementation of our method at\nhttps://github.com/EliasKassapis/CARSSS.",
          "link": "http://arxiv.org/abs/2006.13144",
          "publishedOn": "2021-08-05T01:56:20.203Z",
          "wordCount": 711,
          "title": "Calibrated Adversarial Refinement for Stochastic Semantic Segmentation. (arXiv:2006.13144v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01177",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shipitsin_V/0/1/0/all/0/1\">Viktor Shipitsin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bespalov_I/0/1/0/all/0/1\">Iaroslav Bespalov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dylov_D/0/1/0/all/0/1\">Dmitry V. Dylov</a>",
          "description": "We devise a universal adaptive neural layer to \"learn\" optimal frequency\nfilter for each image together with the weights of the base neural network that\nperforms some computer vision task. The proposed approach takes the source\nimage in the spatial domain, automatically selects the best frequencies from\nthe frequency domain, and transmits the inverse-transform image to the main\nneural network. Remarkably, such a simple add-on layer dramatically improves\nthe performance of the main network regardless of its design. We observe that\nthe light networks gain a noticeable boost in the performance metrics; whereas,\nthe training of the heavy ones converges faster when our adaptive layer is\nallowed to \"learn\" alongside the main architecture. We validate the idea in\nfour classical computer vision tasks: classification, segmentation, denoising,\nand erasing, considering popular natural and medical data benchmarks.",
          "link": "http://arxiv.org/abs/2010.01177",
          "publishedOn": "2021-08-05T01:56:20.195Z",
          "wordCount": 643,
          "title": "Global Adaptive Filtering Layer for Computer Vision. (arXiv:2010.01177v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valiuddin_M/0/1/0/all/0/1\">M.M.A. Valiuddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viviers_C/0/1/0/all/0/1\">C.G.A. Viviers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sloun_R/0/1/0/all/0/1\">R.J.G. van Sloun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+With_P/0/1/0/all/0/1\">P.H.N. de With</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sommen_F/0/1/0/all/0/1\">F. van der Sommen</a>",
          "description": "Quantifying uncertainty in medical image segmentation applications is\nessential, as it is often connected to vital decision-making. Compelling\nattempts have been made in quantifying the uncertainty in image segmentation\narchitectures, e.g. to learn a density segmentation model conditioned on the\ninput image. Typical work in this field restricts these learnt densities to be\nstrictly Gaussian. In this paper, we propose to use a more flexible approach by\nintroducing Normalizing Flows (NFs), which enables the learnt densities to be\nmore complex and facilitate more accurate modeling for uncertainty. We prove\nthis hypothesis by adopting the Probabilistic U-Net and augmenting the\nposterior density with an NF, allowing it to be more expressive. Our\nqualitative as well as quantitative (GED and IoU) evaluations on the\nmulti-annotated and single-annotated LIDC-IDRI and Kvasir-SEG segmentation\ndatasets, respectively, show a clear improvement. This is mostly apparent in\nthe quantification of aleatoric uncertainty and the increased predictive\nperformance of up to 14 percent. This result strongly indicates that a more\nflexible density model should be seriously considered in architectures that\nattempt to capture segmentation ambiguity through density modeling. The benefit\nof this improved modeling will increase human confidence in annotation and\nsegmentation, and enable eager adoption of the technology in practice.",
          "link": "http://arxiv.org/abs/2108.02155",
          "publishedOn": "2021-08-05T01:56:20.173Z",
          "wordCount": 664,
          "title": "Improving Aleatoric Uncertainty Quantification in Multi-Annotated Medical ImageSegmentation with Normalizing Flows. (arXiv:2108.02155v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2002.07088",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ryan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_E/0/1/0/all/0/1\">Earlence Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Atul Prakash</a>",
          "description": "The physical, black-box hard-label setting is arguably the most realistic\nthreat model for cyber-physical vision systems. In this setting, the attacker\nonly has query access to the model and only receives the top-1 class label\nwithout confidence information. Creating small physical stickers that are\nrobust to environmental variation is difficult in the discrete and\ndiscontinuous hard-label space because the attack must both design a small\nshape to perturb within and find robust noise to fill it with. Unfortunately,\nwe find that existing $\\ell_2$ or $\\ell_\\infty$ minimizing hard-label attacks\ndo not easily extend to finding such robust physical perturbation attacks.\nThus, we propose GRAPHITE, the first algorithm for hard-label physical attacks\non computer vision models. We show that \"survivability\", an estimate of\nphysical variation robustness, can be used in new ways to generate small masks\nand is a sufficiently smooth function to optimize with gradient-free\noptimization. We use GRAPHITE to attack a traffic sign classifier and a\npublicly-available Automatic License Plate Recognition (ALPR) tool using only\nquery access. We evaluate both tools in real-world field tests to measure its\nphysical-world robustness. We successfully cause a Stop sign to be\nmisclassified as a Speed Limit 30 km/hr sign in 95.7% of physical images and\ncause errors in 75% of physical images for the ALPR tool.",
          "link": "http://arxiv.org/abs/2002.07088",
          "publishedOn": "2021-08-05T01:56:20.164Z",
          "wordCount": 708,
          "title": "Robust Physical Hard-Label Attacks on Deep Learning Visual Classification. (arXiv:2002.07088v4 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.02189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yechan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Younkwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1\">Moongu Jeon</a>",
          "description": "Recently, deep learning models have achieved great success in computer vision\napplications, relying on large-scale class-balanced datasets. However,\nimbalanced class distributions still limit the wide applicability of these\nmodels due to degradation in performance. To solve this problem, in this paper,\nwe concentrate on the study of cross entropy which mostly ignores output scores\non incorrect classes. This work discovers that neutralizing predicted\nprobabilities on incorrect classes improves the prediction accuracy for\nimbalanced image classification. This paper proposes a simple but effective\nloss named complement cross entropy based on this finding. The proposed loss\nmakes the ground truth class overwhelm the other classes in terms of softmax\nprobability, by neutralizing probabilities of incorrect classes, without\nadditional training procedures. Along with it, this loss facilitates the models\nto learn key information especially from samples on minority classes. It\nensures more accurate and robust classification results on imbalanced\ndistributions. Extensive experiments on imbalanced datasets demonstrate the\neffectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2009.02189",
          "publishedOn": "2021-08-05T01:56:20.156Z",
          "wordCount": 646,
          "title": "Imbalanced Image Classification with Complement Cross Entropy. (arXiv:2009.02189v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Muhammad Zaigham Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1\">Arif Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Astrid_M/0/1/0/all/0/1\">Marcella Astrid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seung-Ik Lee</a>",
          "description": "Learning to detect real-world anomalous events through video-level labels is\na challenging task due to the rare occurrence of anomalies as well as noise in\nthe labels. In this work, we propose a weakly supervised anomaly detection\nmethod which has manifold contributions including1) a random batch based\ntraining procedure to reduce inter-batch correlation, 2) a normalcy suppression\nmechanism to minimize anomaly scores of the normal regions of a video by taking\ninto account the overall information available in one training batch, and 3) a\nclustering distance based loss to contribute towards mitigating the label noise\nand to produce better anomaly representations by encouraging our model to\ngenerate distinct normal and anomalous clusters. The proposed method\nobtains83.03% and 89.67% frame-level AUC performance on the UCF Crime and\nShanghaiTech datasets respectively, demonstrating its superiority over the\nexisting state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2011.12077",
          "publishedOn": "2021-08-05T01:56:20.148Z",
          "wordCount": 694,
          "title": "CLAWS: Clustering Assisted Weakly Supervised Learning with Normalcy Suppression for Anomalous Event Detection. (arXiv:2011.12077v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ryan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wu-chi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Atul Prakash</a>",
          "description": "To remove the effects of adversarial perturbations, preprocessing defenses\nsuch as pixel discretization are appealing due to their simplicity but have so\nfar been shown to be ineffective except on simple datasets such as MNIST,\nleading to the belief that pixel discretization approaches are doomed to\nfailure as a defense technique. This paper revisits the pixel discretization\napproaches. We hypothesize that the reason why existing approaches have failed\nis that they have used a fixed codebook for the entire dataset. In particular,\nwe find that can lead to situations where images become more susceptible to\nadversarial perturbations and also suffer significant loss of accuracy after\ndiscretization. We propose a novel image preprocessing technique called\nEssential Features that uses an adaptive codebook that is based on per-image\ncontent and threat model. Essential Features adaptively selects a separable set\nof color clusters for each image to reduce the color space while preserving the\npertinent features of the original image, maximizing both separability and\nrepresentation of colors. Additionally, to limit the adversary's ability to\ninfluence the chosen color clusters, Essential Features takes advantage of\nspatial correlation with an adaptive blur that moves pixels closer to their\noriginal value without destroying original edge information. We design several\nadaptive attacks and find that our approach is more robust than previous\nbaselines on $L_\\infty$ and $L_2$ bounded attacks for several challenging\ndatasets including CIFAR-10, GTSRB, RESISC45, and ImageNet.",
          "link": "http://arxiv.org/abs/2012.01699",
          "publishedOn": "2021-08-05T01:56:20.140Z",
          "wordCount": 712,
          "title": "Essential Features: Content-Adaptive Pixel Discretization to Improve Model Robustness to Adaptive Adversarial Attacks. (arXiv:2012.01699v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pegoraro_M/0/1/0/all/0/1\">Marco Pegoraro</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1\">Riccardo Marin</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Castellani_U/0/1/0/all/0/1\">Umberto Castellani</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Melzi_S/0/1/0/all/0/1\">Simone Melzi</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1\">Emanuele Rodol&#xe0;</a> (2) ((1) University of Verona, (2) Sapienza University of Rome)",
          "description": "In this work, we present a new learning-based pipeline for the generation of\n3D shapes. We build our method on top of recent advances on the so called\nshape-from-spectrum paradigm, which aims at recovering the full 3D geometric\nstructure of an object only from the eigenvalues of its Laplacian operator. In\ndesigning our learning strategy, we consider the spectrum as a natural and\nready to use representation to encode variability of the shapes. Therefore, we\npropose a simple decoder-only architecture that directly maps spectra to 3D\nembeddings; in particular, we combine information from global and local\nspectra, the latter being obtained from localized variants of the manifold\nLaplacian. This combination captures the relations between the full shape and\nits local parts, leading to more accurate generation of geometric details and\nan improved semantic control in shape synthesis and novel editing applications.\nOur results confirm the improvement of the proposed approach in comparison to\nexisting and alternative methods.",
          "link": "http://arxiv.org/abs/2108.02161",
          "publishedOn": "2021-08-05T01:56:20.119Z",
          "wordCount": 607,
          "title": "Learning to generate shape from global-local spectra. (arXiv:2108.02161v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02104",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fayao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1\">Chuan-Sheng Foo</a>",
          "description": "Recently deep learning has achieved significant progress on point cloud\nanalysis tasks. Learning good representations is of vital importance to these\ntasks. Most current methods rely on massive labelled data for training. We here\npropose a point discriminative learning method for unsupervised representation\nlearning on 3D point clouds, which can learn local and global geometry\nfeatures. We achieve this by imposing a novel point discrimination loss on the\nmiddle level and global level point features produced in the backbone network.\nThis point discrimination loss enforces the features to be consistent with\npoints belonging to the shape surface and inconsistent with randomly sampled\nnoisy points. Our method is simple in design, which works by adding an extra\nadaptation module and a point consistency module for unsupervised training of\nthe encoder in the backbone network. Once trained, these two modules can be\ndiscarded during supervised training of the classifier or decoder for\ndown-stream tasks. We conduct extensive experiments on 3D object\nclassification, 3D part segmentation and shape reconstruction in various\nunsupervised and transfer settings. Both quantitative and qualitative results\nshow that our method learns powerful representations and achieves new\nstate-of-the-art performance.",
          "link": "http://arxiv.org/abs/2108.02104",
          "publishedOn": "2021-08-05T01:56:20.111Z",
          "wordCount": 629,
          "title": "Point Discriminative Learning for Unsupervised Representation Learning on 3D Point Clouds. (arXiv:2108.02104v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1908.09073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Danfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>",
          "description": "A complex visual navigation task puts an agent in different situations which\ncall for a diverse range of visual perception abilities. For example, to \"go to\nthe nearest chair\", the agent might need to identify a chair in a living room\nusing semantics, follow along a hallway using vanishing point cues, and avoid\nobstacles using depth. Therefore, utilizing the appropriate visual perception\nabilities based on a situational understanding of the visual environment can\nempower these navigation models in unseen visual environments. We propose to\ntrain an agent to fuse a large set of visual representations that correspond to\ndiverse visual perception abilities. To fully utilize each representation, we\ndevelop an action-level representation fusion scheme, which predicts an action\ncandidate from each representation and adaptively consolidate these action\ncandidates into the final action. Furthermore, we employ a data-driven\ninter-task affinity regularization to reduce redundancies and improve\ngeneralization. Our approach leads to a significantly improved performance in\nnovel environments over ImageNet-pretrained baseline and other fusion methods.",
          "link": "http://arxiv.org/abs/1908.09073",
          "publishedOn": "2021-08-05T01:56:20.104Z",
          "wordCount": 638,
          "title": "Situational Fusion of Visual Representation for Visual Navigation. (arXiv:1908.09073v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Braude_T/0/1/0/all/0/1\">Tom Braude</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander Schwing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1\">Ariel Shamir</a>",
          "description": "We address the problem of visual storytelling, i.e., generating a story for a\ngiven sequence of images. While each sentence of the story should describe a\ncorresponding image, a coherent story also needs to be consistent and relate to\nboth future and past images. To achieve this we develop ordered image attention\n(OIA). OIA models interactions between the sentence-corresponding image and\nimportant regions in other images of the sequence. To highlight the important\nobjects, a message-passing-like algorithm collects representations of those\nobjects in an order-aware manner. To generate the story's sentences, we then\nhighlight important image attention vectors with an Image-Sentence Attention\n(ISA). Further, to alleviate common linguistic mistakes like repetitiveness, we\nintroduce an adaptive prior. The obtained results improve the METEOR score on\nthe VIST dataset by 1%. In addition, an extensive human study verifies\ncoherency improvements and shows that OIA and ISA generated stories are more\nfocused, shareable, and image-grounded.",
          "link": "http://arxiv.org/abs/2108.02180",
          "publishedOn": "2021-08-05T01:56:20.097Z",
          "wordCount": 597,
          "title": "Towards Coherent Visual Storytelling with Ordered Image Attention. (arXiv:2108.02180v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_A/0/1/0/all/0/1\">Ahmed Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Refai_A/0/1/0/all/0/1\">Ayman El-Refai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Sara Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aboul_Ela_M/0/1/0/all/0/1\">Mariam Aboul-Ela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eraqi_H/0/1/0/all/0/1\">Hesham M. Eraqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moustafa_M/0/1/0/all/0/1\">Mohamed Moustafa</a>",
          "description": "Due to the mass advancement in ubiquitous technologies nowadays, new\npervasive methods have come into the practice to provide new innovative\nfeatures and stimulate the research on new human-computer interactions. This\npaper presents a hand gesture recognition method that utilizes the smartphone's\nbuilt-in speakers and microphones. The proposed system emits an ultrasonic\nsonar-based signal (inaudible sound) from the smartphone's stereo speakers,\nwhich is then received by the smartphone's microphone and processed via a\nConvolutional Neural Network (CNN) for Hand Gesture Recognition. Data\naugmentation techniques are proposed to improve the detection accuracy and\nthree dual-channel input fusion methods are compared. The first method merges\nthe dual-channel audio as a single input spectrogram image. The second method\nadopts early fusion by concatenating the dual-channel spectrograms. The third\nmethod adopts late fusion by having two convectional input branches processing\neach of the dual-channel spectrograms and then the outputs are merged by the\nlast layers. Our experimental results demonstrate a promising detection\naccuracy for the six gestures presented in our publicly available dataset with\nan accuracy of 93.58\\% as a baseline.",
          "link": "http://arxiv.org/abs/2108.02148",
          "publishedOn": "2021-08-05T01:56:20.088Z",
          "wordCount": 640,
          "title": "Pervasive Hand Gesture Recognition for Smartphones using Non-audible Sound and Deep Learning. (arXiv:2108.02148v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Fangzhou Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Hao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>",
          "description": "Despite recent breakthroughs in deep learning methods for image lighting\nenhancement, they are inferior when applied to portraits because 3D facial\ninformation is ignored in their models. To address this, we present a novel\ndeep learning framework for portrait lighting enhancement based on 3D facial\nguidance. Our framework consists of two stages. In the first stage, corrected\nlighting parameters are predicted by a network from the input bad lighting\nimage, with the assistance of a 3D morphable model and a differentiable\nrenderer. Given the predicted lighting parameter, the differentiable renderer\nrenders a face image with corrected shading and texture, which serves as the 3D\nguidance for learning image lighting enhancement in the second stage. To better\nexploit the long-range correlations between the input and the guidance, in the\nsecond stage, we design an image-to-image translation network with a novel\ntransformer architecture, which automatically produces a lighting-enhanced\nresult. Experimental results on the FFHQ dataset and in-the-wild images show\nthat the proposed method outperforms state-of-the-art methods in terms of both\nquantitative metrics and visual quality. We will publish our dataset along with\nmore results on https://cassiepython.github.io/egsr/index.html.",
          "link": "http://arxiv.org/abs/2108.02121",
          "publishedOn": "2021-08-05T01:56:20.080Z",
          "wordCount": 646,
          "title": "Deep Portrait Lighting Enhancement with 3D Guidance. (arXiv:2108.02121v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02158",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wei_K/0/1/0/all/0/1\">Kaixuan Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Ying Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinqiang Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jiaolong Yang</a>",
          "description": "Enhancing the visibility in extreme low-light environments is a challenging\ntask. Under nearly lightless condition, existing image denoising methods could\neasily break down due to significantly low SNR. In this paper, we\nsystematically study the noise statistics in the imaging pipeline of CMOS\nphotosensors, and formulate a comprehensive noise model that can accurately\ncharacterize the real noise structures. Our novel model considers the noise\nsources caused by digital camera electronics which are largely overlooked by\nexisting methods yet have significant influence on raw measurement in the dark.\nIt provides a way to decouple the intricate noise structure into different\nstatistical distributions with physical interpretations. Moreover, our noise\nmodel can be used to synthesize realistic training data for learning-based\nlow-light denoising algorithms. In this regard, although promising results have\nbeen shown recently with deep convolutional neural networks, the success\nheavily depends on abundant noisy clean image pairs for training, which are\ntremendously difficult to obtain in practice. Generalizing their trained models\nto images from new devices is also problematic. Extensive experiments on\nmultiple low-light denoising datasets -- including a newly collected one in\nthis work covering various devices -- show that a deep neural network trained\nwith our proposed noise formation model can reach surprisingly-high accuracy.\nThe results are on par with or sometimes even outperform training with paired\nreal data, opening a new door to real-world extreme low-light photography.",
          "link": "http://arxiv.org/abs/2108.02158",
          "publishedOn": "2021-08-05T01:56:20.061Z",
          "wordCount": 699,
          "title": "Physics-based Noise Modeling for Extreme Low-light Photography. (arXiv:2108.02158v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yap_B/0/1/0/all/0/1\">Boon Peng Yap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_B/0/1/0/all/0/1\">Beng Koon Ng</a>",
          "description": "We explore the value of weak labels in learning transferable representations\nfor medical images. Compared to hand-labeled datasets, weak or inexact labels\ncan be acquired in large quantities at significantly lower cost and can provide\nuseful training signals for data-hungry models such as deep neural networks. We\nconsider weak labels in the form of pseudo-labels and propose a semi-weakly\nsupervised contrastive learning (SWCL) framework for representation learning\nusing semi-weakly annotated images. Specifically, we train a semi-supervised\nmodel to propagate labels from a small dataset consisting of diverse\nimage-level annotations to a large unlabeled dataset. Using the propagated\nlabels, we generate a patch-level dataset for pretraining and formulate a\nmulti-label contrastive learning objective to capture position-specific\nfeatures encoded in each patch. We empirically validate the transfer learning\nperformance of SWCL on seven public retinal fundus datasets, covering three\ndisease classification tasks and two anatomical structure segmentation tasks.\nOur experiment results suggest that, under very low data regime, large-scale\nImageNet pretraining on improved architecture remains a very strong baseline,\nand recently proposed self-supervised methods falter in segmentation tasks,\npossibly due to the strong invariant constraint imposed. Our method surpasses\nall prior self-supervised methods and standard cross-entropy training, while\nclosing the gaps with ImageNet pretraining.",
          "link": "http://arxiv.org/abs/2108.02122",
          "publishedOn": "2021-08-05T01:56:20.054Z",
          "wordCount": 643,
          "title": "Semi-weakly Supervised Contrastive Representation Learning for Retinal Fundus Images. (arXiv:2108.02122v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02016",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Joshi_A/0/1/0/all/0/1\">Anirudh Joshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eyuboglu_S/0/1/0/all/0/1\">Sabri Eyuboglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_S/0/1/0/all/0/1\">Shih-Cheng Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dunnmon_J/0/1/0/all/0/1\">Jared Dunnmon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soin_A/0/1/0/all/0/1\">Arjun Soin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Davidzon_G/0/1/0/all/0/1\">Guido Davidzon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chaudhari_A/0/1/0/all/0/1\">Akshay Chaudhari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P Lungren</a>",
          "description": "FDG PET/CT imaging is a resource intensive examination critical for managing\nmalignant disease and is particularly important for longitudinal assessment\nduring therapy. Approaches to automate longtudinal analysis present many\nchallenges including lack of available longitudinal datasets, managing complex\nlarge multimodal imaging examinations, and need for detailed annotations for\ntraditional supervised machine learning. In this work we develop OncoNet, novel\nmachine learning algorithm that assesses treatment response from a 1,954 pairs\nof sequential FDG PET/CT exams through weak supervision using the standard\nuptake values (SUVmax) in associated radiology reports. OncoNet demonstrates an\nAUROC of 0.86 and 0.84 on internal and external institution test sets\nrespectively for determination of change between scans while also showing\nstrong agreement to clinical scoring systems with a kappa score of 0.8. We also\ncurated a dataset of 1,954 paired FDG PET/CT exams designed for response\nassessment for the broader machine learning in healthcare research community.\nAutomated assessment of radiographic response from FDG PET/CT with OncoNet\ncould provide clinicians with a valuable tool to rapidly and consistently\ninterpret change over time in longitudinal multi-modal imaging exams.",
          "link": "http://arxiv.org/abs/2108.02016",
          "publishedOn": "2021-08-05T01:56:20.040Z",
          "wordCount": 649,
          "title": "OncoNet: Weakly Supervised Siamese Network to automate cancer treatment response assessment between longitudinal FDG PET/CT examinations. (arXiv:2108.02016v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hori_C/0/1/0/all/0/1\">Chiori Hori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hori_T/0/1/0/all/0/1\">Takaaki Hori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1\">Jonathan Le Roux</a>",
          "description": "Video captioning is an essential technology to understand scenes and describe\nevents in natural language. To apply it to real-time monitoring, a system needs\nnot only to describe events accurately but also to produce the captions as soon\nas possible. Low-latency captioning is needed to realize such functionality,\nbut this research area for online video captioning has not been pursued yet.\nThis paper proposes a novel approach to optimize each caption's output timing\nbased on a trade-off between latency and caption quality. An audio-visual\nTrans-former is trained to generate ground-truth captions using only a small\nportion of all video frames, and to mimic outputs of a pre-trained Transformer\nto which all the frames are given. A CNN-based timing detector is also trained\nto detect a proper output timing, where the captions generated by the two\nTrans-formers become sufficiently close to each other. With the jointly trained\nTransformer and timing detector, a caption can be generated in the early stages\nof an event-triggered video clip, as soon as an event happens or when it can be\nforecasted. Experiments with the ActivityNet Captions dataset show that our\napproach achieves 94% of the caption quality of the upper bound given by the\npre-trained Transformer using the entire video clips, using only 28% of frames\nfrom the beginning.",
          "link": "http://arxiv.org/abs/2108.02147",
          "publishedOn": "2021-08-05T01:56:20.031Z",
          "wordCount": 659,
          "title": "Optimizing Latency for Online Video CaptioningUsing Audio-Visual Transformers. (arXiv:2108.02147v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>",
          "description": "For an image with multiple scene texts, different people may be interested in\ndifferent text information. Current text-aware image captioning models are not\nable to generate distinctive captions according to various information needs.\nTo explore how to generate personalized text-aware captions, we define a new\nchallenging task, namely Question-controlled Text-aware Image Captioning\n(Qc-TextCap). With questions as control signals, this task requires models to\nunderstand questions, find related scene texts and describe them together with\nobjects fluently in human language. Based on two existing text-aware captioning\ndatasets, we automatically construct two datasets, ControlTextCaps and\nControlVizWiz to support the task. We propose a novel Geometry and Question\nAware Model (GQAM). GQAM first applies a Geometry-informed Visual Encoder to\nfuse region-level object features and region-level scene text features with\nconsidering spatial relationships. Then, we design a Question-guided Encoder to\nselect the most relevant visual features for each question. Finally, GQAM\ngenerates a personalized text-aware caption with a Multimodal Decoder. Our\nmodel achieves better captioning performance and question answering ability\nthan carefully designed baselines on both two datasets. With questions as\ncontrol signals, our model generates more informative and diverse captions than\nthe state-of-the-art text-aware captioning model. Our code and datasets are\npublicly available at https://github.com/HAWLYQ/Qc-TextCap.",
          "link": "http://arxiv.org/abs/2108.02059",
          "publishedOn": "2021-08-05T01:56:20.020Z",
          "wordCount": 644,
          "title": "Question-controlled Text-aware Image Captioning. (arXiv:2108.02059v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pene_C/0/1/0/all/0/1\">Cosmin Octavian Pene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghiassi_A/0/1/0/all/0/1\">Amirmasoud Ghiassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Younesian_T/0/1/0/all/0/1\">Taraneh Younesian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birke_R/0/1/0/all/0/1\">Robert Birke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lydia Y.Chen</a>",
          "description": "Multi-label learning is an emerging extension of the multi-class\nclassification where an image contains multiple labels. Not only acquiring a\nclean and fully labeled dataset in multi-label learning is extremely expensive,\nbut also many of the actual labels are corrupted or missing due to the\nautomated or non-expert annotation techniques. Noisy label data decrease the\nprediction performance drastically. In this paper, we propose a novel Gold\nAsymmetric Loss Correction with Single-Label Regulators (GALC-SLR) that\noperates robust against noisy labels. GALC-SLR estimates the noise confusion\nmatrix using single-label samples, then constructs an asymmetric loss\ncorrection via estimated confusion matrix to avoid overfitting to the noisy\nlabels. Empirical results show that our method outperforms the state-of-the-art\noriginal asymmetric loss multi-label classifier under all corruption levels,\nshowing mean average precision improvement up to 28.67% on a real world dataset\nof MS-COCO, yielding a better generalization of the unseen data and increased\nprediction performance.",
          "link": "http://arxiv.org/abs/2108.02032",
          "publishedOn": "2021-08-05T01:56:20.002Z",
          "wordCount": 596,
          "title": "Multi-Label Gold Asymmetric Loss Correction with Single-Label Regulators. (arXiv:2108.02032v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02160",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sikka_A/0/1/0/all/0/1\">Apoorva Sikka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Skand/0/1/0/all/0/1\">Skand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Virk_J/0/1/0/all/0/1\">Jitender Singh Virk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bathula_D/0/1/0/all/0/1\">Deepti R. Bathula</a>",
          "description": "Medical imaging datasets are inherently high dimensional with large\nvariability and low sample sizes that limit the effectiveness of deep learning\nalgorithms. Recently, generative adversarial networks (GANs) with the ability\nto synthesize realist images have shown great potential as an alternative to\nstandard data augmentation techniques. Our work focuses on cross-modality\nsynthesis of fluorodeoxyglucose~(FDG) Positron Emission Tomography~(PET) scans\nfrom structural Magnetic Resonance~(MR) images using generative models to\nfacilitate multi-modal diagnosis of Alzheimer's disease (AD). Specifically, we\npropose a novel end-to-end, globally and locally aware image-to-image\ntranslation GAN (GLA-GAN) with a multi-path architecture that enforces both\nglobal structural integrity and fidelity to local details. We further\nsupplement the standard adversarial loss with voxel-level intensity,\nmulti-scale structural similarity (MS-SSIM) and region-of-interest (ROI) based\nloss components that reduce reconstruction error, enforce structural\nconsistency at different scales and perceive variation in regional sensitivity\nto AD respectively. Experimental results demonstrate that our GLA-GAN not only\ngenerates synthesized FDG-PET scans with enhanced image quality but also\nsuperior clinical utility in improving AD diagnosis compared to\nstate-of-the-art models. Finally, we attempt to interpret some of the internal\nunits of the GAN that are closely related to this specific cross-modality\ngeneration task.",
          "link": "http://arxiv.org/abs/2108.02160",
          "publishedOn": "2021-08-05T01:56:19.995Z",
          "wordCount": 661,
          "title": "MRI to PET Cross-Modality Translation using Globally and Locally Aware GAN (GLA-GAN) for Multi-Modal Diagnosis of Alzheimer's Disease. (arXiv:2108.02160v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jingwen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Ying Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhigeng Pan</a>",
          "description": "Existing state-of-the-art human pose estimation methods require heavy\ncomputational resources for accurate predictions. One promising technique to\nobtain an accurate yet lightweight pose estimator is knowledge distillation,\nwhich distills the pose knowledge from a powerful teacher model to a\nless-parameterized student model. However, existing pose distillation works\nrely on a heavy pre-trained estimator to perform knowledge transfer and require\na complex two-stage learning procedure. In this work, we investigate a novel\nOnline Knowledge Distillation framework by distilling Human Pose structure\nknowledge in a one-stage manner to guarantee the distillation efficiency,\ntermed OKDHP. Specifically, OKDHP trains a single multi-branch network and\nacquires the predicted heatmaps from each, which are then assembled by a\nFeature Aggregation Unit (FAU) as the target heatmaps to teach each branch in\nreverse. Instead of simply averaging the heatmaps, FAU which consists of\nmultiple parallel transformations with different receptive fields, leverages\nthe multi-scale information, thus obtains target heatmaps with higher-quality.\nSpecifically, the pixel-wise Kullback-Leibler (KL) divergence is utilized to\nminimize the discrepancy between the target heatmaps and the predicted ones,\nwhich enables the student network to learn the implicit keypoint relationship.\nBesides, an unbalanced OKDHP scheme is introduced to customize the student\nnetworks with different compression rates. The effectiveness of our approach is\ndemonstrated by extensive experiments on two common benchmark datasets, MPII\nand COCO.",
          "link": "http://arxiv.org/abs/2108.02092",
          "publishedOn": "2021-08-05T01:56:19.988Z",
          "wordCount": 657,
          "title": "Online Knowledge Distillation for Efficient Pose Estimation. (arXiv:2108.02092v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingdong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesh_P/0/1/0/all/0/1\">Prakhar Ganesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>",
          "description": "We unveil a long-standing problem in the prevailing co-saliency detection\nsystems: there is indeed inconsistency between training and testing.\nConstructing a high-quality co-saliency detection dataset involves\ntime-consuming and labor-intensive pixel-level labeling, which has forced most\nrecent works to rely instead on semantic segmentation or saliency detection\ndatasets for training. However, the lack of proper co-saliency and the absence\nof multiple foreground objects in these datasets can lead to spurious\nvariations and inherent biases learned by models. To tackle this, we introduce\nthe idea of counterfactual training through context adjustment, and propose a\n\"cost-free\" group-cut-paste (GCP) procedure to leverage images from\noff-the-shelf saliency detection datasets and synthesize new samples. Following\nGCP, we collect a novel dataset called Context Adjustment Training. The two\nvariants of our dataset, i.e., CAT and CAT+, consist of 16,750 and 33,500\nimages, respectively. All images are automatically annotated with high-quality\nmasks. As a side-product, object categories, as well as edge information, are\nalso provided to facilitate other related works. Extensive experiments with\nstate-of-the-art models are conducted to demonstrate the superiority of our\ndataset. We hope that the scale, diversity, and quality of CAT/CAT+ can benefit\nresearchers in this area and beyond. The dataset and benchmark toolkit will be\naccessible through our project page.",
          "link": "http://arxiv.org/abs/2108.02093",
          "publishedOn": "2021-08-05T01:56:19.968Z",
          "wordCount": 643,
          "title": "Free Lunch for Co-Saliency Detection: Context Adjustment. (arXiv:2108.02093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02002",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ewen_N/0/1/0/all/0/1\">Nicolas Ewen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_N/0/1/0/all/0/1\">Naimul Khan</a>",
          "description": "Neural networks often require large amounts of expert annotated data to\ntrain. When changes are made in the process of medical imaging, trained\nnetworks may not perform as well, and obtaining large amounts of expert\nannotations for each change in the imaging process can be time consuming and\nexpensive. Online unsupervised learning is a method that has been proposed to\ndeal with situations where there is a domain shift in incoming data, and a lack\nof annotations. The aim of this study is to see whether online unsupervised\nlearning can help COVID-19 CT scan classification models adjust to slight\ndomain shifts, when there are no annotations available for the new data. A\ntotal of six experiments are performed using three test datasets with differing\namounts of domain shift. These experiments compare the performance of the\nonline unsupervised learning strategy to a baseline, as well as comparing how\nthe strategy performs on different domain shifts. Code for online unsupervised\nlearning can be found at this link:\nhttps://github.com/Mewtwo/online-unsupervised-learning",
          "link": "http://arxiv.org/abs/2108.02002",
          "publishedOn": "2021-08-05T01:56:19.956Z",
          "wordCount": 664,
          "title": "Online unsupervised Learning for domain shift in COVID-19 CT scan datasets. (arXiv:2108.02002v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01998",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Du_Z/0/1/0/all/0/1\">Zhekai Du</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_K/0/1/0/all/0/1\">Ke Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>",
          "description": "Energy disaggregation, also known as non-intrusive load monitoring (NILM),\nchallenges the problem of separating the whole-home electricity usage into\nappliance-specific individual consumptions, which is a typical application of\ndata analysis. {NILM aims to help households understand how the energy is used\nand consequently tell them how to effectively manage the energy, thus allowing\nenergy efficiency which is considered as one of the twin pillars of sustainable\nenergy policy (i.e., energy efficiency and renewable energy).} Although NILM is\nunidentifiable, it is widely believed that the NILM problem can be addressed by\ndata science. Most of the existing approaches address the energy disaggregation\nproblem by conventional techniques such as sparse coding, non-negative matrix\nfactorization, and hidden Markov model. Recent advances reveal that deep neural\nnetworks (DNNs) can get favorable performance for NILM since DNNs can\ninherently learn the discriminative signatures of the different appliances. In\nthis paper, we propose a novel method named adversarial energy disaggregation\n(AED) based on DNNs. We introduce the idea of adversarial learning into NILM,\nwhich is new for the energy disaggregation task. Our method trains a generator\nand multiple discriminators via an adversarial fashion. The proposed method not\nonly learns shard representations for different appliances, but captures the\nspecific multimode structures of each appliance. Extensive experiments on\nreal-world datasets verify that our method can achieve new state-of-the-art\nperformance.",
          "link": "http://arxiv.org/abs/2108.01998",
          "publishedOn": "2021-08-05T01:56:19.938Z",
          "wordCount": 686,
          "title": "Adversarial Energy Disaggregation for Non-intrusive Load Monitoring. (arXiv:2108.01998v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qiang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chixiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaoqiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feng Zhou</a>",
          "description": "Achieving backward compatibility when rolling out new models can highly\nreduce costs or even bypass feature re-encoding of existing gallery images for\nin-production visual retrieval systems. Previous related works usually leverage\nlosses used in knowledge distillation which can cause performance degradations\nor not guarantee compatibility. To address these issues, we propose a general\nframework called Learning Compatible Embeddings (LCE) which is applicable for\nboth cross model compatibility and compatible training in\ndirect/forward/backward manners. Our compatibility is achieved by aligning\nclass centers between models directly or via a transformation, and restricting\nmore compact intra-class distributions for the new model. Experiments are\nconducted in extensive scenarios such as changes of training dataset, loss\nfunctions, network architectures as well as feature dimensions, and demonstrate\nthat LCE efficiently enables model compatibility with marginal sacrifices of\naccuracies. The code will be available at https://github.com/IrvingMeng/LCE.",
          "link": "http://arxiv.org/abs/2108.01958",
          "publishedOn": "2021-08-05T01:56:19.928Z",
          "wordCount": 582,
          "title": "Learning Compatible Embeddings. (arXiv:2108.01958v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anamika Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satish Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Krishna Pratap Singh</a>",
          "description": "Signature verification has been one of the major researched areas in the\nfield of computer vision. Many financial and legal organizations use signature\nverification as access control and authentication. Signature images are not\nrich in texture; however, they have much vital geometrical information. Through\nthis work, we have proposed a signature verification methodology that is simple\nyet effective. The technique presented in this paper harnesses the geometrical\nfeatures of a signature image like center, isolated points, connected\ncomponents, etc., and with the power of Artificial Neural Network (ANN)\nclassifier, classifies the signature image based on their geometrical features.\nPublicly available dataset MCYT, BHSig260 (contains the image of two regional\nlanguages Bengali and Hindi) has been used in this paper to test the\neffectiveness of the proposed method. We have received a lower Equal Error Rate\n(EER) on MCYT 100 dataset and higher accuracy on the BHSig260 dataset.",
          "link": "http://arxiv.org/abs/2108.02029",
          "publishedOn": "2021-08-05T01:56:19.918Z",
          "wordCount": 621,
          "title": "Signature Verification using Geometrical Features and Artificial Neural Network Classifier. (arXiv:2108.02029v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>",
          "description": "Most current image captioning systems focus on describing general image\ncontent, and lack background knowledge to deeply understand the image, such as\nexact named entities or concrete events. In this work, we focus on the\nentity-aware news image captioning task which aims to generate informative\ncaptions by leveraging the associated news articles to provide background\nknowledge about the target image. However, due to the length of news articles,\nprevious works only employ news articles at the coarse article or sentence\nlevel, which are not fine-grained enough to refine relevant events and choose\nnamed entities accurately. To overcome these limitations, we propose an\nInformation Concentrated Entity-aware news image CAPtioning (ICECAP) model,\nwhich progressively concentrates on relevant textual information within the\ncorresponding news article from the sentence level to the word level. Our model\nfirst creates coarse concentration on relevant sentences using a cross-modality\nretrieval model and then generates captions by further concentrating on\nrelevant words within the sentences. Extensive experiments on both BreakingNews\nand GoodNews datasets demonstrate the effectiveness of our proposed method,\nwhich outperforms other state-of-the-arts. The code of ICECAP is publicly\navailable at https://github.com/HAWLYQ/ICECAP.",
          "link": "http://arxiv.org/abs/2108.02050",
          "publishedOn": "2021-08-05T01:56:19.912Z",
          "wordCount": 630,
          "title": "ICECAP: Information Concentrated Entity-aware Image Captioning. (arXiv:2108.02050v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xingjiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tianlong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Liang He</a>",
          "description": "Document layout analysis (DLA) aims to divide a document image into different\ntypes of regions. DLA plays an important role in the document content\nunderstanding and information extraction systems. Exploring a method that can\nuse less data for effective training contributes to the development of DLA. We\nconsider a Human-in-the-loop (HITL) collaborative intelligence in the DLA. Our\napproach was inspired by the fact that the HITL push the model to learn from\nthe unknown problems by adding a small amount of data based on knowledge. The\nHITL select key samples by using confidence. However, using confidence to find\nkey samples is not suitable for DLA tasks. We propose the Key Samples Selection\n(KSS) method to find key samples in high-level tasks (semantic segmentation)\nmore accurately through agent collaboration, effectively reducing costs. Once\nselected, these key samples are passed to human beings for active labeling,\nthen the model will be updated with the labeled samples. Hence, we revisited\nthe learning system from reinforcement learning and designed a sample-based\nagent update strategy, which effectively improves the agent's ability to accept\nnew samples. It achieves significant improvement results in two benchmarks\n(DSSE-200 (from 77.1% to 86.3%) and CS-150 (from 88.0% to 95.6%)) by using 10%\nof labeled data.",
          "link": "http://arxiv.org/abs/2108.02095",
          "publishedOn": "2021-08-05T01:56:19.905Z",
          "wordCount": 634,
          "title": "Human-In-The-Loop Document Layout Analysis. (arXiv:2108.02095v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Siyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Er_M/0/1/0/all/0/1\">Meng Hwa Er</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1\">Alex C. Kot</a>",
          "description": "Skeleton-based human action recognition has attracted increasing attention in\nrecent years. However, most of the existing works focus on supervised learning\nwhich requiring a large number of annotated action sequences that are often\nexpensive to collect. We investigate unsupervised representation learning for\nskeleton action recognition, and design a novel skeleton cloud colorization\ntechnique that is capable of learning skeleton representations from unlabeled\nskeleton sequence data. Specifically, we represent a skeleton action sequence\nas a 3D skeleton cloud and colorize each point in the cloud according to its\ntemporal and spatial orders in the original (unannotated) skeleton sequence.\nLeveraging the colorized skeleton point cloud, we design an auto-encoder\nframework that can learn spatial-temporal features from the artificial color\nlabels of skeleton joints effectively. We evaluate our skeleton cloud\ncolorization approach with action classifiers trained under different\nconfigurations, including unsupervised, semi-supervised and fully-supervised\nsettings. Extensive experiments on NTU RGB+D and NW-UCLA datasets show that the\nproposed method outperforms existing unsupervised and semi-supervised 3D action\nrecognition methods by large margins, and it achieves competitive performance\nin supervised 3D action recognition as well.",
          "link": "http://arxiv.org/abs/2108.01959",
          "publishedOn": "2021-08-05T01:56:19.885Z",
          "wordCount": 627,
          "title": "Skeleton Cloud Colorization for Unsupervised 3D Action Representation Learning. (arXiv:2108.01959v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01997",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1\">Chengtao Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Long_Y/0/1/0/all/0/1\">Yunfei Long</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_S/0/1/0/all/0/1\">Senhua Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tu_D/0/1/0/all/0/1\">Dandan Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>",
          "description": "Early detection of the coronavirus disease 2019 (COVID-19) helps to treat\npatients timely and increase the cure rate, thus further suppressing the spread\nof the disease. In this study, we propose a novel deep learning based detection\nand similar case recommendation network to help control the epidemic. Our\nproposed network contains two stages: the first one is a lung region\nsegmentation step and is used to exclude irrelevant factors, and the second is\na detection and recommendation stage. Under this framework, in the second\nstage, we develop a dual-children network (DuCN) based on a pre-trained\nResNet-18 to simultaneously realize the disease diagnosis and similar case\nrecommendation. Besides, we employ triplet loss and intrapulmonary distance\nmaps to assist the detection, which helps incorporate tiny differences between\ntwo images and is conducive to improving the diagnostic accuracy. For each\nconfirmed COVID-19 case, we give similar cases to provide radiologists with\ndiagnosis and treatment references. We conduct experiments on a large publicly\navailable dataset (CC-CCII) and compare the proposed model with\nstate-of-the-art COVID-19 detection methods. The results show that our proposed\nmodel achieves a promising clinical performance.",
          "link": "http://arxiv.org/abs/2108.01997",
          "publishedOn": "2021-08-05T01:56:19.860Z",
          "wordCount": 688,
          "title": "DuCN: Dual-children Network for Medical Diagnosis and Similar Case Recommendation towards COVID-19. (arXiv:2108.01997v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiaojiao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guizhong Liu</a>",
          "description": "Self-supervised learning of depth and ego-motion from unlabeled monocular\nvideo has acquired promising results and drawn extensive attention. Most\nexisting methods jointly train the depth and pose networks by photometric\nconsistency of adjacent frames based on the principle of structure-from-motion\n(SFM). However, the coupling relationship of the depth and pose networks\nseriously influences the learning performance, and the re-projection relations\nis sensitive to scale ambiguity, especially for pose learning. In this paper,\nwe aim to improve the depth-pose learning performance without the auxiliary\ntasks and address the above issues by alternative training each task and\nincorporating the epipolar geometric constraints into the Iterative Closest\nPoint (ICP) based point clouds match process. Distinct from jointly training\nthe depth and pose networks, our key idea is to better utilize the mutual\ndependency of these two tasks by alternatively training each network with\nrespective losses while fixing the other. We also design a log-scale 3D\nstructural consistency loss to put more emphasis on the smaller depth values\nduring training. To makes the optimization easier, we further incorporate the\nepipolar geometry into the ICP based learning process for pose learning.\nExtensive experiments on various benchmarks datasets indicate the superiority\nof our algorithm over the state-of-the-art self-supervised methods.",
          "link": "http://arxiv.org/abs/2108.01980",
          "publishedOn": "2021-08-05T01:56:19.853Z",
          "wordCount": 664,
          "title": "Self-Supervised Learning of Depth and Ego-Motion from Video by Alternative Training and Geometric Constraints from 3D to 2D. (arXiv:2108.01980v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_R/0/1/0/all/0/1\">Runmin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qinwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>",
          "description": "The popularity and promotion of depth maps have brought new vigor and\nvitality into salient object detection (SOD), and a mass of RGB-D SOD\nalgorithms have been proposed, mainly concentrating on how to better integrate\ncross-modality features from RGB image and depth map. For the cross-modality\ninteraction in feature encoder, existing methods either indiscriminately treat\nRGB and depth modalities, or only habitually utilize depth cues as auxiliary\ninformation of the RGB branch. Different from them, we reconsider the status of\ntwo modalities and propose a novel Cross-modality Discrepant Interaction\nNetwork (CDINet) for RGB-D SOD, which differentially models the dependence of\ntwo modalities according to the feature representations of different layers. To\nthis end, two components are designed to implement the effective cross-modality\ninteraction: 1) the RGB-induced Detail Enhancement (RDE) module leverages RGB\nmodality to enhance the details of the depth features in low-level encoder\nstage. 2) the Depth-induced Semantic Enhancement (DSE) module transfers the\nobject positioning and internal consistency of depth features to the RGB branch\nin high-level encoder stage. Furthermore, we also design a Dense Decoding\nReconstruction (DDR) structure, which constructs a semantic block by combining\nmulti-level encoder features to upgrade the skip connection in the feature\ndecoding. Extensive experiments on five benchmark datasets demonstrate that our\nnetwork outperforms $15$ state-of-the-art methods both quantitatively and\nqualitatively. Our code is publicly available at:\nhttps://rmcong.github.io/proj_CDINet.html.",
          "link": "http://arxiv.org/abs/2108.01971",
          "publishedOn": "2021-08-05T01:56:19.846Z",
          "wordCount": 679,
          "title": "Cross-modality Discrepant Interaction Network for RGB-D Salient Object Detection. (arXiv:2108.01971v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01975",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Guang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhiping Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1\">En Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianping Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chengzhang Zhu</a>",
          "description": "Video anomaly detection (VAD) has constantly been a vital topic in video\nanalysis. As anomalies are often rare, it is typically addressed under a\nsemi-supervised setup, which requires a training set with pure normal videos.\nTo avoid exhausted manual labeling, we are inspired by how humans sense\nanomalies and propose a hominine framework that enables both unsupervised and\nend-to-end VAD. The framework is based on two key observations: 1) Human\nperception is usually local, i.e. focusing on local foreground and its context\nwhen sensing anomalies. Thus, we propose to impose locality-awareness by\nlocalizing foreground with generic knowledge, and a region localization\nstrategy is designed to exploit local context. 2) Frequently-occurred events\nwill mould humans' definition of normality, which motivates us to devise a\nsurrogate training paradigm. It trains a deep neural network (DNN) to learn a\nsurrogate task with unlabeled videos, and frequently-occurred events will play\na dominant role in \"moulding\" the DNN. In this way, a training loss gap will\nautomatically manifest rarely-seen novel events as anomalies. For\nimplementation, we explore various surrogate tasks as well as both classic and\nemerging DNN models. Extensive evaluations on commonly-used VAD benchmarks\njustify the framework's applicability to different surrogate tasks or DNN\nmodels, and demonstrate its astonishing effectiveness: It not only outperforms\nexisting unsupervised solutions by a wide margin (8% to 10% AUROC gain), but\nalso achieves comparable or even superior performance to state-of-the-art\nsemi-supervised counterparts.",
          "link": "http://arxiv.org/abs/2108.01975",
          "publishedOn": "2021-08-05T01:56:19.839Z",
          "wordCount": 693,
          "title": "Sensing Anomalies like Humans: A Hominine Framework to Detect Abnormal Events from Unlabeled Videos. (arXiv:2108.01975v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Suofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zirui Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiofu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Quan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1\">Bin Kang</a>",
          "description": "High performance person Re-Identification (Re-ID) requires the model to focus\non both global silhouette and local details of pedestrian. To extract such more\nrepresentative features, an effective way is to exploit deep models with\nmultiple branches. However, most multi-branch based methods implemented by\nduplication of part backbone structure normally lead to severe increase of\ncomputational cost. In this paper, we propose a lightweight Feature Pyramid\nBranch (FPB) to extract features from different layers of networks and\naggregate them in a bidirectional pyramid structure. Cooperated by attention\nmodules and our proposed cross orthogonality regularization, FPB significantly\nprompts the performance of backbone network by only introducing less than 1.5M\nextra parameters. Extensive experimental results on standard benchmark datasets\ndemonstrate that our proposed FPB based model outperforms state-of-the-art\nmethods with obvious margin as well as much less model complexity. FPB borrows\nthe idea of the Feature Pyramid Network (FPN) from prevailing object detection\nmethods. To our best knowledge, it is the first successful application of\nsimilar structure in person Re-ID tasks, which empirically proves that pyramid\nnetwork as affiliated branch could be a potential structure in related feature\nembedding models. The source code is publicly available at\nhttps://github.com/anocodetest1/FPB.git.",
          "link": "http://arxiv.org/abs/2108.01901",
          "publishedOn": "2021-08-05T01:56:19.829Z",
          "wordCount": 635,
          "title": "FPB: Feature Pyramid Branch for Person Re-Identification. (arXiv:2108.01901v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Quach_B/0/1/0/all/0/1\">Boi M. Quach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuong_D/0/1/0/all/0/1\">Dinh V. Cuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Nhung Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_D/0/1/0/all/0/1\">Dang Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh T. Nguyen</a>",
          "description": "There is a warning light for the loss of plant habitats worldwide that\nentails concerted efforts to conserve plant biodiversity. Thus, plant species\nclassification is of crucial importance to address this environmental\nchallenge. In recent years, there is a considerable increase in the number of\nstudies related to plant taxonomy. While some researchers try to improve their\nrecognition performance using novel approaches, others concentrate on\ncomputational optimization for their framework. In addition, a few studies are\ndiving into feature extraction to gain significantly in terms of accuracy. In\nthis paper, we propose an effective method for the leaf recognition problem. In\nour proposed approach, a leaf goes through some pre-processing to extract its\nrefined color image, vein image, xy-projection histogram, handcrafted shape,\ntexture features, and Fourier descriptors. These attributes are then\ntransformed into a better representation by neural network-based encoders\nbefore a support vector machine (SVM) model is utilized to classify different\nleaves. Overall, our approach performs a state-of-the-art result on the Flavia\nleaf dataset, achieving the accuracy of 99.58\\% on test sets under random\n10-fold cross-validation and bypassing the previous methods. We also release\nour codes\\footnote{Scripts are available at\n\\url{https://github.com/dinhvietcuong1996/LeafRecognition}} for contributing to\nthe research community in the leaf classification problem.",
          "link": "http://arxiv.org/abs/2108.01808",
          "publishedOn": "2021-08-05T01:56:19.807Z",
          "wordCount": 664,
          "title": "An Effective Leaf Recognition Using Convolutional Neural Networks Based Features. (arXiv:2108.01808v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>",
          "description": "State-of-the-art deep neural networks (DNNs) have been proved to have\nexcellent performance on unsupervised domain adaption (UDA). However, recent\nwork shows that DNNs perform poorly when being attacked by adversarial samples,\nwhere these attacks are implemented by simply adding small disturbances to the\noriginal images. Although plenty of work has focused on this, as far as we\nknow, there is no systematic research on the robustness of unsupervised domain\nadaption model. Hence, we discuss the robustness of unsupervised domain\nadaption against adversarial attacking for the first time. We benchmark various\nsettings of adversarial attack and defense in domain adaption, and propose a\ncross domain attack method based on pseudo label. Most importantly, we analyze\nthe impact of different datasets, models, attack methods and defense methods.\nDirectly, our work proves the limited robustness of unsupervised domain\nadaptation model, and we hope our work may facilitate the community to pay more\nattention to improve the robustness of the model against attacking.",
          "link": "http://arxiv.org/abs/2108.01807",
          "publishedOn": "2021-08-05T01:56:19.783Z",
          "wordCount": 599,
          "title": "On the Robustness of Domain Adaption to Adversarial Attacks. (arXiv:2108.01807v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01941",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Valverde_J/0/1/0/all/0/1\">Juan Miguel Valverde</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shatillo_A/0/1/0/all/0/1\">Artem Shatillo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feo_R/0/1/0/all/0/1\">Riccardo de Feo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tohka_J/0/1/0/all/0/1\">Jussi Tohka</a>",
          "description": "We present MedicDeepLabv3+, a convolutional neural network that is the first\ncompletely automatic method to segment brain hemispheres in magnetic resonance\n(MR) images of rodents with lesions. MedicDeepLabv3+ improves the\nstate-of-the-art DeepLabv3+ with an advanced decoder, incorporating spatial\nattention layers and additional skip connections that, as we show in our\nexperiments, lead to more precise segmentations. MedicDeepLabv3+ requires no MR\nimage preprocessing, such as bias-field correction or registration to a\ntemplate, produces segmentations in less than a second, and its GPU memory\nrequirements can be adjusted based on the available resources. Using a large\ndataset of 723 MR rat brain images, we evaluated our MedicDeepLabv3+, two\nstate-of-the-art convolutional neural networks (DeepLabv3+, UNet) and three\napproaches that were specifically designed for skull-stripping rodent MR images\n(Demon, RATS and RBET). In our experiments, MedicDeepLabv3+ outperformed the\nother methods, yielding an average Dice coefficient of 0.952 and 0.944 in the\nbrain and contralateral hemisphere regions. Additionally, we show that despite\nlimiting the GPU memory and the training data to only three images, our\nMedicDeepLabv3+ also provided satisfactory segmentations. In conclusion, our\nmethod, publicly available at https://github.com/jmlipman/MedicDeepLabv3Plus,\nyielded excellent results in multiple scenarios, demonstrating its capability\nto reduce human workload in rodent neuroimaging studies.",
          "link": "http://arxiv.org/abs/2108.01941",
          "publishedOn": "2021-08-05T01:56:19.773Z",
          "wordCount": 649,
          "title": "Automatic hemisphere segmentation in rodent MRI with lesions. (arXiv:2108.01941v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eliasof_M/0/1/0/all/0/1\">Moshe Eliasof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haber_E/0/1/0/all/0/1\">Eldad Haber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treister_E/0/1/0/all/0/1\">Eran Treister</a>",
          "description": "Graph neural networks are increasingly becoming the go-to approach in various\nfields such as computer vision, computational biology and chemistry, where data\nare naturally explained by graphs. However, unlike traditional convolutional\nneural networks, deep graph networks do not necessarily yield better\nperformance than shallow graph networks. This behavior usually stems from the\nover-smoothing phenomenon. In this work, we propose a family of architectures\nto control this behavior by design. Our networks are motivated by numerical\nmethods for solving Partial Differential Equations (PDEs) on manifolds, and as\nsuch, their behavior can be explained by similar analysis. Moreover, as we\ndemonstrate using an extensive set of experiments, our PDE-motivated networks\ncan generalize and be effective for various types of problems from different\nfields. Our architectures obtain better or on par with the current\nstate-of-the-art results for problems that are typically approached using\ndifferent architectures.",
          "link": "http://arxiv.org/abs/2108.01938",
          "publishedOn": "2021-08-05T01:56:19.753Z",
          "wordCount": 591,
          "title": "PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations. (arXiv:2108.01938v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01817",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangteng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yiliang Lv</a>",
          "description": "Existing video copy detection methods generally measure video similarity\nbased on spatial similarities between key frames, neglecting the latent\nsimilarity in temporal dimension, so that the video similarity is biased\ntowards spatial information. There are methods modeling unified video\nsimilarity in an end-to-end way, but losing detailed partial alignment\ninformation, which causes the incapability of copy segments localization. To\naddress the above issues, we propose the Video Similarity and Alignment\nLearning (VSAL) approach, which jointly models spatial similarity, temporal\nsimilarity and partial alignment. To mitigate the spatial similarity bias, we\nmodel the temporal similarity as the mask map predicted from frame-level\nspatial similarity, where each element indicates the probability of frame pair\nlying right on the partial alignments. To further localize partial copies, the\nstep map is learned from the spatial similarity where the elements indicate\nextending directions of the current partial alignments on the spatial-temporal\nsimilarity map. Obtained from the mask map, the start points extend out into\npartial optimal alignments following instructions of the step map. With the\nsimilarity and alignment learning strategy, VSAL achieves the state-of-the-art\nF1-score on VCDB core dataset. Furthermore, we construct a new benchmark of\npartial video copy detection and localization by adding new segment-level\nannotations for FIVR-200k dataset, where VSAL also achieves the best\nperformance, verifying its effectiveness in more challenging situations. Our\nproject is publicly available at https://pvcd-vsal.github.io/vsal/.",
          "link": "http://arxiv.org/abs/2108.01817",
          "publishedOn": "2021-08-05T01:56:19.744Z",
          "wordCount": 677,
          "title": "Video Similarity and Alignment Learning on Partial Video Copy Detection. (arXiv:2108.01817v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1\">Cong Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Deming Chen</a>",
          "description": "Most existing neural architecture search (NAS) algorithms are dedicated to\nthe downstream tasks, e.g., image classification in computer vision. However,\nextensive experiments have shown that, prominent neural architectures, such as\nResNet in computer vision and LSTM in natural language processing, are\ngenerally good at extracting patterns from the input data and perform well on\ndifferent downstream tasks. These observations inspire us to ask: Is it\nnecessary to use the performance of specific downstream tasks to evaluate and\nsearch for good neural architectures? Can we perform NAS effectively and\nefficiently while being agnostic to the downstream task? In this work, we\nattempt to affirmatively answer the above two questions and improve the\nstate-of-the-art NAS solution by proposing a novel and generic NAS framework,\ntermed Generic NAS (GenNAS). GenNAS does not use task-specific labels but\ninstead adopts \\textit{regression} on a set of manually designed synthetic\nsignal bases for architecture evaluation. Such a self-supervised regression\ntask can effectively evaluate the intrinsic power of an architecture to capture\nand transform the input signal patterns, and allow more sufficient usage of\ntraining samples. We then propose an automatic task search to optimize the\ncombination of synthetic signals using limited downstream-task-specific labels,\nfurther improving the performance of GenNAS. We also thoroughly evaluate\nGenNAS's generality and end-to-end NAS performance on all search spaces, which\noutperforms almost all existing works with significant speedup.",
          "link": "http://arxiv.org/abs/2108.01899",
          "publishedOn": "2021-08-05T01:56:19.736Z",
          "wordCount": 669,
          "title": "Generic Neural Architecture Search via Regression. (arXiv:2108.01899v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_X/0/1/0/all/0/1\">Xue-Cheng Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glowinski_R/0/1/0/all/0/1\">Roland Glowinski</a>",
          "description": "Gaussian curvature is an important geometric property of surfaces, which has\nbeen used broadly in mathematical modeling. Due to the full nonlinearity of the\nGaussian curvature, efficient numerical methods for models based on it are\nuncommon in literature. In this article, we propose an operator-splitting\nmethod for a general Gaussian curvature model. In our method, we decouple the\nfull nonlinearity of Gaussian curvature from differential operators by\nintroducing two matrix- and vector-valued functions. The optimization problem\nis then converted into the search for the steady state solution of a time\ndependent PDE system. The above PDE system is well-suited to time\ndiscretization by operator splitting, the sub-problems encountered at each\nfractional step having either a closed form solution or being solvable by\nefficient algorithms. The proposed method is not sensitive to the choice of\nparameters, its efficiency and performances being demonstrated via systematic\nexperiments on surface smoothing and image denoising.",
          "link": "http://arxiv.org/abs/2108.01914",
          "publishedOn": "2021-08-05T01:56:19.720Z",
          "wordCount": 604,
          "title": "An Operator-Splitting Method for the Gaussian Curvature Regularization Model with Applications in Surface Smoothing and Imaging. (arXiv:2108.01914v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_H/0/1/0/all/0/1\">Hao Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tengfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "We propose a novel framework for video inpainting by adopting an internal\nlearning strategy. Unlike previous methods that use optical flow for\ncross-frame context propagation to inpaint unknown regions, we show that this\ncan be achieved implicitly by fitting a convolutional neural network to the\nknown region. Moreover, to handle challenging sequences with ambiguous\nbackgrounds or long-term occlusion, we design two regularization terms to\npreserve high-frequency details and long-term temporal consistency. Extensive\nexperiments on the DAVIS dataset demonstrate that the proposed method achieves\nstate-of-the-art inpainting quality quantitatively and qualitatively. We\nfurther extend the proposed method to another challenging task: learning to\nremove an object from a video giving a single object mask in only one frame in\na 4K video.",
          "link": "http://arxiv.org/abs/2108.01912",
          "publishedOn": "2021-08-05T01:56:19.713Z",
          "wordCount": 555,
          "title": "Internal Video Inpainting by Implicit Long-range Propagation. (arXiv:2108.01912v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_M/0/1/0/all/0/1\">Miki Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiota_S/0/1/0/all/0/1\">Sayaka Shiota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>",
          "description": "We propose a novel universal detector for detecting images generated by using\nCNNs. In this paper, properties of checkerboard artifacts in CNN-generated\nimages are considered, and the spectrum of images is enhanced in accordance\nwith the properties. Next, a classifier is trained by using the enhanced\nspectrums to judge a query image to be a CNN-generated ones or not. In\naddition, an ensemble of the proposed detector with emphasized spectrums and a\nconventional detector is proposed to improve the performance of these methods.\nIn an experiment, the proposed ensemble is demonstrated to outperform a\nstate-of-the-art method under some conditions.",
          "link": "http://arxiv.org/abs/2108.01892",
          "publishedOn": "2021-08-05T01:56:19.705Z",
          "wordCount": 561,
          "title": "A universal detector of CNN-generated images using properties of checkerboard artifacts in the frequency domain. (arXiv:2108.01892v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yurui Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yubo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Thomas H. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>",
          "description": "Pose-guided person image synthesis aims to synthesize person images by\ntransforming reference images into target poses. In this paper, we observe that\nthe commonly used spatial transformation blocks have complementary advantages.\nWe propose a novel model by combining the attention operation with the\nflow-based operation. Our model not only takes the advantage of the attention\noperation to generate accurate target structures but also uses the flow-based\noperation to sample realistic source textures. Both objective and subjective\nexperiments demonstrate the superiority of our model. Meanwhile, comprehensive\nablation studies verify our hypotheses and show the efficacy of the proposed\nmodules. Besides, additional experiments on the portrait image editing task\ndemonstrate the versatility of the proposed combination.",
          "link": "http://arxiv.org/abs/2108.01823",
          "publishedOn": "2021-08-05T01:56:19.678Z",
          "wordCount": 552,
          "title": "Combining Attention with Flow for Person Image Synthesis. (arXiv:2108.01823v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1\">Zaid Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>",
          "description": "Multimodal target/aspect sentiment classification combines multimodal\nsentiment analysis and aspect/target sentiment classification. The goal of the\ntask is to combine vision and language to understand the sentiment towards a\ntarget entity in a sentence. Twitter is an ideal setting for the task because\nit is inherently multimodal, highly emotional, and affects real world events.\nHowever, multimodal tweets are short and accompanied by complex, possibly\nirrelevant images. We introduce a two-stream model that translates images in\ninput space using an object-aware transformer followed by a single-pass\nnon-autoregressive text generation approach. We then leverage the translation\nto construct an auxiliary sentence that provides multimodal information to a\nlanguage model. Our approach increases the amount of text available to the\nlanguage model and distills the object-level information in complex images. We\nachieve state-of-the-art performance on two multimodal Twitter datasets without\nmodifying the internals of the language model to accept multimodal data,\ndemonstrating the effectiveness of our translation. In addition, we explain a\nfailure mode of a popular approach for aspect sentiment analysis when applied\nto tweets. Our code is available at\n\\textcolor{blue}{\\url{https://github.com/codezakh/exploiting-BERT-thru-translation}}.",
          "link": "http://arxiv.org/abs/2108.01682",
          "publishedOn": "2021-08-05T01:56:19.671Z",
          "wordCount": 623,
          "title": "Exploiting BERT For Multimodal Target SentimentClassification Through Input Space Translation. (arXiv:2108.01682v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01821",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ju_L/0/1/0/all/0/1\">Lie Ju</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_K/0/1/0/all/0/1\">Kaimin Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_Q/0/1/0/all/0/1\">Qingyi Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>",
          "description": "Retinal vessel segmentation plays a key role in computer-aided screening,\ndiagnosis, and treatment of various cardiovascular and ophthalmic diseases.\nRecently, deep learning-based retinal vessel segmentation algorithms have\nachieved remarkable performance. However, due to the domain shift problem, the\nperformance of these algorithms often degrades when they are applied to new\ndata that is different from the training data. Manually labeling new data for\neach test domain is often a time-consuming and laborious task. In this work, we\nexplore unsupervised domain adaptation in retinal vessel segmentation by using\nentropy-based adversarial learning and transfer normalization layer to train a\nsegmentation network, which generalizes well across domains and requires no\nannotation of the target domain. Specifically, first, an entropy-based\nadversarial learning strategy is developed to reduce the distribution\ndiscrepancy between the source and target domains while also achieving the\nobjective of entropy minimization on the target domain. In addition, a new\ntransfer normalization layer is proposed to further boost the transferability\nof the deep network. It normalizes the features of each domain separately to\ncompensate for the domain distribution gap. Besides, it also adaptively selects\nthose feature channels that are more transferable between domains, thus further\nenhancing the generalization performance of the network. We conducted extensive\nexperiments on three regular fundus image datasets and an ultra-widefield\nfundus image dataset, and the results show that our approach yields significant\nperformance gains compared to other state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.01821",
          "publishedOn": "2021-08-05T01:56:19.649Z",
          "wordCount": 694,
          "title": "Unsupervised Domain Adaptation for Retinal Vessel Segmentation with Adversarial Learning and Transfer Normalization. (arXiv:2108.01821v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Edirisooriya_S/0/1/0/all/0/1\">Sachinda Edirisooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao-Wen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>",
          "description": "Previous work has shown that neural architectures are able to perform optical\nmusic recognition (OMR) on monophonic and homophonic music with high accuracy.\nHowever, piano and orchestral scores frequently exhibit polyphonic passages,\nwhich add a second dimension to the task. Monophonic and homophonic music can\nbe described as homorhythmic, or having a single musical rhythm. Polyphonic\nmusic, on the other hand, can be seen as having multiple rhythmic sequences, or\nvoices, concurrently. We first introduce a workflow for creating large-scale\npolyphonic datasets suitable for end-to-end recognition from sheet music\npublicly available on the MuseScore forum. We then propose two novel\nformulations for end-to-end polyphonic OMR -- one treating the problem as a\ntype of multi-task binary classification, and the other treating it as\nmulti-sequence detection. Building upon the encoder-decoder architecture and an\nimage encoder proposed in past work on end-to-end OMR, we propose two novel\ndecoder models -- FlagDecoder and RNNDecoder -- that correspond to the two\nformulations. Finally, we compare the empirical performance of these end-to-end\napproaches to polyphonic OMR and observe a new state-of-the-art performance\nwith our multi-sequence detection decoder, RNNDecoder.",
          "link": "http://arxiv.org/abs/2108.01769",
          "publishedOn": "2021-08-05T01:56:19.634Z",
          "wordCount": 639,
          "title": "An Empirical Evaluation of End-to-End Polyphonic Optical Music Recognition. (arXiv:2108.01769v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_H/0/1/0/all/0/1\">Hong-Wing Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingshu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1\">Binh-Son Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1\">Sai-Kit Yeung</a>",
          "description": "Furnishing and rendering an indoor scene is a common but tedious task for\ninterior design: an artist needs to observe the space, create a conceptual\ndesign, build a 3D model, and perform rendering. In this paper, we introduce a\nnew problem of domain-specific image synthesis using generative modeling,\nnamely neural scene decoration. Given a photograph of an empty indoor space, we\naim to synthesize a new image of the same space that is fully furnished and\ndecorated. Neural scene decoration can be applied in practice to efficiently\ngenerate conceptual but realistic interior designs, bypassing the traditional\nmulti-step and time-consuming pipeline. Our attempt to neural scene decoration\nin this paper is a generative adversarial neural network that takes the input\nphotograph and directly produce the image of the desired furnishing and\ndecorations. Our network contains a novel image generator that transforms an\ninitial point-based object layout into a realistic photograph. We demonstrate\nthe performance of our proposed method by showing that it outperforms the\nbaselines built upon previous works on image translations both qualitatively\nand quantitatively. Our user study further validates the plausibility and\naesthetics in the generated designs.",
          "link": "http://arxiv.org/abs/2108.01806",
          "publishedOn": "2021-08-05T01:56:19.602Z",
          "wordCount": 640,
          "title": "Neural Scene Decoration from a Single Photograph. (arXiv:2108.01806v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1\">Matthias Zwicker</a>",
          "description": "Human pose information is a critical component in many downstream image\nprocessing tasks, such as activity recognition and motion tracking. Likewise, a\npose estimator for the illustrated character domain would provide a valuable\nprior for assistive content creation tasks, such as reference pose retrieval\nand automatic character animation. But while modern data-driven techniques have\nsubstantially improved pose estimation performance on natural images, little\nwork has been done for illustrations. In our work, we bridge this domain gap by\nefficiently transfer-learning from both domain-specific and task-specific\nsource models. Additionally, we upgrade and expand an existing illustrated pose\nestimation dataset, and introduce two new datasets for classification and\nsegmentation subtasks. We then apply the resultant state-of-the-art character\npose estimator to solve the novel task of pose-guided illustration retrieval.\nAll data, models, and code will be made publicly available.",
          "link": "http://arxiv.org/abs/2108.01819",
          "publishedOn": "2021-08-05T01:56:19.461Z",
          "wordCount": 571,
          "title": "Transfer Learning for Pose Estimation of Illustrated Characters. (arXiv:2108.01819v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_C/0/1/0/all/0/1\">Chi-Wei Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hwann-Tzong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>",
          "description": "We present a novel pyramidal output representation to ensure parsimony with\nour \"specialize and fuse\" process for semantic segmentation. A pyramidal\n\"output\" representation consists of coarse-to-fine levels, where each level is\n\"specialize\" in a different class distribution (e.g., more stuff than things\nclasses at coarser levels). Two types of pyramidal outputs (i.e., unity and\nsemantic pyramid) are \"fused\" into the final semantic output, where the unity\npyramid indicates unity-cells (i.e., all pixels in such cell share the same\nsemantic label). The process ensures parsimony by predicting a relatively small\nnumber of labels for unity-cells (e.g., a large cell of grass) to build the\nfinal semantic output. In addition to the \"output\" representation, we design a\ncoarse-to-fine contextual module to aggregate the \"features\" representation\nfrom different levels. We validate the effectiveness of each key module in our\nmethod through comprehensive ablation studies. Finally, our approach achieves\nstate-of-the-art performance on three widely-used semantic segmentation\ndatasets -- ADE20K, COCO-Stuff, and Pascal-Context.",
          "link": "http://arxiv.org/abs/2108.01866",
          "publishedOn": "2021-08-05T01:56:19.453Z",
          "wordCount": 596,
          "title": "Specialize and Fuse: Pyramidal Output Representation for Semantic Segmentation. (arXiv:2108.01866v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen-Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>",
          "description": "Modern deep learning models require large amounts of accurately annotated\ndata, which is often difficult to satisfy. Hence, weakly supervised tasks,\nincluding weakly supervised object localization~(WSOL) and detection~(WSOD),\nhave recently received attention in the computer vision community. In this\npaper, we motivate and propose the weakly supervised foreground learning (WSFL)\ntask by showing that both WSOL and WSOD can be greatly improved if groundtruth\nforeground masks are available. More importantly, we propose a complete WSFL\npipeline with low computational cost, which generates pseudo boxes, learns\nforeground masks, and does not need any localization annotations. With the help\nof foreground masks predicted by our WSFL model, we achieve 72.97% correct\nlocalization accuracy on CUB for WSOL, and 55.7% mean average precision on\nVOC07 for WSOD, thereby establish new state-of-the-art for both tasks. Our WSFL\nmodel also shows excellent transfer ability.",
          "link": "http://arxiv.org/abs/2108.01785",
          "publishedOn": "2021-08-05T01:56:19.446Z",
          "wordCount": 580,
          "title": "Weakly Supervised Foreground Learning for Weakly Supervised Localization and Detection. (arXiv:2108.01785v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiaoyu Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shuyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1\">Zhanghui Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Meng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wayne Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>",
          "description": "Transformers with powerful global relation modeling abilities have been\nintroduced to fundamental computer vision tasks recently. As a typical example,\nthe Vision Transformer (ViT) directly applies a pure transformer architecture\non image classification, by simply splitting images into tokens with a fixed\nlength, and employing transformers to learn relations between these tokens.\nHowever, such naive tokenization could destruct object structures, assign grids\nto uninterested regions such as background, and introduce interference signals.\nTo mitigate the above issues, in this paper, we propose an iterative and\nprogressive sampling strategy to locate discriminative regions. At each\niteration, embeddings of the current sampling step are fed into a transformer\nencoder layer, and a group of sampling offsets is predicted to update the\nsampling locations for the next step. The progressive sampling is\ndifferentiable. When combined with the Vision Transformer, the obtained PS-ViT\nnetwork can adaptively learn where to look. The proposed PS-ViT is both\neffective and efficient. When trained from scratch on ImageNet, PS-ViT performs\n3.8% higher than the vanilla ViT in terms of top-1 accuracy with about\n$4\\times$ fewer parameters and $10\\times$ fewer FLOPs. Code is available at\nhttps://github.com/yuexy/PS-ViT.",
          "link": "http://arxiv.org/abs/2108.01684",
          "publishedOn": "2021-08-05T01:56:19.439Z",
          "wordCount": 630,
          "title": "Vision Transformer with Progressive Sampling. (arXiv:2108.01684v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Costa_V/0/1/0/all/0/1\">Victor G. Turrisi da Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>",
          "description": "This paper presents solo-learn, a library of self-supervised methods for\nvisual representation learning. Implemented in Python, using Pytorch and\nPytorch lightning, the library fits both research and industry needs by\nfeaturing distributed training pipelines with mixed-precision, faster data\nloading via Nvidia DALI, online linear evaluation for better prototyping, and\nmany additional training tricks. Our goal is to provide an easy-to-use library\ncomprising a large amount of Self-supervised Learning (SSL) methods, that can\nbe easily extended and fine-tuned by the community. solo-learn opens up avenues\nfor exploiting large-budget SSL solutions on inexpensive smaller\ninfrastructures and seeks to democratize SSL by making it accessible to all.\nThe source code is available at https://github.com/vturrisi/solo-learn.",
          "link": "http://arxiv.org/abs/2108.01775",
          "publishedOn": "2021-08-05T01:56:19.430Z",
          "wordCount": 566,
          "title": "Solo-learn: A Library of Self-supervised Methods for Visual Representation Learning. (arXiv:2108.01775v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingchuan Meng</a>",
          "description": "Attention-based transformer networks have demonstrated promising potential as\ntheir applications extend from natural language processing to vision. However,\ndespite the recent improvements, such as sub-quadratic attention approximation\nand various training enhancements, the compact vision transformers to date\nusing the regular attention still fall short in comparison with its convnet\ncounterparts, in terms of \\textit{accuracy,} \\textit{model size}, \\textit{and}\n\\textit{throughput}. This paper introduces a compact self-attention mechanism\nthat is fundamental and highly generalizable. The proposed method reduces\nredundancy and improves efficiency on top of the existing attention\noptimizations. We show its drop-in applicability for both the regular attention\nmechanism and some most recent variants in vision transformers. As a result, we\nproduced smaller and faster models with the same or better accuracies.",
          "link": "http://arxiv.org/abs/2108.01778",
          "publishedOn": "2021-08-05T01:56:19.411Z",
          "wordCount": 547,
          "title": "Armour: Generalizable Compact Self-Attention for Vision Transformers. (arXiv:2108.01778v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1\">Ajil Jalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arvinte_M/0/1/0/all/0/1\">Marius Arvinte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daras_G/0/1/0/all/0/1\">Giannis Daras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1\">Eric Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamir_J/0/1/0/all/0/1\">Jonathan I. Tamir</a>",
          "description": "The CSGM framework (Bora-Jalal-Price-Dimakis'17) has shown that deep\ngenerative priors can be powerful tools for solving inverse problems. However,\nto date this framework has been empirically successful only on certain datasets\n(for example, human faces and MNIST digits), and it is known to perform poorly\non out-of-distribution samples. In this paper, we present the first successful\napplication of the CSGM framework on clinical MRI data. We train a generative\nprior on brain scans from the fastMRI dataset, and show that posterior sampling\nvia Langevin dynamics achieves high quality reconstructions. Furthermore, our\nexperiments and theory show that posterior sampling is robust to changes in the\nground-truth distribution and measurement process. Our code and models are\navailable at: \\url{https://github.com/utcsilab/csgm-mri-langevin}.",
          "link": "http://arxiv.org/abs/2108.01368",
          "publishedOn": "2021-08-04T01:59:24.158Z",
          "wordCount": 572,
          "title": "Robust Compressed Sensing MRI with Deep Generative Priors. (arXiv:2108.01368v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01495",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Linder_T/0/1/0/all/0/1\">Timm Linder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaskevicius_N/0/1/0/all/0/1\">Narunas Vaskevicius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schirmer_R/0/1/0/all/0/1\">Robert Schirmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arras_K/0/1/0/all/0/1\">Kai O. Arras</a>",
          "description": "Advances in sensing and learning algorithms have led to increasingly mature\nsolutions for human detection by robots, particularly in selected use-cases\nsuch as pedestrian detection for self-driving cars or close-range person\ndetection in consumer settings. Despite this progress, the simple question\n\"which sensor-algorithm combination is best suited for a person detection task\nat hand?\" remains hard to answer. In this paper, we tackle this issue by\nconducting a systematic cross-modal analysis of sensor-algorithm combinations\ntypically used in robotics. We compare the performance of state-of-the-art\nperson detectors for 2D range data, 3D lidar, and RGB-D data as well as\nselected combinations thereof in a challenging industrial use-case.\n\nWe further address the related problems of data scarcity in the industrial\ntarget domain, and that recent research on human detection in 3D point clouds\nhas mostly focused on autonomous driving scenarios. To leverage these\nmethodological advances for robotics applications, we utilize a simple, yet\neffective multi-sensor transfer learning strategy by extending a strong\nimage-based RGB-D detector to provide cross-modal supervision for lidar\ndetectors in the form of weak 3D bounding box labels.\n\nOur results show a large variance among the different approaches in terms of\ndetection performance, generalization, frame rates and computational\nrequirements. As our use-case contains difficulties representative for a wide\nrange of service robot applications, we believe that these results point to\nrelevant open challenges for further research and provide valuable support to\npractitioners for the design of their robot system.",
          "link": "http://arxiv.org/abs/2108.01495",
          "publishedOn": "2021-08-04T01:59:23.918Z",
          "wordCount": 697,
          "title": "Cross-Modal Analysis of Human Detection for Robotics: An Industrial Case Study. (arXiv:2108.01495v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yangtao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1\">Boyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>",
          "description": "This paper proposes a new deep learning approach to antipodal grasp\ndetection, named Double-Dot Network (DD-Net). It follows the recent anchor-free\nobject detection framework, which does not depend on empirically pre-set\nanchors and thus allows more generalized and flexible prediction on unseen\nobjects. Specifically, unlike the widely used 5-dimensional rectangle, the\ngripper configuration is defined as a pair of fingertips. An effective CNN\narchitecture is introduced to localize such fingertips, and with the help of\nauxiliary centers for refinement, it accurately and robustly infers grasp\ncandidates. Additionally, we design a specialized loss function to measure the\nquality of grasps, and in contrast to the IoU scores of bounding boxes adopted\nin object detection, it is more consistent to the grasp detection task. Both\nthe simulation and robotic experiments are executed and state of the art\naccuracies are achieved, showing that DD-Net is superior to the counterparts in\nhandling unseen objects.",
          "link": "http://arxiv.org/abs/2108.01527",
          "publishedOn": "2021-08-04T01:59:23.371Z",
          "wordCount": 592,
          "title": "Double-Dot Network for Antipodal Grasp Detection. (arXiv:2108.01527v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Existing methods for arbitrary-shaped text detection in natural scenes face\ntwo critical issues, i.e., 1) fracture detections at the gaps in a text\ninstance; and 2) inaccurate detections of arbitrary-shaped text instances with\ndiverse background context. To address these issues, we propose a novel method\nnamed Intra- and Inter-Instance Collaborative Learning (I3CL). Specifically, to\naddress the first issue, we design an effective convolutional module with\nmultiple receptive fields, which is able to collaboratively learn better\ncharacter and gap feature representations at local and long ranges inside a\ntext instance. To address the second issue, we devise an instance-based\ntransformer module to exploit the dependencies between different text instances\nand a pixel-based transformer module to exploit the global context from the\nshared background, which are able to collaboratively learn more discriminative\ntext feature representations. In this way, I3CL can effectively exploit the\nintra- and inter-instance dependencies together in a unified end-to-end\ntrainable framework. Experimental results show that the proposed I3CL sets new\nstate-of-the-art performances on three challenging public benchmarks, i.e., an\nF-measure of 76.4% on ICDAR2019-ArT, 86.2% on Total-Text, and 85.8% on\nCTW-1500. Besides, I3CL with ResNeSt-101 backbone ranked 1st place on the\nICDAR2019-ArT leaderboard. The source code will be made publicly available.",
          "link": "http://arxiv.org/abs/2108.01343",
          "publishedOn": "2021-08-04T01:59:23.341Z",
          "wordCount": 644,
          "title": "I3CL:Intra- and Inter-Instance Collaborative Learning for Arbitrary-shaped Scene Text Detection. (arXiv:2108.01343v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01625",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jung_W/0/1/0/all/0/1\">Wooseok Jung</a>",
          "description": "Cell image analysis is crucial in Alzheimer's research to detect the presence\nof A$\\beta$ protein inhibiting cell function. Deep learning speeds up the\nprocess by making only low-level data sufficient for fruitful inspection. We\nfirst found Unet is most suitable in augmented microscopy by comparing\nperformance in multi-class semantics segmentation. We develop the augmented\nmicroscopy method to capture nuclei in a brightfield image and the transformer\nusing Unet model to convert an input image into a sequence of topological\ninformation. The performance regarding Intersection-over-Union is consistent\nconcerning the choice of image preprocessing and ground-truth generation.\nTraining model with data of a specific cell type demonstrates transfer learning\napplies to some extent.\n\nThe topological transformer aims to extract persistence silhouettes or\nlandscape signatures containing geometric information of a given image of\ncells. This feature extraction facilitates studying an image as a collection of\none-dimensional data, substantially reducing computational costs. Using the\ntransformer, we attempt grouping cell images by their cell type relying solely\non topological features. Performances of the transformers followed by SVM,\nXGBoost, LGBM, and simple convolutional neural network classifiers are inferior\nto the conventional image classification. However, since this research\ninitiates a new perspective in biomedical research by combining deep learning\nand topology for image analysis, we speculate follow-up investigation will\nreinforce our genuine regime.",
          "link": "http://arxiv.org/abs/2108.01625",
          "publishedOn": "2021-08-04T01:59:22.866Z",
          "wordCount": 687,
          "title": "From augmented microscopy to the topological transformer: a new approach in cell image analysis for Alzheimer's research. (arXiv:2108.01625v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.16328",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Garcia_Uceda_A/0/1/0/all/0/1\">A. Garcia-Uceda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Selvan_R/0/1/0/all/0/1\">R. Selvan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saghir_Z/0/1/0/all/0/1\">Z. Saghir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tiddens_H/0/1/0/all/0/1\">H.A.W.M. Tiddens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bruijne_M/0/1/0/all/0/1\">M. de Bruijne</a>",
          "description": "This paper presents a fully automatic and end-to-end optimised airway\nsegmentation method for thoracic computed tomography, based on the U-Net\narchitecture. We use a simple and low-memory 3D U-Net as backbone, which allows\nthe method to process large 3D image patches, often comprising full lungs, in a\nsingle pass through the network. This makes the method simple, robust and\nefficient. We validated the proposed method on three datasets with very\ndifferent characteristics and various airway abnormalities: i) a dataset of\npediatric patients including subjects with cystic fibrosis, ii) a subset of the\nDanish Lung Cancer Screening Trial, including subjects with chronic obstructive\npulmonary disease, and iii) the EXACT'09 public dataset. We compared our method\nwith other state-of-the-art airway segmentation methods, including relevant\nlearning-based methods in the literature evaluated on the EXACT'09 data. We\nshow that our method can extract highly complete airway trees with few false\npositive errors, on scans from both healthy and diseased subjects, and also\nthat the method generalizes well across different datasets. On the EXACT'09\ntest set, our method achieved the second highest sensitivity score among all\nmethods that reported good specificity.",
          "link": "http://arxiv.org/abs/2103.16328",
          "publishedOn": "2021-08-04T01:59:22.507Z",
          "wordCount": 680,
          "title": "Automatic airway segmentation from Computed Tomography using robust and efficient 3-D convolutional neural networks. (arXiv:2103.16328v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_Z/0/1/0/all/0/1\">Zhibin Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_M/0/1/0/all/0/1\">Mu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wankou Yang</a>",
          "description": "While CNN-based models have made remarkable progress on human pose\nestimation, what spatial dependencies they capture to localize keypoints\nremains unclear. In this work, we propose a model called \\textbf{TransPose},\nwhich introduces Transformer for human pose estimation. The attention layers\nbuilt in Transformer enable our model to capture long-range relationships\nefficiently and also can reveal what dependencies the predicted keypoints rely\non. To predict keypoint heatmaps, the last attention layer acts as an\naggregator, which collects contributions from image clues and forms maximum\npositions of keypoints. Such a heatmap-based localization approach via\nTransformer conforms to the principle of Activation\nMaximization~\\cite{erhan2009visualizing}. And the revealed dependencies are\nimage-specific and fine-grained, which also can provide evidence of how the\nmodel handles special cases, e.g., occlusion. The experiments show that\nTransPose achieves 75.8 AP and 75.0 AP on COCO validation and test-dev sets,\nwhile being more lightweight and faster than mainstream CNN architectures. The\nTransPose model also transfers very well on MPII benchmark, achieving superior\nperformance on the test set when fine-tuned with small training costs. Code and\npre-trained models are publicly\navailable\\footnote{\\url{https://github.com/yangsenius/TransPose}}.",
          "link": "http://arxiv.org/abs/2012.14214",
          "publishedOn": "2021-08-04T01:59:22.493Z",
          "wordCount": 661,
          "title": "TransPose: Keypoint Localization via Transformer. (arXiv:2012.14214v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Joel Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Abhishek Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijmans_E/0/1/0/all/0/1\">Erik Wijmans</a>",
          "description": "ObjectGoal Navigation (ObjectNav) is an embodied task wherein agents are to\nnavigate to an object instance in an unseen environment. Prior works have shown\nthat end-to-end ObjectNav agents that use vanilla visual and recurrent modules,\ne.g. a CNN+RNN, perform poorly due to overfitting and sample inefficiency. This\nhas motivated current state-of-the-art methods to mix analytic and learned\ncomponents and operate on explicit spatial maps of the environment. We instead\nre-enable a generic learned agent by adding auxiliary learning tasks and an\nexploration reward. Our agents achieve 24.5% success and 8.1% SPL, a 37% and 8%\nrelative improvement over prior state-of-the-art, respectively, on the Habitat\nObjectNav Challenge. From our analysis, we propose that agents will act to\nsimplify their visual inputs so as to smooth their RNN dynamics, and that\nauxiliary tasks reduce overfitting by minimizing effective RNN dimensionality;\ni.e. a performant ObjectNav agent that must maintain coherent plans over long\nhorizons does so by learning smooth, low-dimensional recurrent dynamics. Site:\nhttps://joel99.github.io/objectnav/",
          "link": "http://arxiv.org/abs/2104.04112",
          "publishedOn": "2021-08-04T01:59:22.486Z",
          "wordCount": 626,
          "title": "Auxiliary Tasks and Exploration Enable ObjectNav. (arXiv:2104.04112v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.09381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lust_J/0/1/0/all/0/1\">Julia Lust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1\">Alexandru Paul Condurache</a>",
          "description": "Deep Neural Networks (DNNs) achieve state-of-the-art performance on numerous\napplications. However, it is difficult to tell beforehand if a DNN receiving an\ninput will deliver the correct output since their decision criteria are usually\nnontransparent. A DNN delivers the correct output if the input is within the\narea enclosed by its generalization envelope. In this case, the information\ncontained in the input sample is processed reasonably by the network. It is of\nlarge practical importance to assess at inference time if a DNN generalizes\ncorrectly. Currently, the approaches to achieve this goal are investigated in\ndifferent problem set-ups rather independently from one another, leading to\nthree main research and literature fields: predictive uncertainty,\nout-of-distribution detection and adversarial example detection. This survey\nconnects the three fields within the larger framework of investigating the\ngeneralization performance of machine learning methods and in particular DNNs.\nWe underline the common ground, point at the most promising approaches and give\na structured overview of the methods that provide at inference time means to\nestablish if the current input is within the generalization envelope of a DNN.",
          "link": "http://arxiv.org/abs/2008.09381",
          "publishedOn": "2021-08-04T01:59:22.413Z",
          "wordCount": 671,
          "title": "A Survey on Assessing the Generalization Envelope of Deep Neural Networks: Predictive Uncertainty, Out-of-distribution and Adversarial Samples. (arXiv:2008.09381v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shiqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herranz_L/0/1/0/all/0/1\">Luis Herranz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1\">Shangling Jui</a>",
          "description": "Domain adaptation (DA) aims to transfer the knowledge learned from a source\ndomain to an unlabeled target domain. Some recent works tackle source-free\ndomain adaptation (SFDA) where only a source pre-trained model is available for\nadaptation to the target domain. However, those methods do not consider keeping\nsource performance which is of high practical value in real world applications.\nIn this paper, we propose a new domain adaptation paradigm called Generalized\nSource-free Domain Adaptation (G-SFDA), where the learned model needs to\nperform well on both the target and source domains, with only access to current\nunlabeled target data during adaptation. First, we propose local structure\nclustering (LSC), aiming to cluster the target features with its semantically\nsimilar neighbors, which successfully adapts the model to the target domain in\nthe absence of source data. Second, we propose sparse domain attention (SDA),\nit produces a binary domain specific attention to activate different feature\nchannels for different domains, meanwhile the domain attention will be utilized\nto regularize the gradient during adaptation to keep source information. In the\nexperiments, for target performance our method is on par with or better than\nexisting DA and SFDA methods, specifically it achieves state-of-the-art\nperformance (85.4%) on VisDA, and our method works well for all domains after\nadapting to single or multiple target domains. Code is available in\nhttps://github.com/Albert0147/G-SFDA.",
          "link": "http://arxiv.org/abs/2108.01614",
          "publishedOn": "2021-08-04T01:59:22.388Z",
          "wordCount": 660,
          "title": "Generalized Source-free Domain Adaptation. (arXiv:2108.01614v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Song Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhiyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrich_N/0/1/0/all/0/1\">Norman Hendrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1\">Fanyu Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shuzhi Sam Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianwei Zhang</a>",
          "description": "In the classic setting of unsupervised domain adaptation (UDA), the labeled\nsource data are available in the training phase. However, in many real-world\nscenarios, owing to some reasons such as privacy protection and information\nsecurity, the source data is inaccessible, and only a model trained on the\nsource domain is available. This paper proposes a novel deep clustering method\nfor this challenging task. Aiming at the dynamical clustering at feature-level,\nwe introduce extra constraints hidden in the geometric structure between data\nto assist the process. Concretely, we propose a geometry-based constraint,\nnamed semantic consistency on the nearest neighborhood (SCNNH), and use it to\nencourage robust clustering. To reach this goal, we construct the nearest\nneighborhood for every target data and take it as the fundamental clustering\nunit by building our objective on the geometry. Also, we develop a more\nSCNNH-compliant structure with an additional semantic credibility constraint,\nnamed semantic hyper-nearest neighborhood (SHNNH). After that, we extend our\nmethod to this new geometry. Extensive experiments on three challenging UDA\ndatasets indicate that our method achieves state-of-the-art results. The\nproposed method has significant improvement on all datasets (as we adopt SHNNH,\nthe average accuracy increases by over 3.0% on the large-scaled dataset). Code\nis available at https://github.com/tntek/N2DCX.",
          "link": "http://arxiv.org/abs/2107.12585",
          "publishedOn": "2021-08-04T01:59:22.238Z",
          "wordCount": 692,
          "title": "Nearest Neighborhood-Based Deep Clustering for Source Data-absent Unsupervised Domain Adaptation. (arXiv:2107.12585v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.",
          "link": "http://arxiv.org/abs/2107.12651",
          "publishedOn": "2021-08-04T01:59:22.200Z",
          "wordCount": 633,
          "title": "Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Guerrero_C/0/1/0/all/0/1\">Carmina P&#xe9;rez-Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palacios_A/0/1/0/all/0/1\">Adriana Palacios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mata_C/0/1/0/all/0/1\">Christian Mata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Mendoza_M/0/1/0/all/0/1\">Miguel Gonzalez-Mendoza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcon_Morales_L/0/1/0/all/0/1\">Luis Eduardo Falc&#xf3;n-Morales</a>",
          "description": "Risk assessment is relevant in any workplace, however there is a degree of\nunpredictability when dealing with flammable or hazardous materials so that\ndetection of fire accidents by itself may not be enough. An example of this is\nthe impingement of jet fires, where the heat fluxes of the flame could reach\nnearby equipment and dramatically increase the probability of a domino effect\nwith catastrophic results. Because of this, the characterization of such fire\naccidents is important from a risk management point of view. One such\ncharacterization would be the segmentation of different radiation zones within\nthe flame, so this paper presents an exploratory research regarding several\ntraditional computer vision and Deep Learning segmentation approaches to solve\nthis specific problem. A data set of propane jet fires is used to train and\nevaluate the different approaches and given the difference in the distribution\nof the zones and background of the images, different loss functions, that seek\nto alleviate data imbalance, are also explored. Additionally, different metrics\nare correlated to a manual ranking performed by experts to make an evaluation\nthat closely resembles the expert's criteria. The Hausdorff Distance and\nAdjusted Random Index were the metrics with the highest correlation and the\nbest results were obtained from the UNet architecture with a Weighted\nCross-Entropy Loss. These results can be used in future research to extract\nmore geometric information from the segmentation masks or could even be\nimplemented on other types of fire accidents.",
          "link": "http://arxiv.org/abs/2107.03461",
          "publishedOn": "2021-08-04T01:59:22.163Z",
          "wordCount": 729,
          "title": "Comparing Machine Learning based Segmentation Models on Jet Fire Radiation Zones. (arXiv:2107.03461v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02319",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jha_D/0/1/0/all/0/1\">Debesh Jha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Sharib Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tomar_N/0/1/0/all/0/1\">Nikhil Kumar Tomar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Johansen_D/0/1/0/all/0/1\">Dag Johansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Johansen_H/0/1/0/all/0/1\">H&#xe5;vard D. Johansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>",
          "description": "Minimally invasive surgery is a surgical intervention used to examine the\norgans inside the abdomen and has been widely used due to its effectiveness\nover open surgery. Due to the hardware improvements such as high definition\ncameras, this procedure has significantly improved and new software methods\nhave demonstrated potential for computer-assisted procedures. However, there\nexists challenges and requirements to improve detection and tracking of the\nposition of the instruments during these surgical procedures. To this end, we\nevaluate and compare some popular deep learning methods that can be explored\nfor the automated segmentation of surgical instruments in laparoscopy, an\nimportant step towards tool tracking. Our experimental results exhibit that the\nDual decoder attention network (DDANet) produces a superior result compared to\nother recent deep learning methods. DDANet yields a Dice coefficient of 0.8739\nand mean intersection-over-union of 0.8183 for the Robust Medical Instrument\nSegmentation (ROBUST-MIS) Challenge 2019 dataset, at a real-time speed of\n101.36 frames-per-second that is critical for such procedures.",
          "link": "http://arxiv.org/abs/2107.02319",
          "publishedOn": "2021-08-04T01:59:22.134Z",
          "wordCount": 641,
          "title": "Exploring Deep Learning Methods for Real-Time Surgical Instrument Segmentation in Laparoscopy. (arXiv:2107.02319v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08543",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saparov_T/0/1/0/all/0/1\">Talgat Saparov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurmukov_A/0/1/0/all/0/1\">Anvar Kurmukov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shirokih_B/0/1/0/all/0/1\">Boris Shirokih</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Belyaev_M/0/1/0/all/0/1\">Mikhail Belyaev</a>",
          "description": "Domain shift is one of the most salient challenges in medical computer\nvision. Due to immense variability in scanners' parameters and imaging\nprotocols, even images obtained from the same person and the same scanner could\ndiffer significantly. We address variability in computed tomography (CT) images\ncaused by different convolution kernels used in the reconstruction process, the\ncritical domain shift factor in CT. The choice of a convolution kernel affects\npixels' granularity, image smoothness, and noise level. We analyze a dataset of\npaired CT images, where smooth and sharp images were reconstructed from the\nsame sinograms with different kernels, thus providing identical anatomy but\ndifferent style. Though identical predictions are desired, we show that the\nconsistency, measured as the average Dice between predictions on pairs, is just\n0.54. We propose Filtered Back-Projection Augmentation (FBPAug), a simple and\nsurprisingly efficient approach to augment CT images in sinogram space\nemulating reconstruction with different kernels. We apply the proposed method\nin a zero-shot domain adaptation setup and show that the consistency boosts\nfrom 0.54 to 0.92 outperforming other augmentation approaches. Neither specific\npreparation of source domain data nor target domain data is required, so our\npublicly released FBPAug can be used as a plug-and-play module for zero-shot\ndomain adaptation in any CT-based task.",
          "link": "http://arxiv.org/abs/2107.08543",
          "publishedOn": "2021-08-04T01:59:22.111Z",
          "wordCount": 681,
          "title": "Zero-Shot Domain Adaptation in CT Segmentation by Filtered Back Projection Augmentation. (arXiv:2107.08543v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Csaba_B/0/1/0/all/0/1\">Botos Csaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhry_A/0/1/0/all/0/1\">Arslan Chaudhry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1\">Puneet Dokania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>",
          "description": "Domain shift is a well known problem where a model trained on a particular\ndomain (source) does not perform well when exposed to samples from a different\ndomain (target). Unsupervised methods that can adapt to domain shift are highly\ndesirable as they allow effective utilization of the source data without\nrequiring additional annotated training data from the target. Practically,\nobtaining sufficient amount of annotated data from the target domain can be\nboth infeasible and extremely expensive. In this work, we address the domain\nshift problem for the object detection task. Our approach relies on gradually\nremoving the domain shift between the source and the target domains. The key\ningredients to our approach are -- (a) mapping the source to the target domain\non pixel-level; (b) training a teacher network on the mapped source and the\nunannotated target domain using adversarial feature alignment; and (c) finally\ntraining a student network using the pseudo-labels obtained from the teacher.\nExperimentally, when tested on challenging scenarios involving domain shift, we\nconsistently obtain significantly large performance gains over various recent\nstate of the art approaches.",
          "link": "http://arxiv.org/abs/2108.00977",
          "publishedOn": "2021-08-04T01:59:22.088Z",
          "wordCount": 636,
          "title": "Multilevel Knowledge Transfer for Cross-Domain Object Detection. (arXiv:2108.00977v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_K/0/1/0/all/0/1\">Kota Yamaguchi</a>",
          "description": "Vector graphic documents present visual elements in a resolution free,\ncompact format and are often seen in creative applications. In this work, we\nattempt to learn a generative model of vector graphic documents. We define\nvector graphic documents by a multi-modal set of attributes associated to a\ncanvas and a sequence of visual elements such as shapes, images, or texts, and\ntrain variational auto-encoders to learn the representation of the documents.\nWe collect a new dataset of design templates from an online service that\nfeatures complete document structure including occluded elements. In\nexperiments, we show that our model, named CanvasVAE, constitutes a strong\nbaseline for generative modeling of vector graphic documents.",
          "link": "http://arxiv.org/abs/2108.01249",
          "publishedOn": "2021-08-04T01:59:22.059Z",
          "wordCount": 545,
          "title": "CanvasVAE: Learning to Generate Vector Graphic Documents. (arXiv:2108.01249v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keser_M/0/1/0/all/0/1\">Mert Keser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savkin_A/0/1/0/all/0/1\">Artem Savkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "Synthetic data generation is an appealing approach to generate novel traffic\nscenarios in autonomous driving. However, deep learning perception algorithms\ntrained solely on synthetic data encounter serious performance drops when they\nare tested on real data. Such performance drops are commonly attributed to the\ndomain gap between real and synthetic data. Domain adaptation methods that have\nbeen applied to mitigate the aforementioned domain gap achieve visually\nappealing results, but usually introduce semantic inconsistencies into the\ntranslated samples. In this work, we propose a novel, unsupervised, end-to-end\ndomain adaptation network architecture that enables semantically consistent\n\\textit{sim2real} image transfer. Our method performs content disentanglement\nby employing shared content encoder and fixed style code.",
          "link": "http://arxiv.org/abs/2105.08704",
          "publishedOn": "2021-08-04T01:59:22.000Z",
          "wordCount": 590,
          "title": "Content Disentanglement for Semantically Consistent Synthetic-to-Real Domain Adaptation. (arXiv:2105.08704v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dana_A/0/1/0/all/0/1\">Alexandra Dana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shutman_M/0/1/0/all/0/1\">Maor Shutman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perlitz_Y/0/1/0/all/0/1\">Yotam Perlitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitek_R/0/1/0/all/0/1\">Ran Vitek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peleg_T/0/1/0/all/0/1\">Tomer Peleg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jevnisek_R/0/1/0/all/0/1\">Roy J Jevnisek</a>",
          "description": "General object detectors use powerful backbones that uniformly extract\nfeatures from images for enabling detection of a vast amount of object types.\nHowever, utilization of such backbones in object detection applications\ndeveloped for specific object types can unnecessarily over-process an extensive\namount of background. In addition, they are agnostic to object scales, thus\nredundantly process all image regions at the same resolution. In this work we\nintroduce BLT-net, a new low-computation two-stage object detection\narchitecture designed to process images with a significant amount of background\nand objects of variate scales. BLT-net reduces computations by separating\nobjects from background using a very lite first-stage. BLT-net then efficiently\nmerges obtained proposals to further decrease processed background and then\ndynamically reduces their resolution to minimize computations. Resulting image\nproposals are then processed in the second-stage by a highly accurate model. We\ndemonstrate our architecture on the pedestrian detection problem, where objects\nare of different sizes, images are of high resolution and object detection is\nrequired to run in real-time. We show that our design reduces computations by a\nfactor of x4-x7 on the Citypersons and Caltech datasets with respect to leading\npedestrian detectors, on account of a small accuracy degradation. This method\ncan be applied on other object detection applications in scenes with a\nconsiderable amount of background and variate object sizes to reduce\ncomputations.",
          "link": "http://arxiv.org/abs/2107.10050",
          "publishedOn": "2021-08-04T01:59:21.974Z",
          "wordCount": 701,
          "title": "You Better Look Twice: a new perspective for designing accurate detectors with reduced computations. (arXiv:2107.10050v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dellenbach_P/0/1/0/all/0/1\">Pierre Dellenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deschaud_J/0/1/0/all/0/1\">Jean-Emmanuel Deschaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacquet_B/0/1/0/all/0/1\">Bastien Jacquet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goulette_F/0/1/0/all/0/1\">Fran&#xe7;ois Goulette</a>",
          "description": "With the democratization of 3D LiDAR sensors, precise LiDAR odometries and\nSLAM are in high demand. New methods regularly appear, proposing solutions\nranging from small variations in classical algorithms to radically new\nparadigms based on deep learning. Yet it is often difficult to compare these\nmethods, notably due to the few datasets on which the methods can be evaluated\nand compared. Furthermore, their weaknesses are rarely examined, often letting\nthe user discover the hard way whether a method would be appropriate for a use\ncase. In this paper, we review and organize the main 3D LiDAR odometries into\ndistinct categories. We implemented several approaches (geometric based, deep\nlearning based, and hybrid methods) to conduct an in-depth analysis of their\nstrengths and weaknesses on multiple datasets, guiding the reader through the\ndifferent LiDAR odometries available. Implementation of the methods has been\nmade publicly available at https://github.com/Kitware/pyLiDAR-SLAM.",
          "link": "http://arxiv.org/abs/2103.09708",
          "publishedOn": "2021-08-04T01:59:21.967Z",
          "wordCount": 644,
          "title": "What s in My LiDAR Odometry Toolbox?. (arXiv:2103.09708v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qiaojun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jadhav_S/0/1/0/all/0/1\">Sai Jadhav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1\">Nikolay Atanasov</a>",
          "description": "This paper considers online object-level mapping using partial point-cloud\nobservations obtained online in an unknown environment. We develop and approach\nfor fully Convolutional Object Retrieval and Symmetry-AIded Registration\n(CORSAIR). Our model extends the Fully Convolutional Geometric Features model\nto learn a global object-shape embedding in addition to local point-wise\nfeatures from the point-cloud observations. The global feature is used to\nretrieve a similar object from a category database, and the local features are\nused for robust pose registration between the observed and the retrieved\nobject. Our formulation also leverages symmetries, present in the object\nshapes, to obtain promising local-feature pairs from different symmetry classes\nfor matching. We present results from synthetic and real-world datasets with\ndifferent object categories to verify the robustness of our method.",
          "link": "http://arxiv.org/abs/2103.06911",
          "publishedOn": "2021-08-04T01:59:21.941Z",
          "wordCount": 595,
          "title": "CORSAIR: Convolutional Object Retrieval and Symmetry-AIded Registration. (arXiv:2103.06911v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1\">Himanshu Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iaboni_C/0/1/0/all/0/1\">Craig Iaboni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobo_D/0/1/0/all/0/1\">Deepan Lobo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Ji-won Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abichandani_P/0/1/0/all/0/1\">Pramod Abichandani</a>",
          "description": "This paper presents a real-time method to detect and track multiple mobile\nground robots using event cameras. The method uses density-based spatial\nclustering of applications with noise (DBSCAN) to detect the robots and a\nsingle k-dimensional ($k - d$) tree to accurately keep track of them as they\nmove in an indoor arena. Robust detections and tracks are maintained in the\nface of event camera noise and lack of events (due to robots moving slowly or\nstopping). An off-the-shelf RGB camera-based tracking system was used to\nprovide ground truth. Experiments including up to 4 robots are performed to\nstudy the effect of i) varying DBSCAN parameters, ii) the event accumulation\ntime, iii) the number of robots in the arena, iv) the speed of the robots, and\nv) variation in ambient light conditions on the detection and tracking\nperformance. The experimental results showed 100% detection and tracking\nfidelity in the face of event camera noise and robots stopping for tests\ninvolving up to 3 robots (and upwards of 93% for 4 robots). When the lighting\nconditions were varied, a graceful degradation in detection and tracking\nfidelity was observed.",
          "link": "http://arxiv.org/abs/2102.11916",
          "publishedOn": "2021-08-04T01:59:21.933Z",
          "wordCount": 655,
          "title": "Event Camera Based Real-Time Detection and Tracking of Indoor Ground Robots. (arXiv:2102.11916v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01200",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barros_T/0/1/0/all/0/1\">T. Barros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conde_P/0/1/0/all/0/1\">P. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_G/0/1/0/all/0/1\">G. Gon&#xe7;alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Premebida_C/0/1/0/all/0/1\">C. Premebida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monteiro_M/0/1/0/all/0/1\">M. Monteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_C/0/1/0/all/0/1\">C.S.S. Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nunes_U/0/1/0/all/0/1\">U.J. Nunes</a>",
          "description": "Digital agriculture has evolved significantly over the last few years due to\nthe technological developments in automation and computational intelligence\napplied to the agricultural sector, including vineyards which are a relevant\ncrop in the Mediterranean region. In this paper, a study of semantic\nsegmentation for vine detection in real-world vineyards is presented by\nexploring state-of-the-art deep segmentation networks and conventional\nunsupervised methods. Camera data was collected on vineyards using an Unmanned\nAerial System (UAS) equipped with a dual imaging sensor payload, namely a\nhigh-resolution color camera and a five-band multispectral and thermal camera.\nExtensive experiments of the segmentation networks and unsupervised methods\nhave been performed on multimodal datasets representing three distinct\nvineyards located in the central region of Portugal. The reported results\nindicate that the best segmentation performances are obtained with deep\nnetworks, while traditional (non-deep) approaches using the NIR band shown\ncompetitive results. The results also show that multimodality slightly improves\nthe performance of vine segmentation but the NIR spectrum alone generally is\nsufficient on most of the datasets. The code and dataset are publicly available\non \\url{https://github.com/Cybonic/DL_vineyard_segmentation_study.git",
          "link": "http://arxiv.org/abs/2108.01200",
          "publishedOn": "2021-08-04T01:59:21.912Z",
          "wordCount": 674,
          "title": "Multispectral Vineyard Segmentation: A Deep Learning approach. (arXiv:2108.01200v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00940",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ramirez_J/0/1/0/all/0/1\">Juan Marcos Ram&#xed;rez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Torre_J/0/1/0/all/0/1\">Jos&#xe9; Ignacio Mart&#xed;nez Torre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fuentes_H/0/1/0/all/0/1\">Henry Arguello Fuentes</a>",
          "description": "Image fusion aims at estimating a high-resolution spectral image from a\nlow-spatial-resolution hyperspectral image and a low-spectral-resolution\nmultispectral image. In this regard, compressive spectral imaging (CSI) has\nemerged as an acquisition framework that captures the relevant information of\nspectral images using a reduced number of measurements. Recently, various image\nfusion methods from CSI measurements have been proposed. However, these methods\nexhibit high running times and face the challenging task of choosing\nsparsity-inducing bases. In this paper, a deep network under the algorithm\nunrolling approach is proposed for fusing spectral images from compressive\nmeasurements. This architecture, dubbed LADMM-Net, casts each iteration of a\nlinearized version of the alternating direction method of multipliers into a\nprocessing layer whose concatenation deploys a deep network. The linearized\napproach enables obtaining fusion estimates without resorting to costly matrix\ninversions. Furthermore, this approach exploits the benefits of learnable\ntransforms to estimate the image details included in both the auxiliary\nvariable and the Lagrange multiplier. Finally, the performance of the proposed\ntechnique is evaluated on two spectral image databases and one dataset captured\nat the laboratory. Extensive simulations show that the proposed method\noutperforms the state-of-the-art approaches that fuse spectral images from\ncompressive measurements.",
          "link": "http://arxiv.org/abs/2103.00940",
          "publishedOn": "2021-08-04T01:59:21.904Z",
          "wordCount": 710,
          "title": "LADMM-Net: An Unrolled Deep Network For Spectral Image Fusion From Compressive Data. (arXiv:2103.00940v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gansbeke_W/0/1/0/all/0/1\">Wouter Van Gansbeke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandenhende_S/0/1/0/all/0/1\">Simon Vandenhende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "Being able to learn dense semantic representations of images without\nsupervision is an important problem in computer vision. However, despite its\nsignificance, this problem remains rather unexplored, with a few exceptions\nthat considered unsupervised semantic segmentation on small-scale datasets with\na narrow visual domain. In this paper, we make a first attempt to tackle the\nproblem on datasets that have been traditionally utilized for the supervised\ncase. To achieve this, we introduce a two-step framework that adopts a\npredetermined mid-level prior in a contrastive optimization objective to learn\npixel embeddings. This marks a large deviation from existing works that relied\non proxy tasks or end-to-end clustering. Additionally, we argue about the\nimportance of having a prior that contains information about objects, or their\nparts, and discuss several possibilities to obtain such a prior in an\nunsupervised manner.\n\nExperimental evaluation shows that our method comes with key advantages over\nexisting works. First, the learned pixel embeddings can be directly clustered\nin semantic groups using K-Means on PASCAL. Under the fully unsupervised\nsetting, there is no precedent in solving the semantic segmentation task on\nsuch a challenging benchmark. Second, our representations can improve over\nstrong baselines when transferred to new datasets, e.g. COCO and DAVIS. The\ncode is available.",
          "link": "http://arxiv.org/abs/2102.06191",
          "publishedOn": "2021-08-04T01:59:21.897Z",
          "wordCount": 698,
          "title": "Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals. (arXiv:2102.06191v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.14509",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1\">Joseph P. Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1\">Zaid Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1\">Ming Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>",
          "description": "Kinship, a soft biometric detectable in media, is fundamental for a myriad of\nuse-cases. Despite the difficulty of detecting kinship, annual data challenges\nusing still-images have consistently improved performances and attracted new\nresearchers. Now, systems reach performance levels unforeseeable a decade ago,\nclosing in on performances acceptable to deploy in practice. Similar to other\nbiometric tasks, we expect systems can benefit from additional modalities. We\nhypothesize that adding modalities to FIW, which contains only still-images,\nwill improve performance. Thus, to narrow the gap between research and reality\nand enhance the power of kinship recognition systems, we extend FIW with\nmultimedia (MM) data (i.e., video, audio, and text captions). Specifically, we\nintroduce the first publicly available multi-task MM kinship dataset. To build\nFIW MM, we developed machinery to automatically collect, annotate, and prepare\nthe data, requiring minimal human input and no financial cost. The proposed MM\ncorpus allows the problem statements to be more realistic template-based\nprotocols. We show significant improvements in all benchmarks with the added\nmodalities. The results highlight edge cases to inspire future research with\ndifferent areas of improvement. FIW MM provides the data required to increase\nthe potential of automated systems to detect kinship in MM. It also allows\nexperts from diverse fields to collaborate in novel ways.",
          "link": "http://arxiv.org/abs/2007.14509",
          "publishedOn": "2021-08-04T01:59:21.874Z",
          "wordCount": 715,
          "title": "Families In Wild Multimedia (FIW MM): A Multi-Modal Database for Recognizing Kinship. (arXiv:2007.14509v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1\">Aravind Srinivas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_N/0/1/0/all/0/1\">Niki Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1\">Jonathon Shlens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1\">Ashish Vaswani</a>",
          "description": "We present BoTNet, a conceptually simple yet powerful backbone architecture\nthat incorporates self-attention for multiple computer vision tasks including\nimage classification, object detection and instance segmentation. By just\nreplacing the spatial convolutions with global self-attention in the final\nthree bottleneck blocks of a ResNet and no other changes, our approach improves\nupon the baselines significantly on instance segmentation and object detection\nwhile also reducing the parameters, with minimal overhead in latency. Through\nthe design of BoTNet, we also point out how ResNet bottleneck blocks with\nself-attention can be viewed as Transformer blocks. Without any bells and\nwhistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance\nSegmentation benchmark using the Mask R-CNN framework; surpassing the previous\nbest published single model and single scale results of ResNeSt evaluated on\nthe COCO validation set. Finally, we present a simple adaptation of the BoTNet\ndesign for image classification, resulting in models that achieve a strong\nperformance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to\n1.64x faster in compute time than the popular EfficientNet models on TPU-v3\nhardware. We hope our simple and effective approach will serve as a strong\nbaseline for future research in self-attention models for vision",
          "link": "http://arxiv.org/abs/2101.11605",
          "publishedOn": "2021-08-04T01:59:21.867Z",
          "wordCount": 685,
          "title": "Bottleneck Transformers for Visual Recognition. (arXiv:2101.11605v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weizhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>",
          "description": "Modern methods for counting people in crowded scenes rely on deep networks to\nestimate people densities in individual images. As such, only very few take\nadvantage of temporal consistency in video sequences, and those that do only\nimpose weak smoothness constraints across consecutive frames. In this paper, we\nadvocate estimating people flows across image locations between consecutive\nimages and inferring the people densities from these flows instead of directly\nregressing them. This enables us to impose much stronger constraints encoding\nthe conservation of the number of people. As a result, it significantly boosts\nperformance without requiring a more complex architecture. Furthermore, it\nallows us to exploit the correlation between people flow and optical flow to\nfurther improve the results. We also show that leveraging people conservation\nconstraints in both a spatial and temporal manner makes it possible to train a\ndeep crowd counting model in an active learning setting with much fewer\nannotations. This significantly reduces the annotation cost while still leading\nto similar performance to the full supervision case.",
          "link": "http://arxiv.org/abs/2012.00452",
          "publishedOn": "2021-08-04T01:59:21.856Z",
          "wordCount": 650,
          "title": "Counting People by Estimating People Flows. (arXiv:2012.00452v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.03459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wanling Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1\">Fei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1\">Jianfeng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xu Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Chuanxin Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chunjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zihan Jiang</a>",
          "description": "Modern real-world application scenarios like Internet services consist of a\ndiversity of AI and non-AI modules with huge code sizes and long and\ncomplicated execution paths, which raises serious benchmarking or evaluating\nchallenges. Using AI components or micro benchmarks alone can lead to\nerror-prone conclusions. This paper presents a methodology to attack the above\nchallenge. We formalize a real-world application scenario as a Directed Acyclic\nGraph-based model and propose the rules to distill it into a permutation of\nessential AI and non-AI tasks, which we call a scenario benchmark. Together\nwith seventeen industry partners, we extract nine typical scenario benchmarks.\nWe design and implement an extensible, configurable, and flexible benchmark\nframework. We implement two Internet service AI scenario benchmarks based on\nthe framework as proxies to two real-world application scenarios. We consider\nscenario, component, and micro benchmarks as three indispensable parts for\nevaluating. Our evaluation shows the advantage of our methodology against using\ncomponent or micro AI benchmarks alone. The specifications, source code,\ntestbed, and results are publicly available from\n\\url{https://www.benchcouncil.org/aibench/scenario/}.",
          "link": "http://arxiv.org/abs/2005.03459",
          "publishedOn": "2021-08-04T01:59:21.837Z",
          "wordCount": 672,
          "title": "AIBench Scenario: Scenario-distilling AI Benchmarking. (arXiv:2005.03459v3 [cs.PF] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pfeil_T/0/1/0/all/0/1\">Thomas Pfeil</a>",
          "description": "Deep neural networks have usually to be compressed and accelerated for their\nusage in low-power, e.g. mobile, devices. Recently, massively-parallel hardware\naccelerators were developed that offer high throughput and low latency at low\npower by utilizing in-memory computation. However, to exploit these benefits\nthe computational graph of a neural network has to fit into the in-computation\nmemory of these hardware systems that is usually rather limited in size. In\nthis study, we introduce a class of network models that have a small memory\nfootprint in terms of their computational graphs. To this end, the graph is\ndesigned to contain loops by iteratively executing a single network building\nblock. Furthermore, the trade-off between accuracy and latency of these\nso-called iterative neural networks is improved by adding multiple intermediate\noutputs during both training and inference. We show state-of-the-art results\nfor semantic segmentation on the CamVid and Cityscapes datasets that are\nespecially demanding in terms of computational resources. In ablation studies,\nthe improvement of network training by intermediate network outputs as well as\nthe trade-off between weight sharing over iterations and the network size are\ninvestigated.",
          "link": "http://arxiv.org/abs/2101.08685",
          "publishedOn": "2021-08-04T01:59:21.831Z",
          "wordCount": 675,
          "title": "ItNet: iterative neural networks with small graphs for accurate, efficient and anytime semantic segmentation. (arXiv:2101.08685v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01659",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_R/0/1/0/all/0/1\">Ruizhe Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bastiani_M/0/1/0/all/0/1\">Matteo Bastiani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Auer_D/0/1/0/all/0/1\">Dorothee Auer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wagner_C/0/1/0/all/0/1\">Christian Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>",
          "description": "Brain age estimation based on magnetic resonance imaging (MRI) is an active\nresearch area in early diagnosis of some neurodegenerative diseases (e.g.\nAlzheimer, Parkinson, Huntington, etc.) for elderly people or brain\nunderdevelopment for the young group. Deep learning methods have achieved the\nstate-of-the-art performance in many medical image analysis tasks, including\nbrain age estimation. However, the performance and generalisability of the deep\nlearning model are highly dependent on the quantity and quality of the training\ndata set. Both collecting and annotating brain MRI data are extremely\ntime-consuming. In this paper, to overcome the data scarcity problem, we\npropose a generative adversarial network (GAN) based image synthesis method.\nDifferent from the existing GAN-based methods, we integrate a task-guided\nbranch (a regression model for age estimation) to the end of the generator in\nGAN. By adding a task-guided loss to the conventional GAN loss, the learned\nlow-dimensional latent space and the synthesised images are more task-specific.\nIt helps to boost the performance of the down-stream task by combining the\nsynthesised images and real images for model training. The proposed method was\nevaluated on a public brain MRI data set for age estimation. Our proposed\nmethod outperformed (statistically significant) a deep convolutional neural\nnetwork based regression model and the GAN-based image synthesis method without\nthe task-guided branch. More importantly, it enables the identification of\nage-related brain regions in the image space. The code is available on GitHub\n(https://github.com/ruizhe-l/tgb-gan).",
          "link": "http://arxiv.org/abs/2108.01659",
          "publishedOn": "2021-08-04T01:59:21.825Z",
          "wordCount": 713,
          "title": "Image Augmentation Using a Task Guided Generative Adversarial Network for Age Estimation on Brain MRI. (arXiv:2108.01659v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yunpeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jiayi Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoshuai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Liujuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "Descriptive region features extracted by object detection networks have\nplayed an important role in the recent advancements of image captioning.\nHowever, they are still criticized for the lack of contextual information and\nfine-grained details, which in contrast are the merits of traditional grid\nfeatures. In this paper, we introduce a novel Dual-Level Collaborative\nTransformer (DLCT) network to realize the complementary advantages of the two\nfeatures. Concretely, in DLCT, these two features are first processed by a\nnovelDual-way Self Attenion (DWSA) to mine their intrinsic properties, where a\nComprehensive Relation Attention component is also introduced to embed the\ngeometric information. In addition, we propose a Locality-Constrained Cross\nAttention module to address the semantic noises caused by the direct fusion of\nthese two features, where a geometric alignment graph is constructed to\naccurately align and reinforce region and grid features. To validate our model,\nwe conduct extensive experiments on the highly competitive MS-COCO dataset, and\nachieve new state-of-the-art performance on both local and online test sets,\ni.e., 133.8% CIDEr-D on Karpathy split and 135.4% CIDEr on the official split.\nCode is available at https://github.com/luo3300612/image-captioning-DLCT.",
          "link": "http://arxiv.org/abs/2101.06462",
          "publishedOn": "2021-08-04T01:59:21.813Z",
          "wordCount": 658,
          "title": "Dual-Level Collaborative Transformer for Image Captioning. (arXiv:2101.06462v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.07054",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yu-Huan Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_S/0/1/0/all/0/1\">Shang-Hua Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1\">Rong-Guo Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>",
          "description": "Recently, the coronavirus disease 2019 (COVID-19) has caused a pandemic\ndisease in over 200 countries, influencing billions of humans. To control the\ninfection, identifying and separating the infected people is the most crucial\nstep. The main diagnostic tool is the Reverse Transcription Polymerase Chain\nReaction (RT-PCR) test. Still, the sensitivity of the RT-PCR test is not high\nenough to effectively prevent the pandemic. The chest CT scan test provides a\nvaluable complementary tool to the RT-PCR test, and it can identify the\npatients in the early-stage with high sensitivity. However, the chest CT scan\ntest is usually time-consuming, requiring about 21.5 minutes per case. This\npaper develops a novel Joint Classification and Segmentation (JCS) system to\nperform real-time and explainable COVID-19 chest CT diagnosis. To train our JCS\nsystem, we construct a large scale COVID-19 Classification and Segmentation\n(COVID-CS) dataset, with 144,167 chest CT images of 400 COVID-19 patients and\n350 uninfected cases. 3,855 chest CT images of 200 patients are annotated with\nfine-grained pixel-level labels of opacifications, which are increased\nattenuation of the lung parenchyma. We also have annotated lesion counts,\nopacification areas, and locations and thus benefit various diagnosis aspects.\nExtensive experiments demonstrate that the proposed JCS diagnosis system is\nvery efficient for COVID-19 classification and segmentation. It obtains an\naverage sensitivity of 95.0% and a specificity of 93.0% on the classification\ntest set, and 78.5% Dice score on the segmentation test set of our COVID-CS\ndataset. The COVID-CS dataset and code are available at\nhttps://github.com/yuhuan-wu/JCS.",
          "link": "http://arxiv.org/abs/2004.07054",
          "publishedOn": "2021-08-04T01:59:21.807Z",
          "wordCount": 802,
          "title": "JCS: An Explainable COVID-19 Diagnosis System by Joint Classification and Segmentation. (arXiv:2004.07054v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01321",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Naruenatthanaset_K/0/1/0/all/0/1\">Korranat Naruenatthanaset</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chalidabhongse_T/0/1/0/all/0/1\">Thanarat H. Chalidabhongse</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Palasuwan_D/0/1/0/all/0/1\">Duangdao Palasuwan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Palasuwan_A/0/1/0/all/0/1\">Attakorn Palasuwan</a>",
          "description": "Automated red blood cell (RBC) classification on blood smear images helps\nhematologists to analyze RBC lab results in a reduced time and cost. However,\noverlapping cells can cause incorrect predicted results, and so they have to be\nseparated into multiple single RBCs before classifying. To classify multiple\nclasses with deep learning, imbalance problems are common in medical imaging\nbecause normal samples are always higher than rare disease samples. This paper\npresents a new method to segment and classify RBCs from blood smear images,\nspecifically to tackle cell overlapping and data imbalance problems. Focusing\non overlapping cell separation, our segmentation process first estimates\nellipses to represent RBCs. The method detects the concave points and then\nfinds the ellipses using directed ellipse fitting. The accuracy from 20 blood\nsmear images was 0.889. Classification requires balanced training datasets.\nHowever, some RBC types are rare. The imbalance ratio of this dataset was\n34.538 for 12 RBC classes from 20,875 individual RBC samples. The use of\nmachine learning for RBC classification with an imbalanced dataset is hence\nmore challenging than many other applications. We analyzed techniques to deal\nwith this problem. The best accuracy and F1-score were 0.921 and 0.8679,\nrespectively, using EfficientNet-B1 with augmentation. Experimental results\nshowed that the weight balancing technique with augmentation had the potential\nto deal with imbalance problems by improving the F1-score on minority classes,\nwhile data augmentation significantly improved the overall classification\nperformance.",
          "link": "http://arxiv.org/abs/2012.01321",
          "publishedOn": "2021-08-04T01:59:21.790Z",
          "wordCount": 753,
          "title": "Red Blood Cell Segmentation with Overlapping Cell Separation and Classification on Imbalanced Dataset. (arXiv:2012.01321v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sergazinov_R/0/1/0/all/0/1\">Renat Sergazinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kramar_M/0/1/0/all/0/1\">Miroslav Kramar</a>",
          "description": "Photoelastic techniques have a long tradition in both qualitative and\nquantitative analysis of the stresses in granular materials. Over the last two\ndecades, computational methods for reconstructing forces between particles from\ntheir photoelastic response have been developed by many different experimental\nteams. Unfortunately, all of these methods are computationally expensive. This\nlimits their use for processing extensive data sets that capture the time\nevolution of granular ensembles consisting of a large number of particles. In\nthis paper, we present a novel approach to this problem which leverages the\npower of convolutional neural networks to recognize complex spatial patterns.\nThe main drawback of using neural networks is that training them usually\nrequires a large labeled data set which is hard to obtain experimentally. We\nshow that this problem can be successfully circumvented by pretraining the\nnetworks on a large synthetic data set and then fine-tuning them on much\nsmaller experimental data sets. Due to our current lack of experimental data,\nwe demonstrate the potential of our method by changing the size of the\nconsidered particles which alters the exhibited photoelastic patterns more than\ntypical experimental errors.",
          "link": "http://arxiv.org/abs/2010.01163",
          "publishedOn": "2021-08-04T01:59:21.757Z",
          "wordCount": 680,
          "title": "Machine learning approach to force reconstruction in photoelastic materials. (arXiv:2010.01163v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arnold_S/0/1/0/all/0/1\">S&#xe9;bastien M. R. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhillon_G/0/1/0/all/0/1\">Guneet S. Dhillon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "Episodic training is a core ingredient of few-shot learning to train models\non tasks with limited labelled data. Despite its success, episodic training\nremains largely understudied, prompting us to ask the question: what is the\nbest way to sample episodes? In this paper, we first propose a method to\napproximate episode sampling distributions based on their difficulty. Building\non this method, we perform an extensive analysis and find that sampling\nuniformly over episode difficulty outperforms other sampling schemes, including\ncurriculum and easy-/hard-mining. As the proposed sampling method is algorithm\nagnostic, we can leverage these insights to improve few-shot learning\naccuracies across many episodic training algorithms. We demonstrate the\nefficacy of our method across popular few-shot learning datasets, algorithms,\nnetwork architectures, and protocols.",
          "link": "http://arxiv.org/abs/2108.01662",
          "publishedOn": "2021-08-04T01:59:21.741Z",
          "wordCount": 562,
          "title": "Uniform Sampling over Episode Difficulty. (arXiv:2108.01662v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mostofa_M/0/1/0/all/0/1\">Moktari Mostofa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamadi_S/0/1/0/all/0/1\">Salman Mohamadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "In recent years, cross-spectral iris recognition has emerged as a promising\nbiometric approach to establish the identity of individuals. However, matching\niris images acquired at different spectral bands (i.e., matching a visible\n(VIS) iris probe to a gallery of near-infrared (NIR) iris images or vice versa)\nshows a significant performance degradation when compared to intraband NIR\nmatching. Hence, in this paper, we have investigated a range of deep\nconvolutional generative adversarial network (DCGAN) architectures to further\nimprove the accuracy of cross-spectral iris recognition methods. Moreover,\nunlike the existing works in the literature, we introduce a resolution\ndifference into the classical cross-spectral matching problem domain. We have\ndeveloped two different techniques using the conditional generative adversarial\nnetwork (cGAN) as a backbone architecture for cross-spectral iris matching. In\nthe first approach, we simultaneously address the cross-resolution and\ncross-spectral matching problem by training a cGAN that jointly translates\ncross-resolution as well as cross-spectral tasks to the same resolution and\nwithin the same spectrum. In the second approach, we design a coupled\ngenerative adversarial network (cpGAN) architecture consisting of a pair of\ncGAN modules that project the VIS and NIR iris images into a low-dimensional\nembedding domain to ensure maximum pairwise similarity between the feature\nvectors from the two iris modalities of the same subject.",
          "link": "http://arxiv.org/abs/2108.01569",
          "publishedOn": "2021-08-04T01:59:21.705Z",
          "wordCount": 661,
          "title": "Deep GAN-Based Cross-Spectral Cross-Resolution Iris Recognition. (arXiv:2108.01569v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01634",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Besnier_V/0/1/0/all/0/1\">Victor Besnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1\">Andrei Bursuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1\">David Picard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briot_A/0/1/0/all/0/1\">Alexandre Briot</a>",
          "description": "In this paper, we tackle the detection of out-of-distribution (OOD) objects\nin semantic segmentation. By analyzing the literature, we found that current\nmethods are either accurate or fast but not both which limits their usability\nin real world applications. To get the best of both aspects, we propose to\nmitigate the common shortcomings by following four design principles:\ndecoupling the OOD detection from the segmentation task, observing the entire\nsegmentation network instead of just its output, generating training data for\nthe OOD detector by leveraging blind spots in the segmentation network and\nfocusing the generated data on localized regions in the image to simulate OOD\nobjects. Our main contribution is a new OOD detection architecture called\nObsNet associated with a dedicated training scheme based on Local Adversarial\nAttacks (LAA). We validate the soundness of our approach across numerous\nablation studies. We also show it obtains top performances both in speed and\naccuracy when compared to ten recent methods of the literature on three\ndifferent datasets.",
          "link": "http://arxiv.org/abs/2108.01634",
          "publishedOn": "2021-08-04T01:59:21.697Z",
          "wordCount": 610,
          "title": "Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation. (arXiv:2108.01634v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharafutdinov_D/0/1/0/all/0/1\">Dinar Sharafutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griguletskii_M/0/1/0/all/0/1\">Mark Griguletskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopanev_P/0/1/0/all/0/1\">Pavel Kopanev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurenkov_M/0/1/0/all/0/1\">Mikhail Kurenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_G/0/1/0/all/0/1\">Gonzalo Ferrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burkov_A/0/1/0/all/0/1\">Aleksey Burkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonnochenko_A/0/1/0/all/0/1\">Aleksei Gonnochenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsetserukou_D/0/1/0/all/0/1\">Dzmitry Tsetserukou</a>",
          "description": "SLAM is one of the most fundamental areas of research in robotics and\ncomputer vision. State of the art solutions has advanced significantly in terms\nof accuracy and stability. Unfortunately, not all the approaches are available\nas open-source solutions and free to use. The results of some of them are\ndifficult to reproduce, and there is a lack of comparison on common datasets.\nIn our work, we make a comparative analysis of state of the art open-source\nmethods. We assess the algorithms based on accuracy, computational performance,\nrobustness, and fault tolerance. Moreover, we present a comparison of datasets\nas well as an analysis of algorithms from a practical point of view. The\nfindings of the work raise several crucial questions for SLAM researchers.",
          "link": "http://arxiv.org/abs/2108.01654",
          "publishedOn": "2021-08-04T01:59:21.680Z",
          "wordCount": 566,
          "title": "Comparison of modern open-source visual SLAM approaches. (arXiv:2108.01654v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Saumya Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_D/0/1/0/all/0/1\">Diplav Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_U/0/1/0/all/0/1\">Umang Chaturvedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anurag Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_G/0/1/0/all/0/1\">Gaurav Khandelwal</a>",
          "description": "The quality of images captured by smartphones is an important specification\nsince smartphones are becoming ubiquitous as primary capturing devices. The\ntraditional image signal processing (ISP) pipeline in a smartphone camera\nconsists of several image processing steps performed sequentially to\nreconstruct a high quality sRGB image from the raw sensor data. These steps\nconsist of demosaicing, denoising, white balancing, gamma correction, colour\nenhancement, etc. Since each of them are performed sequentially using\nhand-crafted algorithms, the residual error from each processing module\naccumulates in the final reconstructed signal. Thus, the traditional ISP\npipeline has limited reconstruction quality in terms of generalizability across\ndifferent lighting conditions and associated noise levels while capturing the\nimage. Deep learning methods using convolutional neural networks (CNN) have\nbecome popular in solving many image-related tasks such as image denoising,\ncontrast enhancement, super resolution, deblurring, etc. Furthermore, recent\napproaches for the RAW to sRGB conversion using deep learning methods have also\nbeen published, however, their immense complexity in terms of their memory\nrequirement and number of Mult-Adds make them unsuitable for mobile camera ISP.\nIn this paper we propose DelNet - a single end-to-end deep learning model - to\nlearn the entire ISP pipeline within reasonable complexity for smartphone\ndeployment. Del-Net is a multi-scale architecture that uses spatial and channel\nattention to capture global features like colour, as well as a series of\nlightweight modified residual attention blocks to help with denoising. For\nvalidation, we provide results to show the proposed Del-Net achieves compelling\nreconstruction quality.",
          "link": "http://arxiv.org/abs/2108.01623",
          "publishedOn": "2021-08-04T01:59:21.665Z",
          "wordCount": 685,
          "title": "Del-Net: A Single-Stage Network for Mobile Camera ISP. (arXiv:2108.01623v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bingwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mingwu Ren</a>",
          "description": "Image-based virtual try-on is challenging in fitting a target in-shop clothes\ninto a reference person under diverse human poses. Previous works focus on\npreserving clothing details ( e.g., texture, logos, patterns ) when\ntransferring desired clothes onto a target person under a fixed pose. However,\nthe performances of existing methods significantly dropped when extending\nexisting methods to multi-pose virtual try-on. In this paper, we propose an\nend-to-end Semantic Prediction Guidance multi-pose Virtual Try-On Network\n(SPG-VTON), which could fit the desired clothing into a reference person under\narbitrary poses. Concretely, SPG-VTON is composed of three sub-modules. First,\na Semantic Prediction Module (SPM) generates the desired semantic map. The\npredicted semantic map provides more abundant guidance to locate the desired\nclothes region and produce a coarse try-on image. Second, a Clothes Warping\nModule (CWM) warps in-shop clothes to the desired shape according to the\npredicted semantic map and the desired pose. Specifically, we introduce a\nconductible cycle consistency loss to alleviate the misalignment in the clothes\nwarping process. Third, a Try-on Synthesis Module (TSM) combines the coarse\nresult and the warped clothes to generate the final virtual try-on image,\npreserving details of the desired clothes and under the desired pose. Besides,\nwe introduce a face identity loss to refine the facial appearance and maintain\nthe identity of the final virtual try-on result at the same time. We evaluate\nthe proposed method on the most massive multi-pose dataset (MPV) and the\nDeepFashion dataset. The qualitative and quantitative experiments show that\nSPG-VTON is superior to the state-of-the-art methods and is robust to the data\nnoise, including background and accessory changes, i.e., hats and handbags,\nshowing good scalability to the real-world scenario.",
          "link": "http://arxiv.org/abs/2108.01578",
          "publishedOn": "2021-08-04T01:59:21.657Z",
          "wordCount": 712,
          "title": "SPG-VTON: Semantic Prediction Guidance for Multi-pose Virtual Try-on. (arXiv:2108.01578v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mansilla_L/0/1/0/all/0/1\">Lucas Mansilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Echeveste_R/0/1/0/all/0/1\">Rodrigo Echeveste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milone_D/0/1/0/all/0/1\">Diego H. Milone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrante_E/0/1/0/all/0/1\">Enzo Ferrante</a>",
          "description": "In real-life applications, machine learning models often face scenarios where\nthere is a change in data distribution between training and test domains. When\nthe aim is to make predictions on distributions different from those seen at\ntraining, we incur in a domain generalization problem. Methods to address this\nissue learn a model using data from multiple source domains, and then apply\nthis model to the unseen target domain. Our hypothesis is that when training\nwith multiple domains, conflicting gradients within each mini-batch contain\ninformation specific to the individual domains which is irrelevant to the\nothers, including the test domain. If left untouched, such disagreement may\ndegrade generalization performance. In this work, we characterize the\nconflicting gradients emerging in domain shift scenarios and devise novel\ngradient agreement strategies based on gradient surgery to alleviate their\neffect. We validate our approach in image classification tasks with three\nmulti-domain datasets, showing the value of the proposed agreement strategy in\nenhancing the generalization capability of deep learning models in domain shift\nscenarios.",
          "link": "http://arxiv.org/abs/2108.01621",
          "publishedOn": "2021-08-04T01:59:21.650Z",
          "wordCount": 614,
          "title": "Domain Generalization via Gradient Surgery. (arXiv:2108.01621v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Budka_M/0/1/0/all/0/1\">Marcin Budka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennet_M/0/1/0/all/0/1\">Matthew R. Bennet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynolds_S/0/1/0/all/0/1\">Sally Reynolds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barefoot_S/0/1/0/all/0/1\">Shelby Barefoot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reel_S/0/1/0/all/0/1\">Sarah Reel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reidy_S/0/1/0/all/0/1\">Selina Reidy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1\">Jeremy Walker</a>",
          "description": "Footprints are left, or obtained, in a variety of scenarios from crime scenes\nto anthropological investigations. Determining the sex of a footprint can be\nuseful in screening such impressions and attempts have been made to do so using\nsingle or multi landmark distances, shape analyses and via the density of\nfriction ridges. Here we explore the relative importance of different\ncomponents in sexing two-dimensional foot impressions namely, size, shape and\ntexture. We use a machine learning approach and compare this to more\ntraditional methods of discrimination. Two datasets are used, a pilot data set\ncollected from students at Bournemouth University (N=196) and a larger data set\ncollected by podiatrists at Sheffield NHS Teaching Hospital (N=2677). Our\nconvolutional neural network can sex a footprint with accuracy of around 90% on\na test set of N=267 footprint images using all image components, which is\nbetter than an expert can achieve. However, the quality of the impressions\nimpacts on this success rate, but the results are promising and in time it may\nbe possible to create an automated screening algorithm in which practitioners\nof whatever sort (medical or forensic) can obtain a first order sexing of a\ntwo-dimensional footprint.",
          "link": "http://arxiv.org/abs/2108.01554",
          "publishedOn": "2021-08-04T01:59:21.572Z",
          "wordCount": 644,
          "title": "Sexing Caucasian 2D footprints using convolutional neural networks. (arXiv:2108.01554v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01513",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yandong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>",
          "description": "State-of-the-art deep face recognition methods are mostly trained with a\nsoftmax-based multi-class classification framework. Despite being popular and\neffective, these methods still have a few shortcomings that limit empirical\nperformance. In this paper, we first identify the discrepancy between training\nand evaluation in the existing multi-class classification framework and then\ndiscuss the potential limitations caused by the \"competitive\" nature of softmax\nnormalization. Motivated by these limitations, we propose a novel binary\nclassification training framework, termed SphereFace2. In contrast to existing\nmethods, SphereFace2 circumvents the softmax normalization, as well as the\ncorresponding closed-set assumption. This effectively bridges the gap between\ntraining and evaluation, enabling the representations to be improved\nindividually by each binary classification task. Besides designing a specific\nwell-performing loss function, we summarize a few general principles for this\n\"one-vs-all\" binary classification framework so that it can outperform current\ncompetitive methods. We conduct comprehensive experiments on popular benchmarks\nto demonstrate that SphereFace2 can consistently outperform current\nstate-of-the-art deep face recognition methods.",
          "link": "http://arxiv.org/abs/2108.01513",
          "publishedOn": "2021-08-04T01:59:21.555Z",
          "wordCount": 618,
          "title": "SphereFace2: Binary Classification is All You Need for Deep Face Recognition. (arXiv:2108.01513v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bowren_J/0/1/0/all/0/1\">Joshua Bowren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Giraldo_L/0/1/0/all/0/1\">Luis Sanchez-Giraldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_O/0/1/0/all/0/1\">Odelia Schwartz</a>",
          "description": "Sparse coding has been incorporated in models of the visual cortex for its\ncomputational advantages and connection to biology. But how the level of\nsparsity contributes to performance on visual tasks is not well understood. In\nthis work, sparse coding has been integrated into an existing hierarchical V2\nmodel (Hosoya and Hyv\\\"arinen, 2015), but replacing the Independent Component\nAnalysis (ICA) with an explicit sparse coding in which the degree of sparsity\ncan be controlled. After training, the sparse coding basis functions with a\nhigher degree of sparsity resembled qualitatively different structures, such as\ncurves and corners. The contributions of the models were assessed with image\nclassification tasks, including object classification, and tasks associated\nwith mid-level vision including figure-ground classification, texture\nclassification, and angle prediction between two line stimuli. In addition, the\nmodels were assessed in comparison to a texture sensitivity measure that has\nbeen reported in V2 (Freeman et al., 2013), and a deleted-region inference\ntask. The results from the experiments show that while sparse coding performed\nworse than ICA at classifying images, only sparse coding was able to better\nmatch the texture sensitivity level of V2 and infer deleted image regions, both\nby increasing the degree of sparsity in sparse coding. Higher degrees of\nsparsity allowed for inference over larger deleted image regions. The mechanism\nthat allows for this inference capability in sparse coding is described here.",
          "link": "http://arxiv.org/abs/2108.01548",
          "publishedOn": "2021-08-04T01:59:21.548Z",
          "wordCount": 667,
          "title": "Inference via Sparse Coding in a Hierarchical Vision Model. (arXiv:2108.01548v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01516",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yaofang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyue Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_W/0/1/0/all/0/1\">Wenlong Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shaoyu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yingdi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Hu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_X/0/1/0/all/0/1\">Xueying Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qing Zhang</a>",
          "description": "Coronary angiography is the \"gold standard\" for the diagnosis of coronary\nheart disease. At present, the methods for detecting coronary artery stenoses\nand evaluating the degree of it in coronary angiograms are either subjective or\nnot efficient enough. Two vascular stenoses detection methods in coronary\nangiograms are proposed to assist the diagnosis. The first one is an automatic\nmethod, which can automatically segment the entire coronary vessels and mark\nthe stenoses. The second one is an interactive method. With this method, the\nuser only needs to give a start point and an end point to detect the stenoses\nof a certain vascular segment. We have shown that the proposed tracking methods\nare robust for angiograms with various vessel structure. The automatic\ndetection method can effectively measure the diameter of the vessel and mark\nthe stenoses in different angiograms. Further investigation proves that the\nresults of interactive detection method can accurately reflect the true\nstenoses situation. The proposed automatic method and interactive method are\neffective in various angiograms and can complement each other in clinical\npractice. The first method can be used for preliminary screening and the second\nmethod can be used for further quantitative analysis. It has the potential to\nimprove the level of clinical diagnosis of coronary heart disease.",
          "link": "http://arxiv.org/abs/2108.01516",
          "publishedOn": "2021-08-04T01:59:21.541Z",
          "wordCount": 668,
          "title": "Two New Stenoses Detection Methods of Coronary Angiograms. (arXiv:2108.01516v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bowen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zitong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuelin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>",
          "description": "Weakly-supervised object detection (WSOD) has emerged as an inspiring recent\ntopic to avoid expensive instance-level object annotations. However, the\nbounding boxes of most existing WSOD methods are mainly determined by\nprecomputed proposals, thereby being limited in precise object localization. In\nthis paper, we defend the problem setting for improving localization\nperformance by leveraging the bounding box regression knowledge from a\nwell-annotated auxiliary dataset. First, we use the well-annotated auxiliary\ndataset to explore a series of learnable bounding box adjusters (LBBAs) in a\nmulti-stage training manner, which is class-agnostic. Then, only LBBAs and a\nweakly-annotated dataset with non-overlapped classes are used for training\nLBBA-boosted WSOD. As such, our LBBAs are practically more convenient and\neconomical to implement while avoiding the leakage of the auxiliary\nwell-annotated dataset. In particular, we formulate learning bounding box\nadjusters as a bi-level optimization problem and suggest an EM-like multi-stage\ntraining algorithm. Then, a multi-stage scheme is further presented for\nLBBA-boosted WSOD. Additionally, a masking strategy is adopted to improve\nproposal classification. Experimental results verify the effectiveness of our\nmethod. Our method performs favorably against state-of-the-art WSOD methods and\nknowledge transfer model with similar problem setting. Code is publicly\navailable at \\url{https://github.com/DongSky/lbba_boosted_wsod}.",
          "link": "http://arxiv.org/abs/2108.01499",
          "publishedOn": "2021-08-04T01:59:21.422Z",
          "wordCount": 644,
          "title": "Boosting Weakly Supervised Object Detection via Learning Bounding Box Adjusters. (arXiv:2108.01499v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1\">Gustavo Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>",
          "description": "We consider the problem of adapting a network trained on three-channel color\nimages to a hyperspectral domain with a large number of channels. To this end,\nwe propose domain adaptor networks that map the input to be compatible with a\nnetwork trained on large-scale color image datasets such as ImageNet. Adaptors\nenable learning on small hyperspectral datasets where training a network from\nscratch may not be effective. We investigate architectures and strategies for\ntraining adaptors and evaluate them on a benchmark consisting of multiple\nhyperspectral datasets. We find that simple schemes such as linear projection\nor subset selection are often the most effective, but can lead to a loss in\nperformance in some cases. We also propose a novel multi-view adaptor where of\nthe inputs are combined in an intermediate layer of the network in an order\ninvariant manner that provides further improvements. We present extensive\nexperiments by varying the number of training examples in the benchmark to\ncharacterize the accuracy and computational trade-offs offered by these\nadaptors.",
          "link": "http://arxiv.org/abs/2108.01555",
          "publishedOn": "2021-08-04T01:59:21.404Z",
          "wordCount": 597,
          "title": "Domain Adaptor Networks for Hyperspectral Image Recognition. (arXiv:2108.01555v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01434",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dai_T/0/1/0/all/0/1\">Tianhong Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_X/0/1/0/all/0/1\">Xilei Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_X/0/1/0/all/0/1\">Xu Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Youliang Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_S/0/1/0/all/0/1\">Shanxin Yuan</a>",
          "description": "High dynamic range (HDR) imaging from multiple low dynamic range (LDR) images\nhas been suffering from ghosting artifacts caused by scene and objects motion.\nExisting methods, such as optical flow based and end-to-end deep learning based\nsolutions, are error-prone either in detail restoration or ghosting artifacts\nremoval. Comprehensive empirical evidence shows that ghosting artifacts caused\nby large foreground motion are mainly low-frequency signals and the details are\nmainly high-frequency signals. In this work, we propose a novel\nfrequency-guided end-to-end deep neural network (FHDRNet) to conduct HDR fusion\nin the frequency domain, and Discrete Wavelet Transform (DWT) is used to\ndecompose inputs into different frequency bands. The low-frequency signals are\nused to avoid specific ghosting artifacts, while the high-frequency signals are\nused for preserving details. Using a U-Net as the backbone, we propose two\nnovel modules: merging module and frequency-guided upsampling module. The\nmerging module applies the attention mechanism to the low-frequency components\nto deal with the ghost caused by large foreground motion. The frequency-guided\nupsampling module reconstructs details from multiple frequency-specific\ncomponents with rich details. In addition, a new RAW dataset is created for\ntraining and evaluating multi-frame HDR imaging algorithms in the RAW domain.\nExtensive experiments are conducted on public datasets and our RAW dataset,\nshowing that the proposed FHDRNet achieves state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2108.01434",
          "publishedOn": "2021-08-04T01:59:21.386Z",
          "wordCount": 665,
          "title": "Wavelet-Based Network For High Dynamic Range Imaging. (arXiv:2108.01434v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01518",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dianhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1\">Ngo Anh Vien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Van_M/0/1/0/all/0/1\">Mien Van</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLoone_S/0/1/0/all/0/1\">Sean McLoone</a>",
          "description": "3D skeleton-based motion prediction and activity recognition are two\ninterwoven tasks in human behaviour analysis. In this work, we propose a motion\ncontext modeling methodology that provides a new way to combine the advantages\nof both graph convolutional neural networks and recurrent neural networks for\njoint human motion prediction and activity recognition. Our approach is based\non using an LSTM encoder-decoder and a non-local feature extraction attention\nmechanism to model the spatial correlation of human skeleton data and temporal\ncorrelation among motion frames. The proposed network can easily include two\noutput branches, one for Activity Recognition and one for Future Motion\nPrediction, which can be jointly trained for enhanced performance. Experimental\nresults on Human 3.6M, CMU Mocap and NTU RGB-D datasets show that our proposed\napproach provides the best prediction capability among baseline LSTM-based\nmethods, while achieving comparable performance to other state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2108.01518",
          "publishedOn": "2021-08-04T01:59:21.379Z",
          "wordCount": 598,
          "title": "Non-local Graph Convolutional Network for joint Activity Recognition and Motion Prediction. (arXiv:2108.01518v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Al_Hindawi_A/0/1/0/all/0/1\">Ahmed Al-Hindawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vizcaychipi_M/0/1/0/all/0/1\">Marcela Paula Vizcaychipi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demiris_Y/0/1/0/all/0/1\">Yiannis Demiris</a>",
          "description": "Delirium, an acute confusional state, is a common occurrence in Intensive\nCare Units (ICUs). Patients who develop delirium have globally worse outcomes\nthan those who do not and thus the diagnosis of delirium is of importance.\nCurrent diagnostic methods have several limitations leading to the suggestion\nof eye-tracking for its diagnosis through in-attention. To ascertain the\nrequirements for an eye-tracking system in an adult ICU, measurements were\ncarried out at Chelsea & Westminster Hospital NHS Foundation Trust. Clinical\ncriteria guided empirical requirements of invasiveness and calibration methods\nwhile accuracy and precision were measured. A non-invasive system was then\ndeveloped utilising a patient-facing RGB-camera and a scene-facing RGBD-camera.\nThe system's performance was measured in a replicated laboratory environment\nwith healthy volunteers revealing an accuracy and precision that outperforms\nwhat is required while simultaneously being non-invasive and calibration-free\nThe system was then deployed as part CONfuSED, a clinical feasibility study\nwhere we report aggregated data from 5 patients as well as the acceptability of\nthe system to bedside nursing staff. The system is the first eye-tracking\nsystem to be deployed in an ICU.",
          "link": "http://arxiv.org/abs/2108.01439",
          "publishedOn": "2021-08-04T01:59:21.371Z",
          "wordCount": 621,
          "title": "Continuous Non-Invasive Eye Tracking In Intensive Care. (arXiv:2108.01439v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01389",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kuijf_H/0/1/0/all/0/1\">Hugo J. Kuijf</a>",
          "description": "Cerebral microbleeds are small hypointense lesions visible on magnetic\nresonance imaging (MRI) with gradient echo, T2*, or susceptibility weighted\n(SWI) imaging. Assessment of cerebral microbleeds is mostly performed by visual\ninspection. The past decade has seen the rise of semi-automatic tools to assist\nwith rating and more recently fully automatic tools for microbleed detection.\nIn this work, we explore the use of nnU-Net as a fully automated tool for\nmicrobleed segmentation. Data was provided by the ``Where is VALDO?'' challenge\nof MICCAI 2021. The final method consists of nnU-Net in the ``3D full\nresolution U-Net'' configuration trained on all data (fold = `all'). No\npost-processing options of nnU-Net were used. Self-evaluation on the training\ndata showed an estimated Dice of 0.80, false discovery rate of 0.16, and false\nnegative rate of 0.15. Final evaluation on the test set of the VALDO challenge\nis pending. Visual inspection of the results showed that most of the reported\nfalse positives could be an actual microbleed that might have been missed\nduring visual rating. Source code is available at:\nhttps://github.com/hjkuijf/MixMicrobleedNet . The docker container\nhjkuijf/mixmicrobleednet can be pulled from\nhttps://hub.docker.com/r/hjkuijf/mixmicrobleednet .",
          "link": "http://arxiv.org/abs/2108.01389",
          "publishedOn": "2021-08-04T01:59:21.339Z",
          "wordCount": 638,
          "title": "MixMicrobleedNet: segmentation of cerebral microbleeds using nnU-Net. (arXiv:2108.01389v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Han Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhanning Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Peiran Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lizhen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "Noisy labels are commonly found in real-world data, which cause performance\ndegradation of deep neural networks. Cleaning data manually is labour-intensive\nand time-consuming. Previous research mostly focuses on enhancing\nclassification models against noisy labels, while the robustness of deep metric\nlearning (DML) against noisy labels remains less well-explored. In this paper,\nwe bridge this important gap by proposing Probabilistic Ranking-based Instance\nSelection with Memory (PRISM) approach for DML. PRISM calculates the\nprobability of a label being clean, and filters out potentially noisy samples.\nSpecifically, we propose three methods to calculate this probability: 1)\nAverage Similarity Method (AvgSim), which calculates the average similarity\nbetween potentially noisy data and clean data; 2) Proxy Similarity Method\n(ProxySim), which replaces the centers maintained by AvgSim with the proxies\ntrained by proxy-based method; and 3) von Mises-Fisher Distribution Similarity\n(vMF-Sim), which estimates a von Mises-Fisher distribution for each data class.\nWith such a design, the proposed approach can deal with challenging DML\nsituations in which the majority of the samples are noisy. Extensive\nexperiments on both synthetic and real-world noisy dataset show that the\nproposed approach achieves up to 8.37% higher Precision@1 compared with the\nbest performing state-of-the-art baseline approaches, within reasonable\ntraining time.",
          "link": "http://arxiv.org/abs/2108.01431",
          "publishedOn": "2021-08-04T01:59:21.333Z",
          "wordCount": 657,
          "title": "Noise-Resistant Deep Metric Learning with Probabilistic Instance Filtering. (arXiv:2108.01431v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01405",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Valverde_J/0/1/0/all/0/1\">Juan Miguel Valverde</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tohka_J/0/1/0/all/0/1\">Jussi Tohka</a>",
          "description": "We propose Region-wise (RW) loss for biomedical image segmentation.\nRegion-wise loss is versatile, can simultaneously account for class imbalance\nand pixel importance, and it can be easily implemented as the pixel-wise\nmultiplication between the softmax output and a RW map. We show that, under the\nproposed Region-wise loss framework, certain loss functions, such as Active\nContour and Boundary loss, can be reformulated similarly with appropriate RW\nmaps, thus revealing their underlying similarities and a new perspective to\nunderstand these loss functions. We investigate the observed optimization\ninstability caused by certain RW maps, such as Boundary loss distance maps, and\nwe introduce a mathematically-grounded principle to avoid such instability.\nThis principle provides excellent adaptability to any dataset and practically\nensures convergence without extra regularization terms or optimization tricks.\nFollowing this principle, we propose a simple version of boundary distance maps\ncalled rectified RW maps that, as we demonstrate in our experiments, achieve\nstate-of-the-art performance with similar or better Dice coefficients and\nHausdorff distances than Dice, Focal, and Boundary losses in three distinct\nsegmentation tasks. We quantify the optimization instability provided by\nBoundary loss distance maps, and we empirically show that our rectified RW maps\nare stable to optimize. The code to run all our experiments is publicly\navailable at: https://github.com/jmlipman/RegionWiseLoss.",
          "link": "http://arxiv.org/abs/2108.01405",
          "publishedOn": "2021-08-04T01:59:21.278Z",
          "wordCount": 650,
          "title": "Region-wise Loss for Biomedical Image Segmentation. (arXiv:2108.01405v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kostiuk_I/0/1/0/all/0/1\">Ivan Kostiuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stachura_P/0/1/0/all/0/1\">Przemys&#x142;aw Stachura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadeja_S/0/1/0/all/0/1\">S&#x142;awomir K. Tadeja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spurek_P/0/1/0/all/0/1\">Przemys&#x142;aw Spurek</a>",
          "description": "Designing a 3D game scene is a tedious task that often requires a substantial\namount of work. Typically, this task involves synthesis, coloring, and\nplacement of 3D models within the game scene. To lessen this workload, we can\napply machine learning to automate some aspects of the game scene development.\nEarlier research has already tackled automated generation of the game scene\nbackground with machine learning. However, model auto-coloring remains an\nunderexplored problem. The automatic coloring of a 3D model is a challenging\ntask, especially when dealing with the digital representation of a colorful,\nmultipart object. In such a case, we have to ``understand'' the object's\ncomposition and coloring scheme of each part. Existing single-stage methods\nhave their own caveats such as the need for segmentation of the object or\ngenerating individual parts that have to be assembled together to yield the\nfinal model. We address these limitations by proposing a two-stage training\napproach to synthesize auto-colored 3D models. In the first stage, we obtain a\n3D point cloud representing a 3D object, whilst in the second stage, we assign\ncolors to points within such cloud. Next, by leveraging the so-called\ntriangulation trick, we generate a 3D mesh in which the surfaces are colored\nbased on interpolation of colored points representing vertices of a given mesh\ntriangle. This approach allows us to generate a smooth coloring scheme.\nExperimental evaluation shows that our two-stage approach gives better results\nin terms of shape reconstruction and coloring when compared to traditional\nsingle-stage techniques.",
          "link": "http://arxiv.org/abs/2108.01411",
          "publishedOn": "2021-08-04T01:59:21.271Z",
          "wordCount": 701,
          "title": "HyperColor: A HyperNetwork Approach for Synthesizing Auto-colored 3D Models for Game Scenes Population. (arXiv:2108.01411v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+behera_S/0/1/0/all/0/1\">Saswati kumari behera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Y_A/0/1/0/all/0/1\">Aouthithiye Barathwaj SR Y</a>, <a href=\"http://arxiv.org/find/cs/1/au:+L_V/0/1/0/all/0/1\">Vasundhara L</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_S/0/1/0/all/0/1\">Saisudha G</a>, <a href=\"http://arxiv.org/find/cs/1/au:+C_H/0/1/0/all/0/1\">Haariharan N C</a>",
          "description": "Waste management is a certainly a very complex and difficult process\nespecially in very large cities. It needs immense man power and also uses up\nother resources such as electricity and fuel. This creates a need to use a\nnovel method with help of latest technologies. Here in this article we present\na new waste classification technique using Computer Vision (CV) and deep\nlearning (DL). To further improve waste classification ability, support machine\nvectors (SVM) are used. We also decompose the degradable waste with help of\nrapid composting. In this article we have mainly worked on segregation of\nmunicipal solid waste (MSW). For this model, we use YOLOv3 (You Only Look Once)\na computer vision-based algorithm popularly used to detect objects which is\ndeveloped based on Convolution Neural Networks (CNNs) which is a machine\nlearning (ML) based tool. They are extensively used to extract features from a\ndata especially image-oriented data. In this article we propose a waste\nclassification technique which will be faster and more efficient. And we\ndecompose the biodegradable waste by Berkley Method of composting (BKC)",
          "link": "http://arxiv.org/abs/2108.01394",
          "publishedOn": "2021-08-04T01:59:21.263Z",
          "wordCount": 628,
          "title": "AI Based Waste classifier with Thermo-Rapid Composting. (arXiv:2108.01394v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yifan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhijie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengdan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1\">Kekai Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Weiming Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>",
          "description": "Vision transformers have recently received explosive popularity, but huge\ncomputational cost is still a severe issue. Recent efficient designs for vision\ntransformers follow two pipelines, namely, structural compression based on\nlocal spatial prior and non-structural token pruning. However, rough token\npruning breaks the spatial structure that is indispensable for local spatial\nprior. To take advantage of both two pipelines, this work seeks to dynamically\nidentify uninformative tokens for each instance and trim down both the training\nand inference complexity while maintain complete spatial structure and\ninformation flow. To achieve this goal, we propose Evo-ViT, a self-motivated\nslow-fast token evolution method for vision transformers. Specifically, we\nconduct unstructured instance-wise token selection by taking advantage of the\nglobal class attention that is unique to vision transformers. Then, we propose\nto update information tokens and placeholder tokens that contribute little to\nthe final prediction with different computational properties, namely, slow-fast\nupdating. Thanks to the slow-fast updating mechanism that guarantees\ninformation flow and spatial structure, our Evo-ViT can accelerate vanilla\ntransformers of both flat and deep-narrow structures from the very beginning of\nthe training process. Experimental results demonstrate that the proposed method\ncan significantly reduce the computational costs of vision transformers while\nmaintaining comparable performance on image classification. For example, our\nmethod accelerates DeiT-S by over 60% throughput while only sacrificing 0.4%\ntop-1 accuracy.",
          "link": "http://arxiv.org/abs/2108.01390",
          "publishedOn": "2021-08-04T01:59:21.256Z",
          "wordCount": 682,
          "title": "Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer. (arXiv:2108.01390v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1\">Amartya Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbhuiya_F/0/1/0/all/0/1\">Ferdous Ahmed Barbhuiya</a>",
          "description": "The current work deals with the problem of attempting to predict the\npopularity of images before even being uploaded. This method is specifically\nfocused on Flickr images. Social features of each image as well as that of the\nuser who had uploaded it, have been recorded. The dataset also includes the\nengagement score of each image which is the ground truth value of the views\nobtained by each image over a period of 30 days. The work aims to predict the\npopularity of images on Flickr over a period of 30 days using the social\nfeatures of the user and the image, as well as the visual features of the\nimages. The method states that the engagement sequence of an image can be said\nto depend on two independent quantities, namely scale and shape of an image.\nOnce the shape and scale of an image have been predicted, combining them the\npredicted sequence of an image over 30 days is obtained. The current work\nfollows a previous work done in the same direction, with certain speculations\nand suggestions of improvement.",
          "link": "http://arxiv.org/abs/2108.01326",
          "publishedOn": "2021-08-04T01:59:21.241Z",
          "wordCount": 628,
          "title": "Predicting Popularity of Images Over 30 Days. (arXiv:2108.01326v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miron_A/0/1/0/all/0/1\">Alina Miron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosan_C/0/1/0/all/0/1\">Crina Grosan</a>",
          "description": "The work in this paper focuses on the role of machine learning in assessing\nthe correctness of a human motion or action. This task proves to be more\nchallenging than the gesture and action recognition ones. We will demonstrate,\nthrough a set of experiments on a recent dataset, that machine learning\nalgorithms can produce good results for certain actions, but can also fall into\nthe trap of classifying an incorrect execution of an action as a correct\nexecution of another action.",
          "link": "http://arxiv.org/abs/2108.01375",
          "publishedOn": "2021-08-04T01:59:21.235Z",
          "wordCount": 530,
          "title": "Classifying action correctness in physical rehabilitation exercises. (arXiv:2108.01375v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zelin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Peng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Licheng Jiao</a>",
          "description": "Semantic segmentation has been continuously investigated in the last ten\nyears, and majority of the established technologies are based on supervised\nmodels. In recent years, image-level weakly supervised semantic segmentation\n(WSSS), including single- and multi-stage process, has attracted large\nattention due to data labeling efficiency. In this paper, we propose to embed\naffinity learning of multi-stage approaches in a single-stage model. To be\nspecific, we introduce an adaptive affinity loss to thoroughly learn the local\npairwise affinity. As such, a deep neural network is used to deliver\ncomprehensive semantic information in the training phase, whilst improving the\nperformance of the final prediction module. On the other hand, considering the\nexistence of errors in the pseudo labels, we propose a novel label reassign\nloss to mitigate over-fitting. Extensive experiments are conducted on the\nPASCAL VOC 2012 dataset to evaluate the effectiveness of our proposed approach\nthat outperforms other standard single-stage methods and achieves comparable\nperformance against several multi-stage methods.",
          "link": "http://arxiv.org/abs/2108.01344",
          "publishedOn": "2021-08-04T01:59:21.228Z",
          "wordCount": 607,
          "title": "Adaptive Affinity Loss and Erroneous Pseudo-Label Refinement for Weakly Supervised Semantic Segmentation. (arXiv:2108.01344v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01361",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C. H. Hoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "This paper investigates an open research task of text-to-image synthesis for\nautomatically generating or manipulating images from text descriptions.\nPrevailing methods mainly use the text as conditions for GAN generation, and\ntrain different models for the text-guided image generation and manipulation\ntasks. In this paper, we propose a novel unified framework of Cycle-consistent\nInverse GAN (CI-GAN) for both text-to-image generation and text-guided image\nmanipulation tasks. Specifically, we first train a GAN model without text\ninput, aiming to generate images with high diversity and quality. Then we learn\na GAN inversion model to convert the images back to the GAN latent space and\nobtain the inverted latent codes for each image, where we introduce the\ncycle-consistency training to learn more robust and consistent inverted latent\ncodes. We further uncover the latent space semantics of the trained GAN model,\nby learning a similarity model between text representations and the latent\ncodes. In the text-guided optimization module, we generate images with the\ndesired semantic attributes by optimizing the inverted latent codes. Extensive\nexperiments on the Recipe1M and CUB datasets validate the efficacy of our\nproposed framework.",
          "link": "http://arxiv.org/abs/2108.01361",
          "publishedOn": "2021-08-04T01:59:21.213Z",
          "wordCount": 623,
          "title": "Cycle-Consistent Inverse GAN for Text-to-Image Synthesis. (arXiv:2108.01361v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Levin_R/0/1/0/all/0/1\">Roman Levin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1\">Manli Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgnia_E/0/1/0/all/0/1\">Eitan Borgnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "Conventional saliency maps highlight input features to which neural network\npredictions are highly sensitive. We take a different approach to saliency, in\nwhich we identify and analyze the network parameters, rather than inputs, which\nare responsible for erroneous decisions. We find that samples which cause\nsimilar parameters to malfunction are semantically similar. We also show that\npruning the most salient parameters for a wrongly classified sample often\nimproves model behavior. Furthermore, fine-tuning a small number of the most\nsalient parameters on a single sample results in error correction on other\nsamples that are misclassified for similar reasons. Based on our parameter\nsaliency method, we also introduce an input-space saliency technique that\nreveals how image features cause specific network components to malfunction.\nFurther, we rigorously validate the meaningfulness of our saliency maps on both\nthe dataset and case-study levels.",
          "link": "http://arxiv.org/abs/2108.01335",
          "publishedOn": "2021-08-04T01:59:21.206Z",
          "wordCount": 586,
          "title": "Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability. (arXiv:2108.01335v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alsawadi_M/0/1/0/all/0/1\">Motasem S. Alsawadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rio_M/0/1/0/all/0/1\">Miguel Rio</a>",
          "description": "A skeleton representation of the human body has been proven to be effective\nfor this task. The skeletons are presented in graphs form-like. However, the\ntopology of a graph is not structured like Euclidean-based data. Therefore, a\nnew set of methods to perform the convolution operation upon the skeleton graph\nis presented. Our proposal is based upon the ST-GCN framework proposed by Yan\net al. [1]. In this study, we present an improved set of label mapping methods\nfor the ST-GCN framework. We introduce three split processes (full distance\nsplit, connection split, and index split) as an alternative approach for the\nconvolution operation. To evaluate the performance, the experiments presented\nin this study have been trained using two benchmark datasets: NTU-RGB+D and\nKinetics. Our results indicate that all of our split processes outperform the\nprevious partition strategies and are more stable during training without using\nthe edge importance weighting additional training parameter. Therefore, our\nproposal can provide a more realistic solution for real-time applications\ncentred on daily living recognition systems activities for indoor environments.",
          "link": "http://arxiv.org/abs/2108.01309",
          "publishedOn": "2021-08-04T01:59:21.185Z",
          "wordCount": 618,
          "title": "Skeleton Split Strategies for Spatial Temporal Graph Convolution Networks. (arXiv:2108.01309v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01286",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1\">Shikui Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lei Xu</a>",
          "description": "Current face recognition tasks are usually carried out on high-quality face\nimages, but in reality, most face images are captured under unconstrained or\npoor conditions, e.g., by video surveillance. Existing methods are featured by\nlearning data uncertainty to avoid overfitting the noise, or by adding margins\nto the angle or cosine space of the normalized softmax loss to penalize the\ntarget logit, which enforces intra-class compactness and inter-class\ndiscrepancy. In this paper, we propose a deep Rival Penalized Competitive\nLearning (RPCL) for deep face recognition in low-resolution (LR) images.\nInspired by the idea of the RPCL, our method further enforces regulation on the\nrival logit, which is defined as the largest non-target logit for an input\nimage. Different from existing methods that only consider penalization on the\ntarget logit, our method not only strengthens the learning towards the target\nlabel, but also enforces a reverse direction, i.e., becoming de-learning, away\nfrom the rival label. Comprehensive experiments demonstrate that our method\nimproves the existing state-of-the-art methods to be very robust for LR face\nrecognition.",
          "link": "http://arxiv.org/abs/2108.01286",
          "publishedOn": "2021-08-04T01:59:21.169Z",
          "wordCount": 614,
          "title": "Deep Rival Penalized Competitive Learning for Low-resolution Face Recognition. (arXiv:2108.01286v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jooyoung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungbeom Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yonghyun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "Recent image generation models show remarkable generation performance.\nHowever, they mirror strong location preference in datasets, which we call\nspatial bias. Therefore, generators render poor samples at unseen locations and\nscales. We argue that the generators rely on their implicit positional encoding\nto render spatial content. From our observations, the generator's implicit\npositional encoding is translation-variant, making the generator spatially\nbiased. To address this issue, we propose injecting explicit positional\nencoding at each scale of the generator. By learning the spatially unbiased\ngenerator, we facilitate the robust use of generators in multiple tasks, such\nas GAN inversion, multi-scale generation, generation of arbitrary sizes and\naspect ratios. Furthermore, we show that our method can also be applied to\ndenoising diffusion probabilistic models.",
          "link": "http://arxiv.org/abs/2108.01285",
          "publishedOn": "2021-08-04T01:59:21.163Z",
          "wordCount": 554,
          "title": "Toward Spatially Unbiased Generative Models. (arXiv:2108.01285v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hengbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malla_S/0/1/0/all/0/1\">Srikanth Malla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1\">Masayoshi Tomizuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1\">Chiho Choi</a>",
          "description": "Motion forecasting plays a significant role in various domains (e.g.,\nautonomous driving, human-robot interaction), which aims to predict future\nmotion sequences given a set of historical observations. However, the observed\nelements may be of different levels of importance. Some information may be\nirrelevant or even distracting to the forecasting in certain situations. To\naddress this issue, we propose a generic motion forecasting framework (named\nRAIN) with dynamic key information selection and ranking based on a hybrid\nattention mechanism. The general framework is instantiated to handle\nmulti-agent trajectory prediction and human motion forecasting tasks,\nrespectively. In the former task, the model learns to recognize the relations\nbetween agents with a graph representation and to determine their relative\nsignificance. In the latter task, the model learns to capture the temporal\nproximity and dependency in long-term human motions. We also propose an\neffective double-stage training pipeline with an alternating training strategy\nto optimize the parameters in different modules of the framework. We validate\nthe framework on both synthetic simulations and motion forecasting benchmarks\nin different domains, demonstrating that our method not only achieves\nstate-of-the-art forecasting performance, but also provides interpretable and\nreasonable hybrid attention weights.",
          "link": "http://arxiv.org/abs/2108.01316",
          "publishedOn": "2021-08-04T01:59:21.152Z",
          "wordCount": 657,
          "title": "RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting. (arXiv:2108.01316v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jimin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>",
          "description": "We focus on tackling weakly supervised semantic segmentation with\nscribble-level annotation. The regularized loss has been proven to be an\neffective solution for this task. However, most existing regularized losses\nonly leverage static shallow features (color, spatial information) to compute\nthe regularized kernel, which limits its final performance since such static\nshallow features fail to describe pair-wise pixel relationship in complicated\ncases. In this paper, we propose a new regularized loss which utilizes both\nshallow and deep features that are dynamically updated in order to aggregate\nsufficient information to represent the relationship of different pixels.\nMoreover, in order to provide accurate deep features, we adopt vision\ntransformer as the backbone and design a feature consistency head to train the\npair-wise feature relationship. Unlike most approaches that adopt multi-stage\ntraining strategy with many bells and whistles, our approach can be directly\ntrained in an end-to-end manner, in which the feature consistency head and our\nregularized loss can benefit from each other. Extensive experiments show that\nour approach achieves new state-of-the-art performances, outperforming other\napproaches by a significant margin with more than 6\\% mIoU increase.",
          "link": "http://arxiv.org/abs/2108.01296",
          "publishedOn": "2021-08-04T01:59:21.133Z",
          "wordCount": 622,
          "title": "Dynamic Feature Regularized Loss for Weakly Supervised Semantic Segmentation. (arXiv:2108.01296v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schperberg_A/0/1/0/all/0/1\">Alexander Schperberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsuei_S/0/1/0/all/0/1\">Stephanie Tsuei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Dennis Hong</a>",
          "description": "We present an end-to-end online motion planning framework that uses a\ndata-driven approach to navigate a heterogeneous robot team towards a global\ngoal while avoiding obstacles in uncertain environments. First, we use\nstochastic model predictive control (SMPC) to calculate control inputs that\nsatisfy robot dynamics, and consider uncertainty during obstacle avoidance with\nchance constraints. Second, recurrent neural networks are used to provide a\nquick estimate of future state uncertainty considered in the SMPC finite-time\nhorizon solution, which are trained on uncertainty outputs of various\nsimultaneous localization and mapping algorithms. When two or more robots are\nin communication range, these uncertainties are then updated using a\ndistributed Kalman filtering approach. Lastly, a Deep Q-learning agent is\nemployed to serve as a high-level path planner, providing the SMPC with target\npositions that move the robots towards a desired global goal. Our complete\nmethods are demonstrated on a ground and aerial robot simultaneously (code\navailable at: https://github.com/AlexS28/SABER).",
          "link": "http://arxiv.org/abs/2108.01262",
          "publishedOn": "2021-08-04T01:59:21.121Z",
          "wordCount": 630,
          "title": "SABER: Data-Driven Motion Planner for Autonomously Navigating Heterogeneous Robots. (arXiv:2108.01262v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1\">Matias Valdenegro-Toro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preciado_Grijalva_A/0/1/0/all/0/1\">Alan Preciado-Grijalva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wehbe_B/0/1/0/all/0/1\">Bilal Wehbe</a>",
          "description": "Machine learning and neural networks are now ubiquitous in sonar perception,\nbut it lags behind the computer vision field due to the lack of data and\npre-trained models specifically for sonar images. In this paper we present the\nMarine Debris Turntable dataset and produce pre-trained neural networks trained\non this dataset, meant to fill the gap of missing pre-trained models for sonar\nimages. We train Resnet 20, MobileNets, DenseNet121, SqueezeNet, MiniXception,\nand an Autoencoder, over several input image sizes, from 32 x 32 to 96 x 96, on\nthe Marine Debris turntable dataset. We evaluate these models using transfer\nlearning for low-shot classification in the Marine Debris Watertank and another\ndataset captured using a Gemini 720i sonar. Our results show that in both\ndatasets the pre-trained models produce good features that allow good\nclassification accuracy with low samples (10-30 samples per class). The Gemini\ndataset validates that the features transfer to other kinds of sonar sensors.\nWe expect that the community benefits from the public release of our\npre-trained models and the turntable dataset.",
          "link": "http://arxiv.org/abs/2108.01111",
          "publishedOn": "2021-08-04T01:59:21.106Z",
          "wordCount": 618,
          "title": "Pre-trained Models for Sonar Images. (arXiv:2108.01111v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01234",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majchrowska_S/0/1/0/all/0/1\">Sylwia Majchrowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawlowski_J/0/1/0/all/0/1\">Jaros&#x142;aw Paw&#x142;owski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gula_G/0/1/0/all/0/1\">Grzegorz Gu&#x142;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonus_T/0/1/0/all/0/1\">Tomasz Bonus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanas_A/0/1/0/all/0/1\">Agata Hanas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loch_A/0/1/0/all/0/1\">Adam Loch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawlak_A/0/1/0/all/0/1\">Agnieszka Pawlak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roszkowiak_J/0/1/0/all/0/1\">Justyna Roszkowiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golan_T/0/1/0/all/0/1\">Tomasz Golan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drulis_Kawa_Z/0/1/0/all/0/1\">Zuzanna Drulis-Kawa</a>",
          "description": "The Annotated Germs for Automated Recognition (AGAR) dataset is an image\ndatabase of microbial colonies cultured on agar plates. It contains 18000\nphotos of five different microorganisms as single or mixed cultures, taken\nunder diverse lighting conditions with two different cameras. All the images\nare classified into \"countable\", \"uncountable\", and \"empty\", with the\n\"countable\" class labeled by microbiologists with colony location and species\nidentification (336442 colonies in total). This study describes the dataset\nitself and the process of its development. In the second part, the performance\nof selected deep neural network architectures for object detection, namely\nFaster R-CNN and Cascade R-CNN, was evaluated on the AGAR dataset. The results\nconfirmed the great potential of deep learning methods to automate the process\nof microbe localization and classification based on Petri dish photos.\nMoreover, AGAR is the first publicly available dataset of this kind and size\nand will facilitate the future development of machine learning models. The data\nused in these studies can be found at https://agar.neurosys.com/.",
          "link": "http://arxiv.org/abs/2108.01234",
          "publishedOn": "2021-08-04T01:59:21.038Z",
          "wordCount": 637,
          "title": "AGAR a microbial colony dataset for deep learning detection. (arXiv:2108.01234v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huayan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaofei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junfeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1\">Tin Lun Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijayakumar_S/0/1/0/all/0/1\">Sethu Vijayakumar</a>",
          "description": "Dynamic objects in the environment, such as people and other agents, lead to\nchallenges for existing simultaneous localization and mapping (SLAM)\napproaches. To deal with dynamic environments, computer vision researchers\nusually apply some learning-based object detectors to remove these dynamic\nobjects. However, these object detectors are computationally too expensive for\nmobile robot on-board processing. In practical applications, these objects\noutput noisy sounds that can be effectively detected by on-board sound source\nlocalization. The directional information of the sound source object can be\nefficiently obtained by direction of sound arrival (DoA) estimation, but depth\nestimation is difficult. Therefore, in this paper, we propose a novel\naudio-visual fusion approach that fuses sound source direction into the RGB-D\nimage and thus removes the effect of dynamic obstacles on the multi-robot SLAM\nsystem. Experimental results of multi-robot SLAM in different dynamic\nenvironments show that the proposed method uses very small computational\nresources to obtain very stable self-localization results.",
          "link": "http://arxiv.org/abs/2108.01246",
          "publishedOn": "2021-08-04T01:59:21.031Z",
          "wordCount": 603,
          "title": "AcousticFusion: Fusing Sound Source Localization to Visual SLAM in Dynamic Environments. (arXiv:2108.01246v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1\">Ron Shmelkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1\">Tomer Friedlander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "A master face is a face image that passes face-based identity-authentication\nfor a large portion of the population. These faces can be used to impersonate,\nwith a high probability of success, any user, without having access to any user\ninformation. We optimize these faces, by using an evolutionary algorithm in the\nlatent embedding space of the StyleGAN face generator. Multiple evolutionary\nstrategies are compared, and we propose a novel approach that employs a neural\nnetwork in order to direct the search in the direction of promising samples,\nwithout adding fitness evaluations. The results we present demonstrate that it\nis possible to obtain a high coverage of the population (over 40%) with less\nthan 10 master faces, for three leading deep face recognition systems.",
          "link": "http://arxiv.org/abs/2108.01077",
          "publishedOn": "2021-08-04T01:59:21.024Z",
          "wordCount": 590,
          "title": "Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution. (arXiv:2108.01077v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01224",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>",
          "description": "We study a new challenging problem of efficient deployment for diverse tasks\nwith different resources, where the resource constraint and task of interest\ncorresponding to a group of classes are dynamically specified at testing time.\nPrevious NAS approaches seek to design architectures for all classes\nsimultaneously, which may not be optimal for some individual tasks. A\nstraightforward solution is to search an architecture from scratch for each\ndeployment scenario, which however is computation-intensive and impractical. To\naddress this, we present a novel and general framework, called Elastic\nArchitecture Search (EAS), permitting instant specializations at runtime for\ndiverse tasks with various resource constraints. To this end, we first propose\nto effectively train the over-parameterized network via a task dropout strategy\nto disentangle the tasks during training. In this way, the resulting model is\nrobust to the subsequent task dropping at inference time. Based on the\nwell-trained over-parameterized network, we then propose an efficient\narchitecture generator to obtain optimal architectures within a single forward\npass. Experiments on two image classification datasets show that EAS is able to\nfind more compact networks with better performance while remarkably being\norders of magnitude faster than state-of-the-art NAS methods. For example, our\nproposed EAS finds compact architectures within 0.1 second for 50 deployment\nscenarios.",
          "link": "http://arxiv.org/abs/2108.01224",
          "publishedOn": "2021-08-04T01:59:21.017Z",
          "wordCount": 660,
          "title": "Elastic Architecture Search for Diverse Tasks with Different Resources. (arXiv:2108.01224v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhoutong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_F/0/1/0/all/0/1\">Forrester Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_R/0/1/0/all/0/1\">Richard Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1\">William T. Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dekel_T/0/1/0/all/0/1\">Tali Dekel</a>",
          "description": "We present a method to estimate depth of a dynamic scene, containing\narbitrary moving objects, from an ordinary video captured with a moving camera.\nWe seek a geometrically and temporally consistent solution to this\nunderconstrained problem: the depth predictions of corresponding points across\nframes should induce plausible, smooth motion in 3D. We formulate this\nobjective in a new test-time training framework where a depth-prediction CNN is\ntrained in tandem with an auxiliary scene-flow prediction MLP over the entire\ninput video. By recursively unrolling the scene-flow prediction MLP over\nvarying time steps, we compute both short-range scene flow to impose local\nsmooth motion priors directly in 3D, and long-range scene flow to impose\nmulti-view consistency constraints with wide baselines. We demonstrate accurate\nand temporally coherent results on a variety of challenging videos containing\ndiverse moving objects (pets, people, cars), as well as camera motion. Our\ndepth maps give rise to a number of depth-and-motion aware video editing\neffects such as object and lighting insertion.",
          "link": "http://arxiv.org/abs/2108.01166",
          "publishedOn": "2021-08-04T01:59:20.891Z",
          "wordCount": 622,
          "title": "Consistent Depth of Moving Objects in Video. (arXiv:2108.01166v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1\">Seonghyeon Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1\">Michael S. Brown</a>",
          "description": "We propose a framework for aligning and fusing multiple images into a single\ncoordinate-based neural representations. Our framework targets burst images\nthat have misalignment due to camera ego motion and small changes in the scene.\nWe describe different strategies for alignment depending on the assumption of\nthe scene motion, namely, perspective planar (i.e., homography), optical flow\nwith minimal scene change, and optical flow with notable occlusion and\ndisocclusion. Our framework effectively combines the multiple inputs into a\nsingle neural implicit function without the need for selecting one of the\nimages as a reference frame. We demonstrate how to use this multi-frame fusion\nframework for various layer separation tasks.",
          "link": "http://arxiv.org/abs/2108.01199",
          "publishedOn": "2021-08-04T01:59:20.852Z",
          "wordCount": 546,
          "title": "Neural Image Representations for Multi-Image Fusion and Layer Separation. (arXiv:2108.01199v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shailja_S/0/1/0/all/0/1\">S. Shailja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Angela Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunath_B/0/1/0/all/0/1\">B.S. Manjunath</a>",
          "description": "We propose a novel and efficient algorithm to model high-level topological\nstructures of neuronal fibers. Tractography constructs complex neuronal fibers\nin three dimensions that exhibit the geometry of white matter pathways in the\nbrain. However, most tractography analysis methods are time consuming and\nintractable. We develop a computational geometry-based tractography\nrepresentation that aims to simplify the connectivity of white matter fibers.\nGiven the trajectories of neuronal fiber pathways, we model the evolution of\ntrajectories that encodes geometrically significant events and calculate their\npoint correspondence in the 3D brain space. Trajectory inter-distance is used\nas a parameter to control the granularity of the model that allows local or\nglobal representation of the tractogram. Using diffusion MRI data from\nAlzheimer's patient study, we extract tractography features from our model for\ndistinguishing the Alzheimer's subject from the normal control. Software\nimplementation of our algorithm is available on GitHub.",
          "link": "http://arxiv.org/abs/2108.01175",
          "publishedOn": "2021-08-04T01:59:20.844Z",
          "wordCount": 581,
          "title": "A computational geometry approach for modeling neuronal fiber pathways. (arXiv:2108.01175v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lechao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zunlei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ya Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>",
          "description": "Given a reference object of an unknown type in an image, human observers can\neffortlessly find the objects of the same category in another image and\nprecisely tell their visual boundaries. Such visual cognition capability of\nhumans seems absent from the current research spectrum of computer vision.\nExisting segmentation networks, for example, rely on a humongous amount of\nlabeled data, which is laborious and costly to collect and annotate; besides,\nthe performance of segmentation networks tend to downgrade as the number of the\ncategory increases. In this paper, we introduce a novel Reference semantic\nsegmentation Network (Ref-Net) to conduct visual boundary knowledge\ntranslation. Ref-Net contains a Reference Segmentation Module (RSM) and a\nBoundary Knowledge Translation Module (BKTM). Inspired by the human recognition\nmechanism, RSM is devised only to segment the same category objects based on\nthe features of the reference objects. BKTM, on the other hand, introduces two\nboundary discriminator branches to conduct inner and outer boundary\nsegmentation of the target objectin an adversarial manner, and translate the\nannotated boundary knowledge of open-source datasets into the segmentation\nnetwork. Exhaustive experiments demonstrate that, with tens of finely-grained\nannotated samples as guidance, Ref-Net achieves results on par with fully\nsupervised methods on six datasets.",
          "link": "http://arxiv.org/abs/2108.01075",
          "publishedOn": "2021-08-04T01:59:20.828Z",
          "wordCount": 652,
          "title": "Boundary Knowledge Translation based Reference Semantic Segmentation. (arXiv:2108.01075v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chuanjian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">An Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yiping Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>",
          "description": "Recent studies on deep convolutional neural networks present a simple\nparadigm of architecture design, i.e., models with more MACs typically achieve\nbetter accuracy, such as EfficientNet and RegNet. These works try to enlarge\nall the stages in the model with one unified rule by sampling and statistical\nmethods. However, we observe that some network architectures have similar MACs\nand accuracies, but their allocations on computations for different stages are\nquite different. In this paper, we propose to enlarge the capacity of CNN\nmodels by improving their width, depth and resolution on stage level. Under the\nassumption that the top-performing smaller CNNs are a proper subcomponent of\nthe top-performing larger CNNs, we propose an greedy network enlarging method\nbased on the reallocation of computations. With step-by-step modifying the\ncomputations on different stages, the enlarged network will be equipped with\noptimal allocation and utilization of MACs. On EfficientNet, our method\nconsistently outperforms the performance of the original scaling method. In\nparticular, with application of our method on GhostNet, we achieve\nstate-of-the-art 80.9% and 84.3% ImageNet top-1 accuracies under the setting of\n600M and 4.4B MACs, respectively.",
          "link": "http://arxiv.org/abs/2108.00177",
          "publishedOn": "2021-08-03T02:06:33.600Z",
          "wordCount": 620,
          "title": "Greedy Network Enlarging. (arXiv:2108.00177v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08624",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weidong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>",
          "description": "With the complexity of the network structure, uncertainty inference has\nbecome an important task to improve the classification accuracy for artificial\nintelligence systems. For image classification tasks, we propose a structured\nDropConnect (SDC) framework to model the output of a deep neural network by a\nDirichlet distribution. We introduce a DropConnect strategy on weights in the\nfully connected layers during training. In test, we split the network into\nseveral sub-networks, and then model the Dirichlet distribution by match its\nmoments with the mean and variance of the outputs of these sub-networks. The\nentropy of the estimated Dirichlet distribution is finally utilized for\nuncertainty inference. In this paper, this framework is implemented on LeNet$5$\nand VGG$16$ models for misclassification detection and out-of-distribution\ndetection on MNIST and CIFAR-$10$ datasets. Experimental results show that the\nperformance of the proposed SDC can be comparable to other uncertainty\ninference methods. Furthermore, the SDC is adapted well to different network\nstructures with certain generalization capabilities and research prospects.",
          "link": "http://arxiv.org/abs/2106.08624",
          "publishedOn": "2021-08-03T02:06:33.463Z",
          "wordCount": 645,
          "title": "Structured DropConnect for Uncertainty Inference in Image Classification. (arXiv:2106.08624v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Scene text detection and recognition have been well explored in the past few\nyears. Despite the progress, efficient and accurate end-to-end spotting of\narbitrarily-shaped text remains challenging. In this work, we propose an\nend-to-end text spotting framework, termed PAN++, which can efficiently detect\nand recognize text of arbitrary shapes in natural scenes. PAN++ is based on the\nkernel representation that reformulates a text line as a text kernel (central\nregion) surrounded by peripheral pixels. By systematically comparing with\nexisting scene text representations, we show that our kernel representation can\nnot only describe arbitrarily-shaped text but also well distinguish adjacent\ntext. Moreover, as a pixel-based representation, the kernel representation can\nbe predicted by a single fully convolutional network, which is very friendly to\nreal-time applications. Taking the advantages of the kernel representation, we\ndesign a series of components as follows: 1) a computationally efficient\nfeature enhancement network composed of stacked Feature Pyramid Enhancement\nModules (FPEMs); 2) a lightweight detection head cooperating with Pixel\nAggregation (PA); and 3) an efficient attention-based recognition head with\nMasked RoI. Benefiting from the kernel representation and the tailored\ncomponents, our method achieves high inference speed while maintaining\ncompetitive accuracy. Extensive experiments show the superiority of our method.\nFor example, the proposed PAN++ achieves an end-to-end text spotting F-measure\nof 64.9 at 29.2 FPS on the Total-Text dataset, which significantly outperforms\nthe previous best method. Code will be available at: https://git.io/PAN.",
          "link": "http://arxiv.org/abs/2105.00405",
          "publishedOn": "2021-08-03T02:06:33.422Z",
          "wordCount": 737,
          "title": "PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text. (arXiv:2105.00405v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Assran_M/0/1/0/all/0/1\">Mahmoud Assran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1\">Mathilde Caron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1\">Piotr Bojanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1\">Nicolas Ballas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1\">Michael Rabbat</a>",
          "description": "This paper proposes a novel method of learning by predicting view assignments\nwith support samples (PAWS). The method trains a model to minimize a\nconsistency loss, which ensures that different views of the same unlabeled\ninstance are assigned similar pseudo-labels. The pseudo-labels are generated\nnon-parametrically, by comparing the representations of the image views to\nthose of a set of randomly sampled labeled images. The distance between the\nview representations and labeled representations is used to provide a weighting\nover class labels, which we interpret as a soft pseudo-label. By\nnon-parametrically incorporating labeled samples in this way, PAWS extends the\ndistance-metric loss used in self-supervised methods such as BYOL and SwAV to\nthe semi-supervised setting. Despite the simplicity of the approach, PAWS\noutperforms other semi-supervised methods across architectures, setting a new\nstate-of-the-art for a ResNet-50 on ImageNet trained with either 10% or 1% of\nthe labels, reaching 75.5% and 66.5% top-1 respectively. PAWS requires 4x to\n12x less training than the previous best methods.",
          "link": "http://arxiv.org/abs/2104.13963",
          "publishedOn": "2021-08-03T02:06:33.337Z",
          "wordCount": 674,
          "title": "Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples. (arXiv:2104.13963v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1\">Siyuan Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Ailing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>",
          "description": "Heatmap-based methods dominate in the field of human pose estimation by\nmodelling the output distribution through likelihood heatmaps. In contrast,\nregression-based methods are more efficient but suffer from inferior\nperformance. In this work, we explore maximum likelihood estimation (MLE) to\ndevelop an efficient and effective regression-based methods. From the\nperspective of MLE, adopting different regression losses is making different\nassumptions about the output density function. A density function closer to the\ntrue distribution leads to a better regression performance. In light of this,\nwe propose a novel regression paradigm with Residual Log-likelihood Estimation\n(RLE) to capture the underlying output distribution. Concretely, RLE learns the\nchange of the distribution instead of the unreferenced underlying distribution\nto facilitate the training process. With the proposed reparameterization\ndesign, our method is compatible with off-the-shelf flow models. The proposed\nmethod is effective, efficient and flexible. We show its potential in various\nhuman pose estimation tasks with comprehensive experiments. Compared to the\nconventional regression paradigm, regression with RLE bring 12.4 mAP\nimprovement on MSCOCO without any test-time overhead. Moreover, for the first\ntime, especially on multi-person pose estimation, our regression method is\nsuperior to the heatmap-based methods. Our code is available at\nhttps://github.com/Jeff-sjtu/res-loglikelihood-regression",
          "link": "http://arxiv.org/abs/2107.11291",
          "publishedOn": "2021-08-03T02:06:33.329Z",
          "wordCount": 687,
          "title": "Human Pose Regression with Residual Log-likelihood Estimation. (arXiv:2107.11291v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chugg_B/0/1/0/all/0/1\">Ben Chugg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1\">Brandon Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eicher_S/0/1/0/all/0/1\">Seiji Eicher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sandy Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1\">Daniel E. Ho</a>",
          "description": "Much environmental enforcement in the United States has historically relied\non either self-reported data or physical, resource-intensive, infrequent\ninspections. Advances in remote sensing and computer vision, however, have the\npotential to augment compliance monitoring by detecting early warning signs of\nnoncompliance. We demonstrate a process for rapid identification of significant\nstructural expansion using Planet's 3m/pixel satellite imagery products and\nfocusing on Concentrated Animal Feeding Operations (CAFOs) in the US as a test\ncase. Unpermitted building expansion has been a particular challenge with\nCAFOs, which pose significant health and environmental risks. Using new\nhand-labeled dataset of 145,053 images of 1,513 CAFOs, we combine\nstate-of-the-art building segmentation with a likelihood-based change-point\ndetection model to provide a robust signal of building expansion (AUC = 0.86).\nA major advantage of this approach is that it can work with higher cadence\n(daily to weekly), but lower resolution (3m/pixel), satellite imagery than\npreviously used in similar environmental settings. It is also highly\ngeneralizable and thus provides a near real-time monitoring tool to prioritize\nenforcement resources in other settings where unpermitted construction poses\nenvironmental risk, e.g. zoning, habitat modification, or wetland protection.",
          "link": "http://arxiv.org/abs/2105.14159",
          "publishedOn": "2021-08-03T02:06:32.905Z",
          "wordCount": 683,
          "title": "Enhancing Environmental Enforcement with Near Real-Time Monitoring: Likelihood-Based Detection of Structural Expansion of Intensive Livestock Farms. (arXiv:2105.14159v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_G/0/1/0/all/0/1\">Guojia Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhenkuan Pan</a>",
          "description": "Images captured underwater are often characterized by low contrast, color\ndistortion, and noise. To address these visual degradations, we propose a novel\nscheme by constructing an adaptive color and contrast enhancement, and\ndenoising (ACCE-D) framework for underwater image enhancement. In the proposed\nframework, Difference of Gaussian (DoG) filter and bilateral filter are\nrespectively employed to decompose the high-frequency and low-frequency\ncomponents. Benefited from this separation, we utilize soft-thresholding\noperation to suppress the noise in the high-frequency component. Specially, the\nlow-frequency component is enhanced by using an adaptive color and contrast\nenhancement (ACCE) strategy. The proposed ACCE is an adaptive variational\nframework implemented in the HSI color space, which integrates data term and\nregularized term, as well as introduces Gaussian weight and Heaviside function\nto avoid over-enhancement and oversaturation. Moreover, we derive a numerical\nsolution for ACCE, and adopt a pyramid-based strategy to accelerate the solving\nprocedure. Experimental results demonstrate that our strategy is effective in\ncolor correction, visibility improvement, and detail revealing. Comparison with\nstate-of-the-art techniques also validate the superiority of proposed method.\nFurthermore, we have verified the utility of our proposed ACCE-D for enhancing\nother types of degraded scenes, including foggy scene, sandstorm scene and\nlow-light scene.",
          "link": "http://arxiv.org/abs/2104.01073",
          "publishedOn": "2021-08-03T02:06:32.870Z",
          "wordCount": 664,
          "title": "Enhancing Underwater Image via Adaptive Color and Contrast Enhancement, and Denoising. (arXiv:2104.01073v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.12906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1\">Uttaran Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_T/0/1/0/all/0/1\">Trisha Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohan Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Randhavane_T/0/1/0/all/0/1\">Tanmay Randhavane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1\">Aniket Bera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>",
          "description": "We present a novel classifier network called STEP, to classify perceived\nhuman emotion from gaits, based on a Spatial Temporal Graph Convolutional\nNetwork (ST-GCN) architecture. Given an RGB video of an individual walking, our\nformulation implicitly exploits the gait features to classify the emotional\nstate of the human into one of four emotions: happy, sad, angry, or neutral. We\nuse hundreds of annotated real-world gait videos and augment them with\nthousands of annotated synthetic gaits generated using a novel generative\nnetwork called STEP-Gen, built on an ST-GCN based Conditional Variational\nAutoencoder (CVAE). We incorporate a novel push-pull regularization loss in the\nCVAE formulation of STEP-Gen to generate realistic gaits and improve the\nclassification accuracy of STEP. We also release a novel dataset (E-Gait),\nwhich consists of $2,177$ human gaits annotated with perceived emotions along\nwith thousands of synthetic gaits. In practice, STEP can learn the affective\nfeatures and exhibits classification accuracy of 89% on E-Gait, which is 14 -\n30% more accurate over prior methods.",
          "link": "http://arxiv.org/abs/1910.12906",
          "publishedOn": "2021-08-03T02:06:32.791Z",
          "wordCount": 659,
          "title": "STEP: Spatial Temporal Graph Convolutional Networks for Emotion Perception from Gaits. (arXiv:1910.12906v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Joy T. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agu_N/0/1/0/all/0/1\">Nkechinyere N. Agu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1\">Ismini Lourentzou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Arjun Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paguio_J/0/1/0/all/0/1\">Joseph A. Paguio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jasper S. Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dee_E/0/1/0/all/0/1\">Edward C. Dee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_W/0/1/0/all/0/1\">William Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashyap_S/0/1/0/all/0/1\">Satyananda Kashyap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giovannini_A/0/1/0/all/0/1\">Andrea Giovannini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1\">Leo A. Celi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Mehdi Moradi</a>",
          "description": "Despite the progress in automatic detection of radiologic findings from chest\nX-ray (CXR) images in recent years, a quantitative evaluation of the\nexplainability of these models is hampered by the lack of locally labeled\ndatasets for different findings. With the exception of a few expert-labeled\nsmall-scale datasets for specific findings, such as pneumonia and pneumothorax,\nmost of the CXR deep learning models to date are trained on global \"weak\"\nlabels extracted from text reports, or trained via a joint image and\nunstructured text learning strategy. Inspired by the Visual Genome effort in\nthe computer vision community, we constructed the first Chest ImaGenome dataset\nwith a scene graph data structure to describe $242,072$ images. Local\nannotations are automatically produced using a joint rule-based natural\nlanguage processing (NLP) and atlas-based bounding box detection pipeline.\nThrough a radiologist constructed CXR ontology, the annotations for each CXR\nare connected as an anatomy-centered scene graph, useful for image-level\nreasoning and multimodal fusion applications. Overall, we provide: i) $1,256$\ncombinations of relation annotations between $29$ CXR anatomical locations\n(objects with bounding box coordinates) and their attributes, structured as a\nscene graph per image, ii) over $670,000$ localized comparison relations (for\nimproved, worsened, or no change) between the anatomical locations across\nsequential exams, as well as ii) a manually annotated gold standard scene graph\ndataset from $500$ unique patients.",
          "link": "http://arxiv.org/abs/2108.00316",
          "publishedOn": "2021-08-03T02:06:32.765Z",
          "wordCount": 697,
          "title": "Chest ImaGenome Dataset for Clinical Reasoning. (arXiv:2108.00316v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiageng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_M/0/1/0/all/0/1\">Minzhe Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chenhan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hanxue Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yamin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chaoqiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>",
          "description": "Current perception models in autonomous driving have become notorious for\ngreatly relying on a mass of annotated data to cover unseen cases and address\nthe long-tail problem. On the other hand, learning from unlabeled large-scale\ncollected data and incrementally self-training powerful recognition models have\nreceived increasing attention and may become the solutions of next-generation\nindustry-level powerful and robust perception models in autonomous driving.\nHowever, the research community generally suffered from data inadequacy of\nthose essential real-world scene data, which hampers the future exploration of\nfully/semi/self-supervised methods for 3D perception. In this paper, we\nintroduce the ONCE (One millioN sCenEs) dataset for 3D object detection in the\nautonomous driving scenario. The ONCE dataset consists of 1 million LiDAR\nscenes and 7 million corresponding camera images. The data is selected from 144\ndriving hours, which is 20x longer than the largest 3D autonomous driving\ndataset available (e.g. nuScenes and Waymo), and it is collected across a range\nof different areas, periods and weather conditions. To facilitate future\nresearch on exploiting unlabeled data for 3D detection, we additionally provide\na benchmark in which we reproduce and evaluate a variety of self-supervised and\nsemi-supervised methods on the ONCE dataset. We conduct extensive analyses on\nthose methods and provide valuable observations on their performance related to\nthe scale of used data. Data, code, and more information are available at\nhttps://once-for-auto-driving.github.io/index.html.",
          "link": "http://arxiv.org/abs/2106.11037",
          "publishedOn": "2021-08-03T02:06:32.661Z",
          "wordCount": 720,
          "title": "One Million Scenes for Autonomous Driving: ONCE Dataset. (arXiv:2106.11037v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.08708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1\">Uttaran Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roncal_C/0/1/0/all/0/1\">Christian Roncal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_T/0/1/0/all/0/1\">Trisha Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohan Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapsaskis_K/0/1/0/all/0/1\">Kyra Kapsaskis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_K/0/1/0/all/0/1\">Kurt Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1\">Aniket Bera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>",
          "description": "We present an autoencoder-based semi-supervised approach to classify\nperceived human emotions from walking styles obtained from videos or\nmotion-captured data and represented as sequences of 3D poses. Given the motion\non each joint in the pose at each time step extracted from 3D pose sequences,\nwe hierarchically pool these joint motions in a bottom-up manner in the\nencoder, following the kinematic chains in the human body. We also constrain\nthe latent embeddings of the encoder to contain the space of\npsychologically-motivated affective features underlying the gaits. We train the\ndecoder to reconstruct the motions per joint per time step in a top-down manner\nfrom the latent embeddings. For the annotated data, we also train a classifier\nto map the latent embeddings to emotion labels. Our semi-supervised approach\nachieves a mean average precision of 0.84 on the Emotion-Gait benchmark\ndataset, which contains both labeled and unlabeled gaits collected from\nmultiple sources. We outperform current state-of-art algorithms for both\nemotion recognition and action recognition from 3D gaits by 7%--23% on the\nabsolute. More importantly, we improve the average precision by 10%--50% on the\nabsolute on classes that each makes up less than 25% of the labeled part of the\nEmotion-Gait benchmark dataset.",
          "link": "http://arxiv.org/abs/1911.08708",
          "publishedOn": "2021-08-03T02:06:32.638Z",
          "wordCount": 732,
          "title": "Take an Emotion Walk: Perceiving Emotions from Gaits Using Hierarchical Attention Pooling and Affective Mapping. (arXiv:1911.08708v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zihan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "Binary neural networks (BNNs) have received increasing attention due to their\nsuperior reductions of computation and memory. Most existing works focus on\neither lessening the quantization error by minimizing the gap between the\nfull-precision weights and their binarization or designing a gradient\napproximation to mitigate the gradient mismatch, while leaving the \"dead\nweights\" untouched. This leads to slow convergence when training BNNs. In this\npaper, for the first time, we explore the influence of \"dead weights\" which\nrefer to a group of weights that are barely updated during the training of\nBNNs, and then introduce rectified clamp unit (ReCU) to revive the \"dead\nweights\" for updating. We prove that reviving the \"dead weights\" by ReCU can\nresult in a smaller quantization error. Besides, we also take into account the\ninformation entropy of the weights, and then mathematically analyze why the\nweight standardization can benefit BNNs. We demonstrate the inherent\ncontradiction between minimizing the quantization error and maximizing the\ninformation entropy, and then propose an adaptive exponential scheduler to\nidentify the range of the \"dead weights\". By considering the \"dead weights\",\nour method offers not only faster BNN training, but also state-of-the-art\nperformance on CIFAR-10 and ImageNet, compared with recent methods. Code can be\navailable at https://github.com/z-hXu/ReCU.",
          "link": "http://arxiv.org/abs/2103.12369",
          "publishedOn": "2021-08-03T02:06:32.606Z",
          "wordCount": 696,
          "title": "ReCU: Reviving the Dead Weights in Binary Neural Networks. (arXiv:2103.12369v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14256",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_B/0/1/0/all/0/1\">Boing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yinxi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weitz_P/0/1/0/all/0/1\">Philippe Weitz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lindberg_J/0/1/0/all/0/1\">Johan Lindberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hartman_J/0/1/0/all/0/1\">Johan Hartman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Egevad_L/0/1/0/all/0/1\">Lars Egevad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gronberg_H/0/1/0/all/0/1\">Henrik Gr&#xf6;nberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eklund_M/0/1/0/all/0/1\">Martin Eklund</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rantalainen_M/0/1/0/all/0/1\">Mattias Rantalainen</a>",
          "description": "Background: Transrectal ultrasound guided systematic biopsies of the prostate\nis a routine procedure to establish a prostate cancer diagnosis. However, the\n10-12 prostate core biopsies only sample a relatively small volume of the\nprostate, and tumour lesions in regions between biopsy cores can be missed,\nleading to a well-known low sensitivity to detect clinically relevant cancer.\nAs a proof-of-principle, we developed and validated a deep convolutional neural\nnetwork model to distinguish between morphological patterns in benign prostate\nbiopsy whole slide images from men with and without established cancer.\nMethods: This study included 14,354 hematoxylin and eosin stained whole slide\nimages from benign prostate biopsies from 1,508 men in two groups: men without\nan established prostate cancer (PCa) diagnosis and men with at least one core\nbiopsy diagnosed with PCa. 80% of the participants were assigned as training\ndata and used for model optimization (1,211 men), and the remaining 20% (297\nmen) as a held-out test set used to evaluate model performance. An ensemble of\n10 deep convolutional neural network models was optimized for classification of\nbiopsies from men with and without established cancer. Hyperparameter\noptimization and model selection was performed by cross-validation in the\ntraining data . Results: Area under the receiver operating characteristic curve\n(ROC-AUC) was estimated as 0.727 (bootstrap 95% CI: 0.708-0.745) on biopsy\nlevel and 0.738 (bootstrap 95% CI: 0.682 - 0.796) on man level. At a\nspecificity of 0.9 the model had an estimated sensitivity of 0.348. Conclusion:\nThe developed model has the ability to detect men with risk of missed PCa due\nto under-sampling of the prostate. The proposed model has the potential to\nreduce the number of false negative cases in routine systematic prostate\nbiopsies and to indicate men who could benefit from MRI-guided re-biopsy.",
          "link": "http://arxiv.org/abs/2106.14256",
          "publishedOn": "2021-08-03T02:06:32.587Z",
          "wordCount": 783,
          "title": "Using deep learning to detect patients at risk for prostate cancer despite benign biopsies. (arXiv:2106.14256v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Heng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miththanthaya_H/0/1/0/all/0/1\">Halady Akhilesha Miththanthaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harshit/0/1/0/all/0/1\">Harshit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_S/0/1/0/all/0/1\">Siranjiv Ramana Rajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoqiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhilin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuewei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>",
          "description": "Visual tracking has achieved considerable progress in recent years. However,\ncurrent research in the field mainly focuses on tracking of opaque objects,\nwhile little attention is paid to transparent object tracking. In this paper,\nwe make the first attempt in exploring this problem by proposing a Transparent\nObject Tracking Benchmark (TOTB). Specifically, TOTB consists of 225 videos\n(86K frames) from 15 diverse transparent object categories. Each sequence is\nmanually labeled with axis-aligned bounding boxes. To the best of our\nknowledge, TOTB is the first benchmark dedicated to transparent object\ntracking. In order to understand how existing trackers perform and to provide\ncomparison for future research on TOTB, we extensively evaluate 25\nstate-of-the-art tracking algorithms. The evaluation results exhibit that more\nefforts are needed to improve transparent object tracking. Besides, we observe\nsome nontrivial findings from the evaluation that are discrepant with some\ncommon beliefs in opaque object tracking. For example, we find that deeper\nfeatures are not always good for improvements. Moreover, to encourage future\nresearch, we introduce a novel tracker, named TransATOM, which leverages\ntransparency features for tracking and surpasses all 25 evaluated approaches by\na large margin. By releasing TOTB, we expect to facilitate future research and\napplication of transparent object tracking in both the academia and industry.\nThe TOTB and evaluation results as well as TransATOM are available at\nhttps://hengfan2010.github.io/projects/TOTB.",
          "link": "http://arxiv.org/abs/2011.10875",
          "publishedOn": "2021-08-03T02:06:32.568Z",
          "wordCount": 693,
          "title": "Transparent Object Tracking Benchmark. (arXiv:2011.10875v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1903.06519",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Platonova_G/0/1/0/all/0/1\">Ganna Platonova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stys_D/0/1/0/all/0/1\">Dalibor Stys</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soucek_P/0/1/0/all/0/1\">Pavel Soucek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lonhus_K/0/1/0/all/0/1\">Kirill Lonhus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valenta_J/0/1/0/all/0/1\">Jan Valenta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rychtarikova_R/0/1/0/all/0/1\">Renata Rychtarikova</a>",
          "description": "The most realistic information about the transparent sample such as a live\ncell can be obtained only using bright-field light microscopy. At\nhigh-intensity pulsing LED illumination, we captured a primary\n12-bit-per-channel (bpc) response from an observed sample using a bright-field\nmicroscope equipped with a high-resolution (4872x3248) image sensor. In order\nto suppress data distortions originating from the light interactions with\nelements in the optical path, poor sensor reproduction (geometrical defects of\nthe camera sensor and some peculiarities of sensor sensitivity), we propose a\nspectroscopic approach for the correction of this uncompressed 12-bpc data by\nsimultaneous calibration of all parts of the experimental arrangement.\nMoreover, the final intensities of the corrected images are proportional to the\nphoton fluxes detected by a camera sensor. It can be visualized in 8-bpc\nintensity depth after the Least Information Loss compression [Lect. Notes\nBioinform. 9656, 527 (2016)].",
          "link": "http://arxiv.org/abs/1903.06519",
          "publishedOn": "2021-08-03T02:06:32.554Z",
          "wordCount": 641,
          "title": "Spectroscopic Approach to Correction and Visualisation of Bright-Field Light Transmission Microscopy Biological Data. (arXiv:1903.06519v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1\">Zhanghui Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aojun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jing-Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinjiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qingmin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wayne Zhang</a>",
          "description": "Network compression has been widely studied since it is able to reduce the\nmemory and computation cost during inference. However, previous methods seldom\ndeal with complicated structures like residual connections, group/depth-wise\nconvolution and feature pyramid network, where channels of multiple layers are\ncoupled and need to be pruned simultaneously. In this paper, we present a\ngeneral channel pruning approach that can be applied to various complicated\nstructures. Particularly, we propose a layer grouping algorithm to find coupled\nchannels automatically. Then we derive a unified metric based on Fisher\ninformation to evaluate the importance of a single channel and coupled\nchannels. Moreover, we find that inference speedup on GPUs is more correlated\nwith the reduction of memory rather than FLOPs, and thus we employ the memory\nreduction of each channel to normalize the importance. Our method can be used\nto prune any structures including those with coupled channels. We conduct\nextensive experiments on various backbones, including the classic ResNet and\nResNeXt, mobile-friendly MobileNetV2, and the NAS-based RegNet, both on image\nclassification and object detection which is under-explored. Experimental\nresults validate that our method can effectively prune sophisticated networks,\nboosting inference speed without sacrificing accuracy.",
          "link": "http://arxiv.org/abs/2108.00708",
          "publishedOn": "2021-08-03T02:06:32.548Z",
          "wordCount": 649,
          "title": "Group Fisher Pruning for Practical Network Compression. (arXiv:2108.00708v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1\">Daniel Berleant</a>",
          "description": "The accuracy of DL classifiers is unstable in that it often changes\nsignificantly when retested on adversarial images, imperfect images, or\nperturbed images. This paper adds to the small but fundamental body of work on\nbenchmarking the robustness of DL classifiers on defective images. Unlike\nexisted single-factor digital perturbation work, we provide state-of-the-art\ntwo-factor perturbation that provides two natural perturbations on images\napplied in different sequences. The two-factor perturbation includes (1) two\ndigital perturbations (Salt & pepper noise and Gaussian noise) applied in both\nsequences. (2) one digital perturbation (salt & pepper noise) and a geometric\nperturbation (rotation) applied in different sequences. To measure robust DL\nclassifiers, previous scientists provided 15 types of single-factor corruption.\nWe created 69 benchmarking image sets, including a clean set, sets with single\nfactor perturbations, and sets with two-factor perturbation conditions. To be\nbest of our knowledge, this is the first report that two-factor perturbed\nimages improves both robustness and accuracy of DL classifiers. Previous\nresearch evaluating deep learning (DL) classifiers has often used top-1/top-5\naccuracy, so researchers have usually offered tables, line diagrams, and bar\ncharts to display accuracy of DL classifiers. But these existed approaches\ncannot quantitively evaluate robustness of DL classifiers. We innovate a new\ntwo-dimensional, statistical visualization tool, including mean accuracy and\ncoefficient of variation (CV), to benchmark the robustness of DL classifiers.\nAll source codes and related image sets are shared on websites\n(this http URL or\nhttps://github.com/daiweiworking/RobustDeepLearningUsingPerturbations ) to\nsupport future academic research and industry projects.",
          "link": "http://arxiv.org/abs/2103.03102",
          "publishedOn": "2021-08-03T02:06:32.535Z",
          "wordCount": 722,
          "title": "Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00639",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+White_J/0/1/0/all/0/1\">Jacob M. White</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crozier_S/0/1/0/all/0/1\">Stuart Crozier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandra_S/0/1/0/all/0/1\">Shekhar S. Chandra</a>",
          "description": "Sampling strategies are important for sparse imaging methodologies,\nespecially those employing the discrete Fourier transform (DFT). Chaotic\nsensing is one such methodology that employs deterministic, fractal sampling in\nconjunction with finite, iterative reconstruction schemes to form an image from\nlimited samples. Using a sampling pattern constructed entirely from periodic\nlines in DFT space, chaotic sensing was found to outperform traditional\ncompressed sensing for magnetic resonance imaging; however, only one such\nsampling pattern was presented and the reason for its fractal nature was not\nproven. Through the introduction of a novel image transform known as the\nkaleidoscope transform, which formalises and extends upon the concept of\ndownsampling and concatenating an image with itself, this paper: (1)\ndemonstrates a fundamental relationship between multiplication in modular\narithmetic and downsampling; (2) provides a rigorous mathematical explanation\nfor the fractal nature of the sampling pattern in the DFT; and (3) leverages\nthis understanding to develop a collection of novel fractal sampling patterns\nfor the 2D DFT with customisable properties. The ability to design tailor-made\nfractal sampling patterns expands the utility of the DFT in chaotic imaging and\nmay form the basis for a bespoke chaotic sensing methodology, in which the\nfractal sampling matches the imaging task for improved reconstruction.",
          "link": "http://arxiv.org/abs/2108.00639",
          "publishedOn": "2021-08-03T02:06:32.529Z",
          "wordCount": 662,
          "title": "Bespoke Fractal Sampling Patterns for Discrete Fourier Space via the Kaleidoscope Transform. (arXiv:2108.00639v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Y/0/1/0/all/0/1\">Yen Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_B/0/1/0/all/0/1\">Bao Le</a>",
          "description": "Recent breakthroughs in the field of semi-supervised learning have achieved\nresults that match state-of-the-art traditional supervised learning methods.\nMost successful semi-supervised learning approaches in computer vision focus on\nleveraging huge amount of unlabeled data, learning the general representation\nvia data augmentation and transformation, creating pseudo labels, implementing\ndifferent loss functions, and eventually transferring this knowledge to more\ntask-specific smaller models. In this paper, we aim to conduct our analyses on\nthree different aspects of SimCLR, the current state-of-the-art semi-supervised\nlearning framework for computer vision. First, we analyze properties of\ncontrast learning on fine-tuning, as we understand that contrast learning is\nwhat makes this method so successful. Second, we research knowledge\ndistillation through teacher-forcing paradigm. We observe that when the teacher\nand the student share the same base model, knowledge distillation will achieve\nbetter result. Finally, we study how transfer learning works and its\nrelationship with the number of classes on different data sets. Our results\nindicate that transfer learning performs better when number of classes are\nsmaller.",
          "link": "http://arxiv.org/abs/2108.00587",
          "publishedOn": "2021-08-03T02:06:32.456Z",
          "wordCount": 606,
          "title": "Semi-Supervising Learning, Transfer Learning, and Knowledge Distillation with SimCLR. (arXiv:2108.00587v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jingxian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lichao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Y/0/1/0/all/0/1\">Yufei Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Garcia_A/0/1/0/all/0/1\">Abel Gonzalez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanning Zhang</a>",
          "description": "The target representation learned by convolutional neural networks plays an\nimportant role in Thermal Infrared (TIR) tracking. Currently, most of the\ntop-performing TIR trackers are still employing representations learned by the\nmodel trained on the RGB data. However, this representation does not take into\naccount the information in the TIR modality itself, limiting the performance of\nTIR tracking. To solve this problem, we propose to distill representations of\nthe TIR modality from the RGB modality with Cross-Modal Distillation (CMD) on a\nlarge amount of unlabeled paired RGB-TIR data. We take advantage of the\ntwo-branch architecture of the baseline tracker, i.e. DiMP, for cross-modal\ndistillation working on two components of the tracker. Specifically, we use one\nbranch as a teacher module to distill the representation learned by the model\ninto the other branch. Benefiting from the powerful model in the RGB modality,\nthe cross-modal distillation can learn the TIR-specific representation for\npromoting TIR tracking. The proposed approach can be incorporated into\ndifferent baseline trackers conveniently as a generic and independent\ncomponent. Furthermore, the semantic coherence of paired RGB and TIR images is\nutilized as a supervised signal in the distillation loss for cross-modal\nknowledge transfer. In practice, three different approaches are explored to\ngenerate paired RGB-TIR patches with the same semantics for training in an\nunsupervised way. It is easy to extend to an even larger scale of unlabeled\ntraining data. Extensive experiments on the LSOTB-TIR dataset and PTB-TIR\ndataset demonstrate that our proposed cross-modal distillation method\neffectively learns TIR-specific target representations transferred from the RGB\nmodality. Our tracker outperforms the baseline tracker by achieving absolute\ngains of 2.3% Success, 2.7% Precision, and 2.5% Normalized Precision\nrespectively.",
          "link": "http://arxiv.org/abs/2108.00187",
          "publishedOn": "2021-08-03T02:06:32.441Z",
          "wordCount": 733,
          "title": "Unsupervised Cross-Modal Distillation for Thermal Infrared Tracking. (arXiv:2108.00187v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaobo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>",
          "description": "In this paper, we study the task of hallucinating an authentic\nhigh-resolution (HR) face from an occluded thumbnail. We propose a multi-stage\nProgressive Upsampling and Inpainting Generative Adversarial Network, dubbed\nPro-UIGAN, which exploits facial geometry priors to replenish and upsample (8*)\nthe occluded and tiny faces (16*16 pixels). Pro-UIGAN iteratively (1) estimates\nfacial geometry priors for low-resolution (LR) faces and (2) acquires\nnon-occluded HR face images under the guidance of the estimated priors. Our\nmulti-stage hallucination network super-resolves and inpaints occluded LR faces\nin a coarse-to-fine manner, thus reducing unwanted blurriness and artifacts\nsignificantly. Specifically, we design a novel cross-modal transformer module\nfor facial priors estimation, in which an input face and its landmark features\nare formulated as queries and keys, respectively. Such a design encourages\njoint feature learning across the input facial and landmark features, and deep\nfeature correspondences will be discovered by attention. Thus, facial\nappearance features and facial geometry priors are learned in a mutual\npromotion manner. Extensive experiments demonstrate that our Pro-UIGAN achieves\nvisually pleasing HR faces, reaching superior performance in downstream tasks,\ni.e., face alignment, face parsing, face recognition and expression\nclassification, compared with other state-of-the-art (SotA) methods.",
          "link": "http://arxiv.org/abs/2108.00602",
          "publishedOn": "2021-08-03T02:06:32.419Z",
          "wordCount": 627,
          "title": "Pro-UIGAN: Progressive Face Hallucination from Occluded Thumbnails. (arXiv:2108.00602v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.07935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Libo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shenghui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenxi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiaoliang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkinson_P/0/1/0/all/0/1\">Peter M. Atkinson</a>",
          "description": "Assigning geospatial objects with specific categories at the pixel level is a\nfundamental task in remote sensing image analysis. Along with rapid development\nin sensor technologies, remotely sensed images can be captured at multiple\nspatial resolutions (MSR) with information content manifested at different\nscales. Extracting information from these MSR images represents huge\nopportunities for enhanced feature representation and characterisation.\nHowever, MSR images suffer from two critical issues: 1) increased scale\nvariation of geo-objects and 2) loss of detailed information at coarse spatial\nresolutions. To bridge these gaps, in this paper, we propose a novel\nscale-aware neural network (SaNet) for semantic segmentation of MSR remotely\nsensed imagery. SaNet deploys a densely connected feature network (DCFPN)\nmodule to capture high-quality multi-scale context, such that the scale\nvariation is handled properly and the quality of segmentation is increased for\nboth large and small objects. A spatial feature recalibration (SFR) module is\nfurther incorporated into the network to learn intact semantic content with\nenhanced spatial relationships, where the negative effects of information loss\nare removed. The combination of DCFPN and SFR allows SaNet to learn scale-aware\nfeature representation, which outperforms the existing multi-scale feature\nrepresentation. Extensive experiments on three semantic segmentation datasets\ndemonstrated the effectiveness of the proposed SaNet in cross-resolution\nsegmentation.",
          "link": "http://arxiv.org/abs/2103.07935",
          "publishedOn": "2021-08-03T02:06:32.393Z",
          "wordCount": 698,
          "title": "Scale-aware Neural Network for Semantic Segmentation of Multi-resolution Remotely Sensed Images. (arXiv:2103.07935v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sim_I/0/1/0/all/0/1\">Issac Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Ju-Hyung Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Young-Wan Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">JiHwan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">SeonTaek Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Keun Kim</a>",
          "description": "Latest CNN-based object detection models are quite accurate but require a\nhigh-performance GPU to run in real-time. They still are heavy in terms of\nmemory size and speed for an embedded system with limited memory space. Since\nthe object detection for autonomous system is run on an embedded processor, it\nis preferable to compress the detection network as light as possible while\npreserving the detection accuracy. There are several popular lightweight\ndetection models but their accuracy is too low for safe driving applications.\nTherefore, this paper proposes a new object detection model, referred as\nYOffleNet, which is compressed at a high ratio while minimizing the accuracy\nloss for real-time and safe driving application on an autonomous system. The\nbackbone network architecture is based on YOLOv4, but we could compress the\nnetwork greatly by replacing the high-calculation-load CSP DenseNet with the\nlighter modules of ShuffleNet. Experiments with KITTI dataset showed that the\nproposed YOffleNet is compressed by 4.7 times than the YOLOv4-s that could\nachieve as fast as 46 FPS on an embedded GPU system(NVIDIA Jetson AGX Xavier).\nCompared to the high compression ratio, the accuracy is reduced slightly to\n85.8% mAP, that is only 2.6% lower than YOLOv4-s. Thus, the proposed network\nshowed a high potential to be deployed on the embedded system of the autonomous\nsystem for the real-time and accurate object detection applications.",
          "link": "http://arxiv.org/abs/2108.00392",
          "publishedOn": "2021-08-03T02:06:32.352Z",
          "wordCount": 688,
          "title": "Developing a Compressed Object Detection Model based on YOLOv4 for Deployment on Embedded GPU Platform of Autonomous System. (arXiv:2108.00392v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00454",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_L/0/1/0/all/0/1\">Le Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>",
          "description": "Point clouds obtained from 3D sensors are usually sparse. Existing methods\nmainly focus on upsampling sparse point clouds in a supervised manner by using\ndense ground truth point clouds. In this paper, we propose a self-supervised\npoint cloud upsampling network (SSPU-Net) to generate dense point clouds\nwithout using ground truth. To achieve this, we exploit the consistency between\nthe input sparse point cloud and generated dense point cloud for the shapes and\nrendered images. Specifically, we first propose a neighbor expansion unit (NEU)\nto upsample the sparse point clouds, where the local geometric structures of\nthe sparse point clouds are exploited to learn weights for point interpolation.\nThen, we develop a differentiable point cloud rendering unit (DRU) as an\nend-to-end module in our network to render the point cloud into multi-view\nimages. Finally, we formulate a shape-consistent loss and an image-consistent\nloss to train the network so that the shapes of the sparse and dense point\nclouds are as consistent as possible. Extensive results on the CAD and scanned\ndatasets demonstrate that our method can achieve impressive results in a\nself-supervised manner. Code is available at https://github.com/Avlon/SSPU-Net.",
          "link": "http://arxiv.org/abs/2108.00454",
          "publishedOn": "2021-08-03T02:06:32.193Z",
          "wordCount": 629,
          "title": "SSPU-Net: Self-Supervised Point Cloud Upsampling via Differentiable Rendering. (arXiv:2108.00454v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>",
          "description": "Super-resolution (SR) is a fundamental and representative task of low-level\nvision area. It is generally thought that the features extracted from the SR\nnetwork have no specific semantic information, and the network simply learns\ncomplex non-linear mappings from input to output. Can we find any \"semantics\"\nin SR networks? In this paper, we give affirmative answers to this question. By\nanalyzing the feature representations with dimensionality reduction and\nvisualization, we successfully discover the deep semantic representations in SR\nnetworks, \\textit{i.e.}, deep degradation representations (DDR), which relate\nto the image degradation types and degrees. We also reveal the differences in\nrepresentation semantics between classification and SR networks. Through\nextensive experiments and analysis, we draw a series of observations and\nconclusions, which are of great significance for future work, such as\ninterpreting the intrinsic mechanisms of low-level CNN networks and developing\nnew evaluation approaches for blind SR.",
          "link": "http://arxiv.org/abs/2108.00406",
          "publishedOn": "2021-08-03T02:06:32.154Z",
          "wordCount": 592,
          "title": "Discovering \"Semantics\" in Super-Resolution Networks. (arXiv:2108.00406v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Men_H/0/1/0/all/0/1\">Hui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hanhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenadeleh_M/0/1/0/all/0/1\">Mohsen Jenadeleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saupe_D/0/1/0/all/0/1\">Dietmar Saupe</a>",
          "description": "In subjective full-reference image quality assessment, differences between\nperceptual image qualities of the reference image and its distorted versions\nare evaluated, often using degradation category ratings (DCR). However, the DCR\nhas been criticized since differences between rating categories on this ordinal\nscale might not be perceptually equidistant, and observers may have different\nunderstandings of the categories. Pair comparisons (PC) of distorted images,\nfollowed by Thurstonian reconstruction of scale values, overcome these\nproblems. In addition, PC is more sensitive than DCR, and it can provide scale\nvalues in fractional, just noticeable difference (JND) units that express a\nprecise perceptional interpretation. Still, the comparison of images of nearly\nthe same quality can be difficult. We introduce boosting techniques embedded in\nmore general triplet comparisons (TC) that increase the sensitivity even more.\nBoosting amplifies the artefacts of distorted images, enlarges their visual\nrepresentation by zooming, increases the visibility of the distortions by a\nflickering effect, or combines some of the above. Experimental results show the\neffectiveness of boosted TC for seven types of distortion. We crowdsourced over\n1.7 million responses to triplet questions. A detailed analysis shows that\nboosting increases the discriminatory power and allows to reduce the number of\nsubjective ratings without sacrificing the accuracy of the resulting relative\nimage quality values. Our technique paves the way to fine-grained image quality\ndatasets, allowing for more distortion levels, yet with high-quality subjective\nannotations. We also provide the details for Thurstonian scale reconstruction\nfrom TC and our annotated dataset, KonFiG-IQA, containing 10 source images,\nprocessed using 7 distortion types at 12 or even 30 levels, uniformly spaced\nover a span of 3 JND units.",
          "link": "http://arxiv.org/abs/2108.00201",
          "publishedOn": "2021-08-03T02:06:32.132Z",
          "wordCount": 705,
          "title": "Subjective Image Quality Assessment with Boosted Triplet Comparisons. (arXiv:2108.00201v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.16507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yating Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinchi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>",
          "description": "Regression-based methods have recently shown promising results in\nreconstructing human meshes from monocular images. By directly mapping raw\npixels to model parameters, these methods can produce parametric models in a\nfeed-forward manner via neural networks. However, minor deviation in parameters\nmay lead to noticeable misalignment between the estimated meshes and image\nevidences. To address this issue, we propose a Pyramidal Mesh Alignment\nFeedback (PyMAF) loop to leverage a feature pyramid and rectify the predicted\nparameters explicitly based on the mesh-image alignment status in our deep\nregressor. In PyMAF, given the currently predicted parameters, mesh-aligned\nevidences will be extracted from finer-resolution features accordingly and fed\nback for parameter rectification. To reduce noise and enhance the reliability\nof these evidences, an auxiliary pixel-wise supervision is imposed on the\nfeature encoder, which provides mesh-image correspondence guidance for our\nnetwork to preserve the most related information in spatial features. The\nefficacy of our approach is validated on several benchmarks, including\nHuman3.6M, 3DPW, LSP, and COCO, where experimental results show that our\napproach consistently improves the mesh-image alignment of the reconstruction.\nThe project page with code and video results can be found at\nhttps://hongwenzhang.github.io/pymaf.",
          "link": "http://arxiv.org/abs/2103.16507",
          "publishedOn": "2021-08-03T02:06:32.085Z",
          "wordCount": 697,
          "title": "PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop. (arXiv:2103.16507v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nasibullah/0/1/0/all/0/1\">Nasibullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanta_P/0/1/0/all/0/1\">Partha Pratim Mohanta</a>",
          "description": "Video captioning is one of the challenging problems at the intersection of\nvision and language, having many real-life applications in video retrieval,\nvideo surveillance, assisting visually challenged people, Human-machine\ninterface, and many more. Recent deep learning-based methods have shown\npromising results but are still on the lower side than other vision tasks (such\nas image classification, object detection). A significant drawback with\nexisting video captioning methods is that they are optimized over cross-entropy\nloss function, which is uncorrelated to the de facto evaluation metrics (BLEU,\nMETEOR, CIDER, ROUGE).In other words, cross-entropy is not a proper surrogate\nof the true loss function for video captioning. This paper addresses the\ndrawback by introducing a dynamic loss network (DLN), which provides an\nadditional feedback signal that directly reflects the evaluation metrics. Our\nresults on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to\nText (MSRVTT) datasets outperform previous methods.",
          "link": "http://arxiv.org/abs/2107.11707",
          "publishedOn": "2021-08-03T02:06:32.079Z",
          "wordCount": 609,
          "title": "Boosting Video Captioning with Dynamic Loss Network. (arXiv:2107.11707v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenmann_M/0/1/0/all/0/1\">Matthias Eisenmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarikaya_D/0/1/0/all/0/1\">Duygu Sarikaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marz_K/0/1/0/all/0/1\">Keno M&#xe4;rz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_T/0/1/0/all/0/1\">Toby Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malpani_A/0/1/0/all/0/1\">Anand Malpani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallert_J/0/1/0/all/0/1\">Johannes Fallert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feussner_H/0/1/0/all/0/1\">Hubertus Feussner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannarou_S/0/1/0/all/0/1\">Stamatia Giannarou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1\">Pietro Mascagni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakawala_H/0/1/0/all/0/1\">Hirenkumar Nakawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_A/0/1/0/all/0/1\">Adrian Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pugh_C/0/1/0/all/0/1\">Carla Pugh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedula_S/0/1/0/all/0/1\">Swaroop S. Vedula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cleary_K/0/1/0/all/0/1\">Kevin Cleary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fichtinger_G/0/1/0/all/0/1\">Gabor Fichtinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forestier_G/0/1/0/all/0/1\">Germain Forestier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibaud_B/0/1/0/all/0/1\">Bernard Gibaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grantcharov_T/0/1/0/all/0/1\">Teodor Grantcharov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashizume_M/0/1/0/all/0/1\">Makoto Hashizume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckmann_Notzel_D/0/1/0/all/0/1\">Doreen Heckmann-N&#xf6;tzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenngott_H/0/1/0/all/0/1\">Hannes G. Kenngott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kikinis_R/0/1/0/all/0/1\">Ron Kikinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mundermann_L/0/1/0/all/0/1\">Lars M&#xfc;ndermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onogur_S/0/1/0/all/0/1\">Sinan Onogur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1\">Raphael Sznitman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1\">Russell H. Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu D. Tizabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1\">Martin Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1\">Gregory D. Hager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumuth_T/0/1/0/all/0/1\">Thomas Neumuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_J/0/1/0/all/0/1\">Justin Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gockel_I/0/1/0/all/0/1\">Ines Gockel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goedeke_J/0/1/0/all/0/1\">Jan Goedeke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_D/0/1/0/all/0/1\">Daniel A. Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joyeux_L/0/1/0/all/0/1\">Luc Joyeux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1\">Kyle Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leff_D/0/1/0/all/0/1\">Daniel R. Leff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madani_A/0/1/0/all/0/1\">Amin Madani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_H/0/1/0/all/0/1\">Hani J. Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meireles_O/0/1/0/all/0/1\">Ozanan Meireles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seitel_A/0/1/0/all/0/1\">Alexander Seitel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teber_D/0/1/0/all/0/1\">Dogu Teber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uckert_F/0/1/0/all/0/1\">Frank &#xdc;ckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Stich_B/0/1/0/all/0/1\">Beat P. M&#xfc;ller-Stich</a>, et al. (2 additional authors not shown)",
          "description": "Recent developments in data science in general and machine learning in\nparticular have transformed the way experts envision the future of surgery.\nSurgical Data Science (SDS) is a new research field that aims to improve the\nquality of interventional healthcare through the capture, organization,\nanalysis and modeling of data. While an increasing number of data-driven\napproaches and clinical applications have been studied in the fields of\nradiological and clinical data science, translational success stories are still\nlacking in surgery. In this publication, we shed light on the underlying\nreasons and provide a roadmap for future advances in the field. Based on an\ninternational workshop involving leading researchers in the field of SDS, we\nreview current practice, key achievements and initiatives as well as available\nstandards and tools for a number of topics relevant to the field, namely (1)\ninfrastructure for data acquisition, storage and access in the presence of\nregulatory constraints, (2) data annotation and sharing and (3) data analytics.\nWe further complement this technical perspective with (4) a review of currently\navailable SDS products and the translational progress from academia and (5) a\nroadmap for faster clinical translation and exploitation of the full potential\nof SDS, based on an international multi-round Delphi process.",
          "link": "http://arxiv.org/abs/2011.02284",
          "publishedOn": "2021-08-03T02:06:32.050Z",
          "wordCount": 787,
          "title": "Surgical Data Science -- from Concepts toward Clinical Translation. (arXiv:2011.02284v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09543",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1\">Richard Osuala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1\">Kaisar Kushibar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garrucho_L/0/1/0/all/0/1\">Lidia Garrucho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1\">Akis Linardos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szafranowska_Z/0/1/0/all/0/1\">Zuzanna Szafranowska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1\">Stefan Klein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1\">Oliver Diaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>",
          "description": "Despite technological and medical advances, the detection, interpretation,\nand treatment of cancer based on imaging data continue to pose significant\nchallenges. These include high inter-observer variability, difficulty of\nsmall-sized lesion detection, nodule interpretation and malignancy\ndetermination, inter- and intra-tumour heterogeneity, class imbalance,\nsegmentation inaccuracies, and treatment effect uncertainty. The recent\nadvancements in Generative Adversarial Networks (GANs) in computer vision as\nwell as in medical imaging may provide a basis for enhanced capabilities in\ncancer detection and analysis. In this review, we assess the potential of GANs\nto address a number of key challenges of cancer imaging, including data\nscarcity and imbalance, domain and dataset shifts, data access and privacy,\ndata annotation and quantification, as well as cancer detection, tumour\nprofiling and treatment planning. We provide a critical appraisal of the\nexisting literature of GANs applied to cancer imagery, together with\nsuggestions on future research directions to address these challenges. We\nanalyse and discuss 163 papers that apply adversarial training techniques in\nthe context of cancer imaging and elaborate their methodologies, advantages and\nlimitations. With this work, we strive to bridge the gap between the needs of\nthe clinical cancer imaging community and the current and prospective research\non GANs in the artificial intelligence community.",
          "link": "http://arxiv.org/abs/2107.09543",
          "publishedOn": "2021-08-03T02:06:32.034Z",
          "wordCount": 695,
          "title": "A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions. (arXiv:2107.09543v1 [eess.IV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Dachuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabanovic_E/0/1/0/all/0/1\">Eldar Sabanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizzetto_L/0/1/0/all/0/1\">Luca Rizzetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrickij_V/0/1/0/all/0/1\">Viktor Skrickij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliverio_R/0/1/0/all/0/1\">Roberto Oliverio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaviani_N/0/1/0/all/0/1\">Nadia Kaviani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yunguang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bureika_G/0/1/0/all/0/1\">Gintautas Bureika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_S/0/1/0/all/0/1\">Stefano Ricci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hecht_M/0/1/0/all/0/1\">Markus Hecht</a>",
          "description": "In the application of computer-vision based displacement measurement, an\noptical target is usually required to prove the reference. In the case that the\noptical target cannot be attached to the measuring objective, edge detection,\nfeature matching and template matching are the most common approaches in\ntarget-less photogrammetry. However, their performance significantly relies on\nparameter settings. This becomes problematic in dynamic scenes where\ncomplicated background texture exists and varies over time. To tackle this\nissue, we propose virtual point tracking for real-time target-less dynamic\ndisplacement measurement, incorporating deep learning techniques and domain\nknowledge. Our approach consists of three steps: 1) automatic calibration for\ndetection of region of interest; 2) virtual point detection for each video\nframe using deep convolutional neural network; 3) domain-knowledge based rule\nengine for point tracking in adjacent frames. The proposed approach can be\nexecuted on an edge computer in a real-time manner (i.e. over 30 frames per\nsecond). We demonstrate our approach for a railway application, where the\nlateral displacement of the wheel on the rail is measured during operation. We\nalso implement an algorithm using template matching and line detection as the\nbaseline for comparison. The numerical experiments have been performed to\nevaluate the performance and the latency of our approach in the harsh railway\nenvironment with noisy and varying backgrounds.",
          "link": "http://arxiv.org/abs/2101.06702",
          "publishedOn": "2021-08-03T02:06:32.028Z",
          "wordCount": 708,
          "title": "Deep Learning based Virtual Point Tracking for Real-Time Target-less Dynamic Displacement Measurement in Railway Applications. (arXiv:2101.06702v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaxiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xueming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Li Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "Recently, some approaches are proposed to harness deep convolutional networks\nto facilitate superpixel segmentation. The common practice is to first evenly\ndivide the image into a pre-defined number of grids and then learn to associate\neach pixel with its surrounding grids. However, simply applying a series of\nconvolution operations with limited receptive fields can only implicitly\nperceive the relations between the pixel and its surrounding grids.\nConsequently, existing methods often fail to provide an effective context when\ninferring the association map. To remedy this issue, we propose a novel\n\\textbf{A}ssociation \\textbf{I}mplantation (AI) module to enable the network to\nexplicitly capture the relations between the pixel and its surrounding grids.\nThe proposed AI module directly implants the features of grid cells to the\nsurrounding of its corresponding central pixel, and conducts convolution on the\npadded window to adaptively transfer knowledge between them. With such an\nimplantation operation, the network could explicitly harvest the pixel-grid\nlevel context, which is more in line with the target of superpixel segmentation\ncomparing to the pixel-wise relation. Furthermore, to pursue better boundary\nprecision, we design a boundary-perceiving loss to help the network\ndiscriminate the pixels around boundaries in hidden feature level, which could\nbenefit the subsequent inferring modules to accurately identify more boundary\npixels. Extensive experiments on BSDS500 and NYUv2 datasets show that our\nmethod could not only achieve state-of-the-art performance but maintain\nsatisfactory inference efficiency.",
          "link": "http://arxiv.org/abs/2101.10696",
          "publishedOn": "2021-08-03T02:06:31.986Z",
          "wordCount": 714,
          "title": "AINet: Association Implantation for Superpixel Segmentation. (arXiv:2101.10696v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_P/0/1/0/all/0/1\">Pranav Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaucorps_P/0/1/0/all/0/1\">Pierre de Beaucorps</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>",
          "description": "Deep reinforcement Learning for end-to-end driving is limited by the need of\ncomplex reward engineering. Sparse rewards can circumvent this challenge but\nsuffers from long training time and leads to sub-optimal policy. In this work,\nwe explore full-control driving with only goal-constrained sparse reward and\npropose a curriculum learning approach for end-to-end driving using only\nnavigation view maps that benefit from small virtual-to-real domain gap. To\naddress the complexity of multiple driving policies, we learn concurrent\nindividual policies selected at inference by a navigation system. We\ndemonstrate the ability of our proposal to generalize on unseen road layout,\nand to drive significantly longer than in the training.",
          "link": "http://arxiv.org/abs/2103.09189",
          "publishedOn": "2021-08-03T02:06:31.976Z",
          "wordCount": 578,
          "title": "Goal-constrained Sparse Reinforcement Learning for End-to-End Driving. (arXiv:2103.09189v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Rui Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Ming Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1\">Raquel Urtasun</a>",
          "description": "3D object detection is a key component of many robotic applications such as\nself-driving vehicles. While many approaches rely on expensive 3D sensors such\nas LiDAR to produce accurate 3D estimates, methods that exploit stereo cameras\nhave recently shown promising results at a lower cost. Existing approaches\ntackle this problem in two steps: first depth estimation from stereo images is\nperformed to produce a pseudo LiDAR point cloud, which is then used as input to\na 3D object detector. However, this approach is suboptimal due to the\nrepresentation mismatch, as the two tasks are optimized in two different metric\nspaces. In this paper we propose a model that unifies these two tasks and\nperforms them in the same metric space. Specifically, we directly construct a\npseudo LiDAR feature volume (PLUME) in 3D space, which is then used to solve\nboth depth estimation and object detection tasks. Our approach achieves\nstate-of-the-art performance with much faster inference times when compared to\nexisting methods on the challenging KITTI benchmark.",
          "link": "http://arxiv.org/abs/2101.06594",
          "publishedOn": "2021-08-03T02:06:31.966Z",
          "wordCount": 645,
          "title": "PLUMENet: Efficient 3D Object Detection from Stereo Images. (arXiv:2101.06594v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nenggan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "People live in a 3D world. However, existing works on person\nre-identification (re-id) mostly consider the semantic representation learning\nin a 2D space, intrinsically limiting the understanding of people. In this\nwork, we address this limitation by exploring the prior knowledge of the 3D\nbody structure. Specifically, we project 2D images to a 3D space and introduce\na novel parameter-efficient Omni-scale Graph Network (OG-Net) to learn the\npedestrian representation directly from 3D point clouds. OG-Net effectively\nexploits the local information provided by sparse 3D points and takes advantage\nof the structure and appearance information in a coherent manner. With the help\nof 3D geometry information, we can learn a new type of deep re-id feature free\nfrom noisy variants, such as scale and viewpoint. To our knowledge, we are\namong the first attempts to conduct person re-identification in the 3D space.\nWe demonstrate through extensive experiments that the proposed method (1) eases\nthe matching difficulty in the traditional 2D space, (2) exploits the\ncomplementary information of 2D appearance and 3D structure, (3) achieves\ncompetitive results with limited parameters on four large-scale person re-id\ndatasets, and (4) has good scalability to unseen datasets. Our code, models and\ngenerated 3D human data are publicly available at\nhttps://github.com/layumi/person-reid-3d .",
          "link": "http://arxiv.org/abs/2006.04569",
          "publishedOn": "2021-08-03T02:06:31.958Z",
          "wordCount": 685,
          "title": "Parameter-Efficient Person Re-identification in the 3D Space. (arXiv:2006.04569v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12086",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yubao Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Ying Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qingshan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>",
          "description": "Hyperspectral compressive imaging takes advantage of compressive sensing\ntheory to achieve coded aperture snapshot measurement without temporal\nscanning, and the entire three-dimensional spatial-spectral data is captured by\na two-dimensional projection during a single integration period. Its core issue\nis how to reconstruct the underlying hyperspectral image using compressive\nsensing reconstruction algorithms. Due to the diversity in the spectral\nresponse characteristics and wavelength range of different spectral imaging\ndevices, previous works are often inadequate to capture complex spectral\nvariations or lack the adaptive capacity to new hyperspectral imagers. In order\nto address these issues, we propose an unsupervised spatial-spectral network to\nreconstruct hyperspectral images only from the compressive snapshot\nmeasurement. The proposed network acts as a conditional generative model\nconditioned on the snapshot measurement, and it exploits the spatial-spectral\nattention module to capture the joint spatial-spectral correlation of\nhyperspectral images. The network parameters are optimized to make sure that\nthe network output can closely match the given snapshot measurement according\nto the imaging model, thus the proposed network can adapt to different imaging\nsettings, which can inherently enhance the applicability of the network.\nExtensive experiments upon multiple datasets demonstrate that our network can\nachieve better reconstruction results than the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2012.12086",
          "publishedOn": "2021-08-03T02:06:31.942Z",
          "wordCount": 664,
          "title": "Unsupervised Spatial-spectral Network Learning for Hyperspectral Compressive Snapshot Reconstruction. (arXiv:2012.12086v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zunlei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhonghua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiuming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lechao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuexuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>",
          "description": "The microvascular invasion (MVI) is a major prognostic factor in\nhepatocellular carcinoma, which is one of the malignant tumors with the highest\nmortality rate. The diagnosis of MVI needs discovering the vessels that contain\nhepatocellular carcinoma cells and counting their number in each vessel, which\ndepends heavily on experiences of the doctor, is largely subjective and\ntime-consuming. However, there is no algorithm as yet tailored for the MVI\ndetection from pathological images. This paper collects the first pathological\nliver image dataset containing 522 whole slide images with labels of vessels,\nMVI, and hepatocellular carcinoma grades. The first and essential step for the\nautomatic diagnosis of MVI is the accurate segmentation of vessels. The unique\ncharacteristics of pathological liver images, such as super-large size,\nmulti-scale vessel, and blurred vessel edges, make the accurate vessel\nsegmentation challenging. Based on the collected dataset, we propose an\nEdge-competing Vessel Segmentation Network (EVS-Net), which contains a\nsegmentation network and two edge segmentation discriminators. The segmentation\nnetwork, combined with an edge-aware self-supervision mechanism, is devised to\nconduct vessel segmentation with limited labeled patches. Meanwhile, two\ndiscriminators are introduced to distinguish whether the segmented vessel and\nbackground contain residual features in an adversarial manner. In the training\nstage, two discriminators are devised tocompete for the predicted position of\nedges. Exhaustive experiments demonstrate that, with only limited labeled\npatches, EVS-Net achieves a close performance of fully supervised methods,\nwhich provides a convenient tool for the pathological liver vessel\nsegmentation. Code is publicly available at\nhttps://github.com/zju-vipa/EVS-Net.",
          "link": "http://arxiv.org/abs/2108.00384",
          "publishedOn": "2021-08-03T02:06:31.925Z",
          "wordCount": 697,
          "title": "Edge-competing Pathological Liver Vessel Segmentation with Limited Labels. (arXiv:2108.00384v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wentao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Zhiyu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_C/0/1/0/all/0/1\">Chengyu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiman Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Tingming Bai</a>",
          "description": "Although instance segmentation has made considerable advancement over recent\nyears, it's still a challenge to design high accuracy algorithms with real-time\nperformance. In this paper, we propose a real-time instance segmentation\nframework termed OrienMask. Upon the one-stage object detector YOLOv3, a mask\nhead is added to predict some discriminative orientation maps, which are\nexplicitly defined as spatial offset vectors for both foreground and background\npixels. Thanks to the discrimination ability of orientation maps, masks can be\nrecovered without the need for extra foreground segmentation. All instances\nthat match with the same anchor size share a common orientation map. This\nspecial sharing strategy reduces the amortized memory utilization for mask\npredictions but without loss of mask granularity. Given the surviving box\npredictions after NMS, instance masks can be concurrently constructed from the\ncorresponding orientation maps with low complexity. Owing to the concise design\nfor mask representation and its effective integration with the anchor-based\nobject detector, our method is qualified under real-time conditions while\nmaintaining competitive accuracy. Experiments on COCO benchmark show that\nOrienMask achieves 34.8 mask AP at the speed of 42.7 fps evaluated with a\nsingle RTX 2080 Ti. The code is available at https://github.com/duwt/OrienMask.",
          "link": "http://arxiv.org/abs/2106.12204",
          "publishedOn": "2021-08-03T02:06:31.879Z",
          "wordCount": 659,
          "title": "Real-time Instance Segmentation with Discriminative Orientation Maps. (arXiv:2106.12204v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+K_P/0/1/0/all/0/1\">Priyadarshini K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Siddhartha Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borkar_V/0/1/0/all/0/1\">Vivek Borkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Subhasis Chaudhuri</a>",
          "description": "Active metric learning is the problem of incrementally selecting high-utility\nbatches of training data (typically, ordered triplets) to annotate, in order to\nprogressively improve a learned model of a metric over some input domain as\nrapidly as possible. Standard approaches, which independently assess the\ninformativeness of each triplet in a batch, are susceptible to highly\ncorrelated batches with many redundant triplets and hence low overall utility.\nWhile a recent work \\cite{kumari2020batch} proposes batch-decorrelation\nstrategies for metric learning, they rely on ad hoc heuristics to estimate the\ncorrelation between two triplets at a time. We present a novel batch active\nmetric learning method that leverages the Maximum Entropy Principle to learn\nthe least biased estimate of triplet distribution for a given set of prior\nconstraints. To avoid redundancy between triplets, our method collectively\nselects batches with maximum joint entropy, which simultaneously captures both\ninformativeness and diversity. We take advantage of the submodularity of the\njoint entropy function to construct a tractable solution using an efficient\ngreedy algorithm based on Gram-Schmidt orthogonalization that is provably\n$\\left( 1 - \\frac{1}{e} \\right)$-optimal. Our approach is the first batch\nactive metric learning method to define a unified score that balances\ninformativeness and diversity for an entire batch of triplets. Experiments with\nseveral real-world datasets demonstrate that our algorithm is robust,\ngeneralizes well to different applications and input modalities, and\nconsistently outperforms the state-of-the-art.",
          "link": "http://arxiv.org/abs/2102.07365",
          "publishedOn": "2021-08-03T02:06:31.872Z",
          "wordCount": 736,
          "title": "A Unified Batch Selection Policy for Active Metric Learning. (arXiv:2102.07365v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feixiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zheheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhihua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_L/0/1/0/all/0/1\">Lei Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhile Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haikuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_M/0/1/0/all/0/1\">Minrui Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Ling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>",
          "description": "Automated analysis of mouse behaviours is crucial for many applications in\nneuroscience. However, quantifying mouse behaviours from videos or images\nremains a challenging problem, where pose estimation plays an important role in\ndescribing mouse behaviours. Although deep learning based methods have made\npromising advances in human pose estimation, they cannot be directly applied to\npose estimation of mice due to different physiological natures. Particularly,\nsince mouse body is highly deformable, it is a challenge to accurately locate\ndifferent keypoints on the mouse body. In this paper, we propose a novel\nHourglass network based model, namely Graphical Model based Structured Context\nEnhancement Network (GM-SCENet) where two effective modules, i.e., Structured\nContext Mixer (SCM) and Cascaded Multi-Level Supervision (CMLS) are\nsubsequently implemented. SCM can adaptively learn and enhance the proposed\nstructured context information of each mouse part by a novel graphical model\nthat takes into account the motion difference between body parts. Then, the\nCMLS module is designed to jointly train the proposed SCM and the Hourglass\nnetwork by generating multi-level information, increasing the robustness of the\nwhole network.Using the multi-level prediction information from SCM and CMLS,\nwe develop an inference method to ensure the accuracy of the localisation\nresults. Finally, we evaluate our proposed approach against several\nbaselines...",
          "link": "http://arxiv.org/abs/2012.00630",
          "publishedOn": "2021-08-03T02:06:31.865Z",
          "wordCount": 707,
          "title": "Structured Context Enhancement Network for Mouse Pose Estimation. (arXiv:2012.00630v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nakashima_K/0/1/0/all/0/1\">Kazuto Nakashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurazume_R/0/1/0/all/0/1\">Ryo Kurazume</a>",
          "description": "3D laser scanning by LiDAR sensors plays an important role for mobile robots\nto understand their surroundings. Nevertheless, not all systems have high\nresolution and accuracy due to hardware limitations, weather conditions, and so\non. Generative modeling of LiDAR data as scene priors is one of the promising\nsolutions to compensate for unreliable or incomplete observations. In this\npaper, we propose a novel generative model for learning LiDAR data based on\ngenerative adversarial networks. As in the related studies, we process LiDAR\ndata as a compact yet lossless representation, a cylindrical depth map.\nHowever, despite the smoothness of real-world objects, many points on the depth\nmap are dropped out through the laser measurement, which causes learning\ndifficulty on generative models. To circumvent this issue, we introduce\nmeasurement uncertainty into the generation process, which allows the model to\nlearn a disentangled representation of the underlying shape and the dropout\nnoises from a collection of real LiDAR data. To simulate the lossy measurement,\nwe adopt a differentiable sampling framework to drop points based on the\nlearned uncertainty. We demonstrate the effectiveness of our method on\nsynthesis and reconstruction tasks using two datasets. We further showcase\npotential applications by restoring LiDAR data with various types of\ncorruption.",
          "link": "http://arxiv.org/abs/2102.11952",
          "publishedOn": "2021-08-03T02:06:31.828Z",
          "wordCount": 675,
          "title": "Learning to Drop Points for LiDAR Scan Synthesis. (arXiv:2102.11952v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1\">Mustafa Hajij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batayneh_F/0/1/0/all/0/1\">Fawwaz Batayneh</a>",
          "description": "Topological Data Analysis (TDA) has emerged recently as a robust tool to\nextract and compare the structure of datasets. TDA identifies features in data\nsuch as connected components and holes and assigns a quantitative measure to\nthese features. Several studies reported that topological features extracted by\nTDA tools provide unique information about the data, discover new insights, and\ndetermine which feature is more related to the outcome. On the other hand, the\noverwhelming success of deep neural networks in learning patterns and\nrelationships has been proven on a vast array of data applications, images in\nparticular. To capture the characteristics of both powerful tools, we propose\n\\textit{TDA-Net}, a novel ensemble network that fuses topological and deep\nfeatures for the purpose of enhancing model generalizability and accuracy. We\napply the proposed \\textit{TDA-Net} to a critical application, which is the\nautomated detection of COVID-19 from CXR images. The experimental results\nshowed that the proposed network achieved excellent performance and suggests\nthe applicability of our method in practice.",
          "link": "http://arxiv.org/abs/2101.08398",
          "publishedOn": "2021-08-03T02:06:31.821Z",
          "wordCount": 718,
          "title": "TDA-Net: Fusion of Persistent Homology and Deep Learning Features for COVID-19 Detection in Chest X-Ray Images. (arXiv:2101.08398v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Je Hyeong Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hanjo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1\">Gi Pyo Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Junghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hyeong-Seok Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1\">Ig-Jae Kim</a>",
          "description": "Face recognition now requires a large number of labelled masked face images\nin the era of this unprecedented COVID-19 pandemic. Unfortunately, the rapid\nspread of the virus has left us little time to prepare for such dataset in the\nwild. To circumvent this issue, we present a 3D model-based approach called\nWearMask3D for augmenting face images of various poses to the masked face\ncounterparts. Our method proceeds by first fitting a 3D morphable model on the\ninput image, second overlaying the mask surface onto the face model and warping\nthe respective mask texture, and last projecting the 3D mask back to 2D. The\nmask texture is adapted based on the brightness and resolution of the input\nimage. By working in 3D, our method can produce more natural masked faces of\ndiverse poses from a single mask texture. To compare precisely between\ndifferent augmentation approaches, we have constructed a dataset comprising\nmasked and unmasked faces with labels called MFW-mini. Experimental results\ndemonstrate WearMask3D produces more realistic masked faces, and utilizing\nthese images for training leads to state-of-the-art recognition accuracy for\nmasked faces.",
          "link": "http://arxiv.org/abs/2103.00803",
          "publishedOn": "2021-08-03T02:06:31.794Z",
          "wordCount": 721,
          "title": "A 3D model-based approach for fitting masks to faces in the wild. (arXiv:2103.00803v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.10420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1\">Gustavo Henrique de Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcao_A/0/1/0/all/0/1\">Alexandre Xavier Falc&#xe3;o</a>",
          "description": "Machine learning techniques have been paramount throughout the last years,\nbeing applied in a wide range of tasks, such as classification, object\nrecognition, person identification, and image segmentation. Nevertheless,\nconventional classification algorithms, e.g., Logistic Regression, Decision\nTrees, and Bayesian classifiers, might lack complexity and diversity, not\nsuitable when dealing with real-world data. A recent graph-inspired classifier,\nknown as the Optimum-Path Forest, has proven to be a state-of-the-art\ntechnique, comparable to Support Vector Machines and even surpassing it in some\ntasks. This paper proposes a Python-based Optimum-Path Forest framework,\ndenoted as OPFython, where all of its functions and classes are based upon the\noriginal C language implementation. Additionally, as OPFython is a Python-based\nlibrary, it provides a more friendly environment and a faster prototyping\nworkspace than the C language.",
          "link": "http://arxiv.org/abs/2001.10420",
          "publishedOn": "2021-08-03T02:06:31.787Z",
          "wordCount": 628,
          "title": "OPFython: A Python-Inspired Optimum-Path Forest Classifier. (arXiv:2001.10420v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruilong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1\">David A. Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1\">Angjoo Kanazawa</a>",
          "description": "We present AIST++, a new multi-modal dataset of 3D dance motion and music,\nalong with FACT, a Full-Attention Cross-modal Transformer network for\ngenerating 3D dance motion conditioned on music. The proposed AIST++ dataset\ncontains 5.2 hours of 3D dance motion in 1408 sequences, covering 10 dance\ngenres with multi-view videos with known camera poses -- the largest dataset of\nthis kind to our knowledge. We show that naively applying sequence models such\nas transformers to this dataset for the task of music conditioned 3D motion\ngeneration does not produce satisfactory 3D motion that is well correlated with\nthe input music. We overcome these shortcomings by introducing key changes in\nits architecture design and supervision: FACT model involves a deep cross-modal\ntransformer block with full-attention that is trained to predict $N$ future\nmotions. We empirically show that these changes are key factors in generating\nlong sequences of realistic dance motion that are well-attuned to the input\nmusic. We conduct extensive experiments on AIST++ with user studies, where our\nmethod outperforms recent state-of-the-art methods both qualitatively and\nquantitatively.",
          "link": "http://arxiv.org/abs/2101.08779",
          "publishedOn": "2021-08-03T02:06:31.779Z",
          "wordCount": 670,
          "title": "AI Choreographer: Music Conditioned 3D Dance Generation with AIST++. (arXiv:2101.08779v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.02620",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1\">Tian Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1\">Agisilaos Chartsias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chengjia Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>",
          "description": "How will my face look when I get older? Or, for a more challenging question:\nHow will my brain look when I get older? To answer this question one must\ndevise (and learn from data) a multivariate auto-regressive function which\ngiven an image and a desired target age generates an output image. While\ncollecting data for faces may be easier, collecting longitudinal brain data is\nnot trivial. We propose a deep learning-based method that learns to simulate\nsubject-specific brain ageing trajectories without relying on longitudinal\ndata. Our method synthesises images conditioned on two factors: age (a\ncontinuous variable), and status of Alzheimer's Disease (AD, an ordinal\nvariable). With an adversarial formulation we learn the joint distribution of\nbrain appearance, age and AD status, and define reconstruction losses to\naddress the challenging problem of preserving subject identity. We compare with\nseveral benchmarks using two widely used datasets. We evaluate the quality and\nrealism of synthesised images using ground-truth longitudinal data and a\npre-trained age predictor. We show that, despite the use of cross-sectional\ndata, our model learns patterns of gray matter atrophy in the middle temporal\ngyrus in patients with AD. To demonstrate generalisation ability, we train on\none dataset and evaluate predictions on the other. In conclusion, our model\nshows an ability to separate age, disease influence and anatomy using only 2D\ncross-sectional data that should should be useful in large studies into\nneurodegenerative disease, that aim to combine several data sources. To\nfacilitate such future studies by the community at large our code is made\navailable at https://github.com/xiat0616/BrainAgeing.",
          "link": "http://arxiv.org/abs/1912.02620",
          "publishedOn": "2021-08-03T02:06:31.769Z",
          "wordCount": 762,
          "title": "Learning to synthesise the ageing brain without longitudinal data. (arXiv:1912.02620v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeevan_P/0/1/0/all/0/1\">Pranav Jeevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1\">Amit Sethi</a> (Indian Institute of Technology Bombay)",
          "description": "We propose three improvements to vision transformers (ViT) to reduce the\nnumber of trainable parameters without compromising classification accuracy. We\naddress two shortcomings of the early ViT architectures -- quadratic bottleneck\nof the attention mechanism and the lack of an inductive bias in their\narchitectures that rely on unrolling the two-dimensional image structure.\nLinear attention mechanisms overcome the bottleneck of quadratic complexity,\nwhich restricts application of transformer models in vision tasks. We modify\nthe ViT architecture to work on longer sequence data by replacing the quadratic\nattention with efficient transformers, such as Performer, Linformer and\nNystr\\\"omformer of linear complexity creating Vision X-formers (ViX). We show\nthat all three versions of ViX may be more accurate than ViT for image\nclassification while using far fewer parameters and computational resources. We\nalso compare their performance with FNet and multi-layer perceptron (MLP)\nmixer. We further show that replacing the initial linear embedding layer by\nconvolutional layers in ViX further increases their performance. Furthermore,\nour tests on recent vision transformer models, such as LeViT, Convolutional\nvision Transformer (CvT), Compact Convolutional Transformer (CCT) and\nPooling-based Vision Transformer (PiT) show that replacing the attention with\nNystr\\\"omformer or Performer saves GPU usage and memory without deteriorating\nthe classification accuracy. We also show that replacing the standard learnable\n1D position embeddings in ViT with Rotary Position Embedding (RoPE) give\nfurther improvements in accuracy. Incorporating these changes can democratize\ntransformers by making them accessible to those with limited data and computing\nresources.",
          "link": "http://arxiv.org/abs/2107.02239",
          "publishedOn": "2021-08-03T02:06:31.748Z",
          "wordCount": 733,
          "title": "Vision Xformers: Efficient Attention for Image Classification. (arXiv:2107.02239v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>",
          "description": "Siamese tracking has achieved groundbreaking performance in recent years,\nwhere the essence is the efficient matching operator cross-correlation and its\nvariants. Besides the remarkable success, it is important to note that the\nheuristic matching network design relies heavily on expert experience.\nMoreover, we experimentally find that one sole matching operator is difficult\nto guarantee stable tracking in all challenging environments. Thus, in this\nwork, we introduce six novel matching operators from the perspective of feature\nfusion instead of explicit similarity learning, namely Concatenation,\nPointwise-Addition, Pairwise-Relation, FiLM, Simple-Transformer and\nTransductive-Guidance, to explore more feasibility on matching operator\nselection. The analyses reveal these operators' selective adaptability on\ndifferent environment degradation types, which inspires us to combine them to\nexplore complementary features. To this end, we propose binary channel\nmanipulation (BCM) to search for the optimal combination of these operators.\nBCM determines to retrain or discard one operator by learning its contribution\nto other tracking steps. By inserting the learned matching networks to a strong\nbaseline tracker Ocean, our model achieves favorable gains by $67.2 \\rightarrow\n71.4$, $52.6 \\rightarrow 58.3$, $70.3 \\rightarrow 76.0$ success on OTB100,\nLaSOT, and TrackingNet, respectively. Notably, Our tracker, dubbed AutoMatch,\nuses less than half of training data/time than the baseline tracker, and runs\nat 50 FPS using PyTorch. Code and model will be released at\nhttps://github.com/JudasDie/SOTS.",
          "link": "http://arxiv.org/abs/2108.00803",
          "publishedOn": "2021-08-03T02:06:31.725Z",
          "wordCount": 666,
          "title": "Learn to Match: Automatic Matching Network Design for Visual Tracking. (arXiv:2108.00803v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1\">Olivier Deforges</a>",
          "description": "Salient human detection (SHD) in dynamic 360{\\deg} immersive videos is of\ngreat importance for various applications such as robotics, inter-human and\nhuman-object interaction in augmented reality. However, 360{\\deg} video SHD has\nbeen seldom discussed in the computer vision community due to a lack of\ndatasets with large-scale omnidirectional videos and rich annotations. To this\nend, we propose SHD360, the first 360{\\deg} video SHD dataset which contains\nvarious real-life daily scenes. Our SHD360 provides six-level hierarchical\nannotations for 6,268 key frames uniformly sampled from 37,403 omnidirectional\nvideo frames at 4K resolution. Specifically, each collected frame is labeled\nwith a super-class, a sub-class, associated attributes (e.g., geometrical\ndistortion), bounding boxes and per-pixel object-/instance-level masks. As a\nresult, our SHD360 contains totally 16,238 salient human instances with\nmanually annotated pixel-wise ground truth. Since so far there is no method\nproposed for 360{\\deg} image/video SHD, we systematically benchmark 11\nrepresentative state-of-the-art salient object detection (SOD) approaches on\nour SHD360, and explore key issues derived from extensive experimenting\nresults. We hope our proposed dataset and benchmark could serve as a good\nstarting point for advancing human-centric researches towards 360{\\deg}\npanoramic data. Our dataset and benchmark will be publicly available at\nhttps://github.com/PanoAsh/SHD360.",
          "link": "http://arxiv.org/abs/2105.11578",
          "publishedOn": "2021-08-03T02:06:31.712Z",
          "wordCount": 691,
          "title": "SHD360: A Benchmark Dataset for Salient Human Detection in 360{\\deg} Videos. (arXiv:2105.11578v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sarah Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_E/0/1/0/all/0/1\">Esther Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Praveen_S/0/1/0/all/0/1\">Satyarth Praveen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1\">Meher Anand Kasam</a>",
          "description": "Due to the nature of their pathways, NASA Terra and NASA Aqua satellites\ncapture imagery containing swath gaps, which are areas of no data. Swath gaps\ncan overlap the region of interest (ROI) completely, often rendering the entire\nimagery unusable by Machine Learning (ML) models. This problem is further\nexacerbated when the ROI rarely occurs (e.g. a hurricane) and, on occurrence,\nis partially overlapped with a swath gap. With annotated data as supervision, a\nmodel can learn to differentiate between the area of focus and the swath gap.\nHowever, annotation is expensive and currently the vast majority of existing\ndata is unannotated. Hence, we propose an augmentation technique that\nconsiderably removes the existence of swath gaps in order to allow CNNs to\nfocus on the ROI, and thus successfully use data with swath gaps for training.\nWe experiment on the UC Merced Land Use Dataset, where we add swath gaps\nthrough empty polygons (up to 20 percent areas) and then apply augmentation\ntechniques to fill the swath gaps. We compare the model trained with our\naugmentation techniques on the swath gap-filled data with the model trained on\nthe original swath gap-less data and note highly augmented performance.\nAdditionally, we perform a qualitative analysis using activation maps that\nvisualizes the effectiveness of our trained network in not paying attention to\nthe swath gaps. We also evaluate our results with a human baseline and show\nthat, in certain cases, the filled swath gaps look so realistic that even a\nhuman evaluator did not distinguish between original satellite images and swath\ngap-filled images. Since this method is aimed at unlabeled data, it is widely\ngeneralizable and impactful for large scale unannotated datasets from various\nspace data domains.",
          "link": "http://arxiv.org/abs/2106.07113",
          "publishedOn": "2021-08-03T02:06:31.702Z",
          "wordCount": 790,
          "title": "Reducing Effects of Swath Gaps on Unsupervised Machine Learning Models for NASA MODIS Instruments. (arXiv:2106.07113v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.01037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiamei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1\">Sebastian Lapuschkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1\">Wojciech Samek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1\">Alexander Binder</a>",
          "description": "This paper analyzes the predictions of image captioning models with attention\nmechanisms beyond visualizing the attention itself. We develop variants of\nlayer-wise relevance propagation (LRP) and gradient-based explanation methods,\ntailored to image captioning models with attention mechanisms. We compare the\ninterpretability of attention heatmaps systematically against the explanations\nprovided by explanation methods such as LRP, Grad-CAM, and Guided Grad-CAM. We\nshow that explanation methods provide simultaneously pixel-wise image\nexplanations (supporting and opposing pixels of the input image) and linguistic\nexplanations (supporting and opposing words of the preceding sequence) for each\nword in the predicted captions. We demonstrate with extensive experiments that\nexplanation methods 1) can reveal additional evidence used by the model to make\ndecisions compared to attention; 2) correlate to object locations with high\nprecision; 3) are helpful to \"debug\" the model, e.g. by analyzing the reasons\nfor hallucinated object words. With the observed properties of explanations, we\nfurther design an LRP-inference fine-tuning strategy that reduces the issue of\nobject hallucination in image captioning models, and meanwhile, maintains the\nsentence fluency. We conduct experiments with two widely used attention\nmechanisms: the adaptive attention mechanism calculated with the additive\nattention and the multi-head attention mechanism calculated with the scaled dot\nproduct.",
          "link": "http://arxiv.org/abs/2001.01037",
          "publishedOn": "2021-08-03T02:06:31.685Z",
          "wordCount": 703,
          "title": "Explain and Improve: LRP-Inference Fine-Tuning for Image Captioning Models. (arXiv:2001.01037v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1\">Changyong Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Knowledge distillation (KD) has been proven to be a simple and effective tool\nfor training compact models. Almost all KD variants for dense prediction tasks\nalign the student and teacher networks' feature maps in the spatial domain,\ntypically by minimizing point-wise and/or pair-wise discrepancy. Observing that\nin semantic segmentation, some layers' feature activations of each channel tend\nto encode saliency of scene categories (analogue to class activation mapping),\nwe propose to align features channel-wise between the student and teacher\nnetworks. To this end, we first transform the feature map of each channel into\na probabilty map using softmax normalization, and then minimize the\nKullback-Leibler (KL) divergence of the corresponding channels of the two\nnetworks. By doing so, our method focuses on mimicking the soft distributions\nof channels between networks. In particular, the KL divergence enables learning\nto pay more attention to the most salient regions of the channel-wise maps,\npresumably corresponding to the most useful signals for semantic segmentation.\nExperiments demonstrate that our channel-wise distillation outperforms almost\nall existing spatial distillation methods for semantic segmentation\nconsiderably, and requires less computational cost during training. We\nconsistently achieve superior performance on three benchmarks with various\nnetwork structures. Code is available at: https://git.io/ChannelDis",
          "link": "http://arxiv.org/abs/2011.13256",
          "publishedOn": "2021-08-03T02:06:31.670Z",
          "wordCount": 690,
          "title": "Channel-wise Knowledge Distillation for Dense Prediction. (arXiv:2011.13256v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04007",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Legarreta_J/0/1/0/all/0/1\">Jon Haitz Legarreta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petit_L/0/1/0/all/0/1\">Laurent Petit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rheault_F/0/1/0/all/0/1\">Fran&#xe7;ois Rheault</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Theaud_G/0/1/0/all/0/1\">Guillaume Theaud</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lemaire_C/0/1/0/all/0/1\">Carl Lemaire</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Descoteaux_M/0/1/0/all/0/1\">Maxime Descoteaux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>",
          "description": "Current brain white matter fiber tracking techniques show a number of\nproblems, including: generating large proportions of streamlines that do not\naccurately describe the underlying anatomy; extracting streamlines that are not\nsupported by the underlying diffusion signal; and under-representing some fiber\npopulations, among others. In this paper, we describe a novel autoencoder-based\nlearning method to filter streamlines from diffusion MRI tractography, and\nhence, to obtain more reliable tractograms. Our method, dubbed FINTA (Filtering\nin Tractography using Autoencoders) uses raw, unlabeled tractograms to train\nthe autoencoder, and to learn a robust representation of brain streamlines.\nSuch an embedding is then used to filter undesired streamline samples using a\nnearest neighbor algorithm. Our experiments on both synthetic and in vivo human\nbrain diffusion MRI tractography data obtain accuracy scores exceeding the 90\\%\nthreshold on the test set. Results reveal that FINTA has a superior filtering\nperformance compared to conventional, anatomy-based methods, and the\nRecoBundles state-of-the-art method. Additionally, we demonstrate that FINTA\ncan be applied to partial tractograms without requiring changes to the\nframework. We also show that the proposed method generalizes well across\ndifferent tracking methods and datasets, and shortens significantly the\ncomputation time for large (>1 M streamlines) tractograms. Together, this work\nbrings forward a new deep learning framework in tractography based on\nautoencoders, which offers a flexible and powerful method for white matter\nfiltering and bundling that could enhance tractometry and connectivity\nanalyses.",
          "link": "http://arxiv.org/abs/2010.04007",
          "publishedOn": "2021-08-03T02:06:31.662Z",
          "wordCount": 748,
          "title": "Filtering in tractography using autoencoders (FINTA). (arXiv:2010.04007v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1808.08186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Misra_R/0/1/0/all/0/1\">Rajesh Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_K/0/1/0/all/0/1\">Kumar S. Ray</a>",
          "description": "In Computer Vision,object tracking is a very old and complex problem.Though\nthere are several existing algorithms for object tracking, still there are\nseveral challenges remain to be solved. For instance, variation of illumination\nof light, noise, occlusion, sudden start and stop of moving object, shading\netc,make the object tracking a complex problem not only for dynamic background\nbut also for static background. In this paper we propose a dual approach for\nobject tracking based on optical flow and swarm Intelligence.The optical flow\nbased KLT(Kanade-Lucas-Tomasi) tracker, tracks the dominant points of the\ntarget object from first frame to last frame of a video sequence;whereas swarm\nIntelligence based PSO (Particle Swarm Optimization) tracker simultaneously\ntracks the boundary information of the target object from second frame to last\nframe of the same video sequence.This dual function of tracking makes the\ntrackers very much robust with respect to the above stated problems. The\nflexibility of our approach is that it can be successfully applicable in\nvariable background as well as static background.We compare the performance of\nthe proposed dual tracking algorithm with several benchmark datasets and obtain\nvery competitive results in general and in most of the cases we obtained\nsuperior results using dual tracking algorithm. We also compare the performance\nof the proposed dual tracker with some existing PSO based algorithms for\ntracking and achieved better results.",
          "link": "http://arxiv.org/abs/1808.08186",
          "publishedOn": "2021-08-03T02:06:31.636Z",
          "wordCount": 700,
          "title": "Dual approach for object tracking based on optical flow and swarm intelligence. (arXiv:1808.08186v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conway_D/0/1/0/all/0/1\">Dennis Conway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_L/0/1/0/all/0/1\">Loic Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lechervy_A/0/1/0/all/0/1\">Alexis Lechervy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurie_F/0/1/0/all/0/1\">Frederic Jurie</a>",
          "description": "Machine learning tools are becoming increasingly powerful and widely used.\nUnfortunately membership attacks, which seek to uncover information from data\nsets used in machine learning, have the potential to limit data sharing. In\nthis paper we consider an approach to increase the privacy protection of data\nsets, as applied to face recognition. Using an auxiliary face recognition\nmodel, we build on the StyleGAN generative adversarial network and feed it with\nlatent codes combining two distinct sub-codes, one encoding visual identity\nfactors, and, the other, non-identity factors. By independently varying these\nvectors during image generation, we create a synthetic data set of fictitious\nface identities. We use this data set to train a face recognition model. The\nmodel performance degrades in comparison to the state-of-the-art of face\nverification. When tested with a simple membership attack our model provides\ngood privacy protection, however the model performance degrades in comparison\nto the state-of-the-art of face verification. We find that the addition of a\nsmall amount of private data greatly improves the performance of our model,\nwhich highlights the limitations of using synthetic data to train machine\nlearning models.",
          "link": "http://arxiv.org/abs/2108.00800",
          "publishedOn": "2021-08-03T02:06:31.629Z",
          "wordCount": 622,
          "title": "Training face verification models from generated face identity data. (arXiv:2108.00800v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1904.05519",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1\">Uttaran Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Govindu_V/0/1/0/all/0/1\">Venu Madhav Govindu</a>",
          "description": "We present an accurate, robust and fast method for registration of 3D scans.\nOur motion estimation optimizes a robust cost function on the intrinsic\nrepresentation of rigid motions, i.e., the Special Euclidean group\n$\\mathbb{SE}(3)$. We exploit the geometric properties of Lie groups as well as\nthe robustness afforded by an iteratively reweighted least squares\noptimization. We also generalize our approach to a joint multiview method that\nsimultaneously solves for the registration of a set of scans. We demonstrate\nthe efficacy of our approach by thorough experimental validation. Our approach\nsignificantly outperforms the state-of-the-art robust 3D registration method\nbased on a line process in terms of both speed and accuracy. We also show that\nthis line process method is a special case of our principled geometric\nsolution. Finally, we also present scenarios where global registration based on\nfeature correspondences fails but multiview ICP based on our robust motion\nestimation is successful.",
          "link": "http://arxiv.org/abs/1904.05519",
          "publishedOn": "2021-08-03T02:06:31.613Z",
          "wordCount": 635,
          "title": "Efficient and Robust Registration on the 3D Special Euclidean Group. (arXiv:1904.05519v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.03727",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Andreeva_M/0/1/0/all/0/1\">M.V. Andreeva</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kalyuzhnyuk_A/0/1/0/all/0/1\">A.V. Kalyuzhnyuk</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Krutko_V/0/1/0/all/0/1\">V.V. Krutko</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Russkikh_N/0/1/0/all/0/1\">N.E. Russkikh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Taimanov_I/0/1/0/all/0/1\">I.A. Taimanov</a>",
          "description": "Representative Elementary Volume (REV) at which the material properties do\nnot vary with change in volume is an important quantity for making measurements\nor simulations which represent the whole. We discuss the geometrical method to\nevaluation of REV based on the quantities coming in the Steiner formula from\nconvex geometry. For bodies in the three-space this formula gives us four\nscalar functionals known as scalar Minkowski functionals. We demonstrate on\ncertain samples that the values of such averaged functionals almost stabilize\nfor cells for which the length of edges are greater than certain threshold\nvalue R. Therefore, from this point of view, it is reasonable to consider cubes\nof volume R^3 as representative elementary volumes.",
          "link": "http://arxiv.org/abs/2008.03727",
          "publishedOn": "2021-08-03T02:06:31.602Z",
          "wordCount": 576,
          "title": "Representative elementary volume via averaged scalar Minkowski functionals. (arXiv:2008.03727v2 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haichou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yishu Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeqin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haohua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1\">Bingzhong Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaofeng Li</a>",
          "description": "Delineating the lesion area is an important task in image-based diagnosis.\nPixel-wise classification is a popular approach to segmenting the region of\ninterest. However, at fuzzy boundaries such methods usually result in glitches,\ndiscontinuity, or disconnection, inconsistent with the fact that lesions are\nsolid and smooth. To overcome these undesirable artifacts, we propose the\nBezierSeg model which outputs bezier curves encompassing the region of\ninterest. Directly modelling the contour with analytic equations ensures that\nthe segmentation is connected, continuous, and the boundary is smooth. In\naddition, it offers sub-pixel accuracy. Without loss of accuracy, the bezier\ncontour can be resampled and overlaid with images of any resolution. Moreover,\na doctor can conveniently adjust the curve's control points to refine the\nresult. Our experiments show that the proposed method runs in real time and\nachieves accuracy competitive with pixel-wise segmentation models.",
          "link": "http://arxiv.org/abs/2108.00760",
          "publishedOn": "2021-08-03T02:06:31.596Z",
          "wordCount": 592,
          "title": "BezierSeg: Parametric Shape Representation for Fast Object Segmentation in Medical Images. (arXiv:2108.00760v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07200",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sun_K/0/1/0/all/0/1\">Kai Sun</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yanhua Gao</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Xie_T/0/1/0/all/0/1\">Ting Xie</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1\">Qingqing Yang</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Le Chen</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1\">Kuansong Wang</a> (3), <a href=\"http://arxiv.org/find/eess/1/au:+Yu_G/0/1/0/all/0/1\">Gang Yu</a> (1) ((1) Department of Biomedical Engineering, School of Basic Medical Sciences, Central South University, 172 Tongzipo Road, Changsha, 410013, China. (2) Department of Ultrasound, Shaanxi Provincial People&#x27;s Hospital,256 Youyixi Road, Xi&#x27;an, 710068, China. (3) Department of Pathology, School of Basic Medical Sciences, Central South University, 172 Tongzipo Road, Changsha, 410013, China.)",
          "description": "Background. Digital pathology has aroused widespread interest in modern\npathology. The key of digitalization is to scan the whole slide image (WSI) at\nhigh magnification. The lager the magnification is, the richer details WSI will\nprovide, but the scanning time is longer and the file size of obtained is\nlarger. Methods. We design a strategy to scan slides with low resolution (5X)\nand a super-resolution method is proposed to restore the image details when in\ndiagnosis. The method is based on a multi-scale generative adversarial network,\nwhich sequentially generates three high-resolution images such as 10X, 20X and\n40X. Results. The peak-signal-to-noise-ratio of 10X to 40X generated images are\n24.16, 22.27 and 20.44, and the structural-similarity-index are 0.845, 0.680\nand 0.512, which are better than other super-resolution networks. Visual\nscoring average and standard deviation from three pathologists is 3.63\nplus-minus 0.52, 3.70 plus-minus 0.57 and 3.74 plus-minus 0.56 and the p value\nof analysis of variance is 0.37, indicating that generated images include\nsufficient information for diagnosis. The average value of Kappa test is 0.99,\nmeaning the diagnosis of generated images is highly consistent with that of the\nreal images. Conclusion. This proposed method can generate high-quality 10X,\n20X, 40X images from 5X images at the same time, in which the time and storage\ncosts of digitalization can be effectively reduced up to 1/64 of the previous\ncosts. The proposed method provides a better alternative for low-cost storage,\nfaster image share of digital pathology. Keywords. Digital pathology;\nSuper-resolution; Low resolution scanning; Low cost",
          "link": "http://arxiv.org/abs/2105.07200",
          "publishedOn": "2021-08-03T02:06:31.477Z",
          "wordCount": 783,
          "title": "Multi-scale super-resolution generation of low-resolution scanned pathological images. (arXiv:2105.07200v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1\">Anusua Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1\">Mohit Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1\">Nikhil Kumar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinsche_M/0/1/0/all/0/1\">Markus Hinsche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Prashant Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matiaschek_M/0/1/0/all/0/1\">Markus Matiaschek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behrens_T/0/1/0/all/0/1\">Tristan Behrens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Militeri_M/0/1/0/all/0/1\">Mirco Militeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birge_C/0/1/0/all/0/1\">Cameron Birge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaushik_S/0/1/0/all/0/1\">Shivangi Kaushik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_A/0/1/0/all/0/1\">Archisman Mohapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_R/0/1/0/all/0/1\">Rita Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1\">Rahul Dodhia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1\">Juan Lavista Ferres</a>",
          "description": "Malnutrition is a global health crisis and is the leading cause of death\namong children under five. Detecting malnutrition requires anthropometric\nmeasurements of weight, height, and middle-upper arm circumference. However,\nmeasuring them accurately is a challenge, especially in the global south, due\nto limited resources. In this work, we propose a CNN-based approach to estimate\nthe height of standing children under five years from depth images collected\nusing a smart-phone. According to the SMART Methodology Manual [5], the\nacceptable accuracy for height is less than 1.4 cm. On training our deep\nlearning model on 87131 depth images, our model achieved an average mean\nabsolute error of 1.64% on 57064 test images. For 70.3% test images, we\nestimated height accurately within the acceptable 1.4 cm range. Thus, our\nproposed solution can accurately detect stunting (low height-for-age) in\nstanding children below five years of age.",
          "link": "http://arxiv.org/abs/2105.01688",
          "publishedOn": "2021-08-03T02:06:31.441Z",
          "wordCount": 641,
          "title": "Height Estimation of Children under Five Years using Depth Images. (arXiv:2105.01688v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14255",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kang_M/0/1/0/all/0/1\">Myeongkyun Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chikontwe_P/0/1/0/all/0/1\">Philip Chikontwe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luna_M/0/1/0/all/0/1\">Miguel Luna</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hong_K/0/1/0/all/0/1\">Kyung Soo Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahn_J/0/1/0/all/0/1\">June Hong Ahn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1\">Sang Hyun Park</a>",
          "description": "Following the pandemic outbreak, several works have proposed to diagnose\nCOVID-19 with deep learning in computed tomography (CT); reporting performance\non-par with experts. However, models trained/tested on the same in-distribution\ndata may rely on the inherent data biases for successful prediction, failing to\ngeneralize on out-of-distribution samples or CT with different scanning\nprotocols. Early attempts have partly addressed bias-mitigation and\ngeneralization through augmentation or re-sampling, but are still limited by\ncollection costs and the difficulty of quantifying bias in medical images. In\nthis work, we propose Mixing-AdaSIN; a bias mitigation method that uses a\ngenerative model to generate de-biased images by mixing texture information\nbetween different labeled CT scans with semantically similar features. Here, we\nuse Adaptive Structural Instance Normalization (AdaSIN) to enhance de-biasing\ngeneration quality and guarantee structural consistency. Following, a\nclassifier trained with the generated images learns to correctly predict the\nlabel without bias and generalizes better. To demonstrate the efficacy of our\nmethod, we construct a biased COVID-19 vs. bacterial pneumonia dataset based on\nCT protocols and compare with existing state-of-the-art de-biasing methods. Our\nexperiments show that classifiers trained with de-biased generated images\nreport improved in-distribution performance and generalization on an external\nCOVID-19 dataset.",
          "link": "http://arxiv.org/abs/2103.14255",
          "publishedOn": "2021-08-03T02:06:31.434Z",
          "wordCount": 721,
          "title": "Mixing-AdaSIN: Constructing a De-biased Dataset using Adaptive Structural Instance Normalization and Texture Mixing. (arXiv:2103.14255v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_P/0/1/0/all/0/1\">Pablo G&#xf3;mez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meoni_G/0/1/0/all/0/1\">Gabriele Meoni</a>",
          "description": "Supervised learning techniques are at the center of many tasks in remote\nsensing. Unfortunately, these methods, especially recent deep learning methods,\noften require large amounts of labeled data for training. Even though\nsatellites acquire large amounts of data, labeling the data is often tedious,\nexpensive and requires expert knowledge. Hence, improved methods that require\nfewer labeled samples are needed. We present MSMatch, the first semi-supervised\nlearning approach competitive with supervised methods on scene classification\non the EuroSAT and UC Merced Land Use benchmark datasets. We test both RGB and\nmultispectral images of EuroSAT and perform various ablation studies to\nidentify the critical parts of the model. The trained neural network achieves\nstate-of-the-art results on EuroSAT with an accuracy that is up to 19.76%\nbetter than previous methods depending on the number of labeled training\nexamples. With just five labeled examples per class, we reach 94.53% and 95.86%\naccuracy on the EuroSAT RGB and multispectral datasets, respectively. On the UC\nMerced Land Use dataset, we outperform previous works by up to 5.59% and reach\n90.71% with five labeled examples. Our results show that MSMatch is capable of\ngreatly reducing the requirements for labeled data. It translates well to\nmultispectral data and should enable various applications that are currently\ninfeasible due to a lack of labeled data. We provide the source code of MSMatch\nonline to enable easy reproduction and quick adoption.",
          "link": "http://arxiv.org/abs/2103.10368",
          "publishedOn": "2021-08-03T02:06:31.416Z",
          "wordCount": 713,
          "title": "MSMatch: Semi-Supervised Multispectral Scene Classification with Few Labels. (arXiv:2103.10368v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonard_J/0/1/0/all/0/1\">John J. Leonard</a>",
          "description": "For a robot deployed in the world, it is desirable to have the ability of\nautonomous learning to improve its initial pre-set knowledge. We formalize this\nas a bootstrapped self-supervised learning problem where a system is initially\nbootstrapped with supervised training on a labeled dataset and we look for a\nself-supervised training method that can subsequently improve the system over\nthe supervised training baseline using only unlabeled data. In this work, we\nleverage temporal consistency between frames in monocular video to perform this\nbootstrapped self-supervised training. We show that a well-trained\nstate-of-the-art semantic segmentation network can be further improved through\nour method. In addition, we show that the bootstrapped self-supervised training\nframework can help a network learn depth estimation better than pure supervised\ntraining or self-supervised training.",
          "link": "http://arxiv.org/abs/2103.11031",
          "publishedOn": "2021-08-03T02:06:31.403Z",
          "wordCount": 602,
          "title": "Bootstrapped Self-Supervised Training with Monocular Video for Semantic Segmentation and Depth Estimation. (arXiv:2103.11031v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiangpeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengqing Zhu</a>",
          "description": "Continual learning aims to learn new tasks incrementally using less\ncomputation and memory resources instead of retraining the model from scratch\nwhenever new task arrives. However, existing approaches are designed in\nsupervised fashion assuming all data from new tasks have been manually\nannotated, which are not practical for many real-life applications. In this\nwork, we propose to use pseudo label instead of the ground truth to make\ncontinual learning feasible in unsupervised mode. The pseudo labels of new data\nare obtained by applying global clustering algorithm and we propose to use the\nmodel updated from last incremental step as the feature extractor. Due to the\nscarcity of existing work, we introduce a new benchmark experimental protocol\nfor unsupervised continual learning of image classification task under\nclass-incremental setting where no class label is provided for each incremental\nlearning step. Our method is evaluated on the CIFAR-100 and ImageNet (ILSVRC)\ndatasets by incorporating the pseudo label with various existing supervised\napproaches and show promising results in unsupervised scenario.",
          "link": "http://arxiv.org/abs/2104.07164",
          "publishedOn": "2021-08-03T02:06:31.397Z",
          "wordCount": 636,
          "title": "Unsupervised Continual Learning Via Pseudo Labels. (arXiv:2104.07164v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_F/0/1/0/all/0/1\">Feifei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yawei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1\">Lu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>",
          "description": "The recent emerged weakly supervised object localization (WSOL) methods can\nlearn to localize an object in the image only using image-level labels.\nPrevious works endeavor to perceive the interval objects from the small and\nsparse discriminative attention map, yet ignoring the co-occurrence confounder\n(e.g., bird and sky), which makes the model inspection (e.g., CAM) hard to\ndistinguish between the object and context. In this paper, we make an early\nattempt to tackle this challenge via causal intervention (CI). Our proposed\nmethod, dubbed CI-CAM, explores the causalities among images, contexts, and\ncategories to eliminate the biased co-occurrence in the class activation maps\nthus improving the accuracy of object localization. Extensive experiments on\nseveral benchmarks demonstrate the effectiveness of CI-CAM in learning the\nclear object boundaries from confounding contexts. Particularly, in\nCUB-200-2011 which severely suffers from the co-occurrence confounder, CI-CAM\nsignificantly outperforms the traditional CAM-based baseline (58.39% vs 52.4%\nin top-1 localization accuracy). While in more general scenarios such as\nImageNet, CI-CAM can also perform on par with the state of the arts.",
          "link": "http://arxiv.org/abs/2104.10351",
          "publishedOn": "2021-08-03T02:06:31.375Z",
          "wordCount": 663,
          "title": "Improving Weakly-supervised Object Localization via Causal Intervention. (arXiv:2104.10351v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Stan Weixian Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadiyaram_D/0/1/0/all/0/1\">Deepti Ghadiyaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feiszli_M/0/1/0/all/0/1\">Matt Feiszli</a>",
          "description": "This paper presents a novel task together with a new benchmark for detecting\ngeneric, taxonomy-free event boundaries that segment a whole video into chunks.\nConventional work in temporal video segmentation and action detection focuses\non localizing pre-defined action categories and thus does not scale to generic\nvideos. Cognitive Science has known since last century that humans consistently\nsegment videos into meaningful temporal chunks. This segmentation happens\nnaturally, without pre-defined event categories and without being explicitly\nasked to do so. Here, we repeat these cognitive experiments on mainstream CV\ndatasets; with our novel annotation guideline which addresses the complexities\nof taxonomy-free event boundary annotation, we introduce the task of Generic\nEvent Boundary Detection (GEBD) and the new benchmark Kinetics-GEBD. Our\nKinetics-GEBD has the largest number of boundaries (e.g. 32 of ActivityNet, 8\nof EPIC-Kitchens-100) which are in-the-wild, taxonomy-free, cover generic event\nchange, and respect human perception diversity. We view GEBD as an important\nstepping stone towards understanding the video as a whole, and believe it has\nbeen previously neglected due to a lack of proper task definition and\nannotations. Through experiment and human study we demonstrate the value of the\nannotations. Further, we benchmark supervised and un-supervised GEBD approaches\non the TAPOS dataset and our Kinetics-GEBD, together with method design\nexplorations that suggest future directions. We release our annotations and\nbaseline codes at CVPR'21 LOVEU Challenge:\nhttps://sites.google.com/view/loveucvpr21.",
          "link": "http://arxiv.org/abs/2101.10511",
          "publishedOn": "2021-08-03T02:06:31.367Z",
          "wordCount": 719,
          "title": "Generic Event Boundary Detection: A Benchmark for Event Segmentation. (arXiv:2101.10511v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lantao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dehong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_H/0/1/0/all/0/1\">Hassan Mansour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boufounos_P/0/1/0/all/0/1\">Petros T. Boufounos</a>",
          "description": "Blind pansharpening addresses the problem of generating a high\nspatial-resolution multi-spectral (HRMS) image given a low spatial-resolution\nmulti-spectral (LRMS) image with the guidance of its associated spatially\nmisaligned high spatial-resolution panchromatic (PAN) image without parametric\nside information. In this paper, we propose a fast approach to blind\npansharpening and achieve state-of-the-art image reconstruction quality.\nTypical blind pansharpening algorithms are often computationally intensive\nsince the blur kernel and the target HRMS image are often computed using\niterative solvers and in an alternating fashion. To achieve fast blind\npansharpening, we decouple the solution of the blur kernel and of the HRMS\nimage. First, we estimate the blur kernel by computing the kernel coefficients\nwith minimum total generalized variation that blur a downsampled version of the\nPAN image to approximate a linear combination of the LRMS image channels. Then,\nwe estimate each channel of the HRMS image using local Laplacian prior to\nregularize the relationship between each HRMS channel and the PAN image.\nSolving the HRMS image is accelerated by both parallelizing across the channels\nand by fast numerical algorithms for each channel. Due to the fast scheme and\nthe powerful priors we used on the blur kernel coefficients (total generalized\nvariation) and on the cross-channel relationship (local Laplacian prior),\nnumerical experiments demonstrate that our algorithm outperforms\nstate-of-the-art model-based counterparts in terms of both computational time\nand reconstruction quality of the HRMS images.",
          "link": "http://arxiv.org/abs/2103.09943",
          "publishedOn": "2021-08-03T02:06:31.361Z",
          "wordCount": 730,
          "title": "Fast and High-Quality Blind Multi-Spectral Image Pansharpening. (arXiv:2103.09943v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Anindita Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheema_N/0/1/0/all/0/1\">Noshaba Cheema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_C/0/1/0/all/0/1\">Cennet Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slusallek_P/0/1/0/all/0/1\">Philipp Slusallek</a>",
          "description": "\"How can we animate 3D-characters from a movie script or move robots by\nsimply telling them what we would like them to do?\" \"How unstructured and\ncomplex can we make a sentence and still generate plausible movements from it?\"\nThese are questions that need to be answered in the long-run, as the field is\nstill in its infancy. Inspired by these problems, we present a new technique\nfor generating compositional actions, which handles complex input sentences.\nOur output is a 3D pose sequence depicting the actions in the input sentence.\nWe propose a hierarchical two-stream sequential model to explore a finer\njoint-level mapping between natural language sentences and 3D pose sequences\ncorresponding to the given motion. We learn two manifold representations of the\nmotion -- one each for the upper body and the lower body movements. Our model\ncan generate plausible pose sequences for short sentences describing single\nactions as well as long compositional sentences describing multiple sequential\nand superimposed actions. We evaluate our proposed model on the publicly\navailable KIT Motion-Language Dataset containing 3D pose data with\nhuman-annotated sentences. Experimental results show that our model advances\nthe state-of-the-art on text-based motion synthesis in objective evaluations by\na margin of 50%. Qualitative evaluations based on a user study indicate that\nour synthesized motions are perceived to be the closest to the ground-truth\nmotion captures for both short and compositional sentences.",
          "link": "http://arxiv.org/abs/2103.14675",
          "publishedOn": "2021-08-03T02:06:31.350Z",
          "wordCount": 723,
          "title": "Synthesis of Compositional Animations from Textual Descriptions. (arXiv:2103.14675v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13833",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Calli_E/0/1/0/all/0/1\">Erdi &#xc7;all&#x131;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Murphy_K/0/1/0/all/0/1\">Keelin Murphy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurstjens_S/0/1/0/all/0/1\">Steef Kurstjens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samson_T/0/1/0/all/0/1\">Tijs Samson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herpers_R/0/1/0/all/0/1\">Robert Herpers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smits_H/0/1/0/all/0/1\">Henk Smits</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rutten_M/0/1/0/all/0/1\">Matthieu Rutten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1\">Bram van Ginneken</a>",
          "description": "In the context of the current global pandemic and the limitations of the\nRT-PCR test, we propose a novel deep learning architecture, DFCN (Denoising\nFully Connected Network). Since medical facilities around the world differ\nenormously in what laboratory tests or chest imaging may be available, DFCN is\ndesigned to be robust to missing input data. An ablation study extensively\nevaluates the performance benefits of the DFCN as well as its robustness to\nmissing inputs. Data from 1088 patients with confirmed RT-PCR results are\nobtained from two independent medical facilities. The data includes results\nfrom 27 laboratory tests and a chest x-ray scored by a deep learning model.\nTraining and test datasets are taken from different medical facilities. Data is\nmade publicly available. The performance of DFCN in predicting the RT-PCR\nresult is compared with 3 related architectures as well as a Random Forest\nbaseline. All models are trained with varying levels of masked input data to\nencourage robustness to missing inputs. Missing data is simulated at test time\nby masking inputs randomly. DFCN outperforms all other models with statistical\nsignificance using random subsets of input data with 2-27 available inputs.\nWhen all 28 inputs are available DFCN obtains an AUC of 0.924, higher than any\nother model. Furthermore, with clinically meaningful subsets of parameters\nconsisting of just 6 and 7 inputs respectively, DFCN achieves higher AUCs than\nany other model, with values of 0.909 and 0.919.",
          "link": "http://arxiv.org/abs/2103.13833",
          "publishedOn": "2021-08-03T02:06:31.341Z",
          "wordCount": 766,
          "title": "Deep Learning with robustness to missing data: A novel approach to the detection of COVID-19. (arXiv:2103.13833v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jianlong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhibin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>",
          "description": "Recently, significant progress has been made on semantic segmentation.\nHowever, the success of supervised semantic segmentation typically relies on a\nlarge amount of labelled data, which is time-consuming and costly to obtain.\nInspired by the success of semi-supervised learning methods in image\nclassification, here we propose a simple yet effective semi-supervised learning\nframework for semantic segmentation. We demonstrate that the devil is in the\ndetails: a set of simple design and training techniques can collectively\nimprove the performance of semi-supervised semantic segmentation significantly.\nPrevious works [3, 27] fail to employ strong augmentation in pseudo label\nlearning efficiently, as the large distribution change caused by strong\naugmentation harms the batch normalisation statistics. We design a new batch\nnormalisation, namely distribution-specific batch normalisation (DSBN) to\naddress this problem and demonstrate the importance of strong augmentation for\nsemantic segmentation. Moreover, we design a self correction loss which is\neffective in noise resistance. We conduct a series of ablation studies to show\nthe effectiveness of each component. Our method achieves state-of-the-art\nresults in the semi-supervised settings on the Cityscapes and Pascal VOC\ndatasets.",
          "link": "http://arxiv.org/abs/2104.07256",
          "publishedOn": "2021-08-03T02:06:31.305Z",
          "wordCount": 670,
          "title": "A Simple Baseline for Semi-supervised Semantic Segmentation with Strong Data Augmentation. (arXiv:2104.07256v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.12175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Ying Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hairong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwan_C/0/1/0/all/0/1\">Chiman Kwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoya_N/0/1/0/all/0/1\">Naoto Yokoya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>",
          "description": "Hyperspectral images (HSI) provide rich spectral information that contributed\nto the successful performance improvement of numerous computer vision tasks.\nHowever, it can only be achieved at the expense of images' spatial resolution.\nHyperspectral image super-resolution (HSI-SR) addresses this problem by fusing\nlow resolution (LR) HSI with multispectral image (MSI) carrying much higher\nspatial resolution (HR). All existing HSI-SR approaches require the LR HSI and\nHR MSI to be well registered and the reconstruction accuracy of the HR HSI\nrelies heavily on the registration accuracy of different modalities. This paper\nexploits the uncharted problem domain of HSI-SR without the requirement of\nmulti-modality registration. Given the unregistered LR HSI and HR MSI with\noverlapped regions, we design a unique unsupervised learning structure linking\nthe two unregistered modalities by projecting them into the same statistical\nspace through the same encoder. The mutual information (MI) is further adopted\nto capture the non-linear statistical dependencies between the representations\nfrom two modalities (carrying spatial information) and their raw inputs. By\nmaximizing the MI, spatial correlations between different modalities can be\nwell characterized to further reduce the spectral distortion. A collaborative\n$l_{2,1}$ norm is employed as the reconstruction error instead of the more\ncommon $l_2$ norm, so that individual pixels can be recovered as accurately as\npossible. With this design, the network allows to extract correlated spectral\nand spatial information from unregistered images that better preserves the\nspectral information. The proposed method is referred to as unregistered and\nunsupervised mutual Dirichlet Net ($u^2$-MDN). Extensive experimental results\nusing benchmark HSI datasets demonstrate the superior performance of $u^2$-MDN\nas compared to the state-of-the-art.",
          "link": "http://arxiv.org/abs/1904.12175",
          "publishedOn": "2021-08-03T02:06:31.292Z",
          "wordCount": 776,
          "title": "Unsupervised and Unregistered Hyperspectral Image Super-Resolution with Mutual Dirichlet-Net. (arXiv:1904.12175v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1\">Md Afzal Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meraz_M/0/1/0/all/0/1\">Md Meraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Pavan Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1\">Mohammed Javed</a>",
          "description": "In this paper, we present new feature encoding methods for Detection of 3D\nobjects in point clouds. We used a graph neural network (GNN) for Detection of\n3D objects namely cars, pedestrians, and cyclists. Feature encoding is one of\nthe important steps in Detection of 3D objects. The dataset used is point cloud\ndata which is irregular and unstructured and it needs to be encoded in such a\nway that ensures better feature encapsulation. Earlier works have used relative\ndistance as one of the methods to encode the features. These methods are not\nresistant to rotation variance problems in Graph Neural Networks. We have\nincluded angular-based measures while performing feature encoding in graph\nneural networks. Along with that, we have performed a comparison between other\nmethods like Absolute, Relative, Euclidean distances, and a combination of the\nAngle and Relative methods. The model is trained and evaluated on the subset of\nthe KITTI object detection benchmark dataset under resource constraints. Our\nresults demonstrate that a combination of angle measures and relative distance\nhas performed better than other methods. In comparison to the baseline\nmethod(relative), it achieved better performance. We also performed time\nanalysis of various feature encoding methods.",
          "link": "http://arxiv.org/abs/2108.00780",
          "publishedOn": "2021-08-03T02:06:31.285Z",
          "wordCount": 652,
          "title": "Angle Based Feature Learning in GNN for 3D Object Detection using Point Cloud. (arXiv:2108.00780v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuxin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_H/0/1/0/all/0/1\">Haoran Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianqiao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>",
          "description": "Nodule segmentation from breast ultrasound images is challenging yet\nessential for the diagnosis. Weakly-supervised segmentation (WSS) can help\nreduce time-consuming and cumbersome manual annotation. Unlike existing\nweakly-supervised approaches, in this study, we propose a novel and general WSS\nframework called Flip Learning, which only needs the box annotation.\nSpecifically, the target in the label box will be erased gradually to flip the\nclassification tag, and the erased region will be considered as the\nsegmentation result finally. Our contribution is three-fold. First, our\nproposed approach erases on superpixel level using a Multi-agent Reinforcement\nLearning framework to exploit the prior boundary knowledge and accelerate the\nlearning process. Second, we design two rewards: classification score and\nintensity distribution reward, to avoid under- and over-segmentation,\nrespectively. Third, we adopt a coarse-to-fine learning strategy to reduce the\nresidual errors and improve the segmentation performance. Extensively validated\non a large dataset, our proposed approach achieves competitive performance and\nshows great potential to narrow the gap between fully-supervised and\nweakly-supervised learning.",
          "link": "http://arxiv.org/abs/2108.00752",
          "publishedOn": "2021-08-03T02:06:31.274Z",
          "wordCount": 624,
          "title": "Flip Learning: Erase to Segment. (arXiv:2108.00752v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.13744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reiser_C/0/1/0/all/0/1\">Christian Reiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Songyou Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yiyi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>",
          "description": "NeRF synthesizes novel views of a scene with unprecedented quality by fitting\na neural radiance field to RGB images. However, NeRF requires querying a deep\nMulti-Layer Perceptron (MLP) millions of times, leading to slow rendering\ntimes, even on modern GPUs. In this paper, we demonstrate that real-time\nrendering is possible by utilizing thousands of tiny MLPs instead of one single\nlarge MLP. In our setting, each individual MLP only needs to represent parts of\nthe scene, thus smaller and faster-to-evaluate MLPs can be used. By combining\nthis divide-and-conquer strategy with further optimizations, rendering is\naccelerated by three orders of magnitude compared to the original NeRF model\nwithout incurring high storage costs. Further, using teacher-student\ndistillation for training, we show that this speed-up can be achieved without\nsacrificing visual quality.",
          "link": "http://arxiv.org/abs/2103.13744",
          "publishedOn": "2021-08-03T02:06:31.265Z",
          "wordCount": 614,
          "title": "KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs. (arXiv:2103.13744v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanzhina_N/0/1/0/all/0/1\">Natalia Khanzhina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapenok_A/0/1/0/all/0/1\">Alexey Lapenok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1\">Andrey Filchenkov</a>",
          "description": "According to recent studies, commonly used computer vision datasets contain\nabout 4% of label errors. For example, the COCO dataset is known for its high\nlevel of noise in data labels, which limits its use for training robust neural\ndeep architectures in a real-world scenario. To model such a noise, in this\npaper we have proposed the homoscedastic aleatoric uncertainty estimation, and\npresent a series of novel loss functions to address the problem of image object\ndetection at scale. Specifically, the proposed functions are based on Bayesian\ninference and we have incorporated them into the common community-adopted\nobject detection deep learning architecture RetinaNet. We have also shown that\nmodeling of homoscedastic aleatoric uncertainty using our novel functions\nallows to increase the model interpretability and to improve the object\ndetection performance being evaluated on the COCO dataset.",
          "link": "http://arxiv.org/abs/2108.00784",
          "publishedOn": "2021-08-03T02:06:31.244Z",
          "wordCount": 587,
          "title": "Towards Robust Object Detection: Bayesian RetinaNet for Homoscedastic Aleatoric Uncertainty Modeling. (arXiv:2108.00784v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parchami_M/0/1/0/all/0/1\">Mostafa Parchami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayed_S/0/1/0/all/0/1\">Saif Iftekar Sayed</a>",
          "description": "Feature tracking is the building block of many applications such as visual\nodometry, augmented reality, and target tracking. Unfortunately, the\nstate-of-the-art vision-based tracking algorithms fail in surgical images due\nto the challenges imposed by the nature of such environments. In this paper, we\nproposed a novel and unified deep learning-based approach that can learn how to\ntrack features reliably as well as learn how to detect such reliable features\nfor tracking purposes. The proposed network dubbed as Deep-PT, consists of a\ntracker network which is a convolutional neural network simulating\ncross-correlation in terms of deep learning and two fully connected networks\nthat operate on the output of intermediate layers of the tracker to detect\nfeatures and predict trackability of the detected points. The ability to detect\nfeatures based on the capabilities of the tracker distinguishes the proposed\nmethod from previous algorithms used in this area and improves the robustness\nof the algorithms against dynamics of the scene. The network is trained using\nmultiple datasets due to the lack of specialized dataset for feature tracking\ndatasets and extensive comparisons are conducted to compare the accuracy of\nDeep-PT against recent pixel tracking algorithms. As the experiments suggest,\nthe proposed deep architecture deliberately learns what to track and how to\ntrack and outperforms the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.00105",
          "publishedOn": "2021-08-03T02:06:31.238Z",
          "wordCount": 658,
          "title": "Deep Feature Tracker: A Novel Application for Deep Convolutional Neural Networks. (arXiv:2108.00105v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00690",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huilin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1\">Junyan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pujin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>",
          "description": "We innovatively propose a flexible and consistent face alignment framework,\nLDDMM-Face, the key contribution of which is a deformation layer that naturally\nembeds facial geometry in a diffeomorphic way. Instead of predicting facial\nlandmarks via heatmap or coordinate regression, we formulate this task in a\ndiffeomorphic registration manner and predict momenta that uniquely\nparameterize the deformation between initial boundary and true boundary, and\nthen perform large deformation diffeomorphic metric mapping (LDDMM)\nsimultaneously for curve and landmark to localize the facial landmarks. Due to\nthe embedding of LDDMM into a deep network, LDDMM-Face can consistently\nannotate facial landmarks without ambiguity and flexibly handle various\nannotation schemes, and can even predict dense annotations from sparse ones.\nOur method can be easily integrated into various face alignment networks. We\nextensively evaluate LDDMM-Face on four benchmark datasets: 300W, WFLW, HELEN\nand COFW-68. LDDMM-Face is comparable or superior to state-of-the-art methods\nfor traditional within-dataset and same-annotation settings, but truly\ndistinguishes itself with outstanding performance when dealing with\nweakly-supervised learning (partial-to-full), challenging cases (e.g., occluded\nfaces), and different training and prediction datasets. In addition, LDDMM-Face\nshows promising results on the most challenging task of predicting across\ndatasets with different annotation schemes.",
          "link": "http://arxiv.org/abs/2108.00690",
          "publishedOn": "2021-08-03T02:06:31.233Z",
          "wordCount": 644,
          "title": "LDDMM-Face: Large Deformation Diffeomorphic Metric Learning for Flexible and Consistent Face Alignment. (arXiv:2108.00690v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huayan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1\">Tin Lun Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijayakumar_S/0/1/0/all/0/1\">Sethu Vijayakumar</a>",
          "description": "Dynamic environments that include unstructured moving objects pose a hard\nproblem for Simultaneous Localization and Mapping (SLAM) performance. The\nmotion of rigid objects can be typically tracked by exploiting their texture\nand geometric features. However, humans moving in the scene are often one of\nthe most important, interactive targets - they are very hard to track and\nreconstruct robustly due to non-rigid shapes. In this work, we present a fast,\nlearning-based human object detector to isolate the dynamic human objects and\nrealise a real-time dense background reconstruction framework. We go further by\nestimating and reconstructing the human pose and shape. The final output\nenvironment maps not only provide the dense static backgrounds but also contain\nthe dynamic human meshes and their trajectories. Our Dynamic SLAM system runs\nat around 26 frames per second (fps) on GPUs, while additionally turning on\naccurate human pose estimation can be executed at up to 10 fps.",
          "link": "http://arxiv.org/abs/2108.00695",
          "publishedOn": "2021-08-03T02:06:31.221Z",
          "wordCount": 602,
          "title": "PoseFusion2: Simultaneous Background Reconstruction and Human Shape Recovery in Real-time. (arXiv:2108.00695v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingsong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hai Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhimin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>",
          "description": "Automated tagging of video advertisements has been a critical yet challenging\nproblem, and it has drawn increasing interests in last years as its\napplications seem to be evident in many fields. Despite sustainable efforts\nhave been made, the tagging task is still suffered from several challenges,\nsuch as, efficiently feature fusion approach is desirable, but under-explored\nin previous studies. In this paper, we present our approach for Multimodal\nVideo Ads Tagging in the 2021 Tencent Advertising Algorithm Competition.\nSpecifically, we propose a novel multi-modal feature fusion framework, with the\ngoal to combine complementary information from multiple modalities. This\nframework introduces stacking-based ensembling approach to reduce the influence\nof varying levels of noise and conflicts between different modalities. Thus,\nour framework can boost the performance of the tagging task, compared to\nprevious methods. To empirically investigate the effectiveness and robustness\nof the proposed framework, we conduct extensive experiments on the challenge\ndatasets. The obtained results suggest that our framework can significantly\noutperform related approaches and our method ranks as the 1st place on the\nfinal leaderboard, with a Global Average Precision (GAP) of 82.63%. To better\npromote the research in this field, we will release our code in the final\nversion.",
          "link": "http://arxiv.org/abs/2108.00679",
          "publishedOn": "2021-08-03T02:06:31.213Z",
          "wordCount": 658,
          "title": "Multimodal Feature Fusion for Video Advertisements Tagging Via Stacking Ensemble. (arXiv:2108.00679v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>",
          "description": "It is widely acknowledged that learning joint embeddings of recipes with\nimages is challenging due to the diverse composition and deformation of\ningredients in cooking procedures. We present a Multi-modal Semantics enhanced\nJoint Embedding approach (MSJE) for learning a common feature space between the\ntwo modalities (text and image), with the ultimate goal of providing\nhigh-performance cross-modal retrieval services. Our MSJE approach has three\nunique features. First, we extract the TFIDF feature from the title,\ningredients and cooking instructions of recipes. By determining the\nsignificance of word sequences through combining LSTM learned features with\ntheir TFIDF features, we encode a recipe into a TFIDF weighted vector for\ncapturing significant key terms and how such key terms are used in the\ncorresponding cooking instructions. Second, we combine the recipe TFIDF feature\nwith the recipe sequence feature extracted through two-stage LSTM networks,\nwhich is effective in capturing the unique relationship between a recipe and\nits associated image(s). Third, we further incorporate TFIDF enhanced category\nsemantics to improve the mapping of image modality and to regulate the\nsimilarity loss function during the iterative learning of cross-modal joint\nembedding. Experiments on the benchmark dataset Recipe1M show the proposed\napproach outperforms the state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.00724",
          "publishedOn": "2021-08-03T02:06:31.207Z",
          "wordCount": 655,
          "title": "Learning TFIDF Enhanced Joint Embedding for Recipe-Image Cross-Modal Retrieval Service. (arXiv:2108.00724v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rossi_L/0/1/0/all/0/1\">Leonardo Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimi_A/0/1/0/all/0/1\">Akbar Karimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prati_A/0/1/0/all/0/1\">Andrea Prati</a>",
          "description": "Within the field of instance segmentation, most of the state-of-the-art deep\nlearning networks rely nowadays on cascade architectures, where multiple object\ndetectors are trained sequentially, re-sampling the ground truth at each step.\nThis offers a solution to the problem of exponentially vanishing positive\nsamples. However, it also translates into an increase in network complexity in\nterms of the number of parameters. To address this issue, we propose\nRecursively Refined R-CNN (R^3-CNN) which avoids duplicates by introducing a\nloop mechanism instead. At the same time, it achieves a quality boost using a\nrecursive re-sampling technique, where a specific IoU quality is utilized in\neach recursion to eventually equally cover the positive spectrum. Our\nexperiments highlight the specific encoding of the loop mechanism in the\nweights, requiring its usage at inference time. The R^3-CNN architecture is\nable to surpass the recently proposed HTC model, while reducing the number of\nparameters significantly. Experiments on COCO minival 2017 dataset show\nperformance boost independently from the utilized baseline model. The code is\navailable online at https://github.com/IMPLabUniPr/mmdetection/tree/r3_cnn.",
          "link": "http://arxiv.org/abs/2104.01329",
          "publishedOn": "2021-08-03T02:06:31.190Z",
          "wordCount": 638,
          "title": "Recursively Refined R-CNN: Instance Segmentation with Self-RoI Rebalancing. (arXiv:2104.01329v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>",
          "description": "This paper introduces a two-phase deep feature calibration framework for\nefficient learning of semantics enhanced text-image cross-modal joint\nembedding, which clearly separates the deep feature calibration in data\npreprocessing from training the joint embedding model. We use the Recipe1M\ndataset for the technical description and empirical validation. In\npreprocessing, we perform deep feature calibration by combining deep feature\nengineering with semantic context features derived from raw text-image input\ndata. We leverage LSTM to identify key terms, NLP methods to produce ranking\nscores for key terms before generating the key term feature. We leverage\nwideResNet50 to extract and encode the image category semantics to help\nsemantic alignment of the learned recipe and image embeddings in the joint\nlatent space. In joint embedding learning, we perform deep feature calibration\nby optimizing the batch-hard triplet loss function with soft-margin and double\nnegative sampling, also utilizing the category-based alignment loss and\ndiscriminator-based alignment loss. Extensive experiments demonstrate that our\nSEJE approach with the deep feature calibration significantly outperforms the\nstate-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.00705",
          "publishedOn": "2021-08-03T02:06:31.184Z",
          "wordCount": 615,
          "title": "Efficient Deep Feature Calibration for Cross-Modal Joint Embedding Learning. (arXiv:2108.00705v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shanlin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xiangyi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaohui Xie</a>",
          "description": "Although having achieved great success in medical image segmentation, deep\nconvolutional neural networks usually require a large dataset with manual\nannotations for training and are difficult to generalize to unseen classes.\nFew-shot learning has the potential to address these challenges by learning new\nclasses from only a few labeled examples. In this work, we propose a new\nframework for few-shot medical image segmentation based on prototypical\nnetworks. Our innovation lies in the design of two key modules: 1) a context\nrelation encoder (CRE) that uses correlation to capture local relation features\nbetween foreground and background regions; and 2) a recurrent mask refinement\nmodule that repeatedly uses the CRE and a prototypical network to recapture the\nchange of context relationship and refine the segmentation mask iteratively.\nExperiments on two abdomen CT datasets and an abdomen MRI dataset show the\nproposed method obtains substantial improvement over the state-of-the-art\nmethods by an average of 16.32%, 8.45% and 6.24% in terms of DSC, respectively.\nCode is publicly available.",
          "link": "http://arxiv.org/abs/2108.00622",
          "publishedOn": "2021-08-03T02:06:31.177Z",
          "wordCount": 605,
          "title": "Recurrent Mask Refinement for Few-Shot Medical Image Segmentation. (arXiv:2108.00622v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pu_M/0/1/0/all/0/1\">Mengyang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yaping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Q/0/1/0/all/0/1\">Qingji Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>",
          "description": "As a fundamental building block in computer vision, edges can be categorised\ninto four types according to the discontinuity in surface-Reflectance,\nIllumination, surface-Normal or Depth. While great progress has been made in\ndetecting generic or individual types of edges, it remains under-explored to\ncomprehensively study all four edge types together. In this paper, we propose a\nnovel neural network solution, RINDNet, to jointly detect all four types of\nedges. Taking into consideration the distinct attributes of each type of edges\nand the relationship between them, RINDNet learns effective representations for\neach of them and works in three stages. In stage I, RINDNet uses a common\nbackbone to extract features shared by all edges. Then in stage II it branches\nto prepare discriminative features for each edge type by the corresponding\ndecoder. In stage III, an independent decision head for each type aggregates\nthe features from previous stages to predict the initial results. Additionally,\nan attention module learns attention maps for all types to capture the\nunderlying relations between them, and these maps are combined with initial\nresults to generate the final edge detection results. For training and\nevaluation, we construct the first public benchmark, BSDS-RIND, with all four\ntypes of edges carefully annotated. In our experiments, RINDNet yields\npromising results in comparison with state-of-the-art methods. Additional\nanalysis is presented in supplementary material.",
          "link": "http://arxiv.org/abs/2108.00616",
          "publishedOn": "2021-08-03T02:06:31.171Z",
          "wordCount": 668,
          "title": "RINDNet: Edge Detection for Discontinuity in Reflectance, Illumination, Normal and Depth. (arXiv:2108.00616v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chongyi Li</a>",
          "description": "Object detection in three-dimensional (3D) space attracts much interest from\nacademia and industry since it is an essential task in AI-driven applications\nsuch as robotics, autonomous driving, and augmented reality. As the basic\nformat of 3D data, the point cloud can provide detailed geometric information\nabout the objects in the original 3D space. However, due to 3D data's sparsity\nand unorderedness, specially designed networks and modules are needed to\nprocess this type of data. Attention mechanism has achieved impressive\nperformance in diverse computer vision tasks; however, it is unclear how\nattention modules would affect the performance of 3D point cloud object\ndetection and what sort of attention modules could fit with the inherent\nproperties of 3D data. This work investigates the role of the attention\nmechanism in 3D point cloud object detection and provides insights into the\npotential of different attention modules. To achieve that, we comprehensively\ninvestigate classical 2D attentions, novel 3D attentions, including the latest\npoint cloud transformers on SUN RGB-D and ScanNetV2 datasets. Based on the\ndetailed experiments and analysis, we conclude the effects of different\nattention modules. This paper is expected to serve as a reference source for\nbenefiting attention-embedded 3D point cloud object detection. The code and\ntrained models are available at:\nhttps://github.com/ShiQiu0419/attentions_in_3D_detection.",
          "link": "http://arxiv.org/abs/2108.00620",
          "publishedOn": "2021-08-03T02:06:31.164Z",
          "wordCount": 658,
          "title": "Investigating Attention Mechanism in 3D Point Cloud Object Detection. (arXiv:2108.00620v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Safronov_E/0/1/0/all/0/1\">Evgenii Safronov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piga_N/0/1/0/all/0/1\">Nicola Piga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colledanchise_M/0/1/0/all/0/1\">Michele Colledanchise</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natale_L/0/1/0/all/0/1\">Lorenzo Natale</a>",
          "description": "Recent visual pose estimation and tracking solutions provide notable results\non popular datasets such as T-LESS and YCB. However, in the real world, we can\nfind ambiguous objects that do not allow exact classification and detection\nfrom a single view. In this work, we propose a framework that, given a single\nview of an object, provides the coordinates of a next viewpoint to discriminate\nthe object against similar ones, if any, and eliminates ambiguities. We also\ndescribe a complete pipeline from a real object's scans to the viewpoint\nselection and classification. We validate our approach with a Franka Emika\nPanda robot and common household objects featured with ambiguities. We released\nthe source code to reproduce our experiments.",
          "link": "http://arxiv.org/abs/2108.00737",
          "publishedOn": "2021-08-03T02:06:31.148Z",
          "wordCount": 567,
          "title": "Active Perception for Ambiguous Objects Classification. (arXiv:2108.00737v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Trung X. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mina_R/0/1/0/all/0/1\">Rusty John Lloyd Mina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Issa_D/0/1/0/all/0/1\">Dias Issa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1\">Chang D. Yoo</a>",
          "description": "In this work, we propose a novel methodology for self-supervised learning for\ngenerating global and local attention-aware visual features. Our approach is\nbased on training a model to differentiate between specific image\ntransformations of an input sample and the patched images. Utilizing this\napproach, the proposed method is able to outperform the previous best\ncompetitor by 1.03% on the Tiny-ImageNet dataset and by 2.32% on the STL-10\ndataset. Furthermore, our approach outperforms the fully-supervised learning\nmethod on the STL-10 dataset. Experimental results and visualizations show the\ncapability of successfully learning global and local attention-aware visual\nrepresentations.",
          "link": "http://arxiv.org/abs/2108.00475",
          "publishedOn": "2021-08-03T02:06:31.143Z",
          "wordCount": 544,
          "title": "Self-supervised Learning with Local Attention-Aware Feature. (arXiv:2108.00475v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heidler_K/0/1/0/all/0/1\">Konrad Heidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lichao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Di Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Pu Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guangyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>",
          "description": "Many current deep learning approaches make extensive use of backbone networks\npre-trained on large datasets like ImageNet, which are then fine-tuned to\nperform a certain task. In remote sensing, the lack of comparable large\nannotated datasets and the wide diversity of sensing platforms impedes similar\ndevelopments. In order to contribute towards the availability of pre-trained\nbackbone networks in remote sensing, we devise a self-supervised approach for\npre-training deep neural networks. By exploiting the correspondence between\ngeo-tagged audio recordings and remote sensing imagery, this is done in a\ncompletely label-free manner, eliminating the need for laborious manual\nannotation. For this purpose, we introduce the SoundingEarth dataset, which\nconsists of co-located aerial imagery and audio samples all around the world.\nUsing this dataset, we then pre-train ResNet models to map samples from both\nmodalities into a common embedding space, which encourages the models to\nunderstand key properties of a scene that influence both visual and auditory\nappearance. To validate the usefulness of the proposed approach, we evaluate\nthe transfer learning performance of pre-trained weights obtained against\nweights obtained through other means. By fine-tuning the models on a number of\ncommonly used remote sensing datasets, we show that our approach outperforms\nexisting pre-training strategies for remote sensing imagery. The dataset, code\nand pre-trained model weights will be available at\nhttps://github.com/khdlr/SoundingEarth.",
          "link": "http://arxiv.org/abs/2108.00688",
          "publishedOn": "2021-08-03T02:06:31.137Z",
          "wordCount": 690,
          "title": "Self-supervised Audiovisual Representation Learning for Remote Sensing Data. (arXiv:2108.00688v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00713",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1\">Brennan Nichyporuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardinell_J/0/1/0/all/0/1\">Jillian Cardinell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1\">Justin Szeto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mehta_R/0/1/0/all/0/1\">Raghav Mehta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios Tsaftaris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1\">Douglas L. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>",
          "description": "Many automatic machine learning models developed for focal pathology (e.g.\nlesions, tumours) detection and segmentation perform well, but do not\ngeneralize as well to new patient cohorts, impeding their widespread adoption\ninto real clinical contexts. One strategy to create a more diverse,\ngeneralizable training set is to naively pool datasets from different cohorts.\nSurprisingly, training on this \\it{big data} does not necessarily increase, and\nmay even reduce, overall performance and model generalizability, due to the\nexistence of cohort biases that affect label distributions. In this paper, we\npropose a generalized affine conditioning framework to learn and account for\ncohort biases across multi-source datasets, which we call Source-Conditioned\nInstance Normalization (SCIN). Through extensive experimentation on three\ndifferent, large scale, multi-scanner, multi-centre Multiple Sclerosis (MS)\nclinical trial MRI datasets, we show that our cohort bias adaptation method (1)\nimproves performance of the network on pooled datasets relative to naively\npooling datasets and (2) can quickly adapt to a new cohort by fine-tuning the\ninstance normalization parameters, thus learning the new cohort bias with only\n10 labelled samples.",
          "link": "http://arxiv.org/abs/2108.00713",
          "publishedOn": "2021-08-03T02:06:31.120Z",
          "wordCount": 638,
          "title": "Cohort Bias Adaptation in Aggregated Datasets for Lesion Segmentation. (arXiv:2108.00713v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Maoguo Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>",
          "description": "Crowd localization is a new computer vision task, evolved from crowd\ncounting. Different from the latter, it provides more precise location\ninformation for each instance, not just counting numbers for the whole crowd\nscene, which brings greater challenges, especially in extremely congested crowd\nscenes. In this paper, we focus on how to achieve precise instance localization\nin high-density crowd scenes, and to alleviate the problem that the feature\nextraction ability of the traditional model is reduced due to the target\nocclusion, the image blur, etc. To this end, we propose a Dilated Convolutional\nSwin Transformer (DCST) for congested crowd scenes. Specifically, a\nwindow-based vision transformer is introduced into the crowd localization task,\nwhich effectively improves the capacity of representation learning. Then, the\nwell-designed dilated convolutional module is inserted into some different\nstages of the transformer to enhance the large-range contextual information.\nExtensive experiments evidence the effectiveness of the proposed methods and\nachieve state-of-the-art performance on five popular datasets. Especially, the\nproposed model achieves F1-measure of 77.5\\% and MAE of 84.2 in terms of\nlocalization and counting performance, respectively.",
          "link": "http://arxiv.org/abs/2108.00584",
          "publishedOn": "2021-08-03T02:06:31.112Z",
          "wordCount": 618,
          "title": "Congested Crowd Instance Localization with Dilated Convolutional Swin Transformer. (arXiv:2108.00584v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00552",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Peng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lingyun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egorov_A/0/1/0/all/0/1\">Anton Egorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>",
          "description": "Accurate localization on autonomous driving cars is essential for autonomy\nand driving safety, especially for complex urban streets and search-and-rescue\nsubterranean environments where high-accurate GPS is not available. However\ncurrent odometry estimation may introduce the drifting problems in long-term\nnavigation without robust global localization. The main challenges involve\nscene divergence under the interference of dynamic environments and effective\nperception of observation and object layout variance from different viewpoints.\nTo tackle these challenges, we present PSE-Match, a viewpoint-free place\nrecognition method based on parallel semantic analysis of isolated semantic\nattributes from 3D point-cloud models. Compared with the original point cloud,\nthe observed variance of semantic attributes is smaller. PSE-Match incorporates\na divergence place learning network to capture different semantic attributes\nparallelly through the spherical harmonics domain. Using both existing\nbenchmark datasets and two in-field collected datasets, our experiments show\nthat the proposed method achieves above 70% average recall with top one\nretrieval and above 95% average recall with top ten retrieval cases. And\nPSE-Match has also demonstrated an obvious generalization ability with a\nlimited training dataset.",
          "link": "http://arxiv.org/abs/2108.00552",
          "publishedOn": "2021-08-03T02:06:31.078Z",
          "wordCount": 630,
          "title": "PSE-Match: A Viewpoint-free Place Recognition Method with Parallel Semantic Embedding. (arXiv:2108.00552v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bowen Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekris_K/0/1/0/all/0/1\">Kostas Bekris</a>",
          "description": "Tracking the 6D pose of objects in video sequences is important for robot\nmanipulation. Most prior efforts, however, often assume that the target\nobject's CAD model, at least at a category-level, is available for offline\ntraining or during online template matching. This work proposes BundleTrack, a\ngeneral framework for 6D pose tracking of novel objects, which does not depend\nupon 3D models, either at the instance or category-level. It leverages the\ncomplementary attributes of recent advances in deep learning for segmentation\nand robust feature extraction, as well as memory-augmented pose graph\noptimization for spatiotemporal consistency. This enables long-term, low-drift\ntracking under various challenging scenarios, including significant occlusions\nand object motions. Comprehensive experiments given two public benchmarks\ndemonstrate that the proposed approach significantly outperforms state-of-art,\ncategory-level 6D tracking or dynamic SLAM methods. When compared against\nstate-of-art methods that rely on an object instance CAD model, comparable\nperformance is achieved, despite the proposed method's reduced information\nrequirements. An efficient implementation in CUDA provides a real-time\nperformance of 10Hz for the entire framework. Code is available at:\nhttps://github.com/wenbowen123/BundleTrack",
          "link": "http://arxiv.org/abs/2108.00516",
          "publishedOn": "2021-08-03T02:06:31.072Z",
          "wordCount": 628,
          "title": "BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models. (arXiv:2108.00516v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miao_B/0/1/0/all/0/1\">Bo Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Liguang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1\">Tin Lun Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangsheng Xu</a>",
          "description": "Accurate perception of the surrounding scene is helpful for robots to make\nreasonable judgments and behaviours. Therefore, developing effective scene\nrepresentation and recognition methods are of significant importance in\nrobotics. Currently, a large body of research focuses on developing novel\nauxiliary features and networks to improve indoor scene recognition ability.\nHowever, few of them focus on directly constructing object features and\nrelations for indoor scene recognition. In this paper, we analyze the\nweaknesses of current methods and propose an Object-to-Scene (OTS) method,\nwhich extracts object features and learns object relations to recognize indoor\nscenes. The proposed OTS first extracts object features based on the\nsegmentation network and the proposed object feature aggregation module (OFAM).\nAfterwards, the object relations are calculated and the scene representation is\nconstructed based on the proposed object attention module (OAM) and global\nrelation aggregation module (GRAM). The final results in this work show that\nOTS successfully extracts object features and learns object relations from the\nsegmentation network. Moreover, OTS outperforms the state-of-the-art methods by\nmore than 2\\% on indoor scene recognition without using any additional streams.\nCode is publicly available at: https://github.com/FreeformRobotics/OTS.",
          "link": "http://arxiv.org/abs/2108.00399",
          "publishedOn": "2021-08-03T02:06:31.053Z",
          "wordCount": 634,
          "title": "Object-to-Scene: Learning to Transfer Object Knowledge to Indoor Scene Recognition. (arXiv:2108.00399v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Albanese_A/0/1/0/all/0/1\">Andrea Albanese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nardello_M/0/1/0/all/0/1\">Matteo Nardello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunelli_D/0/1/0/all/0/1\">Davide Brunelli</a>",
          "description": "Artificial intelligence has smoothly penetrated several economic activities,\nespecially monitoring and control applications, including the agriculture\nsector. However, research efforts toward low-power sensing devices with fully\nfunctional machine learning (ML) on-board are still fragmented and limited in\nsmart farming. Biotic stress is one of the primary causes of crop yield\nreduction. With the development of deep learning in computer vision technology,\nautonomous detection of pest infestation through images has become an important\nresearch direction for timely crop disease diagnosis. This paper presents an\nembedded system enhanced with ML functionalities, ensuring continuous detection\nof pest infestation inside fruit orchards. The embedded solution is based on a\nlow-power embedded sensing system along with a Neural Accelerator able to\ncapture and process images inside common pheromone-based traps. Three different\nML algorithms have been trained and deployed, highlighting the capabilities of\nthe platform. Moreover, the proposed approach guarantees an extended battery\nlife thanks to the integration of energy harvesting functionalities. Results\nshow how it is possible to automate the task of pest infestation for unlimited\ntime without the farmer's intervention.",
          "link": "http://arxiv.org/abs/2108.00421",
          "publishedOn": "2021-08-03T02:06:31.047Z",
          "wordCount": 626,
          "title": "Automated Pest Detection with DNN on the Edge for Precision Agriculture. (arXiv:2108.00421v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00568",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guihong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1\">Sumit K. Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogras_U/0/1/0/all/0/1\">Umit Y. Ogras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marculescu_R/0/1/0/all/0/1\">Radu Marculescu</a>",
          "description": "Neural architecture search (NAS) is a promising technique to design efficient\nand high-performance deep neural networks (DNNs). As the performance\nrequirements of ML applications grow continuously, the hardware accelerators\nstart playing a central role in DNN design. This trend makes NAS even more\ncomplicated and time-consuming for most real applications. This paper proposes\nFLASH, a very fast NAS methodology that co-optimizes the DNN accuracy and\nperformance on a real hardware platform. As the main theoretical contribution,\nwe first propose the NN-Degree, an analytical metric to quantify the\ntopological characteristics of DNNs with skip connections (e.g., DenseNets,\nResNets, Wide-ResNets, and MobileNets). The newly proposed NN-Degree allows us\nto do training-free NAS within one second and build an accuracy predictor by\ntraining as few as 25 samples out of a vast search space with more than 63\nbillion configurations. Second, by performing inference on the target hardware,\nwe fine-tune and validate our analytical models to estimate the latency, area,\nand energy consumption of various DNN architectures while executing standard ML\ndatasets. Third, we construct a hierarchical algorithm based on simplicial\nhomology global optimization (SHGO) to optimize the model-architecture\nco-design process, while considering the area, latency, and energy consumption\nof the target hardware. We demonstrate that, compared to the state-of-the-art\nNAS approaches, our proposed hierarchical SHGO-based algorithm enables more\nthan four orders of magnitude speedup (specifically, the execution time of the\nproposed algorithm is about 0.1 seconds). Finally, our experimental evaluations\nshow that FLASH is easily transferable to different hardware architectures,\nthus enabling us to do NAS on a Raspberry Pi-3B processor in less than 3\nseconds.",
          "link": "http://arxiv.org/abs/2108.00568",
          "publishedOn": "2021-08-03T02:06:31.033Z",
          "wordCount": 711,
          "title": "FLASH: Fast Neural Architecture Search with Hardware Optimization. (arXiv:2108.00568v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaojun Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Huanqian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonglin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xingxing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>",
          "description": "In recent years, intellectual property (IP), which represents literary,\ninventions, artistic works, etc, gradually attract more and more people's\nattention. Particularly, with the rise of e-commerce, the IP not only\nrepresents the product design and brands, but also represents the images/videos\ndisplayed on e-commerce platforms. Unfortunately, some attackers adopt some\nadversarial methods to fool the well-trained logo detection model for\ninfringement. To overcome this problem, a novel logo detector based on the\nmechanism of looking and thinking twice is proposed in this paper for robust\nlogo detection. The proposed detector is different from other mainstream\ndetectors, which can effectively detect small objects, long-tail objects, and\nis robust to adversarial images. In detail, we extend detectoRS algorithm to a\ncascade schema with an equalization loss function, multi-scale transformations,\nand adversarial data augmentation. A series of experimental results have shown\nthat the proposed method can effectively improve the robustness of the\ndetection model. Moreover, we have applied the proposed methods to competition\nACM MM2021 Robust Logo Detection that is organized by Alibaba on the Tianchi\nplatform and won top 2 in 36489 teams. Code is available at\nhttps://github.com/jiaxiaojunQAQ/Robust-Logo-Detection.",
          "link": "http://arxiv.org/abs/2108.00422",
          "publishedOn": "2021-08-03T02:06:31.026Z",
          "wordCount": 631,
          "title": "An Effective and Robust Detector for Logo Detection. (arXiv:2108.00422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iftekhar_A/0/1/0/all/0/1\">A S M Iftekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Satish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McEver_R/0/1/0/all/0/1\">R. Austin McEver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Suya You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunath_B/0/1/0/all/0/1\">B.S. Manjunath</a>",
          "description": "The human-object interaction (HOI) detection task refers to localizing\nhumans, localizing objects, and predicting the interactions between each\nhuman-object pair. HOI is considered one of the fundamental steps in truly\nunderstanding complex visual scenes. For detecting HOI, it is important to\nutilize relative spatial configurations and object semantics to find salient\nspatial regions of images that highlight the interactions between human object\npairs. This issue is addressed by the proposed self-attention based guided\ntransformer network, GTNet. GTNet encodes this spatial contextual information\nin human and object visual features via self-attention while achieving a 4%-6%\nimprovement over previous state of the art results on both the V-COCO and\nHICO-DET datasets. Code will be made available online.",
          "link": "http://arxiv.org/abs/2108.00596",
          "publishedOn": "2021-08-03T02:06:31.008Z",
          "wordCount": 562,
          "title": "GTNet:Guided Transformer Network for Detecting Human-Object Interactions. (arXiv:2108.00596v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiju Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taejoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>",
          "description": "Adversarial training based on the maximum classifier discrepancy between the\ntwo classifier structures has achieved great success in unsupervised domain\nadaptation tasks for image classification. The approach adopts the structure of\ntwo classifiers, though simple and intuitive, the learned classification\nboundary may not well represent the data property in the new domain. In this\npaper, we propose to extend the structure to multiple classifiers to further\nboost its performance. To this end, we propose a very straightforward approach\nto adding more classifiers. We employ the principle that the classifiers are\ndifferent from each other to construct a discrepancy loss function for multiple\nclassifiers. Through the loss function construction method, we make it possible\nto add any number of classifiers to the original framework. The proposed\napproach is validated through extensive experimental evaluations. We\ndemonstrate that, on average, adopting the structure of three classifiers\nnormally yields the best performance as a trade-off between the accuracy and\nefficiency. With minimum extra computational costs, the proposed approach can\nsignificantly improve the original algorithm.",
          "link": "http://arxiv.org/abs/2108.00610",
          "publishedOn": "2021-08-03T02:06:30.996Z",
          "wordCount": 607,
          "title": "Multiple Classifiers Based Maximum Classifier Discrepancy for Unsupervised Domain Adaptation. (arXiv:2108.00610v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Gangming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Weifeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>",
          "description": "Feature pyramids have been proven powerful in image understanding tasks that\nrequire multi-scale features. State-of-the-art methods for multi-scale feature\nlearning focus on performing feature interactions across space and scales using\nneural networks with a fixed topology. In this paper, we propose graph feature\npyramid networks that are capable of adapting their topological structures to\nvarying intrinsic image structures and supporting simultaneous feature\ninteractions across all scales. We first define an image-specific superpixel\nhierarchy for each input image to represent its intrinsic image structures. The\ngraph feature pyramid network inherits its structure from this superpixel\nhierarchy. Contextual and hierarchical layers are designed to achieve feature\ninteractions within the same scale and across different scales. To make these\nlayers more powerful, we introduce two types of local channel attention for\ngraph neural networks by generalizing global channel attention for\nconvolutional neural networks. The proposed graph feature pyramid network can\nenhance the multiscale features from a convolutional feature pyramid network.\nWe evaluate our graph feature pyramid network in the object detection task by\nintegrating it into the Faster R-CNN algorithm. The modified algorithm\noutperforms not only previous state-of-the-art feature pyramid-based methods\nwith a clear margin but also other popular detection methods on both MS-COCO\n2017 validation and test datasets.",
          "link": "http://arxiv.org/abs/2108.00580",
          "publishedOn": "2021-08-03T02:06:30.988Z",
          "wordCount": 646,
          "title": "GraphFPN: Graph Feature Pyramid Network for Object Detection. (arXiv:2108.00580v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haitong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shuang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xia Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kaiyue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hongjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nizhuan Wang</a>",
          "description": "It is a challenging task to accurately perform semantic segmentation due to\nthe complexity of real picture scenes. Many semantic segmentation methods based\non traditional deep learning insufficiently captured the semantic and\nappearance information of images, which put limit on their generality and\nrobustness for various application scenes. In this paper, we proposed a novel\nstrategy that reformulated the popularly-used convolution operation to\nmulti-layer convolutional sparse coding block to ease the aforementioned\ndeficiency. This strategy can be possibly used to significantly improve the\nsegmentation performance of any semantic segmentation model that involves\nconvolutional operations. To prove the effectiveness of our idea, we chose the\nwidely-used U-Net model for the demonstration purpose, and we designed CSC-Unet\nmodel series based on U-Net. Through extensive analysis and experiments, we\nprovided credible evidence showing that the multi-layer convolutional sparse\ncoding block enables semantic segmentation model to converge faster, can\nextract finer semantic and appearance information of images, and improve the\nability to recover spatial detail information. The best CSC-Unet model\nsignificantly outperforms the results of the original U-Net on three public\ndatasets with different scenarios, i.e., 87.14% vs. 84.71% on DeepCrack\ndataset, 68.91% vs. 67.09% on Nuclei dataset, and 53.68% vs. 48.82% on CamVid\ndataset, respectively.",
          "link": "http://arxiv.org/abs/2108.00408",
          "publishedOn": "2021-08-03T02:06:30.938Z",
          "wordCount": 661,
          "title": "CSC-Unet: A Novel Convolutional Sparse Coding Strategy based Neural Network for Semantic Segmentation. (arXiv:2108.00408v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1\">Navid Kardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>",
          "description": "Deep Learning (DL) is the most widely used tool in the contemporary field of\ncomputer vision. Its ability to accurately solve complex problems is employed\nin vision research to learn deep neural models for a variety of tasks,\nincluding security critical applications. However, it is now known that DL is\nvulnerable to adversarial attacks that can manipulate its predictions by\nintroducing visually imperceptible perturbations in images and videos. Since\nthe discovery of this phenomenon in 2013~[1], it has attracted significant\nattention of researchers from multiple sub-fields of machine intelligence. In\n[2], we reviewed the contributions made by the computer vision community in\nadversarial attacks on deep learning (and their defenses) until the advent of\nyear 2018. Many of those contributions have inspired new directions in this\narea, which has matured significantly since witnessing the first generation\nmethods. Hence, as a legacy sequel of [2], this literature review focuses on\nthe advances in this area since 2018. To ensure authenticity, we mainly\nconsider peer-reviewed contributions published in the prestigious sources of\ncomputer vision and machine learning research. Besides a comprehensive\nliterature review, the article also provides concise definitions of technical\nterminologies for non-experts in this domain. Finally, this article discusses\nchallenges and future outlook of this direction based on the literature\nreviewed herein and [2].",
          "link": "http://arxiv.org/abs/2108.00401",
          "publishedOn": "2021-08-03T02:06:30.932Z",
          "wordCount": 673,
          "title": "Threat of Adversarial Attacks on Deep Learning in Computer Vision: Survey II. (arXiv:2108.00401v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Choubo Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>",
          "description": "Existing anomaly detection paradigms overwhelmingly focus on training\ndetection models using exclusively normal data or unlabeled data (mostly normal\nsamples). One notorious issue with these approaches is that they are weak in\ndiscriminating anomalies from normal samples due to the lack of the knowledge\nabout the anomalies. Here, we study the problem of few-shot anomaly detection,\nin which we aim at using a few labeled anomaly examples to train\nsample-efficient discriminative detection models. To address this problem, we\nintroduce a novel weakly-supervised anomaly detection framework to train\ndetection models without assuming the examples illustrating all possible\nclasses of anomaly.\n\nSpecifically, the proposed approach learns discriminative normality\n(regularity) by leveraging the labeled anomalies and a prior probability to\nenforce expressive representations of normality and unbounded deviated\nrepresentations of abnormality. This is achieved by an end-to-end optimization\nof anomaly scores with a neural deviation learning, in which the anomaly scores\nof normal samples are imposed to approximate scalar scores drawn from the prior\nwhile that of anomaly examples is enforced to have statistically significant\ndeviations from these sampled scores in the upper tail. Furthermore, our model\nis optimized to learn fine-grained normality and abnormality by top-K\nmultiple-instance-learning-based feature subspace deviation learning, allowing\nmore generalized representations. Comprehensive experiments on nine real-world\nimage anomaly detection benchmarks show that our model is substantially more\nsample-efficient and robust, and performs significantly better than\nstate-of-the-art competing methods in both closed-set and open-set settings.\nOur model can also offer explanation capability as a result of its prior-driven\nanomaly score learning. Code and datasets are available at:\nhttps://git.io/DevNet.",
          "link": "http://arxiv.org/abs/2108.00462",
          "publishedOn": "2021-08-03T02:06:30.926Z",
          "wordCount": 714,
          "title": "Explainable Deep Few-shot Anomaly Detection with Deviation Networks. (arXiv:2108.00462v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>",
          "description": "Low-light images captured in the real world are inevitably corrupted by\nsensor noise. Such noise is spatially variant and highly dependent on the\nunderlying pixel intensity, deviating from the oversimplified assumptions in\nconventional denoising. Existing light enhancement methods either overlook the\nimportant impact of real-world noise during enhancement, or treat noise removal\nas a separate pre- or post-processing step. We present Coordinated Enhancement\nfor Real-world Low-light Noisy Images (CERL), that seamlessly integrates light\nenhancement and noise suppression parts into a unified and physics-grounded\noptimization framework. For the real low-light noise removal part, we customize\na self-supervised denoising model that can easily be adapted without referring\nto clean ground-truth images. For the light enhancement part, we also improve\nthe design of a state-of-the-art backbone. The two parts are then joint\nformulated into one principled plug-and-play optimization. Our approach is\ncompared against state-of-the-art low-light enhancement methods both\nqualitatively and quantitatively. Besides standard benchmarks, we further\ncollect and test on a new realistic low-light mobile photography dataset\n(RLMP), whose mobile-captured photos display heavier realistic noise than those\ntaken by high-quality cameras. CERL consistently produces the most visually\npleasing and artifact-free results across all experiments. Our RLMP dataset and\ncodes are available at: https://github.com/VITA-Group/CERL.",
          "link": "http://arxiv.org/abs/2108.00478",
          "publishedOn": "2021-08-03T02:06:30.919Z",
          "wordCount": 651,
          "title": "CERL: A Unified Optimization Framework for Light Enhancement with Realistic Noise. (arXiv:2108.00478v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Liguang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cen_J/0/1/0/all/0/1\">Jun Cen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenglong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1\">Tin Lun Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangsheng Xu</a>",
          "description": "Scene recognition is a fundamental task in robotic perception. For human\nbeings, scene recognition is reasonable because they have abundant object\nknowledge of the real world. The idea of transferring prior object knowledge\nfrom humans to scene recognition is significant but still less exploited. In\nthis paper, we propose to utilize meaningful object representations for indoor\nscene representation. First, we utilize an improved object model (IOM) as a\nbaseline that enriches the object knowledge by introducing a scene parsing\nalgorithm pretrained on the ADE20K dataset with rich object categories related\nto the indoor scene. To analyze the object co-occurrences and pairwise object\nrelations, we formulate the IOM from a Bayesian perspective as the Bayesian\nobject relation model (BORM). Meanwhile, we incorporate the proposed BORM with\nthe PlacesCNN model as the combined Bayesian object relation model (CBORM) for\nscene recognition and significantly outperforms the state-of-the-art methods on\nthe reduced Places365 dataset, and SUN RGB-D dataset without retraining,\nshowing the excellent generalization ability of the proposed method. Code can\nbe found at https://github.com/hszhoushen/borm.",
          "link": "http://arxiv.org/abs/2108.00397",
          "publishedOn": "2021-08-03T02:06:30.913Z",
          "wordCount": 630,
          "title": "BORM: Bayesian Object Relation Model for Indoor Scene Recognition. (arXiv:2108.00397v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zust_L/0/1/0/all/0/1\">Lojze &#x17d;ust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kristan_M/0/1/0/all/0/1\">Matej Kristan</a>",
          "description": "Coastal water autonomous boats rely on robust perception methods for obstacle\ndetection and timely collision avoidance. The current state-of-the-art is based\non deep segmentation networks trained on large datasets. Per-pixel ground truth\nlabeling of such datasets, however, is labor-intensive and expensive. We\nobserve that far less information is required for practical obstacle avoidance\n- the location of water edge on static obstacles like shore and approximate\nlocation and bounds of dynamic obstacles in the water is sufficient to plan a\nreaction. We propose a new scaffolding learning regime (SLR) that allows\ntraining obstacle detection segmentation networks only from such weak\nannotations, thus significantly reducing the cost of ground-truth labeling.\nExperiments show that maritime obstacle segmentation networks trained using SLR\nsubstantially outperform the same networks trained with dense ground truth\nlabels. Thus accuracy is not sacrificed for labelling simplicity but is in fact\nimproved, which is a remarkable result.",
          "link": "http://arxiv.org/abs/2108.00564",
          "publishedOn": "2021-08-03T02:06:30.906Z",
          "wordCount": 591,
          "title": "Learning Maritime Obstacle Detection from Weak Annotations by Scaffolding. (arXiv:2108.00564v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fassold_H/0/1/0/all/0/1\">Hannes Fassold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karakottas_A/0/1/0/all/0/1\">Antonis Karakottas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsatsou_D/0/1/0/all/0/1\">Dorothea Tsatsou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarpalas_D/0/1/0/all/0/1\">Dimitrios Zarpalas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takacs_B/0/1/0/all/0/1\">Barnabas Takacs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuhrhop_C/0/1/0/all/0/1\">Christian Fuhrhop</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manfredi_A/0/1/0/all/0/1\">Angelo Manfredi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patz_N/0/1/0/all/0/1\">Nicolas Patz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonoli_S/0/1/0/all/0/1\">Simona Tonoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dulskaia_I/0/1/0/all/0/1\">Iana Dulskaia</a>",
          "description": "Spherical 360{\\deg} video is a novel media format, rapidly becoming adopted\nin media production and consumption of immersive media. Due to its novelty,\nthere is a lack of tools for producing highly engaging interactive 360{\\deg}\nvideo for consumption on a multitude of platforms. In this work, we describe\nthe work done so far in the Hyper360 project on tools for mixed 360{\\deg} video\nand 3D content. Furthermore, the first pilots which have been produced with the\nHyper360 tools and results of the audience assessment of the produced pilots\nare presented.",
          "link": "http://arxiv.org/abs/2108.00430",
          "publishedOn": "2021-08-03T02:06:30.845Z",
          "wordCount": 543,
          "title": "Hyper360 -- a Next Generation Toolset for Immersive Media. (arXiv:2108.00430v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhoubo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Puqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raveaux_R/0/1/0/all/0/1\">Romain Raveaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huadong Liu</a>",
          "description": "Graph matching is an important problem that has received widespread\nattention, especially in the field of computer vision. Recently,\nstate-of-the-art methods seek to incorporate graph matching with deep learning.\nHowever, there is no research to explain what role the graph matching algorithm\nplays in the model. Therefore, we propose an approach integrating a MILP\nformulation of the graph matching problem. This formulation is solved to\noptimal and it provides inherent baseline. Meanwhile, similar approaches are\nderived by releasing the optimal guarantee of the graph matching solver and by\nintroducing a quality level. This quality level controls the quality of the\nsolutions provided by the graph matching solver. In addition, several\nrelaxations of the graph matching problem are put to the test. Our experimental\nevaluation gives several theoretical insights and guides the direction of deep\ngraph matching methods.",
          "link": "http://arxiv.org/abs/2108.00394",
          "publishedOn": "2021-08-03T02:06:30.834Z",
          "wordCount": 604,
          "title": "Deep graph matching meets mixed-integer linear programming: Relax at your own risk ?. (arXiv:2108.00394v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xujie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fuwei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoye Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1\">Michael C. Kampffmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Haonan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>",
          "description": "Despite recent progress on image-based virtual try-on, current methods are\nconstraint by shared warping networks and thus fail to synthesize natural\ntry-on results when faced with clothing categories that require different\nwarping operations. In this paper, we address this problem by finding clothing\ncategory-specific warping networks for the virtual try-on task via Neural\nArchitecture Search (NAS). We introduce a NAS-Warping Module and elaborately\ndesign a bilevel hierarchical search space to identify the optimal\nnetwork-level and operation-level flow estimation architecture. Given the\nnetwork-level search space, containing different numbers of warping blocks, and\nthe operation-level search space with different convolution operations, we\njointly learn a combination of repeatable warping cells and convolution\noperations specifically for the clothing-person alignment. Moreover, a\nNAS-Fusion Module is proposed to synthesize more natural final try-on results,\nwhich is realized by leveraging particular skip connections to produce\nbetter-fused features that are required for seamlessly fusing the warped\nclothing and the unchanged person part. We adopt an efficient and stable\none-shot searching strategy to search the above two modules. Extensive\nexperiments demonstrate that our WAS-VTON significantly outperforms the\nprevious fixed-architecture try-on methods with more natural warping results\nand virtual try-on results.",
          "link": "http://arxiv.org/abs/2108.00386",
          "publishedOn": "2021-08-03T02:06:30.817Z",
          "wordCount": 635,
          "title": "WAS-VTON: Warping Architecture Search for Virtual Try-on Network. (arXiv:2108.00386v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guoxing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yizhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1\">Anqi Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Pei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>",
          "description": "4D reconstruction of human-object interaction is critical for immersive VR/AR\nexperience and human activity understanding. Recent advances still fail to\nrecover fine geometry and texture results from sparse RGB inputs, especially\nunder challenging human-object interactions scenarios. In this paper, we\npropose a neural human performance capture and rendering system to generate\nboth high-quality geometry and photo-realistic texture of both human and\nobjects under challenging interaction scenarios in arbitrary novel views, from\nonly sparse RGB streams. To deal with complex occlusions raised by human-object\ninteractions, we adopt a layer-wise scene decoupling strategy and perform\nvolumetric reconstruction and neural rendering of the human and object.\nSpecifically, for geometry reconstruction, we propose an interaction-aware\nhuman-object capture scheme that jointly considers the human reconstruction and\nobject reconstruction with their correlations. Occlusion-aware human\nreconstruction and robust human-aware object tracking are proposed for\nconsistent 4D human-object dynamic reconstruction. For neural texture\nrendering, we propose a layer-wise human-object rendering scheme, which\ncombines direction-aware neural blending weight learning and spatial-temporal\ntexture completion to provide high-resolution and photo-realistic texture\nresults in the occluded scenarios. Extensive experiments demonstrate the\neffectiveness of our approach to achieve high-quality geometry and texture\nreconstruction in free viewpoints for challenging human-object interactions.",
          "link": "http://arxiv.org/abs/2108.00362",
          "publishedOn": "2021-08-03T02:06:30.800Z",
          "wordCount": 656,
          "title": "Neural Free-Viewpoint Performance Rendering under ComplexHuman-object Interactions. (arXiv:2108.00362v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zunlei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lechao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yajie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiangtong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>",
          "description": "When confronted with objects of unknown types in an image, humans can\neffortlessly and precisely tell their visual boundaries. This recognition\nmechanism and underlying generalization capability seem to contrast to\nstate-of-the-art image segmentation networks that rely on large-scale\ncategory-aware annotated training samples. In this paper, we make an attempt\ntowards building models that explicitly account for visual boundary knowledge,\nin hope to reduce the training effort on segmenting unseen categories.\nSpecifically, we investigate a new task termed as Boundary Knowledge\nTranslation (BKT). Given a set of fully labeled categories, BKT aims to\ntranslate the visual boundary knowledge learned from the labeled categories, to\na set of novel categories, each of which is provided only a few labeled\nsamples. To this end, we propose a Translation Segmentation Network\n(Trans-Net), which comprises a segmentation network and two boundary\ndiscriminators. The segmentation network, combined with a boundary-aware\nself-supervised mechanism, is devised to conduct foreground segmentation, while\nthe two discriminators work together in an adversarial manner to ensure an\naccurate segmentation of the novel categories under light supervision.\nExhaustive experiments demonstrate that, with only tens of labeled samples as\nguidance, Trans-Net achieves close results on par with fully supervised\nmethods.",
          "link": "http://arxiv.org/abs/2108.00379",
          "publishedOn": "2021-08-03T02:06:30.733Z",
          "wordCount": 640,
          "title": "Visual Boundary Knowledge Translation for Foreground Segmentation. (arXiv:2108.00379v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00402",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhendong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manh_V/0/1/0/all/0/1\">Van Manh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoqiong Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campello_V/0/1/0/all/0/1\">V&#xed;ctor Campello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F Frangi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>",
          "description": "The performance of deep segmentation models often degrades due to\ndistribution shifts in image intensities between the training and test data\nsets. This is particularly pronounced in multi-centre studies involving data\nacquired using multi-vendor scanners, with variations in acquisition protocols.\nIt is challenging to address this degradation because the shift is often not\nknown \\textit{a priori} and hence difficult to model. We propose a novel\nframework to ensure robust segmentation in the presence of such distribution\nshifts. Our contribution is three-fold. First, inspired by the spirit of\ncurriculum learning, we design a novel style curriculum to train the\nsegmentation models using an easy-to-hard mode. A style transfer model with\nstyle fusion is employed to generate the curriculum samples. Gradually focusing\non complex and adversarial style samples can significantly boost the robustness\nof the models. Second, instead of subjectively defining the curriculum\ncomplexity, we adopt an automated gradient manipulation method to control the\nhard and adversarial sample generation process. Third, we propose the Local\nGradient Sign strategy to aggregate the gradient locally and stabilise training\nduring gradient manipulation. The proposed framework can generalise to unknown\ndistribution without using any target data. Extensive experiments on the public\nM\\&Ms Challenge dataset demonstrate that our proposed framework can generalise\ndeep models well to unknown distributions and achieve significant improvements\nin segmentation accuracy.",
          "link": "http://arxiv.org/abs/2108.00402",
          "publishedOn": "2021-08-03T02:06:30.727Z",
          "wordCount": 678,
          "title": "Style Curriculum Learning for Robust Medical Image Segmentation. (arXiv:2108.00402v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Min Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lingxiao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1\">Xingyu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>",
          "description": "Person re-identification (Re-ID) aims to match pedestrians under dis-joint\ncameras. Most Re-ID methods formulate it as visual representation learning and\nimage search, and its accuracy is consequently affected greatly by the search\nspace. Spatial-temporal information has been proven to be efficient to filter\nirrelevant negative samples and significantly improve Re-ID accuracy. However,\nexisting spatial-temporal person Re-ID methods are still rough and do not\nexploit spatial-temporal information sufficiently. In this paper, we propose a\nnovel Instance-level and Spatial-Temporal Disentangled Re-ID method (InSTD), to\nimprove Re-ID accuracy. In our proposed framework, personalized information\nsuch as moving direction is explicitly considered to further narrow down the\nsearch space. Besides, the spatial-temporal transferring probability is\ndisentangled from joint distribution to marginal distribution, so that outliers\ncan also be well modeled. Abundant experimental analyses are presented, which\ndemonstrates the superiority and provides more insights into our method. The\nproposed method achieves mAP of 90.8% on Market-1501 and 89.1% on\nDukeMTMC-reID, improving from the baseline 82.2% and 72.7%, respectively.\nBesides, in order to provide a better benchmark for person re-identification,\nwe release a cleaned data list of DukeMTMC-reID with this paper:\nhttps://github.com/RenMin1991/cleaned-DukeMTMC-reID/",
          "link": "http://arxiv.org/abs/2108.00171",
          "publishedOn": "2021-08-03T02:06:30.720Z",
          "wordCount": 631,
          "title": "Learning Instance-level Spatial-Temporal Patterns for Person Re-identification. (arXiv:2108.00171v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>",
          "description": "Microorganisms are widely distributed in the human daily living environment.\nThey play an essential role in environmental pollution control, disease\nprevention and treatment, and food and drug production. The identification,\ncounting, and detection are the basic steps for making full use of different\nmicroorganisms. However, the conventional analysis methods are expensive,\nlaborious, and time-consuming. To overcome these limitations, artificial neural\nnetworks are applied for microorganism image analysis. We conduct this review\nto understand the development process of microorganism image analysis based on\nartificial neural networks. In this review, the background and motivation are\nintroduced first. Then, the development of artificial neural networks and\nrepresentative networks are introduced. After that, the papers related to\nmicroorganism image analysis based on classical and deep neural networks are\nreviewed from the perspectives of different tasks. In the end, the methodology\nanalysis and potential direction are discussed.",
          "link": "http://arxiv.org/abs/2108.00358",
          "publishedOn": "2021-08-03T02:06:30.707Z",
          "wordCount": 614,
          "title": "Applications of Artificial Neural Networks in Microorganism Image Analysis: A Comprehensive Review from Conventional Multilayer Perceptron to Popular Convolutional Neural Network and Potential Visual Transformer. (arXiv:2108.00358v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Sayak Nag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raychaudhuri_D/0/1/0/all/0/1\">Dripta S. Raychaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sujoy Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>",
          "description": "We study the problem of how to identify samples from unseen categories\n(open-set classification) when there are only a few samples given from the seen\ncategories (few-shot setting). The challenge of learning a good abstraction for\na class with very few samples makes it extremely difficult to detect samples\nfrom the unseen categories; consequently, open-set recognition has received\nminimal attention in the few-shot setting. Most open-set few-shot\nclassification methods regularize the softmax score to indicate uniform\nprobability for open class samples but we argue that this approach is often\ninaccurate, especially at a fine-grained level. Instead, we propose a novel\nexemplar reconstruction-based meta-learning strategy for jointly detecting open\nclass samples, as well as, categorizing samples from seen classes via\nmetric-based classification. The exemplars, which act as representatives of a\nclass, can either be provided in the training dataset or estimated in the\nfeature domain. Our framework, named Reconstructing Exemplar based Few-shot\nOpen-set ClaSsifier (ReFOCS), is tested on a wide variety of datasets and the\nexperimental results clearly highlight our method as the new state of the art.",
          "link": "http://arxiv.org/abs/2108.00340",
          "publishedOn": "2021-08-03T02:06:30.700Z",
          "wordCount": 613,
          "title": "Learning Few-shot Open-set Classifiers using Exemplar Reconstruction. (arXiv:2108.00340v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shooter_M/0/1/0/all/0/1\">Moira Shooter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malleson_C/0/1/0/all/0/1\">Charles Malleson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilton_A/0/1/0/all/0/1\">Adrian Hilton</a> (University of Surrey)",
          "description": "Estimating the pose of animals can facilitate the understanding of animal\nmotion which is fundamental in disciplines such as biomechanics, neuroscience,\nethology, robotics and the entertainment industry. Human pose estimation models\nhave achieved high performance due to the huge amount of training data\navailable. Achieving the same results for animal pose estimation is challenging\ndue to the lack of animal pose datasets. To address this problem we introduce\nSyDog: a synthetic dataset of dogs containing ground truth pose and bounding\nbox coordinates which was generated using the game engine, Unity. We\ndemonstrate that pose estimation models trained on SyDog achieve better\nperformance than models trained purely on real data and significantly reduce\nthe need for the labour intensive labelling of images. We release the SyDog\ndataset as a training and evaluation benchmark for research in animal motion.",
          "link": "http://arxiv.org/abs/2108.00249",
          "publishedOn": "2021-08-03T02:06:30.683Z",
          "wordCount": 608,
          "title": "SyDog: A Synthetic Dog Dataset for Improved 2D Pose Estimation. (arXiv:2108.00249v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Heng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1\">Yew-Soon Ong</a>",
          "description": "Current one-stage methods for visual grounding encode the language query as\none holistic sentence embedding before fusion with visual feature. Such a\nformulation does not treat each word of a query sentence on par when modeling\nlanguage to visual attention, therefore prone to neglect words which are less\nimportant for sentence embedding but critical for visual grounding. In this\npaper we propose Word2Pix: a one-stage visual grounding network based on\nencoder-decoder transformer architecture that enables learning for textual to\nvisual feature correspondence via word to pixel attention. The embedding of\neach word from the query sentence is treated alike by attending to visual\npixels individually instead of single holistic sentence embedding. In this way,\neach word is given equivalent opportunity to adjust the language to vision\nattention towards the referent target through multiple stacks of transformer\ndecoder layers. We conduct the experiments on RefCOCO, RefCOCO+ and RefCOCOg\ndatasets and the proposed Word2Pix outperforms existing one-stage methods by a\nnotable margin. The results obtained also show that Word2Pix surpasses\ntwo-stage visual grounding models, while at the same time keeping the merits of\none-stage paradigm namely end-to-end training and real-time inference speed\nintact.",
          "link": "http://arxiv.org/abs/2108.00205",
          "publishedOn": "2021-08-03T02:06:30.675Z",
          "wordCount": 639,
          "title": "Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding. (arXiv:2108.00205v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shapira_G/0/1/0/all/0/1\">Gil Shapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_N/0/1/0/all/0/1\">Noga Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldin_I/0/1/0/all/0/1\">Ishay Goldin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jevnisek_R/0/1/0/all/0/1\">Roy J. Jevnisek</a>",
          "description": "Facial landmarks (FLM) estimation is a critical component in many\nface-related applications. In this work, we aim to optimize for both accuracy\nand speed and explore the trade-off between them. Our key observation is that\nnot all faces are created equal. Frontal faces with neutral expressions\nconverge faster than faces with extreme poses or expressions. To differentiate\namong samples, we train our model to predict the regression error after each\niteration. If the current iteration is accurate enough, we stop iterating,\nsaving redundant iterations while keeping the accuracy in check. We also\nobserve that as neighboring patches overlap, we can infer all facial landmarks\n(FLMs) with only a small number of patches without a major accuracy sacrifice.\nArchitecturally, we offer a multi-scale, patch-based, lightweight feature\nextractor with a fine-grained local patch attention module, which computes a\npatch weighting according to the information in the patch itself and enhances\nthe expressive power of the patch features. We analyze the patch attention data\nto infer where the model is attending when regressing facial landmarks and\ncompare it to face attention in humans. Our model runs in real-time on a mobile\ndevice GPU, with 95 Mega Multiply-Add (MMA) operations, outperforming all\nstate-of-the-art methods under 1000 MMA, with a normalized mean error of 8.16\non the 300W challenging dataset.",
          "link": "http://arxiv.org/abs/2108.00377",
          "publishedOn": "2021-08-03T02:06:30.642Z",
          "wordCount": 663,
          "title": "Knowing When to Quit: Selective Cascaded Regression with Patch Attention for Real-Time Face Alignment. (arXiv:2108.00377v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jinyuan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yupei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>",
          "description": "Self-supervised learning in computer vision aims to pre-train an image\nencoder using a large amount of unlabeled images or (image, text) pairs. The\npre-trained image encoder can then be used as a feature extractor to build\ndownstream classifiers for many downstream tasks with a small amount of or no\nlabeled training data. In this work, we propose BadEncoder, the first backdoor\nattack to self-supervised learning. In particular, our BadEncoder injects\nbackdoors into a pre-trained image encoder such that the downstream classifiers\nbuilt based on the backdoored image encoder for different downstream tasks\nsimultaneously inherit the backdoor behavior. We formulate our BadEncoder as an\noptimization problem and we propose a gradient descent based method to solve\nit, which produces a backdoored image encoder from a clean one. Our extensive\nempirical evaluation results on multiple datasets show that our BadEncoder\nachieves high attack success rates while preserving the accuracy of the\ndownstream classifiers. We also show the effectiveness of BadEncoder using two\npublicly available, real-world image encoders, i.e., Google's image encoder\npre-trained on ImageNet and OpenAI's Contrastive Language-Image Pre-training\n(CLIP) image encoder pre-trained on 400 million (image, text) pairs collected\nfrom the Internet. Moreover, we consider defenses including Neural Cleanse and\nMNTD (empirical defenses) as well as PatchGuard (a provable defense). Our\nresults show that these defenses are insufficient to defend against BadEncoder,\nhighlighting the needs for new defenses against our BadEncoder. Our code is\npublicly available at: https://github.com/jjy1994/BadEncoder.",
          "link": "http://arxiv.org/abs/2108.00352",
          "publishedOn": "2021-08-03T02:06:30.633Z",
          "wordCount": 693,
          "title": "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning. (arXiv:2108.00352v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>",
          "description": "Most existing Siamese-based tracking methods execute the classification and\nregression of the target object based on the similarity maps. However, they\neither employ a single map from the last convolutional layer which degrades the\nlocalization accuracy in complex scenarios or separately use multiple maps for\ndecision making, introducing intractable computations for aerial mobile\nplatforms. Thus, in this work, we propose an efficient and effective\nhierarchical feature transformer (HiFT) for aerial tracking. Hierarchical\nsimilarity maps generated by multi-level convolutional layers are fed into the\nfeature transformer to achieve the interactive fusion of spatial (shallow\nlayers) and semantics cues (deep layers). Consequently, not only the global\ncontextual information can be raised, facilitating the target search, but also\nour end-to-end architecture with the transformer can efficiently learn the\ninterdependencies among multi-level features, thereby discovering a\ntracking-tailored feature space with strong discriminability. Comprehensive\nevaluations on four aerial benchmarks have proven the effectiveness of HiFT.\nReal-world tests on the aerial platform have strongly validated its\npracticability with a real-time speed. Our code is available at\nhttps://github.com/vision4robotics/HiFT.",
          "link": "http://arxiv.org/abs/2108.00202",
          "publishedOn": "2021-08-03T02:06:30.623Z",
          "wordCount": 621,
          "title": "HiFT: Hierarchical Feature Transformer for Aerial Tracking. (arXiv:2108.00202v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Fang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sanping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>",
          "description": "Understanding complex social interactions among agents is a key challenge for\ntrajectory prediction. Most existing methods consider the interactions between\npairwise traffic agents or in a local area, while the nature of interactions is\nunlimited, involving an uncertain number of agents and non-local areas\nsimultaneously. Besides, they only focus on homogeneous trajectory prediction,\nnamely those among agents of the same category, while neglecting people's\ndiverse reaction patterns toward traffic agents in different categories. To\naddress these problems, we propose a simple yet effective Unlimited\nNeighborhood Interaction Network (UNIN), which predicts trajectories of\nheterogeneous agents in multiply categories. Specifically, the proposed\nunlimited neighborhood interaction module generates the fused-features of all\nagents involved in an interaction simultaneously, which is adaptive to any\nnumber of agents and any range of interaction area. Meanwhile, a hierarchical\ngraph attention module is proposed to obtain category-tocategory interaction\nand agent-to-agent interaction. Finally, parameters of a Gaussian Mixture Model\nare estimated for generating the future trajectories. Extensive experimental\nresults on benchmark datasets demonstrate a significant performance improvement\nof our method over the state-ofthe-art methods.",
          "link": "http://arxiv.org/abs/2108.00238",
          "publishedOn": "2021-08-03T02:06:30.605Z",
          "wordCount": 618,
          "title": "Unlimited Neighborhood Interaction for Heterogeneous Trajectory Prediction. (arXiv:2108.00238v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shan_M/0/1/0/all/0/1\">Mo Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qiaojun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jau_Y/0/1/0/all/0/1\">You-Yi Jau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1\">Nikolay Atanasov</a>",
          "description": "Autonomous systems need to understand the semantics and geometry of their\nsurroundings in order to comprehend and safely execute object-level task\nspecifications. This paper proposes an expressive yet compact model for joint\nobject pose and shape optimization, and an associated optimization algorithm to\ninfer an object-level map from multi-view RGB-D camera observations. The model\nis expressive because it captures the identities, positions, orientations, and\nshapes of objects in the environment. It is compact because it relies on a\nlow-dimensional latent representation of implicit object shape, allowing\nonboard storage of large multi-category object maps. Different from other works\nthat rely on a single object representation format, our approach has a bi-level\nobject model that captures both the coarse level scale as well as the fine\nlevel shape details. Our approach is evaluated on the large-scale real-world\nScanNet dataset and compared against state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.00355",
          "publishedOn": "2021-08-03T02:06:30.599Z",
          "wordCount": 598,
          "title": "ELLIPSDF: Joint Object Pose and Shape Optimization with a Bi-level Ellipsoid and Signed Distance Function Description. (arXiv:2108.00355v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Mingyuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoqiong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuxin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xindi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>",
          "description": "3D ultrasound (US) is widely used for its rich diagnostic information.\nHowever, it is criticized for its limited field of view. 3D freehand US\nreconstruction is promising in addressing the problem by providing broad range\nand freeform scan. The existing deep learning based methods only focus on the\nbasic cases of skill sequences, and the model relies on the training data\nheavily. The sequences in real clinical practice are a mix of diverse skills\nand have complex scanning paths. Besides, deep models should adapt themselves\nto the testing cases with prior knowledge for better robustness, rather than\nonly fit to the training cases. In this paper, we propose a novel approach to\nsensorless freehand 3D US reconstruction considering the complex skill\nsequences. Our contribution is three-fold. First, we advance a novel online\nlearning framework by designing a differentiable reconstruction algorithm. It\nrealizes an end-to-end optimization from section sequences to the reconstructed\nvolume. Second, a self-supervised learning method is developed to explore the\ncontext information that reconstructed by the testing data itself, promoting\nthe perception of the model. Third, inspired by the effectiveness of shape\nprior, we also introduce adversarial training to strengthen the learning of\nanatomical shape prior in the reconstructed volume. By mining the context and\nstructural cues of the testing data, our online learning methods can drive the\nmodel to handle complex skill sequences. Experimental results on developmental\ndysplasia of the hip US and fetal US datasets show that, our proposed method\ncan outperform the start-of-the-art methods regarding the shift errors and path\nsimilarities.",
          "link": "http://arxiv.org/abs/2108.00274",
          "publishedOn": "2021-08-03T02:06:30.590Z",
          "wordCount": 720,
          "title": "Self Context and Shape Prior for Sensorless Freehand 3D Ultrasound Reconstruction. (arXiv:2108.00274v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Li Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kaiwen Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Z. Jane Wang</a>",
          "description": "Deep learning based image classification models are shown vulnerable to\nadversarial attacks by injecting deliberately crafted noises to clean images.\nTo defend against adversarial attacks in a training-free and attack-agnostic\nmanner, this work proposes a novel and effective reconstruction-based defense\nframework by delving into deep image prior (DIP). Fundamentally different from\nexisting reconstruction-based defenses, the proposed method analyzes and\nexplicitly incorporates the model decision process into our defense. Given an\nadversarial image, firstly we map its reconstructed images during DIP\noptimization to the model decision space, where cross-boundary images can be\ndetected and on-boundary images can be further localized. Then, adversarial\nnoise is purified by perturbing on-boundary images along the reverse direction\nto the adversarial image. Finally, on-manifold images are stitched to construct\nan image that can be correctly predicted by the victim classifier. Extensive\nexperiments demonstrate that the proposed method outperforms existing\nstate-of-the-art reconstruction-based methods both in defending white-box\nattacks and defense-aware attacks. Moreover, the proposed method can maintain a\nhigh visual quality during adversarial image reconstruction.",
          "link": "http://arxiv.org/abs/2108.00180",
          "publishedOn": "2021-08-03T02:06:30.581Z",
          "wordCount": 629,
          "title": "Delving into Deep Image Prior for Adversarial Defense: A Novel Reconstruction-based Defense Framework. (arXiv:2108.00180v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaibing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1\">Renshu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toyoura_M/0/1/0/all/0/1\">Masahiro Toyoura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Gang Xu</a>",
          "description": "A key challenge in the task of human pose and shape estimation is occlusion,\nincluding self-occlusions, object-human occlusions, and inter-person\nocclusions. The lack of diverse and accurate pose and shape training data\nbecomes a major bottleneck, especially for scenes with occlusions in the wild.\nIn this paper, we focus on the estimation of human pose and shape in the case\nof inter-person occlusions, while also handling object-human occlusions and\nself-occlusion. We propose a framework that synthesizes occlusion-aware\nsilhouette and 2D keypoints data and directly regress to the SMPL pose and\nshape parameters. A neural 3D mesh renderer is exploited to enable silhouette\nsupervision on the fly, which contributes to great improvements in shape\nestimation. In addition, keypoints-and-silhouette-driven training data in\npanoramic viewpoints are synthesized to compensate for the lack of viewpoint\ndiversity in any existing dataset. Experimental results show that we are among\nstate-of-the-art on the 3DPW dataset in terms of pose accuracy and evidently\noutperform the rank-1 method in terms of shape accuracy. Top performance is\nalso achieved on SSP-3D in terms of shape prediction accuracy.",
          "link": "http://arxiv.org/abs/2108.00351",
          "publishedOn": "2021-08-03T02:06:30.573Z",
          "wordCount": 628,
          "title": "LASOR: Learning Accurate 3D Human Pose and Shape Via Synthetic Occlusion-Aware Data and Neural Mesh Rendering. (arXiv:2108.00351v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_D/0/1/0/all/0/1\">Deming Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>",
          "description": "Learning with noisy labels is an important and challenging task for training\naccurate deep neural networks. Some commonly-used loss functions, such as Cross\nEntropy (CE), suffer from severe overfitting to noisy labels. Robust loss\nfunctions that satisfy the symmetric condition were tailored to remedy this\nproblem, which however encounter the underfitting effect. In this paper, we\ntheoretically prove that \\textbf{any loss can be made robust to noisy labels}\nby restricting the network output to the set of permutations over a fixed\nvector. When the fixed vector is one-hot, we only need to constrain the output\nto be one-hot, which however produces zero gradients almost everywhere and thus\nmakes gradient-based optimization difficult. In this work, we introduce the\nsparse regularization strategy to approximate the one-hot constraint, which is\ncomposed of network output sharpening operation that enforces the output\ndistribution of a network to be sharp and the $\\ell_p$-norm ($p\\le 1$)\nregularization that promotes the network output to be sparse. This simple\napproach guarantees the robustness of arbitrary loss functions while not\nhindering the fitting ability. Experimental results demonstrate that our method\ncan significantly improve the performance of commonly-used loss functions in\nthe presence of noisy labels and class imbalance, and outperform the\nstate-of-the-art methods. The code is available at\nhttps://github.com/hitcszx/lnl_sr.",
          "link": "http://arxiv.org/abs/2108.00192",
          "publishedOn": "2021-08-03T02:06:30.550Z",
          "wordCount": 657,
          "title": "Learning with Noisy Labels via Sparse Regularization. (arXiv:2108.00192v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kelvin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Healey_C/0/1/0/all/0/1\">Christopher Healey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianfu Wu</a>",
          "description": "Stereo matching has recently witnessed remarkable progress using Deep Neural\nNetworks (DNNs). But, how robust are they? Although it has been well-known that\nDNNs often suffer from adversarial vulnerability with a catastrophic drop in\nperformance, the situation is even worse in stereo matching. This paper first\nshows that a type of weak white-box attacks can fail state-of-the-art methods.\nThe attack is learned by a proposed stereo-constrained projected gradient\ndescent (PGD) method in stereo matching. This observation raises serious\nconcerns for the deployment of DNN-based stereo matching. Parallel to the\nadversarial vulnerability, DNN-based stereo matching is typically trained under\nthe so-called simulation to reality pipeline, and thus domain generalizability\nis an important problem. This paper proposes to rethink the learnable DNN-based\nfeature backbone towards adversarially-robust and domain generalizable stereo\nmatching, either by completely removing it or by applying it only to the left\nreference image. It computes the matching cost volume using the classic\nmulti-scale census transform (i.e., local binary pattern) of the raw input\nstereo images, followed by a stacked Hourglass head sub-network solving the\nmatching problem. In experiments, the proposed method is tested in the\nSceneFlow dataset and the KITTI2015 benchmark. It significantly improves the\nadversarial robustness, while retaining accuracy performance comparable to\nstate-of-the-art methods. It also shows better generalizability from simulation\n(SceneFlow) to real (KITTI) datasets when no fine-tuning is used.",
          "link": "http://arxiv.org/abs/2108.00335",
          "publishedOn": "2021-08-03T02:06:30.542Z",
          "wordCount": 670,
          "title": "Towards Adversarially Robust and Domain Generalizable Stereo Matching by Rethinking DNN Feature Backbones. (arXiv:2108.00335v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boekhoudt_K/0/1/0/all/0/1\">Kayleigh Boekhoudt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matei_A/0/1/0/all/0/1\">Alina Matei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghaei_M/0/1/0/all/0/1\">Maya Aghaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talavera_E/0/1/0/all/0/1\">Estefan&#xed;a Talavera</a>",
          "description": "The automatic detection of anomalies captured by surveillance settings is\nessential for speeding the otherwise laborious approach. To date, UCF-Crime is\nthe largest available dataset for automatic visual analysis of anomalies and\nconsists of real-world crime scenes of various categories. In this paper, we\nintroduce HR-Crime, a subset of the UCF-Crime dataset suitable for\nhuman-related anomaly detection tasks. We rely on state-of-the-art techniques\nto build the feature extraction pipeline for human-related anomaly detection.\nFurthermore, we present the baseline anomaly detection analysis on the\nHR-Crime. HR-Crime as well as the developed feature extraction pipeline and the\nextracted features will be publicly available for further research in the\nfield.",
          "link": "http://arxiv.org/abs/2108.00246",
          "publishedOn": "2021-08-03T02:06:30.533Z",
          "wordCount": 548,
          "title": "HR-Crime: Human-Related Anomaly Detection in Surveillance Videos. (arXiv:2108.00246v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00273",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Muhammad Monjurul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ruwen Qin</a>",
          "description": "Traffic accident anticipation is a vital function of Automated Driving\nSystems (ADSs) for providing a safety-guaranteed driving experience. An\naccident anticipation model aims to predict accidents promptly and accurately\nbefore they occur. Existing Artificial Intelligence (AI) models of accident\nanticipation lack a human-interpretable explanation of their decision-making.\nAlthough these models perform well, they remain a black-box to the ADS users,\nthus difficult to get their trust. To this end, this paper presents a Gated\nRecurrent Unit (GRU) network that learns spatio-temporal relational features\nfor the early anticipation of traffic accidents from dashcam video data. A\npost-hoc attention mechanism named Grad-CAM is integrated into the network to\ngenerate saliency maps as the visual explanation of the accident anticipation\ndecision. An eye tracker captures human eye fixation points for generating\nhuman attention maps. The explainability of network-generated saliency maps is\nevaluated in comparison to human attention maps. Qualitative and quantitative\nresults on a public crash dataset confirm that the proposed explainable network\ncan anticipate an accident on average 4.57 seconds before it occurs, with\n94.02% average precision. In further, various post-hoc attention-based XAI\nmethods are evaluated and compared. It confirms that the Grad-CAM chosen by\nthis study can generate high-quality, human-interpretable saliency maps (with\n1.42 Normalized Scanpath Saliency) for explaining the crash anticipation\ndecision. Importantly, results confirm that the proposed AI model, with a\nhuman-inspired design, can outperform humans in the accident anticipation.",
          "link": "http://arxiv.org/abs/2108.00273",
          "publishedOn": "2021-08-03T02:06:30.522Z",
          "wordCount": 677,
          "title": "Towards explainable artificial intelligence (XAI) for early anticipation of traffic accidents. (arXiv:2108.00273v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Ziyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhenghao Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Gangming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Weifeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>",
          "description": "Deep features have been proven powerful in building accurate dense semantic\ncorrespondences in various previous works. However, the multi-scale and\npyramidal hierarchy of convolutional neural networks has not been well studied\nto learn discriminative pixel-level features for semantic correspondence. In\nthis paper, we propose a multi-scale matching network that is sensitive to tiny\nsemantic differences between neighboring pixels. We follow the coarse-to-fine\nmatching strategy and build a top-down feature and matching enhancement scheme\nthat is coupled with the multi-scale hierarchy of deep convolutional neural\nnetworks. During feature enhancement, intra-scale enhancement fuses\nsame-resolution feature maps from multiple layers together via local\nself-attention and cross-scale enhancement hallucinates higher-resolution\nfeature maps along the top-down hierarchy. Besides, we learn complementary\nmatching details at different scales thus the overall matching score is refined\nby features of different semantic levels gradually. Our multi-scale matching\nnetwork can be trained end-to-end easily with few additional learnable\nparameters. Experimental results demonstrate that the proposed method achieves\nstate-of-the-art performance on three popular benchmarks with high\ncomputational efficiency.",
          "link": "http://arxiv.org/abs/2108.00211",
          "publishedOn": "2021-08-03T02:06:30.514Z",
          "wordCount": 609,
          "title": "Multi-scale Matching Networks for Semantic Correspondence. (arXiv:2108.00211v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>",
          "description": "Transformers have made much progress in dealing with visual tasks. However,\nexisting vision transformers still do not possess an ability that is important\nto visual input: building the attention among features of different scales. The\nreasons for this problem are two-fold: (1) Input embeddings of each layer are\nequal-scale without cross-scale features; (2) Some vision transformers\nsacrifice the small-scale features of embeddings to lower the cost of the\nself-attention module. To make up this defect, we propose Cross-scale Embedding\nLayer (CEL) and Long Short Distance Attention (LSDA). In particular, CEL blends\neach embedding with multiple patches of different scales, providing the model\nwith cross-scale embeddings. LSDA splits the self-attention module into a\nshort-distance and long-distance one, also lowering the cost but keeping both\nsmall-scale and large-scale features in embeddings. Through these two designs,\nwe achieve cross-scale attention. Besides, we propose dynamic position bias for\nvision transformers to make the popular relative position bias apply to\nvariable-sized images. Based on these proposed modules, we construct our vision\narchitecture called CrossFormer. Experiments show that CrossFormer outperforms\nother transformers on several representative visual tasks, especially object\ndetection and segmentation. The code has been released:\nhttps://github.com/cheerss/CrossFormer.",
          "link": "http://arxiv.org/abs/2108.00154",
          "publishedOn": "2021-08-03T02:06:30.495Z",
          "wordCount": 648,
          "title": "CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention. (arXiv:2108.00154v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Emam_Z/0/1/0/all/0/1\">Zeyad Emam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondrich_A/0/1/0/all/0/1\">Andrew Kondrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_S/0/1/0/all/0/1\">Sasha Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_F/0/1/0/all/0/1\">Felix Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yushi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1\">Aerin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Branson_E/0/1/0/all/0/1\">Elliot Branson</a>",
          "description": "High-quality labeled datasets play a crucial role in fueling the development\nof machine learning (ML), and in particular the development of deep learning\n(DL). However, since the emergence of the ImageNet dataset and the AlexNet\nmodel in 2012, the size of new open-source labeled vision datasets has remained\nroughly constant. Consequently, only a minority of publications in the computer\nvision community tackle supervised learning on datasets that are orders of\nmagnitude larger than Imagenet. In this paper, we survey computer vision\nresearch domains that study the effects of such large datasets on model\nperformance across different vision tasks. We summarize the community's current\nunderstanding of those effects, and highlight some open questions related to\ntraining with massive datasets. In particular, we tackle: (a) The largest\ndatasets currently used in computer vision research and the interesting\ntakeaways from training on such datasets; (b) The effectiveness of pre-training\non large datasets; (c) Recent advancements and hurdles facing synthetic\ndatasets; (d) An overview of double descent and sample non-monotonicity\nphenomena; and finally, (e) A brief discussion of lifelong/continual learning\nand how it fares compared to learning from huge labeled datasets in an offline\nsetting. Overall, our findings are that research on optimization for deep\nlearning focuses on perfecting the training routine and thus making DL models\nless data hungry, while research on synthetic datasets aims to offset the cost\nof data labeling. However, for the time being, acquiring non-synthetic labeled\ndata remains indispensable to boost performance.",
          "link": "http://arxiv.org/abs/2108.00114",
          "publishedOn": "2021-08-03T02:06:30.486Z",
          "wordCount": 707,
          "title": "On The State of Data In Computer Vision: Human Annotations Remain Indispensable for Developing Deep Learning Models. (arXiv:2108.00114v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gichoya_J/0/1/0/all/0/1\">Judy Wawira Gichoya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purkayastha_S/0/1/0/all/0/1\">Saptarshi Purkayastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_I/0/1/0/all/0/1\">Imon Banerjee</a>",
          "description": "Traditional anomaly detection methods focus on detecting inter-class\nvariations while medical image novelty identification is inherently an\nintra-class detection problem. For example, a machine learning model trained\nwith normal chest X-ray and common lung abnormalities, is expected to discover\nand flag idiopathic pulmonary fibrosis which a rare lung disease and unseen by\nthe model during training. The nuances from intra-class variations and lack of\nrelevant training data in medical image analysis pose great challenges for\nexisting anomaly detection methods. To tackle the challenges, we propose a\nhybrid model - Transformation-based Embedding learning for Novelty Detection\n(TEND) which without any out-of-distribution training data, performs novelty\nidentification by combining both autoencoder-based and classifier-based method.\nWith a pre-trained autoencoder as image feature extractor, TEND learns to\ndiscriminate the feature embeddings of in-distribution data from the\ntransformed counterparts as fake out-of-distribution inputs. To enhance the\nseparation, a distance objective is optimized to enforce a margin between the\ntwo classes. Extensive experimental results on both natural image datasets and\nmedical image datasets are presented and our method out-performs\nstate-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.00117",
          "publishedOn": "2021-08-03T02:06:30.477Z",
          "wordCount": 614,
          "title": "Margin-Aware Intra-Class Novelty Identification for Medical Images. (arXiv:2108.00117v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>",
          "description": "Occluded person re-identification (ReID) aims to match person images with\nocclusion. It is fundamentally challenging because of the serious occlusion\nwhich aggravates the misalignment problem between images. At the cost of\nincorporating a pose estimator, many works introduce pose information to\nalleviate the misalignment in both training and testing. To achieve high\naccuracy while preserving low inference complexity, we propose a network named\nPose-Guided Feature Learning with Knowledge Distillation (PGFL-KD), where the\npose information is exploited to regularize the learning of semantics aligned\nfeatures but is discarded in testing. PGFL-KD consists of a main branch (MB),\nand two pose-guided branches, \\ieno, a foreground-enhanced branch (FEB), and a\nbody part semantics aligned branch (SAB). The FEB intends to emphasise the\nfeatures of visible body parts while excluding the interference of obstructions\nand background (\\ieno, foreground feature alignment). The SAB encourages\ndifferent channel groups to focus on different body parts to have body part\nsemantics aligned representation. To get rid of the dependency on pose\ninformation when testing, we regularize the MB to learn the merits of the FEB\nand SAB through knowledge distillation and interaction-based training.\nExtensive experiments on occluded, partial, and holistic ReID tasks show the\neffectiveness of our proposed network.",
          "link": "http://arxiv.org/abs/2108.00139",
          "publishedOn": "2021-08-03T02:06:30.442Z",
          "wordCount": 652,
          "title": "Pose-Guided Feature Learning with Knowledge Distillation for Occluded Person Re-Identification. (arXiv:2108.00139v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">You-Wei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1\">Chuan-Xian Ren</a>",
          "description": "As a vital problem in classification-oriented transfer, unsupervised domain\nadaptation (UDA) has attracted widespread attention in recent years. Previous\nUDA methods assume the marginal distributions of different domains are shifted\nwhile ignoring the discriminant information in the label distributions. This\nleads to classification performance degeneration in real applications. In this\nwork, we focus on the conditional distribution shift problem which is of great\nconcern to current conditional invariant models. We aim to seek a kernel\ncovariance embedding for conditional distribution which remains yet unexplored.\nTheoretically, we propose the Conditional Kernel Bures (CKB) metric for\ncharacterizing conditional distribution discrepancy, and derive an empirical\nestimation for the CKB metric without introducing the implicit kernel feature\nmap. It provides an interpretable approach to understand the knowledge transfer\nmechanism. The established consistency theory of the empirical estimation\nprovides a theoretical guarantee for convergence. A conditional distribution\nmatching network is proposed to learn the conditional invariant and\ndiscriminative features for UDA. Extensive experiments and analysis show the\nsuperiority of our proposed model.",
          "link": "http://arxiv.org/abs/2108.00302",
          "publishedOn": "2021-08-03T02:06:30.434Z",
          "wordCount": 599,
          "title": "Conditional Bures Metric for Domain Adaptation. (arXiv:2108.00302v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fengping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Chun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_D/0/1/0/all/0/1\">Danmin Miao</a>",
          "description": "Micro-expressions are spontaneous, unconscious facial movements that show\npeople's true inner emotions and have great potential in related fields of\npsychological testing. Since the face is a 3D deformation object, the\noccurrence of an expression can arouse spatial deformation of the face, but\nlimited by the available databases are 2D videos, which lack the description of\n3D spatial information of micro-expressions. Therefore, we proposed a new\nmicro-expression database containing 2D video sequences and 3D point clouds\nsequences. The database includes 259 micro-expressions sequences, and these\nsamples were classified using the objective method based on facial action\ncoding system, as well as the non-objective method that combines video contents\nand participants' self-reports. We extracted facial 2D and 3D features using\nlocal binary patterns on three orthogonal planes and curvature descriptors,\nrespectively, and performed baseline evaluations of the two features and their\nfusion results with leave-one-subject-out(LOSO) and 10-fold cross-validation\nmethods. The best fusion performances were 58.84% and 73.03% for non-objective\nclassification and 66.36% and 77.42% for objective classification, both of\nwhich have improved performance compared to using LBP-TOP features only.The\ndatabase offers original and cropped micro-expression samples, which will\nfacilitate the exploration and research on 3D Spatio-temporal features of\nmicro-expressions.",
          "link": "http://arxiv.org/abs/2108.00166",
          "publishedOn": "2021-08-03T02:06:30.427Z",
          "wordCount": 637,
          "title": "A Dynamic 3D Spontaneous Micro-expression Database: Establishment and Evaluation. (arXiv:2108.00166v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00094",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gutierrez_N/0/1/0/all/0/1\">Nolan B. Gutierrez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beksi_W/0/1/0/all/0/1\">William J. Beksi</a>",
          "description": "Thermal images model the long-infrared range of the electromagnetic spectrum\nand provide meaningful information even when there is no visible illumination.\nYet, unlike imagery that represents radiation from the visible continuum,\ninfrared images are inherently low-resolution due to hardware constraints. The\nrestoration of thermal images is critical for applications that involve safety,\nsearch and rescue, and military operations. In this paper, we introduce a\nsystem to efficiently reconstruct thermal images. Specifically, we explore how\nto effectively attend to contrasting receptive fields (RFs) where increasing\nthe RFs of a network can be computationally expensive. For this purpose, we\nintroduce a deep attention to varying receptive fields network (AVRFN). We\nsupply a gated convolutional layer with higher-order information extracted from\ndisparate RFs, whereby an RF is parameterized by a dilation rate. In this way,\nthe dilation rate can be tuned to use fewer parameters thus increasing the\nefficacy of AVRFN. Our experimental results show an improvement over the state\nof the art when compared against competing thermal image super-resolution\nmethods.",
          "link": "http://arxiv.org/abs/2108.00094",
          "publishedOn": "2021-08-03T02:06:30.395Z",
          "wordCount": 631,
          "title": "Thermal Image Super-Resolution Using Second-Order Channel Attention with Varying Receptive Fields. (arXiv:2108.00094v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00146",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1\">Lipeng Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>",
          "description": "Top-$k$ multi-label learning, which returns the top-$k$ predicted labels from\nan input, has many practical applications such as image annotation, document\nanalysis, and web search engine. However, the vulnerabilities of such\nalgorithms with regards to dedicated adversarial perturbation attacks have not\nbeen extensively studied previously. In this work, we develop methods to create\nadversarial perturbations that can be used to attack top-$k$ multi-label\nlearning-based image annotation systems (TkML-AP). Our methods explicitly\nconsider the top-$k$ ranking relation and are based on novel loss functions.\nExperimental evaluations on large-scale benchmark datasets including PASCAL VOC\nand MS COCO demonstrate the effectiveness of our methods in reducing the\nperformance of state-of-the-art top-$k$ multi-label learning methods, under\nboth untargeted and targeted attacks.",
          "link": "http://arxiv.org/abs/2108.00146",
          "publishedOn": "2021-08-03T02:06:30.388Z",
          "wordCount": 563,
          "title": "T$_k$ML-AP: Adversarial Attacks to Top-$k$ Multi-Label Learning. (arXiv:2108.00146v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhaoming Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1\">Liang Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>",
          "description": "In this paper, we propose MGNet, a simple and effective multiplex graph\nconvolutional network (GCN) model for multimodal brain network analysis. The\nproposed method integrates tensor representation into the multiplex GCN model\nto extract the latent structures of a set of multimodal brain networks, which\nallows an intuitive 'grasping' of the common space for multimodal data.\nMultimodal representations are then generated with multiplex GCNs to capture\nspecific graph structures. We conduct classification task on two challenging\nreal-world datasets (HIV and Bipolar disorder), and the proposed MGNet\ndemonstrates state-of-the-art performance compared to competitive benchmark\nmethods. Apart from objective evaluations, this study may bear special\nsignificance upon network theory to the understanding of human connectome in\ndifferent modalities. The code is available at\nhttps://github.com/ZhaomingKong/MGNets.",
          "link": "http://arxiv.org/abs/2108.00158",
          "publishedOn": "2021-08-03T02:06:30.381Z",
          "wordCount": 563,
          "title": "Multiplex Graph Networks for Multimodal Brain Network Analysis. (arXiv:2108.00158v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1\">Zhongyun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Gang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chunxia Xiao</a>",
          "description": "The seamless illumination integration between a foreground object and a\nbackground scene is an important but challenging task in computer vision and\naugmented reality community. However, to our knowledge, there is no publicly\navailable high-quality dataset that meets the illumination seamless integration\ntask, which greatly hinders the development of this research direction. To this\nend, we apply a physically-based rendering method to create a large-scale,\nhigh-quality dataset, named IH dataset, which provides rich illumination\ninformation for seamless illumination integration task. In addition, we propose\na deep learning-based SI-GAN method, a multi-task collaborative network, which\nmakes full use of the multi-scale attention mechanism and adversarial learning\nstrategy to directly infer mapping relationship between the inserted foreground\nobject and corresponding background environment, and edit object illumination\naccording to the proposed illumination exchange mechanism in parallel network.\nBy this means, we can achieve the seamless illumination integration without\nexplicit estimation of 3D geometric information. Comprehensive experiments on\nboth our dataset and real-world images collected from the Internet show that\nour proposed SI-GAN provides a practical and effective solution for image-based\nobject illumination editing, and validate the superiority of our method against\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.00150",
          "publishedOn": "2021-08-03T02:06:30.360Z",
          "wordCount": 633,
          "title": "Scene Inference for Object Illumination Editing. (arXiv:2108.00150v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara L. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "We introduce mTVR, a large-scale multilingual video moment retrieval dataset,\ncontaining 218K English and Chinese queries from 21.8K TV show video clips. The\ndataset is collected by extending the popular TVR dataset (in English) with\npaired Chinese queries and subtitles. Compared to existing moment retrieval\ndatasets, mTVR is multilingual, larger, and comes with diverse annotations. We\nfurther propose mXML, a multilingual moment retrieval model that learns and\noperates on data from both languages, via encoder parameter sharing and\nlanguage neighborhood constraints. We demonstrate the effectiveness of mXML on\nthe newly collected MTVR dataset, where mXML outperforms strong monolingual\nbaselines while using fewer parameters. In addition, we also provide detailed\ndataset analyses and model ablations. Data and code are publicly available at\nhttps://github.com/jayleicn/mTVRetrieval",
          "link": "http://arxiv.org/abs/2108.00061",
          "publishedOn": "2021-08-03T02:06:30.354Z",
          "wordCount": 569,
          "title": "MTVR: Multilingual Moment Retrieval in Videos. (arXiv:2108.00061v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Sangwoo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hyunwoo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kihyuk Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Contrastive self-supervised learning has shown impressive results in learning\nvisual representations from unlabeled images by enforcing invariance against\ndifferent data augmentations. However, the learned representations are often\ncontextually biased to the spurious scene correlations of different objects or\nobject and background, which may harm their generalization on the downstream\ntasks. To tackle the issue, we develop a novel object-aware contrastive\nlearning framework that first (a) localizes objects in a self-supervised manner\nand then (b) debias scene correlations via appropriate data augmentations\nconsidering the inferred object locations. For (a), we propose the contrastive\nclass activation map (ContraCAM), which finds the most discriminative regions\n(e.g., objects) in the image compared to the other images using the\ncontrastively trained models. We further improve the ContraCAM to detect\nmultiple objects and entire shapes via an iterative refinement procedure. For\n(b), we introduce two data augmentations based on ContraCAM, object-aware\nrandom crop and background mixup, which reduce contextual and background biases\nduring contrastive self-supervised learning, respectively. Our experiments\ndemonstrate the effectiveness of our representation learning framework,\nparticularly when trained under multi-object images or evaluated under the\nbackground (and distribution) shifted images.",
          "link": "http://arxiv.org/abs/2108.00049",
          "publishedOn": "2021-08-03T02:06:30.347Z",
          "wordCount": 632,
          "title": "Object-aware Contrastive Learning for Debiased Scene Representation. (arXiv:2108.00049v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Gary Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wessler_B/0/1/0/all/0/1\">Benjamin Wessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1\">Michael C. Hughes</a>",
          "description": "Semi-supervised image classification has shown substantial progress in\nlearning from limited labeled data, but recent advances remain largely untested\nfor clinical applications. Motivated by the urgent need to improve timely\ndiagnosis of life-threatening heart conditions, especially aortic stenosis, we\ndevelop a benchmark dataset to assess semi-supervised approaches to two tasks\nrelevant to cardiac ultrasound (echocardiogram) interpretation: view\nclassification and disease severity classification. We find that a\nstate-of-the-art method called MixMatch achieves promising gains in heldout\naccuracy on both tasks, learning from a large volume of truly unlabeled images\nas well as a labeled set collected at great expense to achieve better\nperformance than is possible with the labeled set alone. We further pursue\npatient-level diagnosis prediction, which requires aggregating across hundreds\nof images of diverse view types, most of which are irrelevant, to make a\ncoherent prediction. The best patient-level performance is achieved by new\nmethods that prioritize diagnosis predictions from images that are predicted to\nbe clinically-relevant views and transfer knowledge from the view task to the\ndiagnosis task. We hope our released Tufts Medical Echocardiogram Dataset and\nevaluation framework inspire further improvements in multi-task semi-supervised\nlearning for clinical applications.",
          "link": "http://arxiv.org/abs/2108.00080",
          "publishedOn": "2021-08-03T02:06:30.267Z",
          "wordCount": 674,
          "title": "A New Semi-supervised Learning Benchmark for Classifying View and Diagnosing Aortic Stenosis from Echocardiograms. (arXiv:2108.00080v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00145",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lantao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kuida Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orchard_M/0/1/0/all/0/1\">Michael T. Orchard</a>",
          "description": "Manifold models consider natural-image patches to be on a low-dimensional\nmanifold embedded in a high dimensional state space and each patch and its\nsimilar patches to approximately lie on a linear affine subspace. Manifold\nmodels are closely related to semi-local similarity, a well-known property of\nnatural images, referring to that for most natural-image patches, several\nsimilar patches can be found in its spatial neighborhood. Many approaches to\nsingle image interpolation use manifold models to exploit semi-local similarity\nby two mutually exclusive parts: i) searching each target patch's similar\npatches and ii) operating on the searched similar patches, the target patch and\nthe measured input pixels to estimate the target patch. Unfortunately, aliasing\nin the input image makes it challenging for both parts. A very few works\nexplicitly deal with those challenges and only ad-hoc solutions are proposed.\n\nTo overcome the challenge in the first part, we propose a carefully-designed\nadaptive technique to remove aliasing in severely aliased regions, which cannot\nbe removed from traditional techniques. This technique enables reliable\nidentification of similar patches even in the presence of strong aliasing. To\novercome the challenge in the second part, we propose to use the\naliasing-removed image to guide the initialization of the interpolated image\nand develop a progressive scheme to refine the interpolated image based on\nmanifold models. Experimental results demonstrate that our approach\nreconstructs edges with both smoothness along contours and sharpness across\nprofiles, and achieves an average Peak Signal-to-Noise Ratio (PSNR)\nsignificantly higher than existing model-based approaches.",
          "link": "http://arxiv.org/abs/2108.00145",
          "publishedOn": "2021-08-03T02:06:30.254Z",
          "wordCount": 676,
          "title": "Manifold-Inspired Single Image Interpolation. (arXiv:2108.00145v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dyck_L/0/1/0/all/0/1\">Leonard E. van Dyck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwitt_R/0/1/0/all/0/1\">Roland Kwitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_S/0/1/0/all/0/1\">Sebastian J. Denzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruber_W/0/1/0/all/0/1\">Walter R. Gruber</a>",
          "description": "Deep convolutional neural networks (DCNNs) and the ventral visual pathway\nshare vast architectural and functional similarities in visual challenges such\nas object recognition. Recent insights have demonstrated that both hierarchical\ncascades can be compared in terms of both exerted behavior and underlying\nactivation. However, these approaches ignore key differences in spatial\npriorities of information processing. In this proof-of-concept study, we\ndemonstrate a comparison of human observers (N = 45) and three feedforward\nDCNNs through eye tracking and saliency maps. The results reveal fundamentally\ndifferent resolutions in both visualization methods that need to be considered\nfor an insightful comparison. Moreover, we provide evidence that a DCNN with\nbiologically plausible receptive field sizes called vNet reveals higher\nagreement with human viewing behavior as contrasted with a standard ResNet\narchitecture. We find that image-specific factors such as category, animacy,\narousal, and valence have a direct link to the agreement of spatial object\nrecognition priorities in humans and DCNNs, while other measures such as\ndifficulty and general image properties do not. With this approach, we try to\nopen up new perspectives at the intersection of biological and computer vision\nresearch.",
          "link": "http://arxiv.org/abs/2108.00107",
          "publishedOn": "2021-08-03T02:06:30.233Z",
          "wordCount": 648,
          "title": "Comparing object recognition in humans and deep convolutional neural networks -- An eye tracking study. (arXiv:2108.00107v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alamri_F/0/1/0/all/0/1\">Faisal Alamri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1\">Anjan Dutta</a>",
          "description": "Zero-Shot Learning (ZSL) aims to recognise unseen object classes, which are\nnot observed during the training phase. The existing body of works on ZSL\nmostly relies on pretrained visual features and lacks the explicit attribute\nlocalisation mechanism on images. In this work, we propose an attention-based\nmodel in the problem settings of ZSL to learn attributes useful for unseen\nclass recognition. Our method uses an attention mechanism adapted from Vision\nTransformer to capture and learn discriminative attributes by splitting images\ninto small patches. We conduct experiments on three popular ZSL benchmarks\n(i.e., AWA2, CUB and SUN) and set new state-of-the-art harmonic mean results\n{on all the three datasets}, which illustrate the effectiveness of our proposed\nmethod.",
          "link": "http://arxiv.org/abs/2108.00045",
          "publishedOn": "2021-08-03T02:06:30.226Z",
          "wordCount": 562,
          "title": "Multi-Head Self-Attention via Vision Transformer for Zero-Shot Learning. (arXiv:2108.00045v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1\">Dario Augusto Borges Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_J/0/1/0/all/0/1\">Jorge Guevara Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadrozny_B/0/1/0/all/0/1\">Bianca Zadrozny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watson_C/0/1/0/all/0/1\">Campbell Watson</a>",
          "description": "One of the consequences of climate change is anobserved increase in the\nfrequency of extreme cli-mate events. That poses a challenge for\nweatherforecast and generation algorithms, which learnfrom historical data but\nshould embed an often un-certain bias to create correct scenarios. This\npaperinvestigates how mapping climate data to a knowndistribution using\nvariational autoencoders mighthelp explore such biases and control the\nsynthesisof weather fields towards more extreme climatescenarios. We\nexperimented using a monsoon-affected precipitation dataset from southwest\nIn-dia, which should give a roughly stable pattern ofrainy days and ease our\ninvestigation. We reportcompelling results showing that mapping complexweather\ndata to a known distribution implementsan efficient control for weather field\nsynthesis to-wards more (or less) extreme scenarios.",
          "link": "http://arxiv.org/abs/2108.00048",
          "publishedOn": "2021-08-03T02:06:30.145Z",
          "wordCount": 558,
          "title": "Controlling Weather Field Synthesis Using Variational Autoencoders. (arXiv:2108.00048v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.08334",
          "author": "<a href=\"http://arxiv.org/find/hep-ph/1/au:+Araz_J/0/1/0/all/0/1\">Jack Y. Araz</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Spannowsky_M/0/1/0/all/0/1\">Michael Spannowsky</a>",
          "description": "Tensor Networks are non-trivial representations of high-dimensional tensors,\noriginally designed to describe quantum many-body systems. We show that Tensor\nNetworks are ideal vehicles to connect quantum mechanical concepts to machine\nlearning techniques, thereby facilitating an improved interpretability of\nneural networks. This study presents the discrimination of top quark signal\nover QCD background processes using a Matrix Product State classifier. We show\nthat entanglement entropy can be used to interpret what a network learns, which\ncan be used to reduce the complexity of the network and feature space without\nloss of generality or performance. For the optimisation of the network, we\ncompare the Density Matrix Renormalization Group (DMRG) algorithm to stochastic\ngradient descent (SGD) and propose a joined training algorithm to harness the\nexplainability of DMRG with the efficiency of SGD.",
          "link": "http://arxiv.org/abs/2106.08334",
          "publishedOn": "2021-08-09T00:49:28.895Z",
          "wordCount": 615,
          "title": "Quantum-inspired event reconstruction with Tensor Networks: Matrix Product States. (arXiv:2106.08334v2 [hep-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samavatian_M/0/1/0/all/0/1\">Mohammad Hossein Samavatian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Saikat Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barber_K/0/1/0/all/0/1\">Kristin Barber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_R/0/1/0/all/0/1\">Radu Teodorescu</a>",
          "description": "Deep Neural Networks (DNNs) are employed in an increasing number of\napplications, some of which are safety critical. Unfortunately, DNNs are known\nto be vulnerable to so-called adversarial attacks that manipulate inputs to\ncause incorrect results that can be beneficial to an attacker or damaging to\nthe victim. Multiple defenses have been proposed to increase the robustness of\nDNNs. In general, these defenses have high overhead, some require\nattack-specific re-training of the model or careful tuning to adapt to\ndifferent attacks.\n\nThis paper presents HASI, a hardware-accelerated defense that uses a process\nwe call stochastic inference to detect adversarial inputs. We show that by\ncarefully injecting noise into the model at inference time, we can\ndifferentiate adversarial inputs from benign ones. HASI uses the output\ndistribution characteristics of noisy inference compared to a non-noisy\nreference to detect adversarial inputs. We show an adversarial detection rate\nof 86% when applied to VGG16 and 93% when applied to ResNet50, which exceeds\nthe detection rate of the state of the art approaches, with a much lower\noverhead. We demonstrate two software/hardware-accelerated co-designs, which\nreduces the performance impact of stochastic inference to 1.58X-2X relative to\nthe unprotected baseline, compared to 15X-20X overhead for a software-only GPU\nimplementation.",
          "link": "http://arxiv.org/abs/2106.05825",
          "publishedOn": "2021-08-09T00:49:28.877Z",
          "wordCount": 686,
          "title": "HASI: Hardware-Accelerated Stochastic Inference, A Defense Against Adversarial Machine Learning Attacks. (arXiv:2106.05825v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14144",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1\">Shichao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Yangyang Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yixuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ONeill_Z/0/1/0/all/0/1\">Zheng O&#x27;Neill</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>",
          "description": "As people spend up to 87% of their time indoors, intelligent Heating,\nVentilation, and Air Conditioning (HVAC) systems in buildings are essential for\nmaintaining occupant comfort and reducing energy consumption. These HVAC\nsystems in smart buildings rely on real-time sensor readings, which in practice\noften suffer from various faults and could also be vulnerable to malicious\nattacks. Such faulty sensor inputs may lead to the violation of indoor\nenvironment requirements (e.g., temperature, humidity, etc.) and the increase\nof energy consumption. While many model-based approaches have been proposed in\nthe literature for building HVAC control, it is costly to develop accurate\nphysical models for ensuring their performance and even more challenging to\naddress the impact of sensor faults. In this work, we present a novel\nlearning-based framework for sensor fault-tolerant HVAC control, which includes\nthree deep learning based components for 1) generating temperature proposals\nwith the consideration of possible sensor faults, 2) selecting one of the\nproposals based on the assessment of their accuracy, and 3) applying\nreinforcement learning with the selected temperature proposal. Moreover, to\naddress the challenge of training data insufficiency in building-related tasks,\nwe propose a model-assisted learning method leveraging an abstract model of\nbuilding physical dynamics. Through extensive experiments, we demonstrate that\nthe proposed fault-tolerant HVAC control framework can significantly reduce\nbuilding temperature violations under a variety of sensor fault patterns while\nmaintaining energy efficiency.",
          "link": "http://arxiv.org/abs/2106.14144",
          "publishedOn": "2021-08-09T00:49:28.870Z",
          "wordCount": 694,
          "title": "Learning-based Framework for Sensor Fault-Tolerant Building HVAC Control with Model-assisted Learning. (arXiv:2106.14144v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06073",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Royset_J/0/1/0/all/0/1\">Johannes O. Royset</a>",
          "description": "A basic requirement for a mathematical model is often that its solution\n(output) shouldn't change much if the model's parameters (input) are perturbed.\nThis is important because the exact values of parameters may not be known and\none would like to avoid being mislead by an output obtained using incorrect\nvalues. Thus, it's rarely enough to address an application by formulating a\nmodel, solving the resulting optimization problem and presenting the solution\nas the answer. One would need to confirm that the model is suitable, i.e.,\n\"good,\" and this can, at least in part, be achieved by considering a family of\noptimization problems constructed by perturbing parameters of concern. The\nresulting sensitivity analysis uncovers troubling situations with unstable\nsolutions, which we referred to as \"bad\" models, and indicates better model\nformulations. Embedding an actual problem of interest within a family of\nproblems is also a primary path to optimality conditions as well as\ncomputationally attractive, alternative problems, which under ideal\ncircumstances, and when properly tuned, may even furnish the minimum value of\nthe actual problem. The tuning of these alternative problems turns out to be\nintimately tied to finding multipliers in optimality conditions and thus\nemerges as a main component of several optimization algorithms. In fact, the\ntuning amounts to solving certain dual optimization problems. In this tutorial,\nwe'll discuss the opportunities and insights afforded by this broad\nperspective.",
          "link": "http://arxiv.org/abs/2105.06073",
          "publishedOn": "2021-08-09T00:49:28.863Z",
          "wordCount": 687,
          "title": "Good and Bad Optimization Models: Insights from Rockafellians. (arXiv:2105.06073v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03167",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yamac_M/0/1/0/all/0/1\">Mehmet Yamac</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akpinar_U/0/1/0/all/0/1\">Ugur Akpinar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sahin_E/0/1/0/all/0/1\">Erdem Sahin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gabbouj_M/0/1/0/all/0/1\">Moncef Gabbouj</a>",
          "description": "In CS literature, the efforts can be divided into two groups: finding a\nmeasurement matrix that preserves the compressed information at the maximum\nlevel, and finding a reconstruction algorithm for the compressed information.\nIn the traditional CS setup, the measurement matrices are selected as random\nmatrices, and optimization-based iterative solutions are used to recover the\nsignals. However, when we handle large signals, using random matrices become\ncumbersome especially when it comes to iterative optimization-based solutions.\nEven though recent deep learning-based solutions boost the reconstruction\naccuracy performance while speeding up the recovery, still jointly learning the\nwhole measurement matrix is a difficult process. In this work, we introduce a\nseparable multi-linear learning of the CS matrix by representing it as the\nsummation of arbitrary number of tensors. For a special case where the CS\noperation is set as a single tensor multiplication, the model is reduced to the\nlearning-based separable CS; while a dense CS matrix can be approximated and\nlearned as the summation of multiple tensors. Both cases can be used in CS of\ntwo or multi-dimensional signals e.g., images, multi-spectral images, videos,\netc. Structural CS matrices can also be easily approximated and learned in our\nmulti-linear separable learning setup with structural tensor sum\nrepresentation. Hence, our learnable generalized tensor summation CS operation\nencapsulates most CS setups including separable CS, non-separable CS\n(traditional vector-matrix multiplication), structural CS, and CS of the\nmulti-dimensional signals. For both gray-scale and RGB images, the proposed\nscheme surpasses most state-of-the-art solutions, especially in lower\nmeasurement rates. Although the performance gain remains limited from tensor to\nthe sum of tensor representation for gray-scale images, it becomes significant\nin the RGB case.",
          "link": "http://arxiv.org/abs/2108.03167",
          "publishedOn": "2021-08-09T00:49:28.856Z",
          "wordCount": 747,
          "title": "Generalized Tensor Summation Compressive Sensing Network (GTSNET): An Easy to Learn Compressive Sensing Operation. (arXiv:2108.03167v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>",
          "description": "Active learning (AL) aims at reducing labeling effort by identifying the most\nvaluable unlabeled data points from a large pool. Traditional AL frameworks\nhave two limitations: First, they perform data selection in a multi-round\nmanner, which is time-consuming and impractical. Second, they usually assume\nthat there are a small amount of labeled data points available in the same\ndomain as the data in the unlabeled pool. Recent work proposes a solution for\none-round active learning based on data utility learning and optimization,\nwhich fixes the first issue but still requires the initially labeled data\npoints in the same domain. In this paper, we propose $\\mathrm{D^2ULO}$ as a\nsolution that solves both issues. Specifically, $\\mathrm{D^2ULO}$ leverages the\nidea of domain adaptation (DA) to train a data utility model which can\neffectively predict the utility for any given unlabeled data in the target\ndomain once labeled. The trained data utility model can then be used to select\nhigh-utility data and at the same time, provide an estimate for the utility of\nthe selected data. Our algorithm does not rely on any feedback from annotators\nin the target domain and hence, can be used to perform zero-round active\nlearning or warm-start existing multi-round active learning strategies. Our\nexperiments show that $\\mathrm{D^2ULO}$ outperforms the existing\nstate-of-the-art AL strategies equipped with domain adaptation over various\ndomain shift settings (e.g., real-to-real data and synthetic-to-real data).\nParticularly, $\\mathrm{D^2ULO}$ is applicable to the scenario where source and\ntarget labels have mismatches, which is not supported by the existing works.",
          "link": "http://arxiv.org/abs/2107.06703",
          "publishedOn": "2021-08-09T00:49:28.849Z",
          "wordCount": 705,
          "title": "Zero-Round Active Learning. (arXiv:2107.06703v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11713",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Nanni_L/0/1/0/all/0/1\">Loris Nanni</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lumini_A/0/1/0/all/0/1\">Alessandra Lumini</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Brahnam_S/0/1/0/all/0/1\">Sheryl Brahnam</a>",
          "description": "Motivation: Automatic Anatomical Therapeutic Chemical (ATC) classification is\na critical and highly competitive area of research in bioinformatics because of\nits potential for expediting drug develop-ment and research. Predicting an\nunknown compound's therapeutic and chemical characteristics ac-cording to how\nthese characteristics affect multiple organs/systems makes automatic ATC\nclassifica-tion a challenging multi-label problem. Results: In this work, we\npropose combining multiple multi-label classifiers trained on distinct sets of\nfeatures, including sets extracted from a Bidirectional Long Short-Term Memory\nNetwork (BiLSTM). Experiments demonstrate the power of this approach, which is\nshown to outperform the best methods reported in the literature, including the\nstate-of-the-art developed by the fast.ai research group. Availability: All\nsource code developed for this study is available at\nhttps://github.com/LorisNanni. Contact: loris.nanni@unipd.it",
          "link": "http://arxiv.org/abs/2101.11713",
          "publishedOn": "2021-08-09T00:49:28.831Z",
          "wordCount": 582,
          "title": "Neural networks for Anatomical Therapeutic Chemical (ATC) classification. (arXiv:2101.11713v3 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06782",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Rana_M/0/1/0/all/0/1\">Mimansa Rana</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Mao_N/0/1/0/all/0/1\">Nanxiang Mao</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Ao_M/0/1/0/all/0/1\">Ming Ao</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Wu_X/0/1/0/all/0/1\">Xiaohui Wu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Liang_P/0/1/0/all/0/1\">Poning Liang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Khushi_M/0/1/0/all/0/1\">Matloob Khushi</a>",
          "description": "The foreign exchange market has taken an important role in the global\nfinancial market. While foreign exchange trading brings high-yield\nopportunities to investors, it also brings certain risks. Since the\nestablishment of the foreign exchange market in the 20th century, foreign\nexchange rate forecasting has become a hot issue studied by scholars from all\nover the world. Due to the complexity and number of factors affecting the\nforeign exchange market, technical analysis cannot respond to administrative\nintervention or unexpected events. Our team chose several pairs of foreign\ncurrency historical data and derived technical indicators from 2005 to 2021 as\nthe dataset and established different machine learning models for event-driven\nprice prediction for oversold scenario.",
          "link": "http://arxiv.org/abs/2107.06782",
          "publishedOn": "2021-08-09T00:49:28.824Z",
          "wordCount": 569,
          "title": "Clustering and attention model based for intelligent trading. (arXiv:2107.06782v2 [q-fin.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garibaldi_J/0/1/0/all/0/1\">Jonathan M Garibaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>",
          "description": "The goal of representation learning is different from the ultimate objective\nof machine learning such as decision making, it is therefore very difficult to\nestablish clear and direct objectives for training representation learning\nmodels. It has been argued that a good representation should disentangle the\nunderlying variation factors, yet how to translate this into training\nobjectives remains unknown. This paper presents an attempt to establish direct\ntraining criterions and design principles for developing good representation\nlearning models. We propose that a good representation learning model should be\nmaximally expressive, i.e., capable of distinguishing the maximum number of\ninput configurations. We formally define expressiveness and introduce the\nmaximum expressiveness (MEXS) theorem of a general learning model. We propose\nto train a model by maximizing its expressiveness while at the same time\nincorporating general priors such as model smoothness. We present a conscience\ncompetitive learning algorithm which encourages the model to reach its MEXS\nwhilst at the same time adheres to model smoothness prior. We also introduce a\nlabel consistent training (LCT) technique to boost model smoothness by\nencouraging it to assign consistent labels to similar samples. We present\nextensive experimental results to show that our method can indeed design\nrepresentation learning models capable of developing representations that are\nas good as or better than state of the art. We also show that our technique is\ncomputationally efficient, robust against different parameter settings and can\nwork effectively on a variety of datasets. Code available at\nhttps://github.com/qlilx/odgrlm.git",
          "link": "http://arxiv.org/abs/2107.05948",
          "publishedOn": "2021-08-09T00:49:28.812Z",
          "wordCount": 716,
          "title": "On Designing Good Representation Learning Models. (arXiv:2107.05948v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1806.00984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fahad_M/0/1/0/all/0/1\">Md. Shah Fahad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_J/0/1/0/all/0/1\">Jainath Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_G/0/1/0/all/0/1\">Gyadhar Pradhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deepak_A/0/1/0/all/0/1\">Akshay Deepak</a>",
          "description": "Speech is produced when time varying vocal tract system is excited with time\nvarying excitation source. Therefore, the information present in a speech such\nas message, emotion, language, speaker is due to the combined effect of both\nexcitation source and vocal tract system. However, there is very less\nutilization of excitation source features to recognize emotion. In our earlier\nwork, we have proposed a novel method to extract glottal closure instants\n(GCIs) known as epochs. In this paper, we have explored epoch features namely\ninstantaneous pitch, phase and strength of epochs for discriminating emotions.\nWe have combined the excitation source features and the well known\nMale-frequency cepstral coefficient (MFCC) features to develop an emotion\nrecognition system with improved performance. DNN-HMM speaker adaptive models\nhave been developed using MFCC, epoch and combined features. IEMOCAP emotional\ndatabase has been used to evaluate the models. The average accuracy for emotion\nrecognition system when using MFCC and epoch features separately is 59.25% and\n54.52% respectively. The recognition performance improves to 64.2% when MFCC\nand epoch features are combined.",
          "link": "http://arxiv.org/abs/1806.00984",
          "publishedOn": "2021-08-09T00:49:28.805Z",
          "wordCount": 650,
          "title": "DNN-HMM based Speaker Adaptive Emotion Recognition using Proposed Epoch and MFCC Features. (arXiv:1806.00984v1 [cs.SD] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandagale_S/0/1/0/all/0/1\">Sujay Khandagale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1\">Colin White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1\">Willie Neiswanger</a>",
          "description": "As machine learning models grow more complex and their applications become\nmore high-stakes, tools for explaining model predictions have become\nincreasingly important. This has spurred a flurry of research in model\nexplainability and has given rise to feature attribution methods such as LIME\nand SHAP. Despite their widespread use, evaluating and comparing different\nfeature attribution methods remains challenging: evaluations ideally require\nhuman studies, and empirical evaluation metrics are often data-intensive or\ncomputationally prohibitive on real-world datasets. In this work, we address\nthis issue by releasing XAI-Bench: a suite of synthetic datasets along with a\nlibrary for benchmarking feature attribution algorithms. Unlike real-world\ndatasets, synthetic datasets allow the efficient computation of conditional\nexpected values that are needed to evaluate ground-truth Shapley values and\nother metrics. The synthetic datasets we release offer a wide variety of\nparameters that can be configured to simulate real-world data. We demonstrate\nthe power of our library by benchmarking popular explainability techniques\nacross several evaluation metrics and across a variety of settings. The\nversatility and efficiency of our library will help researchers bring their\nexplainability methods from development to deployment. Our code is available at\nhttps://github.com/abacusai/xai-bench.",
          "link": "http://arxiv.org/abs/2106.12543",
          "publishedOn": "2021-08-09T00:49:28.797Z",
          "wordCount": 661,
          "title": "Synthetic Benchmarks for Scientific Research in Explainable Machine Learning. (arXiv:2106.12543v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03169",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Abouheaf_M/0/1/0/all/0/1\">Mohammed Abouheaf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qu_S/0/1/0/all/0/1\">Shuzheng Qu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gueaieb_W/0/1/0/all/0/1\">Wail Gueaieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abielmona_R/0/1/0/all/0/1\">Rami Abielmona</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harb_M/0/1/0/all/0/1\">Moufid Harb</a>",
          "description": "This article elaborates on how machine learning (ML) can leverage the\nsolution of a contemporary problem related to the security of maritime domains.\nThe worldwide ``Illegal, Unreported, and Unregulated'' (IUU) fishing incidents\nhave led to serious environmental and economic consequences which involve\ndrastic changes in our ecosystems in addition to financial losses caused by the\ndepletion of natural resources. The Fisheries and Aquatic Department (FAD) of\nthe United Nation's Food and Agriculture Organization (FAO) issued a report\nwhich indicated that the annual losses due to IUU fishing reached $25 Billion.\nThis imposes negative impacts on the future-biodiversity of the marine\necosystem and domestic Gross National Product (GNP). Hence, robust interception\nmechanisms are increasingly needed for detecting and pursuing the unrelenting\nillegal fishing incidents in maritime territories. This article addresses the\nproblem of coordinating the motion of a fleet of marine vessels (pursuers) to\ncatch an IUU vessel while still in local waters. The problem is formulated as a\npursuer-evader problem that is tackled within an ML framework. One or more\npursuers, such as law enforcement vessels, intercept an evader (i.e., the\nillegal fishing ship) using an online reinforcement learning mechanism that is\nbased on a value iteration process. It employs real-time navigation\nmeasurements of the evader ship as well as those of the pursuing vessels and\nreturns back model-free interception strategies.",
          "link": "http://arxiv.org/abs/2108.03169",
          "publishedOn": "2021-08-09T00:49:28.775Z",
          "wordCount": 696,
          "title": "Responding to Illegal Activities Along the Canadian Coastlines Using Reinforcement Learning. (arXiv:2108.03169v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03877",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Konstantinova_T/0/1/0/all/0/1\">Tatiana Konstantinova</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Wiegart_L/0/1/0/all/0/1\">Lutz Wiegart</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Rakitin_M/0/1/0/all/0/1\">Maksim Rakitin</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+DeGennaro_A/0/1/0/all/0/1\">Anthony M. DeGennaro</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Barbour_A/0/1/0/all/0/1\">Andi M. Barbour</a>",
          "description": "Like other experimental techniques, X-ray Photon Correlation Spectroscopy is\nsubject to various kinds of noise. Random and correlated fluctuations and\nheterogeneities can be present in a two-time correlation function and obscure\nthe information about the intrinsic dynamics of a sample. Simultaneously\naddressing the disparate origins of noise in the experimental data is\nchallenging. We propose a computational approach for improving the\nsignal-to-noise ratio in two-time correlation functions that is based on\nConvolutional Neural Network Encoder-Decoder (CNN-ED) models. Such models\nextract features from an image via convolutional layers, project them to a low\ndimensional space and then reconstruct a clean image from this reduced\nrepresentation via transposed convolutional layers. Not only are ED models a\ngeneral tool for random noise removal, but their application to low\nsignal-to-noise data can enhance the data quantitative usage since they are\nable to learn the functional form of the signal. We demonstrate that the CNN-ED\nmodels trained on real-world experimental data help to effectively extract\nequilibrium dynamics parameters from two-time correlation functions, containing\nstatistical noise and dynamic heterogeneities. Strategies for optimizing the\nmodels performance and their applicability limits are discussed.",
          "link": "http://arxiv.org/abs/2102.03877",
          "publishedOn": "2021-08-09T00:49:28.767Z",
          "wordCount": 657,
          "title": "Noise Reduction in X-ray Photon Correlation Spectroscopy with Convolutional Neural Networks Encoder-Decoder Models. (arXiv:2102.03877v2 [cond-mat.mtrl-sci] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1\">Ching Pui Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "Federated learning is vulnerable to various attacks, such as model poisoning\nand backdoor attacks, even if some existing defense strategies are used. To\naddress this challenge, we propose an attack-adaptive aggregation strategy to\ndefend against various attacks for robust federated learning. The proposed\napproach is based on training a neural network with an attention mechanism that\nlearns the vulnerability of federated learning models from a set of plausible\nattacks. To the best of our knowledge, our aggregation strategy is the first\none that can be adapted to defend against various attacks in a data-driven\nfashion. Our approach has achieved competitive performance in defending model\npoisoning and backdoor attacks in federated learning tasks on image and text\ndatasets.",
          "link": "http://arxiv.org/abs/2102.05257",
          "publishedOn": "2021-08-09T00:49:28.759Z",
          "wordCount": 577,
          "title": "Robust Federated Learning with Attack-Adaptive Aggregation. (arXiv:2102.05257v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Joey Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1\">David Dohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rishabh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1\">Charles Sutton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>",
          "description": "In many sequence learning tasks, such as program synthesis and document\nsummarization, a key problem is searching over a large space of possible output\nsequences. We propose to learn representations of the outputs that are\nspecifically meant for search: rich enough to specify the desired output but\ncompact enough to make search more efficient. Discrete latent codes are\nappealing for this purpose, as they naturally allow sophisticated combinatorial\nsearch strategies. The latent codes are learned using a self-supervised\nlearning principle, in which first a discrete autoencoder is trained on the\noutput sequences, and then the resulting latent codes are used as intermediate\ntargets for the end-to-end sequence prediction task. Based on these insights,\nwe introduce the \\emph{Latent Programmer}, a program synthesis method that\nfirst predicts a discrete latent code from input/output examples, and then\ngenerates the program in the target language. We evaluate the Latent Programmer\non two domains: synthesis of string transformation programs, and generation of\nprograms from natural language descriptions. We demonstrate that the discrete\nlatent representation significantly improves synthesis accuracy.",
          "link": "http://arxiv.org/abs/2012.00377",
          "publishedOn": "2021-08-09T00:49:28.752Z",
          "wordCount": 646,
          "title": "Latent Programmer: Discrete Latent Codes for Program Synthesis. (arXiv:2012.00377v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09638",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Glasgow_M/0/1/0/all/0/1\">Margalit Glasgow</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wootters_M/0/1/0/all/0/1\">Mary Wootters</a>",
          "description": "In distributed optimization problems, a technique called gradient coding,\nwhich involves replicating data points, has been used to mitigate the effect of\nstraggling machines. Recent work has studied approximate gradient coding, which\nconcerns coding schemes where the replication factor of the data is too low to\nrecover the full gradient exactly. Our work is motivated by the challenge of\ncreating approximate gradient coding schemes that simultaneously work well in\nboth the adversarial and stochastic models. To that end, we introduce novel\napproximate gradient codes based on expander graphs, in which each machine\nreceives exactly two blocks of data points. We analyze the decoding error both\nin the random and adversarial straggler setting, when optimal decoding\ncoefficients are used. We show that in the random setting, our schemes achieve\nan error to the gradient that decays exponentially in the replication factor.\nIn the adversarial setting, the error is nearly a factor of two smaller than\nany existing code with similar performance in the random setting. We show\nconvergence bounds both in the random and adversarial setting for gradient\ndescent under standard assumptions using our codes. In the random setting, our\nconvergence rate improves upon block-box bounds. In the adversarial setting, we\nshow that gradient descent can converge down to a noise floor that scales\nlinearly with the adversarial error to the gradient. We demonstrate empirically\nthat our schemes achieve near-optimal error in the random setting and converge\nfaster than algorithms which do not use the optimal decoding coefficients.",
          "link": "http://arxiv.org/abs/2006.09638",
          "publishedOn": "2021-08-09T00:49:28.745Z",
          "wordCount": 717,
          "title": "Approximate Gradient Coding with Optimal Decoding. (arXiv:2006.09638v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shangtong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Hengshuai Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1\">Shimon Whiteson</a>",
          "description": "The deadly triad refers to the instability of a reinforcement learning\nalgorithm when it employs off-policy learning, function approximation, and\nbootstrapping simultaneously. In this paper, we investigate the target network\nas a tool for breaking the deadly triad, providing theoretical support for the\nconventional wisdom that a target network stabilizes training. We first propose\nand analyze a novel target network update rule which augments the commonly used\nPolyak-averaging style update with two projections. We then apply the target\nnetwork and ridge regularization in several divergent algorithms and show their\nconvergence to regularized TD fixed points. Those algorithms are off-policy\nwith linear function approximation and bootstrapping, spanning both policy\nevaluation and control, as well as both discounted and average-reward settings.\nIn particular, we provide the first convergent linear $Q$-learning algorithms\nunder nonrestrictive and changing behavior policies without bi-level\noptimization.",
          "link": "http://arxiv.org/abs/2101.08862",
          "publishedOn": "2021-08-09T00:49:28.725Z",
          "wordCount": 624,
          "title": "Breaking the Deadly Triad with a Target Network. (arXiv:2101.08862v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.06175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Juhyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kihyun Kim</a>",
          "description": "We propose a novel approach to optimize fleet management by combining\nmulti-agent reinforcement learning with graph neural network. To provide\nride-hailing service, one needs to optimize dynamic resources and demands over\nspatial domain. While the spatial structure was previously approximated with a\nregular grid, our approach represents the road network with a graph, which\nbetter reflects the underlying geometric structure. Dynamic resource allocation\nis formulated as multi-agent reinforcement learning, whose action-value\nfunction (Q function) is approximated with graph neural networks. We use\nstochastic policy update rule over the graph with deep Q-networks (DQN), and\nachieve superior results over the greedy policy update. We design a realistic\nsimulator that emulates the empirical taxi call data, and confirm the\neffectiveness of the proposed model under various conditions.",
          "link": "http://arxiv.org/abs/2011.06175",
          "publishedOn": "2021-08-09T00:49:28.717Z",
          "wordCount": 604,
          "title": "Optimizing Large-Scale Fleet Management on a Road Network using Multi-Agent Deep Reinforcement Learning with Graph Neural Network. (arXiv:2011.06175v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Torrent_N/0/1/0/all/0/1\">Neus Llop Torrent</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Visani_G/0/1/0/all/0/1\">Giorgio Visani</a> (2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Bagli_E/0/1/0/all/0/1\">Enrico Bagli</a> (2) ((1) Politecnico di Milano Graduate School of Business, (2) CRIF S.p.A, (3) University of Bologna School of Informatics and Engineering)",
          "description": "The aim of this project is to develop and test advanced analytical methods to\nimprove the prediction accuracy of Credit Risk Models, preserving at the same\ntime the model interpretability. In particular, the project focuses on applying\nan explainable machine learning model to bank-related databases. The input data\nwere obtained from open data. Over the total proven models, CatBoost has shown\nthe highest performance. The algorithm implementation produces a GINI of 0.68\nafter tuning the hyper-parameters. SHAP package is used to provide a global and\nlocal interpretation of the model predictions to formulate a\nhuman-comprehensive approach to understanding the decision-maker algorithm. The\n20 most important features are selected using the Shapley values to present a\nfull human-understandable model that reveals how the attributes of an\nindividual are related to its model prediction.",
          "link": "http://arxiv.org/abs/2011.10367",
          "publishedOn": "2021-08-09T00:49:28.710Z",
          "wordCount": 623,
          "title": "PSD2 Explainable AI Model for Credit Scoring. (arXiv:2011.10367v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03068",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Huth_B/0/1/0/all/0/1\">Benjamin Huth</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Salzburger_A/0/1/0/all/0/1\">Andreas Salzburger</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wettig_T/0/1/0/all/0/1\">Tilo Wettig</a>",
          "description": "We present an ongoing R&D activity for machine-learning-assisted navigation\nthrough detectors to be used for track reconstruction. We investigate different\napproaches of training neural networks for surface prediction and compare their\nresults. This work is carried out in the context of the ACTS tracking toolkit.",
          "link": "http://arxiv.org/abs/2108.03068",
          "publishedOn": "2021-08-09T00:49:28.703Z",
          "wordCount": 495,
          "title": "Machine learning for surface prediction in ACTS. (arXiv:2108.03068v1 [physics.ins-det])"
        },
        {
          "id": "http://arxiv.org/abs/2009.13370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Truong_L/0/1/0/all/0/1\">Lan V. Truong</a>",
          "description": "This paper estimates free energy, average mutual information, and minimum\nmean square error (MMSE) of a linear model under two assumptions: (1) the\nsource is generated by a Markov chain, (2) the source is generated via a hidden\nMarkov model. Our estimates are based on the replica method in statistical\nphysics. We show that under the posterior mean estimator, the linear model with\nMarkov sources or hidden Markov sources is decoupled into single-input AWGN\nchannels with state information available at both encoder and decoder where the\nstate distribution follows the left Perron-Frobenius eigenvector with unit\nManhattan norm of the stochastic matrix of Markov chains. Numerical results\nshow that the free energies and MSEs obtained via the replica method are\nclosely approximate to their counterparts achieved by the Metropolis-Hastings\nalgorithm or some well-known approximate message passing algorithms in the\nresearch literature.",
          "link": "http://arxiv.org/abs/2009.13370",
          "publishedOn": "2021-08-09T00:49:28.696Z",
          "wordCount": 638,
          "title": "Replica Analysis of the Linear Model with Markov or Hidden Markov Signal Priors. (arXiv:2009.13370v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03132",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Q/0/1/0/all/0/1\">Qiang Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">Dongxiao Zhang</a>",
          "description": "Random reconstruction of three-dimensional (3D) digital rocks from\ntwo-dimensional (2D) slices is crucial for elucidating the microstructure of\nrocks and its effects on pore-scale flow in terms of numerical modeling, since\nmassive samples are usually required to handle intrinsic uncertainties. Despite\nremarkable advances achieved by traditional process-based methods, statistical\napproaches and recently famous deep learning-based models, few works have\nfocused on producing several kinds of rocks with one trained model and allowing\nthe reconstructed samples to satisfy certain given properties, such as\nporosity. To fill this gap, we propose a new framework, named RockGPT, which is\ncomposed of VQ-VAE and conditional GPT, to synthesize 3D samples based on a\nsingle 2D slice from the perspective of video generation. The VQ-VAE is\nutilized to compress high-dimensional input video, i.e., the sequence of\ncontinuous rock slices, to discrete latent codes and reconstruct them. In order\nto obtain diverse reconstructions, the discrete latent codes are modeled using\nconditional GPT in an autoregressive manner, while incorporating conditional\ninformation from a given slice, rock type, and porosity. We conduct two\nexperiments on five kinds of rocks, and the results demonstrate that RockGPT\ncan produce different kinds of rocks with the same model, and the reconstructed\nsamples can successfully meet certain specified porosities. In a broader sense,\nthrough leveraging the proposed conditioning scheme, RockGPT constitutes an\neffective way to build a general model to produce multiple kinds of rocks\nsimultaneously that also satisfy user-defined properties.",
          "link": "http://arxiv.org/abs/2108.03132",
          "publishedOn": "2021-08-09T00:49:28.677Z",
          "wordCount": 702,
          "title": "RockGPT: Reconstructing three-dimensional digital rocks from single two-dimensional slice from the perspective of video generation. (arXiv:2108.03132v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.11777",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Elbir_A/0/1/0/all/0/1\">Ahmet M. Elbir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Papazafeiropoulos_A/0/1/0/all/0/1\">Anastasios K. Papazafeiropoulos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chatzinotas_S/0/1/0/all/0/1\">Symeon Chatzinotas</a>",
          "description": "Model-free techniques, such as machine learning (ML), have recently attracted\nmuch interest towards the physical layer design, e.g., symbol detection,\nchannel estimation, and beamforming. Most of these ML techniques employ\ncentralized learning (CL) schemes and assume the availability of datasets at a\nparameter server (PS), demanding the transmission of data from edge devices,\nsuch as mobile phones, to the PS. Exploiting the data generated at the edge,\nfederated learning (FL) has been proposed recently as a distributed learning\nscheme, in which each device computes the model parameters and sends them to\nthe PS for model aggregation while the datasets are kept intact at the edge.\nThus, FL is more communication-efficient and privacy-preserving than CL and\napplicable to the wireless communication scenarios, wherein the data are\ngenerated at the edge devices. This article presents the recent advances in\nFL-based training for physical layer design problems. Compared to CL, the\neffectiveness of FL is presented in terms of communication overhead with a\nslight performance loss in the learning accuracy. The design challenges, such\nas model, data, and hardware complexity, are also discussed in detail along\nwith possible solutions.",
          "link": "http://arxiv.org/abs/2102.11777",
          "publishedOn": "2021-08-09T00:49:28.669Z",
          "wordCount": 651,
          "title": "Federated Learning for Physical Layer Design. (arXiv:2102.11777v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03120",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Joshi_G/0/1/0/all/0/1\">Girish Joshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhary_G/0/1/0/all/0/1\">Girish Chowdhary</a>",
          "description": "In this paper, we present a Stochastic Deep Neural Network-based Model\nReference Adaptive Control. Building on our work \"Deep Model Reference Adaptive\nControl\", we extend the controller capability by using Bayesian deep neural\nnetworks (DNN) to represent uncertainties and model non-linearities. Stochastic\nDeep Model Reference Adaptive Control uses a Lyapunov-based method to adapt the\noutput-layer weights of the DNN model in real-time, while a data-driven\nsupervised learning algorithm is used to update the inner-layers parameters.\nThis asynchronous network update ensures boundedness and guaranteed tracking\nperformance with a learning-based real-time feedback controller. A Bayesian\napproach to DNN learning helped avoid over-fitting the data and provide\nconfidence intervals over the predictions. The controller's stochastic nature\nalso ensured \"Induced Persistency of excitation,\" leading to convergence of the\noverall system signal.",
          "link": "http://arxiv.org/abs/2108.03120",
          "publishedOn": "2021-08-09T00:49:28.663Z",
          "wordCount": 572,
          "title": "Stochastic Deep Model Reference Adaptive Control. (arXiv:2108.03120v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2005.00478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Putatunda_S/0/1/0/all/0/1\">Sayan Putatunda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ubrangala_D/0/1/0/all/0/1\">Dayananda Ubrangala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rama_K/0/1/0/all/0/1\">Kiran Rama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondapalli_R/0/1/0/all/0/1\">Ravi Kondapalli</a>",
          "description": "In recent years, the concept of automated machine learning has become very\npopular. Automated Machine Learning (AutoML) mainly refers to the automated\nmethods for model selection and hyper-parameter optimization of various\nalgorithms such as random forests, gradient boosting, neural networks, etc. In\nthis paper, we introduce a new package i.e. DriveML for automated machine\nlearning. DriveML helps in implementing some of the pillars of an automated\nmachine learning pipeline such as automated data preparation, feature\nengineering, model building and model explanation by running the function\ninstead of writing lengthy R codes. The DriveML package is available in CRAN.\nWe compare the DriveML package with other relevant packages in CRAN/Github and\nfind that DriveML performs the best across different parameters. We also\nprovide an illustration by applying the DriveML package with default\nconfiguration on a real world dataset. Overall, the main benefits of DriveML\nare in development time savings, reduce developer's errors, optimal tuning of\nmachine learning models and reproducibility.",
          "link": "http://arxiv.org/abs/2005.00478",
          "publishedOn": "2021-08-09T00:49:28.656Z",
          "wordCount": 663,
          "title": "DriveML: An R Package for Driverless Machine Learning. (arXiv:2005.00478v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1\">Chaitanya Devaguptapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Devansh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_G/0/1/0/all/0/1\">Gaurav Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>",
          "description": "Adversarial robustness of deep learning models has gained much traction in\nthe last few years. Various attacks and defenses are proposed to improve the\nadversarial robustness of modern-day deep learning architectures. While all\nthese approaches help improve the robustness, one promising direction for\nimproving adversarial robustness is un-explored, i.e., the complex topology of\nthe neural network architecture. In this work, we answer the following\nquestion: \"Can the complex topology of a neural network give adversarial\nrobustness without any form of adversarial training?\" empirically by\nexperimenting with different hand-crafted and NAS based architectures. Our\nfindings show that, for small-scale attacks, NAS-based architectures are more\nrobust for small-scale datasets and simple tasks than hand-crafted\narchitectures. However, as the dataset's size or the task's complexity\nincrease, hand-crafted architectures are more robust than NAS-based\narchitectures. We perform the first large scale study to understand adversarial\nrobustness purely from an architectural perspective. Our results show that\nrandom sampling in the search space of DARTS (a popular NAS method) with simple\nensembling can improve the robustness to PGD attack by nearly ~12\\%. We show\nthat NAS, which is popular for SoTA accuracy, can provide adversarial accuracy\nas a free add-on without any form of adversarial training. Our results show\nthat leveraging the power of neural network topology with methods like\nensembles can be an excellent way to achieve adversarial robustness without any\nform of adversarial training. We also introduce a metric that can be used to\ncalculate the trade-off between clean accuracy and adversarial robustness.",
          "link": "http://arxiv.org/abs/2007.08428",
          "publishedOn": "2021-08-09T00:49:28.648Z",
          "wordCount": 753,
          "title": "On Adversarial Robustness: A Neural Architecture Search perspective. (arXiv:2007.08428v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khetarpal_K/0/1/0/all/0/1\">Khimya Khetarpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1\">Zafarali Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comanici_G/0/1/0/all/0/1\">Gheorghe Comanici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>",
          "description": "Humans and animals have the ability to reason and make predictions about\ndifferent courses of action at many time scales. In reinforcement learning,\noption models (Sutton, Precup \\& Singh, 1999; Precup, 2000) provide the\nframework for this kind of temporally abstract prediction and reasoning.\nNatural intelligent agents are also able to focus their attention on courses of\naction that are relevant or feasible in a given situation, sometimes termed\naffordable actions. In this paper, we define a notion of affordances for\noptions, and develop temporally abstract partial option models, that take into\naccount the fact that an option might be affordable only in certain situations.\nWe analyze the trade-offs between estimation and approximation error in\nplanning and learning when using such models, and identify some interesting\nspecial cases. Additionally, we demonstrate empirically the potential impact of\npartial option models on the efficiency of planning.",
          "link": "http://arxiv.org/abs/2108.03213",
          "publishedOn": "2021-08-09T00:49:28.628Z",
          "wordCount": 580,
          "title": "Temporally Abstract Partial Models. (arXiv:2108.03213v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.03809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumais_S/0/1/0/all/0/1\">Susan Dumais</a>",
          "description": "Leveraging weak or noisy supervision for building effective machine learning\nmodels has long been an important research problem. Its importance has further\nincreased recently due to the growing need for large-scale datasets to train\ndeep learning models. Weak or noisy supervision could originate from multiple\nsources including non-expert annotators or automatic labeling based on\nheuristics or user interaction signals. There is an extensive amount of\nprevious work focusing on leveraging noisy labels. Most notably, recent work\nhas shown impressive gains by using a meta-learned instance re-weighting\napproach where a meta-learning framework is used to assign instance weights to\nnoisy labels. In this paper, we extend this approach via posing the problem as\nlabel correction problem within a meta-learning framework. We view the label\ncorrection procedure as a meta-process and propose a new meta-learning based\nframework termed MLC (Meta Label Correction) for learning with noisy labels.\nSpecifically, a label correction network is adopted as a meta-model to produce\ncorrected labels for noisy labels while the main model is trained to leverage\nthe corrected labeled. Both models are jointly trained by solving a bi-level\noptimization problem. We run extensive experiments with different label noise\nlevels and types on both image recognition and text classification tasks. We\ncompare the reweighing and correction approaches showing that the correction\nframing addresses some of the limitation of reweighting. We also show that the\nproposed MLC approach achieves large improvements over previous methods in many\nsettings.",
          "link": "http://arxiv.org/abs/1911.03809",
          "publishedOn": "2021-08-09T00:49:28.622Z",
          "wordCount": 704,
          "title": "Meta Label Correction for Noisy Label Learning. (arXiv:1911.03809v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cayci_S/0/1/0/all/0/1\">Semih Cayci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satpathi_S/0/1/0/all/0/1\">Siddhartha Satpathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_N/0/1/0/all/0/1\">Niao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1\">R. Srikant</a>",
          "description": "In this paper, we study the dynamics of temporal difference learning with\nneural network-based value function approximation over a general state space,\nnamely, \\emph{Neural TD learning}. We consider two practically used algorithms,\nprojection-free and max-norm regularized Neural TD learning, and establish the\nfirst convergence bounds for these algorithms. An interesting observation from\nour results is that max-norm regularization can dramatically improve the\nperformance of TD learning algorithms, both in terms of sample complexity and\noverparameterization. In particular, we prove that max-norm regularization\nimproves state-of-the-art sample complexity and overparameterization bounds.\nThe results in this work rely on a novel Lyapunov drift analysis of the network\nparameters as a stopped and controlled random process.",
          "link": "http://arxiv.org/abs/2103.01391",
          "publishedOn": "2021-08-09T00:49:28.615Z",
          "wordCount": 606,
          "title": "Sample Complexity and Overparameterization Bounds for Temporal Difference Learning with Neural Network Approximation. (arXiv:2103.01391v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>",
          "description": "This survey is meant to provide an introduction to linear models and the\ntheories behind them. Our goal is to give a rigorous introduction to the\nreaders with prior exposure to ordinary least squares. In machine learning, the\noutput is usually a nonlinear function of the input. Deep learning even aims to\nfind a nonlinear dependence with many layers which require a large amount of\ncomputation. However, most of these algorithms build upon simple linear models.\nWe then describe linear models from different views and find the properties and\ntheories behind the models. The linear model is the main technique in\nregression problems and the primary tool for it is the least squares\napproximation which minimizes a sum of squared errors. This is a natural choice\nwhen we're interested in finding the regression function which minimizes the\ncorresponding expected squared error. This survey is primarily a summary of\npurpose, significance of important theories behind linear models, e.g.,\ndistribution theory, minimum variance estimator. We first describe ordinary\nleast squares from three different points of view upon which we disturb the\nmodel with random noise and Gaussian noise. By Gaussian noise, the model gives\nrise to the likelihood so that we introduce a maximum likelihood estimator. It\nalso develops some distribution theories via this Gaussian disturbance. The\ndistribution theory of least squares will help us answer various questions and\nintroduce related applications. We then prove least squares is the best\nunbiased linear model in the sense of mean squared error and most importantly,\nit actually approaches the theoretical limit. We end up with linear models with\nthe Bayesian approach and beyond.",
          "link": "http://arxiv.org/abs/2105.04240",
          "publishedOn": "2021-08-09T00:49:28.608Z",
          "wordCount": 728,
          "title": "A rigorous introduction for linear models. (arXiv:2105.04240v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1\">Shagufta Henna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reji_A/0/1/0/all/0/1\">Aparna Reji</a>",
          "description": "Covid-19 detection at an early stage can aid in an effective treatment and\nisolation plan to prevent its spread. Recently, transfer learning has been used\nfor Covid-19 detection using X-ray, ultrasound, and CT scans. One of the major\nlimitations inherent to these proposed methods is limited labeled dataset size\nthat affects the reliability of Covid-19 diagnosis and disease progression. In\nthis work, we demonstrate that how we can augment limited X-ray images data by\nusing Contrast limited adaptive histogram equalization (CLAHE) to train the\nlast layer of the pre-trained deep learning models to mitigate the bias of\ntransfer learning for Covid-19 detection. We transfer learned various\npre-trained deep learning models including AlexNet, ZFNet, VGG-16, ResNet-18,\nand GoogLeNet, and fine-tune the last layer by using CLAHE-augmented dataset.\nThe experiment results reveal that the CLAHE-based augmentation to various\npre-trained deep learning models significantly improves the model efficiency.\nThe pre-trained VCG-16 model with CLAHEbased augmented images achieves a\nsensitivity of 95% using 15 epochs. AlexNet works show good sensitivity when\ntrained on non-augmented data. Other models demonstrate a value of less than\n60% when trained on non-augmented data. Our results reveal that the sample bias\ncan negatively impact the performance of transfer learning which is\nsignificantly improved by using CLAHE-based augmentation.",
          "link": "http://arxiv.org/abs/2108.02870",
          "publishedOn": "2021-08-09T00:49:28.600Z",
          "wordCount": 692,
          "title": "A Data Augmented Approach to Transfer Learning for Covid-19 Detection. (arXiv:2108.02870v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+HaoChen_J/0/1/0/all/0/1\">Jeff Z. HaoChen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Colin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>",
          "description": "Recent works in self-supervised learning have advanced the state-of-the-art\nby relying on the contrastive learning paradigm, which learns representations\nby pushing positive pairs, or similar examples from the same class, closer\ntogether while keeping negative pairs far apart. Despite the empirical\nsuccesses, theoretical foundations are limited -- prior analyses assume\nconditional independence of the positive pairs given the same class label, but\nrecent empirical applications use heavily correlated positive pairs (i.e., data\naugmentations of the same image). Our work analyzes contrastive learning\nwithout assuming conditional independence of positive pairs using a novel\nconcept of the augmentation graph on data. Edges in this graph connect\naugmentations of the same data, and ground-truth classes naturally form\nconnected sub-graphs. We propose a loss that performs spectral decomposition on\nthe population augmentation graph and can be succinctly written as a\ncontrastive learning objective on neural net representations. Minimizing this\nobjective leads to features with provable accuracy guarantees under linear\nprobe evaluation. By standard generalization bounds, these accuracy guarantees\nalso hold when minimizing the training contrastive loss. Empirically, the\nfeatures learned by our objective can match or outperform several strong\nbaselines on benchmark vision datasets. In all, this work provides the first\nprovable analysis for contrastive learning where guarantees for linear probe\nevaluation can apply to realistic empirical settings.",
          "link": "http://arxiv.org/abs/2106.04156",
          "publishedOn": "2021-08-09T00:49:28.592Z",
          "wordCount": 710,
          "title": "Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss. (arXiv:2106.04156v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05476",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1\">Kun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zengfeng Huang</a>",
          "description": "Node embedding learns a low-dimensional representation for each node in the\ngraph. Recent progress on node embedding shows that proximity matrix\nfactorization methods gain superb performance and scale to large graphs with\nmillions of nodes. Existing approaches first define a proximity matrix and then\nlearn the embeddings that fit the proximity by matrix factorization. Most\nexisting matrix factorization methods adopt the same proximity for different\ntasks, while it is observed that different tasks and datasets may require\ndifferent proximity, limiting their representation power.\n\nMotivated by this, we propose {\\em Lemane}, a framework with trainable\nproximity measures, which can be learned to best suit the datasets and tasks at\nhand automatically. Our method is end-to-end, which incorporates differentiable\nSVD in the pipeline so that the parameters can be trained via backpropagation.\nHowever, this learning process is still expensive on large graphs. To improve\nthe scalability, we train proximity measures only on carefully subsampled\ngraphs, and then apply standard proximity matrix factorization on the original\ngraph using the learned proximity. Note that, computing the learned proximities\nfor each pair is still expensive for large graphs, and existing techniques for\ncomputing proximities are not applicable to the learned proximities. Thus, we\npresent generalized push techniques to make our solution scalable to large\ngraphs with millions of nodes. Extensive experiments show that our proposed\nsolution outperforms existing solutions on both link prediction and node\nclassification tasks on almost all datasets.",
          "link": "http://arxiv.org/abs/2106.05476",
          "publishedOn": "2021-08-09T00:49:28.574Z",
          "wordCount": 757,
          "title": "Learning Based Proximity Matrix Factorization for Node Embedding. (arXiv:2106.05476v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02504",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Meunier_D/0/1/0/all/0/1\">Dimitri Meunier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Alquier_P/0/1/0/all/0/1\">Pierre Alquier</a>",
          "description": "Online learning methods, like the online gradient algorithm (OGA) and\nexponentially weighted aggregation (EWA), often depend on tuning parameters\nthat are difficult to set in practice. We consider an online meta-learning\nscenario, and we propose a meta-strategy to learn these parameters from past\ntasks. Our strategy is based on the minimization of a regret bound. It allows\nto learn the initialization and the step size in OGA with guarantees. It also\nallows to learn the prior or the learning rate in EWA. We provide a regret\nanalysis of the strategy. It allows to identify settings where meta-learning\nindeed improves on learning each task in isolation.",
          "link": "http://arxiv.org/abs/2102.02504",
          "publishedOn": "2021-08-09T00:49:28.566Z",
          "wordCount": 568,
          "title": "Meta-strategy for Learning Tuning Parameters with Guarantees. (arXiv:2102.02504v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohtasib_A/0/1/0/all/0/1\">Abdalkarim Mohtasib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1\">Gerhard Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuayahuitl_H/0/1/0/all/0/1\">Heriberto Cuayahuitl</a>",
          "description": "Deep Reinforcement Learning (DRL) is a promising approach for teaching robots\nnew behaviour. However, one of its main limitations is the need for carefully\nhand-coded reward signals by an expert. We argue that it is crucial to automate\nthe reward learning process so that new skills can be taught to robots by their\nusers. To address such automation, we consider task success classifiers using\nvisual observations to estimate the rewards in terms of task success. In this\nwork, we study the performance of multiple state-of-the-art deep reinforcement\nlearning algorithms under different types of reward: Dense, Sparse, Visual\nDense, and Visual Sparse rewards. Our experiments in various simulation tasks\n(Pendulum, Reacher, Pusher, and Fetch Reach) show that while DRL agents can\nlearn successful behaviours using visual rewards when the goal targets are\ndistinguishable, their performance may decrease if the task goal is not clearly\nvisible. Our results also show that visual dense rewards are more successful\nthan visual sparse rewards and that there is no single best algorithm for all\ntasks.",
          "link": "http://arxiv.org/abs/2108.03222",
          "publishedOn": "2021-08-09T00:49:28.560Z",
          "wordCount": 607,
          "title": "A Study on Dense and Sparse (Visual) Rewards in Robot Policy Learning. (arXiv:2108.03222v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demir_U/0/1/0/all/0/1\">Ufuk Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozer_A/0/1/0/all/0/1\">Atahan Ozer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahin_Y/0/1/0/all/0/1\">Yusuf H. Sahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_G/0/1/0/all/0/1\">Gozde Unal</a>",
          "description": "In recent years, deep learning based methods have shown success in essential\nmedical image analysis tasks such as segmentation. Post-processing and refining\nthe results of segmentation is a common practice to decrease the\nmisclassifications originating from the segmentation network. In addition to\nwidely used methods like Conditional Random Fields (CRFs) which focus on the\nstructure of the segmented volume/area, a graph-based recent approach makes use\nof certain and uncertain points in a graph and refines the segmentation\naccording to a small graph convolutional network (GCN). However, there are two\ndrawbacks of the approach: most of the edges in the graph are assigned randomly\nand the GCN is trained independently from the segmentation network. To address\nthese issues, we define a new neighbor-selection mechanism according to feature\ndistances and combine the two networks in the training procedure. According to\nthe experimental results on pancreas segmentation from Computed Tomography (CT)\nimages, we demonstrate improvement in the quantitative measures. Also,\nexamining the dynamic neighbors created by our method, edges between\nsemantically similar image parts are observed. The proposed method also shows\nqualitative enhancements in the segmentation maps, as demonstrated in the\nvisual results.",
          "link": "http://arxiv.org/abs/2108.03117",
          "publishedOn": "2021-08-09T00:49:28.553Z",
          "wordCount": 651,
          "title": "Uncertainty-Based Dynamic Graph Neighborhoods For Medical Segmentation. (arXiv:2108.03117v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1\">Antonio Pepe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1\">Christina Gsaxner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fen-hua Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1\">Jens Kleesiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F. Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1\">Jan Egger</a>",
          "description": "The aortic vessel tree is composed of the aorta and its branching arteries,\nand plays a key role in supplying the whole body with blood. Aortic diseases,\nlike aneurysms or dissections, can lead to an aortic rupture, whose treatment\nwith open surgery is highly risky. Therefore, patients commonly undergo drug\ntreatment under constant monitoring, which requires regular inspections of the\nvessels through imaging. The standard imaging modality for diagnosis and\nmonitoring is computed tomography (CT), which can provide a detailed picture of\nthe aorta and its branching vessels if combined with a contrast agent,\nresulting in a CT angiography (CTA). Optimally, the whole aortic vessel tree\ngeometry from consecutive CTAs, are overlaid and compared. This allows to not\nonly detect changes in the aorta, but also more peripheral vessel tree changes,\ncaused by the primary pathology or newly developed. When performed manually,\nthis reconstruction requires slice by slice contouring, which could easily take\na whole day for a single aortic vessel tree and, hence, is not feasible in\nclinical practice. Automatic or semi-automatic vessel tree segmentation\nalgorithms, on the other hand, can complete this task in a fraction of the\nmanual execution time and run in parallel to the clinical routine of the\nclinicians. In this paper, we systematically review computing techniques for\nthe automatic and semi-automatic segmentation of the aortic vessel tree. The\nreview concludes with an in-depth discussion on how close these\nstate-of-the-art approaches are to an application in clinical practice and how\nactive this research field is, taking into account the number of publications,\ndatasets and challenges.",
          "link": "http://arxiv.org/abs/2108.02998",
          "publishedOn": "2021-08-09T00:49:28.536Z",
          "wordCount": 744,
          "title": "AI-based Aortic Vessel Tree Segmentation for Cardiovascular Diseases Treatment: Status Quo. (arXiv:2108.02998v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08991",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Banchi_L/0/1/0/all/0/1\">Leonardo Banchi</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pereira_J/0/1/0/all/0/1\">Jason Pereira</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pirandola_S/0/1/0/all/0/1\">Stefano Pirandola</a>",
          "description": "Quantum classification and hypothesis testing are two tightly related\nsubjects, the main difference being that the former is data driven: how to\nassign to quantum states $\\rho(x)$ the corresponding class $c$ (or hypothesis)\nis learnt from examples during training, where $x$ can be either tunable\nexperimental parameters or classical data \"embedded\" into quantum states. Does\nthe model generalize? This is the main question in any data-driven strategy,\nnamely the ability to predict the correct class even of previously unseen\nstates. Here we establish a link between quantum machine learning\nclassification and quantum hypothesis testing (state and channel\ndiscrimination) and then show that the accuracy and generalization capability\nof quantum classifiers depend on the (R\\'enyi) mutual informations $I(C{:}Q)$\nand $I_2(X{:}Q)$ between the quantum state space $Q$ and the classical\nparameter space $X$ or class space $C$. Based on the above characterization, we\nthen show how different properties of $Q$ affect classification accuracy and\ngeneralization, such as the dimension of the Hilbert space, the amount of\nnoise, and the amount of neglected information from $X$ via, e.g., pooling\nlayers. Moreover, we introduce a quantum version of the Information Bottleneck\nprinciple that allows us to explore the various tradeoffs between accuracy and\ngeneralization. Finally, in order to check our theoretical predictions, we\nstudy the classification of the quantum phases of an Ising spin chain, and we\npropose the Variational Quantum Information Bottleneck (VQIB) method to\noptimize quantum embeddings of classical data to favor generalization.",
          "link": "http://arxiv.org/abs/2102.08991",
          "publishedOn": "2021-08-09T00:49:28.528Z",
          "wordCount": 722,
          "title": "Generalization in Quantum Machine Learning: a Quantum Information Perspective. (arXiv:2102.08991v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08637",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Rajpal_S/0/1/0/all/0/1\">Sheetal Rajpal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_M/0/1/0/all/0/1\">Manoj Agarwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpal_A/0/1/0/all/0/1\">Ankit Rajpal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lakhyani_N/0/1/0/all/0/1\">Navin Lakhyani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saggar_A/0/1/0/all/0/1\">Arpita Saggar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_N/0/1/0/all/0/1\">Naveen Kumar</a>",
          "description": "Coronaviruses constitute a family of viruses that gives rise to respiratory\ndiseases. As COVID-19 is highly contagious, early diagnosis of COVID-19 is\ncrucial for an effective treatment strategy. However, the RT-PCR test which is\nconsidered to be a gold standard in the diagnosis of COVID-19 suffers from a\nhigh false-negative rate. Chest X-ray (CXR) image analysis has emerged as a\nfeasible and effective diagnostic technique towards this objective. In this\nwork, we propose the COVID-19 classification problem as a three-class\nclassification problem to distinguish between COVID-19, normal, and pneumonia\nclasses. We propose a three-stage framework, named COV-ELM. Stage one deals\nwith preprocessing and transformation while stage two deals with feature\nextraction. These extracted features are passed as an input to the ELM at the\nthird stage, resulting in the identification of COVID-19. The choice of ELM in\nthis work has been motivated by its faster convergence, better generalization\ncapability, and shorter training time in comparison to the conventional\ngradient-based learning algorithms. As bigger and diverse datasets become\navailable, ELM can be quickly retrained as compared to its gradient-based\ncompetitor models. The proposed model achieved a macro average F1-score of 0.95\nand the overall sensitivity of ${0.94 \\pm 0.02} at a 95% confidence interval.\nWhen compared to state-of-the-art machine learning algorithms, the COV-ELM is\nfound to outperform its competitors in this three-class classification\nscenario. Further, LIME has been integrated with the proposed COV-ELM model to\ngenerate annotated CXR images. The annotations are based on the superpixels\nthat have contributed to distinguish between the different classes. It was\nobserved that the superpixels correspond to the regions of the human lungs that\nare clinically observed in COVID-19 and Pneumonia cases.",
          "link": "http://arxiv.org/abs/2007.08637",
          "publishedOn": "2021-08-09T00:49:28.521Z",
          "wordCount": 831,
          "title": "COV-ELM classifier: An Extreme Learning Machine based identification of COVID-19 using Chest X-Ray Images. (arXiv:2007.08637v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ranjbar_A/0/1/0/all/0/1\">Alireza Ranjbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1\">Ngo Anh Vien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1\">Hanna Ziesche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boedecker_J/0/1/0/all/0/1\">Joschka Boedecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1\">Gerhard Neumann</a>",
          "description": "While classic control theory offers state of the art solutions in many\nproblem scenarios, it is often desired to improve beyond the structure of such\nsolutions and surpass their limitations. To this end, residual policy learning\n(RPL) offers a formulation to improve existing controllers with reinforcement\nlearning (RL) by learning an additive \"residual\" to the output of a given\ncontroller. However, the applicability of such an approach highly depends on\nthe structure of the controller. Often, internal feedback signals of the\ncontroller limit an RL algorithm to adequately change the policy and, hence,\nlearn the task. We propose a new formulation that addresses these limitations\nby also modifying the feedback signals to the controller with an RL policy and\nshow superior performance of our approach on a contact-rich peg-insertion task\nunder position and orientation uncertainty. In addition, we use a recent\nCartesian impedance control architecture as the control framework which can be\navailable to us as a black-box while assuming no knowledge about its\ninput/output structure, and show the difficulties of standard RPL. Furthermore,\nwe introduce an adaptive curriculum for the given task to gradually increase\nthe task difficulty in terms of position and orientation uncertainty. A video\nshowing the results can be found at https://youtu.be/SAZm_Krze7U .",
          "link": "http://arxiv.org/abs/2106.04306",
          "publishedOn": "2021-08-09T00:49:28.514Z",
          "wordCount": 685,
          "title": "Residual Feedback Learning for Contact-Rich Manipulation Tasks with Uncertainty. (arXiv:2106.04306v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trott_A/0/1/0/all/0/1\">Alexander Trott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1\">Sunil Srinivasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wal_D/0/1/0/all/0/1\">Douwe van der Wal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haneuse_S/0/1/0/all/0/1\">Sebastien Haneuse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Stephan Zheng</a>",
          "description": "Optimizing economic and public policy is critical to address socioeconomic\nissues and trade-offs, e.g., improving equality, productivity, or wellness, and\nposes a complex mechanism design problem. A policy designer needs to consider\nmultiple objectives, policy levers, and behavioral responses from strategic\nactors who optimize for their individual objectives. Moreover, real-world\npolicies should be explainable and robust to simulation-to-reality gaps, e.g.,\ndue to calibration issues. Existing approaches are often limited to a narrow\nset of policy levers or objectives that are hard to measure, do not yield\nexplicit optimal policies, or do not consider strategic behavior, for example.\nHence, it remains challenging to optimize policy in real-world scenarios. Here\nwe show that the AI Economist framework enables effective, flexible, and\ninterpretable policy design using two-level reinforcement learning (RL) and\ndata-driven simulations. We validate our framework on optimizing the stringency\nof US state policies and Federal subsidies during a pandemic, e.g., COVID-19,\nusing a simulation fitted to real data. We find that log-linear policies\ntrained using RL significantly improve social welfare, based on both public\nhealth and economic outcomes, compared to past outcomes. Their behavior can be\nexplained, e.g., well-performing policies respond strongly to changes in\nrecovery and vaccination rates. They are also robust to calibration errors,\ne.g., infection rates that are over or underestimated. As of yet, real-world\npolicymaking has not seen adoption of machine learning methods at large,\nincluding RL and AI-driven simulations. Our results show the potential of AI to\nguide policy design and improve social welfare amidst the complexity of the\nreal world.",
          "link": "http://arxiv.org/abs/2108.02904",
          "publishedOn": "2021-08-09T00:49:28.507Z",
          "wordCount": 775,
          "title": "Building a Foundation for Data-Driven, Interpretable, and Robust Policy Design using the AI Economist. (arXiv:2108.02904v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Habib_M/0/1/0/all/0/1\">Mohammad Kasra Habib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1\">Stefan Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graziotin_D/0/1/0/all/0/1\">Daniel Graziotin</a>",
          "description": "Requirements Engineering (RE) is the initial step towards building a software\nsystem. The success or failure of a software project is firmly tied to this\nphase, based on communication among stakeholders using natural language. The\nproblem with natural language is that it can easily lead to different\nunderstandings if it is not expressed precisely by the stakeholders involved,\nwhich results in building a product different from the expected one. Previous\nwork proposed to enhance the quality of the software requirements detecting\nlanguage errors based on ISO 29148 requirements language criteria. The existing\nsolutions apply classical Natural Language Processing (NLP) to detect them. NLP\nhas some limitations, such as domain dependability which results in poor\ngeneralization capability. Therefore, this work aims to improve the previous\nwork by creating a manually labeled dataset and using ensemble learning, Deep\nLearning (DL), and techniques such as word embeddings and transfer learning to\novercome the generalization problem that is tied with classical NLP and improve\nprecision and recall metrics using a manually labeled dataset. The current\nfindings show that the dataset is unbalanced and which class examples should be\nadded more. It is tempting to train algorithms even if the dataset is not\nconsiderably representative. Whence, the results show that models are\noverfitting; in Machine Learning this issue is solved by adding more instances\nto the dataset, improving label quality, removing noise, and reducing the\nlearning algorithms complexity, which is planned for this research.",
          "link": "http://arxiv.org/abs/2108.03087",
          "publishedOn": "2021-08-09T00:49:28.489Z",
          "wordCount": 695,
          "title": "Detecting Requirements Smells With Deep Learning: Experiences, Challenges and Future Work. (arXiv:2108.03087v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2012.06341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1\">Ant&#xf4;nio H. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendriks_J/0/1/0/all/0/1\">Johannes N. Hendriks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wills_A/0/1/0/all/0/1\">Adrian G. Wills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1\">Thomas B. Sch&#xf6;n</a>",
          "description": "System identification aims to build models of dynamical systems from data.\nTraditionally, choosing the model requires the designer to balance between two\ngoals of conflicting nature; the model must be rich enough to capture the\nsystem dynamics, but not so flexible that it learns spurious random effects\nfrom the dataset. It is typically observed that the model validation\nperformance follows a U-shaped curve as the model complexity increases. Recent\ndevelopments in machine learning and statistics, however, have observed\nsituations where a \"double-descent\" curve subsumes this U-shaped\nmodel-performance curve. With a second decrease in performance occurring beyond\nthe point where the model has reached the capacity of interpolating - i.e.,\n(near) perfectly fitting - the training data. To the best of our knowledge,\nsuch phenomena have not been studied within the context of dynamic systems. The\npresent paper aims to answer the question: \"Can such a phenomenon also be\nobserved when estimating parameters of dynamic systems?\" We show that the\nanswer is yes, verifying such behavior experimentally both for artificially\ngenerated and real-world datasets.",
          "link": "http://arxiv.org/abs/2012.06341",
          "publishedOn": "2021-08-09T00:49:28.462Z",
          "wordCount": 671,
          "title": "Beyond Occam's Razor in System Identification: Double-Descent when Modeling Dynamics. (arXiv:2012.06341v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kudisthalert_W/0/1/0/all/0/1\">Wasu Kudisthalert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasupa_K/0/1/0/all/0/1\">Kitsuchart Pasupa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>",
          "description": "Extreme Learning Machine is a powerful classification method very competitive\nexisting classification methods. It is extremely fast at training.\nNevertheless, it cannot perform face verification tasks properly because face\nverification tasks require comparison of facial images of two individuals at\nthe same time and decide whether the two faces identify the same person. The\nstructure of Extreme Leaning Machine was not designed to feed two input data\nstreams simultaneously, thus, in 2-input scenarios Extreme Learning Machine\nmethods are normally applied using concatenated inputs. However, this setup\nconsumes two times more computational resources and it is not optimized for\nrecognition tasks where learning a separable distance metric is critical. For\nthese reasons, we propose and develop a Siamese Extreme Learning Machine\n(SELM). SELM was designed to be fed with two data streams in parallel\nsimultaneously. It utilizes a dual-stream Siamese condition in the extra\nSiamese layer to transform the data before passing it along to the hidden\nlayer. Moreover, we propose a Gender-Ethnicity-Dependent triplet feature\nexclusively trained on a variety of specific demographic groups. This feature\nenables learning and extracting of useful facial features of each group.\nExperiments were conducted to evaluate and compare the performances of SELM,\nExtreme Learning Machine, and DCNN. The experimental results showed that the\nproposed feature was able to perform correct classification at 97.87% accuracy\nand 99.45% AUC. They also showed that using SELM in conjunction with the\nproposed feature provided 98.31% accuracy and 99.72% AUC. They outperformed the\nwell-known DCNN and Extreme Leaning Machine methods by a wide margin.",
          "link": "http://arxiv.org/abs/2108.03140",
          "publishedOn": "2021-08-09T00:49:28.455Z",
          "wordCount": 703,
          "title": "SELM: Siamese Extreme Learning Machine with Application to Face Biometrics. (arXiv:2108.03140v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02837",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Mianroodi_J/0/1/0/all/0/1\">Jaber Rezaei Mianroodi</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Rezaei_S/0/1/0/all/0/1\">Shahed Rezaei</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Siboni_N/0/1/0/all/0/1\">Nima H. Siboni</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Xu_B/0/1/0/all/0/1\">Bai-Xiang Xu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Raabe_D/0/1/0/all/0/1\">Dierk Raabe</a>",
          "description": "The elastic properties of materials derive from their electronic and atomic\nnature. However, simulating bulk materials fully at these scales is not\nfeasible, so that typically homogenized continuum descriptions are used\ninstead. A seamless and lossless transition of the constitutive description of\nthe elastic response of materials between these two scales has been so far\nelusive. Here we show how this problem can be overcome by using Artificial\nIntelligence (AI). A Convolutional Neural Network (CNN) model is trained, by\ntaking the structure image of a nanoporous material as input and the\ncorresponding elasticity tensor, calculated from Molecular Statics (MS), as\noutput. Trained with the atomistic data, the CNN model captures the size- and\npore-dependency of the material's elastic properties which, on the physics\nside, can stem from surfaces and non-local effects. Such effects are often\nignored in upscaling from atomistic to classical continuum theory. To\ndemonstrate the accuracy and the efficiency of the trained CNN model, a Finite\nElement Method (FEM) based result of an elastically deformed nanoporous beam\nequipped with the CNN as constitutive law is compared with that by a full\natomistic simulation. The good agreement between the atomistic simulations and\nthe FEM-AI combination for a system with size and surface effects establishes a\nnew lossless scale bridging approach to such problems. The trained CNN model\ndeviates from the atomistic result by 9.6\\% for porosity scenarios of up to\n90\\% but it is about 230 times faster than the MS calculation and does not\nrequire to change simulation methods between different scales. The efficiency\nof the CNN evaluation together with the preservation of important atomistic\neffects makes the trained model an effective atomistically-informed\nconstitutive model for macroscopic simulations of nanoporous materials and\nsolving of inverse problems.",
          "link": "http://arxiv.org/abs/2108.02837",
          "publishedOn": "2021-08-09T00:49:28.448Z",
          "wordCount": 734,
          "title": "Lossless Multi-Scale Constitutive Elastic Relations with Artificial Intelligence. (arXiv:2108.02837v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/1912.07942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zanella_Beguelin_S/0/1/0/all/0/1\">Santiago Zanella-B&#xe9;guelin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1\">Lukas Wutschitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1\">Shruti Tople</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruhle_V/0/1/0/all/0/1\">Victor R&#xfc;hle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paverd_A/0/1/0/all/0/1\">Andrew Paverd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohrimenko_O/0/1/0/all/0/1\">Olga Ohrimenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopf_B/0/1/0/all/0/1\">Boris K&#xf6;pf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1\">Marc Brockschmidt</a>",
          "description": "To continuously improve quality and reflect changes in data, machine learning\napplications have to regularly retrain and update their core models. We show\nthat a differential analysis of language model snapshots before and after an\nupdate can reveal a surprising amount of detailed information about changes in\nthe training data. We propose two new metrics---\\emph{differential score} and\n\\emph{differential rank}---for analyzing the leakage due to updates of natural\nlanguage models. We perform leakage analysis using these metrics across models\ntrained on several different datasets using different methods and\nconfigurations. We discuss the privacy implications of our findings, propose\nmitigation strategies and evaluate their effect.",
          "link": "http://arxiv.org/abs/1912.07942",
          "publishedOn": "2021-08-09T00:49:28.441Z",
          "wordCount": 609,
          "title": "Analyzing Information Leakage of Updates to Natural Language Models. (arXiv:1912.07942v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrevoets_J/0/1/0/all/0/1\">Jeroen Berrevoets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1\">Mihaela van der Schaar</a>",
          "description": "Conditional average treatment effects (CATEs) allow us to understand the\neffect heterogeneity across a large population of individuals. However, typical\nCATE learners assume all confounding variables are measured in order for the\nCATE to be identifiable. Often, this requirement is satisfied by simply\ncollecting many variables, at the expense of increased sample complexity for\nestimating CATEs. To combat this, we propose an energy-based model (EBM) that\nlearns a low-dimensional representation of the variables by employing a noise\ncontrastive loss function. With our EBM we introduce a preprocessing step that\nalleviates the dimensionality curse for any existing model and learner\ndeveloped for estimating CATE. We prove that our EBM keeps the representations\npartially identifiable up to some universal constant, as well as having\nuniversal approximation capability to avoid excessive information loss from\nmodel misspecification; these properties combined with our loss function,\nenable the representations to converge and keep the CATE estimation consistent.\nExperiments demonstrate the convergence of the representations, as well as show\nthat estimating CATEs on our representations performs better than on the\nvariables or the representations obtained via various benchmark dimensionality\nreduction methods.",
          "link": "http://arxiv.org/abs/2108.03039",
          "publishedOn": "2021-08-09T00:49:28.423Z",
          "wordCount": 630,
          "title": "Identifiable Energy-based Representations: An Application to Estimating Heterogeneous Causal Effects. (arXiv:2108.03039v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03002",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhang_H/0/1/0/all/0/1\">HongBing Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1\">XinYi Liu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fan_H/0/1/0/all/0/1\">HongTao Fan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1\">YaJing Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ye_Y/0/1/0/all/0/1\">Yinlin Ye</a>",
          "description": "More recently, an Approximate SVD Based on Qatar Riyal (QR) Decomposition\n(CSVD-QR) method for matrix complete problem is presented, whose computational\ncomplexity is $O(r^2(m+n))$, which is mainly due to that $r$ is far less than\n$\\min\\{m,n\\}$, where $r$ represents the largest number of singular values of\nmatrix $X$. What is particularly interesting is that after replacing the\nnuclear norm with the $L_{2,1}$ norm proposed based on this decomposition, as\nthe upper bound of the nuclear norm, when the intermediate matrix $D$ in its\ndecomposition is close to the diagonal matrix, it will converge to the nuclear\nnorm, and is exactly equal, when the $D$ matrix is equal to the diagonal\nmatrix, to the nuclear norm, which ingeniously avoids the calculation of the\nsingular value of the matrix. To the best of our knowledge, there is no\nliterature to generalize and apply it to solve tensor complete problems.\nInspired by this, in this paper we propose a class of tensor minimization model\nbased on $L_{2,1}$ norm and CSVD-QR method for the tensor complete problem,\nwhich is convex and therefore has a global minimum solution.",
          "link": "http://arxiv.org/abs/2108.03002",
          "publishedOn": "2021-08-09T00:49:28.416Z",
          "wordCount": 640,
          "title": "Fast and Accurate Low-Rank Tensor Completion Methods Based on QR Decomposition and $L_{2,1}$ Norm Minimization. (arXiv:2108.03002v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03131",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+MacLean_A/0/1/0/all/0/1\">Alexander MacLean</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abbasi_S/0/1/0/all/0/1\">Saad Abbasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebadi_A/0/1/0/all/0/1\">Ashkan Ebadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_A/0/1/0/all/0/1\">Andy Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pavlova_M/0/1/0/all/0/1\">Maya Pavlova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gunraj_H/0/1/0/all/0/1\">Hayden Gunraj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xi_P/0/1/0/all/0/1\">Pengcheng Xi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kohli_S/0/1/0/all/0/1\">Sonny Kohli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>",
          "description": "The Coronavirus Disease 2019 (COVID-19) pandemic has impacted many aspects of\nlife globally, and a critical factor in mitigating its effects is screening\nindividuals for infections, thereby allowing for both proper treatment for\nthose individuals as well as action to be taken to prevent further spread of\nthe virus. Point-of-care ultrasound (POCUS) imaging has been proposed as a\nscreening tool as it is a much cheaper and easier to apply imaging modality\nthan others that are traditionally used for pulmonary examinations, namely\nchest x-ray and computed tomography. Given the scarcity of expert radiologists\nfor interpreting POCUS examinations in many highly affected regions around the\nworld, low-cost deep learning-driven clinical decision support solutions can\nhave a large impact during the on-going pandemic. Motivated by this, we\nintroduce COVID-Net US, a highly efficient, self-attention deep convolutional\nneural network design tailored for COVID-19 screening from lung POCUS images.\nExperimental results show that the proposed COVID-Net US can achieve an AUC of\nover 0.98 while achieving 353X lower architectural complexity, 62X lower\ncomputational complexity, and 14.3X faster inference times on a Raspberry Pi.\nClinical validation was also conducted, where select cases were reviewed and\nreported on by a practicing clinician (20 years of clinical practice)\nspecializing in intensive care (ICU) and 15 years of expertise in POCUS\ninterpretation. To advocate affordable healthcare and artificial intelligence\nfor resource-constrained environments, we have made COVID-Net US open source\nand publicly available as part of the COVID-Net open source initiative.",
          "link": "http://arxiv.org/abs/2108.03131",
          "publishedOn": "2021-08-09T00:49:28.409Z",
          "wordCount": 781,
          "title": "COVID-Net US: A Tailored, Highly Efficient, Self-Attention Deep Convolutional Neural Network Design for Detection of COVID-19 Patient Cases from Point-of-care Ultrasound Imaging. (arXiv:2108.03131v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02838",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Karatas_T/0/1/0/all/0/1\">Tugce Karatas</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Hirsa_A/0/1/0/all/0/1\">Ali Hirsa</a>",
          "description": "Market indicators such as CPI and GDP have been widely used over decades to\nidentify the stage of business cycles and also investment attractiveness of\nsectors given market conditions. In this paper, we propose a two-stage\nmethodology that consists of predicting ETF prices for each sector using market\nindicators and ranking sectors based on their predicted rate of returns. We\ninitially start with choosing sector specific macroeconomic indicators and\nimplement Recursive Feature Elimination algorithm to select the most important\nfeatures for each sector. Using our prediction tool, we implement different\nRecurrent Neural Networks models to predict the future ETF prices for each\nsector. We then rank the sectors based on their predicted rate of returns. We\nselect the best performing model by evaluating the annualized return,\nannualized Sharpe ratio, and Calmar ratio of the portfolios that includes the\ntop four ranked sectors chosen by the model. We also test the robustness of the\nmodel performance with respect to lookback windows and look ahead windows. Our\nempirical results show that our methodology beats the equally weighted\nportfolio performance even in the long run. We also find that Echo State\nNetworks exhibits an outstanding performance compared to other models yet it is\nfaster to implement compared to other RNN models.",
          "link": "http://arxiv.org/abs/2108.02838",
          "publishedOn": "2021-08-09T00:49:28.402Z",
          "wordCount": 647,
          "title": "Two-Stage Sector Rotation Methodology Using Machine Learning and Deep Learning Techniques. (arXiv:2108.02838v1 [q-fin.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jarl_S/0/1/0/all/0/1\">Sanna Jarl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahrovani_S/0/1/0/all/0/1\">Sadegh Rahrovani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chehreghani_M/0/1/0/all/0/1\">Morteza Haghir Chehreghani</a>",
          "description": "Annotating the driving scenario trajectories based only on explicit rules\n(i.e., knowledge-based methods) can be subject to errors, such as false\npositive/negative classification of scenarios that lie on the border of two\nscenario classes, missing unknown scenario classes, and also anomalies. On the\nother side, verifying the labels by the annotators is not cost-efficient. For\nthis purpose, active learning (AL) could potentially improve the annotation\nprocedure by inclusion of an annotator/expert in an efficient way. In this\nstudy, we develop an active learning framework to annotate driving trajectory\ntime-series data. At the first step, we compute an embedding of the time-series\ntrajectories into a latent space in order to extract the temporal nature. For\nthis purpose, we study three different latent space representations:\nmultivariate Time Series t-Distributed Stochastic Neighbor Embedding (mTSNE),\nRecurrent Auto-Encoder (RAE) and Variational Recurrent Auto-Encoder (VRAE). We\nthen apply different active learning paradigms with different classification\nmodels to the embedded data. In particular, we study the two classifiers Neural\nNetwork (NN) and Support Vector Machines (SVM), with three active learning\nquery strategies (i.e., entropy, margin and random). In the following, we\nexplore the possibilities of the framework to discover unknown classes and\ndemonstrate how it can be used to identify the out-of-class trajectories.",
          "link": "http://arxiv.org/abs/2108.03217",
          "publishedOn": "2021-08-09T00:49:28.385Z",
          "wordCount": 640,
          "title": "Analysis of Driving Scenario Trajectories with Active Learning. (arXiv:2108.03217v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendoza_Cardenas_C/0/1/0/all/0/1\">Carlos H. Mendoza-Cardenas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockmeier_A/0/1/0/all/0/1\">Austin J. Brockmeier</a>",
          "description": "Seizure detection algorithms must discriminate abnormal neuronal activity\nassociated with a seizure from normal neural activity in a variety of\nconditions. Our approach is to seek spatiotemporal waveforms with distinct\nmorphology in electrocorticographic (ECoG) recordings of epileptic patients\nthat are indicative of a subsequent seizure (preictal) versus non-seizure\nsegments (interictal). To find these waveforms we apply a shift-invariant\nk-means algorithm to segments of spatially filtered signals to learn codebooks\nof prototypical waveforms. The frequency of the cluster labels from the\ncodebooks is then used to train a binary classifier that predicts the class\n(preictal or interictal) of a test ECoG segment. We use the Matthews\ncorrelation coefficient to evaluate the performance of the classifier and the\nquality of the codebooks. We found that our method finds recurrent\nnon-sinusoidal waveforms that could be used to build interpretable features for\nseizure prediction and that are also physiologically meaningful.",
          "link": "http://arxiv.org/abs/2108.03177",
          "publishedOn": "2021-08-09T00:49:28.378Z",
          "wordCount": 595,
          "title": "Shift-invariant waveform learning on epileptic ECoG. (arXiv:2108.03177v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yunxia Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+chen_S/0/1/0/all/0/1\">Songcan chen</a>",
          "description": "Euler k-means (EulerK) first maps data onto the unit hyper-sphere surface of\nequi-dimensional space via a complex mapping which induces the robust Euler\nkernel and next employs the popular $k$-means. Consequently, besides enjoying\nthe virtues of k-means such as simplicity and scalability to large data sets,\nEulerK is also robust to noises and outliers. Although so, the centroids\ncaptured by EulerK deviate from the unit hyper-sphere surface and thus in\nstrict distributional sense, actually are outliers. This weird phenomenon also\noccurs in some generic kernel clustering methods. Intuitively, using such\noutlier-like centroids should not be quite reasonable but it is still seldom\nattended. To eliminate the deviation, we propose two Rectified Euler k-means\nmethods, i.e., REK1 and REK2, which retain the merits of EulerK while acquire\nreal centroids residing on the mapped space to better characterize the data\nstructures. Specifically, REK1 rectifies EulerK by imposing the constraint on\nthe centroids while REK2 views each centroid as the mapped image from a\npre-image in the original space and optimizes these pre-images in Euler kernel\ninduced space. Undoubtedly, our proposed REKs can methodologically be extended\nto solve problems of such a category. Finally, the experiments validate the\neffectiveness of REK1 and REK2.",
          "link": "http://arxiv.org/abs/2108.03081",
          "publishedOn": "2021-08-09T00:49:28.371Z",
          "wordCount": 619,
          "title": "Rectified Euler k-means and Beyond. (arXiv:2108.03081v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiangqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Quan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhihua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tangzhi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojuan Ma</a>",
          "description": "Bank credit rating classifies banks into different levels based on publicly\ndisclosed and internal information, serving as an important input in financial\nrisk management. However, domain experts have a vague idea of exploring and\ncomparing different bank credit rating schemes. A loose connection between\nsubjective and quantitative analysis and difficulties in determining\nappropriate indicator weights obscure understanding of bank credit ratings.\nFurthermore, existing models fail to consider bank types by just applying a\nunified indicator weight set to all banks. We propose RatingVis to assist\nexperts in exploring and comparing different bank credit rating schemes. It\nsupports interactively inferring indicator weights for banks by involving\ndomain knowledge and considers bank types in the analysis loop. We conduct a\ncase study with real-world bank data to verify the efficacy of RatingVis.\nExpert feedback suggests that our approach helps them better understand\ndifferent rating schemes.",
          "link": "http://arxiv.org/abs/2108.03011",
          "publishedOn": "2021-08-09T00:49:28.361Z",
          "wordCount": 588,
          "title": "Inspecting the Process of Bank Credit Rating via Visual Analytics. (arXiv:2108.03011v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02850",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ojha_R/0/1/0/all/0/1\">Rupam Ojha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sekhar_C/0/1/0/all/0/1\">C Chandra Sekhar</a>",
          "description": "Automatic speech recognition is a difficult problem in pattern recognition\nbecause several sources of variability exist in the speech input like the\nchannel variations, the input might be clean or noisy, the speakers may have\ndifferent accent and variations in the gender, etc. As a result, domain\nadaptation is important in speech recognition where we train the model for a\nparticular source domain and test it on a different target domain. In this\npaper, we propose a technique to perform unsupervised gender-based domain\nadaptation in speech recognition using phonetic features. The experiments are\nperformed on the TIMIT dataset and there is a considerable decrease in the\nphoneme error rate using the proposed approach.",
          "link": "http://arxiv.org/abs/2108.02850",
          "publishedOn": "2021-08-09T00:49:28.354Z",
          "wordCount": 561,
          "title": "Unsupervised Domain Adaptation in Speech Recognition using Phonetic Features. (arXiv:2108.02850v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02889",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi Khac Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masaracchia_A/0/1/0/all/0/1\">Antonino Masaracchia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Do_Duy_T/0/1/0/all/0/1\">Tan Do-Duy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Poor_H/0/1/0/all/0/1\">H. Vincent Poor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duong_T/0/1/0/all/0/1\">Trung Q. Duong</a>",
          "description": "Many of the devices used in Internet-of-Things (IoT) applications are\nenergy-limited, and thus supplying energy while maintaining seamless\nconnectivity for IoT devices is of considerable importance. In this context, we\npropose a simultaneous wireless power transfer and information transmission\nscheme for IoT devices with support from reconfigurable intelligent surface\n(RIS)-aided unmanned aerial vehicle (UAV) communications. In particular, in a\nfirst phase, IoT devices harvest energy from the UAV through wireless power\ntransfer; and then in a second phase, the UAV collects data from the IoT\ndevices through information transmission. To characterise the agility of the\nUAV, we consider two scenarios: a hovering UAV and a mobile UAV. Aiming at\nmaximizing the total network sum-rate, we jointly optimize the trajectory of\nthe UAV, the energy harvesting scheduling of IoT devices, and the phaseshift\nmatrix of the RIS. We formulate a Markov decision process and propose two deep\nreinforcement learning algorithms to solve the optimization problem of\nmaximizing the total network sum-rate. Numerical results illustrate the\neffectiveness of the UAV's flying path optimization and the network's\nthroughput of our proposed techniques compared with other benchmark schemes.\nGiven the strict requirements of the RIS and UAV, the significant improvement\nin processing time and throughput performance demonstrates that our proposed\nscheme is well applicable for practical IoT applications.",
          "link": "http://arxiv.org/abs/2108.02889",
          "publishedOn": "2021-08-09T00:49:28.345Z",
          "wordCount": 687,
          "title": "RIS-assisted UAV Communications for IoT with Wireless Power Transfer Using Deep Reinforcement Learning. (arXiv:2108.02889v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kunho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1\">Sivakanth Gopi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_J/0/1/0/all/0/1\">Janardhan Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yekhanin_S/0/1/0/all/0/1\">Sergey Yekhanin</a>",
          "description": "We revisit the problem of $n$-gram extraction in the differential privacy\nsetting. In this problem, given a corpus of private text data, the goal is to\nrelease as many $n$-grams as possible while preserving user level privacy.\nExtracting $n$-grams is a fundamental subroutine in many NLP applications such\nas sentence completion, response generation for emails etc. The problem also\narises in other applications such as sequence mining, and is a generalization\nof recently studied differentially private set union (DPSU). In this paper, we\ndevelop a new differentially private algorithm for this problem which, in our\nexperiments, significantly outperforms the state-of-the-art. Our improvements\nstem from combining recent advances in DPSU, privacy accounting, and new\nheuristics for pruning in the tree-based approach initiated by Chen et al.\n(2012).",
          "link": "http://arxiv.org/abs/2108.02831",
          "publishedOn": "2021-08-09T00:49:28.337Z",
          "wordCount": 558,
          "title": "Differentially Private n-gram Extraction. (arXiv:2108.02831v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jiaqian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yongxin Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>",
          "description": "Recently published graph neural networks (GNNs) show promising performance at\nsocial event detection tasks. However, most studies are oriented toward\nmonolingual data in languages with abundant training samples. This has left the\nmore common multilingual settings and lesser-spoken languages relatively\nunexplored. Thus, we present a GNN that incorporates cross-lingual word\nembeddings for detecting events in multilingual data streams. The first exploit\nis to make the GNN work with multilingual data. For this, we outline a\nconstruction strategy that aligns messages in different languages at both the\nnode and semantic levels. Relationships between messages are established by\nmerging entities that are the same but are referred to in different languages.\nNon-English message representations are converted into English semantic space\nvia the cross-lingual word embeddings. The resulting message graph is then\nuniformly encoded by a GNN model. In special cases where a lesser-spoken\nlanguage needs to be detected, a novel cross-lingual knowledge distillation\nframework, called CLKD, exploits prior knowledge learned from similar threads\nin English to make up for the paucity of annotated data. Experiments on both\nsynthetic and real-world datasets show the framework to be highly effective at\ndetection in both multilingual data and in languages where training samples are\nscarce.",
          "link": "http://arxiv.org/abs/2108.03084",
          "publishedOn": "2021-08-09T00:49:28.330Z",
          "wordCount": 639,
          "title": "Transferring Knowledge Distillation for Multilingual Social Event Detection. (arXiv:2108.03084v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Regehr_M/0/1/0/all/0/1\">Matthew T. Regehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayoub_A/0/1/0/all/0/1\">Alex Ayoub</a>",
          "description": "Watkins' and Dayan's Q-learning is a model-free reinforcement learning\nalgorithm that iteratively refines an estimate for the optimal action-value\nfunction of an MDP by stochastically \"visiting\" many state-ation pairs [Watkins\nand Dayan, 1992]. Variants of the algorithm lie at the heart of numerous recent\nstate-of-the-art achievements in reinforcement learning, including the\nsuperhuman Atari-playing deep Q-network [Mnih et al., 2015]. The goal of this\npaper is to reproduce a precise and (nearly) self-contained proof that\nQ-learning converges. Much of the available literature leverages powerful\ntheory to obtain highly generalizable results in this vein. However, this\napproach requires the reader to be familiar with and make many deep connections\nto different research areas. A student seeking to deepen their understand of\nQ-learning risks becoming caught in a vicious cycle of \"RL-learning Hell\". For\nthis reason, we give a complete proof from start to finish using only one\nexternal result from the field of stochastic approximation, despite the fact\nthat this minimal dependence on other results comes at the expense of some\n\"shininess\".",
          "link": "http://arxiv.org/abs/2108.02827",
          "publishedOn": "2021-08-09T00:49:28.310Z",
          "wordCount": 597,
          "title": "An Elementary Proof that Q-learning Converges Almost Surely. (arXiv:2108.02827v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Orts_O/0/1/0/all/0/1\">&#xd2;scar Garibo i Orts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_March_M/0/1/0/all/0/1\">Miguel A. Garcia-March</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conejero_J/0/1/0/all/0/1\">J. Alberto Conejero</a>",
          "description": "Anomalous diffusion occurs at very different scales in nature, from atomic\nsystems to motions in cell organelles, biological tissues or ecology, and also\nin artificial materials, such as cement. Being able to accurately measure the\nanomalous exponent associated with a given particle trajectory, thus\ndetermining whether the particle subdiffuses, superdiffuses or performs normal\ndiffusion is of key importance to understand the diffusion process. Also, it is\noften important to trustingly identify the model behind the trajectory, as this\ngives a large amount of information on the system dynamics. Both aspects are\nparticularly difficult when the input data are short and noisy trajectories. It\nis even more difficult if one cannot guarantee that the trajectories output in\nexperiments is homogeneous, hindering the statistical methods based on\nensembles of trajectories. We present a data-driven method able to infer the\nanomalous exponent and to identify the type of anomalous diffusion process\nbehind single, noisy and short trajectories, with good accuracy. This model was\nused in our participation in the Anomalous Diffusion (AnDi) Challenge. A\ncombination of convolutional and recurrent neural networks were used to achieve\nstate-of-the-art results when compared to methods participating in the AnDi\nChallenge, ranking top 4 in both classification and diffusion exponent\nregression.",
          "link": "http://arxiv.org/abs/2108.02834",
          "publishedOn": "2021-08-09T00:49:28.302Z",
          "wordCount": 654,
          "title": "Efficient recurrent neural network methods for anomalously diffusing single particle short and noisy trajectories. (arXiv:2108.02834v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03090",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bartolomaeus_W/0/1/0/all/0/1\">Wiebke Bartolomaeus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Boutaib_Y/0/1/0/all/0/1\">Youness Boutaib</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nestler_S/0/1/0/all/0/1\">Sandra Nestler</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rauhut_H/0/1/0/all/0/1\">Holger Rauhut</a>",
          "description": "We investigate the functioning of a classifying biological neural network\nfrom the perspective of statistical learning theory, modelled, in a simplified\nsetting, as a continuous-time stochastic recurrent neural network (RNN) with\nidentity activation function. In the purely stochastic (robust) regime, we give\na generalisation error bound that holds with high probability, thus showing\nthat the empirical risk minimiser is the best-in-class hypothesis. We show that\nRNNs retain a partial signature of the paths they are fed as the unique\ninformation exploited for training and classification tasks. We argue that\nthese RNNs are easy to train and robust and back these observations with\nnumerical experiments on both synthetic and real data. We also exhibit a\ntrade-off phenomenon between accuracy and robustness.",
          "link": "http://arxiv.org/abs/2108.03090",
          "publishedOn": "2021-08-09T00:49:28.288Z",
          "wordCount": 570,
          "title": "Path classification by stochastic linear recurrent neural networks. (arXiv:2108.03090v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cheng-Ju Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>",
          "description": "Human-robot collaboration is an essential research topic in artificial\nintelligence (AI), enabling researchers to devise cognitive AI systems and\naffords an intuitive means for users to interact with the robot. Of note,\ncommunication plays a central role. To date, prior studies in embodied agent\nnavigation have only demonstrated that human languages facilitate communication\nby instructions in natural languages. Nevertheless, a plethora of other forms\nof communication is left unexplored. In fact, human communication originated in\ngestures and oftentimes is delivered through multimodal cues, e.g. \"go there\"\nwith a pointing gesture. To bridge the gap and fill in the missing dimension of\ncommunication in embodied agent navigation, we propose investigating the\neffects of using gestures as the communicative interface instead of verbal\ncues. Specifically, we develop a VR-based 3D simulation environment, named\nGes-THOR, based on AI2-THOR platform. In this virtual environment, a human\nplayer is placed in the same virtual scene and shepherds the artificial agent\nusing only gestures. The agent is tasked to solve the navigation problem guided\nby natural gestures with unknown semantics; we do not use any predefined\ngestures due to the diversity and versatile nature of human gestures. We argue\nthat learning the semantics of natural gestures is mutually beneficial to\nlearning the navigation task--learn to communicate and communicate to learn. In\na series of experiments, we demonstrate that human gesture cues, even without\npredefined semantics, improve the object-goal navigation for an embodied agent,\noutperforming various state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.02846",
          "publishedOn": "2021-08-09T00:49:28.272Z",
          "wordCount": 701,
          "title": "Communicative Learning with Natural Gestures for Embodied Navigation Agents with Human-in-the-Scene. (arXiv:2108.02846v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1\">Sim&#xf3;n C. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthy_S/0/1/0/all/0/1\">Subramanian Ramamoorthy</a>",
          "description": "Understanding a controller's performance in different scenarios is crucial\nfor robots that are going to be deployed in safety-critical tasks. If we do not\nhave a model of the dynamics of the world, which is often the case in complex\ndomains, we may need to approximate a performance function of the robot based\non its interaction with the environment. Such a performance function gives us\ninsights into the behaviour of the robot, allowing us to fine-tune the\ncontroller with manual interventions. In high-dimensionality systems, where the\nactionstate space is large, fine-tuning a controller is non-trivial. To\novercome this problem, we propose a performance function whose domain is\ndefined by external features and parameters of the controller. Attainment\nregions are defined over such a domain defined by feature-parameter pairs, and\nserve the purpose of enabling prediction of successful execution of the task.\nThe use of the feature-parameter space -in contrast to the action-state space-\nallows us to adapt, explain and finetune the controller over a simpler (i.e.,\nlower dimensional space). When the robot successfully executes the task, we use\nthe attainment regions to gain insights into the limits of the controller, and\nits robustness. When the robot fails to execute the task, we use the regions to\ndebug the controller and find adaptive and counterfactual changes to the\nsolutions. Another advantage of this approach is that we can generalise through\nthe use of Gaussian processes regression of the performance function in the\nhigh-dimensional space. To test our approach, we demonstrate learning an\napproximation to the performance function in simulation, with a mobile robot\ntraversing different terrain conditions. Then, with a sample-efficient method,\nwe propagate the attainment regions to a physical robot in a similar\nenvironment.",
          "link": "http://arxiv.org/abs/2108.03150",
          "publishedOn": "2021-08-09T00:49:28.258Z",
          "wordCount": 743,
          "title": "Attainment Regions in Feature-Parameter Space for High-Level Debugging in Autonomous Robots. (arXiv:2108.03150v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fiedler_J/0/1/0/all/0/1\">James Fiedler</a>",
          "description": "There is growing interest in neural network architectures for tabular data.\nMany general-purpose tabular deep learning models have been introduced\nrecently, with performance sometimes rivaling gradient boosted decision trees\n(GBDTs). These recent models draw inspiration from various sources, including\nGBDTs, factorization machines, and neural networks from other application\ndomains. Previous tabular neural networks are also drawn upon, but are possibly\nunder-considered, especially models associated with specific tabular problems.\nThis paper focuses on several such models, and proposes modifications for\nimproving their performance. When modified, these models are shown to be\ncompetitive with leading general-purpose tabular models, including GBDTs.",
          "link": "http://arxiv.org/abs/2108.03214",
          "publishedOn": "2021-08-09T00:49:28.237Z",
          "wordCount": 520,
          "title": "Simple Modifications to Improve Tabular Neural Networks. (arXiv:2108.03214v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03166",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Rashid_N/0/1/0/all/0/1\">Nafiul Rashid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Luke Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dautta_M/0/1/0/all/0/1\">Manik Dautta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jimenez_A/0/1/0/all/0/1\">Abel Jimenez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tseng_P/0/1/0/all/0/1\">Peter Tseng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Faruque_M/0/1/0/all/0/1\">Mohammad Abdullah Al Faruque</a>",
          "description": "Stress is a physiological state that hampers mental health and has serious\nconsequences to physical health. Moreover, the COVID-19 pandemic has increased\nstress levels among people across the globe. Therefore, continuous monitoring\nand detection of stress are necessary. The recent advances in wearable devices\nhave allowed the monitoring of several physiological signals related to stress.\nAmong them, wrist-worn wearable devices like smartwatches are most popular due\nto their convenient usage. And the photoplethysmography (PPG) sensor is the\nmost prevalent sensor in almost all consumer-grade wrist-worn smartwatches.\nTherefore, this paper focuses on using a wrist-based PPG sensor that collects\nBlood Volume Pulse (BVP) signals to detect stress which may be applicable for\nconsumer-grade wristwatches. Moreover, state-of-the-art works have used either\nclassical machine learning algorithms to detect stress using hand-crafted\nfeatures or have used deep learning algorithms like Convolutional Neural\nNetwork (CNN) which automatically extracts features. This paper proposes a\nnovel hybrid CNN (H-CNN) classifier that uses both the hand-crafted features\nand the automatically extracted features by CNN to detect stress using the BVP\nsignal. Evaluation on the benchmark WESAD dataset shows that, for 3-class\nclassification (Baseline vs. Stress vs. Amusement), our proposed H-CNN\noutperforms traditional classifiers and normal CNN by 5% and 7% accuracy, and\n10% and 7% macro F1 score, respectively. Also for 2-class classification\n(Stress vs. Non-stress), our proposed H-CNN outperforms traditional classifiers\nand normal CNN by 3% and ~5% accuracy, and ~3% and ~7% macro F1 score,\nrespectively.",
          "link": "http://arxiv.org/abs/2108.03166",
          "publishedOn": "2021-08-09T00:49:28.230Z",
          "wordCount": 759,
          "title": "Feature Augmented Hybrid CNN for Stress Recognition Using Wrist-based Photoplethysmography Sensor. (arXiv:2108.03166v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02883",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Donhauser_K/0/1/0/all/0/1\">Konstantin Donhauser</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tifrea_A/0/1/0/all/0/1\">Alexandru &#x162;ifrea</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aerni_M/0/1/0/all/0/1\">Michael Aerni</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Heckel_R/0/1/0/all/0/1\">Reinhard Heckel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_F/0/1/0/all/0/1\">Fanny Yang</a>",
          "description": "Numerous recent works show that overparameterization implicitly reduces\nvariance for min-norm interpolators and max-margin classifiers. These findings\nsuggest that ridge regularization has vanishing benefits in high dimensions. We\nchallenge this narrative by showing that, even in the absence of noise,\navoiding interpolation through ridge regularization can significantly improve\ngeneralization. We prove this phenomenon for the robust risk of both linear\nregression and classification and hence provide the first theoretical result on\nrobust overfitting.",
          "link": "http://arxiv.org/abs/2108.02883",
          "publishedOn": "2021-08-09T00:49:28.209Z",
          "wordCount": 515,
          "title": "Interpolation can hurt robust generalization even when there is no noise. (arXiv:2108.02883v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tuxworth_D/0/1/0/all/0/1\">David Tuxworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antypas_D/0/1/0/all/0/1\">Dimosthenis Antypas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1\">Luis Espinosa-Anke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preece_A/0/1/0/all/0/1\">Alun Preece</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_D/0/1/0/all/0/1\">David Rogers</a>",
          "description": "This paper demonstrates a two-stage method for deriving insights from social\nmedia data relating to disinformation by applying a combination of geospatial\nclassification and embedding-based language modelling across multiple\nlanguages. In particular, the analysis in centered on Twitter and\ndisinformation for three European languages: English, French and Spanish.\nFirstly, Twitter data is classified into European and non-European sets using\nBERT. Secondly, Word2vec is applied to the classified texts resulting in\nEurocentric, non-Eurocentric and global representations of the data for the\nthree target languages. This comparative analysis demonstrates not only the\nefficacy of the classification method but also highlights geographic, temporal\nand linguistic differences in the disinformation-related media. Thus, the\ncontributions of the work are threefold: (i) a novel language-independent\ntransformer-based geolocation method; (ii) an analytical approach that exploits\nlexical specificity and word embeddings to interrogate user-generated content;\nand (iii) a dataset of 36 million disinformation related tweets in English,\nFrench and Spanish.",
          "link": "http://arxiv.org/abs/2108.03067",
          "publishedOn": "2021-08-09T00:49:28.201Z",
          "wordCount": 604,
          "title": "Deriving Disinformation Insights from Geolocalized Twitter Callouts. (arXiv:2108.03067v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haijian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Rose Qingyang Hu</a>",
          "description": "A new machine learning (ML) technique termed as federated learning (FL) aims\nto preserve data at the edge devices and to only exchange ML model parameters\nin the learning process. FL not only reduces the communication needs but also\nhelps to protect the local privacy. Although FL has these advantages, it can\nstill experience large communication latency when there are massive edge\ndevices connected to the central parameter server (PS) and/or millions of model\nparameters involved in the learning process. Over-the-air computation (AirComp)\nis capable of computing while transmitting data by allowing multiple devices to\nsend data simultaneously by using analog modulation. To achieve good\nperformance in FL through AirComp, user scheduling plays a critical role. In\nthis paper, we investigate and compare different user scheduling policies,\nwhich are based on various criteria such as wireless channel conditions and the\nsignificance of model updates. Receiver beamforming is applied to minimize the\nmean-square-error (MSE) of the distortion of function aggregation result via\nAirComp. Simulation results show that scheduling based on the significance of\nmodel updates has smaller fluctuations in the training process while scheduling\nbased on channel condition has the advantage on energy efficiency.",
          "link": "http://arxiv.org/abs/2108.02891",
          "publishedOn": "2021-08-09T00:49:28.181Z",
          "wordCount": 633,
          "title": "User Scheduling for Federated Learning Through Over-the-Air Computation. (arXiv:2108.02891v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sihwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1\">Dae Yon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1\">Taejang Park</a>",
          "description": "The merit of ensemble learning lies in having different outputs from many\nindividual models on a single input, i.e., the diversity of the base models.\nThe high quality of diversity can be achieved when each model is specialized to\ndifferent subsets of the whole dataset. Moreover, when each model explicitly\nknows to which subsets it is specialized, more opportunities arise to improve\ndiversity. In this paper, we propose an advanced ensemble method, called\nAuxiliary class based Multiple Choice Learning (AMCL), to ultimately specialize\neach model under the framework of multiple choice learning (MCL). The\nadvancement of AMCL is originated from three novel techniques which control the\nframework from different directions: 1) the concept of auxiliary class to\nprovide more distinct information through the labels, 2) the strategy, named\nmemory-based assignment, to determine the association between the inputs and\nthe models, and 3) the feature fusion module to achieve generalized features.\nTo demonstrate the performance of our method compared to all variants of MCL\nmethods, we conduct extensive experiments on the image classification and\nsegmentation tasks. Overall, the performance of AMCL exceeds all others in most\nof the public datasets trained with various networks as members of the\nensembles.",
          "link": "http://arxiv.org/abs/2108.02949",
          "publishedOn": "2021-08-09T00:49:28.174Z",
          "wordCount": 623,
          "title": "Auxiliary Class Based Multiple Choice Learning. (arXiv:2108.02949v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02867",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1\">Shagufta Henna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalliadan_S/0/1/0/all/0/1\">Shyam Krishnan Kalliadan</a>",
          "description": "In a business-to-business (B2B) customer relationship management (CRM) use\ncase, each client is a potential business organization/company with a solid\nbusiness strategy and focused and rational decisions. This paper introduces a\ngraph-based analytics approach to improve CRM within a B2B environment. In our\napproach, in the first instance, we have designed a graph database using the\nNeo4j platform. Secondly, the graph database has been investigated by using\ndata mining and exploratory analysis coupled with cypher graph query language.\nSpecifically, we have applied the graph convolution network (GCN) to enable CRM\nanalytics to forecast sales. This is the first step towards a GCN-based binary\nclassification based on graph databases in the domain of B2B CRM. We evaluate\nthe performance of the proposed GCN model on graph databases and compare it\nwith Random Forest (RF), Convolutional Neural Network (CNN), and Artificial\nNeural Network (ANN). The proposed GCN approach is further augmented with the\nshortest path and eigenvector centrality attribute to significantly improve the\naccuracy of sales prediction. Experimental results reveal that the proposed\ngraph-based deep learning approach outperforms the Random Forests (RsF) and two\ndeep learning models, i.e., CNN and ANN under different combinations of graph\nfeatures.",
          "link": "http://arxiv.org/abs/2108.02867",
          "publishedOn": "2021-08-09T00:49:28.168Z",
          "wordCount": 623,
          "title": "Enterprise Analytics using Graph Database and Graph-based Deep Learning. (arXiv:2108.02867v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shuvendu Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>",
          "description": "We propose a self-supervised contrastive learning approach for facial\nexpression recognition (FER) in videos. We propose a novel temporal\nsampling-based augmentation scheme to be utilized in addition to standard\nspatial augmentations used for contrastive learning. Our proposed temporal\naugmentation scheme randomly picks from one of three temporal sampling\ntechniques: (1) pure random sampling, (2) uniform sampling, and (3) sequential\nsampling. This is followed by a combination of up to three standard spatial\naugmentations. We then use a deep R(2+1)D network for FER, which we train in a\nself-supervised fashion based on the augmentations and subsequently fine-tune.\nExperiments are performed on the Oulu-CASIA dataset and the performance is\ncompared to other works in FER. The results indicate that our method achieves\nan accuracy of 89.4%, setting a new state-of-the-art by outperforming other\nworks. Additional experiments and analysis confirm the considerable\ncontribution of the proposed temporal augmentation versus the existing spatial\nones.",
          "link": "http://arxiv.org/abs/2108.03064",
          "publishedOn": "2021-08-09T00:49:28.161Z",
          "wordCount": 599,
          "title": "Spatiotemporal Contrastive Learning of Facial Expressions in Videos. (arXiv:2108.03064v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Seonguk Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joon-Young Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bohyung Han</a>",
          "description": "Dataset bias is a critical challenge in machine learning, and its negative\nimpact is aggravated when models capture unintended decision rules with\nspurious correlations. Although existing works often handle this issue using\nhuman supervision, the availability of the proper annotations is impractical\nand even unrealistic. To better tackle this challenge, we propose a simple but\neffective debiasing technique in an unsupervised manner. Specifically, we\nperform clustering on the feature embedding space and identify pseudoattributes\nby taking advantage of the clustering results even without an explicit\nattribute supervision. Then, we employ a novel cluster-based reweighting scheme\nfor learning debiased representation; this prevents minority groups from being\ndiscounted for minimizing the overall loss, which is desirable for worst-case\ngeneralization. The extensive experiments demonstrate the outstanding\nperformance of our approach on multiple standard benchmarks, which is even as\ncompetitive as the supervised counterpart.",
          "link": "http://arxiv.org/abs/2108.02943",
          "publishedOn": "2021-08-09T00:49:28.153Z",
          "wordCount": 565,
          "title": "Unsupervised Learning of Debiased Representations with Pseudo-Attributes. (arXiv:2108.02943v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kenny Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1\">Arunesh Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1\">Arvind Narayanan</a>",
          "description": "Concerns about privacy, bias, and harmful applications have shone a light on\nthe ethics of machine learning datasets, even leading to the retraction of\nprominent datasets including DukeMTMC, MS-Celeb-1M, TinyImages, and VGGFace2.\nIn response, the machine learning community has called for higher ethical\nstandards, transparency efforts, and technical fixes in the dataset creation\nprocess. The premise of our work is that these efforts can be more effective if\ninformed by an understanding of how datasets are used in practice in the\nresearch community. We study three influential face and person recognition\ndatasets - DukeMTMC, MS-Celeb-1M, and Labeled Faces in the Wild (LFW) - by\nanalyzing nearly 1000 papers that cite them. We found that the creation of\nderivative datasets and models, broader technological and social change, the\nlack of clarity of licenses, and dataset management practices can introduce a\nwide range of ethical concerns. We conclude by suggesting a distributed\napproach that can mitigate these harms, making recommendations to dataset\ncreators, conference program committees, dataset users, and the broader\nresearch community.",
          "link": "http://arxiv.org/abs/2108.02922",
          "publishedOn": "2021-08-09T00:49:28.130Z",
          "wordCount": 605,
          "title": "Mitigating dataset harms requires stewardship: Lessons from 1000 papers. (arXiv:2108.02922v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1\">Shagufta Henna</a>",
          "description": "Since the rapid outbreak of Covid-19 and with no approved vaccines to date,\nprofound research interest has emerged to understand the innate immune response\nto viruses. This understanding can help to inhibit virus replication, prolong\nadaptive immune response, accelerated virus clearance, and tissue recovery, a\nkey milestone to propose a vaccine to combat coronaviruses (CoVs), e.g.,\nCovid-19. Although an innate immune system triggers inflammatory responses\nagainst CoVs upon recognition of viruses, however, a vaccine is the ultimate\nprotection against CoV spread. The development of this vaccine is\ntime-consuming and requires a deep understanding of the innate immune response\nsystem. In this work, we propose a graph neural network-based model that\nexploits the interactions between pattern recognition receptors (PRRs), i.e.,\nthe human immune response system. These interactions can help to recognize\npathogen-associated molecular patterns (PAMPs) to predict the activation\nrequirements of each PRR. The immune response information of each PRR is\nderived from combining its historical PAMPs activation coupled with the modeled\neffect on the same from PRRs in its neighborhood. On one hand, this work can\nhelp to understand how long Covid-19 can confer immunity where a strong immune\nresponse means people already been infected can safely return to work. On the\nother hand, this GNN-based understanding can also abode well for vaccine\ndevelopment efforts. Our proposal has been evaluated using CoVs immune response\ndataset, with results showing an average IFNs activation prediction accuracy of\n90%, compared to 85% using feed-forward neural networks.",
          "link": "http://arxiv.org/abs/2108.02872",
          "publishedOn": "2021-08-09T00:49:28.108Z",
          "wordCount": 715,
          "title": "Understanding Human Innate Immune System Dependencies using Graph Neural Networks. (arXiv:2108.02872v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wet_H/0/1/0/all/0/1\">Harm de Wet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>",
          "description": "Disinformation through fake news is an ongoing problem in our society and has\nbecome easily spread through social media. The most cost and time effective way\nto filter these large amounts of data is to use a combination of human and\ntechnical interventions to identify it. From a technical perspective, Natural\nLanguage Processing (NLP) is widely used in detecting fake news. Social media\ncompanies use NLP techniques to identify the fake news and warn their users,\nbut fake news may still slip through undetected. It is especially a problem in\nmore localised contexts (outside the United States of America). How do we\nadjust fake news detection systems to work better for local contexts such as in\nSouth Africa. In this work we investigate fake news detection on South African\nwebsites. We curate a dataset of South African fake news and then train\ndetection models. We contrast this with using widely available fake news\ndatasets (from mostly USA website). We also explore making the datasets more\ndiverse by combining them and observe the differences in behaviour in writing\nbetween nations' fake news using interpretable machine learning.",
          "link": "http://arxiv.org/abs/2108.02941",
          "publishedOn": "2021-08-09T00:49:28.092Z",
          "wordCount": 640,
          "title": "Is it Fake? News Disinformation Detection on South African News Websites. (arXiv:2108.02941v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arango_S/0/1/0/all/0/1\">Sebastian Pineda Arango</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinrich_F/0/1/0/all/0/1\">Felix Heinrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhusudhanan_K/0/1/0/all/0/1\">Kiran Madhusudhanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1\">Lars Schmidt-Thieme</a>",
          "description": "Recent work has shown the efficiency of deep learning models such as Fully\nConvolutional Networks (FCN) or Recurrent Neural Networks (RNN) to deal with\nTime Series Regression (TSR) problems. These models sometimes need a lot of\ndata to be able to generalize, yet the time series are sometimes not long\nenough to be able to learn patterns. Therefore, it is important to make use of\ninformation across time series to improve learning. In this paper, we will\nexplore the idea of using meta-learning for quickly adapting model parameters\nto new short-history time series by modifying the original idea of Model\nAgnostic Meta-Learning (MAML) \\cite{finn2017model}. Moreover, based on prior\nwork on multimodal MAML \\cite{vuorio2019multimodal}, we propose a method for\nconditioning parameters of the model through an auxiliary network that encodes\nglobal information of the time series to extract meta-features. Finally, we\napply the data to time series of different domains, such as pollution\nmeasurements, heart-rate sensors, and electrical battery data. We show\nempirically that our proposed meta-learning method learns TSR with few data\nfast and outperforms the baselines in 9 of 12 experiments.",
          "link": "http://arxiv.org/abs/2108.02842",
          "publishedOn": "2021-08-09T00:49:28.085Z",
          "wordCount": 612,
          "title": "Multimodal Meta-Learning for Time Series Regression. (arXiv:2108.02842v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Remil_Y/0/1/0/all/0/1\">Youcef Remil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendimerad_A/0/1/0/all/0/1\">Anes Bendimerad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plantevit_M/0/1/0/all/0/1\">Marc Plantevit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robardet_C/0/1/0/all/0/1\">C&#xe9;line Robardet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaytoue_M/0/1/0/all/0/1\">Mehdi Kaytoue</a>",
          "description": "The need of predictive maintenance comes with an increasing number of\nincidents reported by monitoring systems and equipment/software users. In the\nfront line, on-call engineers (OCEs) have to quickly assess the degree of\nseverity of an incident and decide which service to contact for corrective\nactions. To automate these decisions, several predictive models have been\nproposed, but the most efficient models are opaque (say, black box), strongly\nlimiting their adoption. In this paper, we propose an efficient black box model\nbased on 170K incidents reported to our company over the last 7 years and\nemphasize on the need of automating triage when incidents are massively\nreported on thousands of servers running our product, an ERP. Recent\ndevelopments in eXplainable Artificial Intelligence (XAI) help in providing\nglobal explanations to the model, but also, and most importantly, with local\nexplanations for each model prediction/outcome. Sadly, providing a human with\nan explanation for each outcome is not conceivable when dealing with an\nimportant number of daily predictions. To address this problem, we propose an\noriginal data-mining method rooted in Subgroup Discovery, a pattern mining\ntechnique with the natural ability to group objects that share similar\nexplanations of their black box predictions and provide a description for each\ngroup. We evaluate this approach and present our preliminary results which give\nus good hope towards an effective OCE's adoption. We believe that this approach\nprovides a new way to address the problem of model agnostic outcome\nexplanation.",
          "link": "http://arxiv.org/abs/2108.03013",
          "publishedOn": "2021-08-09T00:49:28.078Z",
          "wordCount": 686,
          "title": "Interpretable Summaries of Black Box Incident Triaging with Subgroup Discovery. (arXiv:2108.03013v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02892",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi Khac Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masaracchia_A/0/1/0/all/0/1\">Antonino Masaracchia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yin_C/0/1/0/all/0/1\">Cheng Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_L/0/1/0/all/0/1\">Long D. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dobre_O/0/1/0/all/0/1\">Octavia A. Dobre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duong_T/0/1/0/all/0/1\">Trung Q. Duong</a>",
          "description": "In this paper, we propose a deep reinforcement learning (DRL) approach for\nsolving the optimisation problem of the network's sum-rate in device-to-device\n(D2D) communications supported by an intelligent reflecting surface (IRS). The\nIRS is deployed to mitigate the interference and enhance the signal between the\nD2D transmitter and the associated D2D receiver. Our objective is to jointly\noptimise the transmit power at the D2D transmitter and the phase shift matrix\nat the IRS to maximise the network sum-rate. We formulate a Markov decision\nprocess and then propose the proximal policy optimisation for solving the\nmaximisation game. Simulation results show impressive performance in terms of\nthe achievable rate and processing time.",
          "link": "http://arxiv.org/abs/2108.02892",
          "publishedOn": "2021-08-09T00:49:28.057Z",
          "wordCount": 581,
          "title": "Deep Reinforcement Learning for Intelligent Reflecting Surface-assisted D2D Communications. (arXiv:2108.02892v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadreddin_A/0/1/0/all/0/1\">Armin Sadreddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadaoui_S/0/1/0/all/0/1\">Samira Sadaoui</a>",
          "description": "This study addresses the actual behavior of the credit-card fraud detection\nenvironment where financial transactions containing sensitive data must not be\namassed in an enormous amount to conduct learning. We introduce a new adaptive\nlearning approach that adjusts frequently and efficiently to new transaction\nchunks; each chunk is discarded after each incremental training step. Our\napproach combines transfer learning and incremental feature learning. The\nformer improves the feature relevancy for subsequent chunks, and the latter, a\nnew paradigm, increases accuracy during training by determining the optimal\nnetwork architecture dynamically for each new chunk. The architectures of past\nincremental approaches are fixed; thus, the accuracy may not improve with new\nchunks. We show the effectiveness and superiority of our approach\nexperimentally on an actual fraud dataset.",
          "link": "http://arxiv.org/abs/2108.02932",
          "publishedOn": "2021-08-09T00:49:28.037Z",
          "wordCount": 547,
          "title": "Incremental Feature Learning For Infinite Data. (arXiv:2108.02932v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02853",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Karatas_T/0/1/0/all/0/1\">Tugce Karatas</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Klinkert_F/0/1/0/all/0/1\">Federico Klinkert</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Hirsa_A/0/1/0/all/0/1\">Ali Hirsa</a>",
          "description": "Institutional investors have been increasing the allocation of the illiquid\nalternative assets such as private equity funds in their portfolios, yet there\nexists a very limited literature on cash flow forecasting of illiquid\nalternative assets. The net cash flow of private equity funds typically follow\na J-curve pattern, however the timing and the size of the contributions and\ndistributions depend on the investment opportunities. In this paper, we develop\na benchmark model and present two novel approaches (direct vs. indirect) to\npredict the cash flows of private equity funds. We introduce a sliding window\napproach to apply on our cash flow data because different vintage year funds\ncontain different lengths of cash flow information. We then pass the data to an\nLSTM/ GRU model to predict the future cash flows either directly or indirectly\n(based on the benchmark model). We further integrate macroeconomic indicators\ninto our data, which allows us to consider the impact of market environment on\ncash flows and to apply stress testing. Our results indicate that the direct\nmodel is easier to implement compared to the benchmark model and the indirect\nmodel, but still the predicted cash flows align better with the actual cash\nflows. We also show that macroeconomic variables improve the performance of the\ndirect model whereas the impact is not obvious on the indirect model.",
          "link": "http://arxiv.org/abs/2108.02853",
          "publishedOn": "2021-08-09T00:49:28.021Z",
          "wordCount": 664,
          "title": "Supervised Neural Networks for Illiquid Alternative Asset Cash Flow Forecasting. (arXiv:2108.02853v1 [q-fin.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupte_A/0/1/0/all/0/1\">Amit Gupte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romanov_A/0/1/0/all/0/1\">Alexey Romanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mantravadi_S/0/1/0/all/0/1\">Sahitya Mantravadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banda_D/0/1/0/all/0/1\">Dalitso Banda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1\">Raza Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meenal_L/0/1/0/all/0/1\">Lakshmanan Ramu Meenal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Benjamin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Soundar Srinivasan</a>",
          "description": "Document digitization is essential for the digital transformation of our\nsocieties, yet a crucial step in the process, Optical Character Recognition\n(OCR), is still not perfect. Even commercial OCR systems can produce\nquestionable output depending on the fidelity of the scanned documents. In this\npaper, we demonstrate an effective framework for mitigating OCR errors for any\ndownstream NLP task, using Named Entity Recognition (NER) as an example. We\nfirst address the data scarcity problem for model training by constructing a\ndocument synthesis pipeline, generating realistic but degraded data with NER\nlabels. We measure the NER accuracy drop at various degradation levels and show\nthat a text restoration model, trained on the degraded data, significantly\ncloses the NER accuracy gaps caused by OCR errors, including on an\nout-of-domain dataset. For the benefit of the community, we have made the\ndocument synthesis pipeline available as an open-source project.",
          "link": "http://arxiv.org/abs/2108.02899",
          "publishedOn": "2021-08-09T00:49:28.013Z",
          "wordCount": 621,
          "title": "Lights, Camera, Action! A Framework to Improve NLP Accuracy over OCR documents. (arXiv:2108.02899v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02817",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Floricel_C/0/1/0/all/0/1\">Carla Floricel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nipu_N/0/1/0/all/0/1\">Nafiul Nipu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biggs_M/0/1/0/all/0/1\">Mikayla Biggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wentzel_A/0/1/0/all/0/1\">Andrew Wentzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canahuate_G/0/1/0/all/0/1\">Guadalupe Canahuate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijk_L/0/1/0/all/0/1\">Lisanne Van Dijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdallah Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuller_C/0/1/0/all/0/1\">C. David Fuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marai_G/0/1/0/all/0/1\">G. Elisabeta Marai</a>",
          "description": "Although cancer patients survive years after oncologic therapy, they are\nplagued with long-lasting or permanent residual symptoms, whose severity, rate\nof development, and resolution after treatment vary largely between survivors.\nThe analysis and interpretation of symptoms is complicated by their partial\nco-occurrence, variability across populations and across time, and, in the case\nof cancers that use radiotherapy, by further symptom dependency on the tumor\nlocation and prescribed treatment. We describe THALIS, an environment for\nvisual analysis and knowledge discovery from cancer therapy symptom data,\ndeveloped in close collaboration with oncology experts. Our approach leverages\nunsupervised machine learning methodology over cohorts of patients, and, in\nconjunction with custom visual encodings and interactions, provides context for\nnew patients based on patients with similar diagnostic features and symptom\nevolution. We evaluate this approach on data collected from a cohort of head\nand neck cancer patients. Feedback from our clinician collaborators indicates\nthat THALIS supports knowledge discovery beyond the limits of machines or\nhumans alone, and that it serves as a valuable tool in both the clinic and\nsymptom research.",
          "link": "http://arxiv.org/abs/2108.02817",
          "publishedOn": "2021-08-09T00:49:27.993Z",
          "wordCount": 630,
          "title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy. (arXiv:2108.02817v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Moin Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahzad_K/0/1/0/all/0/1\">Khurram Shahzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1\">Kamran Malik</a>",
          "description": "Hate speech is a specific type of controversial content that is widely\nlegislated as a crime that must be identified and blocked. However, due to the\nsheer volume and velocity of the Twitter data stream, hate speech detection\ncannot be performed manually. To address this issue, several studies have been\nconducted for hate speech detection in European languages, whereas little\nattention has been paid to low-resource South Asian languages, making the\nsocial media vulnerable for millions of users. In particular, to the best of\nour knowledge, no study has been conducted for hate speech detection in Roman\nUrdu text, which is widely used in the sub-continent. In this study, we have\nscrapped more than 90,000 tweets and manually parsed them to identify 5,000\nRoman Urdu tweets. Subsequently, we have employed an iterative approach to\ndevelop guidelines and used them for generating the Hate Speech Roman Urdu 2020\ncorpus. The tweets in the this corpus are classified at three levels:\nNeutral-Hostile, Simple-Complex, and Offensive-Hate speech. As another\ncontribution, we have used five supervised learning techniques, including a\ndeep learning technique, to evaluate and compare their effectiveness for hate\nspeech detection. The results show that Logistic Regression outperformed all\nother techniques, including deep learning techniques for the two levels of\nclassification, by achieved an F1 score of 0.906 for distinguishing between\nNeutral-Hostile tweets, and 0.756 for distinguishing between Offensive-Hate\nspeech tweets.",
          "link": "http://arxiv.org/abs/2108.02830",
          "publishedOn": "2021-08-09T00:49:27.972Z",
          "wordCount": 719,
          "title": "Hate Speech Detection in Roman Urdu. (arXiv:2108.02830v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02799",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Tiffany D. Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Seong Ioi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dylan S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMillian_M/0/1/0/all/0/1\">Matthew G. McMillian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMahan_R/0/1/0/all/0/1\">Ryan P. McMahan</a>",
          "description": "League of Legends (LoL) is the most widely played multiplayer online battle\narena (MOBA) game in the world. An important aspect of LoL is competitive\nranked play, which utilizes a skill-based matchmaking system to form fair\nteams. However, players' skill levels vary widely depending on which champion,\nor hero, that they choose to play as. In this paper, we propose a method for\npredicting game outcomes in ranked LoL games based on players' experience with\ntheir selected champion. Using a deep neural network, we found that game\noutcomes can be predicted with 75.1% accuracy after all players have selected\nchampions, which occurs before gameplay begins. Our results have important\nimplications for playing LoL and matchmaking. Firstly, individual champion\nskill plays a significant role in the outcome of a match, regardless of team\ncomposition. Secondly, even after the skill-based matchmaking, there is still a\nwide variance in team skill before gameplay begins. Finally, players should\nonly play champions that they have mastered, if they want to win games.",
          "link": "http://arxiv.org/abs/2108.02799",
          "publishedOn": "2021-08-09T00:49:27.867Z",
          "wordCount": 640,
          "title": "Using Machine Learning to Predict Game Outcomes Based on Player-Champion Experience in League of Legends. (arXiv:2108.02799v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenjie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhide Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Dong-Ling Deng</a>",
          "description": "Catastrophic forgetting describes the fact that machine learning models will\nlikely forget the knowledge of previously learned tasks after the learning\nprocess of a new one. It is a vital problem in the continual learning scenario\nand recently has attracted tremendous concern across different communities. In\nthis paper, we explore the catastrophic forgetting phenomena in the context of\nquantum machine learning. We find that, similar to those classical learning\nmodels based on neural networks, quantum learning systems likewise suffer from\nsuch forgetting problem in classification tasks emerging from various\napplication scenes. We show that based on the local geometrical information in\nthe loss function landscape of the trained model, a uniform strategy can be\nadapted to overcome the forgetting problem in the incremental learning setting.\nOur results uncover the catastrophic forgetting phenomena in quantum machine\nlearning and offer a practical method to overcome this problem, which opens a\nnew avenue for exploring potential quantum advantages towards continual\nlearning.",
          "link": "http://arxiv.org/abs/2108.02786",
          "publishedOn": "2021-08-09T00:49:27.841Z",
          "wordCount": 595,
          "title": "Quantum Continual Learning Overcoming Catastrophic Forgetting. (arXiv:2108.02786v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02811",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Ubaru_S/0/1/0/all/0/1\">Shashanka Ubaru</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Akhalwaya_I/0/1/0/all/0/1\">Ismail Yunus Akhalwaya</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Squillante_M/0/1/0/all/0/1\">Mark S. Squillante</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Clarkson_K/0/1/0/all/0/1\">Kenneth L. Clarkson</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Horesh_L/0/1/0/all/0/1\">Lior Horesh</a>",
          "description": "Quantum computing offers the potential of exponential speedups for certain\nclassical computations. Over the last decade, many quantum machine learning\n(QML) algorithms have been proposed as candidates for such exponential\nimprovements. However, two issues unravel the hope of exponential speedup for\nsome of these QML algorithms: the data-loading problem and, more recently, the\nstunning dequantization results of Tang et al. A third issue, namely the\nfault-tolerance requirements of most QML algorithms, has further hindered their\npractical realization. The quantum topological data analysis (QTDA) algorithm\nof Lloyd, Garnerone and Zanardi was one of the first QML algorithms that\nconvincingly offered an expected exponential speedup. From the outset, it did\nnot suffer from the data-loading problem. A recent result has also shown that\nthe generalized problem solved by this algorithm is likely classically\nintractable, and would therefore be immune to any dequantization efforts.\nHowever, the QTDA algorithm of Lloyd et~al. has a time complexity of\n$O(n^4/(\\epsilon^2 \\delta))$ (where $n$ is the number of data points,\n$\\epsilon$ is the error tolerance, and $\\delta$ is the smallest nonzero\neigenvalue of the restricted Laplacian) and requires fault-tolerant quantum\ncomputing, which has not yet been achieved. In this paper, we completely\noverhaul the QTDA algorithm to achieve an improved exponential speedup and\ndepth complexity of $O(n\\log(1/(\\delta\\epsilon)))$. Our approach includes three\nkey innovations: (a) an efficient realization of the combinatorial Laplacian as\na sum of Pauli operators; (b) a quantum rejection sampling approach to restrict\nthe superposition to the simplices in the complex; and (c) a stochastic rank\nestimation method to estimate the Betti numbers. We present a theoretical error\nanalysis, and the circuit and computational time and depth complexities for\nBetti number estimation.",
          "link": "http://arxiv.org/abs/2108.02811",
          "publishedOn": "2021-08-09T00:49:27.826Z",
          "wordCount": 729,
          "title": "Quantum Topological Data Analysis with Linear Depth and Exponential Speedup. (arXiv:2108.02811v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Webb_E/0/1/0/all/0/1\">E. William Webb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scott_P/0/1/0/all/0/1\">Peter J.H. Scott</a>",
          "description": "Artificial intelligence and machine learning are poised to disrupt PET\nimaging from bench to clinic. In this perspective we offer insights into how\nthe technology could be applied to improve the design and synthesis of new\nradiopharmaceuticals for PET imaging, including identification of an optimal\nlabeling approach as well as strategies for radiolabeling reaction\noptimization.",
          "link": "http://arxiv.org/abs/2108.02814",
          "publishedOn": "2021-08-09T00:49:27.788Z",
          "wordCount": 496,
          "title": "Potential Applications of Artificial Intelligence and Machine Learning in Radiochemistry and Radiochemical Engineering. (arXiv:2108.02814v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kukacka_J/0/1/0/all/0/1\">Jan Kuka&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zenz_A/0/1/0/all/0/1\">Anja Zenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kollovieh_M/0/1/0/all/0/1\">Marcel Kollovieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Justel_D/0/1/0/all/0/1\">Dominik J&#xfc;stel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntziachristos_V/0/1/0/all/0/1\">Vasilis Ntziachristos</a>",
          "description": "Fundus photography is the primary method for retinal imaging and essential\nfor diabetic retinopathy prevention. Automated segmentation of fundus\nphotographs would improve the quality, capacity, and cost-effectiveness of eye\ncare screening programs. However, current segmentation methods are not robust\ntowards the diversity in imaging conditions and pathologies typical for\nreal-world clinical applications. To overcome these limitations, we utilized\ncontrastive self-supervised learning to exploit the large variety of unlabeled\nfundus images in the publicly available EyePACS dataset. We pre-trained an\nencoder of a U-Net, which we later fine-tuned on several retinal vessel and\nlesion segmentation datasets. We demonstrate for the first time that by using\ncontrastive self-supervised learning, the pre-trained network can recognize\nblood vessels, optic disc, fovea, and various lesions without being provided\nany labels. Furthermore, when fine-tuned on a downstream blood vessel\nsegmentation task, such pre-trained networks achieve state-of-the-art\nperformance on images from different datasets. Additionally, the pre-training\nalso leads to shorter training times and an improved few-shot performance on\nboth blood vessel and lesion segmentation tasks. Altogether, our results\nshowcase the benefits of contrastive self-supervised pre-training which can\nplay a crucial role in real-world clinical applications requiring robust models\nable to adapt to new devices with only a few annotated samples.",
          "link": "http://arxiv.org/abs/2108.02798",
          "publishedOn": "2021-08-09T00:49:27.760Z",
          "wordCount": 650,
          "title": "Self-Supervised Learning from Unlabeled Fundus Photographs Improves Segmentation of the Retina. (arXiv:2108.02798v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xu Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hjelm_D/0/1/0/all/0/1\">Devon Hjelm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>",
          "description": "Piecewise linear neural networks can be split into subfunctions, each with\nits own activation pattern, domain, and empirical error. Empirical error for\nthe full network can be written as an expectation over empirical error of\nsubfunctions. Constructing a generalization bound on subfunction empirical\nerror indicates that the more densely a subfunction is surrounded by training\nsamples in representation space, the more reliable its predictions are.\nFurther, it suggests that models with fewer activation regions generalize\nbetter, and models that abstract knowledge to a greater degree generalize\nbetter, all else equal. We propose not only a theoretical framework to reason\nabout subfunction error bounds but also a pragmatic way of approximately\nevaluating it, which we apply to predicting which samples the network will not\nsuccessfully generalize to. We test our method on detection of\nmisclassification and out-of-distribution samples, finding that it performs\ncompetitively in both cases. In short, some network activation patterns are\nassociated with higher reliability than others, and these can be identified\nusing subfunction error bounds.",
          "link": "http://arxiv.org/abs/2106.08365",
          "publishedOn": "2021-08-06T00:51:48.178Z",
          "wordCount": 642,
          "title": "Predicting Unreliable Predictions by Shattering a Neural Network. (arXiv:2106.08365v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuelu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Ziniu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>",
          "description": "Answering complex First-Order Logical (FOL) queries on large-scale incomplete\nknowledge graphs (KGs) is an important yet challenging task. Recent advances\nembed logical queries and KG entities in the vector space and conduct query\nanswering via dense similarity search. However, most of the designed logical\noperators in existing works do not satisfy the axiomatic system of classical\nlogic. Moreover, these logical operators are parameterized so that they require\na large number of complex FOL queries as training data, which are often arduous\nor even inaccessible to collect in most real-world KGs. In this paper, we\npresent FuzzQE, a fuzzy logic based query embedding framework for answering FOL\nqueries over KGs. FuzzQE follows fuzzy logic to define logical operators in a\nprincipled and learning free manner. Extensive experiments on two benchmark\ndatasets demonstrate that FuzzQE achieves significantly better performance in\nanswering FOL queries compared to the state-of-the-art methods. In addition,\nFuzzQE trained with only KG link prediction without any complex queries can\nachieve comparable performance with the systems trained with all FOL queries.",
          "link": "http://arxiv.org/abs/2108.02390",
          "publishedOn": "2021-08-06T00:51:48.138Z",
          "wordCount": 600,
          "title": "Fuzzy Logic based Logical Query Answering on Knowledge Graph. (arXiv:2108.02390v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1\">Nitish Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prados_D/0/1/0/all/0/1\">David Ramon Prados</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodzic_N/0/1/0/all/0/1\">Nedim Hodzic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanassios_C/0/1/0/all/0/1\">Christos Karanassios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1\">H.R. Tizhoosh</a>",
          "description": "Lung nodules are commonly missed in chest radiographs. We propose and\nevaluate P-AnoGAN, an unsupervised anomaly detection approach for lung nodules\nin radiographs. P-AnoGAN modifies the fast anomaly detection generative\nadversarial network (f-AnoGAN) by utilizing a progressive GAN and a\nconvolutional encoder-decoder-encoder pipeline. Model training uses only\nunlabelled healthy lung patches extracted from the Indiana University Chest\nX-Ray Collection. External validation and testing are performed using healthy\nand unhealthy patches extracted from the ChestX-ray14 and Japanese Society for\nRadiological Technology datasets, respectively. Our model robustly identifies\npatches containing lung nodules in external validation and test data with\nROC-AUC of 91.17% and 87.89%, respectively. These results show unsupervised\nmethods may be useful in challenging tasks such as lung nodule detection in\nradiographs.",
          "link": "http://arxiv.org/abs/2108.02233",
          "publishedOn": "2021-08-06T00:51:47.947Z",
          "wordCount": 594,
          "title": "Unsupervised Detection of Lung Nodules in Chest Radiography Using Generative Adversarial Networks. (arXiv:2108.02233v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Canyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eschweiler_D/0/1/0/all/0/1\">Dennis Eschweiler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stegmaier_J/0/1/0/all/0/1\">Johannes Stegmaier</a>",
          "description": "Recent developments in fluorescence microscopy allow capturing\nhigh-resolution 3D images over time for living model organisms. To be able to\nimage even large specimens, techniques like multi-view light-sheet imaging\nrecord different orientations at each time point that can then be fused into a\nsingle high-quality volume. Based on measured point spread functions (PSF),\ndeconvolution and content fusion are able to largely revert the inevitable\ndegradation occurring during the imaging process. Classical multi-view\ndeconvolution and fusion methods mainly use iterative procedures and\ncontent-based averaging. Lately, Convolutional Neural Networks (CNNs) have been\ndeployed to approach 3D single-view deconvolution microscopy, but the\nmulti-view case waits to be studied. We investigated the efficacy of CNN-based\nmulti-view deconvolution and fusion with two synthetic data sets that mimic\ndeveloping embryos and involve either two or four complementary 3D views.\nCompared with classical state-of-the-art methods, the proposed semi- and\nself-supervised models achieve competitive and superior deconvolution and\nfusion quality in the two-view and quad-view cases, respectively.",
          "link": "http://arxiv.org/abs/2108.02743",
          "publishedOn": "2021-08-06T00:51:47.898Z",
          "wordCount": 620,
          "title": "Semi- and Self-Supervised Multi-View Fusion of 3D Microscopy Images using Generative Adversarial Networks. (arXiv:2108.02743v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Ming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Feng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "Deep learning on graphs has attracted significant interests recently.\nHowever, most of the works have focused on (semi-) supervised learning,\nresulting in shortcomings including heavy label reliance, poor generalization,\nand weak robustness. To address these issues, self-supervised learning (SSL),\nwhich extracts informative knowledge through well-designed pretext tasks\nwithout relying on manual labels, has become a promising and trending learning\nparadigm for graph data. Different from SSL on other domains like computer\nvision and natural language processing, SSL on graphs has an exclusive\nbackground, design ideas, and taxonomies. Under the umbrella of graph\nself-supervised learning, we present a timely and comprehensive review of the\nexisting approaches which employ SSL techniques for graph data. We construct a\nunified framework that mathematically formalizes the paradigm of graph SSL.\nAccording to the objectives of pretext tasks, we divide these approaches into\nfour categories: generation-based, auxiliary property-based, contrast-based,\nand hybrid approaches. We further conclude the applications of graph SSL across\nvarious research fields and summarize the commonly used datasets, evaluation\nbenchmark, performance comparison and open-source codes of graph SSL. Finally,\nwe discuss the remaining challenges and potential future directions in this\nresearch field.",
          "link": "http://arxiv.org/abs/2103.00111",
          "publishedOn": "2021-08-06T00:51:47.887Z",
          "wordCount": 654,
          "title": "Graph Self-Supervised Learning: A Survey. (arXiv:2103.00111v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng-Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun-Yan Zhu</a>",
          "description": "Can a user create a deep generative model by sketching a single example?\nTraditionally, creating a GAN model has required the collection of a\nlarge-scale dataset of exemplars and specialized knowledge in deep learning. In\ncontrast, sketching is possibly the most universally accessible way to convey a\nvisual concept. In this work, we present a method, GAN Sketching, for rewriting\nGANs with one or more sketches, to make GANs training easier for novice users.\nIn particular, we change the weights of an original GAN model according to user\nsketches. We encourage the model's output to match the user sketches through a\ncross-domain adversarial loss. Furthermore, we explore different regularization\nmethods to preserve the original model's diversity and image quality.\nExperiments have shown that our method can mold GANs to match shapes and poses\nspecified by sketches while maintaining realism and diversity. Finally, we\ndemonstrate a few applications of the resulting GAN, including latent space\ninterpolation and image editing.",
          "link": "http://arxiv.org/abs/2108.02774",
          "publishedOn": "2021-08-06T00:51:47.880Z",
          "wordCount": 599,
          "title": "Sketch Your Own GAN. (arXiv:2108.02774v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rafee_S/0/1/0/all/0/1\">Syed Rifat Mahmud Rafee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gyorgy Fazekas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiggins_G/0/1/0/all/0/1\">Geraint A.~Wiggins</a>",
          "description": "Music Performers have their own idiosyncratic way of interpreting a musical\npiece. A group of skilled performers playing the same piece of music would\nlikely to inject their unique artistic styles in their performances. The\nvariations of the tempo, timing, dynamics, articulation etc. from the actual\nnotated music are what make the performers unique in their performances. This\nstudy presents a dataset consisting of four movements of Schubert's ``Sonata in\nB-flat major, D.960\" performed by nine virtuoso pianists individually. We\nproposed and extracted a set of expressive features that are able to capture\nthe characteristics of an individual performer's style. We then present a\nperformer identification method based on the similarity of feature\ndistribution, given a set of piano performances. The identification is done\nconsidering each feature individually as well as a fusion of the features.\nResults show that the proposed method achieved a precision of 0.903 using\nfusion features. Moreover, the onset time deviation feature shows promising\nresult when considered individually.",
          "link": "http://arxiv.org/abs/2108.02576",
          "publishedOn": "2021-08-06T00:51:47.874Z",
          "wordCount": 610,
          "title": "Performer Identification From Symbolic Representation of Music Using Statistical Models. (arXiv:2108.02576v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/1909.08074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Assefa_B/0/1/0/all/0/1\">Beakal Gizachew Assefa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozkasap_O/0/1/0/all/0/1\">Oznur Ozkasap</a>",
          "description": "Software Defined Networking (SDN) achieves programmability of a network\nthrough separation of the control and data planes. It enables flexibility in\nnetwork management and control. Energy efficiency is one of the challenging\nglobal problems which has both economic and environmental impact. A massive\namount of information is generated in the controller of an SDN based network.\nMachine learning gives the ability to computers to progressively learn from\ndata without having to write specific instructions. In this work, we propose\nMER-SDN: a machine learning framework for traffic-aware energy efficient\nrouting in SDN. Feature extraction, training, and testing are the three main\nstages of the learning machine. Experiments are conducted on Mininet and POX\ncontroller using real-world network topology and dynamic traffic traces from\nSNDlib. Results show that our approach achieves more than 65\\% feature size\nreduction, more than 70% accuracy in parameter prediction of an energy\nefficient heuristics algorithm, also our prediction refine heuristics converges\nthe predicted value to the optimal parameters values with up to 25X speedup as\ncompared to the brute force method.",
          "link": "http://arxiv.org/abs/1909.08074",
          "publishedOn": "2021-08-06T00:51:47.852Z",
          "wordCount": 672,
          "title": "MER-SDN: Machine Learning Framework for Traffic Aware Energy Efficient Routing in SDN. (arXiv:1909.08074v3 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08176",
          "author": "<a href=\"http://arxiv.org/find/hep-lat/1/au:+Albergo_M/0/1/0/all/0/1\">Michael S. Albergo</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Boyda_D/0/1/0/all/0/1\">Denis Boyda</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Hackett_D/0/1/0/all/0/1\">Daniel C. Hackett</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Kanwar_G/0/1/0/all/0/1\">Gurtej Kanwar</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Cranmer_K/0/1/0/all/0/1\">Kyle Cranmer</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Racaniere_S/0/1/0/all/0/1\">S&#xe9;bastien Racani&#xe8;re</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Rezende_D/0/1/0/all/0/1\">Danilo Jimenez Rezende</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Shanahan_P/0/1/0/all/0/1\">Phiala E. Shanahan</a>",
          "description": "This notebook tutorial demonstrates a method for sampling Boltzmann\ndistributions of lattice field theories using a class of machine learning\nmodels known as normalizing flows. The ideas and approaches proposed in\narXiv:1904.12072, arXiv:2002.02428, and arXiv:2003.06413 are reviewed and a\nconcrete implementation of the framework is presented. We apply this framework\nto a lattice scalar field theory and to U(1) gauge theory, explicitly encoding\ngauge symmetries in the flow-based approach to the latter. This presentation is\nintended to be interactive and working with the attached Jupyter notebook is\nrecommended.",
          "link": "http://arxiv.org/abs/2101.08176",
          "publishedOn": "2021-08-06T00:51:47.845Z",
          "wordCount": 575,
          "title": "Introduction to Normalizing Flows for Lattice Field Theory. (arXiv:2101.08176v2 [hep-lat] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Stephan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trott_A/0/1/0/all/0/1\">Alexander Trott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1\">Sunil Srinivasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parkes_D/0/1/0/all/0/1\">David C. Parkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1\">Richard Socher</a>",
          "description": "AI and reinforcement learning (RL) have improved many areas, but are not yet\nwidely adopted in economic policy design, mechanism design, or economics at\nlarge. At the same time, current economic methodology is limited by a lack of\ncounterfactual data, simplistic behavioral models, and limited opportunities to\nexperiment with policies and evaluate behavioral responses. Here we show that\nmachine-learning-based economic simulation is a powerful policy and mechanism\ndesign framework to overcome these limitations. The AI Economist is a\ntwo-level, deep RL framework that trains both agents and a social planner who\nco-adapt, providing a tractable solution to the highly unstable and novel\ntwo-level RL challenge. From a simple specification of an economy, we learn\nrational agent behaviors that adapt to learned planner policies and vice versa.\nWe demonstrate the efficacy of the AI Economist on the problem of optimal\ntaxation. In simple one-step economies, the AI Economist recovers the optimal\ntax policy of economic theory. In complex, dynamic economies, the AI Economist\nsubstantially improves both utilitarian social welfare and the trade-off\nbetween equality and productivity over baselines. It does so despite emergent\ntax-gaming strategies, while accounting for agent interactions and behavioral\nchange more accurately than economic theory. These results demonstrate for the\nfirst time that two-level, deep RL can be used for understanding and as a\ncomplement to theory for economic design, unlocking a new computational\nlearning-based approach to understanding economic policy.",
          "link": "http://arxiv.org/abs/2108.02755",
          "publishedOn": "2021-08-06T00:51:47.833Z",
          "wordCount": 690,
          "title": "The AI Economist: Optimal Economic Policy Design via Two-level Deep Reinforcement Learning. (arXiv:2108.02755v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00116",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Memarian_F/0/1/0/all/0/1\">Farzan Memarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemi_A/0/1/0/all/0/1\">Abolfazl Hashemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1\">Scott Niekum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>",
          "description": "We explore methodologies to improve the robustness of generative adversarial\nimitation learning (GAIL) algorithms to observation noise. Towards this\nobjective, we study the effect of local Lipschitzness of the discriminator and\nthe generator on the robustness of policies learned by GAIL. In many robotics\napplications, the learned policies by GAIL typically suffer from a degraded\nperformance at test time since the observations from the environment might be\ncorrupted by noise. Hence, robustifying the learned policies against the\nobservation noise is of critical importance. To this end, we propose a\nregularization method to induce local Lipschitzness in the generator and the\ndiscriminator of adversarial imitation learning methods. We show that the\nmodified objective leads to learning significantly more robust policies.\nMoreover, we demonstrate --- both theoretically and experimentally --- that\ntraining a locally Lipschitz discriminator leads to a locally Lipschitz\ngenerator, thereby improving the robustness of the resultant policy. We perform\nextensive experiments on simulated robot locomotion environments from the\nMuJoCo suite that demonstrate the proposed method learns policies that\nsignificantly outperform the state-of-the-art generative adversarial imitation\nlearning algorithm when applied to test scenarios with noise-corrupted\nobservations.",
          "link": "http://arxiv.org/abs/2107.00116",
          "publishedOn": "2021-08-06T00:51:47.826Z",
          "wordCount": 644,
          "title": "Robust Generative Adversarial Imitation Learning via Local Lipschitzness. (arXiv:2107.00116v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tungyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Youting Wang</a>",
          "description": "For the highly imbalanced credit card fraud detection problem, most existing\nmethods either use data augmentation methods or conventional machine learning\nmodels, while neural network-based anomaly detection approaches are lacking.\nFurthermore, few studies have employed AI interpretability tools to investigate\nthe feature importance of transaction data, which is crucial for the black-box\nfraud detection module. Considering these two points together, we propose a\nnovel anomaly detection framework for credit card fraud detection as well as a\nmodel-explaining module responsible for prediction explanations. The fraud\ndetection model is composed of two deep neural networks, which are trained in\nan unsupervised and adversarial manner. Precisely, the generator is an\nAutoEncoder aiming to reconstruct genuine transaction data, while the\ndiscriminator is a fully-connected network for fraud detection. The explanation\nmodule has three white-box explainers in charge of interpretations of the\nAutoEncoder, discriminator, and the whole detection model, respectively.\nExperimental results show the state-of-the-art performances of our fraud\ndetection model on the benchmark dataset compared with baselines. In addition,\nprediction analyses by three explainers are presented, offering a clear\nperspective on how each feature of an instance of interest contributes to the\nfinal model output.",
          "link": "http://arxiv.org/abs/2108.02501",
          "publishedOn": "2021-08-06T00:51:47.820Z",
          "wordCount": 632,
          "title": "Locally Interpretable One-Class Anomaly Detection for Credit Card Fraud Detection. (arXiv:2108.02501v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Velichko_A/0/1/0/all/0/1\">Andrei Velichko</a>",
          "description": "The study presents a new method for analyzing medical data based on the\nLogNNet neural network, which uses chaotic mappings to transform input\ninformation. The technique calculates risk factors for the presence of a\ndisease in a patient according to a set of medical health indicators. The\nLogNNet architecture allows the implementation of artificial intelligence on\nmedical pe-ripherals of the Internet of Things with low RAM resources, and the\ndevelopment of edge computing in healthcare. The efficiency of LogNNet in\nassessing perinatal risk is illustrated on cardiotocogram data of 2126 pregnant\nwomen, obtained from the UC Irvine machine learning repository. The\nclassification accuracy reaches ~ 91%, with the ~ 3-10 kB of RAM used on the\nArduino microcontroller. In addition, examples for diagnosing COVID-19 are\nprovided, using LogNNet trained on a publicly available database from the\nIsraeli Ministry of Health. The service concept has been developed, which uses\nthe data of the express test for COVID-19 and reaches the classification\naccuracy of ~ 95% with the ~ 0.6 kB of RAM used on Arduino microcontrollers. In\nall examples, the model is tested using standard classification quality\nmetrics: Precision, Recall, and F1-measure. The study results can be used in\nclinical decision support systems.",
          "link": "http://arxiv.org/abs/2108.02428",
          "publishedOn": "2021-08-06T00:51:47.801Z",
          "wordCount": 709,
          "title": "A Method for Medical Data Analysis Using the LogNNet for Clinical Decision Support Systems and Edge Computing in Healthcare. (arXiv:2108.02428v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farinhas_A/0/1/0/all/0/1\">Ant&#xf3;nio Farinhas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1\">Wilker Aziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1\">Vlad Niculae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>",
          "description": "Neural networks and other machine learning models compute continuous\nrepresentations, while humans communicate mostly through discrete symbols.\nReconciling these two forms of communication is desirable for generating\nhuman-readable interpretations or learning discrete latent variable models,\nwhile maintaining end-to-end differentiability. Some existing approaches (such\nas the Gumbel-Softmax transformation) build continuous relaxations that are\ndiscrete approximations in the zero-temperature limit, while others (such as\nsparsemax transformations and the Hard Concrete distribution) produce\ndiscrete/continuous hybrids. In this paper, we build rigorous theoretical\nfoundations for these hybrids, which we call \"mixed random variables.\" Our\nstarting point is a new \"direct sum\" base measure defined on the face lattice\nof the probability simplex. From this measure, we introduce new entropy and\nKullback-Leibler divergence functions that subsume the discrete and\ndifferential cases and have interpretations in terms of code optimality. Our\nframework suggests two strategies for representing and sampling mixed random\nvariables, an extrinsic (\"sample-and-project\") and an intrinsic one (based on\nface stratification). We experiment with both approaches on an emergent\ncommunication benchmark and on modeling MNIST and Fashion-MNIST data with\nvariational auto-encoders with mixed latent variables.",
          "link": "http://arxiv.org/abs/2108.02658",
          "publishedOn": "2021-08-06T00:51:47.788Z",
          "wordCount": 607,
          "title": "Sparse Communication via Mixed Distributions. (arXiv:2108.02658v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ochi_V/0/1/0/all/0/1\">Virginia Ochi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Estrada_R/0/1/0/all/0/1\">Ricardo Estrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaji_T/0/1/0/all/0/1\">Teezal Gaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadea_W/0/1/0/all/0/1\">Wendy Gadea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_E/0/1/0/all/0/1\">Emily Duong</a>",
          "description": "Our analysis reviews and visualizes the audio features and popularity of\nsongs streamed on Spotify*. Our dataset, downloaded from Kaggle and originally\nsourced from Spotify API, consists of multiple Excel files containing\ninformation relevant to our visualization and regression analysis. The exercise\nseeks to determine the connection between the popularity of the songs and the\ndanceability. Insights to be included and factored as part of our analysis\ninclude song energy, valence, BPM, release date, and year.",
          "link": "http://arxiv.org/abs/2108.02370",
          "publishedOn": "2021-08-06T00:51:47.749Z",
          "wordCount": 524,
          "title": "Spotify Danceability and Popularity Analysis using SAP. (arXiv:2108.02370v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bennequin_E/0/1/0/all/0/1\">Etienne Bennequin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouvier_V/0/1/0/all/0/1\">Victor Bouvier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tami_M/0/1/0/all/0/1\">Myriam Tami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toubhans_A/0/1/0/all/0/1\">Antoine Toubhans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1\">C&#xe9;line Hudelot</a>",
          "description": "Few-Shot Learning (FSL) algorithms have made substantial progress in learning\nnovel concepts with just a handful of labelled data. To classify query\ninstances from novel classes encountered at test-time, they only require a\nsupport set composed of a few labelled samples. FSL benchmarks commonly assume\nthat those queries come from the same distribution as instances in the support\nset. However, in a realistic set-ting, data distribution is plausibly subject\nto change, a situation referred to as Distribution Shift (DS). The present work\naddresses the new and challenging problem of Few-Shot Learning under\nSupport/Query Shift (FSQS) i.e., when support and query instances are sampled\nfrom related but different distributions. Our contributions are the following.\nFirst, we release a testbed for FSQS, including datasets, relevant baselines\nand a protocol for a rigorous and reproducible evaluation. Second, we observe\nthat well-established FSL algorithms unsurprisingly suffer from a considerable\ndrop in accuracy when facing FSQS, stressing the significance of our study.\nFinally, we show that transductive algorithms can limit the inopportune effect\nof DS. In particular, we study both the role of Batch-Normalization and Optimal\nTransport (OT) in aligning distributions, bridging Unsupervised Domain\nAdaptation with FSL. This results in a new method that efficiently combines OT\nwith the celebrated Prototypical Networks. We bring compelling experiments\ndemonstrating the advantage of our method. Our work opens an exciting line of\nresearch by providing a testbed and strong baselines. Our code is available at\nhttps://github.com/ebennequin/meta-domain-shift.",
          "link": "http://arxiv.org/abs/2105.11804",
          "publishedOn": "2021-08-06T00:51:47.743Z",
          "wordCount": 713,
          "title": "Bridging Few-Shot Learning and Adaptation: New Challenges of Support-Query Shift. (arXiv:2105.11804v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01583",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Woodworth_B/0/1/0/all/0/1\">Blake Woodworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bullins_B/0/1/0/all/0/1\">Brian Bullins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1\">Ohad Shamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srebro_N/0/1/0/all/0/1\">Nathan Srebro</a>",
          "description": "We resolve the min-max complexity of distributed stochastic convex\noptimization (up to a log factor) in the intermittent communication setting,\nwhere $M$ machines work in parallel over the course of $R$ rounds of\ncommunication to optimize the objective, and during each round of\ncommunication, each machine may sequentially compute $K$ stochastic gradient\nestimates. We present a novel lower bound with a matching upper bound that\nestablishes an optimal algorithm.",
          "link": "http://arxiv.org/abs/2102.01583",
          "publishedOn": "2021-08-06T00:51:47.735Z",
          "wordCount": 544,
          "title": "The Min-Max Complexity of Distributed Stochastic Convex Optimization with Intermittent Communication. (arXiv:2102.01583v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.06610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Havasi_M/0/1/0/all/0/1\">Marton Havasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1\">Rodolphe Jenatton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jeremiah Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_J/0/1/0/all/0/1\">Jasper Snoek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Dustin Tran</a>",
          "description": "Recent approaches to efficiently ensemble neural networks have shown that\nstrong robustness and uncertainty performance can be achieved with a negligible\ngain in parameters over the original network. However, these methods still\nrequire multiple forward passes for prediction, leading to a significant\ncomputational cost. In this work, we show a surprising result: the benefits of\nusing multiple predictions can be achieved `for free' under a single model's\nforward pass. In particular, we show that, using a multi-input multi-output\n(MIMO) configuration, one can utilize a single model's capacity to train\nmultiple subnetworks that independently learn the task at hand. By ensembling\nthe predictions made by the subnetworks, we improve model robustness without\nincreasing compute. We observe a significant improvement in negative\nlog-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet,\nand their out-of-distribution variants compared to previous methods.",
          "link": "http://arxiv.org/abs/2010.06610",
          "publishedOn": "2021-08-06T00:51:47.717Z",
          "wordCount": 632,
          "title": "Training independent subnetworks for robust prediction. (arXiv:2010.06610v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ganzfried_S/0/1/0/all/0/1\">Sam Ganzfried</a>",
          "description": "Many real-world games contain parameters which can affect payoffs, action\nspaces, and information states. For fixed values of the parameters, the game\ncan be solved using standard algorithms. However, in many settings agents must\nact without knowing the values of the parameters that will be encountered in\nadvance. Often the decisions must be made by a human under time and resource\nconstraints, and it is unrealistic to assume that a human can solve the game in\nreal time. We present a new framework that enables human decision makers to\nmake fast decisions without the aid of real-time solvers. We demonstrate\napplicability to a variety of situations including settings with multiple\nplayers and imperfect information.",
          "link": "http://arxiv.org/abs/2104.14744",
          "publishedOn": "2021-08-06T00:51:47.711Z",
          "wordCount": 581,
          "title": "Human strategic decision making in parametrized games. (arXiv:2104.14744v2 [cs.GT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02574",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_F/0/1/0/all/0/1\">Fei Wen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1\">Zeyu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ying_R/0/1/0/all/0/1\">Rendong Ying</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peilin Liu</a>",
          "description": "Recently, much progress has been made in unsupervised restoration learning.\nHowever, existing methods more or less rely on some assumptions on the signal\nand/or degradation model, which limits their practical performance. How to\nconstruct an optimal criterion for unsupervised restoration learning without\nany prior knowledge on the degradation model is still an open question. Toward\nanswering this question, this work proposes a criterion for unsupervised\nrestoration learning based on the optimal transport theory. This criterion has\nfavorable properties, e.g., approximately maximal preservation of the\ninformation of the signal, whilst achieving perceptual reconstruction.\nFurthermore, though a relaxed unconstrained formulation is used in practical\nimplementation, we show that the relaxed formulation in theory has the same\nsolution as the original constrained formulation. Experiments on synthetic and\nreal-world data, including realistic photographic, microscopy, depth, and raw\ndepth images, demonstrate that the proposed method even compares favorably with\nsupervised methods, e.g., approaching the PSNR of supervised methods while\nhaving better perceptual quality. Particularly, for spatially correlated noise\nand realistic microscopy images, the proposed method not only achieves better\nperceptual quality but also has higher PSNR than supervised methods. Besides,\nit shows remarkable superiority in harsh practical conditions with complex\nnoise, e.g., raw depth images.",
          "link": "http://arxiv.org/abs/2108.02574",
          "publishedOn": "2021-08-06T00:51:47.704Z",
          "wordCount": 652,
          "title": "Optimal Transport for Unsupervised Restoration Learning. (arXiv:2108.02574v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weijie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang%2A_C/0/1/0/all/0/1\">Chong Wang*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haohe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shenghao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Song Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xulun Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiafei Wu</a>",
          "description": "Expensive bounding-box annotations have limited the development of object\ndetection task. Thus, it is necessary to focus on more challenging task of\nfew-shot object detection. It requires the detector to recognize objects of\nnovel classes with only a few training samples. Nowadays, many existing popular\nmethods based on meta-learning have achieved promising performance, such as\nMeta R-CNN series. However, only a single category of support data is used as\nthe attention to guide the detecting of query images each time. Their relevance\nto each other remains unexploited. Moreover, a lot of recent works treat the\nsupport data and query images as independent branch without considering the\nrelationship between them. To address this issue, we propose a dynamic\nrelevance learning model, which utilizes the relationship between all support\nimages and Region of Interest (RoI) on the query images to construct a dynamic\ngraph convolutional network (GCN). By adjusting the prediction distribution of\nthe base detector using the output of this GCN, the proposed model can guide\nthe detector to improve the class representation implicitly. Comprehensive\nexperiments have been conducted on Pascal VOC and MS-COCO dataset. The proposed\nmodel achieves the best overall performance, which shows its effectiveness of\nlearning more generalized features. Our code is available at\nhttps://github.com/liuweijie19980216/DRL-for-FSOD.",
          "link": "http://arxiv.org/abs/2108.02235",
          "publishedOn": "2021-08-06T00:51:47.697Z",
          "wordCount": 664,
          "title": "Dynamic Relevance Learning for Few-Shot Object Detection. (arXiv:2108.02235v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1\">Juan P. Vigueras-Guill&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_A/0/1/0/all/0/1\">Arijit Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engkvist_O/0/1/0/all/0/1\">Ola Engkvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1\">Frank Seeliger</a>",
          "description": "Capsule Networks (CapsNets) is a machine learning architecture proposed to\novercome some of the shortcomings of convolutional neural networks (CNNs).\nHowever, CapsNets have mainly outperformed CNNs in datasets where images are\nsmall and/or the objects to identify have minimal background noise. In this\nwork, we present a new architecture, parallel CapsNets, which exploits the\nconcept of branching the network to isolate certain capsules, allowing each\nbranch to identify different entities. We applied our concept to the two\ncurrent types of CapsNet architectures, studying the performance for networks\nwith different layers of capsules. We tested our design in a public, highly\nunbalanced dataset of acute myeloid leukaemia images (15 classes). Our\nexperiments showed that conventional CapsNets show similar performance than our\nbaseline CNN (ResNeXt-50) but depict instability problems. In contrast,\nparallel CapsNets can outperform ResNeXt-50, is more stable, and shows better\nrotational invariance than both, conventional CapsNets and ResNeXt-50.",
          "link": "http://arxiv.org/abs/2108.02644",
          "publishedOn": "2021-08-06T00:51:47.691Z",
          "wordCount": 607,
          "title": "Parallel Capsule Networks for Classification of White Blood Cells. (arXiv:2108.02644v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2001.02309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kjos_Hanssen_B/0/1/0/all/0/1\">Bj&#xf8;rn Kjos-Hanssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felix_C/0/1/0/all/0/1\">Clyde James Felix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sun Young Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamb_E/0/1/0/all/0/1\">Ethan Lamb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_D/0/1/0/all/0/1\">Davin Takahashi</a>",
          "description": "Let $NFA_b(q)$ denote the set of languages accepted by nondeterministic\nfinite automata with $q$ states over an alphabet with $b$ letters. Let $B_n$\ndenote the set of words of length $n$. We give a quadratic lower bound on the\nVC dimension of \\[\n\nNFA_2(q)\\cap B_n = \\{L\\cap B_n \\mid L \\in NFA_2(q)\\} \\] as a function of $q$.\n\nNext, the work of Gruber and Holzer (2007) gives an upper bound for the\nnondeterministic state complexity of finite languages contained in $B_n$, which\nwe strengthen using our methods.\n\nFinally, we give some theoretical and experimental results on the dependence\non $n$ of the VC dimension and testing dimension of $NFA_2(q)\\cap B_n$.",
          "link": "http://arxiv.org/abs/2001.02309",
          "publishedOn": "2021-08-06T00:51:47.679Z",
          "wordCount": 610,
          "title": "VC-dimensions of nondeterministic finite automata for words of equal length. (arXiv:2001.02309v2 [cs.FL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.00964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheikhalishahi_S/0/1/0/all/0/1\">Seyedmostafa Sheikhalishahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaraman_V/0/1/0/all/0/1\">Vevake Balaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osmani_V/0/1/0/all/0/1\">Venet Osmani</a>",
          "description": "Progress of machine learning in critical care has been difficult to track, in\npart due to absence of public benchmarks. Other fields of research (such as\ncomputer vision and natural language processing) have established various\ncompetitions and public benchmarks. Recent availability of large clinical\ndatasets has enabled the possibility of establishing public benchmarks. Taking\nadvantage of this opportunity, we propose a public benchmark suite to address\nfour areas of critical care, namely mortality prediction, estimation of length\nof stay, patient phenotyping and risk of decompensation. We define each task\nand compare the performance of both clinical models as well as baseline and\ndeep learning models using eICU critical care dataset of around 73,000\npatients. This is the first public benchmark on a multi-centre critical care\ndataset, comparing the performance of clinical gold standard with our\npredictive model. We also investigate the impact of numerical variables as well\nas handling of categorical variables on each of the defined tasks. The source\ncode, detailing our methods and experiments is publicly available such that\nanyone can replicate our results and build upon our work.",
          "link": "http://arxiv.org/abs/1910.00964",
          "publishedOn": "2021-08-06T00:51:47.616Z",
          "wordCount": 664,
          "title": "Benchmarking machine learning models on multi-centre eICU critical care dataset. (arXiv:1910.00964v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alves_G/0/1/0/all/0/1\">Guilherme Alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amblard_M/0/1/0/all/0/1\">Maxime Amblard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernier_F/0/1/0/all/0/1\">Fabien Bernier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couceiro_M/0/1/0/all/0/1\">Miguel Couceiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Napoli_A/0/1/0/all/0/1\">Amedeo Napoli</a>",
          "description": "Unintended biases in machine learning (ML) models are among the major\nconcerns that must be addressed to maintain public trust in ML. In this paper,\nwe address process fairness of ML models that consists in reducing the\ndependence of models on sensitive features, without compromising their\nperformance. We revisit the framework FixOut that is inspired in the approach\n\"fairness through unawareness\" to build fairer models. We introduce several\nimprovements such as automating the choice of FixOut's parameters. Also, FixOut\nwas originally proposed to improve fairness of ML models on tabular data. We\nalso demonstrate the feasibility of FixOut's workflow for models on textual\ndata. We present several experimental results that illustrate the fact that\nFixOut improves process fairness on different classification settings.",
          "link": "http://arxiv.org/abs/2108.02662",
          "publishedOn": "2021-08-06T00:51:47.605Z",
          "wordCount": 586,
          "title": "Reducing Unintended Bias of ML Models on Tabular and Textual Data. (arXiv:2108.02662v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatti_A/0/1/0/all/0/1\">Anubhav Bhatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behinaein_B/0/1/0/all/0/1\">Behnam Behinaein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodenburg_D/0/1/0/all/0/1\">Dirk Rodenburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hungler_P/0/1/0/all/0/1\">Paul Hungler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>",
          "description": "Classification of human emotions can play an essential role in the design and\nimprovement of human-machine systems. While individual biological signals such\nas Electrocardiogram (ECG) and Electrodermal Activity (EDA) have been widely\nused for emotion recognition with machine learning methods, multimodal\napproaches generally fuse extracted features or final classification/regression\nresults to boost performance. To enhance multimodal learning, we present a\nnovel attentive cross-modal connection to share information between\nconvolutional neural networks responsible for learning individual modalities.\nSpecifically, these connections improve emotion classification by sharing\nintermediate representations among EDA and ECG and apply attention weights to\nthe shared information, thus learning more effective multimodal embeddings. We\nperform experiments on the WESAD dataset to identify the best configuration of\nthe proposed method for emotion classification. Our experiments show that the\nproposed approach is capable of learning strong multimodal representations and\noutperforms a number of baselines methods.",
          "link": "http://arxiv.org/abs/2108.02241",
          "publishedOn": "2021-08-06T00:51:47.598Z",
          "wordCount": 603,
          "title": "Attentive Cross-modal Connections for Deep Multimodal Wearable-based Emotion Recognition. (arXiv:2108.02241v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Donevski_I/0/1/0/all/0/1\">Igor Donevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_J/0/1/0/all/0/1\">Jimmy Jessen Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popovski_P/0/1/0/all/0/1\">Petar Popovski</a>,",
          "description": "In this paper we envision a federated learning (FL) scenario in service of\namending the performance of autonomous road vehicles, through a drone traffic\nmonitor (DTM), that also acts as an orchestrator. Expecting non-IID data\ndistribution, we focus on the issue of accelerating the learning of a\nparticular class of critical object (CO), that may harm the nominal operation\nof an autonomous vehicle. This can be done through proper allocation of the\nwireless resources for addressing learner and data heterogeneity. Thus, we\npropose a reactive method for the allocation of wireless resources, that\nhappens dynamically each FL round, and is based on each learner's contribution\nto the general model. In addition to this, we explore the use of static methods\nthat remain constant across all rounds. Since we expect partial work from each\nlearner, we use the FedProx FL algorithm, in the task of computer vision. For\ntesting, we construct a non-IID data distribution of the MNIST and FMNIST\ndatasets among four types of learners, in scenarios that represent the quickly\nchanging environment. The results show that proactive measures are effective\nand versatile at improving system accuracy, and quickly learning the CO class\nwhen underrepresented in the network. Furthermore, the experiments show a\ntradeoff between FedProx intensity and resource allocation efforts.\nNonetheless, a well adjusted FedProx local optimizer allows for an even better\noverall accuracy, particularly when using deeper neural network (NN)\nimplementations.",
          "link": "http://arxiv.org/abs/2108.02712",
          "publishedOn": "2021-08-06T00:51:47.589Z",
          "wordCount": 691,
          "title": "On Addressing Heterogeneity in Federated Learning for Autonomous Vehicles Connected to a Drone Orchestrator. (arXiv:2108.02712v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wagenmaker_A/0/1/0/all/0/1\">Andrew Wagenmaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1\">Max Simchowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1\">Kevin Jamieson</a>",
          "description": "The theory of reinforcement learning has focused on two fundamental problems:\nachieving low regret, and identifying $\\epsilon$-optimal policies. While a\nsimple reduction allows one to apply a low-regret algorithm to obtain an\n$\\epsilon$-optimal policy and achieve the worst-case optimal rate, it is\nunknown whether low-regret algorithms can obtain the instance-optimal rate for\npolicy identification. We show that this is not possible -- there exists a\nfundamental tradeoff between achieving low regret and identifying an\n$\\epsilon$-optimal policy at the instance-optimal rate.\n\nMotivated by our negative finding, we propose a new measure of\ninstance-dependent sample complexity for PAC tabular reinforcement learning\nwhich explicitly accounts for the attainable state visitation distributions in\nthe underlying MDP. We then propose and analyze a novel, planning-based\nalgorithm which attains this sample complexity -- yielding a complexity which\nscales with the suboptimality gaps and the ``reachability'' of a state. We show\nthat our algorithm is nearly minimax optimal, and on several examples that our\ninstance-dependent sample complexity offers significant improvements over\nworst-case bounds.",
          "link": "http://arxiv.org/abs/2108.02717",
          "publishedOn": "2021-08-06T00:51:47.581Z",
          "wordCount": 597,
          "title": "Beyond No Regret: Instance-Dependent PAC Reinforcement Learning. (arXiv:2108.02717v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02570",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Kim_M/0/1/0/all/0/1\">Minhong Kim</a>",
          "description": "In this paper, machine learning models are used to predict outcomes for\npatients with persistent post-concussion syndrome (PCS). Patients had sustained\na concussion at an average of two to three months before the study. By\nutilizing assessed data, the machine learning models aimed to predict whether\nor not a patient would continue to have PCS after four to five months. The\nrandom forest classifier achieved the highest performance with an 85% accuracy\nand an area under the receiver operating characteristic curve (AUC) of 0.94.\nFactors found to be predictive of PCS outcome were Post-Traumatic Stress\nDisorder (PTSD), perceived injustice, self-rated prognosis, and symptom\nseverity post-injury. The results of this study demonstrate that machine\nlearning models can predict PCS outcomes with high accuracy. With further\nresearch, machine learning models may be implemented in healthcare settings to\nhelp patients with persistent PCS.",
          "link": "http://arxiv.org/abs/2108.02570",
          "publishedOn": "2021-08-06T00:51:47.564Z",
          "wordCount": 567,
          "title": "Predicting Post-Concussion Syndrome Outcomes with Machine Learning. (arXiv:2108.02570v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02431",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Watanabe_C/0/1/0/all/0/1\">Chihiro Watanabe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1\">Taiji Suzuki</a>",
          "description": "Linear layouts are a graph visualization method that can be used to capture\nan entry pattern in an adjacency matrix of a given graph. By reordering the\nnode indices of the original adjacency matrix, linear layouts provide knowledge\nof latent graph structures. Conventional linear layout methods commonly aim to\nfind an optimal reordering solution based on predefined features of a given\nmatrix and loss function. However, prior knowledge of the appropriate features\nto use or structural patterns in a given adjacency matrix is not always\navailable. In such a case, performing the reordering based on data-driven\nfeature extraction without assuming a specific structure in an adjacency matrix\nis preferable. Recently, a neural-network-based matrix reordering method called\nDeepTMR has been proposed to perform this function. However, it is limited to a\ntwo-mode reordering (i.e., the rows and columns are reordered separately) and\nit cannot be applied in the one-mode setting (i.e., the same node order is used\nfor reordering both rows and columns), owing to the characteristics of its\nmodel architecture. In this study, we extend DeepTMR and propose a new one-mode\nlinear layout method referred to as AutoLL. We developed two types of neural\nnetwork models, AutoLL-D and AutoLL-U, for reordering directed and undirected\nnetworks, respectively. To perform one-mode reordering, these AutoLL models\nhave specific encoder architectures, which extract node features from an\nobserved adjacency matrix. We conducted both qualitative and quantitative\nevaluations of the proposed approach, and the experimental results demonstrate\nits effectiveness.",
          "link": "http://arxiv.org/abs/2108.02431",
          "publishedOn": "2021-08-06T00:51:47.558Z",
          "wordCount": 680,
          "title": "AutoLL: Automatic Linear Layout of Graphs based on Deep Neural Network. (arXiv:2108.02431v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hengyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1\">Adam Lerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Brandon Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">David Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1\">Luis Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1\">Noam Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1\">Jakob Foerster</a>",
          "description": "The standard problem setting in Dec-POMDPs is self-play, where the goal is to\nfind a set of policies that play optimally together. Policies learned through\nself-play may adopt arbitrary conventions and implicitly rely on multi-step\nreasoning based on fragile assumptions about other agents' actions and thus\nfail when paired with humans or independently trained agents at test time. To\naddress this, we present off-belief learning (OBL). At each timestep OBL agents\nfollow a policy $\\pi_1$ that is optimized assuming past actions were taken by a\ngiven, fixed policy ($\\pi_0$), but assuming that future actions will be taken\nby $\\pi_1$. When $\\pi_0$ is uniform random, OBL converges to an optimal policy\nthat does not rely on inferences based on other agents' behavior (an optimal\ngrounded policy). OBL can be iterated in a hierarchy, where the optimal policy\nfrom one level becomes the input to the next, thereby introducing multi-level\ncognitive reasoning in a controlled manner. Unlike existing approaches, which\nmay converge to any equilibrium policy, OBL converges to a unique policy,\nmaking it suitable for zero-shot coordination (ZSC). OBL can be scaled to\nhigh-dimensional settings with a fictitious transition mechanism and shows\nstrong performance in both a toy-setting and the benchmark human-AI & ZSC\nproblem Hanabi.",
          "link": "http://arxiv.org/abs/2103.04000",
          "publishedOn": "2021-08-06T00:51:47.552Z",
          "wordCount": 681,
          "title": "Off-Belief Learning. (arXiv:2103.04000v4 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02507",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ge_S/0/1/0/all/0/1\">Shufei Ge</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1\">Shijia Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Elliott_L/0/1/0/all/0/1\">Lloyd Elliott</a>",
          "description": "Shape modelling (with methods that output shapes) is a new and important task\nin Bayesian nonparametrics and bioinformatics. In this work, we focus on\nBayesian nonparametric methods for capturing shapes by partitioning a space\nusing curves. In related work, the classical Mondrian process is used to\npartition spaces recursively with axis-aligned cuts, and is widely applied in\nmulti-dimensional and relational data. The Mondrian process outputs\nhyper-rectangles. Recently, the random tessellation process was introduced as a\ngeneralization of the Mondrian process, partitioning a domain with non-axis\naligned cuts in an arbitrary dimensional space, and outputting polytopes.\nMotivated by these processes, in this work, we propose a novel parallelized\nBayesian nonparametric approach to partition a domain with curves, enabling\ncomplex data-shapes to be acquired. We apply our method to HIV-1-infected human\nmacrophage image dataset, and also simulated datasets sets to illustrate our\napproach. We compare to support vector machines, random forests and\nstate-of-the-art computer vision methods such as simple linear iterative\nclustering super pixel image segmentation. We develop an R package that is\navailable at\n\\url{https://github.com/ShufeiGe/Shape-Modeling-with-Spline-Partitions}.",
          "link": "http://arxiv.org/abs/2108.02507",
          "publishedOn": "2021-08-06T00:51:47.545Z",
          "wordCount": 601,
          "title": "Shape Modeling with Spline Partitions. (arXiv:2108.02507v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_N/0/1/0/all/0/1\">Naili Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1\">Sai Ho Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chenghao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_T/0/1/0/all/0/1\">Teck Khim Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meihui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Gang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ooi_B/0/1/0/all/0/1\">Beng Chin Ooi</a>",
          "description": "Deep learning has achieved great success in a wide spectrum of multimedia\napplications such as image classification, natural language processing and\nmultimodal data analysis. Recent years have seen the development of many deep\nlearning frameworks that provide a high-level programming interface for users\nto design models, conduct training and deploy inference. However, it remains\nchallenging to build an efficient end-to-end multimedia application with most\nexisting frameworks. Specifically, in terms of usability, it is demanding for\nnon-experts to implement deep learning models, obtain the right settings for\nthe entire machine learning pipeline, manage models and datasets, and exploit\nexternal data sources all together. Further, in terms of adaptability, elastic\ncomputation solutions are much needed as the actual serving workload fluctuates\nconstantly, and scaling the hardware resources to handle the fluctuating\nworkload is typically infeasible. To address these challenges, we introduce\nSINGA-Easy, a new deep learning framework that provides distributed\nhyper-parameter tuning at the training stage, dynamic computational cost\ncontrol at the inference stage, and intuitive user interactions with multimedia\ncontents facilitated by model explanation. Our experiments on the training and\ndeployment of multi-modality data analysis applications show that the framework\nis both usable and adaptable to dynamic inference loads. We implement\nSINGA-Easy on top of Apache SINGA and demonstrate our system with the entire\nmachine learning life cycle.",
          "link": "http://arxiv.org/abs/2108.02572",
          "publishedOn": "2021-08-06T00:51:47.491Z",
          "wordCount": 671,
          "title": "SINGA-Easy: An Easy-to-Use Framework for MultiModal Analysis. (arXiv:2108.02572v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02606",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rixner_M/0/1/0/all/0/1\">Maximilian Rixner</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Koutsourelakis_P/0/1/0/all/0/1\">Phaedon-Stelios Koutsourelakis</a>",
          "description": "While the forward and backward modeling of the process-structure-property\nchain has received a lot of attention from the materials community, fewer\nefforts have taken into consideration uncertainties. Those arise from a\nmultitude of sources and their quantification and integration in the inversion\nprocess are essential in meeting the materials design objectives. The first\ncontribution of this paper is a flexible, fully probabilistic formulation of\nsuch optimization problems that accounts for the uncertainty in the\nprocess-structure and structure-property linkages and enables the\nidentification of optimal, high-dimensional, process parameters. We employ a\nprobabilistic, data-driven surrogate for the structure-property link which\nexpedites computations and enables handling of non-differential objectives. We\ncouple this with a novel active learning strategy, i.e. a self-supervised\ncollection of data, which significantly improves accuracy while requiring small\namounts of training data. We demonstrate its efficacy in optimizing the\nmechanical and thermal properties of two-phase, random media but envision its\napplicability encompasses a wide variety of microstructure-sensitive design\nproblems.",
          "link": "http://arxiv.org/abs/2108.02606",
          "publishedOn": "2021-08-06T00:51:47.485Z",
          "wordCount": 593,
          "title": "Self-supervised optimization of random material microstructures in the small-data regime. (arXiv:2108.02606v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lones_M/0/1/0/all/0/1\">Michael A. Lones</a>",
          "description": "This document gives a concise outline of some of the common mistakes that\noccur when using machine learning techniques, and what can be done to avoid\nthem. It is intended primarily as a guide for research students, and focuses on\nissues that are of particular concern within academic research, such as the\nneed to do rigorous comparisons and reach valid conclusions. It covers five\nstages of the machine learning process: what to do before model building, how\nto reliably build models, how to robustly evaluate models, how to compare\nmodels fairly, and how to report results.",
          "link": "http://arxiv.org/abs/2108.02497",
          "publishedOn": "2021-08-06T00:51:47.458Z",
          "wordCount": 530,
          "title": "How to avoid machine learning pitfalls: a guide for academic researchers. (arXiv:2108.02497v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02221",
          "author": "<a href=\"http://arxiv.org/find/hep-th/1/au:+Erbin_H/0/1/0/all/0/1\">Harold Erbin</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Finotello_R/0/1/0/all/0/1\">Riccardo Finotello</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Schneider_R/0/1/0/all/0/1\">Robin Schneider</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Tamaazousti_M/0/1/0/all/0/1\">Mohamed Tamaazousti</a>",
          "description": "We continue earlier efforts in computing the dimensions of tangent space\ncohomologies of Calabi-Yau manifolds using deep learning. In this paper, we\nconsider the dataset of all Calabi-Yau four-folds constructed as complete\nintersections in products of projective spaces. Employing neural networks\ninspired by state-of-the-art computer vision architectures, we improve earlier\nbenchmarks and demonstrate that all four non-trivial Hodge numbers can be\nlearned at the same time using a multi-task architecture. With 30% (80%)\ntraining ratio, we reach an accuracy of 100% for $h^{(1,1)}$ and 97% for\n$h^{(2,1)}$ (100% for both), 81% (96%) for $h^{(3,1)}$, and 49% (83%) for\n$h^{(2,2)}$. Assuming that the Euler number is known, as it is easy to compute,\nand taking into account the linear constraint arising from index computations,\nwe get 100% total accuracy.",
          "link": "http://arxiv.org/abs/2108.02221",
          "publishedOn": "2021-08-06T00:51:47.441Z",
          "wordCount": 569,
          "title": "Deep multi-task mining Calabi-Yau four-folds. (arXiv:2108.02221v1 [hep-th])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendes_P/0/1/0/all/0/1\">Pedro Mendes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casimiro_M/0/1/0/all/0/1\">Maria Casimiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romano_P/0/1/0/all/0/1\">Paolo Romano</a>",
          "description": "In the literature on hyper-parameter tuning, a number of recent solutions\nrely on low-fidelity observations (e.g., training with sub-sampled datasets or\nfor short periods of time) to extrapolate good configurations to use when\nperforming full training. Among these, HyperBand is arguably one of the most\npopular solutions, due to its efficiency and theoretically provable robustness.\nIn this work, we introduce HyperJump, a new approach that builds on HyperBand's\nrobust search strategy and complements it with novel model-based risk analysis\ntechniques that accelerate the search by jumping the evaluation of low risk\nconfigurations, i.e., configurations that are likely to be discarded by\nHyperBand. We evaluate HyperJump on a suite of hyper-parameter optimization\nproblems and show that it provides over one-order of magnitude speed-ups on a\nvariety of deep-learning and kernel-based learning problems when compared to\nHyperBand as well as to a number of state of the art optimizers.",
          "link": "http://arxiv.org/abs/2108.02479",
          "publishedOn": "2021-08-06T00:51:47.433Z",
          "wordCount": 570,
          "title": "HyperJump: Accelerating HyperBand via Risk Modelling. (arXiv:2108.02479v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.00968",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Berrisch_J/0/1/0/all/0/1\">Jonathan Berrisch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ziel_F/0/1/0/all/0/1\">Florian Ziel</a>",
          "description": "Combination and aggregation techniques can significantly improve forecast\naccuracy. This also holds for probabilistic forecasting methods where\npredictive distributions are combined. There are several time-varying and\nadaptive weighting schemes such as Bayesian model averaging (BMA). However, the\nquality of different forecasts may vary not only over time but also within the\ndistribution. For example, some distribution forecasts may be more accurate in\nthe center of the distributions, while others are better at predicting the\ntails. Therefore, we introduce a new weighting method that considers the\ndifferences in performance over time and within the distribution. We discuss\npointwise combination algorithms based on aggregation across quantiles that\noptimize with respect to the continuous ranked probability score (CRPS). After\nanalyzing the theoretical properties of pointwise CRPS learning, we discuss B-\nand P-Spline-based estimation techniques for batch and online learning, based\non quantile regression and prediction with expert advice. We prove that the\nproposed fully adaptive Bernstein online aggregation (BOA) method for pointwise\nCRPS online learning has optimal convergence properties. They are confirmed in\nsimulations and a probabilistic forecasting study for European emission\nallowance (EUA) prices.",
          "link": "http://arxiv.org/abs/2102.00968",
          "publishedOn": "2021-08-06T00:51:47.426Z",
          "wordCount": 661,
          "title": "CRPS Learning. (arXiv:2102.00968v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khashin_S/0/1/0/all/0/1\">Sergey Khashin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shemyakova_E/0/1/0/all/0/1\">Ekaterina Shemyakova</a>",
          "description": "We propose a new kind of automatic architecture search algorithm. The\nalgorithm alternates pruning connections and adding neurons, and it is not\nrestricted to layered architectures only. Here architecture is an arbitrary\noriented graph with some weights (along with some biases and an activation\nfunction), so there may be no layered structure in such a network. The\nalgorithm minimizes the complexity of staying within a given error. We\ndemonstrate our algorithm on the brightness prediction problem of the next\npoint through the previous points on an image. Our second test problem is the\napproximation of the bivariate function defining the brightness of a black and\nwhite image. Our optimized networks significantly outperform the standard\nsolution for neural network architectures in both cases.",
          "link": "http://arxiv.org/abs/2108.02231",
          "publishedOn": "2021-08-06T00:51:47.399Z",
          "wordCount": 555,
          "title": "Growing an architecture for a neural network. (arXiv:2108.02231v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1\">Juan P. Vigueras-Guill&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1\">Frank Seeliger</a>",
          "description": "The automated segmentation of cancer tissue in histopathology images can help\nclinicians to detect, diagnose, and analyze such disease. Different from other\nnatural images used in many convolutional networks for benchmark,\nhistopathology images can be extremely large, and the cancerous patterns can\nreach beyond 1000 pixels. Therefore, the well-known networks in the literature\nwere never conceived to handle these peculiarities. In this work, we propose a\nFully Convolutional DenseUNet that is particularly designed to solve\nhistopathology problems. We evaluated our network in two public pathology\ndatasets published as challenges in the recent MICCAI 2019: binary segmentation\nin colon cancer images (DigestPath2019), and multi-class segmentation in\nprostate cancer images (Gleason2019), achieving similar and better results than\nthe winners of the challenges, respectively. Furthermore, we discussed some\ngood practices in the training setup to yield the best performance and the main\nchallenges in these histopathology datasets.",
          "link": "http://arxiv.org/abs/2108.02676",
          "publishedOn": "2021-08-06T00:51:47.393Z",
          "wordCount": 609,
          "title": "Redesigning Fully Convolutional DenseUNets for Large Histopathology Images. (arXiv:2108.02676v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asi_H/0/1/0/all/0/1\">Hilal Asi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_D/0/1/0/all/0/1\">Daniel Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duchi_J/0/1/0/all/0/1\">John Duchi</a>",
          "description": "We develop algorithms for private stochastic convex optimization that adapt\nto the hardness of the specific function we wish to optimize. While previous\nwork provide worst-case bounds for arbitrary convex functions, it is often the\ncase that the function at hand belongs to a smaller class that enjoys faster\nrates. Concretely, we show that for functions exhibiting $\\kappa$-growth around\nthe optimum, i.e., $f(x) \\ge f(x^*) + \\lambda \\kappa^{-1} \\|x-x^*\\|_2^\\kappa$\nfor $\\kappa > 1$, our algorithms improve upon the standard\n${\\sqrt{d}}/{n\\varepsilon}$ privacy rate to the faster\n$({\\sqrt{d}}/{n\\varepsilon})^{\\tfrac{\\kappa}{\\kappa - 1}}$. Crucially, they\nachieve these rates without knowledge of the growth constant $\\kappa$ of the\nfunction. Our algorithms build upon the inverse sensitivity mechanism, which\nadapts to instance difficulty (Asi & Duchi, 2020), and recent localization\ntechniques in private optimization (Feldman et al., 2020). We complement our\nalgorithms with matching lower bounds for these function classes and\ndemonstrate that our adaptive algorithm is \\emph{simultaneously} (minimax)\noptimal over all $\\kappa \\ge 1+c$ whenever $c = \\Theta(1)$.",
          "link": "http://arxiv.org/abs/2108.02391",
          "publishedOn": "2021-08-06T00:51:47.369Z",
          "wordCount": 613,
          "title": "Adapting to Function Difficulty and Growth Conditions in Private Optimization. (arXiv:2108.02391v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.14621",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Williamson_S/0/1/0/all/0/1\">Sinead A. Williamson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Henderson_J/0/1/0/all/0/1\">Jette Henderson</a>",
          "description": "Understanding how two datasets differ can help us determine whether one\ndataset under-represents certain sub-populations, and provides insights into\nhow well models will generalize across datasets. Representative points selected\nby a maximum mean discrepency (MMD) coreset can provide interpretable summaries\nof a single dataset, but are not easily compared across datasets. In this paper\nwe introduce dependent MMD coresets, a data summarization method for\ncollections of datasets that facilitates comparison of distributions. We show\nthat dependent MMD coresets are useful for understanding multiple related\ndatasets and understanding model generalization between such datasets.",
          "link": "http://arxiv.org/abs/2006.14621",
          "publishedOn": "2021-08-06T00:51:47.344Z",
          "wordCount": 544,
          "title": "Understanding collections of related datasets using dependent MMD coresets. (arXiv:2006.14621v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaming Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shaohui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1\">Qi Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zidong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunji Chen</a>",
          "description": "Policy gradient methods are appealing in deep reinforcement learning but\nsuffer from high variance of gradient estimate. To reduce the variance, the\nstate value function is applied commonly. However, the effect of the state\nvalue function becomes limited in stochastic dynamic environments, where the\nunexpected state dynamics and rewards will increase the variance. In this\npaper, we propose to replace the state value function with a novel hindsight\nvalue function, which leverages the information from the future to reduce the\nvariance of the gradient estimate for stochastic dynamic environments.\n\nParticularly, to obtain an ideally unbiased gradient estimate, we propose an\ninformation-theoretic approach, which optimizes the embeddings of the future to\nbe independent of previous actions. In our experiments, we apply the proposed\nhindsight value function in stochastic dynamic environments, including\ndiscrete-action environments and continuous-action environments. Compared with\nthe standard state value function, the proposed hindsight value function\nconsistently reduces the variance, stabilizes the training, and improves the\neventual policy.",
          "link": "http://arxiv.org/abs/2107.12216",
          "publishedOn": "2021-08-06T00:51:47.329Z",
          "wordCount": 642,
          "title": "Hindsight Value Function for Variance Reduction in Stochastic Dynamic Environment. (arXiv:2107.12216v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>",
          "description": "Despite tremendous progress in missing data imputation task, designing new\nimputation models has become more and more cumbersome but the corresponding\ngains are relatively small. Is there any simple but general approach that can\nexploit the existing models to further improve the quality of the imputation?\nIn this article, we aim to respond to this concern and propose a novel general\ndata augmentation method called Missingness Augmentation (MA), which can be\napplied in many existing generative imputation frameworks to further improve\nthe performance of these models. For MA, before each training epoch, we use the\noutputs of the generator to expand the incomplete samples on the fly, and then\ndetermine a special reconstruction loss for these augmented samples. This\nreconstruction loss plus the original loss constitutes the final optimization\nobjective of the model. It is noteworthy that MA is very efficient and does not\nneed to change the structure of the original model. Experimental results\ndemonstrate that MA can significantly improve the performance of many recently\ndeveloped generative imputation models on a variety of datasets. Our code is\navailable at https://github.com/WYu-Feng/Missingness-Augmentation.",
          "link": "http://arxiv.org/abs/2108.02566",
          "publishedOn": "2021-08-06T00:51:47.242Z",
          "wordCount": 620,
          "title": "Missingness Augmentation: A General Approach for Improving Generative Imputation Models. (arXiv:2108.02566v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_K/0/1/0/all/0/1\">Kaiming Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chih-Hang J. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Arieh_D/0/1/0/all/0/1\">David Ben-Arieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1\">Ashesh Sinha</a>",
          "description": "At present, high-dimensional global optimization problems with time-series\nmodels have received much attention from engineering fields. Since it was\nproposed, Bayesian optimization has quickly become a popular and promising\napproach for solving global optimization problems. However, the standard\nBayesian optimization algorithm is insufficient to solving the global optimal\nsolution when the model is high-dimensional. Hence, this paper presents a novel\nhigh dimensional Bayesian optimization algorithm by considering dimension\nreduction and different dimension fill-in strategies. Most existing literature\nabout Bayesian optimization algorithms did not discuss the sampling strategies\nto optimize the acquisition function. This study proposed a new sampling method\nbased on both the multi-armed bandit and random search methods while optimizing\nthe acquisition function. Besides, based on the time-dependent or\ndimension-dependent characteristics of the model, the proposed algorithm can\nreduce the dimension evenly. Then, five different dimension fill-in strategies\nwere discussed and compared in this study. Finally, to increase the final\naccuracy of the optimal solution, the proposed algorithm adds a local search\nbased on a series of Adam-based steps at the final stage. Our computational\nexperiments demonstrated that the proposed Bayesian optimization algorithm\ncould achieve reasonable solutions with excellent performances for high\ndimensional global optimization problems with a time-series optimal control\nmodel.",
          "link": "http://arxiv.org/abs/2108.02289",
          "publishedOn": "2021-08-06T00:51:47.223Z",
          "wordCount": 659,
          "title": "High dimensional Bayesian Optimization Algorithm for Complex System in Time Series. (arXiv:2108.02289v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.02703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1\">Stephen Bates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1\">Anastasios Angelopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1\">Lihua Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "While improving prediction accuracy has been the focus of machine learning in\nrecent years, this alone does not suffice for reliable decision-making.\nDeploying learning systems in consequential settings also requires calibrating\nand communicating the uncertainty of predictions. To convey instance-wise\nuncertainty for prediction tasks, we show how to generate set-valued\npredictions from a black-box predictor that control the expected loss on future\ntest points at a user-specified level. Our approach provides explicit\nfinite-sample guarantees for any dataset by using a holdout set to calibrate\nthe size of the prediction sets. This framework enables simple,\ndistribution-free, rigorous error control for many tasks, and we demonstrate it\nin five large-scale machine learning problems: (1) classification problems\nwhere some mistakes are more costly than others; (2) multi-label\nclassification, where each observation has multiple associated labels; (3)\nclassification problems where the labels have a hierarchical structure; (4)\nimage segmentation, where we wish to predict a set of pixels containing an\nobject of interest; and (5) protein structure prediction. Lastly, we discuss\nextensions to uncertainty quantification for ranking, metric learning and\ndistributionally robust learning.",
          "link": "http://arxiv.org/abs/2101.02703",
          "publishedOn": "2021-08-06T00:51:47.208Z",
          "wordCount": 677,
          "title": "Distribution-Free, Risk-Controlling Prediction Sets. (arXiv:2101.02703v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02283",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Bai_Y/0/1/0/all/0/1\">Yang Bai</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Pukthuanthong_K/0/1/0/all/0/1\">Kuntara Pukthuanthong</a>",
          "description": "We design a novel framework to examine market efficiency through\nout-of-sample (OOS) predictability. We frame the asset pricing problem as a\nmachine learning classification problem and construct classification models to\npredict return states. The prediction-based portfolios beat the market with\nsignificant OOS economic gains. We measure prediction accuracies directly. For\neach model, we introduce a novel application of binomial test to test the\naccuracy of 3.34 million return state predictions. The tests show that our\nmodels can extract useful contents from historical information to predict\nfuture return states. We provide unique economic insights about OOS\npredictability and machine learning models.",
          "link": "http://arxiv.org/abs/2108.02283",
          "publishedOn": "2021-08-06T00:51:47.182Z",
          "wordCount": 556,
          "title": "Machine Learning Classification Methods and Portfolio Allocation: An Examination of Market Efficiency. (arXiv:2108.02283v1 [q-fin.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baranchuk_D/0/1/0/all/0/1\">Dmitry Baranchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aliev_V/0/1/0/all/0/1\">Vladimir Aliev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1\">Artem Babenko</a>",
          "description": "Normalizing flows are a powerful class of generative models demonstrating\nstrong performance in several speech and vision problems. In contrast to other\ngenerative models, normalizing flows are latent variable models with tractable\nlikelihoods and allow for stable training. However, they have to be carefully\ndesigned to represent invertible functions with efficient Jacobian determinant\ncalculation. In practice, these requirements lead to overparameterized and\nsophisticated architectures that are inferior to alternative feed-forward\nmodels in terms of inference time and memory consumption. In this work, we\ninvestigate whether one can distill flow-based models into more efficient\nalternatives. We provide a positive answer to this question by proposing a\nsimple distillation approach and demonstrating its effectiveness on\nstate-of-the-art conditional flow-based models for image super-resolution and\nspeech synthesis.",
          "link": "http://arxiv.org/abs/2106.12699",
          "publishedOn": "2021-08-06T00:51:47.176Z",
          "wordCount": 589,
          "title": "Distilling the Knowledge from Conditional Normalizing Flows. (arXiv:2106.12699v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.02156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mu_C/0/1/0/all/0/1\">Cong Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mele_A/0/1/0/all/0/1\">Angelo Mele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_L/0/1/0/all/0/1\">Lingxin Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cape_J/0/1/0/all/0/1\">Joshua Cape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athreya_A/0/1/0/all/0/1\">Avanti Athreya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1\">Carey E. Priebe</a>",
          "description": "In network inference applications, it is often desirable to detect community\nstructure, namely to cluster vertices into groups, or blocks, according to some\nmeasure of similarity. Beyond mere adjacency matrices, many real networks also\ninvolve vertex covariates that carry key information about underlying block\nstructure in graphs. To assess the effects of such covariates on block\nrecovery, we present a comparative analysis of two model-based spectral\nalgorithms for clustering vertices in stochastic blockmodel graphs with vertex\ncovariates. The first algorithm uses only the adjacency matrix, and directly\nestimates the block assignments. The second algorithm incorporates both the\nadjacency matrix and the vertex covariates into the estimation of block\nassignments, and moreover quantifies the explicit impact of the vertex\ncovariates on the resulting estimate of the block assignments. We employ\nChernoff information to analytically compare the algorithms' performance and\nderive the information-theoretic Chernoff ratio for certain models of interest.\nAnalytic results and simulations suggest that the second algorithm is often\npreferred: we can often better estimate the induced block assignments by first\nestimating the effect of vertex covariates. In addition, real data examples\nalso indicate that the second algorithm has the advantages of revealing\nunderlying block structure and taking observed vertex heterogeneity into\naccount in real applications. Our findings emphasize the importance of\ndistinguishing between observed and unobserved factors that can affect block\nstructure in graphs.",
          "link": "http://arxiv.org/abs/2007.02156",
          "publishedOn": "2021-08-06T00:51:47.161Z",
          "wordCount": 735,
          "title": "On spectral algorithms for community detection in stochastic blockmodel graphs with vertex covariates. (arXiv:2007.02156v3 [cs.SI] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khorrami_K/0/1/0/all/0/1\">Khazar Khorrami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>",
          "description": "Systems that can find correspondences between multiple modalities, such as\nbetween speech and images, have great potential to solve different recognition\nand data analysis tasks in an unsupervised manner. This work studies multimodal\nlearning in the context of visually grounded speech (VGS) models, and focuses\non their recently demonstrated capability to extract spatiotemporal alignments\nbetween spoken words and the corresponding visual objects without ever been\nexplicitly trained for object localization or word recognition. As the main\ncontributions, we formalize the alignment problem in terms of an audiovisual\nalignment tensor that is based on earlier VGS work, introduce systematic\nmetrics for evaluating model performance in aligning visual objects and spoken\nwords, and propose a new VGS model variant for the alignment task utilizing\ncross-modal attention layer. We test our model and a previously proposed model\nin the alignment task using SPEECH-COCO captions coupled with MSCOCO images. We\ncompare the alignment performance using our proposed evaluation metrics to the\nsemantic retrieval task commonly used to evaluate VGS models. We show that\ncross-modal attention layer not only helps the model to achieve higher semantic\ncross-modal retrieval performance, but also leads to substantial improvements\nin the alignment performance between image object and spoken words.",
          "link": "http://arxiv.org/abs/2108.02562",
          "publishedOn": "2021-08-06T00:51:47.141Z",
          "wordCount": 652,
          "title": "Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models. (arXiv:2108.02562v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tan M. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suliafu_V/0/1/0/all/0/1\">Vai Suliafu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osher_S/0/1/0/all/0/1\">Stanley J. Osher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bao Wang</a>",
          "description": "We propose FMMformers, a class of efficient and flexible transformers\ninspired by the celebrated fast multipole method (FMM) for accelerating\ninteracting particle simulation. FMM decomposes particle-particle interaction\ninto near-field and far-field components and then performs direct and\ncoarse-grained computation, respectively. Similarly, FMMformers decompose the\nattention into near-field and far-field attention, modeling the near-field\nattention by a banded matrix and the far-field attention by a low-rank matrix.\nComputing the attention matrix for FMMformers requires linear complexity in\ncomputational time and memory footprint with respect to the sequence length. In\ncontrast, standard transformers suffer from quadratic complexity. We analyze\nand validate the advantage of FMMformers over the standard transformer on the\nLong Range Arena and language modeling benchmarks. FMMformers can even\noutperform the standard transformer in terms of accuracy by a significant\nmargin. For instance, FMMformers achieve an average classification accuracy of\n$60.74\\%$ over the five Long Range Arena tasks, which is significantly better\nthan the standard transformer's average accuracy of $58.70\\%$.",
          "link": "http://arxiv.org/abs/2108.02347",
          "publishedOn": "2021-08-06T00:51:47.134Z",
          "wordCount": 620,
          "title": "FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention. (arXiv:2108.02347v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alkhouri_I/0/1/0/all/0/1\">Ismail Alkhouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1\">Alvaro Velasquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atia_G/0/1/0/all/0/1\">George Atia</a>",
          "description": "The design of additive imperceptible perturbations to the inputs of deep\nclassifiers to maximize their misclassification rates is a central focus of\nadversarial machine learning. An alternative approach is to synthesize\nadversarial examples from scratch using GAN-like structures, albeit with the\nuse of large amounts of training data. By contrast, this paper considers\none-shot synthesis of adversarial examples; the inputs are synthesized from\nscratch to induce arbitrary soft predictions at the output of pre-trained\nmodels, while simultaneously maintaining high similarity to specified inputs.\nTo this end, we present a problem that encodes objectives on the distance\nbetween the desired and output distributions of the trained model and the\nsimilarity between such inputs and the synthesized examples. We prove that the\nformulated problem is NP-complete. Then, we advance a generative approach to\nthe solution in which the adversarial examples are obtained as the output of a\ngenerative network whose parameters are iteratively updated by optimizing\nsurrogate loss functions for the dual-objective. We demonstrate the generality\nand versatility of the framework and approach proposed through applications to\nthe design of targeted adversarial attacks, generation of decision boundary\nsamples, and synthesis of low confidence classification inputs. The approach is\nfurther extended to an ensemble of models with different soft output\nspecifications. The experimental results verify that the targeted and\nconfidence reduction attack methods developed perform on par with\nstate-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2108.02756",
          "publishedOn": "2021-08-06T00:51:47.128Z",
          "wordCount": 652,
          "title": "BOSS: Bidirectional One-Shot Synthesis of Adversarial Examples. (arXiv:2108.02756v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.10861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shunqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurkoski_B/0/1/0/all/0/1\">Brian M. Kurkoski</a>",
          "description": "Approximate message passing (AMP) is a low-cost iterative\nparameter-estimation technique for certain high-dimensional linear systems with\nnon-Gaussian distributions. However, AMP only applies to independent\nidentically distributed (IID) transform matrices, but may become unreliable\n(e.g. perform poorly or even diverge) for other matrix ensembles, especially\nfor ill-conditioned ones. Orthogonal/vector AMP (OAMP/VAMP) was proposed for\ngeneral right-unitarily-invariant matrices to handle this difficulty. However,\nthe Bayes-optimal OAMP/VAMP requires a high-complexity linear minimum mean\nsquare error (MMSE) estimator. This limits the application of OAMP/VAMP to\nlarge-scale systems.\n\nTo solve the disadvantages of AMP and OAMP/VAMP, this paper proposes a memory\nAMP (MAMP) framework under an orthogonality principle, which guarantees the\nasymptotic IID Gaussianity of estimation errors in MAMP. We present an\northogonalization procedure for the local memory estimators to realize the\nrequired orthogonality for MAMP. Furthermore, we propose a Bayes-optimal MAMP\n(BO-MAMP), in which a long-memory matched filter is proposed for interference\nsuppression. The complexity of BO-MAMP is comparable to AMP. A state evolution\nis derived to asymptotically characterize the performance of BO-MAMP. Based on\nstate evolution, the relaxation parameters and damping vector in BO-MAMP are\noptimized. For all right-unitarily-invariant matrices, the optimized BO-MAMP\nconverges to the high-complexity OAMP/VAMP, and thus is Bayes-optimal if it has\na unique fixed point. Finally, simulations are provided to verify the validity\nand accuracy of the theoretical results.",
          "link": "http://arxiv.org/abs/2012.10861",
          "publishedOn": "2021-08-06T00:51:47.077Z",
          "wordCount": 755,
          "title": "Memory AMP. (arXiv:2012.10861v4 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Powell_B/0/1/0/all/0/1\">Brian A. Powell</a>",
          "description": "Adversarial lateral movement via compromised accounts remains difficult to\ndiscover via traditional rule-based defenses because it generally lacks\nexplicit indicators of compromise. We propose a behavior-based, unsupervised\nframework comprising two methods of lateral movement detection on enterprise\nnetworks: one aimed at generic lateral movement via either exploit or\nauthenticated connections, and one targeting the specific techniques of process\ninjection and hijacking. The first method is based on the premise that the role\nof a system---the functions it performs on the network---determines the roles\nof the systems it should make connections with. The adversary meanwhile might\nmove between any systems whatever, possibly seeking out systems with unusual\nroles that facilitate certain accesses. We use unsupervised learning to cluster\nsystems according to role and identify connections to systems with novel roles\nas potentially malicious. The second method is based on the premise that the\ntemporal patterns of inter-system processes that facilitate these connections\ndepend on the roles of the systems involved. If a process is compromised by an\nattacker, these normal patterns might be disrupted in discernible ways. We\napply frequent-itemset mining to process sequences to establish regular\npatterns of communication between systems based on role, and identify rare\nprocess sequences as signalling potentially malicious connections.",
          "link": "http://arxiv.org/abs/2108.02713",
          "publishedOn": "2021-08-06T00:51:47.013Z",
          "wordCount": 642,
          "title": "Role-based lateral movement detection with unsupervised learning. (arXiv:2108.02713v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1\">Emanuel Ben-Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1\">Lihi Zelnik-Manor</a>",
          "description": "ImageNet-1K serves as the primary dataset for pretraining deep learning\nmodels for computer vision tasks. ImageNet-21K dataset, which is bigger and\nmore diverse, is used less frequently for pretraining, mainly due to its\ncomplexity, low accessibility, and underestimation of its added value. This\npaper aims to close this gap, and make high-quality efficient pretraining on\nImageNet-21K available for everyone. Via a dedicated preprocessing stage,\nutilization of WordNet hierarchical structure, and a novel training scheme\ncalled semantic softmax, we show that various models significantly benefit from\nImageNet-21K pretraining on numerous datasets and tasks, including small\nmobile-oriented models. We also show that we outperform previous ImageNet-21K\npretraining schemes for prominent new models like ViT and Mixer. Our proposed\npretraining pipeline is efficient, accessible, and leads to SoTA reproducible\nresults, from a publicly available dataset. The training code and pretrained\nmodels are available at: https://github.com/Alibaba-MIIL/ImageNet21K",
          "link": "http://arxiv.org/abs/2104.10972",
          "publishedOn": "2021-08-06T00:51:47.007Z",
          "wordCount": 632,
          "title": "ImageNet-21K Pretraining for the Masses. (arXiv:2104.10972v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10234",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fanglan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1\">Taoran Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Kaiqun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1\">Charu Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chang-Tien Lu</a>",
          "description": "Deep learning's performance has been extensively recognized recently. Graph\nneural networks (GNNs) are designed to deal with graph-structural data that\nclassical deep learning does not easily manage. Since most GNNs were created\nusing distinct theories, direct comparisons are impossible. Prior research has\nprimarily concentrated on categorizing existing models, with little attention\npaid to their intrinsic connections. The purpose of this study is to establish\na unified framework that integrates GNNs based on spectral graph and\napproximation theory. The framework incorporates a strong integration between\nspatial- and spectral-based GNNs while tightly associating approaches that\nexist within each respective domain.",
          "link": "http://arxiv.org/abs/2107.10234",
          "publishedOn": "2021-08-06T00:51:47.000Z",
          "wordCount": 589,
          "title": "Bridging the Gap between Spatial and Spectral Domains: A Unified Framework for Graph Neural Networks. (arXiv:2107.10234v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1\">Juan P. Vigueras-Guill&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1\">Frank Seeliger</a>",
          "description": "Regularization in convolutional neural networks (CNNs) is usually addressed\nwith dropout layers. However, dropout is sometimes detrimental in the\nconvolutional part of a CNN as it simply sets to zero a percentage of pixels in\nthe feature maps, adding unrepresentative examples during training. Here, we\npropose a CNN layer that performs regularization by applying random rotations\nof reflections to a small percentage of feature maps after every convolutional\nlayer. We prove how this concept is beneficial for images with orientational\nsymmetries, such as in medical images, as it provides a certain degree of\nrotational invariance. We tested this method in two datasets, a patch-based set\nof histopathology images (PatchCamelyon) to perform classification using a\ngeneric DenseNet, and a set of specular microscopy images of the corneal\nendothelium to perform segmentation using a tailored U-net, improving the\nperformance in both cases.",
          "link": "http://arxiv.org/abs/2108.02704",
          "publishedOn": "2021-08-06T00:51:46.986Z",
          "wordCount": 615,
          "title": "Rotaflip: A New CNN Layer for Regularization and Rotational Invariance in Medical Images. (arXiv:2108.02704v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00001",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singhal_V/0/1/0/all/0/1\">Vikrant Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinke_T/0/1/0/all/0/1\">Thomas Steinke</a>",
          "description": "Private data analysis suffers a costly curse of dimensionality. However, the\ndata often has an underlying low-dimensional structure. For example, when\noptimizing via gradient descent, the gradients often lie in or near a\nlow-dimensional subspace. If that low-dimensional structure can be identified,\nthen we can avoid paying (in terms of privacy or accuracy) for the high ambient\ndimension.\n\nWe present differentially private algorithms that take input data sampled\nfrom a low-dimensional linear subspace (possibly with a small amount of error)\nand output that subspace (or an approximation to it). These algorithms can\nserve as a pre-processing step for other procedures.",
          "link": "http://arxiv.org/abs/2106.00001",
          "publishedOn": "2021-08-06T00:51:46.961Z",
          "wordCount": 559,
          "title": "Privately Learning Subspaces. (arXiv:2106.00001v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00531",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pourreza_R/0/1/0/all/0/1\">Reza Pourreza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_T/0/1/0/all/0/1\">Taco S Cohen</a>",
          "description": "While most neural video codecs address P-frame coding (predicting each frame\nfrom past ones), in this paper we address B-frame compression (predicting\nframes using both past and future reference frames). Our B-frame solution is\nbased on the existing P-frame methods. As a result, B-frame coding capability\ncan easily be added to an existing neural codec. The basic idea of our B-frame\ncoding method is to interpolate the two reference frames to generate a single\nreference frame and then use it together with an existing P-frame codec to\nencode the input B-frame. Our studies show that the interpolated frame is a\nmuch better reference for the P-frame codec compared to using the previous\nframe as is usually done. Our results show that using the proposed method with\nan existing P-frame codec can lead to 28.5%saving in bit-rate on the UVG\ndataset compared to the P-frame codec while generating the same video quality.",
          "link": "http://arxiv.org/abs/2104.00531",
          "publishedOn": "2021-08-06T00:51:46.953Z",
          "wordCount": 614,
          "title": "Extending Neural P-frame Codecs for B-frame Coding. (arXiv:2104.00531v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xubo Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nouiehed_M/0/1/0/all/0/1\">Maher Nouiehed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kontar_R/0/1/0/all/0/1\">Raed Al Kontar</a>",
          "description": "In this paper we propose \\texttt{GIFAIR-FL}: an approach that imposes group\nand individual fairness to federated learning settings. By adding a\nregularization term, our algorithm penalizes the spread in the loss of client\ngroups to drive the optimizer to fair solutions. Theoretically, we show\nconvergence in non-convex and strongly convex settings. Our convergence\nguarantees hold for both $i.i.d.$ and non-$i.i.d.$ data. To demonstrate the\nempirical performance of our algorithm, we apply our method on image\nclassification and text prediction tasks. Compared to existing algorithms, our\nmethod shows improved fairness results while retaining superior or similar\nprediction accuracy.",
          "link": "http://arxiv.org/abs/2108.02741",
          "publishedOn": "2021-08-06T00:51:46.945Z",
          "wordCount": 539,
          "title": "GIFAIR-FL: An Approach for Group and Individual Fairness in Federated Learning. (arXiv:2108.02741v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Russel_R/0/1/0/all/0/1\">Reazul Hasan Russel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benosman_M/0/1/0/all/0/1\">Mouhacine Benosman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baar_J/0/1/0/all/0/1\">Jeroen Van Baar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corcodel_R/0/1/0/all/0/1\">Radu Corcodel</a>",
          "description": "Safety and robustness are two desired properties for any reinforcement\nlearning algorithm. CMDPs can handle additional safety constraints and RMDPs\ncan perform well under model uncertainties. In this paper, we propose to unite\nthese two frameworks resulting in robust constrained MDPs (RCMDPs). The\nmotivation is to develop a framework that can satisfy safety constraints while\nalso simultaneously offer robustness to model uncertainties. We develop the\nRCMDP objective, derive gradient update formula to optimize this objective and\nthen propose policy gradient based algorithms. We also independently propose\nLyapunov based reward shaping for RCMDPs, yielding better stability and\nconvergence properties.",
          "link": "http://arxiv.org/abs/2108.02701",
          "publishedOn": "2021-08-06T00:51:46.938Z",
          "wordCount": 544,
          "title": "Lyapunov Robust Constrained-MDPs: Soft-Constrained Robustly Stable Policy Optimization under Model Uncertainty. (arXiv:2108.02701v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02081",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Vargas_F/0/1/0/all/0/1\">Francisco Vargas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thodoroff_P/0/1/0/all/0/1\">Pierre Thodoroff</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lawrence_N/0/1/0/all/0/1\">Neil D. Lawrence</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lamacraft_A/0/1/0/all/0/1\">Austen Lamacraft</a>",
          "description": "The Schr\\\"odinger bridge problem (SBP) finds the most likely stochastic\nevolution between two probability distributions given a prior stochastic\nevolution. As well as applications in the natural sciences, problems of this\nkind have important applications in machine learning such as dataset alignment\nand hypothesis testing. Whilst the theory behind this problem is relatively\nmature, scalable numerical recipes to estimate the Schr\\\"odinger bridge remain\nan active area of research. We prove an equivalence between the SBP and maximum\nlikelihood estimation enabling direct application of successful machine\nlearning techniques. We propose a numerical procedure to estimate SBPs using\nGaussian process and demonstrate the practical usage of our approach in\nnumerical simulations and experiments.",
          "link": "http://arxiv.org/abs/2106.02081",
          "publishedOn": "2021-08-06T00:51:46.931Z",
          "wordCount": 585,
          "title": "Solving Schr\\\"odinger Bridges via Maximum Likelihood. (arXiv:2106.02081v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dabeen Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vojnovic_M/0/1/0/all/0/1\">Milan Vojnovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>",
          "description": "Motivated by recent developments in designing algorithms based on individual\nitem scores for solving utility maximization problems, we study the framework\nof using test scores, defined as a statistic of observed individual item\nperformance data, for solving the budgeted stochastic utility maximization\nproblem. We extend an existing scoring mechanism, namely the replication test\nscores, to incorporate heterogeneous item costs as well as item values. We show\nthat a natural greedy algorithm that selects items solely based on their\nreplication test scores outputs solutions within a constant factor of the\noptimum for a broad class of utility functions. Our algorithms and\napproximation guarantees assume that test scores are noisy estimates of certain\nexpected values with respect to marginal distributions of individual item\nvalues, thus making our algorithms practical and extending previous work that\nassumes noiseless estimates. Moreover, we show how our algorithm can be adapted\nto the setting where items arrive in a streaming fashion while maintaining the\nsame approximation guarantee. We present numerical results, using synthetic\ndata and data sets from the Academia.StackExchange Q&A forum, which show that\nour test score algorithm can achieve competitiveness, and in some cases better\nperformance than a benchmark algorithm that requires access to a value oracle\nto evaluate function values.",
          "link": "http://arxiv.org/abs/2012.15194",
          "publishedOn": "2021-08-06T00:51:46.910Z",
          "wordCount": 675,
          "title": "Test Score Algorithms for Budgeted Stochastic Utility Maximization. (arXiv:2012.15194v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhiying Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Raphael Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Ji Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>",
          "description": "Pretrained transformers achieve the state of the art across tasks in natural\nlanguage processing, motivating researchers to investigate their inner\nmechanisms. One common direction is to understand what features are important\nfor prediction. In this paper, we apply information bottlenecks to analyze the\nattribution of each feature for prediction on a black-box model. We use BERT as\nthe example and evaluate our approach both quantitatively and qualitatively. We\nshow the effectiveness of our method in terms of attribution and the ability to\nprovide insight into how information flows through layers. We demonstrate that\nour technique outperforms two competitive methods in degradation tests on four\ndatasets. Code is available at https://github.com/bazingagin/IBA.",
          "link": "http://arxiv.org/abs/2012.13838",
          "publishedOn": "2021-08-06T00:51:46.903Z",
          "wordCount": 594,
          "title": "Inserting Information Bottlenecks for Attribution in Transformers. (arXiv:2012.13838v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_B/0/1/0/all/0/1\">Binayak Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yingtao Jiang</a>",
          "description": "The increasing popularity of deep neural network (DNN) applications demands\nhigh computing power and efficient hardware accelerator architecture. DNN\naccelerators use a large number of processing elements (PEs) and on-chip memory\nfor storing weights and other parameters. As the communication backbone of a\nDNN accelerator, networks-on-chip (NoC) play an important role in supporting\nvarious dataflow patterns and enabling processing with communication\nparallelism in a DNN accelerator. However, the widely used mesh-based NoC\narchitectures inherently cannot support the efficient one-to-many and\nmany-to-one traffic largely existing in DNN workloads. In this paper, we\npropose a modified mesh architecture with a one-way/two-way streaming bus to\nspeedup one-to-many (multicast) traffic, and the use of gather packets to\nsupport many-to-one (gather) traffic. The analysis of the runtime latency of a\nconvolutional layer shows that the two-way streaming architecture achieves\nbetter improvement than the one-way streaming architecture for an Output\nStationary (OS) dataflow architecture. The simulation results demonstrate that\nthe gather packets can help to reduce the runtime latency up to 1.8 times and\nnetwork power consumption up to 1.7 times, compared with the repetitive unicast\nmethod on modified mesh architectures supporting two-way streaming.",
          "link": "http://arxiv.org/abs/2108.02569",
          "publishedOn": "2021-08-06T00:51:46.896Z",
          "wordCount": 644,
          "title": "Data Streaming and Traffic Gathering in Mesh-based NoC for Deep Neural Network Acceleration. (arXiv:2108.02569v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02594",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Perera_S/0/1/0/all/0/1\">Shanaka Perera</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aglietti_V/0/1/0/all/0/1\">Virginia Aglietti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Damoulas_T/0/1/0/all/0/1\">Theodoros Damoulas</a>",
          "description": "We study the problem of estimating potential revenue or demand at business\nfacilities and understanding its generating mechanism. This problem arises in\ndifferent fields such as operation research or urban science, and more\ngenerally, it is crucial for businesses' planning and decision making. We\ndevelop a Bayesian spatial interaction model, henceforth BSIM, which provides\nprobabilistic predictions about revenues generated by a particular business\nlocation provided their features and the potential customers' characteristics\nin a given region. BSIM explicitly accounts for the competition among the\ncompetitive facilities through a probability value determined by evaluating a\nstore-specific Gaussian distribution at a given customer location. We propose a\nscalable variational inference framework that, while being significantly faster\nthan competing Markov Chain Monte Carlo inference schemes, exhibits comparable\nperformances in terms of parameters identification and uncertainty\nquantification. We demonstrate the benefits of BSIM in various synthetic\nsettings characterised by an increasing number of stores and customers.\nFinally, we construct a real-world, large spatial dataset for pub activities in\nLondon, UK, which includes over 1,500 pubs and 150,000 customer regions. We\ndemonstrate how BSIM outperforms competing approaches on this large dataset in\nterms of prediction performances while providing results that are both\ninterpretable and consistent with related indicators observed for the London\nregion.",
          "link": "http://arxiv.org/abs/2108.02594",
          "publishedOn": "2021-08-06T00:51:46.890Z",
          "wordCount": 654,
          "title": "A variational Bayesian spatial interaction model for estimating revenue and demand at business facilities. (arXiv:2108.02594v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Popov_V/0/1/0/all/0/1\">Vadim Popov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vovk_I/0/1/0/all/0/1\">Ivan Vovk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gogoryan_V/0/1/0/all/0/1\">Vladimir Gogoryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadekova_T/0/1/0/all/0/1\">Tasnima Sadekova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kudinov_M/0/1/0/all/0/1\">Mikhail Kudinov</a>",
          "description": "Recently, denoising diffusion probabilistic models and generative score\nmatching have shown high potential in modelling complex data distributions\nwhile stochastic calculus has provided a unified point of view on these\ntechniques allowing for flexible inference schemes. In this paper we introduce\nGrad-TTS, a novel text-to-speech model with score-based decoder producing\nmel-spectrograms by gradually transforming noise predicted by encoder and\naligned with text input by means of Monotonic Alignment Search. The framework\nof stochastic differential equations helps us to generalize conventional\ndiffusion probabilistic models to the case of reconstructing data from noise\nwith different parameters and allows to make this reconstruction flexible by\nexplicitly controlling trade-off between sound quality and inference speed.\nSubjective human evaluation shows that Grad-TTS is competitive with\nstate-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We\nwill make the code publicly available shortly.",
          "link": "http://arxiv.org/abs/2105.06337",
          "publishedOn": "2021-08-06T00:51:46.883Z",
          "wordCount": 602,
          "title": "Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech. (arXiv:2105.06337v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anil_C/0/1/0/all/0/1\">Cem Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_X/0/1/0/all/0/1\">Xuchan Bao</a>",
          "description": "Voting systems have a wide range of applications including recommender\nsystems, web search, product design and elections. Limited by the lack of\ngeneral-purpose analytical tools, it is difficult to hand-engineer desirable\nvoting rules for each use case. For this reason, it is appealing to\nautomatically discover voting rules geared towards each scenario. In this\npaper, we show that set-input neural network architectures such as Set\nTransformers, fully-connected graph networks and DeepSets are both\ntheoretically and empirically well-suited for learning voting rules. In\nparticular, we show that these network models can not only mimic a number of\nexisting voting rules to compelling accuracy --- both position-based (such as\nPlurality and Borda) and comparison-based (such as Kemeny, Copeland and\nMaximin) --- but also discover near-optimal voting rules that maximize\ndifferent social welfare functions. Furthermore, the learned voting rules\ngeneralize well to different voter utility distributions and election sizes\nunseen during training.",
          "link": "http://arxiv.org/abs/2108.02768",
          "publishedOn": "2021-08-06T00:51:46.864Z",
          "wordCount": 569,
          "title": "Learning to Elect. (arXiv:2108.02768v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01403",
          "author": "<a href=\"http://arxiv.org/find/hep-th/1/au:+Erbin_H/0/1/0/all/0/1\">Harold Erbin</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Lahoche_V/0/1/0/all/0/1\">Vincent Lahoche</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Samary_D/0/1/0/all/0/1\">Dine Ousmane Samary</a>",
          "description": "In a recent work arXiv:2008.08601, Halverson, Maiti and Stoner proposed a\ndescription of neural networks in terms of a Wilsonian effective field theory.\nThe infinite-width limit is mapped to a free field theory, while finite $N$\ncorrections are taken into account by interactions (non-Gaussian terms in the\naction). In this paper, we study two related aspects of this correspondence.\nFirst, we comment on the concepts of locality and power-counting in this\ncontext. Indeed, these usual space-time notions may not hold for neural\nnetworks (since inputs can be arbitrary), however, the renormalization group\nprovides natural notions of locality and scaling. Moreover, we comment on\nseveral subtleties, for example, that data components may not have a\npermutation symmetry: in that case, we argue that random tensor field theories\ncould provide a natural generalization. Second, we improve the perturbative\nWilsonian renormalization from arXiv:2008.08601 by providing an analysis in\nterms of the nonperturbative renormalization group using the Wetterich-Morris\nequation. An important difference with usual nonperturbative RG analysis is\nthat only the effective (IR) 2-point function is known, which requires setting\nthe problem with care. Our aim is to provide a useful formalism to investigate\nneural networks behavior beyond the large-width limit (i.e.~far from Gaussian\nlimit) in a nonperturbative fashion. A major result of our analysis is that\nchanging the standard deviation of the neural network weight distribution can\nbe interpreted as a renormalization flow in the space of networks. We focus on\ntranslations invariant kernels and provide preliminary numerical results.",
          "link": "http://arxiv.org/abs/2108.01403",
          "publishedOn": "2021-08-06T00:51:46.858Z",
          "wordCount": 701,
          "title": "Nonperturbative renormalization for the neural network-QFT correspondence. (arXiv:2108.01403v1 [hep-th] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bohn_B/0/1/0/all/0/1\">Bastian Bohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griebel_M/0/1/0/all/0/1\">Michael Griebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_D/0/1/0/all/0/1\">Dinesh Kannan</a>",
          "description": "In this paper, we propose neural networks that tackle the problems of\nstability and field-of-view of a Convolutional Neural Network (CNN). As an\nalternative to increasing the network's depth or width to improve performance,\nwe propose integral-based spatially nonlocal operators which are related to\nglobal weighted Laplacian, fractional Laplacian and inverse fractional\nLaplacian operators that arise in several problems in the physical sciences.\nThe forward propagation of such networks is inspired by partial\nintegro-differential equations (PIDEs). We test the effectiveness of the\nproposed neural architectures on benchmark image classification datasets and\nsemantic segmentation tasks in autonomous driving. Moreover, we investigate the\nextra computational costs of these dense operators and the stability of forward\npropagation of the proposed neural networks.",
          "link": "http://arxiv.org/abs/2108.02430",
          "publishedOn": "2021-08-06T00:51:46.850Z",
          "wordCount": 553,
          "title": "Deep Neural Networks and PIDE discretizations. (arXiv:2108.02430v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1\">Haotian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaoli Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renyuan Xu</a>",
          "description": "One of the challenges for multi-agent reinforcement learning (MARL) is\ndesigning efficient learning algorithms for a large system in which each agent\nhas only limited or partial information of the entire system. In this system,\nit is desirable to learn policies of a decentralized type. A recent and\npromising paradigm to analyze such decentralized MARL is to take network\nstructures into consideration. While exciting progress has been made to analyze\ndecentralized MARL with the network of agents, often found in social networks\nand team video games, little is known theoretically for decentralized MARL with\nthe network of states, frequently used for modeling self-driving vehicles,\nride-sharing, and data and traffic routing.\n\nThis paper proposes a framework called localized training and decentralized\nexecution to study MARL with network of states, with homogeneous (a.k.a.\nmean-field type) agents. Localized training means that agents only need to\ncollect local information in their neighboring states during the training\nphase; decentralized execution implies that, after the training stage, agents\ncan execute the learned decentralized policies, which only requires knowledge\nof the agents' current states. The key idea is to utilize the homogeneity of\nagents and regroup them according to their states, thus the formulation of a\nnetworked Markov decision process with teams of agents, enabling the update of\nthe Q-function in a localized fashion. In order to design an efficient and\nscalable reinforcement learning algorithm under such a framework, we adopt the\nactor-critic approach with over-parameterized neural networks, and establish\nthe convergence and sample complexity for our algorithm, shown to be scalable\nwith respect to the size of both agents and states.",
          "link": "http://arxiv.org/abs/2108.02731",
          "publishedOn": "2021-08-06T00:51:46.844Z",
          "wordCount": 706,
          "title": "Mean-Field Multi-Agent Reinforcement Learning: A Decentralized Network Approach. (arXiv:2108.02731v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2003.04696",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Perez_Garcia_F/0/1/0/all/0/1\">Fernando P&#xe9;rez-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sparks_R/0/1/0/all/0/1\">Rachel Sparks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">S&#xe9;bastien Ourselin</a>",
          "description": "Processing of medical images such as MRI or CT presents unique challenges\ncompared to RGB images typically used in computer vision. These include a lack\nof labels for large datasets, high computational costs, and metadata to\ndescribe the physical properties of voxels. Data augmentation is used to\nartificially increase the size of the training datasets. Training with image\npatches decreases the need for computational power. Spatial metadata needs to\nbe carefully taken into account in order to ensure a correct alignment of\nvolumes.\n\nWe present TorchIO, an open-source Python library to enable efficient\nloading, preprocessing, augmentation and patch-based sampling of medical images\nfor deep learning. TorchIO follows the style of PyTorch and integrates standard\nmedical image processing libraries to efficiently process images during\ntraining of neural networks. TorchIO transforms can be composed, reproduced,\ntraced and extended. We provide multiple generic preprocessing and augmentation\noperations as well as simulation of MRI-specific artifacts.\n\nSource code, comprehensive tutorials and extensive documentation for TorchIO\ncan be found at https://torchio.rtfd.io/. The package can be installed from the\nPython Package Index running 'pip install torchio'. It includes a command-line\ninterface which allows users to apply transforms to image files without using\nPython. Additionally, we provide a graphical interface within a TorchIO\nextension in 3D Slicer to visualize the effects of transforms.\n\nTorchIO was developed to help researchers standardize medical image\nprocessing pipelines and allow them to focus on the deep learning experiments.\nIt encourages open science, as it supports reproducibility and is version\ncontrolled so that the software can be cited precisely. Due to its modularity,\nthe library is compatible with other frameworks for deep learning with medical\nimages.",
          "link": "http://arxiv.org/abs/2003.04696",
          "publishedOn": "2021-08-06T00:51:46.836Z",
          "wordCount": 819,
          "title": "TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning. (arXiv:2003.04696v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02551",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1\">Shagufta Henna</a>",
          "description": "Hybrid FSO/RF system requires an efficient FSO and RF link switching\nmechanism to improve the system capacity by realizing the complementary\nbenefits of both the links. The dynamics of network conditions, such as fog,\ndust, and sand storms compound the link switching problem and control\ncomplexity. To address this problem, we initiate the study of deep\nreinforcement learning (DRL) for link switching of hybrid FSO/RF systems.\nSpecifically, in this work, we focus on actor-critic called Actor/Critic-FSO/RF\nand Deep-Q network (DQN) called DQN-FSO/RF for FSO/RF link switching under\natmospheric turbulences. To formulate the problem, we define the state, action,\nand reward function of a hybrid FSO/RF system. DQN-FSO/RF frequently updates\nthe deployed policy that interacts with the environment in a hybrid FSO/RF\nsystem, resulting in high switching costs. To overcome this, we lift this\nproblem to ensemble consensus-based representation learning for deep\nreinforcement called DQNEnsemble-FSO/RF. The proposed novel DQNEnsemble-FSO/RF\nDRL approach uses consensus learned features representations based on an\nensemble of asynchronous threads to update the deployed policy. Experimental\nresults corroborate that the proposed DQNEnsemble-FSO/RF's consensus-learned\nfeatures switching achieves better performance than Actor/Critic-FSO/RF,\nDQN-FSO/RF, and MyOpic for FSO/RF link switching while keeping the switching\ncost significantly low.",
          "link": "http://arxiv.org/abs/2108.02551",
          "publishedOn": "2021-08-06T00:51:46.809Z",
          "wordCount": 641,
          "title": "Ensemble Consensus-based Representation Deep Reinforcement Learning for Hybrid FSO/RF Communication Systems. (arXiv:2108.02551v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.04005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1\">Tsang Ing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1\">Cleber Zanchettin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Adriano L. I. Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "In this paper, we argue that the unsatisfactory out-of-distribution (OOD)\ndetection performance of neural networks is mainly due to the SoftMax loss\nanisotropy and propensity to produce low entropy probability distributions in\ndisagreement with the principle of maximum entropy. Current out-of-distribution\n(OOD) detection approaches usually do not directly fix the SoftMax loss\ndrawbacks, but rather build techniques to circumvent it. Unfortunately, those\nmethods usually produce undesired side effects (e.g., classification accuracy\ndrop, additional hyperparameters, slower inferences, and collecting extra\ndata). In the opposite direction, we propose replacing SoftMax loss with a\nnovel loss function that does not suffer from the mentioned weaknesses. The\nproposed IsoMax loss is isotropic (exclusively distance-based) and provides\nhigh entropy posterior probability distributions. Replacing the SoftMax loss by\nIsoMax loss requires no model or training changes. Additionally, the models\ntrained with IsoMax loss produce as fast and energy-efficient inferences as\nthose trained using SoftMax loss. Moreover, no classification accuracy drop is\nobserved. The proposed method does not rely on outlier/background data,\nhyperparameter tuning, temperature calibration, feature extraction, metric\nlearning, adversarial training, ensemble procedures, or generative models. Our\nexperiments showed that IsoMax loss works as a seamless SoftMax loss drop-in\nreplacement that significantly improves neural networks' OOD detection\nperformance. Hence, it may be used as a baseline OOD detection approach to be\ncombined with current or future OOD detection techniques to achieve even higher\nresults.",
          "link": "http://arxiv.org/abs/2006.04005",
          "publishedOn": "2021-08-06T00:51:46.799Z",
          "wordCount": 734,
          "title": "Entropic Out-of-Distribution Detection: Seamless Detection of Unknown Examples. (arXiv:2006.04005v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samak_T/0/1/0/all/0/1\">Tanmay Vilas Samak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samak_C/0/1/0/all/0/1\">Chinmay Vilas Samak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandhasamy_S/0/1/0/all/0/1\">Sivanathan Kandhasamy</a>",
          "description": "In this work, we present a lightweight pipeline for robust behavioral cloning\nof a human driver using end-to-end imitation learning. The proposed pipeline\nwas employed to train and deploy three distinct driving behavior models onto a\nsimulated vehicle. The training phase comprised of data collection, balancing,\naugmentation, preprocessing and training a neural network, following which, the\ntrained model was deployed onto the ego vehicle to predict steering commands\nbased on the feed from an onboard camera. A novel coupled control law was\nformulated to generate longitudinal control commands on-the-go based on the\npredicted steering angle and other parameters such as actual speed of the ego\nvehicle and the prescribed constraints for speed and steering. We analyzed\ncomputational efficiency of the pipeline and evaluated robustness of the\ntrained models through exhaustive experimentation during the deployment phase.\nWe also compared our approach against state-of-the-art implementation in order\nto comment on its validity.",
          "link": "http://arxiv.org/abs/2010.04767",
          "publishedOn": "2021-08-06T00:51:46.783Z",
          "wordCount": 651,
          "title": "Robust Behavioral Cloning for Autonomous Vehicles using End-to-End Imitation Learning. (arXiv:2010.04767v4 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15758",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rischel_E/0/1/0/all/0/1\">Eigil F. Rischel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Weichwald_S/0/1/0/all/0/1\">Sebastian Weichwald</a>",
          "description": "Interventional causal models describe several joint distributions over some\nvariables used to describe a system, one for each intervention setting. They\nprovide a formal recipe for how to move between the different joint\ndistributions and make predictions about the variables upon intervening on the\nsystem. Yet, it is difficult to formalise how we may change the underlying\nvariables used to describe the system, say moving from fine-grained to\ncoarse-grained variables. Here, we argue that compositionality is a desideratum\nfor such model transformations and the associated errors: When abstracting a\nreference model M iteratively, first obtaining M' and then further simplifying\nthat to obtain M'', we expect the composite transformation from M to M'' to\nexist and its error to be bounded by the errors incurred by each individual\ntransformation step. Category theory, the study of mathematical objects via\ncompositional transformations between them, offers a natural language to\ndevelop our framework for model transformations and abstractions. We introduce\na category of finite interventional causal models and, leveraging theory of\nenriched categories, prove the desired compositionality properties for our\nframework.",
          "link": "http://arxiv.org/abs/2103.15758",
          "publishedOn": "2021-08-06T00:51:46.730Z",
          "wordCount": 652,
          "title": "Compositional Abstraction Error and a Category of Causal Models. (arXiv:2103.15758v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12636",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fengxiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Detection transformers have recently shown promising object detection results\nand attracted increasing attention. However, how to develop effective domain\nadaptation techniques to improve its cross-domain performance remains\nunexplored and unclear. In this paper, we delve into this topic and empirically\nfind that direct feature distribution alignment on the CNN backbone only brings\nlimited improvements, as it does not guarantee domain-invariant sequence\nfeatures in the transformer for prediction. To address this issue, we propose a\nnovel Sequence Feature Alignment (SFA) method that is specially designed for\nthe adaptation of detection transformers. Technically, SFA consists of a domain\nquery-based feature alignment (DQFA) module and a token-wise feature alignment\n(TDA) module. In DQFA, a novel domain query is used to aggregate and align\nglobal context from the token sequence of both domains. DQFA reduces the domain\ndiscrepancy in global feature representations and object relations when\ndeploying in the transformer encoder and decoder, respectively. Meanwhile, TDA\naligns token features in the sequence from both domains, which reduces the\ndomain gaps in local and instance-level feature representations in the\ntransformer encoder and decoder, respectively. Besides, a novel bipartite\nmatching consistency loss is proposed to enhance the feature discriminability\nfor robust object detection. Experiments on three challenging benchmarks show\nthat SFA outperforms state-of-the-art domain adaptive object detection methods.\nCode has been made available at: https://github.com/encounter1997/SFA.",
          "link": "http://arxiv.org/abs/2107.12636",
          "publishedOn": "2021-08-06T00:51:46.714Z",
          "wordCount": 726,
          "title": "Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1\">Mohammad Javad Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1\">Branislav Kveton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1\">Mohammad Ghavamzadeh</a>",
          "description": "We study the problem of best-arm identification (BAI) in contextual bandits\nin the fixed-budget setting. We propose a general successive elimination\nalgorithm that proceeds in stages and eliminates a fixed fraction of suboptimal\narms in each stage. This design takes advantage of the strengths of static and\nadaptive allocations. We analyze the algorithm in linear models and obtain a\nbetter error bound than prior work. We also apply it to generalized linear\nmodels (GLMs) and bound its error. This is the first BAI algorithm for GLMs in\nthe fixed-budget setting. Our extensive numerical experiments show that our\nalgorithm outperforms the state of art.",
          "link": "http://arxiv.org/abs/2106.04763",
          "publishedOn": "2021-08-06T00:51:46.693Z",
          "wordCount": 602,
          "title": "Fixed-Budget Best-Arm Identification in Contextual Bandits: A Static-Adaptive Algorithm. (arXiv:2106.04763v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02744",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Alamo_M/0/1/0/all/0/1\">Miguel del Alamo</a>",
          "description": "We consider ill-posed inverse problems where the forward operator $T$ is\nunknown, and instead we have access to training data consisting of functions\n$f_i$ and their noisy images $Tf_i$. This is a practically relevant and\nchallenging problem which current methods are able to solve only under strong\nassumptions on the training set. Here we propose a new method that requires\nminimal assumptions on the data, and prove reconstruction rates that depend on\nthe number of training points and the noise level. We show that, in the regime\nof \"many\" training data, the method is minimax optimal. The proposed method\nemploys a type of convolutional neural networks (U-nets) and empirical risk\nminimization in order to \"fit\" the unknown operator. In a nutshell, our\napproach is based on two ideas: the first is to relate U-nets to multiscale\ndecompositions such as wavelets, thereby linking them to the existing theory,\nand the second is to use the hierarchical structure of U-nets and the low\nnumber of parameters of convolutional neural nets to prove entropy bounds that\nare practically useful. A significant difference with the existing works on\nneural networks in nonparametric statistics is that we use them to approximate\noperators and not functions, which we argue is mathematically more natural and\ntechnically more convenient.",
          "link": "http://arxiv.org/abs/2108.02744",
          "publishedOn": "2021-08-06T00:51:46.686Z",
          "wordCount": 656,
          "title": "Deep learning for inverse problems with unknown operator. (arXiv:2108.02744v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1\">Shahroz Tariq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Simon S. Woo</a>",
          "description": "Over the last few decades, artificial intelligence research has made\ntremendous strides, but it still heavily relies on fixed datasets in stationary\nenvironments. Continual learning is a growing field of research that examines\nhow AI systems can learn sequentially from a continuous stream of linked data\nin the same way that biological systems do. Simultaneously, fake media such as\ndeepfakes and synthetic face images have emerged as significant to current\nmultimedia technologies. Recently, numerous method has been proposed which can\ndetect deepfakes with high accuracy. However, they suffer significantly due to\ntheir reliance on fixed datasets in limited evaluation settings. Therefore, in\nthis work, we apply continuous learning to neural networks' learning dynamics,\nemphasizing its potential to increase data efficiency significantly. We propose\nContinual Representation using Distillation (CoReD) method that employs the\nconcept of Continual Learning (CL), Representation Learning (RL), and Knowledge\nDistillation (KD). We design CoReD to perform sequential domain adaptation\ntasks on new deepfake and GAN-generated synthetic face datasets, while\neffectively minimizing the catastrophic forgetting in a teacher-student model\nsetting. Our extensive experimental results demonstrate that our method is\nefficient at domain adaptation to detect low-quality deepfakes videos and\nGAN-generated images from several datasets, outperforming the-state-of-art\nbaseline methods.",
          "link": "http://arxiv.org/abs/2107.02408",
          "publishedOn": "2021-08-06T00:51:46.679Z",
          "wordCount": 710,
          "title": "CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation. (arXiv:2107.02408v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02335",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Malyuta_D/0/1/0/all/0/1\">Danylo Malyuta</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Elango_P/0/1/0/all/0/1\">Purnanand Elango</a>, <a href=\"http://arxiv.org/find/math/1/au:+Acikmese_B/0/1/0/all/0/1\">Behcet Acikmese</a>",
          "description": "Space mission design places a premium on cost and operational efficiency. The\nsearch for new science and life beyond Earth calls for spacecraft that can\ndeliver scientific payloads to geologically rich yet hazardous landing sites.\nAt the same time, the last four decades of optimization research have put a\nsuite of powerful optimization tools at the fingertips of the controls\nengineer. As we enter the new decade, optimization theory, algorithms, and\nsoftware tooling have reached a critical mass to start seeing serious\napplication in space vehicle guidance and control systems. This survey paper\nprovides a detailed overview of recent advances, successes, and promising\ndirections for optimization-based space vehicle control. The considered\napplications include planetary landing, rendezvous and proximity operations,\nsmall body landing, constrained reorientation, endo-atmospheric flight\nincluding ascent and re-entry, and orbit transfer and injection. The primary\nfocus is on the last ten years of progress, which have seen a veritable rise in\nthe number of applications using three core technologies: lossless\nconvexification, sequential convex programming, and model predictive control.\nThe reader will come away with a well-rounded understanding of the\nstate-of-the-art in each space vehicle control application, and will be well\npositioned to tackle important current open problems using convex optimization\nas a core technology.",
          "link": "http://arxiv.org/abs/2108.02335",
          "publishedOn": "2021-08-06T00:51:46.672Z",
          "wordCount": 662,
          "title": "Advances in Trajectory Optimization for Space Vehicle Control. (arXiv:2108.02335v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeilmann_A/0/1/0/all/0/1\">Alexander Zeilmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petra_S/0/1/0/all/0/1\">Stefania Petra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnorr_C/0/1/0/all/0/1\">Christoph Schn&#xf6;rr</a>",
          "description": "We introduce a novel algorithm for estimating optimal parameters of\nlinearized assignment flows for image labeling. An exact formula is derived for\nthe parameter gradient of any loss function that is constrained by the linear\nsystem of ODEs determining the linearized assignment flow. We show how to\nefficiently evaluate this formula using a Krylov subspace and a low-rank\napproximation. This enables us to perform parameter learning by Riemannian\ngradient descent in the parameter space, without the need to backpropagate\nerrors or to solve an adjoint equation, in less than 10 seconds for a\n$512\\times 512$ image using just about $0.5$ GB memory. Experiments demonstrate\nthat our method performs as good as highly-tuned machine learning software\nusing automatic differentiation. Unlike methods employing automatic\ndifferentiation, our approach yields a low-dimensional representation of\ninternal parameters and their dynamics which helps to understand how networks\nwork and perform that realize assignment flows and generalizations thereof.",
          "link": "http://arxiv.org/abs/2108.02571",
          "publishedOn": "2021-08-06T00:51:46.665Z",
          "wordCount": 582,
          "title": "Learning Linearized Assignment Flows for Image Labeling. (arXiv:2108.02571v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.05231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">HanQin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamm_K/0/1/0/all/0/1\">Keaton Hamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Longxiu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Needell_D/0/1/0/all/0/1\">Deanna Needell</a>",
          "description": "This paper considers the use of Robust PCA in a CUR decomposition framework\nand applications thereof. Our main algorithms produce a robust version of\ncolumn-row factorizations of matrices $\\mathbf{D}=\\mathbf{L}+\\mathbf{S}$ where\n$\\mathbf{L}$ is low-rank and $\\mathbf{S}$ contains sparse outliers. These\nmethods yield interpretable factorizations at low computational cost, and\nprovide new CUR decompositions that are robust to sparse outliers, in contrast\nto previous methods. We consider two key imaging applications of Robust PCA:\nvideo foreground-background separation and face modeling. This paper examines\nthe qualitative behavior of our Robust CUR decompositions on the benchmark\nvideos and face datasets, and find that our method works as well as standard\nRobust PCA while being significantly faster. Additionally, we consider hybrid\nrandomized and deterministic sampling methods which produce a compact CUR\ndecomposition of a given matrix, and apply this to video sequences to produce\ncanonical frames thereof.",
          "link": "http://arxiv.org/abs/2101.05231",
          "publishedOn": "2021-08-06T00:51:46.647Z",
          "wordCount": 624,
          "title": "Robust CUR Decomposition: Theory and Imaging Applications. (arXiv:2101.05231v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alman_J/0/1/0/all/0/1\">Josh Alman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1\">Timothy Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_G/0/1/0/all/0/1\">Gary Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shyam Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellke_M/0/1/0/all/0/1\">Mark Sellke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhao Song</a>",
          "description": "In this paper, we develop a new technique which we call representation theory\nof the real hyperrectangle, which describes how to compute the eigenvectors and\neigenvalues of certain matrices arising from hyperrectangles. We show that\nthese matrices arise naturally when analyzing a number of different algorithmic\ntasks such as kernel methods, neural network training, natural language\nprocessing, and the design of algorithms using the polynomial method. We then\nuse our new technique along with these connections to prove several new\nstructural results in these areas, including:\n\n$\\bullet$ A function is a positive definite Manhattan kernel if and only if\nit is a completely monotone function. These kernels are widely used across\nmachine learning; one example is the Laplace kernel which is widely used in\nmachine learning for chemistry.\n\n$\\bullet$ A function transforms Manhattan distances to Manhattan distances if\nand only if it is a Bernstein function. This completes the theory of Manhattan\nto Manhattan metric transforms initiated by Assouad in 1980.\n\n$\\bullet$ A function applied entry-wise to any square matrix of rank $r$\nalways results in a matrix of rank $< 2^{r-1}$ if and only if it is a\npolynomial of sufficiently low degree. This gives a converse to a key lemma\nused by the polynomial method in algorithm design.\n\nOur work includes a sophisticated combination of techniques from different\nfields, including metric embeddings, the polynomial method, and group\nrepresentation theory.",
          "link": "http://arxiv.org/abs/2011.11503",
          "publishedOn": "2021-08-06T00:51:46.641Z",
          "wordCount": 714,
          "title": "Metric Transforms and Low Rank Matrices via Representation Theory of the Real Hyperrectangle. (arXiv:2011.11503v2 [cs.CG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.02443",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pomponi_J/0/1/0/all/0/1\">Jary Pomponi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scardapane_S/0/1/0/all/0/1\">Simone Scardapane</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Uncini_A/0/1/0/all/0/1\">Aurelio Uncini</a>",
          "description": "Catastrophic forgetting (CF) happens whenever a neural network overwrites\npast knowledge while being trained on new tasks. Common techniques to handle CF\ninclude regularization of the weights (using, e.g., their importance on past\ntasks), and rehearsal strategies, where the network is constantly re-trained on\npast data. Generative models have also been applied for the latter, in order to\nhave endless sources of data. In this paper, we propose a novel method that\ncombines the strengths of regularization and generative-based rehearsal\napproaches. Our generative model consists of a normalizing flow (NF), a\nprobabilistic and invertible neural network, trained on the internal embeddings\nof the network. By keeping a single NF conditioned on the task, we show that\nour memory overhead remains constant. In addition, exploiting the invertibility\nof the NF, we propose a simple approach to regularize the network's embeddings\nwith respect to past tasks. We show that our method performs favorably with\nrespect to state-of-the-art approaches in the literature, with bounded\ncomputational power and memory overheads.",
          "link": "http://arxiv.org/abs/2007.02443",
          "publishedOn": "2021-08-06T00:51:46.634Z",
          "wordCount": 655,
          "title": "Pseudo-Rehearsal for Continual Learning with Normalizing Flows. (arXiv:2007.02443v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_B/0/1/0/all/0/1\">Binayak Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yingtao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthukumar_V/0/1/0/all/0/1\">Venkatesan Muthukumar</a>",
          "description": "The increasing application of deep learning technology drives the need for an\nefficient parallel computing architecture for Convolutional Neural Networks\n(CNNs). A significant challenge faced when designing a many-core CNN\naccelerator is to handle the data movement between the processing elements. The\nCNN workload introduces many-to-one traffic in addition to one-to-one and\none-to-many traffic. As the de-facto standard for on-chip communication,\nNetwork-on-Chip (NoC) can support various unicast and multicast traffic. For\nmany-to-one traffic, repetitive unicast is employed which is not an efficient\nway. In this paper, we propose to use the gather packet on mesh-based NoCs\nemploying output stationary systolic array in support of many-to-one traffic.\nThe gather packet will collect the data from the intermediate nodes eventually\nleading to the destination efficiently. This method is evaluated using the\ntraffic traces generated from the convolution layer of AlexNet and VGG-16 with\nimprovement in the latency and power than the repetitive unicast method.",
          "link": "http://arxiv.org/abs/2108.02567",
          "publishedOn": "2021-08-06T00:51:46.627Z",
          "wordCount": 605,
          "title": "Improving the Performance of a NoC-based CNN Accelerator with Gather Support. (arXiv:2108.02567v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mehra_V/0/1/0/all/0/1\">Vipul Mehra</a>",
          "description": "This thesis is divided into two parts:Part I: Analysis of Fruits, Vegetables,\nCheese and Fish based on Image Processing using Computer Vision and Deep\nLearning: A Review. It consists of a comprehensive review of image processing,\ncomputer vision and deep learning techniques applied to carry out analysis of\nfruits, vegetables, cheese and fish.This part also serves as a literature\nreview for Part II.Part II: GuavaNet: A deep neural network architecture for\nautomatic sensory evaluation to predict degree of acceptability for Guava by a\nconsumer. This part introduces to an end-to-end deep neural network\narchitecture that can predict the degree of acceptability by the consumer for a\nguava based on sensory evaluation.",
          "link": "http://arxiv.org/abs/2108.02563",
          "publishedOn": "2021-08-06T00:51:46.621Z",
          "wordCount": 574,
          "title": "GuavaNet: A deep neural network architecture for automatic sensory evaluation to predict degree of acceptability for Guava by a consumer. (arXiv:2108.02563v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Furui Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_F/0/1/0/all/0/1\">Fan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yanna Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zytek_A/0/1/0/all/0/1\">Alexandra Zytek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haomin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1\">Kalyan Veeramachaneni</a>",
          "description": "Machine learning (ML) is increasingly applied to Electronic Health Records\n(EHRs) to solve clinical prediction tasks. Although many ML models perform\npromisingly, issues with model transparency and interpretability limit their\nadoption in clinical practice. Directly using existing explainable ML\ntechniques in clinical settings can be challenging. Through literature surveys\nand collaborations with six clinicians with an average of 17 years of clinical\nexperience, we identified three key challenges, including clinicians'\nunfamiliarity with ML features, lack of contextual information, and the need\nfor cohort-level evidence. Following an iterative design process, we further\ndesigned and developed VBridge, a visual analytics tool that seamlessly\nincorporates ML explanations into clinicians' decision-making workflow. The\nsystem includes a novel hierarchical display of contribution-based feature\nexplanations and enriched interactions that connect the dots between ML\nfeatures, explanations, and data. We demonstrated the effectiveness of VBridge\nthrough two case studies and expert interviews with four clinicians, showing\nthat visually associating model explanations with patients' situational records\ncan help clinicians better interpret and use model predictions when making\nclinician decisions. We further derived a list of design implications for\ndeveloping future explainable ML tools to support clinical decision-making.",
          "link": "http://arxiv.org/abs/2108.02550",
          "publishedOn": "2021-08-06T00:51:46.597Z",
          "wordCount": 664,
          "title": "VBridge: Connecting the Dots Between Features, Explanations, and Data for Healthcare Models. (arXiv:2108.02550v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Towey_D/0/1/0/all/0/1\">Dave Towey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+French_A/0/1/0/all/0/1\">Andrew French</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benford_S/0/1/0/all/0/1\">Steve Benford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhi Quan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tsong Yueh Chen</a>",
          "description": "Software testing is often hindered where it is impossible or impractical to\ndetermine the correctness of the behaviour or output of the software under test\n(SUT), a situation known as the oracle problem. An example of an area facing\nthe oracle problem is automatic image classification, using machine learning to\nclassify an input image as one of a set of predefined classes. An approach to\nsoftware testing that alleviates the oracle problem is metamorphic testing\n(MT). While traditional software testing examines the correctness of individual\ntest cases, MT instead examines the relations amongst multiple executions of\ntest cases and their outputs. These relations are called metamorphic relations\n(MRs): if an MR is found to be violated, then a fault must exist in the SUT.\nThis paper examines the problem of classifying images containing visually\nhidden markers called Artcodes, and applies MT to verify and enhance the\ntrained classifiers. This paper further examines two MRs, Separation and\nOcclusion, and reports on their capability in verifying the image\nclassification using one-way analysis of variance (ANOVA) in conjunction with\nthree other statistical analysis methods: t-test (for unequal variances),\nKruskal-Wallis test, and Dunnett's test. In addition to our previously-studied\nclassifier, that used Random Forests, we introduce a new classifier that uses a\nsupport vector machine, and present its MR-augmented version. Experimental\nevaluations across a number of performance metrics show that the augmented\nclassifiers can achieve better performance than non-augmented classifiers. This\npaper also analyses how the enhanced performance is obtained.",
          "link": "http://arxiv.org/abs/2108.02694",
          "publishedOn": "2021-08-06T00:51:46.590Z",
          "wordCount": 696,
          "title": "Using Metamorphic Relations to Verify and Enhance Artcode Classification. (arXiv:2108.02694v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuang_H/0/1/0/all/0/1\">Haofei Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwertfeger_S/0/1/0/all/0/1\">S&#xf6;ren Schwertfeger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stachniss_C/0/1/0/all/0/1\">Cyrill Stachniss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>",
          "description": "Contrastive learning has revolutionized self-supervised image representation\nlearning field, and recently been adapted to video domain. One of the greatest\nadvantages of contrastive learning is that it allows us to flexibly define\npowerful loss objectives as long as we can find a reasonable way to formulate\npositive and negative samples to contrast. However, existing approaches rely\nheavily on the short-range spatiotemporal salience to form clip-level\ncontrastive signals, thus limit themselves from using global context. In this\npaper, we propose a new video-level contrastive learning method based on\nsegments to formulate positive pairs. Our formulation is able to capture global\ncontext in a video, thus robust to temporal content change. We also incorporate\na temporal order regularization term to enforce the inherent sequential\nstructure of videos. Extensive experiments show that our video-level\ncontrastive learning framework (VCLR) is able to outperform previous\nstate-of-the-arts on five video datasets for downstream action classification,\naction localization and video retrieval. Code is available at\nhttps://github.com/amazon-research/video-contrastive-learning.",
          "link": "http://arxiv.org/abs/2108.02722",
          "publishedOn": "2021-08-06T00:51:46.583Z",
          "wordCount": 618,
          "title": "Video Contrastive Learning with Global Context. (arXiv:2108.02722v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jingyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1\">Qi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1\">Hongyang Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "Unsupervised learning is just at a tipping point where it could really take\noff. Among these approaches, contrastive learning has seen tremendous progress\nand led to state-of-the-art performance. In this paper, we construct a novel\nprobabilistic graphical model that effectively incorporates the low rank\npromoting prior into the framework of contrastive learning, referred to as\nLORAC. In contrast to the existing conventional self-supervised approaches that\nonly considers independent learning, our hypothesis explicitly requires that\nall the samples belonging to the same instance class lie on the same subspace\nwith small dimension. This heuristic poses particular joint learning\nconstraints to reduce the degree of freedom of the problem during the search of\nthe optimal network parameterization. Most importantly, we argue that the low\nrank prior employed here is not unique, and many different priors can be\ninvoked in a similar probabilistic way, corresponding to different hypotheses\nabout underlying truth behind the contrastive features. Empirical evidences\nshow that the proposed algorithm clearly surpasses the state-of-the-art\napproaches on multiple benchmarks, including image classification, object\ndetection, instance segmentation and keypoint detection.",
          "link": "http://arxiv.org/abs/2108.02696",
          "publishedOn": "2021-08-06T00:51:46.521Z",
          "wordCount": 629,
          "title": "A Low Rank Promoting Prior for Unsupervised Contrastive Learning. (arXiv:2108.02696v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Doran_H/0/1/0/all/0/1\">Hans Dermot Doran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ielpo_G/0/1/0/all/0/1\">Gianluca Ielpo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganz_D/0/1/0/all/0/1\">David Ganz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zapke_M/0/1/0/all/0/1\">Michael Zapke</a>",
          "description": "With edge-AI finding an increasing number of real-world applications,\nespecially in industry, the question of functionally safe applications using AI\nhas begun to be asked. In this body of work, we explore the issue of achieving\ndependable operation of neural networks. We discuss the issue of dependability\nin general implementation terms before examining lockstep solutions. We intuit\nthat it is not necessarily a given that two similar neural networks generate\nresults at precisely the same time and that synchronization between the\nplatforms will be required. We perform some preliminary measurements that may\nsupport this intuition and introduce some work in implementing lockstep neural\nnetwork engines.",
          "link": "http://arxiv.org/abs/2108.02565",
          "publishedOn": "2021-08-06T00:51:46.514Z",
          "wordCount": 573,
          "title": "Dependable Neural Networks Through Redundancy, A Comparison of Redundant Architectures. (arXiv:2108.02565v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patil_M/0/1/0/all/0/1\">Mihir Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wehbe_B/0/1/0/all/0/1\">Bilal Wehbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1\">Matias Valdenegro-Toro</a>",
          "description": "Docking control of an autonomous underwater vehicle (AUV) is a task that is\nintegral to achieving persistent long term autonomy. This work explores the\napplication of state-of-the-art model-free deep reinforcement learning (DRL)\napproaches to the task of AUV docking in the continuous domain. We provide a\ndetailed formulation of the reward function, utilized to successfully dock the\nAUV onto a fixed docking platform. A major contribution that distinguishes our\nwork from the previous approaches is the usage of a physics simulator to define\nand simulate the underwater environment as well as the DeepLeng AUV. We propose\na new reward function formulation for the docking task, incorporating several\ncomponents, that outperforms previous reward formulations. We evaluate proximal\npolicy optimization (PPO), twin delayed deep deterministic policy gradients\n(TD3) and soft actor-critic (SAC) in combination with our reward function. Our\nevaluation yielded results that conclusively show the TD3 agent to be most\nefficient and consistent in terms of docking the AUV, over multiple evaluation\nruns it achieved a 100% success rate and episode return of 10667.1 +- 688.8. We\nalso show how our reward function formulation improves over the state of the\nart.",
          "link": "http://arxiv.org/abs/2108.02665",
          "publishedOn": "2021-08-06T00:51:46.507Z",
          "wordCount": 641,
          "title": "Deep Reinforcement Learning for Continuous Docking Control of Autonomous Underwater Vehicles: A Benchmarking Study. (arXiv:2108.02665v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02776",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hono_Y/0/1/0/all/0/1\">Yukiya Hono</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kei Hashimoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oura_K/0/1/0/all/0/1\">Keiichiro Oura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nankaku_Y/0/1/0/all/0/1\">Yoshihiko Nankaku</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tokuda_K/0/1/0/all/0/1\">Keiichi Tokuda</a>",
          "description": "This paper presents Sinsy, a deep neural network (DNN)-based singing voice\nsynthesis (SVS) system. In recent years, DNNs have been utilized in statistical\nparametric SVS systems, and DNN-based SVS systems have demonstrated better\nperformance than conventional hidden Markov model-based ones. SVS systems are\nrequired to synthesize a singing voice with pitch and timing that strictly\nfollow a given musical score. Additionally, singing expressions that are not\ndescribed on the musical score, such as vibrato and timing fluctuations, should\nbe reproduced. The proposed system is composed of four modules: a time-lag\nmodel, a duration model, an acoustic model, and a vocoder, and singing voices\ncan be synthesized taking these characteristics of singing voices into account.\nTo better model a singing voice, the proposed system incorporates improved\napproaches to modeling pitch and vibrato and better training criteria into the\nacoustic model. In addition, we incorporated PeriodNet, a non-autoregressive\nneural vocoder with robustness for the pitch, into our systems to generate a\nhigh-fidelity singing voice waveform. Moreover, we propose automatic pitch\ncorrection techniques for DNN-based SVS to synthesize singing voices with\ncorrect pitch even if the training data has out-of-tune phrases. Experimental\nresults show our system can synthesize a singing voice with better timing, more\nnatural vibrato, and correct pitch, and it can achieve better mean opinion\nscores in subjective evaluation tests.",
          "link": "http://arxiv.org/abs/2108.02776",
          "publishedOn": "2021-08-06T00:51:46.498Z",
          "wordCount": 690,
          "title": "Sinsy: A Deep Neural Network-Based Singing Voice Synthesis System. (arXiv:2108.02776v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Novaes_A/0/1/0/all/0/1\">Andre Luiz Farias Novaes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_R/0/1/0/all/0/1\">Rui Alexandre de Matos Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figueiredo_J/0/1/0/all/0/1\">Jose Figueiredo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavanelli_L/0/1/0/all/0/1\">Lucas Aguiar Pavanelli</a>",
          "description": "Meter-level load forecasting is crucial for efficient energy management and\npower system planning for Smart Grids (SGs), in tasks associated with\nregulation, dispatching, scheduling, and unit commitment of power grids.\nAlthough a variety of algorithms have been proposed and applied on the field,\nmore accurate and robust models are still required: the overall utility cost of\noperations in SGs increases 10 million currency units if the load forecasting\nerror increases 1%, and the mean absolute percentage error (MAPE) in\nforecasting is still much higher than 1%. Transformers have become the new\nstate-of-the-art in a variety of tasks, including the ones in computer vision,\nnatural language processing and time series forecasting, surpassing alternative\nneural models such as convolutional and recurrent neural networks. In this\nletter, we present a new state-of-the-art Transformer-based algorithm for the\nmeter-level load forecasting task, which has surpassed the former\nstate-of-the-art, LSTM, and the traditional benchmark, vanilla RNN, in all\nexperiments by a margin of at least 13% in MAPE.",
          "link": "http://arxiv.org/abs/2108.02628",
          "publishedOn": "2021-08-06T00:51:46.475Z",
          "wordCount": 604,
          "title": "A New State-of-the-Art Transformers-Based Load Forecaster on the Smart Grid Domain. (arXiv:2108.02628v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hornauer_J/0/1/0/all/0/1\">Julia Hornauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nalpantidis_L/0/1/0/all/0/1\">Lazaros Nalpantidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>",
          "description": "Real-world perception systems in many cases build on hardware with limited\nresources to adhere to cost and power limitations of their carrying system.\nDeploying deep neural networks on resource-constrained hardware became possible\nwith model compression techniques, as well as efficient and hardware-aware\narchitecture design. However, model adaptation is additionally required due to\nthe diverse operation environments. In this work, we address the problem of\ntraining deep neural networks on resource-constrained hardware in the context\nof visual domain adaptation. We select the task of monocular depth estimation\nwhere our goal is to transform a pre-trained model to the target's domain data.\nWhile the source domain includes labels, we assume an unlabelled target domain,\nas it happens in real-world applications. Then, we present an adversarial\nlearning approach that is adapted for training on the device with limited\nresources. Since visual domain adaptation, i.e. neural network training, has\nnot been previously explored for resource-constrained hardware, we present the\nfirst feasibility study for image-based depth estimation. Our experiments show\nthat visual domain adaptation is relevant only for efficient network\narchitectures and training sets at the order of a few hundred samples. Models\nand code are publicly available.",
          "link": "http://arxiv.org/abs/2108.02671",
          "publishedOn": "2021-08-06T00:51:46.467Z",
          "wordCount": 650,
          "title": "Visual Domain Adaptation for Monocular Depth Estimation on Resource-Constrained Hardware. (arXiv:2108.02671v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ilin_V/0/1/0/all/0/1\">Valery Ilin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinov_I/0/1/0/all/0/1\">Ivan Kalinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpyshev_P/0/1/0/all/0/1\">Pavel Karpyshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsetserukou_D/0/1/0/all/0/1\">Dzmitry Tsetserukou</a>",
          "description": "In the proposed study, we describe the possibility of automated dataset\ncollection using an articulated robot. The proposed technology reduces the\nnumber of pixel errors on a polygonal dataset and the time spent on manual\nlabeling of 2D objects. The paper describes a novel automatic dataset\ncollection and annotation system, and compares the results of automated and\nmanual dataset labeling. Our approach increases the speed of data labeling\n240-fold, and improves the accuracy compared to manual labeling 13-fold. We\nalso present a comparison of metrics for training a neural network on a\nmanually annotated and an automatically collected dataset.",
          "link": "http://arxiv.org/abs/2108.02555",
          "publishedOn": "2021-08-06T00:51:46.445Z",
          "wordCount": 559,
          "title": "DeepScanner: a Robotic System for Automated 2D Object Dataset Collection with Annotations. (arXiv:2108.02555v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esteves_J/0/1/0/all/0/1\">Jose Jurandir Alves Esteves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boubendir_A/0/1/0/all/0/1\">Amina Boubendir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillemin_F/0/1/0/all/0/1\">Fabrice Guillemin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sens_P/0/1/0/all/0/1\">Pierre Sens</a>",
          "description": "The evaluation of the impact of using Machine Learning in the management of\nsoftwarized networks is considered in multiple research works. Beyond that, we\npropose to evaluate the robustness of online learning for optimal network slice\nplacement. A major assumption to this study is to consider that slice request\narrivals are non-stationary. In this context, we simulate unpredictable network\nload variations and compare two Deep Reinforcement Learning (DRL) algorithms: a\npure DRL-based algorithm and a heuristically controlled DRL as a hybrid\nDRL-heuristic algorithm, to assess the impact of these unpredictable changes of\ntraffic load on the algorithms performance. We conduct extensive simulations of\na large-scale operator infrastructure. The evaluation results show that the\nproposed hybrid DRL-heuristic approach is more robust and reliable in case of\nunpredictable network load changes than pure DRL as it reduces the performance\ndegradation. These results are follow-ups for a series of recent research we\nhave performed showing that the proposed hybrid DRL-heuristic approach is\nefficient and more adapted to real network scenarios than pure DRL.",
          "link": "http://arxiv.org/abs/2108.02505",
          "publishedOn": "2021-08-06T00:51:46.419Z",
          "wordCount": 618,
          "title": "On the Robustness of Controlled Deep Reinforcement Learning for Slice Placement. (arXiv:2108.02505v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02393",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Abouheaf_M/0/1/0/all/0/1\">Mohammed Abouheaf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gueaieb_W/0/1/0/all/0/1\">Wail Gueaieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lewis_F/0/1/0/all/0/1\">Frank Lewis</a>",
          "description": "The control problem of the flexible wing aircraft is challenging due to the\nprevailing and high nonlinear deformations in the flexible wing system. This\nurged for new control mechanisms that are robust to the real-time variations in\nthe wing's aerodynamics. An online control mechanism based on a value iteration\nreinforcement learning process is developed for flexible wing aerial\nstructures. It employs a model-free control policy framework and a guaranteed\nconvergent adaptive learning architecture to solve the system's Bellman\noptimality equation. A Riccati equation is derived and shown to be equivalent\nto solving the underlying Bellman equation. The online reinforcement learning\nsolution is implemented using means of an adaptive-critic mechanism. The\ncontroller is proven to be asymptotically stable in the Lyapunov sense. It is\nassessed through computer simulations and its superior performance is\ndemonstrated on two scenarios under different operating conditions.",
          "link": "http://arxiv.org/abs/2108.02393",
          "publishedOn": "2021-08-06T00:51:46.400Z",
          "wordCount": 613,
          "title": "Online Model-Free Reinforcement Learning for the Automatic Control of a Flexible Wing Aircraft. (arXiv:2108.02393v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02495",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esteves_J/0/1/0/all/0/1\">Jose Jurandir Alves Esteves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boubendir_A/0/1/0/all/0/1\">Amina Boubendir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillemin_F/0/1/0/all/0/1\">Fabrice Guillemin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sens_P/0/1/0/all/0/1\">Pierre Sens</a>",
          "description": "We consider online learning for optimal network slice placement under the\nassumption that slice requests arrive according to a non-stationary Poisson\nprocess. We propose a framework based on Deep Reinforcement Learning (DRL)\ncombined with a heuristic to design algorithms. We specifically design two\npure-DRL algorithms and two families of hybrid DRL-heuristic algorithms. To\nvalidate their performance, we perform extensive simulations in the context of\na large-scale operator infrastructure. The evaluation results show that the\nproposed hybrid DRL-heuristic algorithms require three orders of magnitude of\nlearning episodes less than pure-DRL to achieve convergence. This result\nindicates that the proposed hybrid DRL-heuristic approach is more reliable than\npure-DRL in a real non-stationary network scenario.",
          "link": "http://arxiv.org/abs/2108.02495",
          "publishedOn": "2021-08-06T00:51:46.393Z",
          "wordCount": 550,
          "title": "DRL-based Slice Placement Under Non-Stationary Conditions. (arXiv:2108.02495v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Haotian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhengyang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>",
          "description": "This paper proposes a hypothesis for the aesthetic appreciation that\naesthetic images make a neural network strengthen salient concepts and discard\ninessential concepts. In order to verify this hypothesis, we use multi-variate\ninteractions to represent salient concepts and inessential concepts contained\nin images. Furthermore, we design a set of operations to revise images towards\nmore beautiful ones. In experiments, we find that the revised images are more\naesthetic than the original ones to some extent.",
          "link": "http://arxiv.org/abs/2108.02646",
          "publishedOn": "2021-08-06T00:51:46.386Z",
          "wordCount": 525,
          "title": "A Hypothesis for the Aesthetic Appreciation in Neural Networks. (arXiv:2108.02646v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Siyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Dan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guannan Zhang</a>",
          "description": "We propose a novel prediction interval method to learn prediction mean\nvalues, lower and upper bounds of prediction intervals from three independently\ntrained neural networks only using the standard mean squared error (MSE) loss,\nfor uncertainty quantification in regression tasks. Our method requires no\ndistributional assumption on data, does not introduce unusual hyperparameters\nto either the neural network models or the loss function. Moreover, our method\ncan effectively identify out-of-distribution samples and reasonably quantify\ntheir uncertainty. Numerical experiments on benchmark regression problems show\nthat our method outperforms the state-of-the-art methods with respect to\npredictive uncertainty quality, robustness, and identification of\nout-of-distribution samples.",
          "link": "http://arxiv.org/abs/2108.02327",
          "publishedOn": "2021-08-06T00:51:46.379Z",
          "wordCount": 534,
          "title": "PI3NN: Prediction intervals from three independently trained neural networks. (arXiv:2108.02327v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02424",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_T/0/1/0/all/0/1\">Tiange Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1\">Zijun Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tsui_K/0/1/0/all/0/1\">Kwok-Leung Tsui</a>",
          "description": "Accurate forecasting of traffic conditions is critical for improving safety,\nstability, and efficiency of a city transportation system. In reality, it is\nchallenging to produce accurate traffic forecasts due to the complex and\ndynamic spatiotemporal correlations. Most existing works only consider partial\ncharacteristics and features of traffic data, and result in unsatisfactory\nperformances on modeling and forecasting. In this paper, we propose a periodic\nspatial-temporal deep neural network (PSTN) with three pivotal modules to\nimprove the forecasting performance of traffic conditions through a novel\nintegration of three types of information. First, the historical traffic\ninformation is folded and fed into a module consisting of a graph convolutional\nnetwork and a temporal convolutional network. Second, the recent traffic\ninformation together with the historical output passes through the second\nmodule consisting of a graph convolutional network and a gated recurrent unit\nframework. Finally, a multi-layer perceptron is applied to process the\nauxiliary road attributes and output the final predictions. Experimental\nresults on two publicly accessible real-world urban traffic data sets show that\nthe proposed PSTN outperforms the state-of-the-art benchmarks by significant\nmargins for short-term traffic conditions forecasting",
          "link": "http://arxiv.org/abs/2108.02424",
          "publishedOn": "2021-08-06T00:51:46.373Z",
          "wordCount": 621,
          "title": "PSTN: Periodic Spatial-temporal Deep Neural Network for Traffic Condition Prediction. (arXiv:2108.02424v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02537",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Bharadwaj_P/0/1/0/all/0/1\">Pawan Bharadwaj</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_M/0/1/0/all/0/1\">Matthew Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Demanet_L/0/1/0/all/0/1\">Laurent Demanet</a>",
          "description": "This paper considers physical systems described by hidden states and\nindirectly observed through repeated measurements corrupted by unmodeled\nnuisance parameters. A network-based representation learns to disentangle the\ncoherent information (relative to the state) from the incoherent nuisance\ninformation (relative to the sensing). Instead of physical models, the\nrepresentation uses symmetry and stochastic regularization to inform an\nautoencoder architecture called SymAE. It enables redatuming, i.e., creating\nvirtual data instances where the nuisances are uniformized across measurements.",
          "link": "http://arxiv.org/abs/2108.02537",
          "publishedOn": "2021-08-06T00:51:46.367Z",
          "wordCount": 514,
          "title": "Redatuming physical systems using symmetric autoencoders. (arXiv:2108.02537v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_F/0/1/0/all/0/1\">Flavio Abreu Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riou_M/0/1/0/all/0/1\">Mathieu Riou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torrejon_J/0/1/0/all/0/1\">Jacob Torrejon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravelosona_D/0/1/0/all/0/1\">Dafin&#xe9; Ravelosona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1\">Wang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weisheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grollier_J/0/1/0/all/0/1\">Julie Grollier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Querlioz_D/0/1/0/all/0/1\">Damien Querlioz</a>",
          "description": "Deep learning has an increasing impact to assist research, allowing, for\nexample, the discovery of novel materials. Until now, however, these artificial\nintelligence techniques have fallen short of discovering the full differential\nequation of an experimental physical system. Here we show that a dynamical\nneural network, trained on a minimal amount of data, can predict the behavior\nof spintronic devices with high accuracy and an extremely efficient simulation\ntime, compared to the micromagnetic simulations that are usually employed to\nmodel them. For this purpose, we re-frame the formalism of Neural Ordinary\nDifferential Equations (ODEs) to the constraints of spintronics: few measured\noutputs, multiple inputs and internal parameters. We demonstrate with\nSpin-Neural ODEs an acceleration factor over 200 compared to micromagnetic\nsimulations for a complex problem -- the simulation of a reservoir computer\nmade of magnetic skyrmions (20 minutes compared to three days). In a second\nrealization, we show that we can predict the noisy response of experimental\nspintronic nano-oscillators to varying inputs after training Spin-Neural ODEs\non five milliseconds of their measured response to different excitations.\nSpin-Neural ODE is a disruptive tool for developing spintronic applications in\ncomplement to micromagnetic simulations, which are time-consuming and cannot\nfit experiments when noise or imperfections are present. Spin-Neural ODE can\nalso be generalized to other electronic devices involving dynamics.",
          "link": "http://arxiv.org/abs/2108.02318",
          "publishedOn": "2021-08-06T00:51:46.348Z",
          "wordCount": 695,
          "title": "Forecasting the outcome of spintronic experiments with Neural Ordinary Differential Equations. (arXiv:2108.02318v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahara_S/0/1/0/all/0/1\">Sawan Singh Mahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M%2E_S/0/1/0/all/0/1\">Shruti M.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharath_B/0/1/0/all/0/1\">B. N. Bharath</a>",
          "description": "Federated Learning (FL) has evolved as a promising technique to handle\ndistributed machine learning across edge devices. A single neural network (NN)\nthat optimises a global objective is generally learned in most work in FL,\nwhich could be suboptimal for edge devices. Although works finding a NN\npersonalised for edge device specific tasks exist, they lack generalisation\nand/or convergence guarantees. In this paper, a novel communication efficient\nFL algorithm for personalised learning in a wireless setting with guarantees is\npresented. The algorithm relies on finding a ``better`` empirical estimate of\nlosses at each device, using a weighted average of the losses across different\ndevices. It is devised from a Probably Approximately Correct (PAC) bound on the\ntrue loss in terms of the proposed empirical loss and is bounded by (i) the\nRademacher complexity, (ii) the discrepancy, (iii) and a penalty term. Using a\nsigned gradient feedback to find a personalised NN at each device, it is also\nproven to converge in a Rayleigh flat fading (in the uplink) channel, at a rate\nof the order max{1/SNR,1/sqrt(T)} Experimental results show that the proposed\nalgorithm outperforms locally trained devices as well as the conventionally\nused FedAvg and FedSGD algorithms under practical SNR regimes.",
          "link": "http://arxiv.org/abs/2108.02517",
          "publishedOn": "2021-08-06T00:51:46.337Z",
          "wordCount": 641,
          "title": "Multi-task Federated Edge Learning (MtFEEL) in Wireless Networks. (arXiv:2108.02517v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02307",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Dogan_I/0/1/0/all/0/1\">Ilgin Dogan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Shen_Z/0/1/0/all/0/1\">Zuo-Jun Max Shen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Aswani_A/0/1/0/all/0/1\">Anil Aswani</a>",
          "description": "The exploration/exploitation trade-off is an inherent challenge in\ndata-driven and adaptive control. Though this trade-off has been studied for\nmulti-armed bandits, reinforcement learning (RL) for finite Markov chains, and\nRL for linear control systems; it is less well-studied for learning-based\ncontrol of nonlinear control systems. A significant theoretical challenge in\nthe nonlinear setting is that, unlike the linear case, there is no explicit\ncharacterization of an optimal controller for a given set of cost and system\nparameters. We propose in this paper the use of a finite-horizon oracle\ncontroller with perfect knowledge of all system parameters as a reference for\noptimal control actions. First, this allows us to propose a new regret notion\nwith respect to this oracle finite-horizon controller. Second, this allows us\nto develop learning-based policies that we prove achieve low regret (i.e.,\nsquare-root regret up to a log-squared factor) with respect to this oracle\nfinite-horizon controller. This policy is developed in the context of\nlearning-based model predictive control (LBMPC). We conduct a statistical\nanalysis to prove finite sample concentration bounds for the estimation step of\nour policy, and then we perform a control-theoretic analysis using techniques\nfrom MPC- and optimization-theory to show this policy ensures closed-loop\nstability and achieves low regret. We conclude with numerical experiments on a\nmodel of heating, ventilation, and air-conditioning (HVAC) systems that show\nthe low regret of our policy in a setting where the cost function is\npartially-unknown to the controller.",
          "link": "http://arxiv.org/abs/2108.02307",
          "publishedOn": "2021-08-06T00:51:46.229Z",
          "wordCount": 681,
          "title": "Regret Analysis of Learning-Based MPC with Partially-Unknown Cost Function. (arXiv:2108.02307v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eisermann_A/0/1/0/all/0/1\">Aaron Eisermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jae Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>",
          "description": "Neural networks can be powerful function approximators, which are able to\nmodel high-dimensional feature distributions from a subset of examples drawn\nfrom the target distribution. Naturally, they perform well at generalizing\nwithin the limits of their target function, but they often fail to generalize\noutside of the explicitly learned feature space. It is therefore an open\nresearch topic whether and how neural network-based architectures can be\ndeployed for systematic reasoning. Many studies have shown evidence for poor\ngeneralization, but they often work with abstract data or are limited to\nsingle-channel input. Humans, however, learn and interact through a combination\nof multiple sensory modalities, and rarely rely on just one. To investigate\ncompositional generalization in a multimodal setting, we generate an extensible\ndataset with multimodal input sequences from simulation. We investigate the\ninfluence of the underlying training data distribution on compostional\ngeneralization in a minimal LSTM-based network trained in a supervised, time\ncontinuous setting. We find compositional generalization to fail in simple\nsetups while improving with the number of objects, actions, and particularly\nwith a lot of color overlaps between objects. Furthermore, multimodality\nstrongly improves compositional generalization in settings where a pure vision\nmodel struggles to generalize.",
          "link": "http://arxiv.org/abs/2108.02319",
          "publishedOn": "2021-08-06T00:51:46.200Z",
          "wordCount": 630,
          "title": "Generalization in Multimodal Language Learning from Simulation. (arXiv:2108.02319v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Favaro_S/0/1/0/all/0/1\">Stefano Favaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortini_S/0/1/0/all/0/1\">Sandra Fortini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peluchetti_S/0/1/0/all/0/1\">Stefano Peluchetti</a>",
          "description": "In modern deep learning, there is a recent and growing literature on the\ninterplay between large-width asymptotics for deep Gaussian neural networks\n(NNs), i.e. deep NNs with Gaussian-distributed weights, and classes of Gaussian\nstochastic processes (SPs). Such an interplay has proved to be critical in\nseveral contexts of practical interest, e.g. Bayesian inference under Gaussian\nSP priors, kernel regression for infinite-wide deep NNs trained via gradient\ndescent, and information propagation within infinite-wide NNs. Motivated by\nempirical analysis, showing the potential of replacing Gaussian distributions\nwith Stable distributions for the NN's weights, in this paper we investigate\nlarge-width asymptotics for (fully connected) feed-forward deep Stable NNs,\ni.e. deep NNs with Stable-distributed weights. First, we show that as the width\ngoes to infinity jointly over the NN's layers, a suitable rescaled deep Stable\nNN converges weakly to a Stable SP whose distribution is characterized\nrecursively through the NN's layers. Because of the non-triangular NN's\nstructure, this is a non-standard asymptotic problem, to which we propose a\nnovel and self-contained inductive approach, which may be of independent\ninterest. Then, we establish sup-norm convergence rates of a deep Stable NN to\na Stable SP, quantifying the critical difference between the settings of\n``joint growth\" and ``sequential growth\" of the width over the NN's layers. Our\nwork extends recent results on infinite-wide limits for deep Gaussian NNs to\nthe more general deep Stable NNs, providing the first result on convergence\nrates for infinite-wide deep NNs.",
          "link": "http://arxiv.org/abs/2108.02316",
          "publishedOn": "2021-08-06T00:51:46.193Z",
          "wordCount": 698,
          "title": "Deep Stable neural networks: large-width asymptotics and convergence rates. (arXiv:2108.02316v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02416",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Konstantinidis_K/0/1/0/all/0/1\">Konstantinos Konstantinidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthy_A/0/1/0/all/0/1\">Aditya Ramamoorthy</a>",
          "description": "State of the art machine learning models are routinely trained on large scale\ndistributed clusters. Crucially, such systems can be compromised when some of\nthe computing devices exhibit abnormal (Byzantine) behavior and return\narbitrary results to the parameter server (PS). This behavior may be attributed\nto a plethora of reasons including system failures and orchestrated attacks.\nExisting work suggests robust aggregation and/or computational redundancy to\nalleviate the effect of distorted gradients. However, most of these schemes are\nineffective when an adversary knows the task assignment and can judiciously\nchoose the attacked workers to induce maximal damage. Our proposed method Aspis\nassigns gradient computations to worker nodes using a subset-based assignment\nwhich allows for multiple consistency checks on the behavior of a worker node.\nExamination of the calculated gradients and post-processing (clique-finding in\nan appropriately constructed graph) by the central node allows for efficient\ndetection and subsequent exclusion of adversaries from the training process. We\nprove the Byzantine resilience and detection guarantees of Aspis under weak and\nstrong attacks and extensively evaluate the system on various large-scale\ntraining scenarios. The main metric for our experiments is the test accuracy\nfor which we demonstrate significant improvement of about 30% compared to many\nstate-of-the-art approaches on the CIFAR-10 dataset. The corresponding\nreduction of the fraction of corrupted gradients ranges from 16% to 98%.",
          "link": "http://arxiv.org/abs/2108.02416",
          "publishedOn": "2021-08-06T00:51:46.184Z",
          "wordCount": 668,
          "title": "Aspis: A Robust Detection System for Distributed Learning. (arXiv:2108.02416v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Peng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>",
          "description": "The past decade has seen the rapid development of Reinforcement Learning,\nwhich acquires impressive performance with numerous training resources.\nHowever, one of the greatest challenges in RL is generalization efficiency\n(i.e., generalization performance in a unit time). This paper proposes a\nframework of Active Reinforcement Learning (ARL) over MDPs to improve\ngeneralization efficiency in a limited resource by instance selection. Given a\nnumber of instances, the algorithm chooses out valuable instances as training\nsets while training the policy, thereby costing fewer resources. Unlike\nexisting approaches, we attempt to actively select and use training data rather\nthan train on all the given data, thereby costing fewer resources. Furthermore,\nwe introduce a general instance evaluation metrics and selection mechanism into\nthe framework. Experiments results reveal that the proposed framework with\nProximal Policy Optimization as policy optimizer can effectively improve\ngeneralization efficiency than unselect-ed and unbiased selected methods.",
          "link": "http://arxiv.org/abs/2108.02323",
          "publishedOn": "2021-08-06T00:51:46.175Z",
          "wordCount": 568,
          "title": "Active Reinforcement Learning over MDPs. (arXiv:2108.02323v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Terrill_C/0/1/0/all/0/1\">Caleb Terrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_F/0/1/0/all/0/1\">Fred Chu</a>",
          "description": "Modern hardware design trends have shifted towards specialized hardware\nacceleration for computationally intensive tasks like machine learning and\ncomputer vision. While these complex workloads can be accelerated by commercial\nGPUs, domain-specific hardware is far more optimal when needing to meet the\nstringent memory, throughput, and power constraints of mobile and embedded\ndevices. This paper proposes and evaluates a Binary-Enabled Architecture for\nNeural Network Acceleration (BEANNA), a neural network hardware accelerator\ncapable of processing both floating point and binary network layers. Through\nthe use of a novel 16x16 systolic array based matrix multiplier with processing\nelements that compute both floating point and binary multiply-adds, BEANNA\nseamlessly switches between high precision floating point and binary neural\nnetwork layers. Running at a clock speed of 100MHz, BEANNA achieves a peak\nthroughput of 52.8 GigaOps/second when operating in high precision mode, and\n820 GigaOps/second when operating in binary mode. Evaluation of BEANNA was\nperformed by comparing a hybrid network with floating point outer layers and\nbinary hidden layers to a network with only floating point layers. The hybrid\nnetwork accelerated using BEANNA achieved a 194% throughput increase, a 68%\nmemory usage decrease, and a 66% energy consumption decrease per inference, all\nthis at the cost of a mere 0.23% classification accuracy decrease on the MNIST\ndataset.",
          "link": "http://arxiv.org/abs/2108.02313",
          "publishedOn": "2021-08-06T00:51:46.168Z",
          "wordCount": 653,
          "title": "BEANNA: A Binary-Enabled Architecture for Neural Network Acceleration. (arXiv:2108.02313v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Quiros_A/0/1/0/all/0/1\">Adalberto Claudio Quiros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coudray_N/0/1/0/all/0/1\">Nicolas Coudray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeaton_A/0/1/0/all/0/1\">Anna Yeaton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunhem_W/0/1/0/all/0/1\">Wisuwat Sunhem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_Smith_R/0/1/0/all/0/1\">Roderick Murray-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsirigos_A/0/1/0/all/0/1\">Aristotelis Tsirigos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Ke Yuan</a>",
          "description": "Deep learning based analysis of histopathology images shows promise in\nadvancing the understanding of tumor progression, tumor micro-environment, and\ntheir underpinning biological processes. So far, these approaches have focused\non extracting information associated with annotations. In this work, we ask how\nmuch information can be learned from the tissue architecture itself.\n\nWe present an adversarial learning model to extract feature representations\nof cancer tissue, without the need for manual annotations. We show that these\nrepresentations are able to identify a variety of morphological characteristics\nacross three cancer types: Breast, colon, and lung. This is supported by 1) the\nseparation of morphologic characteristics in the latent space; 2) the ability\nto classify tissue type with logistic regression using latent representations,\nwith an AUC of 0.97 and 85% accuracy, comparable to supervised deep models; 3)\nthe ability to predict the presence of tumor in Whole Slide Images (WSIs) using\nmultiple instance learning (MIL), achieving an AUC of 0.98 and 94% accuracy.\n\nOur results show that our model captures distinct phenotypic characteristics\nof real tissue samples, paving the way for further understanding of tumor\nprogression and tumor micro-environment, and ultimately refining\nhistopathological classification for diagnosis and treatment. The code and\npretrained models are available at:\nhttps://github.com/AdalbertoCq/Adversarial-learning-of-cancer-tissue-representations",
          "link": "http://arxiv.org/abs/2108.02223",
          "publishedOn": "2021-08-06T00:51:46.030Z",
          "wordCount": 655,
          "title": "Adversarial learning of cancer tissue representations. (arXiv:2108.02223v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delbruck_T/0/1/0/all/0/1\">Tobi Delbruck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shih-Chii Liu</a>",
          "description": "Long Short-Term Memory (LSTM) recurrent networks are frequently used for\ntasks involving time sequential data such as speech recognition. However, it is\ndifficult to deploy these networks on hardware to achieve high throughput and\nlow latency because the fully-connected structure makes LSTM networks a\nmemory-bounded algorithm. Previous work in LSTM accelerators either exploited\nweight spatial sparsity or temporal sparsity. In this paper, we present a new\naccelerator called \"Spartus\" that exploits spatio-temporal sparsity to achieve\nultra-low latency inference. The spatial sparsity was induced using our\nproposed pruning method called Column-Balanced Targeted Dropout (CBTD) that\nleads to structured sparse weight matrices benefiting workload balance. It\nachieved up to 96% weight sparsity with negligible accuracy difference for an\nLSTM network trained on a TIMIT phone recognition task. To induce temporal\nsparsity in LSTM, we create the DeltaLSTM by extending the previous DeltaGRU\nmethod to the LSTM network. This combined sparsity saves on weight memory\naccess and associated arithmetic operations simultaneously. Spartus was\nimplemented on a Xilinx Zynq-7100 FPGA. The per-sample latency for a single\nDeltaLSTM layer of 1024 neurons running on Spartus is 1 us. Spartus achieved\n9.4 TOp/s effective batch-1 throughput and 1.1 TOp/J energy efficiency, which\nare respectively 4X and 7X higher than the previous state-of-the-art.",
          "link": "http://arxiv.org/abs/2108.02297",
          "publishedOn": "2021-08-06T00:51:46.007Z",
          "wordCount": 653,
          "title": "Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-temporal Sparsity. (arXiv:2108.02297v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blanzeisky_W/0/1/0/all/0/1\">William Blanzeisky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunningham_P/0/1/0/all/0/1\">P&#xe1;draig Cunningham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_K/0/1/0/all/0/1\">Kenneth Kennedy</a>",
          "description": "A significant impediment to progress in research on bias in machine learning\n(ML) is the availability of relevant datasets. This situation is unlikely to\nchange much given the sensitivity of such data. For this reason, there is a\nrole for synthetic data in this research. In this short paper, we present one\nsuch family of synthetic data sets. We provide an overview of the data,\ndescribe how the level of bias can be varied, and present a simple example of\nan experiment on the data.",
          "link": "http://arxiv.org/abs/2107.08928",
          "publishedOn": "2021-08-05T01:56:21.727Z",
          "wordCount": 558,
          "title": "Introducing a Family of Synthetic Datasets for Research on Bias in Machine Learning. (arXiv:2107.08928v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Longbing Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qing Liu</a>",
          "description": "The SARS-CoV-2 virus and COVID-19 disease have posed unprecedented and\noverwhelming demand, challenges and opportunities to domain, model and data\ndriven modeling. This paper provides a comprehensive review of the challenges,\ntasks, methods, progress, gaps and opportunities in relation to modeling\nCOVID-19 problems, data and objectives. It constructs a research landscape of\nCOVID-19 modeling tasks and methods, and further categorizes, summarizes,\ncompares and discusses the related methods and progress of modeling COVID-19\nepidemic transmission processes and dynamics, case identification and tracing,\ninfection diagnosis and medical treatments, non-pharmaceutical interventions\nand their effects, drug and vaccine development, psychological, economic and\nsocial influence and impact, and misinformation, etc. The modeling methods\ninvolve mathematical and statistical models, domain-driven modeling by\nepidemiological compartmental models, medical and biomedical analysis, AI and\ndata science in particular shallow and deep machine learning, simulation\nmodeling, social science methods, and hybrid modeling.",
          "link": "http://arxiv.org/abs/2104.12556",
          "publishedOn": "2021-08-05T01:56:21.720Z",
          "wordCount": 654,
          "title": "COVID-19 Modeling: A Review. (arXiv:2104.12556v3 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1\">Moritz Knolle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1\">Alexander Ziller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1\">Dmitrii Usynin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1\">Rickmer Braren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus R. Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>",
          "description": "We show that differentially private stochastic gradient descent (DP-SGD) can\nyield poorly calibrated, overconfident deep learning models. This represents a\nserious issue for safety-critical applications, e.g. in medical diagnosis. We\nhighlight and exploit parallels between stochastic gradient Langevin dynamics,\na scalable Bayesian inference technique for training deep neural networks, and\nDP-SGD, in order to train differentially private, Bayesian neural networks with\nminor adjustments to the original (DP-SGD) algorithm. Our approach provides\nconsiderably more reliable uncertainty estimates than DP-SGD, as demonstrated\nempirically by a reduction in expected calibration error (MNIST $\\sim{5}$-fold,\nPediatric Pneumonia Dataset $\\sim{2}$-fold).",
          "link": "http://arxiv.org/abs/2107.04296",
          "publishedOn": "2021-08-05T01:56:21.693Z",
          "wordCount": 593,
          "title": "Differentially private training of neural networks with Langevin dynamics for calibrated predictive uncertainty. (arXiv:2107.04296v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ormazabal_A/0/1/0/all/0/1\">Aitor Ormazabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1\">Aitor Soroa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labaka_G/0/1/0/all/0/1\">Gorka Labaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>",
          "description": "Recent research on cross-lingual word embeddings has been dominated by\nunsupervised mapping approaches that align monolingual embeddings. Such methods\ncritically rely on those embeddings having a similar structure, but it was\nrecently shown that the separate training in different languages causes\ndepartures from this assumption. In this paper, we propose an alternative\napproach that does not have this limitation, while requiring a weak seed\ndictionary (e.g., a list of identical words) as the only form of supervision.\nRather than aligning two fixed embedding spaces, our method works by fixing the\ntarget language embeddings, and learning a new set of embeddings for the source\nlanguage that are aligned with them. To that end, we use an extension of\nskip-gram that leverages translated context words as anchor points, and\nincorporates self-learning and iterative restarts to reduce the dependency on\nthe initial dictionary. Our approach outperforms conventional mapping methods\non bilingual lexicon induction, and obtains competitive results in the\ndownstream XNLI task.",
          "link": "http://arxiv.org/abs/2012.15715",
          "publishedOn": "2021-08-05T01:56:21.672Z",
          "wordCount": 640,
          "title": "Beyond Offline Mapping: Learning Cross Lingual Word Embeddings through Context Anchoring. (arXiv:2012.15715v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1\">Frank Nielsen</a>",
          "description": "The Jeffreys divergence is a renown symmetrization of the statistical\nKullback-Leibler divergence which is often used in statistics, machine\nlearning, signal processing, and information sciences in general. Since the\nJeffreys divergence between the ubiquitous Gaussian Mixture Models are not\navailable in closed-form, many techniques with various pros and cons have been\nproposed in the literature to either (i) estimate, (ii) approximate, or (iii)\nlower and/or upper bound this divergence. In this work, we propose a simple yet\nfast heuristic to approximate the Jeffreys divergence between two univariate\nGMMs of arbitrary number of components. The heuristic relies on converting GMMs\ninto pairs of dually parameterized probability densities belonging to\nexponential families. In particular, we consider Exponential-Polynomial\nDensities, and design a goodness-of-fit criterion to measure the dissimilarity\nbetween a GMM and a EPD which is a generalization of the Hyv\\\"arinen\ndivergence. This criterion allows one to select the orders of the EPDs to\napproximate the GMMs. We demonstrate experimentally that the computational time\nof our heuristic improves over the stochastic Monte Carlo estimation baseline\nby several orders of magnitude while approximating reasonably well the Jeffreys\ndivergence, specially when the univariate mixtures have a small number of\nmodes.",
          "link": "http://arxiv.org/abs/2107.05901",
          "publishedOn": "2021-08-05T01:56:21.665Z",
          "wordCount": 668,
          "title": "Fast approximations of the Jeffreys divergence between univariate Gaussian mixture models via exponential polynomial densities. (arXiv:2107.05901v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roland S. Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borowski_J/0/1/0/all/0/1\">Judy Borowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1\">Robert Geirhos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallis_T/0/1/0/all/0/1\">Thomas S. A. Wallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>",
          "description": "One widely used approach towards understanding the inner workings of deep\nconvolutional neural networks is to visualize unit responses via activation\nmaximization. Feature visualizations via activation maximization are thought to\nprovide humans with precise information about the image features that cause a\nunit to be activated. If this is indeed true, these synthetic images should\nenable humans to predict the effect of an intervention, such as whether\noccluding a certain patch of the image (say, a dog's head) changes a unit's\nactivation. Here, we test this hypothesis by asking humans to predict which of\ntwo square occlusions causes a larger change to a unit's activation. Both a\nlarge-scale crowdsourced experiment and measurements with experts show that on\naverage, the extremely activating feature visualizations by Olah et al. (2017)\nindeed help humans on this task ($67 \\pm 4\\%$ accuracy; baseline performance\nwithout any visualizations is $60 \\pm 3\\%$). However, they do not provide any\nsignificant advantage over other visualizations (such as e.g. dataset samples),\nwhich yield similar performance ($66 \\pm 3\\%$ to $67 \\pm 3\\%$ accuracy). Taken\ntogether, we propose an objective psychophysical task to quantify the benefit\nof unit-level interpretability methods for humans, and find no evidence that\nfeature visualizations provide humans with better \"causal understanding\" than\nsimple alternative visualizations.",
          "link": "http://arxiv.org/abs/2106.12447",
          "publishedOn": "2021-08-05T01:56:21.657Z",
          "wordCount": 721,
          "title": "How Well do Feature Visualizations Support Causal Understanding of CNN Activations?. (arXiv:2106.12447v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1\">Kezhi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Peihong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luque_J/0/1/0/all/0/1\">Juan Luque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>",
          "description": "Randomized smoothing (RS) is an effective and scalable technique for\nconstructing neural network classifiers that are certifiably robust to\nadversarial perturbations. Most RS works focus on training a good base model\nthat boosts the certified robustness of the smoothed model. However, existing\nRS techniques treat every data point the same, i.e., the variance of the\nGaussian noise used to form the smoothed model is preset and universal for all\ntraining and test data. This preset and universal Gaussian noise variance is\nsuboptimal since different data points have different margins and the local\nproperties of the base model vary across the input examples. In this paper, we\nexamine the impact of customized handling of examples and propose Instance-wise\nRandomized Smoothing (Insta-RS) -- a multiple-start search algorithm that\nassigns customized Gaussian variances to test examples. We also design Insta-RS\nTrain -- a novel two-stage training algorithm that adaptively adjusts and\ncustomizes the noise level of each training example for training a base model\nthat boosts the certified robustness of the instance-wise Gaussian smoothed\nmodel. Through extensive experiments on CIFAR-10 and ImageNet, we show that our\nmethod significantly enhances the average certified radius (ACR) as well as the\nclean data accuracy compared to existing state-of-the-art provably robust\nclassifiers.",
          "link": "http://arxiv.org/abs/2103.04436",
          "publishedOn": "2021-08-05T01:56:21.651Z",
          "wordCount": 677,
          "title": "Insta-RS: Instance-wise Randomized Smoothing for Improved Robustness and Accuracy. (arXiv:2103.04436v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milenkoski_M/0/1/0/all/0/1\">Martin Milenkoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>",
          "description": "In this paper, we describe a method to tackle data sparsity and create\nrecommendations in domains with limited knowledge about user preferences. We\nexpand the variational autoencoder collaborative filtering from a single-domain\nto a multi-domain setting. The intuition is that user-item interactions in a\nsource domain can augment the recommendation quality in a target domain. The\nintuition can be taken to its extreme, where, in a cross-domain setup, the user\nhistory in a source domain is enough to generate high-quality recommendations\nin a target one. We thus create a Product-of-Experts (POE) architecture for\nrecommendations that jointly models user-item interactions across multiple\ndomains. The method is resilient to missing data for one or more of the\ndomains, which is a situation often found in real life. We present results on\ntwo widely-used datasets - Amazon and Yelp, which support the claim that\nholistic user preference knowledge leads to better recommendations.\nSurprisingly, we find that in some cases, a POE recommender that does not\naccess the target domain user representation can surpass a strong VAE\nrecommender baseline trained on the target domain.",
          "link": "http://arxiv.org/abs/2104.12822",
          "publishedOn": "2021-08-05T01:56:21.644Z",
          "wordCount": 660,
          "title": "Recommending Burgers based on Pizza Preferences: Addressing Data Sparsity with a Product of Experts. (arXiv:2104.12822v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banbury_C/0/1/0/all/0/1\">Colby Banbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1\">Vijay Janapa Reddi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torelli_P/0/1/0/all/0/1\">Peter Torelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holleman_J/0/1/0/all/0/1\">Jeremy Holleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeffries_N/0/1/0/all/0/1\">Nat Jeffries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiraly_C/0/1/0/all/0/1\">Csaba Kiraly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montino_P/0/1/0/all/0/1\">Pietro Montino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanter_D/0/1/0/all/0/1\">David Kanter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Sebastian Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pau_D/0/1/0/all/0/1\">Danilo Pau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakker_U/0/1/0/all/0/1\">Urmish Thakker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torrini_A/0/1/0/all/0/1\">Antonio Torrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warden_P/0/1/0/all/0/1\">Peter Warden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordaro_J/0/1/0/all/0/1\">Jay Cordaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guglielmo_G/0/1/0/all/0/1\">Giuseppe Di Guglielmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duarte_J/0/1/0/all/0/1\">Javier Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibellini_S/0/1/0/all/0/1\">Stephen Gibellini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_V/0/1/0/all/0/1\">Videet Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Honson Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1\">Nhan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenxu_N/0/1/0/all/0/1\">Niu Wenxu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuesong_X/0/1/0/all/0/1\">Xu Xuesong</a>",
          "description": "Advancements in ultra-low-power tiny machine learning (TinyML) systems\npromise to unlock an entirely new class of smart applications. However,\ncontinued progress is limited by the lack of a widely accepted and easily\nreproducible benchmark for these systems. To meet this need, we present MLPerf\nTiny, the first industry-standard benchmark suite for ultra-low-power tiny\nmachine learning systems. The benchmark suite is the collaborative effort of\nmore than 50 organizations from industry and academia and reflects the needs of\nthe community. MLPerf Tiny measures the accuracy, latency, and energy of\nmachine learning inference to properly evaluate the tradeoffs between systems.\nAdditionally, MLPerf Tiny implements a modular design that enables benchmark\nsubmitters to show the benefits of their product, regardless of where it falls\non the ML deployment stack, in a fair and reproducible manner. The suite\nfeatures four benchmarks: keyword spotting, visual wake words, image\nclassification, and anomaly detection.",
          "link": "http://arxiv.org/abs/2106.07597",
          "publishedOn": "2021-08-05T01:56:21.625Z",
          "wordCount": 653,
          "title": "MLPerf Tiny Benchmark. (arXiv:2106.07597v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13434",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Kovacs_R/0/1/0/all/0/1\">Reka A. Kovacs</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gunluk_O/0/1/0/all/0/1\">Oktay Gunluk</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hauser_R/0/1/0/all/0/1\">Raphael A. Hauser</a>",
          "description": "Binary matrix factorisation is an essential tool for identifying discrete\npatterns in binary data. In this paper we consider the rank-k binary matrix\nfactorisation problem (k-BMF) under Boolean arithmetic: we are given an n x m\nbinary matrix X with possibly missing entries and need to find two binary\nmatrices A and B of dimension n x k and k x m respectively, which minimise the\ndistance between X and the Boolean product of A and B in the squared Frobenius\ndistance. We present a compact and two exponential size integer programs (IPs)\nfor k-BMF and show that the compact IP has a weak LP relaxation, while the\nexponential size IPs have a stronger equivalent LP relaxation. We introduce a\nnew objective function, which differs from the traditional squared Frobenius\nobjective in attributing a weight to zero entries of the input matrix that is\nproportional to the number of times the zero is erroneously covered in a rank-k\nfactorisation. For one of the exponential size IPs we describe a computational\napproach based on column generation. Experimental results on synthetic and real\nword datasets suggest that our integer programming approach is competitive\nagainst available methods for k-BMF and provides accurate low-error\nfactorisations.",
          "link": "http://arxiv.org/abs/2106.13434",
          "publishedOn": "2021-08-05T01:56:21.618Z",
          "wordCount": 660,
          "title": "Binary Matrix Factorisation and Completion via Integer Programming. (arXiv:2106.13434v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10332",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Uthamacumaran_A/0/1/0/all/0/1\">Abicumaran Uthamacumaran</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Elouatik_S/0/1/0/all/0/1\">Samir Elouatik</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Abdouh_M/0/1/0/all/0/1\">Mohamed Abdouh</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Berteau_Rainville_M/0/1/0/all/0/1\">Michael Berteau-Rainville</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gao_Z/0/1/0/all/0/1\">Zhu- Hua Gao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Arena_G/0/1/0/all/0/1\">Goffredo Arena</a>",
          "description": "The early detection of cancer is a challenging problem in medicine. The blood\nsera of cancer patients are enriched with heterogeneous secretory lipid bound\nextracellular vesicles (EVs), which present a complex repertoire of information\nand biomarkers, representing their cell of origin, that are being currently\nstudied in the field of liquid biopsy and cancer screening. Vibrational\nspectroscopies provide non-invasive approaches for the assessment of structural\nand biophysical properties in complex biological samples. In this study,\nmultiple Raman spectroscopy measurements were performed on the EVs extracted\nfrom the blood sera of 9 patients consisting of four different cancer subtypes\n(colorectal cancer, hepatocellular carcinoma, breast cancer and pancreatic\ncancer) and five healthy patients (controls). FTIR(Fourier Transform Infrared)\nspectroscopy measurements were performed as a complementary approach to Raman\nanalysis, on two of the four cancer subtypes.\n\nThe AdaBoost Random Forest Classifier, Decision Trees, and Support Vector\nMachines (SVM) distinguished the baseline corrected Raman spectra of cancer EVs\nfrom those of healthy controls (18 spectra) with a classification accuracy of\ngreater than 90% when reduced to a spectral frequency range of 1800 to 1940\ninverse cm, and subjected to a 0.5 training/testing split. FTIR classification\naccuracy on 14 spectra showed an 80% classification accuracy. Our findings\ndemonstrate that basic machine learning algorithms are powerful tools to\ndistinguish the complex vibrational spectra of cancer patient EVs from those of\nhealthy patients. These experimental methods hold promise as valid and\nefficient liquid biopsy for machine intelligence-assisted early cancer\nscreening.",
          "link": "http://arxiv.org/abs/2107.10332",
          "publishedOn": "2021-08-05T01:56:21.612Z",
          "wordCount": 718,
          "title": "Machine Learning Characterization of Cancer Patients-Derived Extracellular Vesicles using Vibrational Spectroscopies. (arXiv:2107.10332v2 [q-bio.OT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael Ng</a>",
          "description": "Image smoothing is a fundamental procedure in applications of both computer\nvision and graphics. The required smoothing properties can be different or even\ncontradictive among different tasks. Nevertheless, the inherent smoothing\nnature of one smoothing operator is usually fixed and thus cannot meet the\nvarious requirements of different applications. In this paper, we first\nintroduce the truncated Huber penalty function which shows strong flexibility\nunder different parameter settings. A generalized framework is then proposed\nwith the introduced truncated Huber penalty function. When combined with its\nstrong flexibility, our framework is able to achieve diverse smoothing natures\nwhere contradictive smoothing behaviors can even be achieved. It can also yield\nthe smoothing behavior that can seldom be achieved by previous methods, and\nsuperior performance is thus achieved in challenging cases. These together\nenable our framework capable of a range of applications and able to outperform\nthe state-of-the-art approaches in several tasks, such as image detail\nenhancement, clip-art compression artifacts removal, guided depth map\nrestoration, image texture removal, etc. In addition, an efficient numerical\nsolution is provided and its convergence is theoretically guaranteed even the\noptimization framework is non-convex and non-smooth. A simple yet effective\napproach is further proposed to reduce the computational cost of our method\nwhile maintaining its performance. The effectiveness and superior performance\nof our approach are validated through comprehensive experiments in a range of\napplications. Our code is available at\nhttps://github.com/wliusjtu/Generalized-Smoothing-Framework.",
          "link": "http://arxiv.org/abs/2107.07058",
          "publishedOn": "2021-08-05T01:56:21.605Z",
          "wordCount": 756,
          "title": "A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tambon_F/0/1/0/all/0/1\">Florian Tambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laberge_G/0/1/0/all/0/1\">Gabriel Laberge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1\">Le An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1\">Amin Nikanjam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mindom_P/0/1/0/all/0/1\">Paulina Stevia Nouwou Mindom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pequignot_Y/0/1/0/all/0/1\">Yann Pequignot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1\">Foutse Khomh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniol_G/0/1/0/all/0/1\">Giulio Antoniol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merlo_E/0/1/0/all/0/1\">Ettore Merlo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laviolette_F/0/1/0/all/0/1\">Fran&#xe7;ois Laviolette</a>",
          "description": "Context: Machine Learning (ML) has been at the heart of many innovations over\nthe past years. However, including it in so-called 'safety-critical' systems\nsuch as automotive or aeronautic has proven to be very challenging, since the\nshift in paradigm that ML brings completely changes traditional certification\napproaches.\n\nObjective: This paper aims to elucidate challenges related to the\ncertification of ML-based safety-critical systems, as well as the solutions\nthat are proposed in the literature to tackle them, answering the question 'How\nto Certify Machine Learning Based Safety-critical Systems?'.\n\nMethod: We conduct a Systematic Literature Review (SLR) of research papers\npublished between 2015 to 2020, covering topics related to the certification of\nML systems. In total, we identified 217 papers covering topics considered to be\nthe main pillars of ML certification: Robustness, Uncertainty, Explainability,\nVerification, Safe Reinforcement Learning, and Direct Certification. We\nanalyzed the main trends and problems of each sub-field and provided summaries\nof the papers extracted.\n\nResults: The SLR results highlighted the enthusiasm of the community for this\nsubject, as well as the lack of diversity in terms of datasets and type of\nmodels. It also emphasized the need to further develop connections between\nacademia and industries to deepen the domain study. Finally, it also\nillustrated the necessity to build connections between the above mention main\npillars that are for now mainly studied separately.\n\nConclusion: We highlighted current efforts deployed to enable the\ncertification of ML based software systems, and discuss some future research\ndirections.",
          "link": "http://arxiv.org/abs/2107.12045",
          "publishedOn": "2021-08-05T01:56:21.587Z",
          "wordCount": 757,
          "title": "How to Certify Machine Learning Based Safety-critical Systems? A Systematic Literature Review. (arXiv:2107.12045v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tianjin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_Y/0/1/0/all/0/1\">Yulong Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>",
          "description": "Adversarial training is an approach for increasing model's resilience against\nadversarial perturbations. Such approaches have been demonstrated to result in\nmodels with feature representations that generalize better. However, limited\nworks have been done on adversarial training of models on graph data. In this\npaper, we raise such a question { does adversarial training improve the\ngeneralization of graph representations. We formulate L2 and L1 versions of\nadversarial training in two powerful node embedding methods: graph autoencoder\n(GAE) and variational graph autoencoder (VGAE). We conduct extensive\nexperiments on three main applications, i.e. link prediction, node clustering,\ngraph anomaly detection of GAE and VGAE, and demonstrate that both L2 and L1\nadversarial training boost the generalization of GAE and VGAE.",
          "link": "http://arxiv.org/abs/2107.02658",
          "publishedOn": "2021-08-05T01:56:21.579Z",
          "wordCount": 585,
          "title": "On Generalization of Graph Autoencoders with Adversarial Training. (arXiv:2107.02658v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Markovic_D/0/1/0/all/0/1\">Dimitrije Markovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stojic_H/0/1/0/all/0/1\">Hrvoje Stojic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwoebel_S/0/1/0/all/0/1\">Sarah Schwoebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiebel_S/0/1/0/all/0/1\">Stefan J. Kiebel</a>",
          "description": "A key feature of sequential decision making under uncertainty is a need to\nbalance between exploiting--choosing the best action according to the current\nknowledge, and exploring--obtaining information about values of other actions.\nThe multi-armed bandit problem, a classical task that captures this trade-off,\nserved as a vehicle in machine learning for developing bandit algorithms that\nproved to be useful in numerous industrial applications. The active inference\nframework, an approach to sequential decision making recently developed in\nneuroscience for understanding human and animal behaviour, is distinguished by\nits sophisticated strategy for resolving the exploration-exploitation\ntrade-off. This makes active inference an exciting alternative to already\nestablished bandit algorithms. Here we derive an efficient and scalable\napproximate active inference algorithm and compare it to two state-of-the-art\nbandit algorithms: Bayesian upper confidence bound and optimistic Thompson\nsampling. This comparison is done on two types of bandit problems: a stationary\nand a dynamic switching bandit. Our empirical evaluation shows that the active\ninference algorithm does not produce efficient long-term behaviour in\nstationary bandits. However, in the more challenging switching bandit problem\nactive inference performs substantially better than the two state-of-the-art\nbandit algorithms. The results open exciting venues for further research in\ntheoretical and applied machine learning, as well as lend additional\ncredibility to active inference as a general framework for studying human and\nanimal behaviour.",
          "link": "http://arxiv.org/abs/2101.08699",
          "publishedOn": "2021-08-05T01:56:21.562Z",
          "wordCount": 697,
          "title": "An empirical evaluation of active inference in multi-armed bandits. (arXiv:2101.08699v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01290",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tomelleri_E/0/1/0/all/0/1\">Enrico Tomelleri</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tonon_G/0/1/0/all/0/1\">Giustino Tonon</a>",
          "description": "While single-tree transpiration is challenging to compare with earth\nobservation, canopy scale data are suitable for this purpose. To test the\npotentialities of the second approach, we equipped the trees at two measurement\nsites with sap flow sensors in spruce forests. The sites have contrasting\ntopography. The measurement period covered the months between June 2020 and\nJanuary 2021. To link plot scale transpiration with earth observations, we\nutilized Sentinel-2 and local meteorological data. Within a machine learning\nframework, we have tested the suitability of earth observations for modelling\ncanopy transpiration. The R2 of the cross-validated trained models at the\nmeasurement sites was between 0.57 and 0.80. These results demonstrate the\nrelevance of Sentinel-2 data for the data-driven upscaling of ecosystem fluxes\nfrom plot scale sap flow data. If applied to a broader network of sites and\nclimatic conditions, such an approach could offer unprecedented possibilities\nfor investigating our forests' resilience and resistance capacity to an\nintensified hydrological cycle in the contest of a changing climate.",
          "link": "http://arxiv.org/abs/2108.01290",
          "publishedOn": "2021-08-05T01:56:21.555Z",
          "wordCount": 613,
          "title": "Linking Sap Flow Measurements with Earth Observations. (arXiv:2108.01290v1 [stat.AP] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Junwen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yeye He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Surajit Chaudhuri</a>",
          "description": "Recent work has made significant progress in helping users to automate single\ndata preparation steps, such as string-transformations and table-manipulation\noperators (e.g., Join, GroupBy, Pivot, etc.). We in this work propose to\nautomate multiple such steps end-to-end, by synthesizing complex data pipelines\nwith both string transformations and table-manipulation operators. We propose a\nnovel \"by-target\" paradigm that allows users to easily specify the desired\npipeline, which is a significant departure from the traditional by-example\nparadigm. Using by-target, users would provide input tables (e.g., csv or json\nfiles), and point us to a \"target table\" (e.g., an existing database table or\nBI dashboard) to demonstrate how the output from the desired pipeline would\nschematically \"look like\". While the problem is seemingly underspecified, our\nunique insight is that implicit table constraints such as FDs and keys can be\nexploited to significantly constrain the space to make the problem tractable.\nWe develop an Auto-Pipeline system that learns to synthesize pipelines using\nreinforcement learning and search. Experiments on large numbers of real\npipelines crawled from GitHub suggest that Auto-Pipeline can successfully\nsynthesize 60-70% of these complex pipelines with up to 10 steps.",
          "link": "http://arxiv.org/abs/2106.13861",
          "publishedOn": "2021-08-05T01:56:21.536Z",
          "wordCount": 664,
          "title": "Auto-Pipeline: Synthesizing Complex Data Pipelines By-Target Using Reinforcement Learning and Search. (arXiv:2106.13861v2 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>",
          "description": "Assessing an AI agent that can converse in human language and understand\nvisual content is challenging. Generation metrics, such as BLEU scores favor\ncorrect syntax over semantics. Hence a discriminative approach is often used,\nwhere an agent ranks a set of candidate options. The mean reciprocal rank (MRR)\nmetric evaluates the model performance by taking into account the rank of a\nsingle human-derived answer. This approach, however, raises a new challenge:\nthe ambiguity and synonymy of answers, for instance, semantic equivalence\n(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative\ngain (NDCG) metric has been used to capture the relevance of all the correct\nanswers via dense annotations. However, the NDCG metric favors the usually\napplicable uncertain answers such as `I don't know. Crafting a model that\nexcels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should\nanswer a human-like reply and validate the correctness of any answer. To\naddress this issue, we describe a two-step non-parametric ranking approach that\ncan merge strong MRR and NDCG models. Using our approach, we manage to keep\nmost MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG\nstate-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won\nthe recent Visual Dialog 2020 challenge. Source code is available at\nhttps://github.com/idansc/mrr-ndcg.",
          "link": "http://arxiv.org/abs/2104.07511",
          "publishedOn": "2021-08-05T01:56:21.490Z",
          "wordCount": 697,
          "title": "Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00411",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Khakzar_A/0/1/0/all/0/1\">Ashkan Khakzar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mansour_W/0/1/0/all/0/1\">Wejdene Mansour</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_Y/0/1/0/all/0/1\">Yuezhi Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yawei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yucheng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Seong Tae Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>",
          "description": "Neural networks have demonstrated remarkable performance in classification\nand regression tasks on chest X-rays. In order to establish trust in the\nclinical routine, the networks' prediction mechanism needs to be interpretable.\nOne principal approach to interpretation is feature attribution. Feature\nattribution methods identify the importance of input features for the output\nprediction. Building on Information Bottleneck Attribution (IBA) method, for\neach prediction we identify the chest X-ray regions that have high mutual\ninformation with the network's output. Original IBA identifies input regions\nthat have sufficient predictive information. We propose Inverse IBA to identify\nall informative regions. Thus all predictive cues for pathologies are\nhighlighted on the X-rays, a desirable property for chest X-ray diagnosis.\nMoreover, we propose Regression IBA for explaining regression models. Using\nRegression IBA we observe that a model trained on cumulative severity score\nlabels implicitly learns the severity of different X-ray regions. Finally, we\npropose Multi-layer IBA to generate higher resolution and more detailed\nattribution/saliency maps. We evaluate our methods using both human-centric\n(ground-truth-based) interpretability metrics, and human-independent feature\nimportance metrics on NIH Chest X-ray8 and BrixIA datasets. The Code is\npublicly available.",
          "link": "http://arxiv.org/abs/2104.00411",
          "publishedOn": "2021-08-05T01:56:21.474Z",
          "wordCount": 733,
          "title": "Explaining COVID-19 and Thoracic Pathology Model Predictions by Identifying Informative Input Features. (arXiv:2104.00411v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02481",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Khakzar_A/0/1/0/all/0/1\">Ashkan Khakzar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Musatian_S/0/1/0/all/0/1\">Sabrina Musatian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buchberger_J/0/1/0/all/0/1\">Jonas Buchberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quiroz_I/0/1/0/all/0/1\">Icxel Valeriano Quiroz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pinger_N/0/1/0/all/0/1\">Nikolaus Pinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baselizadeh_S/0/1/0/all/0/1\">Soroosh Baselizadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Seong Tae Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>",
          "description": "Convolutional neural networks are showing promise in the automatic diagnosis\nof thoracic pathologies on chest x-rays. Their black-box nature has sparked\nmany recent works to explain the prediction via input feature attribution\nmethods (aka saliency methods). However, input feature attribution methods\nmerely identify the importance of input regions for the prediction and lack\nsemantic interpretation of model behavior. In this work, we first identify the\nsemantics associated with internal units (feature maps) of the network. We\nproceed to investigate the following questions; Does a regression model that is\nonly trained with COVID-19 severity scores implicitly learn visual patterns\nassociated with thoracic pathologies? Does a network that is trained on weakly\nlabeled data (e.g. healthy, unhealthy) implicitly learn pathologies? Moreover,\nwe investigate the effect of pretraining and data imbalance on the\ninterpretability of learned features. In addition to the analysis, we propose\nsemantic attribution to semantically explain each prediction. We present our\nfindings using publicly available chest pathologies (CheXpert, NIH ChestX-ray8)\nand COVID-19 datasets (BrixIA, and COVID-19 chest X-ray segmentation dataset).\nThe Code is publicly available.",
          "link": "http://arxiv.org/abs/2104.02481",
          "publishedOn": "2021-08-05T01:56:21.451Z",
          "wordCount": 719,
          "title": "Towards Semantic Interpretation of Thoracic Disease and COVID-19 Diagnosis Models. (arXiv:2104.02481v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mertes_S/0/1/0/all/0/1\">Silvan Mertes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_T/0/1/0/all/0/1\">Tobias Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weitz_K/0/1/0/all/0/1\">Katharina Weitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heimerl_A/0/1/0/all/0/1\">Alexander Heimerl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1\">Elisabeth Andr&#xe9;</a>",
          "description": "With the ongoing rise of machine learning, the need for methods for\nexplaining decisions made by artificial intelligence systems is becoming a more\nand more important topic. Especially for image classification tasks, many\nstate-of-the-art tools to explain such classifiers rely on visual highlighting\nof important areas of the input data. Contrary, counterfactual explanation\nsystems try to enable a counterfactual reasoning by modifying the input image\nin a way such that the classifier would have made a different prediction. By\ndoing so, the users of counterfactual explanation systems are equipped with a\ncompletely different kind of explanatory information. However, methods for\ngenerating realistic counterfactual explanations for image classifiers are\nstill rare. Especially in medical contexts, where relevant information often\nconsists of textural and structural information, high-quality counterfactual\nimages have the potential to give meaningful insights into decision processes.\nIn this work, we present GANterfactual, an approach to generate such\ncounterfactual image explanations based on adversarial image-to-image\ntranslation techniques. Additionally, we conduct a user study to evaluate our\napproach in an exemplary medical use case. Our results show that, in the chosen\nmedical use-case, counterfactual explanations lead to significantly better\nresults regarding mental models, explanation satisfaction, trust, emotions, and\nself-efficacy than two state-of-the-art systems that work with saliency maps,\nnamely LIME and LRP.",
          "link": "http://arxiv.org/abs/2012.11905",
          "publishedOn": "2021-08-05T01:56:21.431Z",
          "wordCount": 695,
          "title": "GANterfactual -- Counterfactual Explanations for Medical Non-Experts using Generative Adversarial Learning. (arXiv:2012.11905v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Filipovic_M/0/1/0/all/0/1\">Milena Filipovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitrevski_B/0/1/0/all/0/1\">Blagoj Mitrevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1\">Emma Lejal Glaude</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>",
          "description": "Recommender systems research tends to evaluate model performance offline and\non randomly sampled targets, yet the same systems are later used to predict\nuser behavior sequentially from a fixed point in time. Simulating online\nrecommender system performance is notoriously difficult and the discrepancy\nbetween online and offline behaviors is typically not accounted for in offline\nevaluations. This disparity permits weaknesses to go unnoticed until the model\nis deployed in a production setting. In this paper, we first demonstrate how\nomitting temporal context when evaluating recommender system performance leads\nto false confidence. To overcome this, we postulate that offline evaluation\nprotocols can only model real-life use-cases if they account for temporal\ncontext. Next, we propose a training procedure to further embed the temporal\ncontext in existing models: we introduce it in a multi-objective approach to\ntraditionally time-unaware recommender systems and confirm its advantage via\nthe proposed evaluation protocol. Finally, we validate that the Pareto Fronts\nobtained with the added objective dominate those produced by state-of-the-art\nmodels that are only optimized for accuracy on three real-world publicly\navailable datasets. The results show that including our temporal objective can\nimprove recall@20 by up to 20%.",
          "link": "http://arxiv.org/abs/2009.08978",
          "publishedOn": "2021-08-05T01:56:21.422Z",
          "wordCount": 683,
          "title": "Modeling Online Behavior in Recommender Systems: The Importance of Temporal Context. (arXiv:2009.08978v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02200",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Wang_J/0/1/0/all/0/1\">Jiangran Wang</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Luo_D/0/1/0/all/0/1\">Di Luo</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhizhen Zhao</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Hur_V/0/1/0/all/0/1\">Vera Mikyoung Hur</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Clark_B/0/1/0/all/0/1\">Bryan K. Clark</a>",
          "description": "We develop a spacetime neural network method with second order optimization\nfor solving quantum dynamics from the high dimensional Schr\\\"{o}dinger\nequation. In contrast to the standard iterative first order optimization and\nthe time-dependent variational principle, our approach utilizes the implicit\nmid-point method and generates the solution for all spatial and temporal values\nsimultaneously after optimization. We demonstrate the method in the\nSchr\\\"{o}dinger equation with a self-normalized autoregressive spacetime neural\nnetwork construction. Future explorations for solving different high\ndimensional differential equations are discussed.",
          "link": "http://arxiv.org/abs/2108.02200",
          "publishedOn": "2021-08-05T01:56:21.405Z",
          "wordCount": 540,
          "title": "Spacetime Neural Network for High Dimensional Quantum Dynamics. (arXiv:2108.02200v1 [cond-mat.dis-nn])"
        },
        {
          "id": "http://arxiv.org/abs/2010.05150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tsung-Yen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Michael Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chow_Y/0/1/0/all/0/1\">Yinlam Chow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramadge_P/0/1/0/all/0/1\">Peter J. Ramadge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>",
          "description": "While safe reinforcement learning (RL) holds great promise for many practical\napplications like robotics or autonomous cars, current approaches require\nspecifying constraints in mathematical form. Such specifications demand domain\nexpertise, limiting the adoption of safe RL. In this paper, we propose learning\nto interpret natural language constraints for safe RL. To this end, we first\nintroduce HazardWorld, a new multi-task benchmark that requires an agent to\noptimize reward while not violating constraints specified in free-form text. We\nthen develop an agent with a modular architecture that can interpret and adhere\nto such textual constraints while learning new tasks. Our model consists of (1)\na constraint interpreter that encodes textual constraints into spatial and\ntemporal representations of forbidden states, and (2) a policy network that\nuses these representations to produce a policy achieving minimal constraint\nviolations during training. Across different domains in HazardWorld, we show\nthat our method achieves higher rewards (up to11x) and fewer constraint\nviolations (by 1.8x) compared to existing approaches. However, in terms of\nabsolute performance, HazardWorld still poses significant challenges for agents\nto learn efficiently, motivating the need for future work.",
          "link": "http://arxiv.org/abs/2010.05150",
          "publishedOn": "2021-08-05T01:56:21.399Z",
          "wordCount": 664,
          "title": "Safe Reinforcement Learning with Natural Language Constraints. (arXiv:2010.05150v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Desai_A/0/1/0/all/0/1\">Aditya Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_L/0/1/0/all/0/1\">Li Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Anshumali Shrivastava</a>",
          "description": "Deep learning for recommendation data is the one of the most pervasive and\nchallenging AI workload in recent times. State-of-the-art recommendation models\nare one of the largest models rivalling the likes of GPT-3 and Switch\nTransformer. Challenges in deep learning recommendation models (DLRM) stem from\nlearning dense embeddings for each of the categorical values. These embedding\ntables in industrial scale models can be as large as hundreds of terabytes.\nSuch large models lead to a plethora of engineering challenges, not to mention\nprohibitive communication overheads, and slower training and inference times.\nOf these, slower inference time directly impacts user experience. Model\ncompression for DLRM is gaining traction and the community has recently shown\nimpressive compression results. In this paper, we present Random Offset Block\nEmbedding Array (ROBE) as a low memory alternative to embedding tables which\nprovide orders of magnitude reduction in memory usage while maintaining\naccuracy and boosting execution speed. ROBE is a simple fundamental approach in\nimproving both cache performance and the variance of randomized hashing, which\ncould be of independent interest in itself. We demonstrate that we can\nsuccessfully train DLRM models with same accuracy while using $1000 \\times$\nless memory. A $1000\\times$ compressed model directly results in faster\ninference without any engineering. In particular, we show that we can train\nDLRM model using ROBE Array of size 100MB on a single GPU to achieve AUC of\n0.8025 or higher as required by official MLPerf CriteoTB benchmark DLRM model\nof 100GB while achieving about $2.7\\times$ (170\\%) improvement in inference\nthroughput.",
          "link": "http://arxiv.org/abs/2108.02191",
          "publishedOn": "2021-08-05T01:56:21.391Z",
          "wordCount": 711,
          "title": "Random Offset Block Embedding Array (ROBE) for CriteoTB Benchmark MLPerf DLRM Model : 1000$\\times$ Compression and 2.7$\\times$ Faster Inference. (arXiv:2108.02191v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.02469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kidzinski_L/0/1/0/all/0/1\">&#x141;ukasz Kidzi&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_F/0/1/0/all/0/1\">Francis K.C. Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warton_D/0/1/0/all/0/1\">David I. Warton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hastie_T/0/1/0/all/0/1\">Trevor Hastie</a>",
          "description": "Unmeasured or latent variables are often the cause of correlations between\nmultivariate measurements and are studied in a variety of fields such as\npsychology, ecology, and medicine. For Gaussian measurements, there are\nclassical tools such as factor analysis or principal component analysis with a\nwell-established theory and fast algorithms. Generalized Linear Latent Variable\nmodels (GLLVM) generalize such factor models to non-Gaussian responses.\nHowever, current algorithms for estimating model parameters in GLLVMs require\nintensive computation and do not scale to large datasets with thousands of\nobservational units or responses. In this article, we propose a new approach\nfor fitting GLLVMs to such high-volume, high-dimensional datasets. We\napproximate the likelihood using penalized quasi-likelihood and use a Newton\nmethod and Fisher scoring to learn the model parameters. Our method greatly\nreduces the computation time and can be easily parallelized, enabling\nfactorization at unprecedented scale using commodity hardware. We illustrate\napplication of our method on a dataset of 48,000 observational units with over\n2,000 observed species in each unit, finding that most of the variability can\nbe explained with a handful of factors.",
          "link": "http://arxiv.org/abs/2010.02469",
          "publishedOn": "2021-08-05T01:56:21.384Z",
          "wordCount": 636,
          "title": "Generalized Matrix Factorization. (arXiv:2010.02469v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1809.08771",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kidzinski_L/0/1/0/all/0/1\">&#x141;ukasz Kidzi&#x144;ski</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hastie_T/0/1/0/all/0/1\">Trevor Hastie</a>",
          "description": "In clinical practice and biomedical research, measurements are often\ncollected sparsely and irregularly in time while the data acquisition is\nexpensive and inconvenient. Examples include measurements of spine bone mineral\ndensity, cancer growth through mammography or biopsy, a progression of\ndefective vision, or assessment of gait in patients with neurological\ndisorders. Since the data collection is often costly and inconvenient,\nestimation of progression from sparse observations is of great interest for\npractitioners.\n\nFrom the statistical standpoint, such data is often analyzed in the context\nof a mixed-effect model where time is treated as both a fixed-effect\n(population progression curve) and a random-effect (individual variability).\nAlternatively, researchers analyze Gaussian processes or functional data where\nobservations are assumed to be drawn from a certain distribution of processes.\nThese models are flexible but rely on probabilistic assumptions, require very\ncareful implementation, specific to the given problem, and tend to be slow in\npractice.\n\nIn this study, we propose an alternative elementary framework for analyzing\nlongitudinal data, relying on matrix completion. Our method yields estimates of\nprogression curves by iterative application of the Singular Value\nDecomposition. Our framework covers multivariate longitudinal data, regression,\nand can be easily extended to other settings. As it relies on existing tools\nfor matrix algebra it is efficient and easy to implement.\n\nWe apply our methods to understand trends of progression of motor impairment\nin children with Cerebral Palsy. Our model approximates individual progression\ncurves and explains 30% of the variability. Low-rank representation of\nprogression trends enables identification of different progression trends in\nsubtypes of Cerebral Palsy.",
          "link": "http://arxiv.org/abs/1809.08771",
          "publishedOn": "2021-08-05T01:56:21.372Z",
          "wordCount": 711,
          "title": "Modeling longitudinal data using matrix completion. (arXiv:1809.08771v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.06028",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_A/0/1/0/all/0/1\">Andy T. Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>",
          "description": "We introduce a self-supervised speech pre-training method called TERA, which\nstands for Transformer Encoder Representations from Alteration. Recent\napproaches often learn by using a single auxiliary task like contrastive\nprediction, autoregressive prediction, or masked reconstruction. Unlike\nprevious methods, we use alteration along three orthogonal axes to pre-train\nTransformer Encoders on a large amount of unlabeled speech. The model learns\nthrough the reconstruction of acoustic frames from their altered counterpart,\nwhere we use a stochastic policy to alter along various dimensions: time,\nfrequency, and magnitude. TERA can be used for speech representations\nextraction or fine-tuning with downstream models. We evaluate TERA on several\ndownstream tasks, including phoneme classification, keyword spotting, speaker\nrecognition, and speech recognition. We present a large-scale comparison of\nvarious self-supervised models. TERA achieves strong performance in the\ncomparison by improving upon surface features and outperforming previous\nmodels. In our experiments, we study the effect of applying different\nalteration techniques, pre-training on more data, and pre-training on various\nfeatures. We analyze different model sizes and find that smaller models are\nstrong representation learners than larger models, while larger models are more\neffective for downstream fine-tuning than smaller models. Furthermore, we show\nthe proposed method is transferable to downstream datasets not used in\npre-training.",
          "link": "http://arxiv.org/abs/2007.06028",
          "publishedOn": "2021-08-05T01:56:21.362Z",
          "wordCount": 707,
          "title": "TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech. (arXiv:2007.06028v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.14091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biyik_E/0/1/0/all/0/1\">Erdem B&#x131;y&#x131;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Losey_D/0/1/0/all/0/1\">Dylan P. Losey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palan_M/0/1/0/all/0/1\">Malayandi Palan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landolfi_N/0/1/0/all/0/1\">Nicholas C. Landolfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shevchuk_G/0/1/0/all/0/1\">Gleb Shevchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1\">Dorsa Sadigh</a>",
          "description": "Reward functions are a common way to specify the objective of a robot. As\ndesigning reward functions can be extremely challenging, a more promising\napproach is to directly learn reward functions from human teachers.\nImportantly, data from human teachers can be collected either passively or\nactively in a variety of forms: passive data sources include demonstrations,\n(e.g., kinesthetic guidance), whereas preferences (e.g., comparative rankings)\nare actively elicited. Prior research has independently applied reward learning\nto these different data sources. However, there exist many domains where\nmultiple sources are complementary and expressive. Motivated by this general\nproblem, we present a framework to integrate multiple sources of information,\nwhich are either passively or actively collected from human users. In\nparticular, we present an algorithm that first utilizes user demonstrations to\ninitialize a belief about the reward function, and then actively probes the\nuser with preference queries to zero-in on their true reward. This algorithm\nnot only enables us combine multiple data sources, but it also informs the\nrobot when it should leverage each type of information. Further, our approach\naccounts for the human's ability to provide data: yielding user-friendly\npreference queries which are also theoretically optimal. Our extensive\nsimulated experiments and user studies on a Fetch mobile manipulator\ndemonstrate the superiority and the usability of our integrated framework.",
          "link": "http://arxiv.org/abs/2006.14091",
          "publishedOn": "2021-08-05T01:56:21.343Z",
          "wordCount": 719,
          "title": "Learning Reward Functions from Diverse Sources of Human Feedback: Optimally Integrating Demonstrations and Preferences. (arXiv:2006.14091v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1810.01248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xutan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Faqiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yidan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>",
          "description": "Deep learning researches on the transformation problems for image and text\nhave raised great attention. However, present methods for music feature\ntransfer using neural networks are far from practical application. In this\npaper, we initiate a novel system for transferring the texture of music, and\nrelease it as an open source project. Its core algorithm is composed of a\nconverter which represents sounds as texture spectra, a corresponding\nreconstructor and a feed-forward transfer network. We evaluate this system from\nmultiple perspectives, and experimental results reveal that it achieves\nconvincing results in both sound effects and computational performance.",
          "link": "http://arxiv.org/abs/1810.01248",
          "publishedOn": "2021-08-05T01:56:21.336Z",
          "wordCount": 601,
          "title": "A Lightweight Music Texture Transfer System. (arXiv:1810.01248v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karras_O/0/1/0/all/0/1\">Oliver Karras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kristo_E/0/1/0/all/0/1\">Eklekta Kristo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klunder_J/0/1/0/all/0/1\">Jil Kl&#xfc;nder</a>",
          "description": "Vision videos are established for soliciting feedback and stimulating\ndiscussions in requirements engineering (RE) practices, such as focus groups.\nDifferent researchers motivated the transfer of these benefits into crowd-based\nRE (CrowdRE) by using vision videos on social media platforms. So far, however,\nlittle research explored the potential of using vision videos for CrowdRE in\ndetail. In this paper, we analyze and assess this potential, in particular,\nfocusing on video comments as a source of feedback. In a case study, we\nanalyzed 4505 comments on a vision video from YouTube. We found that the video\nsolicited 2770 comments from 2660 viewers in four days. This is more than 50%\nof all comments the video received in four years. Even though only a certain\nfraction of these comments are relevant to RE, the relevant comments address\ntypical intentions and topics of user feedback, such as feature request or\nproblem report. Besides the typical user feedback categories, we found more\nthan 300 comments that address the topic safety, which has not appeared in\nprevious analyses of user feedback. In an automated analysis, we compared the\nperformance of three machine learning algorithms on classifying the video\ncomments. Despite certain differences, the algorithms classified the video\ncomments well. Based on these findings, we conclude that the use of vision\nvideos for CrowdRE has a large potential. Despite the preliminary nature of the\ncase study, we are optimistic that vision videos can motivate stakeholders to\nactively participate in a crowd and solicit numerous of video comments as a\nvaluable source of feedback.",
          "link": "http://arxiv.org/abs/2108.02076",
          "publishedOn": "2021-08-05T01:56:21.329Z",
          "wordCount": 720,
          "title": "The Potential of Using Vision Videos for CrowdRE: Video Comments as a Source of Feedback. (arXiv:2108.02076v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2008.06822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>",
          "description": "This paper focuses on high-transferable adversarial attacks on detectors,\nwhich are hard to attack in a black-box manner, because of their\nmultiple-output characteristics and the diversity across architectures. To\npursue a high attack transferability, one plausible way is to find a common\nproperty across detectors, which facilitates the discovery of common\nweaknesses. We are the first to suggest that the relevance map from\ninterpreters for detectors is such a property. Based on it, we design a\nRelevance Attack on Detectors (RAD), which achieves a state-of-the-art\ntransferability, exceeding existing results by above 20%. On MS COCO, the\ndetection mAPs for all 8 black-box architectures are more than halved and the\nsegmentation mAPs are also significantly influenced. Given the great\ntransferability of RAD, we generate the first adversarial dataset for object\ndetection and instance segmentation, i.e., Adversarial Objects in COntext\n(AOCO), which helps to quickly evaluate and improve the robustness of\ndetectors.",
          "link": "http://arxiv.org/abs/2008.06822",
          "publishedOn": "2021-08-05T01:56:21.322Z",
          "wordCount": 624,
          "title": "Relevance Attack on Detectors. (arXiv:2008.06822v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.07088",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ryan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_E/0/1/0/all/0/1\">Earlence Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Atul Prakash</a>",
          "description": "The physical, black-box hard-label setting is arguably the most realistic\nthreat model for cyber-physical vision systems. In this setting, the attacker\nonly has query access to the model and only receives the top-1 class label\nwithout confidence information. Creating small physical stickers that are\nrobust to environmental variation is difficult in the discrete and\ndiscontinuous hard-label space because the attack must both design a small\nshape to perturb within and find robust noise to fill it with. Unfortunately,\nwe find that existing $\\ell_2$ or $\\ell_\\infty$ minimizing hard-label attacks\ndo not easily extend to finding such robust physical perturbation attacks.\nThus, we propose GRAPHITE, the first algorithm for hard-label physical attacks\non computer vision models. We show that \"survivability\", an estimate of\nphysical variation robustness, can be used in new ways to generate small masks\nand is a sufficiently smooth function to optimize with gradient-free\noptimization. We use GRAPHITE to attack a traffic sign classifier and a\npublicly-available Automatic License Plate Recognition (ALPR) tool using only\nquery access. We evaluate both tools in real-world field tests to measure its\nphysical-world robustness. We successfully cause a Stop sign to be\nmisclassified as a Speed Limit 30 km/hr sign in 95.7% of physical images and\ncause errors in 75% of physical images for the ALPR tool.",
          "link": "http://arxiv.org/abs/2002.07088",
          "publishedOn": "2021-08-05T01:56:21.306Z",
          "wordCount": 708,
          "title": "Robust Physical Hard-Label Attacks on Deep Learning Visual Classification. (arXiv:2002.07088v4 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kondmann_L/0/1/0/all/0/1\">Lukas Kondmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>",
          "description": "Humanitarian mapping from space with machine learning helps policy-makers to\ntimely and accurately identify people in need. However, recent concerns around\nfairness and transparency of algorithmic decision-making are a significant\nobstacle for applying these methods in practice. In this paper, we study if\nhumanitarian mapping approaches from space are prone to bias in their\npredictions. We map village-level poverty and electricity rates in India based\non nighttime lights (NTLs) with linear regression and random forest and analyze\nif the predictions systematically show prejudice against scheduled caste or\ntribe communities. To achieve this, we design a causal approach to measure\ncounterfactual fairness based on propensity score matching. This allows to\ncompare villages within a community of interest to synthetic counterfactuals.\nOur findings indicate that poverty is systematically overestimated and\nelectricity systematically underestimated for scheduled tribes in comparison to\na synthetic counterfactual group of villages. The effects have the opposite\ndirection for scheduled castes where poverty is underestimated and\nelectrification overestimated. These results are a warning sign for a variety\nof applications in humanitarian mapping where fairness issues would compromise\npolicy goals.",
          "link": "http://arxiv.org/abs/2108.02137",
          "publishedOn": "2021-08-05T01:56:21.296Z",
          "wordCount": 634,
          "title": "Under the Radar -- Auditing Fairness in ML for Humanitarian Mapping. (arXiv:2108.02137v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yap_B/0/1/0/all/0/1\">Boon Peng Yap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_B/0/1/0/all/0/1\">Beng Koon Ng</a>",
          "description": "We explore the value of weak labels in learning transferable representations\nfor medical images. Compared to hand-labeled datasets, weak or inexact labels\ncan be acquired in large quantities at significantly lower cost and can provide\nuseful training signals for data-hungry models such as deep neural networks. We\nconsider weak labels in the form of pseudo-labels and propose a semi-weakly\nsupervised contrastive learning (SWCL) framework for representation learning\nusing semi-weakly annotated images. Specifically, we train a semi-supervised\nmodel to propagate labels from a small dataset consisting of diverse\nimage-level annotations to a large unlabeled dataset. Using the propagated\nlabels, we generate a patch-level dataset for pretraining and formulate a\nmulti-label contrastive learning objective to capture position-specific\nfeatures encoded in each patch. We empirically validate the transfer learning\nperformance of SWCL on seven public retinal fundus datasets, covering three\ndisease classification tasks and two anatomical structure segmentation tasks.\nOur experiment results suggest that, under very low data regime, large-scale\nImageNet pretraining on improved architecture remains a very strong baseline,\nand recently proposed self-supervised methods falter in segmentation tasks,\npossibly due to the strong invariant constraint imposed. Our method surpasses\nall prior self-supervised methods and standard cross-entropy training, while\nclosing the gaps with ImageNet pretraining.",
          "link": "http://arxiv.org/abs/2108.02122",
          "publishedOn": "2021-08-05T01:56:21.289Z",
          "wordCount": 643,
          "title": "Semi-weakly Supervised Contrastive Representation Learning for Retinal Fundus Images. (arXiv:2108.02122v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02120",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Blanchet_J/0/1/0/all/0/1\">Jose Blanchet</a>, <a href=\"http://arxiv.org/find/math/1/au:+Murthy_K/0/1/0/all/0/1\">Karthyek Murthy</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nguyen_V/0/1/0/all/0/1\">Viet Anh Nguyen</a>",
          "description": "We consider statistical methods which invoke a min-max distributionally\nrobust formulation to extract good out-of-sample performance in data-driven\noptimization and learning problems. Acknowledging the distributional\nuncertainty in learning from limited samples, the min-max formulations\nintroduce an adversarial inner player to explore unseen covariate data. The\nresulting Distributionally Robust Optimization (DRO) formulations, which\ninclude Wasserstein DRO formulations (our main focus), are specified using\noptimal transportation phenomena. Upon describing how these\ninfinite-dimensional min-max problems can be approached via a\nfinite-dimensional dual reformulation, the tutorial moves into its main\ncomponent, namely, explaining a generic recipe for optimally selecting the size\nof the adversary's budget. This is achieved by studying the limit behavior of\nan optimal transport projection formulation arising from an inquiry on the\nsmallest confidence region that includes the unknown population risk minimizer.\nIncidentally, this systematic prescription coincides with those in specific\nexamples in high-dimensional statistics and results in error bounds that are\nfree from the curse of dimensions. Equipped with this prescription, we present\na central limit theorem for the DRO estimator and provide a recipe for\nconstructing compatible confidence regions that are useful for uncertainty\nquantification. The rest of the tutorial is devoted to insights into the nature\nof the optimizers selected by the min-max formulations and additional\napplications of optimal transport projections.",
          "link": "http://arxiv.org/abs/2108.02120",
          "publishedOn": "2021-08-05T01:56:21.282Z",
          "wordCount": 655,
          "title": "Statistical Analysis of Wasserstein Distributionally Robust Estimators. (arXiv:2108.02120v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02067",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Petkovic_M/0/1/0/all/0/1\">Matej Petkovi&#x107;</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Lucas_L/0/1/0/all/0/1\">Luke Lucas</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Stepisnik_T/0/1/0/all/0/1\">Toma&#x17e; Stepi&#x161;nik</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Panov_P/0/1/0/all/0/1\">Pan&#x10d;e Panov</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Simidjievski_N/0/1/0/all/0/1\">Nikola Simidjievski</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kocev_D/0/1/0/all/0/1\">Dragi Kocev</a>",
          "description": "The Mars Express (MEX) spacecraft has been orbiting Mars since 2004. The\noperators need to constantly monitor its behavior and handle sporadic\ndeviations (outliers) from the expected patterns of measurements of quantities\nthat the satellite is sending to Earth. In this paper, we analyze the patterns\nof the electrical power consumption of MEX's thermal subsystem, that maintains\nthe spacecraft's temperature at the desired level. The consumption is not\nconstant, but should be roughly periodic in the short term, with the period\nthat corresponds to one orbit around Mars. By using long short-term memory\nneural networks, we show that the consumption pattern is more irregular than\nexpected, and successfully detect such irregularities, opening possibility for\nautomatic outlier detection on MEX in the future.",
          "link": "http://arxiv.org/abs/2108.02067",
          "publishedOn": "2021-08-05T01:56:21.275Z",
          "wordCount": 582,
          "title": "Discovering outliers in the Mars Express thermal power consumption patterns. (arXiv:2108.02067v1 [astro-ph.IM])"
        },
        {
          "id": "http://arxiv.org/abs/2008.05228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stojcheski_J/0/1/0/all/0/1\">Jugoslav Stojcheski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felso_V/0/1/0/all/0/1\">Valkyrie Felso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lieder_F/0/1/0/all/0/1\">Falk Lieder</a>",
          "description": "What should I work on first? What can wait until later? Which projects should\nI prioritize and which tasks are not worth my time? These are challenging\nquestions that many people face every day. People's intuitive strategy is to\nprioritize their immediate experience over the long-term consequences. This\nleads to procrastination and the neglect of important long-term projects in\nfavor of seemingly urgent tasks that are less important. Optimal gamification\nstrives to help people overcome these problems by incentivizing each task by a\nnumber of points that communicates how valuable it is in the long-run.\nUnfortunately, computing the optimal number of points with standard dynamic\nprogramming methods quickly becomes intractable as the number of a person's\nprojects and the number of tasks required by each project increase. Here, we\nintroduce and evaluate a scalable method for identifying which tasks are most\nimportant in the long run and incentivizing each task according to its\nlong-term value. Our method makes it possible to create to-do list gamification\napps that can handle the size and complexity of people's to-do lists in the\nreal world.",
          "link": "http://arxiv.org/abs/2008.05228",
          "publishedOn": "2021-08-05T01:56:21.268Z",
          "wordCount": 636,
          "title": "Optimal to-do list gamification. (arXiv:2008.05228v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_A/0/1/0/all/0/1\">Ahmed Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Refai_A/0/1/0/all/0/1\">Ayman El-Refai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Sara Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aboul_Ela_M/0/1/0/all/0/1\">Mariam Aboul-Ela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eraqi_H/0/1/0/all/0/1\">Hesham M. Eraqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moustafa_M/0/1/0/all/0/1\">Mohamed Moustafa</a>",
          "description": "Due to the mass advancement in ubiquitous technologies nowadays, new\npervasive methods have come into the practice to provide new innovative\nfeatures and stimulate the research on new human-computer interactions. This\npaper presents a hand gesture recognition method that utilizes the smartphone's\nbuilt-in speakers and microphones. The proposed system emits an ultrasonic\nsonar-based signal (inaudible sound) from the smartphone's stereo speakers,\nwhich is then received by the smartphone's microphone and processed via a\nConvolutional Neural Network (CNN) for Hand Gesture Recognition. Data\naugmentation techniques are proposed to improve the detection accuracy and\nthree dual-channel input fusion methods are compared. The first method merges\nthe dual-channel audio as a single input spectrogram image. The second method\nadopts early fusion by concatenating the dual-channel spectrograms. The third\nmethod adopts late fusion by having two convectional input branches processing\neach of the dual-channel spectrograms and then the outputs are merged by the\nlast layers. Our experimental results demonstrate a promising detection\naccuracy for the six gestures presented in our publicly available dataset with\nan accuracy of 93.58\\% as a baseline.",
          "link": "http://arxiv.org/abs/2108.02148",
          "publishedOn": "2021-08-05T01:56:21.262Z",
          "wordCount": 640,
          "title": "Pervasive Hand Gesture Recognition for Smartphones using Non-audible Sound and Deep Learning. (arXiv:2108.02148v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2002.06212",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Karamanis_M/0/1/0/all/0/1\">Minas Karamanis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Beutler_F/0/1/0/all/0/1\">Florian Beutler</a>",
          "description": "Slice Sampling has emerged as a powerful Markov Chain Monte Carlo algorithm\nthat adapts to the characteristics of the target distribution with minimal\nhand-tuning. However, Slice Sampling's performance is highly sensitive to the\nuser-specified initial length scale hyperparameter and the method generally\nstruggles with poorly scaled or strongly correlated distributions. This paper\nintroduces Ensemble Slice Sampling (ESS), a new class of algorithms that\nbypasses such difficulties by adaptively tuning the initial length scale and\nutilising an ensemble of parallel walkers in order to efficiently handle strong\ncorrelations between parameters. These affine-invariant algorithms are trivial\nto construct, require no hand-tuning, and can easily be implemented in parallel\ncomputing environments. Empirical tests show that Ensemble Slice Sampling can\nimprove efficiency by more than an order of magnitude compared to conventional\nMCMC methods on a broad range of highly correlated target distributions. In\ncases of strongly multimodal target distributions, Ensemble Slice Sampling can\nsample efficiently even in high dimensions. We argue that the parallel,\nblack-box and gradient-free nature of the method renders it ideal for use in\nscientific fields such as physics, astrophysics and cosmology which are\ndominated by a wide variety of computationally expensive and non-differentiable\nmodels.",
          "link": "http://arxiv.org/abs/2002.06212",
          "publishedOn": "2021-08-05T01:56:21.255Z",
          "wordCount": 688,
          "title": "Ensemble Slice Sampling: Parallel, black-box and gradient-free inference for correlated & multimodal distributions. (arXiv:2002.06212v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.10582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yining Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciao_D/0/1/0/all/0/1\">Daniel Ciao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>",
          "description": "Previous neural solvers of math word problems (MWPs) are learned with full\nsupervision and fail to generate diverse solutions. In this paper, we address\nthis issue by introducing a \\textit{weakly-supervised} paradigm for learning\nMWPs. Our method only requires the annotations of the final answers and can\ngenerate various solutions for a single problem. To boost weakly-supervised\nlearning, we propose a novel \\textit{learning-by-fixing} (LBF) framework, which\ncorrects the misperceptions of the neural network via symbolic reasoning.\nSpecifically, for an incorrect solution tree generated by the neural network,\nthe \\textit{fixing} mechanism propagates the error from the root node to the\nleaf nodes and infers the most probable fix that can be executed to get the\ndesired answer. To generate more diverse solutions, \\textit{tree\nregularization} is applied to guide the efficient shrinkage and exploration of\nthe solution space, and a \\textit{memory buffer} is designed to track and save\nthe discovered various fixes for each problem. Experimental results on the\nMath23K dataset show the proposed LBF framework significantly outperforms\nreinforcement learning baselines in weakly-supervised learning. Furthermore, it\nachieves comparable top-1 and much better top-3/5 answer accuracies than\nfully-supervised methods, demonstrating its strength in producing diverse\nsolutions.",
          "link": "http://arxiv.org/abs/2012.10582",
          "publishedOn": "2021-08-05T01:56:21.236Z",
          "wordCount": 668,
          "title": "Learning by Fixing: Solving Math Word Problems with Weak Supervision. (arXiv:2012.10582v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valiuddin_M/0/1/0/all/0/1\">M.M.A. Valiuddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viviers_C/0/1/0/all/0/1\">C.G.A. Viviers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sloun_R/0/1/0/all/0/1\">R.J.G. van Sloun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+With_P/0/1/0/all/0/1\">P.H.N. de With</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sommen_F/0/1/0/all/0/1\">F. van der Sommen</a>",
          "description": "Quantifying uncertainty in medical image segmentation applications is\nessential, as it is often connected to vital decision-making. Compelling\nattempts have been made in quantifying the uncertainty in image segmentation\narchitectures, e.g. to learn a density segmentation model conditioned on the\ninput image. Typical work in this field restricts these learnt densities to be\nstrictly Gaussian. In this paper, we propose to use a more flexible approach by\nintroducing Normalizing Flows (NFs), which enables the learnt densities to be\nmore complex and facilitate more accurate modeling for uncertainty. We prove\nthis hypothesis by adopting the Probabilistic U-Net and augmenting the\nposterior density with an NF, allowing it to be more expressive. Our\nqualitative as well as quantitative (GED and IoU) evaluations on the\nmulti-annotated and single-annotated LIDC-IDRI and Kvasir-SEG segmentation\ndatasets, respectively, show a clear improvement. This is mostly apparent in\nthe quantification of aleatoric uncertainty and the increased predictive\nperformance of up to 14 percent. This result strongly indicates that a more\nflexible density model should be seriously considered in architectures that\nattempt to capture segmentation ambiguity through density modeling. The benefit\nof this improved modeling will increase human confidence in annotation and\nsegmentation, and enable eager adoption of the technology in practice.",
          "link": "http://arxiv.org/abs/2108.02155",
          "publishedOn": "2021-08-05T01:56:21.216Z",
          "wordCount": 664,
          "title": "Improving Aleatoric Uncertainty Quantification in Multi-Annotated Medical ImageSegmentation with Normalizing Flows. (arXiv:2108.02155v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.09962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lijie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonglong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1\">Dina Katabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>",
          "description": "Contrastive learning is effective at learning useful representations without\nsupervision. Yet contrastive learning is susceptible to shortcuts -- i.e., it\nmay learn shortcut features irrelevant to the downstream task and discard\nrelevant information. Past work has addressed this limitation via handcrafted\ndata augmentations that eliminate the shortcut. However, handcrafted\naugmentations are infeasible for data modalities that are not interpretable by\nhumans (e.g., radio signals). Further, even when the modality is interpretable\n(e.g., RGB), sometimes eliminating the shortcut information may be undesirable.\nFor example, in multi-attribute classification, information related to one\nattribute may act as a shortcut around other attributes. This paper presents\nreconstructive contrastive learning (RCL), a framework for learning\nunsupervised representations that are robust to shortcuts. The key idea is to\nforce the learned representation to reconstruct the input, which naturally\ncounters potential shortcuts. Extensive experiments verify that RCL is highly\nrobust to shortcuts and outperforms state-of-the-art contrastive learning\nmethods on both RGB and RF datasets for a variety of tasks.",
          "link": "http://arxiv.org/abs/2012.09962",
          "publishedOn": "2021-08-05T01:56:21.191Z",
          "wordCount": 650,
          "title": "Making Contrastive Learning Robust to Shortcuts. (arXiv:2012.09962v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pene_C/0/1/0/all/0/1\">Cosmin Octavian Pene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghiassi_A/0/1/0/all/0/1\">Amirmasoud Ghiassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Younesian_T/0/1/0/all/0/1\">Taraneh Younesian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birke_R/0/1/0/all/0/1\">Robert Birke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lydia Y.Chen</a>",
          "description": "Multi-label learning is an emerging extension of the multi-class\nclassification where an image contains multiple labels. Not only acquiring a\nclean and fully labeled dataset in multi-label learning is extremely expensive,\nbut also many of the actual labels are corrupted or missing due to the\nautomated or non-expert annotation techniques. Noisy label data decrease the\nprediction performance drastically. In this paper, we propose a novel Gold\nAsymmetric Loss Correction with Single-Label Regulators (GALC-SLR) that\noperates robust against noisy labels. GALC-SLR estimates the noise confusion\nmatrix using single-label samples, then constructs an asymmetric loss\ncorrection via estimated confusion matrix to avoid overfitting to the noisy\nlabels. Empirical results show that our method outperforms the state-of-the-art\noriginal asymmetric loss multi-label classifier under all corruption levels,\nshowing mean average precision improvement up to 28.67% on a real world dataset\nof MS-COCO, yielding a better generalization of the unseen data and increased\nprediction performance.",
          "link": "http://arxiv.org/abs/2108.02032",
          "publishedOn": "2021-08-05T01:56:21.184Z",
          "wordCount": 596,
          "title": "Multi-Label Gold Asymmetric Loss Correction with Single-Label Regulators. (arXiv:2108.02032v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinghao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Real world traffic sign recognition is an important step towards building\nautonomous vehicles, most of which highly dependent on Deep Neural Networks\n(DNNs). Recent studies demonstrated that DNNs are surprisingly susceptible to\nadversarial examples. Many attack methods have been proposed to understand and\ngenerate adversarial examples, such as gradient based attack, score based\nattack, decision based attack, and transfer based attacks. However, most of\nthese algorithms are ineffective in real-world road sign attack, because (1)\niteratively learning perturbations for each frame is not realistic for a fast\nmoving car and (2) most optimization algorithms traverse all pixels equally\nwithout considering their diverse contribution. To alleviate these problems,\nthis paper proposes the targeted attention attack (TAA) method for real world\nroad sign attack. Specifically, we have made the following contributions: (1)\nwe leverage the soft attention map to highlight those important pixels and skip\nthose zero-contributed areas - this also helps to generate natural\nperturbations, (2) we design an efficient universal attack that optimizes a\nsingle perturbation/noise based on a set of training images under the guidance\nof the pre-trained attention map, (3) we design a simple objective function\nthat can be easily optimized, (4) we evaluate the effectiveness of TAA on real\nworld data sets. Experimental results validate that the TAA method improves the\nattack successful rate (nearly 10%) and reduces the perturbation loss (about a\nquarter) compared with the popular RP2 method. Additionally, our TAA also\nprovides good properties, e.g., transferability and generalization capability.\nWe provide code and data to ensure the reproducibility:\nhttps://github.com/AdvAttack/RoadSignAttack.",
          "link": "http://arxiv.org/abs/2010.04331",
          "publishedOn": "2021-08-05T01:56:21.177Z",
          "wordCount": 745,
          "title": "Targeted Attention Attack on Deep Learning Models in Road Sign Recognition. (arXiv:2010.04331v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samsi_S/0/1/0/all/0/1\">Siddharth Samsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_M/0/1/0/all/0/1\">Matthew L Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bestor_D/0/1/0/all/0/1\">David Bestor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baolin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1\">Michael Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuther_A/0/1/0/all/0/1\">Albert Reuther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edelman_D/0/1/0/all/0/1\">Daniel Edelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arcand_W/0/1/0/all/0/1\">William Arcand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_C/0/1/0/all/0/1\">Chansup Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holodnack_J/0/1/0/all/0/1\">John Holodnack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubbell_M/0/1/0/all/0/1\">Matthew Hubbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kepner_J/0/1/0/all/0/1\">Jeremy Kepner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_A/0/1/0/all/0/1\">Anna Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1\">Joseph McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michaleas_A/0/1/0/all/0/1\">Adam Michaleas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michaleas_P/0/1/0/all/0/1\">Peter Michaleas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milechin_L/0/1/0/all/0/1\">Lauren Milechin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullen_J/0/1/0/all/0/1\">Julia Mullen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yee_C/0/1/0/all/0/1\">Charles Yee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1\">Benjamin Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prout_A/0/1/0/all/0/1\">Andrew Prout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosa_A/0/1/0/all/0/1\">Antonio Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanterpool_A/0/1/0/all/0/1\">Allan Vanterpool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McEvoy_L/0/1/0/all/0/1\">Lindsey McEvoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1\">Anson Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_D/0/1/0/all/0/1\">Devesh Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadepally_V/0/1/0/all/0/1\">Vijay Gadepally</a>",
          "description": "Artificial intelligence (AI) and Machine learning (ML) workloads are an\nincreasingly larger share of the compute workloads in traditional\nHigh-Performance Computing (HPC) centers and commercial cloud systems. This has\nled to changes in deployment approaches of HPC clusters and the commercial\ncloud, as well as a new focus on approaches to optimized resource usage,\nallocations and deployment of new AI frame- works, and capabilities such as\nJupyter notebooks to enable rapid prototyping and deployment. With these\nchanges, there is a need to better understand cluster/datacenter operations\nwith the goal of developing improved scheduling policies, identifying\ninefficiencies in resource utilization, energy/power consumption, failure\nprediction, and identifying policy violations. In this paper we introduce the\nMIT Supercloud Dataset which aims to foster innovative AI/ML approaches to the\nanalysis of large scale HPC and datacenter/cloud operations. We provide\ndetailed monitoring logs from the MIT Supercloud system, which include CPU and\nGPU usage by jobs, memory usage, file system logs, and physical monitoring\ndata. This paper discusses the details of the dataset, collection methodology,\ndata availability, and discusses potential challenge problems being developed\nusing this data. Datasets and future challenge announcements will be available\nvia https://dcc.mit.edu.",
          "link": "http://arxiv.org/abs/2108.02037",
          "publishedOn": "2021-08-05T01:56:21.170Z",
          "wordCount": 676,
          "title": "The MIT Supercloud Dataset. (arXiv:2108.02037v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02083",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang1_C/0/1/0/all/0/1\">Chao Zhang1</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bom_S/0/1/0/all/0/1\">Sthitie Bom</a>",
          "description": "With the proliferation of IoT devices, the distributed control systems are\nnow capturing and processing more sensors at higher frequency than ever before.\nThese new data, due to their volume and novelty, cannot be effectively consumed\nwithout the help of data-driven techniques. Deep learning is emerging as a\npromising technique to analyze these data, particularly in soft sensor\nmodeling. The strong representational capabilities of complex data and the\nflexibility it offers from an architectural perspective make it a topic of\nactive applied research in industrial settings. However, the successful\napplications of deep learning in soft sensing are still not widely integrated\nin factory control systems, because most of the research on soft sensing do not\nhave access to large scale industrial data which are varied, noisy and\nincomplete. The results published in most research papers are therefore not\neasily reproduced when applied to the variety of data in industrial settings.\nHere we provide manufacturing data sets that are much larger and more complex\nthan public open soft sensor data. Moreover, the data sets are from Seagate\nfactories on active service with only necessary anonymization, so that they\nreflect the complex and noisy nature of real-world data. We introduce a\nvariance weighted multi-headed auto-encoder classification model that fits well\ninto the high-dimensional and highly imbalanced data. Besides the use of\nweighting or sampling methods to handle the highly imbalanced data, the model\nalso simultaneously predicts multiple outputs by exploiting output-supervised\nrepresentation learning and multi-task weighting.",
          "link": "http://arxiv.org/abs/2108.02083",
          "publishedOn": "2021-08-05T01:56:21.163Z",
          "wordCount": 688,
          "title": "Auto-encoder based Model for High-dimensional Imbalanced Industrial Data. (arXiv:2108.02083v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguegnang_G/0/1/0/all/0/1\">Gabin Maxime Nguegnang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauhut_H/0/1/0/all/0/1\">Holger Rauhut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terstiege_U/0/1/0/all/0/1\">Ulrich Terstiege</a>",
          "description": "We study the convergence properties of gradient descent for training deep\nlinear neural networks, i.e., deep matrix factorizations, by extending a\nprevious analysis for the related gradient flow. We show that under suitable\nconditions on the step sizes gradient descent converges to a critical point of\nthe loss function, i.e., the square loss in this article. Furthermore, we\ndemonstrate that for almost all initializations gradient descent converges to a\nglobal minimum in the case of two layers. In the case of three or more layers\nwe show that gradient descent converges to a global minimum on the manifold\nmatrices of some fixed rank, where the rank cannot be determined a priori.",
          "link": "http://arxiv.org/abs/2108.02040",
          "publishedOn": "2021-08-05T01:56:21.142Z",
          "wordCount": 548,
          "title": "Convergence of gradient descent for learning linear neural networks. (arXiv:2108.02040v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Venuto_D/0/1/0/all/0/1\">David Venuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_E/0/1/0/all/0/1\">Elaine Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1\">Ofir Nachum</a>",
          "description": "Reasoning about the future -- understanding how decisions in the present time\naffect outcomes in the future -- is one of the central challenges for\nreinforcement learning (RL), especially in highly-stochastic or partially\nobservable environments. While predicting the future directly is hard, in this\nwork we introduce a method that allows an agent to \"look into the future\"\nwithout explicitly predicting it. Namely, we propose to allow an agent, during\nits training on past experience, to observe what \\emph{actually} happened in\nthe future at that time, while enforcing an information bottleneck to avoid the\nagent overly relying on this privileged information. This gives our agent the\nopportunity to utilize rich and useful information about the future trajectory\ndynamics in addition to the present. Our method, Policy Gradients Incorporating\nthe Future (PGIF), is easy to implement and versatile, being applicable to\nvirtually any policy gradient algorithm. We apply our proposed method to a\nnumber of off-the-shelf RL algorithms and show that PGIF is able to achieve\nhigher reward faster in a variety of online and offline RL domains, as well as\nsparse-reward and partially observable environments.",
          "link": "http://arxiv.org/abs/2108.02096",
          "publishedOn": "2021-08-05T01:56:21.136Z",
          "wordCount": 608,
          "title": "Policy Gradients Incorporating the Future. (arXiv:2108.02096v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_Z/0/1/0/all/0/1\">Zih-Yun Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1\">Yi-Lin Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1\">Li-Chen Fu</a>",
          "description": "For reinforcement learning (RL), it is challenging for an agent to master a\ntask that requires a specific series of actions due to sparse rewards. To solve\nthis problem, reverse curriculum generation (RCG) provides a reverse expansion\napproach that automatically generates a curriculum for the agent to learn. More\nspecifically, RCG adapts the initial state distribution from the neighborhood\nof a goal to a distance as training proceeds. However, the initial state\ndistribution generated for each iteration might be biased, thus making the\npolicy overfit or slowing down the reverse expansion rate. While training RCG\nfor actor-critic (AC) based RL algorithms, this poor generalization and slow\nconvergence might be induced by the tight coupling between an AC pair.\nTherefore, we propose a parallelized approach that simultaneously trains\nmultiple AC pairs and periodically exchanges their critics. We empirically\ndemonstrate that this proposed approach can improve RCG in performance and\nconvergence, and it can also be applied to other AC based RL algorithms with\nadapted initial state distribution.",
          "link": "http://arxiv.org/abs/2108.02128",
          "publishedOn": "2021-08-05T01:56:21.130Z",
          "wordCount": 591,
          "title": "Parallelized Reverse Curriculum Generation. (arXiv:2108.02128v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.01699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ryan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wu-chi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Atul Prakash</a>",
          "description": "To remove the effects of adversarial perturbations, preprocessing defenses\nsuch as pixel discretization are appealing due to their simplicity but have so\nfar been shown to be ineffective except on simple datasets such as MNIST,\nleading to the belief that pixel discretization approaches are doomed to\nfailure as a defense technique. This paper revisits the pixel discretization\napproaches. We hypothesize that the reason why existing approaches have failed\nis that they have used a fixed codebook for the entire dataset. In particular,\nwe find that can lead to situations where images become more susceptible to\nadversarial perturbations and also suffer significant loss of accuracy after\ndiscretization. We propose a novel image preprocessing technique called\nEssential Features that uses an adaptive codebook that is based on per-image\ncontent and threat model. Essential Features adaptively selects a separable set\nof color clusters for each image to reduce the color space while preserving the\npertinent features of the original image, maximizing both separability and\nrepresentation of colors. Additionally, to limit the adversary's ability to\ninfluence the chosen color clusters, Essential Features takes advantage of\nspatial correlation with an adaptive blur that moves pixels closer to their\noriginal value without destroying original edge information. We design several\nadaptive attacks and find that our approach is more robust than previous\nbaselines on $L_\\infty$ and $L_2$ bounded attacks for several challenging\ndatasets including CIFAR-10, GTSRB, RESISC45, and ImageNet.",
          "link": "http://arxiv.org/abs/2012.01699",
          "publishedOn": "2021-08-05T01:56:21.122Z",
          "wordCount": 712,
          "title": "Essential Features: Content-Adaptive Pixel Discretization to Improve Model Robustness to Adaptive Adversarial Attacks. (arXiv:2012.01699v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yue Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Linyuan L&#xfc;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanrong Chen</a>",
          "description": "Many real-world complex systems can be described as graphs. For a large-scale\ngraph with low sparsity, a node's adjacency vector is a long and sparse\nrepresentation, limiting the practical utilization of existing machine learning\nmethods on nodal features. In practice, graph embedding (graph representation\nlearning) attempts to learn a lower-dimensional representation vector for each\nnode or the whole graph while maintaining the most basic information of graph.\nSince various machine learning methods can efficiently process\nlower-dimensional vectors, graph embedding has recently attracted a lot of\nattention. However, most node embedding or whole graph embedding methods suffer\nfrom the problem of having more sophisticated methodology, hyperparameter\noptimization, and low explainability. This paper proposes a\nhyperparameter-free, extensible, and explainable whole graph embedding method,\ncombining the DHC (Degree, H-index and Coreness) theorem and Shannon Entropy\n(E), abbreviated as DHC-E. The new whole graph embedding scheme can obtain a\ntrade-off between the simplicity and the quality under some supervised\nclassification learning tasks, using molecular, social, and brain networks. In\naddition, the proposed approach has a good performance in lower-dimensional\ngraph visualization. The new methodology is overall simple,\nhyperparameter-free, extensible, and explainable for whole graph embedding with\npromising potential for exploring graph classification, prediction, and\nlower-dimensional graph visualization.",
          "link": "http://arxiv.org/abs/2108.02113",
          "publishedOn": "2021-08-05T01:56:21.115Z",
          "wordCount": 635,
          "title": "Hyperparameter-free and Explainable Whole Graph Embedding. (arXiv:2108.02113v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.04457",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Kovacs_R/0/1/0/all/0/1\">Reka A. Kovacs</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gunluk_O/0/1/0/all/0/1\">Oktay Gunluk</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hauser_R/0/1/0/all/0/1\">Raphael A. Hauser</a>",
          "description": "Identifying discrete patterns in binary data is an important dimensionality\nreduction tool in machine learning and data mining. In this paper, we consider\nthe problem of low-rank binary matrix factorisation (BMF) under Boolean\narithmetic. Due to the hardness of this problem, most previous attempts rely on\nheuristic techniques. We formulate the problem as a mixed integer linear\nprogram and use a large scale optimisation technique of column generation to\nsolve it without the need of heuristic pattern mining. Our approach focuses on\naccuracy and on the provision of optimality guarantees. Experimental results on\nreal world datasets demonstrate that our proposed method is effective at\nproducing highly accurate factorisations and improves on the previously\navailable best known results for 15 out of 24 problem instances.",
          "link": "http://arxiv.org/abs/2011.04457",
          "publishedOn": "2021-08-05T01:56:21.092Z",
          "wordCount": 598,
          "title": "Binary Matrix Factorisation via Column Generation. (arXiv:2011.04457v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14265",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Marx_A/0/1/0/all/0/1\">Alexander Marx</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1\">Arthur Gretton</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mooij_J/0/1/0/all/0/1\">Joris M. Mooij</a>",
          "description": "One of the core assumptions in causal discovery is the faithfulness\nassumption, i.e., assuming that independencies found in the data are due to\nseparations in the true causal graph. This assumption can, however, be violated\nin many ways, including xor connections, deterministic functions or cancelling\npaths. In this work, we propose a weaker assumption that we call $2$-adjacency\nfaithfulness. In contrast to adjacency faithfulness, which assumes that there\nis no conditional independence between each pair of variables that are\nconnected in the causal graph, we only require no conditional independence\nbetween a node and a subset of its Markov blanket that can contain up to two\nnodes. Equivalently, we adapt orientation faithfulness to this setting. We\nfurther propose a sound orientation rule for causal discovery that applies\nunder weaker assumptions. As a proof of concept, we derive a modified Grow and\nShrink algorithm that recovers the Markov blanket of a target node and prove\nits correctness under strictly weaker assumptions than the standard\nfaithfulness assumption.",
          "link": "http://arxiv.org/abs/2010.14265",
          "publishedOn": "2021-08-05T01:56:21.085Z",
          "wordCount": 632,
          "title": "A Weaker Faithfulness Assumption based on Triple Interactions. (arXiv:2010.14265v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ro_J/0/1/0/all/0/1\">Jae Hun Ro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1\">Ananda Theertha Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Ke Wu</a>",
          "description": "Federated learning is a machine learning technique that enables training\nacross decentralized data. Recently, federated learning has become an active\narea of research due to the increased concerns over privacy and security. In\nlight of this, a variety of open source federated learning libraries have been\ndeveloped and released. We introduce FedJAX, a JAX-based open source library\nfor federated learning simulations that emphasizes ease-of-use in research.\nWith its simple primitives for implementing federated learning algorithms,\nprepackaged datasets, models and algorithms, and fast simulation speed, FedJAX\naims to make developing and evaluating federated algorithms faster and easier\nfor researchers. Our benchmark results show that FedJAX can be used to train\nmodels with federated averaging on the EMNIST dataset in a few minutes and the\nStack Overflow dataset in roughly an hour with standard hyperparmeters using\nTPUs.",
          "link": "http://arxiv.org/abs/2108.02117",
          "publishedOn": "2021-08-05T01:56:21.063Z",
          "wordCount": 561,
          "title": "FedJAX: Federated learning simulation with JAX. (arXiv:2108.02117v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tavasoli_A/0/1/0/all/0/1\">Ali Tavasoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1\">Teague Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_H/0/1/0/all/0/1\">Heman Shakeri</a>",
          "description": "Networks are landmarks of many complex phenomena where interweaving\ninteractions between different agents transform simple local rule-sets into\nnonlinear emergent behaviors. While some recent studies unveil associations\nbetween the network structure and the underlying dynamical process, identifying\nstochastic nonlinear dynamical processes continues to be an outstanding\nproblem. Here we develop a simple data-driven framework based on\noperator-theoretic techniques to identify and control stochastic nonlinear\ndynamics taking place over large-scale networks. The proposed approach requires\nno prior knowledge of the network structure and identifies the underlying\ndynamics solely using a collection of two-step snapshots of the states. This\ndata-driven system identification is achieved by using the Koopman operator to\nfind a low dimensional representation of the dynamical patterns that evolve\nlinearly. Further, we use the global linear Koopman model to solve critical\ncontrol problems by applying to model predictive control (MPC)--typically, a\nchallenging proposition when applied to large networks. We show that our\nproposed approach tackles this by converting the original nonlinear programming\ninto a more tractable optimization problem that is both convex and with far\nfewer variables.",
          "link": "http://arxiv.org/abs/2108.02005",
          "publishedOn": "2021-08-05T01:56:21.055Z",
          "wordCount": 649,
          "title": "A purely data-driven framework for prediction, optimization, and control of networked processes: application to networked SIS epidemic model. (arXiv:2108.02005v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02039",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lundy_C/0/1/0/all/0/1\">Christopher Lundy</a> (1 and 2), <a href=\"http://arxiv.org/find/eess/1/au:+OToole_J/0/1/0/all/0/1\">John M. O&#x27;Toole</a> (1 and 2) ((1) Irish Centre for Maternal and Child Health Research (INFANT), University College Cork, Ireland, (2) Department of Paediatrics and Child Health, University College Cork, Ireland)",
          "description": "Linear classifiers with random convolution kernels are computationally\nefficient methods that need no design or domain knowledge. Unlike deep neural\nnetworks, there is no need to hand-craft a network architecture; the kernels\nare randomly generated and only the linear classifier needs training. A\nrecently proposed method, RandOm Convolutional KErnel Transforms (ROCKETs), has\nshown high accuracy across a range of time-series data sets. Here we propose a\nmulti-scale version of this method, using both high- and low-frequency\ncomponents. We apply our methods to inter-burst detection in a cohort of\npreterm EEG recorded from 36 neonates <30 weeks gestational age. Two features\nfrom the convolution of 10,000 random kernels are combined using ridge\nregression. The proposed multi-scale ROCKET method out-performs the method\nwithout scale: median (interquartile range, IQR) Matthews correlation\ncoefficient (MCC) of 0.859 (0.815 to 0.874) for multi-scale versus 0.841 (0.807\nto 0.865) without scale, p<0.001. The proposed method lags behind an existing\nfeature-based machine learning method developed with deep domain knowledge, but\nis fast to train and can quickly set an initial baseline threshold of\nperformance for generic and biomedical time-series classification.",
          "link": "http://arxiv.org/abs/2108.02039",
          "publishedOn": "2021-08-05T01:56:21.032Z",
          "wordCount": 685,
          "title": "Random Convolution Kernels with Multi-Scale Decomposition for Preterm EEG Inter-burst Detection. (arXiv:2108.02039v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1\">Pai Chet Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spachos_P/0/1/0/all/0/1\">Petros Spachos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregori_S/0/1/0/all/0/1\">Stefano Gregori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos Plataniotis</a>",
          "description": "Digital contact tracing has emerged as a viable tool supplementing manual\ncontact tracing. To date, more than 100 contact tracing applications have been\npublished to slow down the spread of highly contagious Covid-19. Despite subtle\nvariabilities among these applications, all of them achieve contact tracing by\nmanipulating the following three components: a) use a personal device to\nidentify the user while designing a secure protocol to anonymize the user's\nidentity; b) leverage networking technologies to analyze and store the data; c)\nexploit rich sensing features on the user device to detect the interaction\namong users and thus estimate the exposure risk. This paper reviews the current\ndigital contact tracing based on these three components. We focus on two\npersonal devices that are intimate to the user: smartphones and wearables. We\ndiscuss the centralized and decentralized networking approaches that use to\nfacilitate the data flow. Lastly, we investigate the sensing feature available\non smartphones and wearables to detect the proximity between any two users and\npresent experiments comparing the proximity sensing performance between these\ntwo personal devices.",
          "link": "http://arxiv.org/abs/2108.02008",
          "publishedOn": "2021-08-05T01:56:20.993Z",
          "wordCount": 676,
          "title": "Personal Devices for Contact Tracing: Smartphones and Wearables to Fight Covid-19. (arXiv:2108.02008v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Travers_A/0/1/0/all/0/1\">Adelin Travers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Licollari_L/0/1/0/all/0/1\">Lorna Licollari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1\">Varun Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziedzic_A/0/1/0/all/0/1\">Adam Dziedzic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lie_D/0/1/0/all/0/1\">David Lie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1\">Nicolas Papernot</a>",
          "description": "Machine learning (ML) models are known to be vulnerable to adversarial\nexamples. Applications of ML to voice biometrics authentication are no\nexception. Yet, the implications of audio adversarial examples on these\nreal-world systems remain poorly understood given that most research targets\nlimited defenders who can only listen to the audio samples. Conflating\ndetectability of an attack with human perceptibility, research has focused on\nmethods that aim to produce imperceptible adversarial examples which humans\ncannot distinguish from the corresponding benign samples. We argue that this\nperspective is coarse for two reasons: 1. Imperceptibility is impossible to\nverify; it would require an experimental process that encompasses variations in\nlistener training, equipment, volume, ear sensitivity, types of background\nnoise etc, and 2. It disregards pipeline-based detection clues that realistic\ndefenders leverage. This results in adversarial examples that are ineffective\nin the presence of knowledgeable defenders. Thus, an adversary only needs an\naudio sample to be plausible to a human. We thus introduce surreptitious\nadversarial examples, a new class of attacks that evades both human and\npipeline controls. In the white-box setting, we instantiate this class with a\njoint, multi-stage optimization attack. Using an Amazon Mechanical Turk user\nstudy, we show that this attack produces audio samples that are more\nsurreptitious than previous attacks that aim solely for imperceptibility.\nLastly we show that surreptitious adversarial examples are challenging to\ndevelop in the black-box setting.",
          "link": "http://arxiv.org/abs/2108.02010",
          "publishedOn": "2021-08-05T01:56:20.979Z",
          "wordCount": 685,
          "title": "On the Exploitability of Audio Machine Learning Pipelines to Surreptitious Adversarial Examples. (arXiv:2108.02010v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02001",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vikash Kumar</a>",
          "description": "Estimating Worst-Case Execution Time (WCET) is of utmost importance for\ndeveloping Cyber-Physical and Safety-Critical Systems. The system's scheduler\nuses the estimated WCET to schedule each task of these systems, and failure may\nlead to catastrophic events. It is thus imperative to build provably reliable\nsystems. WCET is available to us in the last stage of systems development when\nthe hardware is available and the application code is compiled on it. Different\nmethodologies measure the WCET, but none of them give early insights on WCET,\nwhich is crucial for system development. If the system designers overestimate\nWCET in the early stage, then it would lead to the overqualified system, which\nwill increase the cost of the final product, and if they underestimate WCET in\nthe early stage, then it would lead to financial loss as the system would not\nperform as expected. This paper estimates early WCET using Deep Neural Networks\nas an approximate predictor model for hardware architecture and compiler. This\nmodel predicts the WCET based on the source code without compiling and running\non the hardware architecture. Our WCET prediction model is created using the\nPytorch framework. The resulting WCET is too erroneous to be used as an upper\nbound on the WCET. However, getting these results in the early stages of system\ndevelopment is an essential prerequisite for the system's dimensioning and\nconfiguration of the hardware setup.",
          "link": "http://arxiv.org/abs/2108.02001",
          "publishedOn": "2021-08-05T01:56:20.958Z",
          "wordCount": 670,
          "title": "Deep Neural Network Approach to Estimate Early Worst-Case Execution Time. (arXiv:2108.02001v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01995",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Venton_J/0/1/0/all/0/1\">J. Venton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harris_P/0/1/0/all/0/1\">P. M. Harris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sundar_A/0/1/0/all/0/1\">A. Sundar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smith_N/0/1/0/all/0/1\">N. A. S. Smith</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aston_P/0/1/0/all/0/1\">P. J. Aston</a>",
          "description": "The electrocardiogram (ECG) is one of the most widespread diagnostic tools in\nhealthcare and supports the diagnosis of cardiovascular disorders. Deep\nlearning methods are a successful and popular technique to detect indications\nof disorders from an ECG signal. However, there are open questions around the\nrobustness of these methods to various factors, including physiological ECG\nnoise. In this study we generate clean and noisy versions of an ECG dataset\nbefore applying Symmetric Projection Attractor Reconstruction (SPAR) and\nscalogram image transformations. A pretrained convolutional neural network is\ntrained using transfer learning to classify these image transforms. For the\nclean ECG dataset, F1 scores for SPAR attractor and scalogram transforms were\n0.70 and 0.79, respectively, and the scores decreased by less than 0.05 for the\nnoisy ECG datasets. Notably, when the network trained on clean data was used to\nclassify the noisy datasets, performance decreases of up to 0.18 in F1 scores\nwere seen. However, when the network trained on the noisy data was used to\nclassify the clean dataset, the performance decrease was less than 0.05. We\nconclude that physiological ECG noise impacts classification using deep\nlearning methods and careful consideration should be given to the inclusion of\nnoisy ECG signals in the training data when developing supervised networks for\nECG classification.",
          "link": "http://arxiv.org/abs/2108.01995",
          "publishedOn": "2021-08-05T01:56:20.951Z",
          "wordCount": 661,
          "title": "Robustness of convolutional neural networks to physiological ECG noise. (arXiv:2108.01995v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01998",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Du_Z/0/1/0/all/0/1\">Zhekai Du</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_K/0/1/0/all/0/1\">Ke Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>",
          "description": "Energy disaggregation, also known as non-intrusive load monitoring (NILM),\nchallenges the problem of separating the whole-home electricity usage into\nappliance-specific individual consumptions, which is a typical application of\ndata analysis. {NILM aims to help households understand how the energy is used\nand consequently tell them how to effectively manage the energy, thus allowing\nenergy efficiency which is considered as one of the twin pillars of sustainable\nenergy policy (i.e., energy efficiency and renewable energy).} Although NILM is\nunidentifiable, it is widely believed that the NILM problem can be addressed by\ndata science. Most of the existing approaches address the energy disaggregation\nproblem by conventional techniques such as sparse coding, non-negative matrix\nfactorization, and hidden Markov model. Recent advances reveal that deep neural\nnetworks (DNNs) can get favorable performance for NILM since DNNs can\ninherently learn the discriminative signatures of the different appliances. In\nthis paper, we propose a novel method named adversarial energy disaggregation\n(AED) based on DNNs. We introduce the idea of adversarial learning into NILM,\nwhich is new for the energy disaggregation task. Our method trains a generator\nand multiple discriminators via an adversarial fashion. The proposed method not\nonly learns shard representations for different appliances, but captures the\nspecific multimode structures of each appliance. Extensive experiments on\nreal-world datasets verify that our method can achieve new state-of-the-art\nperformance.",
          "link": "http://arxiv.org/abs/2108.01998",
          "publishedOn": "2021-08-05T01:56:20.944Z",
          "wordCount": 686,
          "title": "Adversarial Energy Disaggregation for Non-intrusive Load Monitoring. (arXiv:2108.01998v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eliasof_M/0/1/0/all/0/1\">Moshe Eliasof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haber_E/0/1/0/all/0/1\">Eldad Haber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treister_E/0/1/0/all/0/1\">Eran Treister</a>",
          "description": "Graph neural networks are increasingly becoming the go-to approach in various\nfields such as computer vision, computational biology and chemistry, where data\nare naturally explained by graphs. However, unlike traditional convolutional\nneural networks, deep graph networks do not necessarily yield better\nperformance than shallow graph networks. This behavior usually stems from the\nover-smoothing phenomenon. In this work, we propose a family of architectures\nto control this behavior by design. Our networks are motivated by numerical\nmethods for solving Partial Differential Equations (PDEs) on manifolds, and as\nsuch, their behavior can be explained by similar analysis. Moreover, as we\ndemonstrate using an extensive set of experiments, our PDE-motivated networks\ncan generalize and be effective for various types of problems from different\nfields. Our architectures obtain better or on par with the current\nstate-of-the-art results for problems that are typically approached using\ndifferent architectures.",
          "link": "http://arxiv.org/abs/2108.01938",
          "publishedOn": "2021-08-05T01:56:20.926Z",
          "wordCount": 591,
          "title": "PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations. (arXiv:2108.01938v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Domingo_Ferrer_J/0/1/0/all/0/1\">Josep Domingo-Ferrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanco_Justicia_A/0/1/0/all/0/1\">Alberto Blanco-Justicia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjon_J/0/1/0/all/0/1\">Jes&#xfa;s Manj&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_D/0/1/0/all/0/1\">David S&#xe1;nchez</a>",
          "description": "The decentralized nature of federated learning, that often leverages the\npower of edge devices, makes it vulnerable to attacks against privacy and\nsecurity. The privacy risk for a peer is that the model update she computes on\nher private data may, when sent to the model manager, leak information on those\nprivate data. Even more obvious are security attacks, whereby one or several\nmalicious peers return wrong model updates in order to disrupt the learning\nprocess and lead to a wrong model being learned. In this paper we build a\nfederated learning framework that offers privacy to the participating peers as\nwell as security against Byzantine and poisoning attacks. Our framework\nconsists of several protocols that provide strong privacy to the participating\npeers via unlinkable anonymity and that are rationally sustainable based on the\nco-utility property. In other words, no rational party is interested in\ndeviating from the proposed protocols. We leverage the notion of co-utility to\nbuild a decentralized co-utile reputation management system that provides\nincentives for parties to adhere to the protocols. Unlike privacy protection\nvia differential privacy, our approach preserves the values of model updates\nand hence the accuracy of plain federated learning; unlike privacy protection\nvia update aggregation, our approach preserves the ability to detect bad model\nupdates while substantially reducing the computational overhead compared to\nmethods based on homomorphic encryption.",
          "link": "http://arxiv.org/abs/2108.01913",
          "publishedOn": "2021-08-05T01:56:20.919Z",
          "wordCount": 690,
          "title": "Secure and Privacy-Preserving Federated Learning via Co-Utility. (arXiv:2108.01913v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01997",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1\">Chengtao Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Long_Y/0/1/0/all/0/1\">Yunfei Long</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_S/0/1/0/all/0/1\">Senhua Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tu_D/0/1/0/all/0/1\">Dandan Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>",
          "description": "Early detection of the coronavirus disease 2019 (COVID-19) helps to treat\npatients timely and increase the cure rate, thus further suppressing the spread\nof the disease. In this study, we propose a novel deep learning based detection\nand similar case recommendation network to help control the epidemic. Our\nproposed network contains two stages: the first one is a lung region\nsegmentation step and is used to exclude irrelevant factors, and the second is\na detection and recommendation stage. Under this framework, in the second\nstage, we develop a dual-children network (DuCN) based on a pre-trained\nResNet-18 to simultaneously realize the disease diagnosis and similar case\nrecommendation. Besides, we employ triplet loss and intrapulmonary distance\nmaps to assist the detection, which helps incorporate tiny differences between\ntwo images and is conducive to improving the diagnostic accuracy. For each\nconfirmed COVID-19 case, we give similar cases to provide radiologists with\ndiagnosis and treatment references. We conduct experiments on a large publicly\navailable dataset (CC-CCII) and compare the proposed model with\nstate-of-the-art COVID-19 detection methods. The results show that our proposed\nmodel achieves a promising clinical performance.",
          "link": "http://arxiv.org/abs/2108.01997",
          "publishedOn": "2021-08-05T01:56:20.913Z",
          "wordCount": 688,
          "title": "DuCN: Dual-children Network for Medical Diagnosis and Similar Case Recommendation towards COVID-19. (arXiv:2108.01997v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01965",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Al_Saffar_A/0/1/0/all/0/1\">A. Al-Saffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">L. Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbosh_A/0/1/0/all/0/1\">A. Abbosh</a>",
          "description": "So far, numerous learned models have been pressed to use in microwave imaging\nproblems. These models however, are oblivious to the imaging geometry. It has\nalways been hard to bake the physical setup of the imaging array into the\nstructure of the network, resulting in a data-intensive models that are not\npractical. This work put forward a graph formulation of the microwave imaging\narray. The architectures proposed is made cognizant of the physical setup,\nallowing it to incorporate the symmetries, resulting in a less data\nrequirements. Graph convolution and attention mechanism is deployed to handle\nthe cases of fully-connected graphs corresponding to multi-static arrays. The\ngraph-treatment of the problem is evaluated on experimental setup in context of\nbrain anomaly localization with microwave imaging.",
          "link": "http://arxiv.org/abs/2108.01965",
          "publishedOn": "2021-08-05T01:56:20.906Z",
          "wordCount": 574,
          "title": "Graph Attention Network For Microwave Imaging of Brain Anomaly. (arXiv:2108.01965v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01991",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1\">Truc Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pernkopf_F/0/1/0/all/0/1\">Franz Pernkopf</a>",
          "description": "In this paper, we use pre-trained ResNet models as backbone architectures for\nclassification of adventitious lung sounds and respiratory diseases. The\nknowledge of the pre-trained model is transferred by using vanilla fine-tuning,\nco-tuning, stochastic normalization and the combination of the co-tuning and\nstochastic normalization techniques. Furthermore, data augmentation in both\ntime domain and time-frequency domain is used to account for the class\nimbalance of the ICBHI and our multi-channel lung sound dataset. Additionally,\nwe apply spectrum correction to consider the variations of the recording device\nproperties on the ICBHI dataset. Empirically, our proposed systems mostly\noutperform all state-of-the-art lung sound classification systems for the\nadventitious lung sounds and respiratory diseases of both datasets.",
          "link": "http://arxiv.org/abs/2108.01991",
          "publishedOn": "2021-08-05T01:56:20.900Z",
          "wordCount": 560,
          "title": "Lung Sound Classification Using Co-tuning and Stochastic Normalization. (arXiv:2108.01991v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01952",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bondugula_K/0/1/0/all/0/1\">Kartheek Bondugula</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mazuelas_S/0/1/0/all/0/1\">Santiago Mazuelas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Perez_A/0/1/0/all/0/1\">Aritz P&#xe9;rez</a>",
          "description": "Existing libraries for supervised classification implement techniques that\nare based on empirical risk minimization and utilize surrogate losses. We\npresent MRCpy library that implements minimax risk classifiers (MRCs) that are\nbased on robust risk minimization and can utilize 0-1-loss. Such techniques\ngive rise to a manifold of classification methods that can provide tight bounds\non the expected loss. MRCpy provides a unified interface for different variants\nof MRCs and follows the standards of popular Python libraries. The presented\nlibrary also provides implementation for popular techniques that can be seen as\nMRCs such as L1-regularized logistic regression, zero-one adversarial, and\nmaximum entropy machines. In addition, MRCpy implements recent feature mappings\nsuch as Fourier, ReLU, and threshold features. The library is designed with an\nobject-oriented approach that facilitates collaborators and users.",
          "link": "http://arxiv.org/abs/2108.01952",
          "publishedOn": "2021-08-05T01:56:20.879Z",
          "wordCount": 569,
          "title": "MRCpy: A Library for Minimax Risk Classifiers. (arXiv:2108.01952v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01772",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Luo_Y/0/1/0/all/0/1\">Yuetian Luo</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_X/0/1/0/all/0/1\">Xudong Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_A/0/1/0/all/0/1\">Anru R. Zhang</a>",
          "description": "In this paper, we consider the geometric landscape connection of the widely\nstudied manifold and factorization formulations in low-rank positive\nsemidefinite (PSD) and general matrix optimization. We establish an equivalence\non the set of first-order stationary points (FOSPs) and second-order stationary\npoints (SOSPs) between the manifold and the factorization formulations. We\nfurther give a sandwich inequality on the spectrum of Riemannian and Euclidean\nHessians at FOSPs, which can be used to transfer more geometric properties from\none formulation to another. Similarities and differences on the landscape\nconnection under the PSD case and the general case are discussed. To the best\nof our knowledge, this is the first geometric landscape connection between the\nmanifold and the factorization formulations for handling rank constraints. In\nthe general low-rank matrix optimization, the landscape connection of two\nfactorization formulations (unregularized and regularized ones) is also\nprovided. By applying these geometric landscape connections, we are able to\nsolve unanswered questions in literature and establish stronger results in the\napplications on geometric analysis of phase retrieval, well-conditioned\nlow-rank matrix optimization, and the role of regularization in factorization\narising from machine learning and signal processing.",
          "link": "http://arxiv.org/abs/2108.01772",
          "publishedOn": "2021-08-05T01:56:20.872Z",
          "wordCount": 644,
          "title": "Nonconvex Factorization and Manifold Formulations are Almost Equivalent in Low-rank Matrix Optimization. (arXiv:2108.01772v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carlton_J/0/1/0/all/0/1\">Jonathan Carlton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1\">Andy Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jay_C/0/1/0/all/0/1\">Caroline Jay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keane_J/0/1/0/all/0/1\">John Keane</a>",
          "description": "Media is evolving from traditional linear narratives to personalised\nexperiences, where control over information (or how it is presented) is given\nto individual audience members. Measuring and understanding audience engagement\nwith this media is important in at least two ways: (1) a post-hoc understanding\nof how engaged audiences are with the content will help production teams learn\nfrom experience and improve future productions; (2), this type of media has\npotential for real-time measures of engagement to be used to enhance the user\nexperience by adapting content on-the-fly. Engagement is typically measured by\nasking samples of users to self-report, which is time consuming and expensive.\nIn some domains, however, interaction data have been used to infer engagement.\nFortuitously, the nature of interactive media facilitates a much richer set of\ninteraction data than traditional media; our research aims to understand if\nthese data can be used to infer audience engagement. In this paper, we report a\nstudy using data captured from audience interactions with an interactive TV\nshow to model and predict engagement. We find that temporal metrics, including\noverall time spent on the experience and the interval between events, are\npredictive of engagement. The results demonstrate that interaction data can be\nused to infer users' engagement during and after an experience, and the\nproposed techniques are relevant to better understand audience preference and\nresponses.",
          "link": "http://arxiv.org/abs/2108.01949",
          "publishedOn": "2021-08-05T01:56:20.866Z",
          "wordCount": 685,
          "title": "Using Interaction Data to Predict Engagement with Interactive Media. (arXiv:2108.01949v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tucker_M/0/1/0/all/0/1\">Mycal Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Siddharth Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_D/0/1/0/all/0/1\">Dana Hughes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1\">Katia Sycara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Michael Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1\">Julie Shah</a>",
          "description": "Neural agents trained in reinforcement learning settings can learn to\ncommunicate among themselves via discrete tokens, accomplishing as a team what\nagents would be unable to do alone. However, the current standard of using\none-hot vectors as discrete communication tokens prevents agents from acquiring\nmore desirable aspects of communication such as zero-shot understanding.\nInspired by word embedding techniques from natural language processing, we\npropose neural agent architectures that enables them to communicate via\ndiscrete tokens derived from a learned, continuous space. We show in a decision\ntheoretic framework that our technique optimizes communication over a wide\nrange of scenarios, whereas one-hot tokens are only optimal under restrictive\nassumptions. In self-play experiments, we validate that our trained agents\nlearn to cluster tokens in semantically-meaningful ways, allowing them\ncommunicate in noisy environments where other techniques fail. Lastly, we\ndemonstrate both that agents using our method can effectively respond to novel\nhuman communication and that humans can understand unlabeled emergent agent\ncommunication, outperforming the use of one-hot communication.",
          "link": "http://arxiv.org/abs/2108.01828",
          "publishedOn": "2021-08-05T01:56:20.857Z",
          "wordCount": 606,
          "title": "Emergent Discrete Communication in SemanticSpaces. (arXiv:2108.01828v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1\">Leonard Adolphs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1\">Shehzaad Dhuliawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>",
          "description": "Large pre-trained language models (LMs) are capable of not only recovering\nlinguistic but also factual and commonsense knowledge. To access the knowledge\nstored in mask-based LMs, we can use cloze-style questions and let the model\nfill in the blank. The flexibility advantage over structured knowledge bases\ncomes with the drawback of finding the right query for a certain information\nneed. Inspired by human behavior to disambiguate a question, we propose to\nquery LMs by example. To clarify the ambivalent question \"Who does Neuer play\nfor?\", a successful strategy is to demonstrate the relation using another\nsubject, e.g., \"Ronaldo plays for Portugal. Who does Neuer play for?\". We apply\nthis approach of querying by example to the LAMA probe and obtain substantial\nimprovements of up to 37.8% for BERT-large on the T-REx data when providing\nonly 10 demonstrations--even outperforming a baseline that queries the model\nwith up to 40 paraphrases of the question. The examples are provided through\nthe model's context and thus require neither fine-tuning nor an additional\nforward pass. This suggests that LMs contain more factual and commonsense\nknowledge than previously assumed--if we query the model in the right way.",
          "link": "http://arxiv.org/abs/2108.01928",
          "publishedOn": "2021-08-05T01:56:20.849Z",
          "wordCount": 622,
          "title": "How to Query Language Models?. (arXiv:2108.01928v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>",
          "description": "Training-time safety violations have been a major concern when we deploy\nreinforcement learning algorithms in the real world. This paper explores the\npossibility of safe RL algorithms with zero training-time safety violations in\nthe challenging setting where we are only given a safe but trivial-reward\ninitial policy without any prior knowledge of the dynamics model and additional\noffline data. We propose an algorithm, Co-trained Barrier Certificate for Safe\nRL (CRABS), which iteratively learns barrier certificates, dynamics models, and\npolicies. The barrier certificates, learned via adversarial training, ensure\nthe policy's safety assuming calibrated learned dynamics model. We also add a\nregularization term to encourage larger certified regions to enable better\nexploration. Empirical simulations show that zero safety violations are already\nchallenging for a suite of simple environments with only 2-4 dimensional state\nspace, especially if high-reward policies have to visit regions near the safety\nboundary. Prior methods require hundreds of violations to achieve decent\nrewards on these tasks, whereas our proposed algorithms incur zero violations.",
          "link": "http://arxiv.org/abs/2108.01846",
          "publishedOn": "2021-08-05T01:56:20.821Z",
          "wordCount": 610,
          "title": "Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations. (arXiv:2108.01846v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01867",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arulkumaran_K/0/1/0/all/0/1\">Kai Arulkumaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lillrank_D/0/1/0/all/0/1\">Dan Ogawa Lillrank</a>",
          "description": "The introduction of the generative adversarial imitation learning (GAIL)\nalgorithm has spurred the development of scalable imitation learning approaches\nusing deep neural networks. The GAIL objective can be thought of as 1) matching\nthe expert policy's state distribution; 2) penalising the learned policy's\nstate distribution; and 3) maximising entropy. While theoretically motivated,\nin practice GAIL can be difficult to apply, not least due to the instabilities\nof adversarial training. In this paper, we take a pragmatic look at GAIL and\nrelated imitation learning algorithms. We implement and automatically tune a\nrange of algorithms in a unified experimental setup, presenting a fair\nevaluation between the competing methods. From our results, our primary\nrecommendation is to consider non-adversarial methods. Furthermore, we discuss\nthe common components of imitation learning objectives, and present promising\navenues for future research.",
          "link": "http://arxiv.org/abs/2108.01867",
          "publishedOn": "2021-08-05T01:56:20.813Z",
          "wordCount": 570,
          "title": "A Pragmatic Look at Deep Imitation Learning. (arXiv:2108.01867v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guet_C/0/1/0/all/0/1\">Claude Guet</a>",
          "description": "We introduce a self-consistent deep-learning framework which, for a noisy\ndeterministic time series, provides unsupervised filtering, state-space\nreconstruction, identification of the underlying differential equations and\nforecasting. Without a priori information on the signal, we embed the time\nseries in a state space, where deterministic structures, i.e. attractors, are\nrevealed. Under the assumption that the evolution of solution trajectories is\ndescribed by an unknown dynamical system, we filter out stochastic outliers.\nThe embedding function, the solution trajectories and the dynamical systems are\nconstructed using deep neural networks, respectively. By exploiting the\ndifferentiability of the neural solution trajectory, the neural dynamical\nsystem is defined locally at each time, mitigating the need for propagating\ngradients through numerical solvers. On a chaotic time series masked by\nadditive Gaussian noise, we demonstrate the filtering ability and the\npredictive power of the proposed framework.",
          "link": "http://arxiv.org/abs/2108.01862",
          "publishedOn": "2021-08-05T01:56:20.807Z",
          "wordCount": 577,
          "title": "Reconstructing a dynamical system and forecasting time series by self-consistent deep learning. (arXiv:2108.01862v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klemsdal_E/0/1/0/all/0/1\">Even Klemsdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herland_S/0/1/0/all/0/1\">Sverre Herland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murad_A/0/1/0/all/0/1\">Abdulmajid Murad</a>",
          "description": "To increase autonomy in reinforcement learning, agents need to learn useful\nbehaviours without reliance on manually designed reward functions. To that end,\nskill discovery methods have been used to learn the intrinsic options available\nto an agent using task-agnostic objectives. However, without the guidance of\ntask-specific rewards, emergent behaviours are generally useless due to the\nunder-constrained problem of skill discovery in complex and high-dimensional\nspaces. This paper proposes a framework for guiding the skill discovery towards\nthe subset of expert-visited states using a learned state projection. We apply\nour method in various reinforcement learning (RL) tasks and show that such a\nprojection results in more useful behaviours.",
          "link": "http://arxiv.org/abs/2108.01869",
          "publishedOn": "2021-08-05T01:56:20.801Z",
          "wordCount": 547,
          "title": "Learning Task Agnostic Skills with Data-driven Guidance. (arXiv:2108.01869v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>",
          "description": "Despite the success of multilingual sequence-to-sequence pretraining, most\nexisting approaches rely on monolingual corpora, and do not make use of the\nstrong cross-lingual signal contained in parallel data. In this paper, we\npresent PARADISE (PARAllel & Denoising Integration in SEquence-to-sequence\nmodels), which extends the conventional denoising objective used to train these\nmodels by (i) replacing words in the noised sequence according to a\nmultilingual dictionary, and (ii) predicting the reference translation\naccording to a parallel corpus instead of recovering the original sequence. Our\nexperiments on machine translation and cross-lingual natural language inference\nshow an average improvement of 2.0 BLEU points and 6.7 accuracy points from\nintegrating parallel data into pretraining, respectively, obtaining results\nthat are competitive with several popular models at a fraction of their\ncomputational cost.",
          "link": "http://arxiv.org/abs/2108.01887",
          "publishedOn": "2021-08-05T01:56:20.794Z",
          "wordCount": 561,
          "title": "PARADISE: Exploiting Parallel Data for Multilingual Sequence-to-Sequence Pretraining. (arXiv:2108.01887v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gulrez_T/0/1/0/all/0/1\">Tauseef Gulrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansell_W/0/1/0/all/0/1\">Warren Mansell</a>",
          "description": "Deep reinforcement learning (DRL) requires large samples and a long training\ntime to operate optimally. Yet humans rarely require long periods training to\nperform well on novel tasks, such as computer games, once they are provided\nwith an accurate program of instructions. We used perceptual control theory\n(PCT) to construct a simple closed-loop model which requires no training\nsamples and training time within a video game study using the Arcade Learning\nEnvironment (ALE). The model was programmed to parse inputs from the\nenvironment into hierarchically organised perceptual signals, and it computed a\ndynamic error signal by subtracting the incoming signal for each perceptual\nvariable from a reference signal to drive output signals to reduce this error.\nWe tested the same model across two different Atari paddle games Breakout and\nPong to achieve performance at least as high as DRL paradigms, and close to\ngood human performance. Our study shows that perceptual control models, based\non simple assumptions, can perform well without learning. We conclude by\nspecifying a parsimonious role of learning that may be more similar to\npsychological functioning.",
          "link": "http://arxiv.org/abs/2108.01895",
          "publishedOn": "2021-08-05T01:56:20.785Z",
          "wordCount": 626,
          "title": "High Performance Across Two Atari Paddle Games Using the Same Perceptual Control Architecture Without Training. (arXiv:2108.01895v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treviso_M/0/1/0/all/0/1\">Marcos Treviso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinhas_A/0/1/0/all/0/1\">Ant&#xf3;nio Farinhas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguiar_P/0/1/0/all/0/1\">Pedro M. Q. Aguiar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figueiredo_M/0/1/0/all/0/1\">M&#xe1;rio A. T. Figueiredo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blondel_M/0/1/0/all/0/1\">Mathieu Blondel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1\">Vlad Niculae</a>",
          "description": "Exponential families are widely used in machine learning; they include many\ndistributions in continuous and discrete domains (e.g., Gaussian, Dirichlet,\nPoisson, and categorical distributions via the softmax transformation).\nDistributions in each of these families have fixed support. In contrast, for\nfinite domains, there has been recent works on sparse alternatives to softmax\n(e.g. sparsemax, $\\alpha$-entmax, and fusedmax) and corresponding losses, which\nhave varying support.\n\nThis paper expands that line of work in several directions: first, it extends\n$\\Omega$-regularized prediction maps and Fenchel-Young losses to arbitrary\ndomains (possibly countably infinite or continuous). For linearly parametrized\nfamilies, we show that minimization of Fenchel-Young losses is equivalent to\nmoment matching of the statistics, generalizing a fundamental property of\nexponential families. When $\\Omega$ is a Tsallis negentropy with parameter\n$\\alpha$, we obtain \"deformed exponential families,\" which include\n$\\alpha$-entmax and sparsemax ($\\alpha$ = 2) as particular cases. For quadratic\nenergy functions in continuous domains, the resulting densities are\n$\\beta$-Gaussians, an instance of elliptical distributions that contain as\nparticular cases the Gaussian, biweight, triweight and Epanechnikov densities,\nand for which we derive closed-form expressions for the variance, Tsallis\nentropy, and Fenchel-Young loss. When $\\Omega$ is a total variation or Sobolev\nregularizer, we obtain a continuous version of the fusedmax. Finally, we\nintroduce continuous-domain attention mechanisms, deriving efficient gradient\nbackpropagation algorithms for $\\alpha \\in \\{1, 4/3, 3/2, 2\\}$. Using them, we\ndemonstrate our sparse continuous distributions for attention-based audio\nclassification and visual question answering, showing that they allow attending\nto time intervals and compact regions.",
          "link": "http://arxiv.org/abs/2108.01988",
          "publishedOn": "2021-08-05T01:56:20.766Z",
          "wordCount": 703,
          "title": "Sparse Continuous Distributions and Fenchel-Young Losses. (arXiv:2108.01988v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1\">Cong Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Deming Chen</a>",
          "description": "Most existing neural architecture search (NAS) algorithms are dedicated to\nthe downstream tasks, e.g., image classification in computer vision. However,\nextensive experiments have shown that, prominent neural architectures, such as\nResNet in computer vision and LSTM in natural language processing, are\ngenerally good at extracting patterns from the input data and perform well on\ndifferent downstream tasks. These observations inspire us to ask: Is it\nnecessary to use the performance of specific downstream tasks to evaluate and\nsearch for good neural architectures? Can we perform NAS effectively and\nefficiently while being agnostic to the downstream task? In this work, we\nattempt to affirmatively answer the above two questions and improve the\nstate-of-the-art NAS solution by proposing a novel and generic NAS framework,\ntermed Generic NAS (GenNAS). GenNAS does not use task-specific labels but\ninstead adopts \\textit{regression} on a set of manually designed synthetic\nsignal bases for architecture evaluation. Such a self-supervised regression\ntask can effectively evaluate the intrinsic power of an architecture to capture\nand transform the input signal patterns, and allow more sufficient usage of\ntraining samples. We then propose an automatic task search to optimize the\ncombination of synthetic signals using limited downstream-task-specific labels,\nfurther improving the performance of GenNAS. We also thoroughly evaluate\nGenNAS's generality and end-to-end NAS performance on all search spaces, which\noutperforms almost all existing works with significant speedup.",
          "link": "http://arxiv.org/abs/2108.01899",
          "publishedOn": "2021-08-05T01:56:20.754Z",
          "wordCount": 669,
          "title": "Generic Neural Architecture Search via Regression. (arXiv:2108.01899v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01994",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Varando_G/0/1/0/all/0/1\">Gherardo Varando</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carli_F/0/1/0/all/0/1\">Federico Carli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Leonelli_M/0/1/0/all/0/1\">Manuele Leonelli</a>",
          "description": "Bayesian networks are a widely-used class of probabilistic graphical models\ncapable of representing symmetric conditional independence between variables of\ninterest using the topology of the underlying graph. They can be seen as a\nspecial case of the much more general class of models called staged trees,\nwhich can represent any type of non-symmetric conditional independence. Here we\nformalize the relationship between these two models and introduce a minimal\nBayesian network representation of the staged tree, which can be used to read\nconditional independences in an intuitive way. Furthermore, we define a new\nlabeled graph, termed asymmetry-labeled directed acyclic graph, whose edges are\nlabeled to denote the type of dependence existing between any two random\nvariables. Various datasets are used to illustrate the methodology,\nhighlighting the need to construct models which more flexibly encode and\nrepresent non-symmetric structures.",
          "link": "http://arxiv.org/abs/2108.01994",
          "publishedOn": "2021-08-05T01:56:20.748Z",
          "wordCount": 566,
          "title": "Staged trees and asymmetry-labeled DAGs. (arXiv:2108.01994v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bonometti_V/0/1/0/all/0/1\">Valerio Bonometti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_M/0/1/0/all/0/1\">Mathieu J. Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drachen_A/0/1/0/all/0/1\">Anders Drachen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wade_A/0/1/0/all/0/1\">Alex Wade</a>",
          "description": "Incentive salience attribution can be understood as a psychobiological\nprocess ascribing relevance to potentially rewarding objects and actions.\nDespite being an important component of the motivational process guiding our\neveryday behaviour its study in naturalistic contexts is not straightforward.\nHere we propose a methodology based on artificial neural networks (ANNs) for\napproximating latent states produced by this process in situations where large\nvolumes of behavioural data are available but no strict experimental control is\npossible. Leveraging knowledge derived from theoretical and computational\naccounts of incentive salience attribution we designed an ANN for estimating\nduration and intensity of future interactions between individuals and a series\nof video games in a large-scale ($N> 3 \\times 10^6$) longitudinal dataset.\nThrough model comparison and inspection we show that our approach outperforms\ncompeting ones while also generating a representation that well approximate\nsome of the functions of attributed incentive salience. We discuss our findings\nwith reference to the adopted theoretical and computational frameworks and\nsuggest how our methodology could be an initial step for estimating attributed\nincentive salience in large scale behavioural studies.",
          "link": "http://arxiv.org/abs/2108.01724",
          "publishedOn": "2021-08-05T01:56:20.739Z",
          "wordCount": 631,
          "title": "Approximating Attributed Incentive Salience In Large Scale Scenarios. A Representation Learning Approach Based on Artificial Neural Networks. (arXiv:2108.01724v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaopeng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiechuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haobin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zongqing Lu</a>",
          "description": "When one agent interacts with a multi-agent environment, it is challenging to\ndeal with various opponents unseen before. Modeling the behaviors, goals, or\nbeliefs of opponents could help the agent adjust its policy to adapt to\ndifferent opponents. In addition, it is also important to consider opponents\nwho are learning simultaneously or capable of reasoning. However, existing work\nusually tackles only one of the aforementioned types of opponent. In this\npaper, we propose model-based opponent modeling (MBOM), which employs the\nenvironment model to adapt to all kinds of opponent. MBOM simulates the\nrecursive reasoning process in the environment model and imagines a set of\nimproving opponent policies. To effectively and accurately represent the\nopponent policy, MBOM further mixes the imagined opponent policies according to\nthe similarity with the real behaviors of opponents. Empirically, we show that\nMBOM achieves more effective adaptation than existing methods in competitive\nand cooperative environments, respectively with different types of opponent,\ni.e., fixed policy, na\\\"ive learner, and reasoning learner.",
          "link": "http://arxiv.org/abs/2108.01843",
          "publishedOn": "2021-08-05T01:56:20.730Z",
          "wordCount": 590,
          "title": "Model-Based Opponent Modeling. (arXiv:2108.01843v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Quach_B/0/1/0/all/0/1\">Boi M. Quach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuong_D/0/1/0/all/0/1\">Dinh V. Cuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Nhung Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_D/0/1/0/all/0/1\">Dang Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh T. Nguyen</a>",
          "description": "There is a warning light for the loss of plant habitats worldwide that\nentails concerted efforts to conserve plant biodiversity. Thus, plant species\nclassification is of crucial importance to address this environmental\nchallenge. In recent years, there is a considerable increase in the number of\nstudies related to plant taxonomy. While some researchers try to improve their\nrecognition performance using novel approaches, others concentrate on\ncomputational optimization for their framework. In addition, a few studies are\ndiving into feature extraction to gain significantly in terms of accuracy. In\nthis paper, we propose an effective method for the leaf recognition problem. In\nour proposed approach, a leaf goes through some pre-processing to extract its\nrefined color image, vein image, xy-projection histogram, handcrafted shape,\ntexture features, and Fourier descriptors. These attributes are then\ntransformed into a better representation by neural network-based encoders\nbefore a support vector machine (SVM) model is utilized to classify different\nleaves. Overall, our approach performs a state-of-the-art result on the Flavia\nleaf dataset, achieving the accuracy of 99.58\\% on test sets under random\n10-fold cross-validation and bypassing the previous methods. We also release\nour codes\\footnote{Scripts are available at\n\\url{https://github.com/dinhvietcuong1996/LeafRecognition}} for contributing to\nthe research community in the leaf classification problem.",
          "link": "http://arxiv.org/abs/2108.01808",
          "publishedOn": "2021-08-05T01:56:20.712Z",
          "wordCount": 664,
          "title": "An Effective Leaf Recognition Using Convolutional Neural Networks Based Features. (arXiv:2108.01808v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jessica Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhulipala_L/0/1/0/all/0/1\">Laxman Dhulipala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstat_D/0/1/0/all/0/1\">David Eisenstat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacki_J/0/1/0/all/0/1\">Jakub &#x141;&#x105;cki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1\">Vahab Mirrokni</a>",
          "description": "Graph clustering and community detection are central problems in modern data\nmining. The increasing need for analyzing billion-scale data calls for faster\nand more scalable algorithms for these problems. There are certain trade-offs\nbetween the quality and speed of such clustering algorithms. In this paper, we\ndesign scalable algorithms that achieve high quality when evaluated based on\nground truth. We develop a generalized sequential and shared-memory parallel\nframework based on the LambdaCC objective (introduced by Veldt et al.), which\nencompasses modularity and correlation clustering. Our framework consists of\nhighly-optimized implementations that scale to large data sets of billions of\nedges and that obtain high-quality clusters compared to ground-truth data, on\nboth unweighted and weighted graphs. Our empirical evaluation shows that this\nframework improves the state-of-the-art trade-offs between speed and quality of\nscalable community detection. For example, on a 30-core machine with two-way\nhyper-threading, our implementations achieve orders of magnitude speedups over\nother correlation clustering baselines, and up to 28.44x speedups over our own\nsequential baselines while maintaining or improving quality.",
          "link": "http://arxiv.org/abs/2108.01731",
          "publishedOn": "2021-08-05T01:56:20.704Z",
          "wordCount": 633,
          "title": "Scalable Community Detection via Parallel Correlation Clustering. (arXiv:2108.01731v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Meng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jasour_A/0/1/0/all/0/1\">Ashkan Jasour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosman_G/0/1/0/all/0/1\">Guy Rosman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Brian Williams</a>",
          "description": "Risk-bounded motion planning is an important yet difficult problem for\nsafety-critical tasks. While existing mathematical programming methods offer\ntheoretical guarantees in the context of constrained Markov decision processes,\nthey either lack scalability in solving larger problems or produce conservative\nplans. Recent advances in deep reinforcement learning improve scalability by\nlearning policy networks as function approximators. In this paper, we propose\nan extension of soft actor critic model to estimate the execution risk of a\nplan through a risk critic and produce risk-bounded policies efficiently by\nadding an extra risk term in the loss function of the policy network. We define\nthe execution risk in an accurate form, as opposed to approximating it through\na summation of immediate risks at each time step that leads to conservative\nplans. Our proposed model is conditioned on a continuous spectrum of risk\nbounds, allowing the user to adjust the risk-averse level of the agent on the\nfly. Through a set of experiments, we show the advantage of our model in terms\nof both computational time and plan quality, compared to a state-of-the-art\nmathematical programming baseline, and validate its performance in more\ncomplicated scenarios, including nonlinear dynamics and larger state space.",
          "link": "http://arxiv.org/abs/2108.01851",
          "publishedOn": "2021-08-05T01:56:20.698Z",
          "wordCount": 644,
          "title": "Risk Conditioned Neural Motion Planning. (arXiv:2108.01851v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Edirisooriya_S/0/1/0/all/0/1\">Sachinda Edirisooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao-Wen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>",
          "description": "Previous work has shown that neural architectures are able to perform optical\nmusic recognition (OMR) on monophonic and homophonic music with high accuracy.\nHowever, piano and orchestral scores frequently exhibit polyphonic passages,\nwhich add a second dimension to the task. Monophonic and homophonic music can\nbe described as homorhythmic, or having a single musical rhythm. Polyphonic\nmusic, on the other hand, can be seen as having multiple rhythmic sequences, or\nvoices, concurrently. We first introduce a workflow for creating large-scale\npolyphonic datasets suitable for end-to-end recognition from sheet music\npublicly available on the MuseScore forum. We then propose two novel\nformulations for end-to-end polyphonic OMR -- one treating the problem as a\ntype of multi-task binary classification, and the other treating it as\nmulti-sequence detection. Building upon the encoder-decoder architecture and an\nimage encoder proposed in past work on end-to-end OMR, we propose two novel\ndecoder models -- FlagDecoder and RNNDecoder -- that correspond to the two\nformulations. Finally, we compare the empirical performance of these end-to-end\napproaches to polyphonic OMR and observe a new state-of-the-art performance\nwith our multi-sequence detection decoder, RNNDecoder.",
          "link": "http://arxiv.org/abs/2108.01769",
          "publishedOn": "2021-08-05T01:56:20.690Z",
          "wordCount": 639,
          "title": "An Empirical Evaluation of End-to-End Polyphonic Optical Music Recognition. (arXiv:2108.01769v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiechuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zongqing Lu</a>",
          "description": "In many real-world multi-agent cooperative tasks, due to high cost and risk,\nagents cannot interact with the environment and collect experiences during\nlearning, but have to learn from offline datasets. However, the transition\nprobabilities calculated from the dataset can be much different from the\ntransition probabilities induced by the learned policies of other agents,\ncreating large errors in value estimates. Moreover, the experience\ndistributions of agents' datasets may vary wildly due to diverse behavior\npolicies, causing large difference in value estimates between agents.\nConsequently, agents will learn uncoordinated suboptimal policies. In this\npaper, we propose MABCQ, which exploits value deviation and transition\nnormalization to modify the transition probabilities. Value deviation\noptimistically increases the transition probabilities of high-value next\nstates, and transition normalization normalizes the biased transition\nprobabilities of next states. They together encourage agents to discover\npotential optimal and coordinated policies. Mathematically, we prove the\nconvergence of Q-learning under the non-stationary transition probabilities\nafter modification. Empirically, we show that MABCQ greatly outperforms\nbaselines and reduces the difference in value estimates between agents.",
          "link": "http://arxiv.org/abs/2108.01832",
          "publishedOn": "2021-08-05T01:56:20.684Z",
          "wordCount": 595,
          "title": "Offline Decentralized Multi-Agent Reinforcement Learning. (arXiv:2108.01832v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gniewkowski_M/0/1/0/all/0/1\">Mateusz Gniewkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maciejewski_H/0/1/0/all/0/1\">Henryk Maciejewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surmacz_T/0/1/0/all/0/1\">Tomasz R. Surmacz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walentynowicz_W/0/1/0/all/0/1\">Wiktor Walentynowicz</a>",
          "description": "Hypertext transfer protocol (HTTP) is one of the most widely used protocols\non the Internet. As a consequence, most attacks (i.e., SQL injection, XSS) use\nHTTP as the transport mechanism. Therefore, it is crucial to develop an\nintelligent solution that would allow to effectively detect and filter out\nanomalies in HTTP traffic. Currently, most of the anomaly detection systems are\neither rule-based or trained using manually selected features. We propose\nutilizing modern unsupervised language representation model for embedding HTTP\nrequests and then using it to classify anomalies in the traffic. The solution\nis motivated by methods used in Natural Language Processing (NLP) such as\nDoc2Vec which could potentially capture the true understanding of HTTP\nmessages, and therefore improve the efficiency of Intrusion Detection System.\nIn our work, we not only aim at generating a suitable embedding space, but also\nat the interpretability of the proposed model. We decided to use the current\nstate-of-the-art RoBERTa, which, as far as we know, has never been used in a\nsimilar problem. To verify how the solution would work in real word conditions,\nwe train the model using only legitimate traffic. We also try to explain the\nresults based on clusters that occur in the vectorized requests space and a\nsimple logistic regression classifier. We compared our approach with the\nsimilar, previously proposed methods. We evaluate the feasibility of our method\non three different datasets: CSIC2010, CSE-CIC-IDS2018 and one that we prepared\nourselves. The results we show are comparable to others or better, and most\nimportantly - interpretable.",
          "link": "http://arxiv.org/abs/2108.01763",
          "publishedOn": "2021-08-05T01:56:20.678Z",
          "wordCount": 694,
          "title": "HTTP2vec: Embedding of HTTP Requests for Detection of Anomalous Traffic. (arXiv:2108.01763v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Suchandra Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_D/0/1/0/all/0/1\">Dhrubasish Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Sohom Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kole_D/0/1/0/all/0/1\">Dipak K. Kole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jana_P/0/1/0/all/0/1\">Premananda Jana</a>",
          "description": "Social media platforms are thriving nowadays, so a huge volume of data is\nproduced. As it includes brief and clear statements, millions of people post\ntheir thoughts on microblogging sites every day. This paper represents and\nanalyze the capacity of diverse strategies to volumetric, delicate, and social\nnetworks to predict critical opinions from online social networking sites. In\nthe exploration of certain searching for relevant, the thoughts of people play\na crucial role. Social media becomes a good outlet since the last decades to\nshare the opinions globally. Sentiment analysis as well as opinion mining is a\ntool that is used to extract the opinions or thoughts of the common public. An\noccurrence in one place, be it economic, political, or social, may trigger\nlarge-scale chain public reaction across many other sites in an increasingly\ninterconnected world. This study demonstrates the evaluation of sentiment\nanalysis techniques using social media contents and creating the association\nbetween subjectivity with herd behavior and clustering coefficient as well as\ntries to predict the election result (2021 election in West Bengal). This is an\nimplementation of sentiment analysis targeted at estimating the results of an\nupcoming election by assessing the public's opinion across social media. This\npaper also has a short discussion section on the usefulness of the idea in\nother fields.",
          "link": "http://arxiv.org/abs/2108.01728",
          "publishedOn": "2021-08-05T01:56:20.648Z",
          "wordCount": 684,
          "title": "A Study on Herd Behavior Using Sentiment Analysis in Online Social Network. (arXiv:2108.01728v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hulse_J/0/1/0/all/0/1\">Jason Van Hulse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_J/0/1/0/all/0/1\">Joshua S. Friedman</a>",
          "description": "Deep neural networks have been applied to a wide range of problems across\ndifferent application domains with great success. Recently, research into\ncombinatorial optimization problems in particular has generated much interest\nin the machine learning community. In this work, we develop deep learning\nmodels to predict the chromatic number and maximum clique size of graphs, both\nof which represent classical NP-complete combinatorial optimization problems\nencountered in graph theory. The neural networks are trained using the most\nbasic representation of the graph, the adjacency matrix, as opposed to\nundergoing complex domain-specific feature engineering. The experimental\nresults show that deep neural networks, and in particular convolutional neural\nnetworks, obtain strong performance on this problem.",
          "link": "http://arxiv.org/abs/2108.01810",
          "publishedOn": "2021-08-05T01:56:20.642Z",
          "wordCount": 549,
          "title": "Deep Learning Chromatic and Clique Numbers of Graphs. (arXiv:2108.01810v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Joo Hun Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_H/0/1/0/all/0/1\">Ha Min Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1\">Hyejun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_E/0/1/0/all/0/1\">Eun-Hye Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1\">Ah Young Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Han Young Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1\">Hong Jin Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1\">Tai-Myoung Chung</a>",
          "description": "While machine learning techniques are being applied to various fields for\ntheir exceptional ability to find complex relations in large datasets, the\nstrengthening of regulations on data ownership and privacy is causing\nincreasing difficulty in its application to medical data. In light of this,\nFederated Learning has recently been proposed as a solution to train on private\ndata without breach of confidentiality. This conservation of privacy is\nparticularly appealing in the field of healthcare, where patient data is highly\nconfidential. However, many studies have shown that its assumption of\nIndependent and Identically Distributed data is unrealistic for medical data.\nIn this paper, we propose Personalized Federated Cluster Models, a hierarchical\nclustering-based FL process, to predict Major Depressive Disorder severity from\nHeart Rate Variability. By allowing clients to receive more personalized model,\nwe address problems caused by non-IID data, showing an accuracy increase in\nseverity prediction. This increase in performance may be sufficient to use\nPersonalized Federated Cluster Models in many existing Federated Learning\nscenarios.",
          "link": "http://arxiv.org/abs/2108.01903",
          "publishedOn": "2021-08-05T01:56:20.633Z",
          "wordCount": 628,
          "title": "Personalized Federated Learning with Clustering: Non-IID Heart Rate Variability Data Application. (arXiv:2108.01903v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weichert_D/0/1/0/all/0/1\">Dorina Weichert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horchler_F/0/1/0/all/0/1\">Felix Horchler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kister_A/0/1/0/all/0/1\">Alexander Kister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trost_M/0/1/0/all/0/1\">Marcus Trost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartung_J/0/1/0/all/0/1\">Johannes Hartung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risse_S/0/1/0/all/0/1\">Stefan Risse</a>",
          "description": "Monte Carlo Tree Search (MCTS) has shown its strength for a lot of\ndeterministic and stochastic examples, but literature lacks reports of\napplications to real world industrial processes. Common reasons for this are\nthat there is no efficient simulator of the process available or there exist\nproblems in applying MCTS to the complex rules of the process. In this paper,\nwe apply MCTS for optimizing a high-precision manufacturing process that has\nstochastic and partially observable outcomes. We make use of an\nexpert-knowledge-based simulator and adapt the MCTS default policy to deal with\nthe manufacturing process.",
          "link": "http://arxiv.org/abs/2108.01789",
          "publishedOn": "2021-08-05T01:56:20.600Z",
          "wordCount": 533,
          "title": "Monte Carlo Tree Search for high precision manufacturing. (arXiv:2108.01789v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alshubaily_I/0/1/0/all/0/1\">Ibrahim Alshubaily</a>",
          "description": "Neural networks are powerful models that have a remarkable ability to extract\npatterns that are too complex to be noticed by humans or other machine learning\nmodels. Neural networks are the first class of models that can train end-to-end\nsystems with large learning capacities. However, we still have the difficult\nchallenge of designing the neural network, which requires human experience and\na long process of trial and error. As a solution, we can use a neural\narchitecture search to find the best network architecture for the task at hand.\nExisting NAS algorithms generally evaluate the fitness of a new architecture by\nfully training from scratch, resulting in the prohibitive computational cost,\neven if operated on high-performance computers. In this paper, an end-to-end\noffline performance predictor is proposed to accelerate the evaluation of\nsampled architectures.\n\nIndex Terms- Learning Curve Prediction, Neural Architecture Search,\nReinforcement Learning.",
          "link": "http://arxiv.org/abs/2108.01854",
          "publishedOn": "2021-08-05T01:56:20.593Z",
          "wordCount": 564,
          "title": "Efficient Neural Architecture Search with Performance Prediction. (arXiv:2108.01854v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinchong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiilang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fasching_P/0/1/0/all/0/1\">Peter A. Fasching</a>",
          "description": "Electronic Health Records often suffer from missing data, which poses a major\nproblem in clinical practice and clinical studies. A novel approach for dealing\nwith missing data are Generative Adversarial Nets (GANs), which have been\ngenerating huge research interest in image generation and transformation.\nRecently, researchers have attempted to apply GANs to missing data generation\nand imputation for EHR data: a major challenge here is the categorical nature\nof the data. State-of-the-art solutions to the GAN-based generation of\ncategorical data involve either reinforcement learning, or learning a\nbidirectional mapping between the categorical and the real latent feature\nspace, so that the GANs only need to generate real-valued features. However,\nthese methods are designed to generate complete feature vectors instead of\nimputing only the subsets of missing features. In this paper we propose a\nsimple and yet effective approach that is based on previous work on GANs for\ndata imputation. We first motivate our solution by discussing the reason why\nadversarial training often fails in case of categorical features. Then we\nderive a novel way to re-code the categorical features to stabilize the\nadversarial training. Based on experiments on two real-world EHR data with\nmultiple settings, we show that our imputation approach largely improves the\nprediction accuracy, compared to more traditional data imputation approaches.",
          "link": "http://arxiv.org/abs/2108.01701",
          "publishedOn": "2021-08-05T01:56:20.586Z",
          "wordCount": 644,
          "title": "Categorical EHR Imputation with Generative Adversarial Nets. (arXiv:2108.01701v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seshadri_P/0/1/0/all/0/1\">Pavan Seshadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerch_A/0/1/0/all/0/1\">Alexander Lerch</a>",
          "description": "Several automatic approaches for objective music performance assessment (MPA)\nhave been proposed in the past, however, existing systems are not yet capable\nof reliably predicting ratings with the same accuracy as professional judges.\nThis study investigates contrastive learning as a potential method to improve\nexisting MPA systems. Contrastive learning is a widely used technique in\nrepresentation learning to learn a structured latent space capable of\nseparately clustering multiple classes. It has been shown to produce state of\nthe art results for image-based classification problems. We introduce a\nweighted contrastive loss suitable for regression tasks applied to a\nconvolutional neural network and show that contrastive loss results in\nperformance gains in regression tasks for MPA. Our results show that\ncontrastive-based methods are able to match and exceed SoTA performance for MPA\nregression tasks by creating better class clusters within the latent space of\nthe neural networks.",
          "link": "http://arxiv.org/abs/2108.01711",
          "publishedOn": "2021-08-05T01:56:20.561Z",
          "wordCount": 584,
          "title": "Improving Music Performance Assessment with Contrastive Learning. (arXiv:2108.01711v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Melotte_S/0/1/0/all/0/1\">Sara Melotte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kejriwal_M/0/1/0/all/0/1\">Mayank Kejriwal</a>",
          "description": "Although the recent rise and uptake of COVID-19 vaccines in the United States\nhas been encouraging, there continues to be significant vaccine hesitancy in\nvarious geographic and demographic clusters of the adult population. Surveys,\nsuch as the one conducted by Gallup over the past year, can be useful in\ndetermining vaccine hesitancy, but can be expensive to conduct and do not\nprovide real-time data. At the same time, the advent of social media suggests\nthat it may be possible to get vaccine hesitancy signals at an aggregate level\n(such as at the level of zip codes) by using machine learning models and\nsocioeconomic (and other) features from publicly available sources. It is an\nopen question at present whether such an endeavor is feasible, and how it\ncompares to baselines that only use constant priors. To our knowledge, a proper\nmethodology and evaluation results using real data has also not been presented.\nIn this article, we present such a methodology and experimental study, using\npublicly available Twitter data collected over the last year. Our goal is not\nto devise novel machine learning algorithms, but to evaluate existing and\nestablished models in a comparative framework. We show that the best models\nsignificantly outperform constant priors, and can be set up using open-source\ntools.",
          "link": "http://arxiv.org/abs/2108.01699",
          "publishedOn": "2021-08-05T01:56:20.555Z",
          "wordCount": 727,
          "title": "Predicting Zip Code-Level Vaccine Hesitancy in US Metropolitan Areas Using Machine Learning Models on Public Tweets. (arXiv:2108.01699v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01758",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Dong_Z/0/1/0/all/0/1\">Zhaolu Dong</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Huang_S/0/1/0/all/0/1\">Shan Huang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Ma_S/0/1/0/all/0/1\">Simiao Ma</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Qian_Y/0/1/0/all/0/1\">Yining Qian</a>",
          "description": "Deep Reinforcement learning is a branch of unsupervised learning in which an\nagent learns to act based on environment state in order to maximize its total\nreward. Deep reinforcement learning provides good opportunity to model the\ncomplexity of portfolio choice in high-dimensional and data-driven environment\nby leveraging the powerful representation of deep neural networks. In this\npaper, we build a portfolio management system using direct deep reinforcement\nlearning to make optimal portfolio choice periodically among S\\&P500 underlying\nstocks by learning a good factor representation (as input). The result shows\nthat an effective learning of market conditions and optimal portfolio\nallocations can significantly outperform the average market.",
          "link": "http://arxiv.org/abs/2108.01758",
          "publishedOn": "2021-08-05T01:56:20.546Z",
          "wordCount": 554,
          "title": "Factor Representation and Decision Making in Stock Markets Using Deep Reinforcement Learning. (arXiv:2108.01758v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+De_S/0/1/0/all/0/1\">Sourav De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hoang-Hiep Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_B/0/1/0/all/0/1\">Bo-Han Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baig_M/0/1/0/all/0/1\">Md. Aftab Baig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_P/0/1/0/all/0/1\">Po-Jung Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chung Jun Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yao-Jen Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Darsen D. Lu</a>",
          "description": "A synergistic approach for optimizing devices, circuits, and neural network\narchitectures was used to abate junction-temperature-change-induced performance\ndegradation of a Fe-FinFET-based artificial neural network. We demonstrated\nthat the digital nature of the binarized neural network, with the \"0\" state\nprogrammed deep in the subthreshold and the \"1\" state in strong inversion, is\ncrucial for robust DNN inference. The performance of a purely software-based\nbinary neural network (BNN), with 96.1% accuracy for Modified National\nInstitute of Standards and Technology (MNIST) handwritten digit recognition,\nwas used as a baseline. The Fe-FinFET-based BNN (including device-to-device\nvariation at 300 K) achieved 95.7% inference accuracy on the MNIST dataset.\nAlthough substantial inference accuracy degradation with temperature change was\nobserved in a nonbinary neural network, the BNN with optimized Fe-FinFETs as\nsynaptic devices had excellent resistance to temperature change effects and\nmaintained a minimum inference accuracy of 95.2% within a temperature range of\n-233K to 398K after gate stack and bias optimization. However, reprogramming to\nadjust device conductance was necessary for temperatures higher than 398K.",
          "link": "http://arxiv.org/abs/2103.03111",
          "publishedOn": "2021-08-04T01:59:24.276Z",
          "wordCount": 684,
          "title": "Robust Binary Neural Network Operation from 233 K to 398 K via Gate Stack and Bias Optimization of Ferroelectric FinFET Synapses. (arXiv:2103.03111v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.07873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_M/0/1/0/all/0/1\">Michael F. Zimmer</a>",
          "description": "The purpose of this paper is to improve upon existing variants of gradient\ndescent by solving two problems: (1) removing (or reducing) the plateau that\noccurs while minimizing the cost function, (2) continually adjusting the\nlearning rate to an \"ideal\" value. The approach taken is to approximately solve\nfor the learning rate as a function of a trust metric. When this technique is\nhybridized with momentum, it creates an especially effective gradient descent\nvariant, called NeogradM. It is shown to outperform Adam on several test\nproblems, and can easily reach cost function values that are smaller by a\nfactor of $10^8$, for example.",
          "link": "http://arxiv.org/abs/2010.07873",
          "publishedOn": "2021-08-04T01:59:24.269Z",
          "wordCount": 577,
          "title": "Neograd: Near-Ideal Gradient Descent. (arXiv:2010.07873v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.03581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_Z/0/1/0/all/0/1\">Zheming Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Han Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1\">Noura Al Moubayed</a>",
          "description": "Disruptive technologies provides unparalleled opportunities to contribute to\nthe identifications of many aspects in pervasive healthcare, from the adoption\nof the Internet of Things through to Machine Learning (ML) techniques. As a\npowerful tool, ML has been widely applied in patient-centric healthcare\nsolutions. To further improve the quality of patient care, Electronic Health\nRecords (EHRs) are widely applied in healthcare facilities nowadays. Due to the\ninherent heterogeneity, unbalanced, incompleteness, and high-dimensional nature\nof EHRs, it is a challenging task to employ machine learning algorithms to\nanalyse such EHRs for prediction and diagnostics within the scope of precision\nmedicine. Dimensionality reduction is an efficient data preprocessing technique\nfor the analysis of high dimensional data that reduces the number of features\nwhile improving the performance of the data analysis, e.g. classification. In\nthis paper, we propose an efficient curvature-based feature selection method\nfor supporting more precise diagnosis. The proposed method is a filter-based\nfeature selection method, which directly utilises the Menger Curvature for\nranking all the attributes in the given data set. We evaluate the performance\nof our method against conventional PCA and recent ones including BPCM, GSAM,\nWCNN, BLS II, VIBES, 2L-MJFA, RFGA, and VAF. Our method achieves\nstate-of-the-art performance on four benchmark healthcare data sets including\nCCRFDS, BCCDS, BTDS, and DRDDS with impressive 24.73% and 13.93% improvements\nrespectively on BTDS and CCRFDS, 7.97% improvement on BCCDS, and 3.63%\nimprovement on DRDDS. Our CFS source code is publicly available at\nhttps://github.com/zhemingzuo/CFS.",
          "link": "http://arxiv.org/abs/2101.03581",
          "publishedOn": "2021-08-04T01:59:24.262Z",
          "wordCount": 721,
          "title": "Curvature-based Feature Selection with Application in Classifying Electronic Health Records. (arXiv:2101.03581v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01624",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1\">Rohan Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazi_B/0/1/0/all/0/1\">Badih Ghazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vineet Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Ravi Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manurangsi_P/0/1/0/all/0/1\">Pasin Manurangsi</a>",
          "description": "In this work, we study the large-scale pretraining of BERT-Large with\ndifferentially private SGD (DP-SGD). We show that combined with a careful\nimplementation, scaling up the batch size to millions (i.e., mega-batches)\nimproves the utility of the DP-SGD step for BERT; we also enhance its\nefficiency by using an increasing batch size schedule. Our implementation\nbuilds on the recent work of [SVK20], who demonstrated that the overhead of a\nDP-SGD step is minimized with effective use of JAX [BFH+18, FJL18] primitives\nin conjunction with the XLA compiler [XLA17]. Our implementation achieves a\nmasked language model accuracy of 60.5% at a batch size of 2M, for $\\epsilon =\n5.36$. To put this number in perspective, non-private BERT models achieve an\naccuracy of $\\sim$70%.",
          "link": "http://arxiv.org/abs/2108.01624",
          "publishedOn": "2021-08-04T01:59:24.255Z",
          "wordCount": 561,
          "title": "Large-Scale Differentially Private BERT. (arXiv:2108.01624v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.05127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1\">Lingyang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>",
          "description": "Model complexity is a fundamental problem in deep learning. In this paper we\nconduct a systematic overview of the latest studies on model complexity in deep\nlearning. Model complexity of deep learning can be categorized into expressive\ncapacity and effective model complexity. We review the existing studies on\nthose two categories along four important factors, including model framework,\nmodel size, optimization process and data complexity. We also discuss the\napplications of deep learning model complexity including understanding model\ngeneralization, model optimization, and model selection and design. We conclude\nby proposing several interesting future directions.",
          "link": "http://arxiv.org/abs/2103.05127",
          "publishedOn": "2021-08-04T01:59:24.249Z",
          "wordCount": 557,
          "title": "Model Complexity of Deep Learning: A Survey. (arXiv:2103.05127v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01538",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kohn_K/0/1/0/all/0/1\">Kathl&#xe9;n Kohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merkh_T/0/1/0/all/0/1\">Thomas Merkh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montufar_G/0/1/0/all/0/1\">Guido Mont&#xfa;far</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trager_M/0/1/0/all/0/1\">Matthew Trager</a>",
          "description": "We study the family of functions that are represented by a linear\nconvolutional neural network (LCN). These functions form a semi-algebraic\nsubset of the set of linear maps from input space to output space. In contrast,\nthe families of functions represented by fully-connected linear networks form\nalgebraic sets. We observe that the functions represented by LCNs can be\nidentified with polynomials that admit certain factorizations, and we use this\nperspective to describe the impact of the network's architecture on the\ngeometry of the resulting function space. We further study the optimization of\nan objective function over an LCN, analyzing critical points in function space\nand in parameter space, and describing dynamical invariants for gradient\ndescent. Overall, our theory predicts that the optimized parameters of an LCN\nwill often correspond to repeated filters across layers, or filters that can be\ndecomposed as repeated filters. We also conduct numerical and symbolic\nexperiments that illustrate our results and present an in-depth analysis of the\nlandscape for small architectures.",
          "link": "http://arxiv.org/abs/2108.01538",
          "publishedOn": "2021-08-04T01:59:24.231Z",
          "wordCount": 609,
          "title": "Geometry of Linear Convolutional Networks. (arXiv:2108.01538v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yangtao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1\">Boyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>",
          "description": "This paper proposes a new deep learning approach to antipodal grasp\ndetection, named Double-Dot Network (DD-Net). It follows the recent anchor-free\nobject detection framework, which does not depend on empirically pre-set\nanchors and thus allows more generalized and flexible prediction on unseen\nobjects. Specifically, unlike the widely used 5-dimensional rectangle, the\ngripper configuration is defined as a pair of fingertips. An effective CNN\narchitecture is introduced to localize such fingertips, and with the help of\nauxiliary centers for refinement, it accurately and robustly infers grasp\ncandidates. Additionally, we design a specialized loss function to measure the\nquality of grasps, and in contrast to the IoU scores of bounding boxes adopted\nin object detection, it is more consistent to the grasp detection task. Both\nthe simulation and robotic experiments are executed and state of the art\naccuracies are achieved, showing that DD-Net is superior to the counterparts in\nhandling unseen objects.",
          "link": "http://arxiv.org/abs/2108.01527",
          "publishedOn": "2021-08-04T01:59:24.220Z",
          "wordCount": 592,
          "title": "Double-Dot Network for Antipodal Grasp Detection. (arXiv:2108.01527v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1\">Aravind Srinivas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_N/0/1/0/all/0/1\">Niki Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1\">Jonathon Shlens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1\">Ashish Vaswani</a>",
          "description": "We present BoTNet, a conceptually simple yet powerful backbone architecture\nthat incorporates self-attention for multiple computer vision tasks including\nimage classification, object detection and instance segmentation. By just\nreplacing the spatial convolutions with global self-attention in the final\nthree bottleneck blocks of a ResNet and no other changes, our approach improves\nupon the baselines significantly on instance segmentation and object detection\nwhile also reducing the parameters, with minimal overhead in latency. Through\nthe design of BoTNet, we also point out how ResNet bottleneck blocks with\nself-attention can be viewed as Transformer blocks. Without any bells and\nwhistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance\nSegmentation benchmark using the Mask R-CNN framework; surpassing the previous\nbest published single model and single scale results of ResNeSt evaluated on\nthe COCO validation set. Finally, we present a simple adaptation of the BoTNet\ndesign for image classification, resulting in models that achieve a strong\nperformance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to\n1.64x faster in compute time than the popular EfficientNet models on TPU-v3\nhardware. We hope our simple and effective approach will serve as a strong\nbaseline for future research in self-attention models for vision",
          "link": "http://arxiv.org/abs/2101.11605",
          "publishedOn": "2021-08-04T01:59:24.212Z",
          "wordCount": 685,
          "title": "Bottleneck Transformers for Visual Recognition. (arXiv:2101.11605v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stelzer_F/0/1/0/all/0/1\">Florian Stelzer</a> (1, 2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Yanchuk_S/0/1/0/all/0/1\">Serhiy Yanchuk</a> (1) ((1) Institute of Mathematics, Technische Universit&#xe4;t Berlin, Germany, (2) Department of Mathematics, Humboldt-Universit&#xe4;t zu Berlin, Germany, (3) Institute of Computer Science, University of Tartu, Estonia)",
          "description": "The method recently introduced in arXiv:2011.10115 realizes a deep neural\nnetwork with just a single nonlinear element and delayed feedback. It is\napplicable for the description of physically implemented neural networks. In\nthis work, we present an infinite-dimensional generalization, which allows for\na more rigorous mathematical analysis and a higher flexibility in choosing the\nweight functions. Precisely speaking, the weights are described by Lebesgue\nintegrable functions instead of step functions. We also provide a functional\nback-propagation algorithm, which enables gradient descent training of the\nweights. In addition, with a slight modification, our concept realizes\nrecurrent neural networks.",
          "link": "http://arxiv.org/abs/2101.02966",
          "publishedOn": "2021-08-04T01:59:24.197Z",
          "wordCount": 668,
          "title": "Infinite-dimensional Folded-in-time Deep Neural Networks. (arXiv:2101.02966v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suzumura_T/0/1/0/all/0/1\">Toyotaro Suzumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Gasulla_D/0/1/0/all/0/1\">Dario Garcia-Gasulla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Napagao_S/0/1/0/all/0/1\">Sergio Alvarez Napagao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maruyama_H/0/1/0/all/0/1\">Hiroshi Maruyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanezashi_H/0/1/0/all/0/1\">Hiroki Kanezashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Arnal_R/0/1/0/all/0/1\">Raquel P&#x27;erez-Arnal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyoshi_K/0/1/0/all/0/1\">Kunihiko Miyoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1\">Euma Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_K/0/1/0/all/0/1\">Keita Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiba_S/0/1/0/all/0/1\">Sayaka Shiba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurokawa_M/0/1/0/all/0/1\">Mariko Kurokawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanzawa_Y/0/1/0/all/0/1\">Yuta Kanzawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakagawa_N/0/1/0/all/0/1\">Naomi Nakagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanai_M/0/1/0/all/0/1\">Masatoshi Hanai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianxiao Li</a>",
          "description": "This paper aims at providing the summary of the Global Data Science Project\n(GDSC) for COVID-19. as on May 31 2020. COVID-19 has largely impacted on our\nsocieties through both direct and indirect effects transmitted by the policy\nmeasures to counter the spread of viruses. We quantitatively analysed the\nmultifaceted impacts of the COVID-19 pandemic on our societies including\npeople's mobility, health, and social behaviour changes. People's mobility has\nchanged significantly due to the implementation of travel restriction and\nquarantine measurements. Indeed, the physical distance has widened at\ninternational (cross-border), national and regional level. At international\nlevel, due to the travel restrictions, the number of international flights has\nplunged overall at around 88 percent during March. In particular, the number of\nflights connecting Europe dropped drastically in mid of March after the United\nStates announced travel restrictions to Europe and the EU and participating\ncountries agreed to close borders, at 84 percent decline compared to March\n10th. Similarly, we examined the impacts of quarantine measures in the major\ncity: Tokyo (Japan), New York City (the United States), and Barcelona (Spain).\nWithin all three cities, we found the significant decline in traffic volume. We\nalso identified the increased concern for mental health through the analysis of\nposts on social networking services such as Twitter and Instagram. Notably, in\nthe beginning of April 2020, the number of post with #depression on Instagram\ndoubled, which might reflect the rise in mental health awareness among\nInstagram users. Besides, we identified the changes in a wide range of people's\nsocial behaviors, as well as economic impacts through the analysis of Instagram\ndata and primary survey data.",
          "link": "http://arxiv.org/abs/2006.05573",
          "publishedOn": "2021-08-04T01:59:24.174Z",
          "wordCount": 818,
          "title": "Global Data Science Project for COVID-19. (arXiv:2006.05573v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.05906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Lianhui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>",
          "description": "Abductive and counterfactual reasoning, core abilities of everyday human\ncognition, require reasoning about what might have happened at time t, while\nconditioning on multiple contexts from the relative past and future. However,\nsimultaneous incorporation of past and future contexts using generative\nlanguage models (LMs) can be challenging, as they are trained either to\ncondition only on the past context or to perform narrowly scoped\ntext-infilling. In this paper, we propose DeLorean, a new unsupervised decoding\nalgorithm that can flexibly incorporate both the past and future contexts using\nonly off-the-shelf, left-to-right language models and no supervision. The key\nintuition of our algorithm is incorporating the future through\nback-propagation, during which, we only update the internal representation of\nthe output while fixing the model parameters. By alternating between forward\nand backward propagation, DeLorean can decode the output representation that\nreflects both the left and right contexts. We demonstrate that our approach is\ngeneral and applicable to two nonmonotonic reasoning tasks: abductive text\ngeneration and counterfactual story revision, where DeLorean outperforms a\nrange of unsupervised and some supervised methods, based on automatic and human\nevaluation.",
          "link": "http://arxiv.org/abs/2010.05906",
          "publishedOn": "2021-08-04T01:59:24.167Z",
          "wordCount": 695,
          "title": "Back to the Future: Unsupervised Backprop-based Decoding for Counterfactual and Abductive Commonsense Reasoning. (arXiv:2010.05906v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02383",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bonassi_F/0/1/0/all/0/1\">Fabio Bonassi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Silva_C/0/1/0/all/0/1\">Caio Fabio Oliveira da Silva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scattolini_R/0/1/0/all/0/1\">Riccardo Scattolini</a>",
          "description": "The use of Recurrent Neural Networks (RNNs) for system identification has\nrecently gathered increasing attention, thanks to their black-box modeling\ncapabilities.Albeit RNNs have been fruitfully adopted in many applications,\nonly few works are devoted to provide rigorous theoretical foundations that\njustify their use for control purposes. The aim of this paper is to describe\nhow stable Gated Recurrent Units (GRUs), a particular RNN architecture, can be\ntrained and employed in a Nonlinear MPC framework to perform offset-free\ntracking of constant references with guaranteed closed-loop stability. The\nproposed approach is tested on a pH neutralization process benchmark, showing\nremarkable performances.",
          "link": "http://arxiv.org/abs/2103.02383",
          "publishedOn": "2021-08-04T01:59:24.032Z",
          "wordCount": 609,
          "title": "Nonlinear MPC for Offset-Free Tracking of systems learned by GRU Neural Networks. (arXiv:2103.02383v3 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01660",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingxing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenrui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Junni Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hongkai Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>",
          "description": "Spectral graph convolutional networks (SGCNs) have been attracting increasing\nattention in graph representation learning partly due to their interpretability\nthrough the prism of the established graph signal processing framework.\nHowever, existing SGCNs are limited in implementing graph convolutions with\nrigid transforms that could not adapt to signals residing on graphs and tasks\nat hand. In this paper, we propose a novel class of spectral graph\nconvolutional networks that implement graph convolutions with adaptive graph\nwavelets. Specifically, the adaptive graph wavelets are learned with neural\nnetwork-parameterized lifting structures, where structure-aware attention-based\nlifting operations are developed to jointly consider graph structures and node\nfeatures. We propose to lift based on diffusion wavelets to alleviate the\nstructural information loss induced by partitioning non-bipartite graphs. By\ndesign, the locality and sparsity of the resulting wavelet transform as well as\nthe scalability of the lifting structure for large and varying-size graphs are\nguaranteed. We further derive a soft-thresholding filtering operation by\nlearning sparse graph representations in terms of the learned wavelets, which\nimproves the scalability and interpretablity, and yield a localized, efficient\nand scalable spectral graph convolution. To ensure that the learned graph\nrepresentations are invariant to node permutations, a layer is employed at the\ninput of the networks to reorder the nodes according to their local topology\ninformation. We evaluate the proposed networks in both node-level and\ngraph-level representation learning tasks on benchmark citation and\nbioinformatics graph datasets. Extensive experiments demonstrate the\nsuperiority of the proposed networks over existing SGCNs in terms of accuracy,\nefficiency and scalability.",
          "link": "http://arxiv.org/abs/2108.01660",
          "publishedOn": "2021-08-04T01:59:24.014Z",
          "wordCount": 691,
          "title": "Spectral Graph Convolutional Networks WithLifting-based Adaptive Graph Wavelets. (arXiv:2108.01660v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoyue Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "We study whether and how can we model a joint distribution $p(x,z)$ using two\nconditional models $p(x|z)$ and $q(z|x)$ that form a cycle. This is motivated\nby the observation that deep generative models, in addition to a likelihood\nmodel $p(x|z)$, often also use an inference model $q(z|x)$ for data\nrepresentation, but they rely on a usually uninformative prior distribution\n$p(z)$ to define a joint distribution, which may render problems like posterior\ncollapse and manifold mismatch. To explore the possibility to model a joint\ndistribution using only $p(x|z)$ and $q(z|x)$, we study their compatibility and\ndeterminacy, corresponding to the existence and uniqueness of a joint\ndistribution whose conditional distributions coincide with them. We develop a\ngeneral theory for novel and operable equivalence criteria for compatibility,\nand sufficient conditions for determinacy. Based on the theory, we propose the\nCyGen framework for cyclic-conditional generative modeling, including methods\nto enforce compatibility and use the determined distribution to fit and\ngenerate data. With the prior constraint removed, CyGen better fits data and\ncaptures more representative features, supported by experiments showing better\ngeneration and downstream classification performance.",
          "link": "http://arxiv.org/abs/2106.15962",
          "publishedOn": "2021-08-04T01:59:23.979Z",
          "wordCount": 661,
          "title": "On the Generative Utility of Cyclic Conditionals. (arXiv:2106.15962v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.05337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Da Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinjing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1\">Qidong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Q/0/1/0/all/0/1\">Quan Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>",
          "description": "Graph neural networks (GNN) have shown great success in learning from\ngraph-structured data. They are widely used in various applications, such as\nrecommendation, fraud detection, and search. In these domains, the graphs are\ntypically large, containing hundreds of millions of nodes and several billions\nof edges. To tackle this challenge, we develop DistDGL, a system for training\nGNNs in a mini-batch fashion on a cluster of machines. DistDGL is based on the\nDeep Graph Library (DGL), a popular GNN development framework. DistDGL\ndistributes the graph and its associated data (initial features and embeddings)\nacross the machines and uses this distribution to derive a computational\ndecomposition by following an owner-compute rule. DistDGL follows a synchronous\ntraining approach and allows ego-networks forming the mini-batches to include\nnon-local nodes. To minimize the overheads associated with distributed\ncomputations, DistDGL uses a high-quality and light-weight min-cut graph\npartitioning algorithm along with multiple balancing constraints. This allows\nit to reduce communication overheads and statically balance the computations.\nIt further reduces the communication by replicating halo nodes and by using\nsparse embedding updates. The combination of these design choices allows\nDistDGL to train high-quality models while achieving high parallel efficiency\nand memory scalability. We demonstrate our optimizations on both inductive and\ntransductive GNN models. Our results show that DistDGL achieves linear speedup\nwithout compromising model accuracy and requires only 13 seconds to complete a\ntraining epoch for a graph with 100 million nodes and 3 billion edges on a\ncluster with 16 machines. DistDGL is now publicly available as part of\nDGL:https://github.com/dmlc/dgl/tree/master/python/dgl/distributed.",
          "link": "http://arxiv.org/abs/2010.05337",
          "publishedOn": "2021-08-04T01:59:23.965Z",
          "wordCount": 746,
          "title": "DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs. (arXiv:2010.05337v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rawat_A/0/1/0/all/0/1\">Ambrish Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levacher_K/0/1/0/all/0/1\">Killian Levacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinn_M/0/1/0/all/0/1\">Mathieu Sinn</a>",
          "description": "Deep Generative Models (DGMs) allow users to synthesize data from complex,\nhigh-dimensional manifolds. Industry applications of DGMs include data\naugmentation to boost performance of (semi-)supervised machine learning, or to\nmitigate fairness or privacy concerns. Large-scale DGMs are notoriously hard to\ntrain, requiring expert skills, large amounts of data and extensive\ncomputational resources. Thus, it can be expected that many enterprises will\nresort to sourcing pre-trained DGMs from potentially unverified third parties,\ne.g.~open source model repositories.\n\nAs we show in this paper, such a deployment scenario poses a new attack\nsurface, which allows adversaries to potentially undermine the integrity of\nentire machine learning development pipelines in a victim organization.\nSpecifically, we describe novel training-time attacks resulting in corrupted\nDGMs that synthesize regular data under normal operations and designated target\noutputs for inputs sampled from a trigger distribution. Depending on the\ncontrol that the adversary has over the random number generation, this imposes\nvarious degrees of risk that harmful data may enter the machine learning\ndevelopment pipelines, potentially causing material or reputational damage to\nthe victim organization.\n\nOur attacks are based on adversarial loss functions that combine the dual\nobjectives of attack stealth and fidelity. We show its effectiveness for a\nvariety of DGM architectures (Generative Adversarial Networks (GANs),\nVariational Autoencoders (VAEs)) and data domains (images, audio). Our\nexperiments show that - even for large-scale industry-grade DGMs - our attack\ncan be mounted with only modest computational efforts. We also investigate the\neffectiveness of different defensive approaches (based on static/dynamic model\nand output inspections) and prescribe a practical defense strategy that paves\nthe way for safe usage of DGMs.",
          "link": "http://arxiv.org/abs/2108.01644",
          "publishedOn": "2021-08-04T01:59:23.956Z",
          "wordCount": 719,
          "title": "The Devil is in the GAN: Defending Deep Generative Models Against Backdoor Attacks. (arXiv:2108.01644v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Csaba_B/0/1/0/all/0/1\">Botos Csaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhry_A/0/1/0/all/0/1\">Arslan Chaudhry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1\">Puneet Dokania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>",
          "description": "Domain shift is a well known problem where a model trained on a particular\ndomain (source) does not perform well when exposed to samples from a different\ndomain (target). Unsupervised methods that can adapt to domain shift are highly\ndesirable as they allow effective utilization of the source data without\nrequiring additional annotated training data from the target. Practically,\nobtaining sufficient amount of annotated data from the target domain can be\nboth infeasible and extremely expensive. In this work, we address the domain\nshift problem for the object detection task. Our approach relies on gradually\nremoving the domain shift between the source and the target domains. The key\ningredients to our approach are -- (a) mapping the source to the target domain\non pixel-level; (b) training a teacher network on the mapped source and the\nunannotated target domain using adversarial feature alignment; and (c) finally\ntraining a student network using the pseudo-labels obtained from the teacher.\nExperimentally, when tested on challenging scenarios involving domain shift, we\nconsistently obtain significantly large performance gains over various recent\nstate of the art approaches.",
          "link": "http://arxiv.org/abs/2108.00977",
          "publishedOn": "2021-08-04T01:59:23.943Z",
          "wordCount": 636,
          "title": "Multilevel Knowledge Transfer for Cross-Domain Object Detection. (arXiv:2108.00977v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.08536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narita_Y/0/1/0/all/0/1\">Yusuke Narita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasui_S/0/1/0/all/0/1\">Shota Yasui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yata_K/0/1/0/all/0/1\">Kohei Yata</a>",
          "description": "Efficient methods to evaluate new algorithms are critical for improving\ninteractive bandit and reinforcement learning systems such as recommendation\nsystems. A/B tests are reliable, but are time- and money-consuming, and entail\na risk of failure. In this paper, we develop an alternative method, which\npredicts the performance of algorithms given historical data that may have been\ngenerated by a different algorithm. Our estimator has the property that its\nprediction converges in probability to the true performance of a counterfactual\nalgorithm at a rate of $\\sqrt{N}$, as the sample size $N$ increases. We also\nshow a correct way to estimate the variance of our prediction, thus allowing\nthe analyst to quantify the uncertainty in the prediction. These properties\nhold even when the analyst does not know which among a large number of\npotentially important state variables are actually important. We validate our\nmethod by a simulation experiment about reinforcement learning. We finally\napply it to improve advertisement design by a major advertisement company. We\nfind that our method produces smaller mean squared errors than state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2002.08536",
          "publishedOn": "2021-08-04T01:59:23.938Z",
          "wordCount": 659,
          "title": "Debiased Off-Policy Evaluation for Recommendation Systems. (arXiv:2002.08536v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01312",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Kato_M/0/1/0/all/0/1\">Masahiro Kato</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Kakehi_H/0/1/0/all/0/1\">Haruo Kakehi</a>, <a href=\"http://arxiv.org/find/econ/1/au:+McAlinn_K/0/1/0/all/0/1\">Kenichiro McAlinn</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Yasui_S/0/1/0/all/0/1\">Shota Yasui</a>",
          "description": "We consider learning causal relationships under conditional moment\nconditions. Unlike causal inference under unconditional moment conditions,\nconditional moment conditions pose serious challenges for causal inference,\nespecially in complex, high-dimensional settings. To address this issue, we\npropose a method that transforms conditional moment conditions to unconditional\nmoment conditions through importance weighting using the conditional density\nratio. Then, using this transformation, we propose a method that successfully\napproximates conditional moment conditions. Our proposed approach allows us to\nemploy methods for estimating causal parameters from unconditional moment\nconditions, such as generalized method of moments, adequately in a\nstraightforward manner. In experiments, we confirm that our proposed method\nperforms well compared to existing methods.",
          "link": "http://arxiv.org/abs/2108.01312",
          "publishedOn": "2021-08-04T01:59:23.911Z",
          "wordCount": 557,
          "title": "Learning Causal Relationships from Conditional Moment Conditions by Importance Weighting. (arXiv:2108.01312v1 [econ.EM])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08970",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Carbajal_G/0/1/0/all/0/1\">Guillaume Carbajal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Richter_J/0/1/0/all/0/1\">Julius Richter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gerkmann_T/0/1/0/all/0/1\">Timo Gerkmann</a>",
          "description": "Recently, the standard variational autoencoder has been successfully used to\nlearn a probabilistic prior over speech signals, which is then used to perform\nspeech enhancement. Variational autoencoders have then been conditioned on a\nlabel describing a high-level speech attribute (e.g. speech activity) that\nallows for a more explicit control of speech generation. However, the label is\nnot guaranteed to be disentangled from the other latent variables, which\nresults in limited performance improvements compared to the standard\nvariational autoencoder. In this work, we propose to use an adversarial\ntraining scheme for variational autoencoders to disentangle the label from the\nother latent variables. At training, we use a discriminator that competes with\nthe encoder of the variational autoencoder. Simultaneously, we also use an\nadditional encoder that estimates the label for the decoder of the variational\nautoencoder, which proves to be crucial to learn disentanglement. We show the\nbenefit of the proposed disentanglement learning when a voice activity label,\nestimated from visual data, is used for speech enhancement.",
          "link": "http://arxiv.org/abs/2105.08970",
          "publishedOn": "2021-08-04T01:59:23.905Z",
          "wordCount": 652,
          "title": "Disentanglement Learning for Variational Autoencoders Applied to Audio-Visual Speech Enhancement. (arXiv:2105.08970v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Q/0/1/0/all/0/1\">Qifan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weigong Zhang</a>",
          "description": "Trajectory prediction plays a pivotal role in the field of intelligent\nvehicles. It currently suffers from several challenges,e.g., accumulative error\nin rollout process and weak adaptability in various scenarios. This paper\nproposes a parametric-learning recursive least squares (RLS) estimation based\non deep neural network for trajectory prediction. We design a flexible plug-in\nmodule which can be readily implanted into rollout approaches. Goal points are\nproposed to capture the long-term prediction stability from the global\nperspective. We carried experiments out on the NGSIM dataset. The promising\nresults indicate that our method could improve rollout trajectory prediction\nmethods effectively.",
          "link": "http://arxiv.org/abs/2102.10859",
          "publishedOn": "2021-08-04T01:59:23.899Z",
          "wordCount": 557,
          "title": "Recursive Least Squares Based Refinement Network for the Rollout Trajectory Prediction Methods. (arXiv:2102.10859v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.",
          "link": "http://arxiv.org/abs/2107.12651",
          "publishedOn": "2021-08-04T01:59:23.893Z",
          "wordCount": 633,
          "title": "Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ball_P/0/1/0/all/0/1\">Philip J. Ball</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1\">Jack Parker-Holder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>",
          "description": "Reinforcement learning from large-scale offline datasets provides us with the\nability to learn policies without potentially unsafe or impractical\nexploration. Significant progress has been made in the past few years in\ndealing with the challenge of correcting for differing behavior between the\ndata collection and learned policies. However, little attention has been paid\nto potentially changing dynamics when transferring a policy to the online\nsetting, where performance can be up to 90% reduced for existing methods. In\nthis paper we address this problem with Augmented World Models (AugWM). We\naugment a learned dynamics model with simple transformations that seek to\ncapture potential changes in physical properties of the robot, leading to more\nrobust policies. We not only train our policy in this new setting, but also\nprovide it with the sampled augmentation as a context, allowing it to adapt to\nchanges in the environment. At test time we learn the context in a\nself-supervised fashion by approximating the augmentation which corresponds to\nthe new environment. We rigorously evaluate our approach on over 100 different\nchanged dynamics settings, and show that this simple approach can significantly\nimprove the zero-shot generalization of a recent state-of-the-art baseline,\noften achieving successful policies where the baseline fails.",
          "link": "http://arxiv.org/abs/2104.05632",
          "publishedOn": "2021-08-04T01:59:23.873Z",
          "wordCount": 697,
          "title": "Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single Offline Environment. (arXiv:2104.05632v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arnold_S/0/1/0/all/0/1\">S&#xe9;bastien M. R. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhillon_G/0/1/0/all/0/1\">Guneet S. Dhillon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "Episodic training is a core ingredient of few-shot learning to train models\non tasks with limited labelled data. Despite its success, episodic training\nremains largely understudied, prompting us to ask the question: what is the\nbest way to sample episodes? In this paper, we first propose a method to\napproximate episode sampling distributions based on their difficulty. Building\non this method, we perform an extensive analysis and find that sampling\nuniformly over episode difficulty outperforms other sampling schemes, including\ncurriculum and easy-/hard-mining. As the proposed sampling method is algorithm\nagnostic, we can leverage these insights to improve few-shot learning\naccuracies across many episodic training algorithms. We demonstrate the\nefficacy of our method across popular few-shot learning datasets, algorithms,\nnetwork architectures, and protocols.",
          "link": "http://arxiv.org/abs/2108.01662",
          "publishedOn": "2021-08-04T01:59:23.852Z",
          "wordCount": 562,
          "title": "Uniform Sampling over Episode Difficulty. (arXiv:2108.01662v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.04853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shaojie Tang</a>",
          "description": "In this paper, we study the non-monotone adaptive submodular maximization\nproblem subject to a knapsack and a $k$-system constraints. The input of our\nproblem is a set of items, where each item has a particular state drawn from a\nknown prior distribution. However, the state of an item is initially unknown,\none must select an item in order to reveal the state of that item. There is a\nutility function which is defined over items and states. Our objective is to\nsequentially select a group of items to maximize the expected utility. Although\nthe cardinality-constrained non-monotone adaptive submodular maximization has\nbeen well studied in the literature, whether there exists a constant\napproximation solution for the knapsack-constrained or $k$-system constrained\nadaptive submodular maximization problem remains an open problem. It fact, it\nhas only been settled given the additional assumption of pointwise\nsubmodularity. In this paper, we remove the common assumption on pointwise\nsubmodularity and propose the first constant approximation solutions for both\ncases. Inspired by two recent studies on non-monotone adaptive submodular\nmaximization, we develop a sampling-based randomized algorithm that achieves a\n$\\frac{1}{10}$ approximation for the case of a knapsack constraint and that\nachieves a $\\frac{1}{2k+4}$ approximation ratio for the case of a $k$-system\nconstraint.",
          "link": "http://arxiv.org/abs/2104.04853",
          "publishedOn": "2021-08-04T01:59:23.830Z",
          "wordCount": 686,
          "title": "Beyond Pointwise Submodularity: Non-Monotone Adaptive Submodular Maximization subject to Knapsack and $k$-System Constraints. (arXiv:2104.04853v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.07054",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yu-Huan Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_S/0/1/0/all/0/1\">Shang-Hua Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1\">Rong-Guo Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>",
          "description": "Recently, the coronavirus disease 2019 (COVID-19) has caused a pandemic\ndisease in over 200 countries, influencing billions of humans. To control the\ninfection, identifying and separating the infected people is the most crucial\nstep. The main diagnostic tool is the Reverse Transcription Polymerase Chain\nReaction (RT-PCR) test. Still, the sensitivity of the RT-PCR test is not high\nenough to effectively prevent the pandemic. The chest CT scan test provides a\nvaluable complementary tool to the RT-PCR test, and it can identify the\npatients in the early-stage with high sensitivity. However, the chest CT scan\ntest is usually time-consuming, requiring about 21.5 minutes per case. This\npaper develops a novel Joint Classification and Segmentation (JCS) system to\nperform real-time and explainable COVID-19 chest CT diagnosis. To train our JCS\nsystem, we construct a large scale COVID-19 Classification and Segmentation\n(COVID-CS) dataset, with 144,167 chest CT images of 400 COVID-19 patients and\n350 uninfected cases. 3,855 chest CT images of 200 patients are annotated with\nfine-grained pixel-level labels of opacifications, which are increased\nattenuation of the lung parenchyma. We also have annotated lesion counts,\nopacification areas, and locations and thus benefit various diagnosis aspects.\nExtensive experiments demonstrate that the proposed JCS diagnosis system is\nvery efficient for COVID-19 classification and segmentation. It obtains an\naverage sensitivity of 95.0% and a specificity of 93.0% on the classification\ntest set, and 78.5% Dice score on the segmentation test set of our COVID-CS\ndataset. The COVID-CS dataset and code are available at\nhttps://github.com/yuhuan-wu/JCS.",
          "link": "http://arxiv.org/abs/2004.07054",
          "publishedOn": "2021-08-04T01:59:23.823Z",
          "wordCount": 802,
          "title": "JCS: An Explainable COVID-19 Diagnosis System by Joint Classification and Segmentation. (arXiv:2004.07054v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.04870",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Fan_J/0/1/0/all/0/1\">Jianqing Fan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Guo_Y/0/1/0/all/0/1\">Yongyi Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_K/0/1/0/all/0/1\">Kaizheng Wang</a>",
          "description": "When the data are stored in a distributed manner, direct application of\ntraditional statistical inference procedures is often prohibitive due to\ncommunication cost and privacy concerns. This paper develops and investigates\ntwo Communication-Efficient Accurate Statistical Estimators (CEASE),\nimplemented through iterative algorithms for distributed optimization. In each\niteration, node machines carry out computation in parallel and communicate with\nthe central processor, which then broadcasts aggregated information to node\nmachines for new updates. The algorithms adapt to the similarity among loss\nfunctions on node machines, and converge rapidly when each node machine has\nlarge enough sample size. Moreover, they do not require good initialization and\nenjoy linear converge guarantees under general conditions. The contraction rate\nof optimization errors is presented explicitly, with dependence on the local\nsample size unveiled. In addition, the improved statistical accuracy per\niteration is derived. By regarding the proposed method as a multi-step\nstatistical estimator, we show that statistical efficiency can be achieved in\nfinite steps in typical statistical applications. In addition, we give the\nconditions under which the one-step CEASE estimator is statistically efficient.\nExtensive numerical experiments on both synthetic and real data validate the\ntheoretical results and demonstrate the superior performance of our algorithms.",
          "link": "http://arxiv.org/abs/1906.04870",
          "publishedOn": "2021-08-04T01:59:23.817Z",
          "wordCount": 648,
          "title": "Communication-Efficient Accurate Statistical Estimation. (arXiv:1906.04870v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.03097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Ngoc Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dang H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duy Nguyen</a>",
          "description": "Variational Bayes (VB) has become a widely-used tool for Bayesian inference\nin statistics and machine learning. Nonetheless, the development of the\nexisting VB algorithms is so far generally restricted to the case where the\nvariational parameter space is Euclidean, which hinders the potential broad\napplication of VB methods. This paper extends the scope of VB to the case where\nthe variational parameter space is a Riemannian manifold. We develop an\nefficient manifold-based VB algorithm that exploits both the geometric\nstructure of the constraint parameter space and the information geometry of the\nmanifold of VB approximating probability distributions. Our algorithm is\nprovably convergent and achieves a convergence rate of order $\\mathcal\nO(1/\\sqrt{T})$ and $\\mathcal O(1/T^{2-2\\epsilon})$ for a non-convex evidence\nlower bound function and a strongly retraction-convex evidence lower bound\nfunction, respectively. We develop in particular two manifold VB algorithms,\nManifold Gaussian VB and Manifold Neural Net VB, and demonstrate through\nnumerical experiments that the proposed algorithms are stable, less sensitive\nto initialization and compares favourably to existing VB methods.",
          "link": "http://arxiv.org/abs/1908.03097",
          "publishedOn": "2021-08-04T01:59:23.797Z",
          "wordCount": 638,
          "title": "Variational Bayes on Manifolds. (arXiv:1908.03097v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mansilla_L/0/1/0/all/0/1\">Lucas Mansilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Echeveste_R/0/1/0/all/0/1\">Rodrigo Echeveste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milone_D/0/1/0/all/0/1\">Diego H. Milone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrante_E/0/1/0/all/0/1\">Enzo Ferrante</a>",
          "description": "In real-life applications, machine learning models often face scenarios where\nthere is a change in data distribution between training and test domains. When\nthe aim is to make predictions on distributions different from those seen at\ntraining, we incur in a domain generalization problem. Methods to address this\nissue learn a model using data from multiple source domains, and then apply\nthis model to the unseen target domain. Our hypothesis is that when training\nwith multiple domains, conflicting gradients within each mini-batch contain\ninformation specific to the individual domains which is irrelevant to the\nothers, including the test domain. If left untouched, such disagreement may\ndegrade generalization performance. In this work, we characterize the\nconflicting gradients emerging in domain shift scenarios and devise novel\ngradient agreement strategies based on gradient surgery to alleviate their\neffect. We validate our approach in image classification tasks with three\nmulti-domain datasets, showing the value of the proposed agreement strategy in\nenhancing the generalization capability of deep learning models in domain shift\nscenarios.",
          "link": "http://arxiv.org/abs/2108.01621",
          "publishedOn": "2021-08-04T01:59:23.787Z",
          "wordCount": 614,
          "title": "Domain Generalization via Gradient Surgery. (arXiv:2108.01621v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.09381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lust_J/0/1/0/all/0/1\">Julia Lust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1\">Alexandru Paul Condurache</a>",
          "description": "Deep Neural Networks (DNNs) achieve state-of-the-art performance on numerous\napplications. However, it is difficult to tell beforehand if a DNN receiving an\ninput will deliver the correct output since their decision criteria are usually\nnontransparent. A DNN delivers the correct output if the input is within the\narea enclosed by its generalization envelope. In this case, the information\ncontained in the input sample is processed reasonably by the network. It is of\nlarge practical importance to assess at inference time if a DNN generalizes\ncorrectly. Currently, the approaches to achieve this goal are investigated in\ndifferent problem set-ups rather independently from one another, leading to\nthree main research and literature fields: predictive uncertainty,\nout-of-distribution detection and adversarial example detection. This survey\nconnects the three fields within the larger framework of investigating the\ngeneralization performance of machine learning methods and in particular DNNs.\nWe underline the common ground, point at the most promising approaches and give\na structured overview of the methods that provide at inference time means to\nestablish if the current input is within the generalization envelope of a DNN.",
          "link": "http://arxiv.org/abs/2008.09381",
          "publishedOn": "2021-08-04T01:59:23.779Z",
          "wordCount": 671,
          "title": "A Survey on Assessing the Generalization Envelope of Deep Neural Networks: Predictive Uncertainty, Out-of-distribution and Adversarial Samples. (arXiv:2008.09381v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.12371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Euler_H/0/1/0/all/0/1\">Hans-Christian Ruiz-Euler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alegre_Ibarra_U/0/1/0/all/0/1\">Unai Alegre-Ibarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ven_B/0/1/0/all/0/1\">Bram van de Ven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broersma_H/0/1/0/all/0/1\">Hajo Broersma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bobbert_P/0/1/0/all/0/1\">Peter A. Bobbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiel_W/0/1/0/all/0/1\">Wilfred G. van der Wiel</a>",
          "description": "The rapidly growing computational demands of deep neural networks require\nnovel hardware designs. Recently, tunable nanoelectronic devices were developed\nbased on hopping electrons through a network of dopant atoms in silicon. These\n\"Dopant Network Processing Units\" (DNPUs) are highly energy-efficient and have\npotentially very high throughput. By adapting the control voltages applied to\nits terminals, a single DNPU can solve a variety of linearly non-separable\nclassification problems. However, using a single device has limitations due to\nthe implicit single-node architecture. This paper presents a promising novel\napproach to neural information processing by introducing DNPUs as high-capacity\nneurons and moving from a single to a multi-neuron framework. By implementing\nand testing a small multi-DNPU classifier in hardware, we show that\nfeed-forward DNPU networks improve the performance of a single DNPU from 77% to\n94% test accuracy on a binary classification task with concentric classes on a\nplane. Furthermore, motivated by the integration of DNPUs with memristor\narrays, we study the potential of using DNPUs in combination with linear\nlayers. We show by simulation that a single-layer MNIST classifier with only 10\nDNPUs achieves over 96% test accuracy. Our results pave the road towards\nhardware neural-network emulators that offer atomic-scale information\nprocessing with low latency and energy consumption.",
          "link": "http://arxiv.org/abs/2007.12371",
          "publishedOn": "2021-08-04T01:59:23.738Z",
          "wordCount": 713,
          "title": "Dopant Network Processing Units: Towards Efficient Neural-network Emulators with High-capacity Nanoelectronic Nodes. (arXiv:2007.12371v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.05079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kour_K/0/1/0/all/0/1\">Kirandeep Kour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolgov_S/0/1/0/all/0/1\">Sergey Dolgov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoll_M/0/1/0/all/0/1\">Martin Stoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benner_P/0/1/0/all/0/1\">Peter Benner</a>",
          "description": "An increasing amount of collected data are high-dimensional multi-way arrays\n(tensors), and it is crucial for efficient learning algorithms to exploit this\ntensorial structure as much as possible. The ever-present curse of\ndimensionality for high dimensional data and the loss of structure when\nvectorizing the data motivates the use of tailored low-rank tensor\nclassification methods. In the presence of small amounts of training data,\nkernel methods offer an attractive choice as they provide the possibility for a\nnonlinear decision boundary. We develop the Tensor Train Multi-way Multi-level\nKernel (TT-MMK), which combines the simplicity of the Canonical Polyadic\ndecomposition, the classification power of the Dual Structure-preserving\nSupport Vector Machine, and the reliability of the Tensor Train (TT)\napproximation. We show by experiments that the TT-MMK method is usually more\nreliable computationally, less sensitive to tuning parameters, and gives higher\nprediction accuracy in the SVM classification when benchmarked against other\nstate-of-the-art techniques.",
          "link": "http://arxiv.org/abs/2002.05079",
          "publishedOn": "2021-08-04T01:59:23.706Z",
          "wordCount": 632,
          "title": "Efficient Structure-preserving Support Tensor Train Machine. (arXiv:2002.05079v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.05768",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>",
          "description": "With the popularity and development of the wearable devices such as\nsmartphones, human activity recognition (HAR) based on sensors has become as a\nkey research area in human computer interaction and ubiquitous computing. The\nemergence of deep learning leads to a recent shift in the research of HAR,\nwhich requires massive strictly labeled data. In comparison with video data,\nactivity data recorded from accelerometer or gyroscope is often more difficult\nto interpret and segment. Recently, several attention mechanisms are proposed\nto handle the weakly labeled human activity data, which do not require accurate\ndata annotation. However, these attention-based models can only handle the\nweakly labeled dataset whose sample includes one target activity, as a result\nit limits efficiency and practicality. In the paper, we propose a recurrent\nattention networks (RAN) to handle sequential weakly labeled multi-activity\nrecognition and location tasks. The model can repeatedly perform steps of\nattention on multiple activities of one sample and each step is corresponding\nto the current focused activity. The effectiveness of the RAN model is\nvalidated on a collected sequential weakly labeled multi-activity dataset and\nthe other two public datasets. The experiment results show that our RAN model\ncan simultaneously infer multi-activity types from the coarse-grained\nsequential weak labels and determine specific locations of every target\nactivity with only knowledge of which types of activities contained in the long\nsequence. It will greatly reduce the burden of manual labeling. The code of our\nwork is available at https://github.com/KennCoder7/RAN.",
          "link": "http://arxiv.org/abs/2004.05768",
          "publishedOn": "2021-08-04T01:59:23.678Z",
          "wordCount": 764,
          "title": "Sequential Weakly Labeled Multi-Activity Localization and Recognition on Wearable Sensors using Recurrent Attention Networks. (arXiv:2004.05768v3 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01625",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jung_W/0/1/0/all/0/1\">Wooseok Jung</a>",
          "description": "Cell image analysis is crucial in Alzheimer's research to detect the presence\nof A$\\beta$ protein inhibiting cell function. Deep learning speeds up the\nprocess by making only low-level data sufficient for fruitful inspection. We\nfirst found Unet is most suitable in augmented microscopy by comparing\nperformance in multi-class semantics segmentation. We develop the augmented\nmicroscopy method to capture nuclei in a brightfield image and the transformer\nusing Unet model to convert an input image into a sequence of topological\ninformation. The performance regarding Intersection-over-Union is consistent\nconcerning the choice of image preprocessing and ground-truth generation.\nTraining model with data of a specific cell type demonstrates transfer learning\napplies to some extent.\n\nThe topological transformer aims to extract persistence silhouettes or\nlandscape signatures containing geometric information of a given image of\ncells. This feature extraction facilitates studying an image as a collection of\none-dimensional data, substantially reducing computational costs. Using the\ntransformer, we attempt grouping cell images by their cell type relying solely\non topological features. Performances of the transformers followed by SVM,\nXGBoost, LGBM, and simple convolutional neural network classifiers are inferior\nto the conventional image classification. However, since this research\ninitiates a new perspective in biomedical research by combining deep learning\nand topology for image analysis, we speculate follow-up investigation will\nreinforce our genuine regime.",
          "link": "http://arxiv.org/abs/2108.01625",
          "publishedOn": "2021-08-04T01:59:23.459Z",
          "wordCount": 687,
          "title": "From augmented microscopy to the topological transformer: a new approach in cell image analysis for Alzheimer's research. (arXiv:2108.01625v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gansbeke_W/0/1/0/all/0/1\">Wouter Van Gansbeke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandenhende_S/0/1/0/all/0/1\">Simon Vandenhende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "Being able to learn dense semantic representations of images without\nsupervision is an important problem in computer vision. However, despite its\nsignificance, this problem remains rather unexplored, with a few exceptions\nthat considered unsupervised semantic segmentation on small-scale datasets with\na narrow visual domain. In this paper, we make a first attempt to tackle the\nproblem on datasets that have been traditionally utilized for the supervised\ncase. To achieve this, we introduce a two-step framework that adopts a\npredetermined mid-level prior in a contrastive optimization objective to learn\npixel embeddings. This marks a large deviation from existing works that relied\non proxy tasks or end-to-end clustering. Additionally, we argue about the\nimportance of having a prior that contains information about objects, or their\nparts, and discuss several possibilities to obtain such a prior in an\nunsupervised manner.\n\nExperimental evaluation shows that our method comes with key advantages over\nexisting works. First, the learned pixel embeddings can be directly clustered\nin semantic groups using K-Means on PASCAL. Under the fully unsupervised\nsetting, there is no precedent in solving the semantic segmentation task on\nsuch a challenging benchmark. Second, our representations can improve over\nstrong baselines when transferred to new datasets, e.g. COCO and DAVIS. The\ncode is available.",
          "link": "http://arxiv.org/abs/2102.06191",
          "publishedOn": "2021-08-04T01:59:23.452Z",
          "wordCount": 698,
          "title": "Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals. (arXiv:2102.06191v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01469",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Barnekow_V/0/1/0/all/0/1\">Vanessa Barnekow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Binder_D/0/1/0/all/0/1\">Dominik Binder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kromrey_N/0/1/0/all/0/1\">Niclas Kromrey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munaretto_P/0/1/0/all/0/1\">Pascal Munaretto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schaad_A/0/1/0/all/0/1\">Andreas Schaad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schmieder_F/0/1/0/all/0/1\">Felix Schmieder</a>",
          "description": "Synthesizing voice with the help of machine learning techniques has made\nrapid progress over the last years [1] and first high profile fraud cases have\nbeen recently reported [2]. Given the current increase in using conferencing\ntools for online teaching, we question just how easy (i.e. needed data,\nhardware, skill set) it would be to create a convincing voice fake. We analyse\nhow much training data a participant (e.g. a student) would actually need to\nfake another participants voice (e.g. a professor). We provide an analysis of\nthe existing state of the art in creating voice deep fakes, as well as offer\ndetailed technical guidance and evidence of just how much effort is needed to\ncopy a voice. A user study with more than 100 participants shows how difficult\nit is to identify real and fake voice (on avg. only 37 percent can distinguish\nbetween real and fake voice of a professor). With a focus on German language\nand an online teaching environment we discuss the societal implications as well\nas demonstrate how to use machine learning techniques to possibly detect such\nfakes.",
          "link": "http://arxiv.org/abs/2108.01469",
          "publishedOn": "2021-08-04T01:59:23.445Z",
          "wordCount": 633,
          "title": "Creation and Detection of German Voice Deepfakes. (arXiv:2108.01469v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1\">Frances Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denain_J/0/1/0/all/0/1\">Jean-Stanislas Denain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>",
          "description": "To understand neural network behavior, recent works quantitatively compare\ndifferent networks' learned representations using canonical correlation\nanalysis (CCA), centered kernel alignment (CKA), and other dissimilarity\nmeasures. Unfortunately, these widely used measures often disagree on\nfundamental observations, such as whether deep networks differing only in\nrandom initialization learn similar representations. These disagreements raise\nthe question: which, if any, of these dissimilarity measures should we believe?\nWe provide a framework to ground this question through a concrete test:\nmeasures should have sensitivity to changes that affect functional behavior,\nand specificity against changes that do not. We quantify this through a variety\nof functional behaviors including probing accuracy and robustness to\ndistribution shift, and examine changes such as varying random initialization\nand deleting principal components. We find that current metrics exhibit\ndifferent weaknesses, note that a classical baseline performs surprisingly\nwell, and highlight settings where all metrics appear to fail, thus providing a\nchallenge set for further improvement.",
          "link": "http://arxiv.org/abs/2108.01661",
          "publishedOn": "2021-08-04T01:59:23.439Z",
          "wordCount": 589,
          "title": "Grounding Representation Similarity with Statistical Testing. (arXiv:2108.01661v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bowren_J/0/1/0/all/0/1\">Joshua Bowren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Giraldo_L/0/1/0/all/0/1\">Luis Sanchez-Giraldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_O/0/1/0/all/0/1\">Odelia Schwartz</a>",
          "description": "Sparse coding has been incorporated in models of the visual cortex for its\ncomputational advantages and connection to biology. But how the level of\nsparsity contributes to performance on visual tasks is not well understood. In\nthis work, sparse coding has been integrated into an existing hierarchical V2\nmodel (Hosoya and Hyv\\\"arinen, 2015), but replacing the Independent Component\nAnalysis (ICA) with an explicit sparse coding in which the degree of sparsity\ncan be controlled. After training, the sparse coding basis functions with a\nhigher degree of sparsity resembled qualitatively different structures, such as\ncurves and corners. The contributions of the models were assessed with image\nclassification tasks, including object classification, and tasks associated\nwith mid-level vision including figure-ground classification, texture\nclassification, and angle prediction between two line stimuli. In addition, the\nmodels were assessed in comparison to a texture sensitivity measure that has\nbeen reported in V2 (Freeman et al., 2013), and a deleted-region inference\ntask. The results from the experiments show that while sparse coding performed\nworse than ICA at classifying images, only sparse coding was able to better\nmatch the texture sensitivity level of V2 and infer deleted image regions, both\nby increasing the degree of sparsity in sparse coding. Higher degrees of\nsparsity allowed for inference over larger deleted image regions. The mechanism\nthat allows for this inference capability in sparse coding is described here.",
          "link": "http://arxiv.org/abs/2108.01548",
          "publishedOn": "2021-08-04T01:59:23.422Z",
          "wordCount": 667,
          "title": "Inference via Sparse Coding in a Hierarchical Vision Model. (arXiv:2108.01548v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01518",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dianhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1\">Ngo Anh Vien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Van_M/0/1/0/all/0/1\">Mien Van</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLoone_S/0/1/0/all/0/1\">Sean McLoone</a>",
          "description": "3D skeleton-based motion prediction and activity recognition are two\ninterwoven tasks in human behaviour analysis. In this work, we propose a motion\ncontext modeling methodology that provides a new way to combine the advantages\nof both graph convolutional neural networks and recurrent neural networks for\njoint human motion prediction and activity recognition. Our approach is based\non using an LSTM encoder-decoder and a non-local feature extraction attention\nmechanism to model the spatial correlation of human skeleton data and temporal\ncorrelation among motion frames. The proposed network can easily include two\noutput branches, one for Activity Recognition and one for Future Motion\nPrediction, which can be jointly trained for enhanced performance. Experimental\nresults on Human 3.6M, CMU Mocap and NTU RGB-D datasets show that our proposed\napproach provides the best prediction capability among baseline LSTM-based\nmethods, while achieving comparable performance to other state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2108.01518",
          "publishedOn": "2021-08-04T01:59:23.414Z",
          "wordCount": 598,
          "title": "Non-local Graph Convolutional Network for joint Activity Recognition and Motion Prediction. (arXiv:2108.01518v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01468",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Kwak_Y/0/1/0/all/0/1\">Yunseok Kwak</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yun_W/0/1/0/all/0/1\">Won Joon Yun</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Jung_S/0/1/0/all/0/1\">Soyi Jung</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kim_J/0/1/0/all/0/1\">Joongheon Kim</a>",
          "description": "Quantum deep learning is a research field for the use of quantum computing\ntechniques for training deep neural networks. The research topics and\ndirections of deep learning and quantum computing have been separated for long\ntime, however by discovering that quantum circuits can act like artificial\nneural networks, quantum deep learning research is widely adopted. This paper\nexplains the backgrounds and basic principles of quantum deep learning and also\nintroduces major achievements. After that, this paper discusses the challenges\nof quantum deep learning research in multiple perspectives. Lastly, this paper\npresents various future research directions and application fields of quantum\ndeep learning.",
          "link": "http://arxiv.org/abs/2108.01468",
          "publishedOn": "2021-08-04T01:59:23.406Z",
          "wordCount": 533,
          "title": "Quantum Neural Networks: Concepts, Applications, and Challenges. (arXiv:2108.01468v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Makarychev_K/0/1/0/all/0/1\">Konstantin Makarychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_L/0/1/0/all/0/1\">Liren Shan</a>",
          "description": "We consider the problem of explainable $k$-medians and $k$-means introduced\nby Dasgupta, Frost, Moshkovitz, and Rashtchian~(ICML 2020). In this problem,\nour goal is to find a threshold decision tree that partitions data into $k$\nclusters and minimizes the $k$-medians or $k$-means objective. The obtained\nclustering is easy to interpret because every decision node of a threshold tree\nsplits data based on a single feature into two groups. We propose a new\nalgorithm for this problem which is $\\tilde O(\\log k)$ competitive with\n$k$-medians with $\\ell_1$ norm and $\\tilde O(k)$ competitive with $k$-means.\nThis is an improvement over the previous guarantees of $O(k)$ and $O(k^2)$ by\nDasgupta et al (2020). We also provide a new algorithm which is $O(\\log^{3/2}\nk)$ competitive for $k$-medians with $\\ell_2$ norm. Our first algorithm is\nnear-optimal: Dasgupta et al (2020) showed a lower bound of $\\Omega(\\log k)$\nfor $k$-medians; in this work, we prove a lower bound of $\\tilde\\Omega(k)$ for\n$k$-means. We also provide a lower bound of $\\Omega(\\log k)$ for $k$-medians\nwith $\\ell_2$ norm.",
          "link": "http://arxiv.org/abs/2107.00798",
          "publishedOn": "2021-08-04T01:59:23.400Z",
          "wordCount": 635,
          "title": "Near-optimal Algorithms for Explainable k-Medians and k-Means. (arXiv:2107.00798v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Onda_R/0/1/0/all/0/1\">Rina Onda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhengyan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotera_M/0/1/0/all/0/1\">Masaaki Kotera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oono_K/0/1/0/all/0/1\">Kenta Oono</a>",
          "description": "It is preferred that feature selectors be \\textit{stable} for better\ninterpretabity and robust prediction. Ensembling is known to be effective for\nimproving the stability of feature selectors. Since ensembling is\ntime-consuming, it is desirable to reduce the computational cost to estimate\nthe stability of the ensemble feature selectors. We propose a simulator of a\nfeature selector, and apply it to a fast estimation of the stability of\nensemble feature selectors. To the best of our knowledge, this is the first\nstudy that estimates the stability of ensemble feature selectors and reduces\nthe computation time theoretically and empirically.",
          "link": "http://arxiv.org/abs/2108.01485",
          "publishedOn": "2021-08-04T01:59:23.394Z",
          "wordCount": 561,
          "title": "Fast Estimation Method for the Stability of Ensemble Feature Selectors. (arXiv:2108.01485v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antaris_S/0/1/0/all/0/1\">Stefanos Antaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafailidis_D/0/1/0/all/0/1\">Dimitrios Rafailidis</a>",
          "description": "Accounting for the fact that users have different sequential patterns, the\nmain drawback of state-of-the-art recommendation strategies is that a fixed\nsequence length of user-item interactions is required as input to train the\nmodels. This might limit the recommendation accuracy, as in practice users\nfollow different trends on the sequential recommendations. Hence, baseline\nstrategies might ignore important sequential interactions or add noise to the\nmodels with redundant interactions, depending on the variety of users'\nsequential behaviours. To overcome this problem, in this study we propose the\nSAR model, which not only learns the sequential patterns but also adjusts the\nsequence length of user-item interactions in a personalized manner. We first\ndesign an actor-critic framework, where the RL agent tries to compute the\noptimal sequence length as an action, given the user's state representation at\na certain time step. In addition, we optimize a joint loss function to align\nthe accuracy of the sequential recommendations with the expected cumulative\nrewards of the critic network, while at the same time we adapt the sequence\nlength with the actor network in a personalized manner. Our experimental\nevaluation on four real-world datasets demonstrates the superiority of our\nproposed model over several baseline approaches. Finally, we make our\nimplementation publicly available at https://github.com/stefanosantaris/sar.",
          "link": "http://arxiv.org/abs/2108.01442",
          "publishedOn": "2021-08-04T01:59:23.378Z",
          "wordCount": 643,
          "title": "Sequence Adaptation via Reinforcement Learning in Recommender Systems. (arXiv:2108.01442v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Veeramachaneni_S/0/1/0/all/0/1\">Sowmini Devi Veeramachaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujari_A/0/1/0/all/0/1\">Arun K Pujari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmanabhan_V/0/1/0/all/0/1\">Vineet Padmanabhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vikas Kumar</a>",
          "description": "Recommender systems(RS), especially collaborative filtering(CF) based RS, has\nbeen playing an important role in many e-commerce applications. As the\ninformation being searched over the internet is rapidly increasing, users often\nface the difficulty of finding items of his/her own interest and RS often\nprovides help in such tasks. Recent studies show that, as the item space\nincreases, and the number of items rated by the users become very less, issues\nlike sparsity arise. To mitigate the sparsity problem, transfer learning\ntechniques are being used wherein the data from dense domain(source) is\nconsidered in order to predict the missing entries in the sparse\ndomain(target). In this paper, we propose a transfer learning approach for\ncross-domain recommendation when both domains have no overlap of users and\nitems. In our approach the transferring of knowledge from source to target\ndomain is done in a novel way. We make use of co-clustering technique to obtain\nthe codebook (cluster-level rating pattern) of source domain. By making use of\nhinge loss function we transfer the learnt codebook of the source domain to\ntarget. The use of hinge loss as a loss function is novel and has not been\ntried before in transfer learning. We demonstrate that our technique improves\nthe approximation of the target matrix on benchmark datasets.",
          "link": "http://arxiv.org/abs/2108.01473",
          "publishedOn": "2021-08-04T01:59:23.365Z",
          "wordCount": 654,
          "title": "A Hinge-Loss based Codebook Transfer for Cross-Domain Recommendation with Nonoverlapping Data. (arXiv:2108.01473v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozkan_M/0/1/0/all/0/1\">Mehmet Fatih Ozkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocque_A/0/1/0/all/0/1\">Abishek Joseph Rocque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yao Ma</a>",
          "description": "Drivers have unique and rich driving behaviors when operating vehicles in\ntraffic. This paper presents a novel driver behavior learning approach that\ncaptures the uniqueness and richness of human driver behavior in realistic\ndriving scenarios. A stochastic inverse reinforcement learning (SIRL) approach\nis proposed to learn a distribution of cost function, which represents the\nrichness of the human driver behavior with a given set of driver-specific\ndemonstrations. Evaluations are conducted on the realistic driving data\ncollected from the 3D driver-in-the-loop driving simulation. The results show\nthat the learned stochastic driver model is capable of expressing the richness\nof the human driving strategies under different realistic driving scenarios.\nCompared to the deterministic baseline driver behavior model, the results\nreveal that the proposed stochastic driver behavior model can better replicate\nthe driver's unique and rich driving strategies in a variety of traffic\nconditions.",
          "link": "http://arxiv.org/abs/2107.06344",
          "publishedOn": "2021-08-04T01:59:23.347Z",
          "wordCount": 622,
          "title": "Inverse Reinforcement Learning Based Stochastic Driver Behavior Learning. (arXiv:2107.06344v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.05328",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Botts_C/0/1/0/all/0/1\">Carsten H. Botts</a>",
          "description": "In this paper we simulate an ensemble of cooperating, mobile sensing agents\nthat implement the cyclic stochastic optimization (CSO) algorithm in an attempt\nto survey and track multiple targets. In the CSO algorithm proposed, each agent\nuses its sensed measurements, its shared information, and its predictions of\nothers' future motion to decide on its next action. This decision is selected\nto minimize a loss function that decreases as the uncertainty in the targets'\nstate estimates decreases. Only noisy measurements of this loss function are\navailable to each agent, and in this study, each agent attempts to minimize\nthis function by calculating its stochastic gradient. This paper examines, via\nsimulation-based experiments, the implications and applicability of CSO\nconvergence in three dimensions.",
          "link": "http://arxiv.org/abs/2010.05328",
          "publishedOn": "2021-08-04T01:59:23.327Z",
          "wordCount": 562,
          "title": "Three-Dimensional Swarming Using Cyclic Stochastic Optimization. (arXiv:2010.05328v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Normandin_F/0/1/0/all/0/1\">Fabrice Normandin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golemo_F/0/1/0/all/0/1\">Florian Golemo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostapenko_O/0/1/0/all/0/1\">Oleksiy Ostapenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1\">Pau Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riemer_M/0/1/0/all/0/1\">Matthew D Riemer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hurtado_J/0/1/0/all/0/1\">Julio Hurtado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khetarpal_K/0/1/0/all/0/1\">Khimya Khetarpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dominic Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindeborg_R/0/1/0/all/0/1\">Ryan Lindeborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lesort_T/0/1/0/all/0/1\">Timoth&#xe9;e Lesort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charlin_L/0/1/0/all/0/1\">Laurent Charlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1\">Irina Rish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caccia_M/0/1/0/all/0/1\">Massimo Caccia</a>",
          "description": "The field of Continual Learning (CL) seeks to develop algorithms that\naccumulate knowledge and skills over time through interaction with\nnon-stationary environments and data distributions. Measuring progress in CL\ncan be difficult because a plethora of evaluation procedures (ettings) and\nalgorithmic solutions (methods) have emerged, each with their own potentially\ndisjoint set of assumptions about the CL problem. In this work, we view each\nsetting as a set of assumptions. We then create a tree-shaped hierarchy of the\nresearch settings in CL, in which more general settings become the parents of\nthose with more restrictive assumptions. This makes it possible to use\ninheritance to share and reuse research, as developing a method for a given\nsetting also makes it directly applicable onto any of its children. We\ninstantiate this idea as a publicly available software framework called\nSequoia, which features a variety of settings from both the Continual\nSupervised Learning (CSL) and Continual Reinforcement Learning (CRL) domains.\nSequoia also includes a growing suite of methods which are easy to extend and\ncustomize, in addition to more specialized methods from third-party libraries.\nWe hope that this new paradigm and its first implementation can serve as a\nfoundation for the unification and acceleration of research in CL. You can help\nus grow the tree by visiting www.github.com/lebrice/Sequoia.",
          "link": "http://arxiv.org/abs/2108.01005",
          "publishedOn": "2021-08-04T01:59:23.319Z",
          "wordCount": 685,
          "title": "Sequoia: A Software Framework to Unify Continual Learning Research. (arXiv:2108.01005v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Munir_M/0/1/0/all/0/1\">Md. Shirajum Munir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Ki Tae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thar_K/0/1/0/all/0/1\">Kyi Thar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1\">Dusit Niyato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1\">Choong Seon Hong</a>",
          "description": "In this paper, the design of a rational decision support system (RDSS) for a\nconnected and autonomous vehicle charging infrastructure (CAV-CI) is studied.\nIn the considered CAV-CI, the distribution system operator (DSO) deploys\nelectric vehicle supply equipment (EVSE) to provide an EV charging facility for\nhuman-driven connected vehicles (CVs) and autonomous vehicles (AVs). The\ncharging request by the human-driven EV becomes irrational when it demands more\nenergy and charging period than its actual need. Therefore, the scheduling\npolicy of each EVSE must be adaptively accumulated the irrational charging\nrequest to satisfy the charging demand of both CVs and AVs. To tackle this, we\nformulate an RDSS problem for the DSO, where the objective is to maximize the\ncharging capacity utilization by satisfying the laxity risk of the DSO. Thus,\nwe devise a rational reward maximization problem to adapt the irrational\nbehavior by CVs in a data-informed manner. We propose a novel risk adversarial\nmulti-agent learning system (RAMALS) for CAV-CI to solve the formulated RDSS\nproblem. In RAMALS, the DSO acts as a centralized risk adversarial agent (RAA)\nfor informing the laxity risk to each EVSE. Subsequently, each EVSE plays the\nrole of a self-learner agent to adaptively schedule its own EV sessions by\ncoping advice from RAA. Experiment results show that the proposed RAMALS\naffords around 46.6% improvement in charging rate, about 28.6% improvement in\nthe EVSE's active charging time and at least 33.3% more energy utilization, as\ncompared to a currently deployed ACN EVSE system, and other baselines.",
          "link": "http://arxiv.org/abs/2108.01466",
          "publishedOn": "2021-08-04T01:59:23.289Z",
          "wordCount": 712,
          "title": "Risk Adversarial Learning System for Connected and Autonomous Vehicle Charging. (arXiv:2108.01466v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lechiakh_M/0/1/0/all/0/1\">Mohamed Lechiakh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurer_A/0/1/0/all/0/1\">Alexandre Maurer</a>",
          "description": "So far, most research on recommender systems focused on maintaining long-term\nuser engagement and satisfaction, by promoting relevant and personalized\ncontent. However, it is still very challenging to evaluate the quality and the\nreliability of this content. In this paper, we propose FEBR (Expert-Based\nRecommendation Framework), an apprenticeship learning framework to assess the\nquality of the recommended content on online platforms. The framework exploits\nthe demonstrated trajectories of an expert (assumed to be reliable) in a\nrecommendation evaluation environment, to recover an unknown utility function.\nThis function is used to learn an optimal policy describing the expert's\nbehavior, which is then used in the framework to provide high-quality and\npersonalized recommendations. We evaluate the performance of our solution\nthrough a user interest simulation environment (using RecSim). We simulate\ninteractions under the aforementioned expert policy for videos recommendation,\nand compare its efficiency with standard recommendation methods. The results\nshow that our approach provides a significant gain in terms of content quality,\nevaluated by experts and watched by users, while maintaining almost the same\nwatch time as the baseline approaches.",
          "link": "http://arxiv.org/abs/2108.01455",
          "publishedOn": "2021-08-04T01:59:23.271Z",
          "wordCount": 614,
          "title": "FEBR: Expert-Based Recommendation Framework for beneficial and personalized content. (arXiv:2108.01455v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01441",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1\">Quanye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianying Lin</a>",
          "description": "Manifold ranking has been successfully applied in query-oriented\nmulti-document summarization. It not only makes use of the relationships among\nthe sentences, but also the relationships between the given query and the\nsentences. However, the information of original query is often insufficient. So\nwe present a query expansion method, which is combined in the manifold ranking\nto resolve this problem. Our method not only utilizes the information of the\nquery term itself and the knowledge base WordNet to expand it by synonyms, but\nalso uses the information of the document set itself to expand the query in\nvarious ways (mean expansion, variance expansion and TextRank expansion).\nCompared with the previous query expansion methods, our method combines\nmultiple query expansion methods to better represent query information, and at\nthe same time, it makes a useful attempt on manifold ranking. In addition, we\nuse the degree of word overlap and the proximity between words to calculate the\nsimilarity between sentences. We performed experiments on the datasets of DUC\n2006 and DUC2007, and the evaluation results show that the proposed query\nexpansion method can significantly improve the system performance and make our\nsystem comparable to the state-of-the-art systems.",
          "link": "http://arxiv.org/abs/2108.01441",
          "publishedOn": "2021-08-04T01:59:23.261Z",
          "wordCount": 643,
          "title": "Using Query Expansion in Manifold Ranking for Query-Oriented Multi-Document Summarization. (arXiv:2108.01441v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01317",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ikemoto_J/0/1/0/all/0/1\">Junya Ikemoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ushio_T/0/1/0/all/0/1\">Toshimitsu Ushio</a>",
          "description": "We present a novel deep reinforcement learning (DRL)-based design of a\nnetworked controller with network delays for signal temporal logic (STL)\nspecifications. We consider the case in which both the system dynamics and\nnetwork delays are unknown. Because the satisfaction of an STL formula is based\nnot only on the current state but also on the behavior of the system, we\npropose an extension of the Markov decision process (MDP), which is called a\n$\\tau\\delta$-MDP, such that we can evaluate the satisfaction of the STL formula\nunder the network delays using the $\\tau\\delta$-MDP. Thereafter, we construct\ndeep neural networks based on the $\\tau\\delta$-MDP and propose a learning\nalgorithm. Through simulations, we also demonstrate the learning performance of\nthe proposed algorithm.",
          "link": "http://arxiv.org/abs/2108.01317",
          "publishedOn": "2021-08-04T01:59:23.254Z",
          "wordCount": 575,
          "title": "Deep Reinforcement Learning Based Networked Control with Network Delays for Signal Temporal Logic Specifications. (arXiv:2108.01317v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pati_A/0/1/0/all/0/1\">Ashis Pati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerch_A/0/1/0/all/0/1\">Alexander Lerch</a>",
          "description": "Improving controllability or the ability to manipulate one or more attributes\nof the generated data has become a topic of interest in the context of deep\ngenerative models of music. Recent attempts in this direction have relied on\nlearning disentangled representations from data such that the underlying\nfactors of variation are well separated. In this paper, we focus on the\nrelationship between disentanglement and controllability by conducting a\nsystematic study using different supervised disentanglement learning algorithms\nbased on the Variational Auto-Encoder (VAE) architecture. Our experiments show\nthat a high degree of disentanglement can be achieved by using different forms\nof supervision to train a strong discriminative encoder. However, in the\nabsence of a strong generative decoder, disentanglement does not necessarily\nimply controllability. The structure of the latent space with respect to the\nVAE-decoder plays an important role in boosting the ability of a generative\nmodel to manipulate different attributes. To this end, we also propose methods\nand metrics to help evaluate the quality of a latent space with respect to the\nafforded degree of controllability.",
          "link": "http://arxiv.org/abs/2108.01450",
          "publishedOn": "2021-08-04T01:59:23.236Z",
          "wordCount": 631,
          "title": "Is Disentanglement enough? On Latent Representations for Controllable Music Generation. (arXiv:2108.01450v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01393",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dai_S/0/1/0/all/0/1\">Shuang Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_F/0/1/0/all/0/1\">Fanlin Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_H/0/1/0/all/0/1\">Hongsheng Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xizhong Chen</a>",
          "description": "The power system is undergoing rapid evolution with the roll-out of advanced\nmetering infrastructure and local energy applications (e.g. electric vehicles)\nas well as the increasing penetration of intermittent renewable energy at both\ntransmission and distribution level, which characterizes the peak load demand\nwith stronger randomness and less predictability and therefore poses a threat\nto the power grid security. Since storing large quantities of electricity to\nsatisfy load demand is neither economically nor environmentally friendly,\neffective peak demand management strategies and reliable peak load forecast\nmethods become essential for optimizing the power system operations. To this\nend, this paper provides a timely and comprehensive overview of peak load\ndemand forecast methods in the literature. To our best knowledge, this is the\nfirst comprehensive review on such topic. In this paper we first give a precise\nand unified problem definition of peak load demand forecast. Second, 139 papers\non peak load forecast methods were systematically reviewed where methods were\nclassified into different stages based on the timeline. Thirdly, a comparative\nanalysis of peak load forecast methods are summarized and different optimizing\nmethods to improve the forecast performance are discussed. The paper ends with\na comprehensive summary of the reviewed papers and a discussion of potential\nfuture research directions.",
          "link": "http://arxiv.org/abs/2108.01393",
          "publishedOn": "2021-08-04T01:59:23.222Z",
          "wordCount": 657,
          "title": "Electrical peak demand forecasting- A review. (arXiv:2108.01393v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01407",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kostovska_A/0/1/0/all/0/1\">Ana Kostovska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petkovicc_M/0/1/0/all/0/1\">Matej Petkovic&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stepisnik_T/0/1/0/all/0/1\">Toma&#x17e; Stepi&#x161;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_L/0/1/0/all/0/1\">Luke Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_T/0/1/0/all/0/1\">Timothy Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Heras_J/0/1/0/all/0/1\">Jos&#xe9; Mart&#xed;nez-Heras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panov_P/0/1/0/all/0/1\">Pan&#x10d;e Panov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dzeroski_S/0/1/0/all/0/1\">Sa&#x161;o D&#x17e;eroski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donati_A/0/1/0/all/0/1\">Alessandro Donati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simidjievski_N/0/1/0/all/0/1\">Nikola Simidjievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocev_D/0/1/0/all/0/1\">Dragi Kocev</a>",
          "description": "We present GalaxAI - a versatile machine learning toolbox for efficient and\ninterpretable end-to-end analysis of spacecraft telemetry data. GalaxAI employs\nvarious machine learning algorithms for multivariate time series analyses,\nclassification, regression and structured output prediction, capable of\nhandling high-throughput heterogeneous data. These methods allow for the\nconstruction of robust and accurate predictive models, that are in turn applied\nto different tasks of spacecraft monitoring and operations planning. More\nimportantly, besides the accurate building of models, GalaxAI implements a\nvisualisation layer, providing mission specialists and operators with a full,\ndetailed and interpretable view of the data analysis process. We show the\nutility and versatility of GalaxAI on two use-cases concerning two different\nspacecraft: i) analysis and planning of Mars Express thermal power consumption\nand ii) predicting of INTEGRAL's crossings through Van Allen belts.",
          "link": "http://arxiv.org/abs/2108.01407",
          "publishedOn": "2021-08-04T01:59:23.206Z",
          "wordCount": 583,
          "title": "GalaxAI: Machine learning toolbox for interpretable analysis of spacecraft telemetry data. (arXiv:2108.01407v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1\">Ajil Jalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arvinte_M/0/1/0/all/0/1\">Marius Arvinte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daras_G/0/1/0/all/0/1\">Giannis Daras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1\">Eric Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamir_J/0/1/0/all/0/1\">Jonathan I. Tamir</a>",
          "description": "The CSGM framework (Bora-Jalal-Price-Dimakis'17) has shown that deep\ngenerative priors can be powerful tools for solving inverse problems. However,\nto date this framework has been empirically successful only on certain datasets\n(for example, human faces and MNIST digits), and it is known to perform poorly\non out-of-distribution samples. In this paper, we present the first successful\napplication of the CSGM framework on clinical MRI data. We train a generative\nprior on brain scans from the fastMRI dataset, and show that posterior sampling\nvia Langevin dynamics achieves high quality reconstructions. Furthermore, our\nexperiments and theory show that posterior sampling is robust to changes in the\nground-truth distribution and measurement process. Our code and models are\navailable at: \\url{https://github.com/utcsilab/csgm-mri-langevin}.",
          "link": "http://arxiv.org/abs/2108.01368",
          "publishedOn": "2021-08-04T01:59:23.191Z",
          "wordCount": 572,
          "title": "Robust Compressed Sensing MRI with Deep Generative Priors. (arXiv:2108.01368v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hengbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malla_S/0/1/0/all/0/1\">Srikanth Malla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1\">Masayoshi Tomizuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1\">Chiho Choi</a>",
          "description": "Motion forecasting plays a significant role in various domains (e.g.,\nautonomous driving, human-robot interaction), which aims to predict future\nmotion sequences given a set of historical observations. However, the observed\nelements may be of different levels of importance. Some information may be\nirrelevant or even distracting to the forecasting in certain situations. To\naddress this issue, we propose a generic motion forecasting framework (named\nRAIN) with dynamic key information selection and ranking based on a hybrid\nattention mechanism. The general framework is instantiated to handle\nmulti-agent trajectory prediction and human motion forecasting tasks,\nrespectively. In the former task, the model learns to recognize the relations\nbetween agents with a graph representation and to determine their relative\nsignificance. In the latter task, the model learns to capture the temporal\nproximity and dependency in long-term human motions. We also propose an\neffective double-stage training pipeline with an alternating training strategy\nto optimize the parameters in different modules of the framework. We validate\nthe framework on both synthetic simulations and motion forecasting benchmarks\nin different domains, demonstrating that our method not only achieves\nstate-of-the-art forecasting performance, but also provides interpretable and\nreasonable hybrid attention weights.",
          "link": "http://arxiv.org/abs/2108.01316",
          "publishedOn": "2021-08-04T01:59:23.185Z",
          "wordCount": 657,
          "title": "RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting. (arXiv:2108.01316v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miron_A/0/1/0/all/0/1\">Alina Miron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosan_C/0/1/0/all/0/1\">Crina Grosan</a>",
          "description": "The work in this paper focuses on the role of machine learning in assessing\nthe correctness of a human motion or action. This task proves to be more\nchallenging than the gesture and action recognition ones. We will demonstrate,\nthrough a set of experiments on a recent dataset, that machine learning\nalgorithms can produce good results for certain actions, but can also fall into\nthe trap of classifying an incorrect execution of an action as a correct\nexecution of another action.",
          "link": "http://arxiv.org/abs/2108.01375",
          "publishedOn": "2021-08-04T01:59:23.167Z",
          "wordCount": 530,
          "title": "Classifying action correctness in physical rehabilitation exercises. (arXiv:2108.01375v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karalus_J/0/1/0/all/0/1\">Jakob Karalus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindner_F/0/1/0/all/0/1\">Felix Lindner</a>",
          "description": "The capability to interactively learn from human feedback would enable robots\nin new social settings. For example, novice users could train service robots in\nnew tasks naturally and interactively. Human-in-the-loop Reinforcement Learning\n(HRL) addresses this issue by combining human feedback and reinforcement\nlearning (RL) techniques. State-of-the-art interactive learning techniques\nsuffer from slow convergence, thus leading to a frustrating experience for the\nhuman. This work approaches this problem by extending the existing TAMER\nFramework with the possibility to enhance human feedback with two different\ntypes of counterfactual explanations. We demonstrate our extensions' success in\nimproving the convergence, especially in the crucial early phases of the\ntraining.",
          "link": "http://arxiv.org/abs/2108.01358",
          "publishedOn": "2021-08-04T01:59:23.152Z",
          "wordCount": 538,
          "title": "Accelerating the Convergence of Human-in-the-Loop Reinforcement Learning with Counterfactual Explanations. (arXiv:2108.01358v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2006.04593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ryffel_T/0/1/0/all/0/1\">Th&#xe9;o Ryffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tholoniat_P/0/1/0/all/0/1\">Pierre Tholoniat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pointcheval_D/0/1/0/all/0/1\">David Pointcheval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>",
          "description": "We propose AriaNN, a low-interaction privacy-preserving framework for private\nneural network training and inference on sensitive data. Our semi-honest\n2-party computation protocol leverages function secret sharing, a recent\nlightweight cryptographic protocol that allows us to achieve an efficient\nonline phase. We design optimized primitives for the building blocks of neural\nnetworks such as ReLU, MaxPool and BatchNorm. For instance, we perform private\ncomparison for ReLU operations with a single message of the size of the input\nduring the online phase, and with preprocessing keys close to 4X smaller than\nprevious work. Last, we propose an extension to support n-party private\nfederated learning. We implement our framework as an extensible system on top\nof PyTorch that leverages CPU and GPU hardware acceleration for cryptographic\nand machine learning operations. We evaluate our end-to-end system for private\ninference and training on standard neural networks such as AlexNet, VGG16 or\nResNet18 between distant servers. We show that computation rather than\ncommunication is the main bottleneck and that using GPUs together with reduced\nkey size is a promising solution to overcome this barrier.",
          "link": "http://arxiv.org/abs/2006.04593",
          "publishedOn": "2021-08-04T01:59:23.076Z",
          "wordCount": 661,
          "title": "ARIANN: Low-Interaction Privacy-Preserving Deep Learning via Function Secret Sharing. (arXiv:2006.04593v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01640",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Cokina_M/0/1/0/all/0/1\">Michal &#x10c;okina</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Maslej_Kresnakova_V/0/1/0/all/0/1\">Viera Maslej-Kre&#x161;&#x148;&#xe1;kov&#xe1;</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Butka_P/0/1/0/all/0/1\">Peter Butka</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Parimucha_S/0/1/0/all/0/1\">&#x160;tefan Parimucha</a>",
          "description": "In the last couple of decades, tremendous progress has been achieved in\ndeveloping robotic telescopes and, as a result, sky surveys (both terrestrial\nand space) have become the source of a substantial amount of new observational\ndata. These data contain a lot of information about binary stars, hidden in\ntheir light curves. With the huge amount of astronomical data gathered, it is\nnot reasonable to expect all the data to be manually processed and analyzed.\nTherefore, in this paper, we focus on the automatic classification of eclipsing\nbinary stars using deep learning methods. Our classifier provides a tool for\nthe categorization of light curves of binary stars into two classes: detached\nand over-contact. We used the ELISa software to obtain synthetic data, which we\nthen used for the training of the classifier. For evaluation purposes, we\ncollected 100 light curves of observed binary stars, in order to evaluate a\nnumber of classifiers. We evaluated semi-detached eclipsing binary stars as\ndetached. The best-performing classifier combines bidirectional Long Short-Term\nMemory (LSTM) and a one-dimensional convolutional neural network, which\nachieved 98% accuracy on the evaluation set. Omitting semi-detached eclipsing\nbinary stars, we could obtain 100% accuracy in classification.",
          "link": "http://arxiv.org/abs/2108.01640",
          "publishedOn": "2021-08-04T01:59:23.049Z",
          "wordCount": 655,
          "title": "Automatic classification of eclipsing binary stars using deep learning methods. (arXiv:2108.01640v1 [astro-ph.SR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01529",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ma_Y/0/1/0/all/0/1\">Yifan Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1\">Yifei Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_X/0/1/0/all/0/1\">Xianghao Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_S/0/1/0/all/0/1\">S.H. Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Letaief_K/0/1/0/all/0/1\">Khaled B. Letaief</a>",
          "description": "Channel estimation and beamforming play critical roles in frequency-division\nduplexing (FDD) massive multiple-input multiple-output (MIMO) systems. However,\nthese two modules have been treated as two stand-alone components, which makes\nit difficult to achieve a global system optimality. In this paper, we propose a\ndeep learning-based approach that directly optimizes the beamformers at the\nbase station according to the received uplink pilots, thereby, bypassing the\nexplicit channel estimation. Different from the existing fully data-driven\napproach where all the modules are replaced by deep neural networks (DNNs), a\nneural calibration method is proposed to improve the scalability of the\nend-to-end design. In particular, the backbone of conventional time-efficient\nalgorithms, i.e., the least-squares (LS) channel estimator and the zero-forcing\n(ZF) beamformer, is preserved and DNNs are leveraged to calibrate their inputs\nfor better performance. The permutation equivariance property of the formulated\nresource allocation problem is then identified to design a low-complexity\nneural network architecture. Simulation results will show the superiority of\nthe proposed neural calibration method over benchmark schemes in terms of both\nthe spectral efficiency and scalability in large-scale wireless networks.",
          "link": "http://arxiv.org/abs/2108.01529",
          "publishedOn": "2021-08-04T01:59:23.040Z",
          "wordCount": 638,
          "title": "Neural Calibration for Scalable Beamforming in FDD Massive MIMO with Implicit Channel Estimation. (arXiv:2108.01529v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Love_J/0/1/0/all/0/1\">Jake Love</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulkers_J/0/1/0/all/0/1\">Jeroen Mulkers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourianoff_G/0/1/0/all/0/1\">George Bourianoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leliaert_J/0/1/0/all/0/1\">Jonathan Leliaert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Everschor_Sitte_K/0/1/0/all/0/1\">Karin Everschor-Sitte</a>",
          "description": "Physical reservoir computing is a computational paradigm that enables\ntemporal pattern recognition to be performed directly in physical matter. By\nexciting non-linear dynamical systems and linearly classifying their changes in\nstate, we can create highly energy-efficient devices capable of solving machine\nlearning tasks without the need to build a modular system consisting of\nmillions of neurons interconnected by synapses. The chosen dynamical system\nmust have three desirable properties: non-linearity, complexity, and fading\nmemory to act as an effective reservoir. We present task agnostic quantitative\nmeasures for each of these three requirements and exemplify them for two\nreservoirs: an echo state network and a simulated magnetic skyrmion-based\nreservoir. We show that, in general, systems with lower damping reach higher\nvalues in all three performance metrics. Whilst for input signal strength,\nthere is a natural trade-off between memory capacity and non-linearity of the\nreservoir's behaviour. In contrast to typical task-dependent reservoir\ncomputing benchmarks, these metrics can be evaluated in parallel from a single\ninput signal, drastically speeding up the parameter search to design efficient\nand high-performance reservoirs.",
          "link": "http://arxiv.org/abs/2108.01512",
          "publishedOn": "2021-08-04T01:59:23.002Z",
          "wordCount": 631,
          "title": "Task Agnostic Metrics for Reservoir Computing. (arXiv:2108.01512v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01513",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yandong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>",
          "description": "State-of-the-art deep face recognition methods are mostly trained with a\nsoftmax-based multi-class classification framework. Despite being popular and\neffective, these methods still have a few shortcomings that limit empirical\nperformance. In this paper, we first identify the discrepancy between training\nand evaluation in the existing multi-class classification framework and then\ndiscuss the potential limitations caused by the \"competitive\" nature of softmax\nnormalization. Motivated by these limitations, we propose a novel binary\nclassification training framework, termed SphereFace2. In contrast to existing\nmethods, SphereFace2 circumvents the softmax normalization, as well as the\ncorresponding closed-set assumption. This effectively bridges the gap between\ntraining and evaluation, enabling the representations to be improved\nindividually by each binary classification task. Besides designing a specific\nwell-performing loss function, we summarize a few general principles for this\n\"one-vs-all\" binary classification framework so that it can outperform current\ncompetitive methods. We conduct comprehensive experiments on popular benchmarks\nto demonstrate that SphereFace2 can consistently outperform current\nstate-of-the-art deep face recognition methods.",
          "link": "http://arxiv.org/abs/2108.01513",
          "publishedOn": "2021-08-04T01:59:22.940Z",
          "wordCount": 618,
          "title": "SphereFace2: Binary Classification is All You Need for Deep Face Recognition. (arXiv:2108.01513v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esteves_J/0/1/0/all/0/1\">Jose Jurandir Alves Esteves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boubendir_A/0/1/0/all/0/1\">Amina Boubendir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillemin_F/0/1/0/all/0/1\">Fabrice Guillemin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sens_P/0/1/0/all/0/1\">Pierre Sens</a>",
          "description": "We present a hybrid ML-heuristic approach that we name \"Heuristically\nAssisted Deep Reinforcement Learning (HA-DRL)\" to solve the problem of Network\nSlice Placement Optimization. The proposed approach leverages recent works on\nDeep Reinforcement Learning (DRL) for slice placement and Virtual Network\nEmbedding (VNE) and uses a heuristic function to optimize the exploration of\nthe action space by giving priority to reliable actions indicated by an\nefficient heuristic algorithm. The evaluation results show that the proposed\nHA-DRL algorithm can accelerate the learning of an efficient slice placement\npolicy improving slice acceptance ratio when compared with state-of-the-art\napproaches that are based only on reinforcement learning.",
          "link": "http://arxiv.org/abs/2108.01544",
          "publishedOn": "2021-08-04T01:59:22.929Z",
          "wordCount": 552,
          "title": "Controlled Deep Reinforcement Learning for Optimized Slice Placement. (arXiv:2108.01544v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_F/0/1/0/all/0/1\">Fuyuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xing Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Huifeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruiming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiuqiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xue Liu</a>",
          "description": "Click-through rate prediction is one of the core tasks in commercial\nrecommender systems. It aims to predict the probability of a user clicking a\nparticular item given user and item features. As feature interactions bring in\nnon-linearity, they are widely adopted to improve the performance of CTR\nprediction models. Therefore, effectively modelling feature interactions has\nattracted much attention in both the research and industry field. The current\napproaches can generally be categorized into three classes: (1) na\\\"ive\nmethods, which do not model feature interactions and only use original\nfeatures; (2) memorized methods, which memorize feature interactions by\nexplicitly viewing them as new features and assigning trainable embeddings; (3)\nfactorized methods, which learn latent vectors for original features and\nimplicitly model feature interactions through factorization functions. Studies\nhave shown that modelling feature interactions by one of these methods alone\nare suboptimal due to the unique characteristics of different feature\ninteractions. To address this issue, we first propose a general framework\ncalled OptInter which finds the most suitable modelling method for each feature\ninteraction. Different state-of-the-art deep CTR models can be viewed as\ninstances of OptInter. To realize the functionality of OptInter, we also\nintroduce a learning algorithm that automatically searches for the optimal\nmodelling method. We conduct extensive experiments on four large datasets. Our\nexperiments show that OptInter improves the best performed state-of-the-art\nbaseline deep CTR models by up to 2.21%. Compared to the memorized method,\nwhich also outperforms baselines, we reduce up to 91% parameters. In addition,\nwe conduct several ablation studies to investigate the influence of different\ncomponents of OptInter. Finally, we provide interpretable discussions on the\nresults of OptInter.",
          "link": "http://arxiv.org/abs/2108.01265",
          "publishedOn": "2021-08-04T01:59:22.914Z",
          "wordCount": 725,
          "title": "Memorize, Factorize, or be Na\\\"ive: Learning Optimal Feature Interaction Methods for CTR Prediction. (arXiv:2108.01265v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Levin_R/0/1/0/all/0/1\">Roman Levin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1\">Manli Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgnia_E/0/1/0/all/0/1\">Eitan Borgnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "Conventional saliency maps highlight input features to which neural network\npredictions are highly sensitive. We take a different approach to saliency, in\nwhich we identify and analyze the network parameters, rather than inputs, which\nare responsible for erroneous decisions. We find that samples which cause\nsimilar parameters to malfunction are semantically similar. We also show that\npruning the most salient parameters for a wrongly classified sample often\nimproves model behavior. Furthermore, fine-tuning a small number of the most\nsalient parameters on a single sample results in error correction on other\nsamples that are misclassified for similar reasons. Based on our parameter\nsaliency method, we also introduce an input-space saliency technique that\nreveals how image features cause specific network components to malfunction.\nFurther, we rigorously validate the meaningfulness of our saliency maps on both\nthe dataset and case-study levels.",
          "link": "http://arxiv.org/abs/2108.01335",
          "publishedOn": "2021-08-04T01:59:22.899Z",
          "wordCount": 586,
          "title": "Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability. (arXiv:2108.01335v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1\">Ioana Baldini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dennis Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamurthy_K/0/1/0/all/0/1\">Karthikeyan Natesan Ramamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yurochkin_M/0/1/0/all/0/1\">Mikhail Yurochkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Moninder Singh</a>",
          "description": "We study the performance-fairness trade-off in more than a dozen fine-tuned\nLMs for toxic text classification. We empirically show that no blanket\nstatement can be made with respect to the bias of large versus regular versus\ncompressed models. Moreover, we find that focusing on fairness-agnostic\nperformance metrics can lead to models with varied fairness characteristics.",
          "link": "http://arxiv.org/abs/2108.01250",
          "publishedOn": "2021-08-04T01:59:22.880Z",
          "wordCount": 506,
          "title": "Your fairness may vary: Group fairness of pretrained language models in toxic text classification. (arXiv:2108.01250v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.08920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dewanto_V/0/1/0/all/0/1\">Vektor Dewanto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunn_G/0/1/0/all/0/1\">George Dunn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eshragh_A/0/1/0/all/0/1\">Ali Eshragh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallagher_M/0/1/0/all/0/1\">Marcus Gallagher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roosta_F/0/1/0/all/0/1\">Fred Roosta</a>",
          "description": "Reinforcement learning is important part of artificial intelligence. In this\npaper, we review model-free reinforcement learning that utilizes the average\nreward optimality criterion in the infinite horizon setting. Motivated by the\nsolo survey by Mahadevan (1996a), we provide an updated review of work in this\narea and extend it to cover policy-iteration and function approximation methods\n(in addition to the value-iteration and tabular counterparts). We present a\ncomprehensive literature mapping. We also identify and discuss opportunities\nfor future work.",
          "link": "http://arxiv.org/abs/2010.08920",
          "publishedOn": "2021-08-04T01:59:22.872Z",
          "wordCount": 555,
          "title": "Average-reward model-free reinforcement learning: a systematic review and literature mapping. (arXiv:2010.08920v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03702",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Solano_Carrillo_E/0/1/0/all/0/1\">Edgardo Solano-Carrillo</a>",
          "description": "A novel non-parametric quantile estimation method for continuous random\nvariables is introduced, based on a minimal neural network architecture\nconsisting of a single unit. Its advantage over estimations from ranking the\norder statistics is shown, specifically for small sample size. In a regression\ncontext, the method can be used to quantify predictive uncertainty under the\nsplit conformal prediction setting, where prediction intervals are estimated\nfrom the residuals of a pre-trained model on a held-out validation set to\nquantify the uncertainty in future predictions. Benchmarking experiments\ndemonstrate that the method is competitive in quality and coverage with\nstate-of-the-art solutions, with the added benefit of being more\ncomputationally efficient.",
          "link": "http://arxiv.org/abs/2106.03702",
          "publishedOn": "2021-08-04T01:59:22.860Z",
          "wordCount": 560,
          "title": "Can a single neuron learn quantiles?. (arXiv:2106.03702v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mauritz_R/0/1/0/all/0/1\">R.R. Mauritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nijweide_F/0/1/0/all/0/1\">F.P.J. Nijweide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goseling_J/0/1/0/all/0/1\">J. Goseling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keulen_M/0/1/0/all/0/1\">M. van Keulen</a>",
          "description": "Data quality problems are a large threat in data science. In this paper, we\npropose a data-cleaning autoencoder capable of near-automatic data quality\nimprovement. It learns the structure and dependencies in the data and uses it\nas evidence to identify and correct doubtful values. We apply a probabilistic\ndatabase approach to represent weak and strong evidence for attribute value\nrepairs. A theoretical framework is provided, and experiments show that it can\nremove significant amounts of noise (i.e., data quality problems) from\ncategorical and numeric probabilistic data. Our method does not require clean\ndata. We do, however, show that manually cleaning a small fraction of the data\nsignificantly improves performance.",
          "link": "http://arxiv.org/abs/2106.09764",
          "publishedOn": "2021-08-04T01:59:22.853Z",
          "wordCount": 601,
          "title": "A probabilistic database approach to autoencoder-based data cleaning. (arXiv:2106.09764v2 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weizhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>",
          "description": "Modern methods for counting people in crowded scenes rely on deep networks to\nestimate people densities in individual images. As such, only very few take\nadvantage of temporal consistency in video sequences, and those that do only\nimpose weak smoothness constraints across consecutive frames. In this paper, we\nadvocate estimating people flows across image locations between consecutive\nimages and inferring the people densities from these flows instead of directly\nregressing them. This enables us to impose much stronger constraints encoding\nthe conservation of the number of people. As a result, it significantly boosts\nperformance without requiring a more complex architecture. Furthermore, it\nallows us to exploit the correlation between people flow and optical flow to\nfurther improve the results. We also show that leveraging people conservation\nconstraints in both a spatial and temporal manner makes it possible to train a\ndeep crowd counting model in an active learning setting with much fewer\nannotations. This significantly reduces the annotation cost while still leading\nto similar performance to the full supervision case.",
          "link": "http://arxiv.org/abs/2012.00452",
          "publishedOn": "2021-08-04T01:59:22.847Z",
          "wordCount": 650,
          "title": "Counting People by Estimating People Flows. (arXiv:2012.00452v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Songting Shi</a>",
          "description": "We present a new method GTSNE to visualize high-dimensional data points in\nthe two dimensional map. The technique is a variation of t-SNE that produces\nbetter visualizations by capturing both the local neighborhood structure and\nthe macro structure in the data. This is particularly important for\nhigh-dimensional data that lie on continuous low-dimensional manifolds. We\nillustrate the performance of GTSNE on a wide variety of datasets and compare\nit the state of art methods, including t-SNE and UMAP. The visualizations\nproduced by GTSNE are better than those produced by the other techniques on\nalmost all of the datasets on the macro structure preservation.",
          "link": "http://arxiv.org/abs/2108.01301",
          "publishedOn": "2021-08-04T01:59:22.829Z",
          "wordCount": 533,
          "title": "Visualizing Data using GTSNE. (arXiv:2108.01301v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wanpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xi Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingzhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Dijun Luo</a>",
          "description": "Model-based reinforcement learning is a widely accepted solution for solving\nexcessive sample demands. However, the predictions of the dynamics models are\noften not accurate enough, and the resulting bias may incur catastrophic\ndecisions due to insufficient robustness. Therefore, it is highly desired to\ninvestigate how to improve the robustness of model-based RL algorithms while\nmaintaining high sampling efficiency. In this paper, we propose Model-Based\nDouble-dropout Planning (MBDP) to balance robustness and efficiency. MBDP\nconsists of two kinds of dropout mechanisms, where the rollout-dropout aims to\nimprove the robustness with a small cost of sample efficiency, while the\nmodel-dropout is designed to compensate for the lost efficiency at a slight\nexpense of robustness. By combining them in a complementary way, MBDP provides\na flexible control mechanism to meet different demands of robustness and\nefficiency by tuning two corresponding dropout ratios. The effectiveness of\nMBDP is demonstrated both theoretically and experimentally.",
          "link": "http://arxiv.org/abs/2108.01295",
          "publishedOn": "2021-08-04T01:59:22.814Z",
          "wordCount": 595,
          "title": "MBDP: A Model-based Approach to Achieve both Robustness and Sample Efficiency via Double Dropout Planning. (arXiv:2108.01295v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01267",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pishgar_M/0/1/0/all/0/1\">Maryam Pishgar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razo_M/0/1/0/all/0/1\">Martha Razo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theis_J/0/1/0/all/0/1\">Julian Theis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darabi_H/0/1/0/all/0/1\">Houshang Darabi</a>",
          "description": "Paralytic Ileus (PI) patients are at high risk of death when admitted to the\nIntensive care unit (ICU), with mortality as high as 40\\%. There is minimal\nresearch concerning PI patient mortality prediction. There is a need for more\naccurate prediction modeling for ICU patients diagnosed with PI. This paper\ndemonstrates performance improvements in predicting the mortality of ICU\npatients diagnosed with PI after 24 hours of being admitted. The proposed\nframework, PMPI(Process Mining Model to predict mortality of PI patients), is a\nmodification of the work used for prediction of in-hospital mortality for ICU\npatients with diabetes. PMPI demonstrates similar if not better performance\nwith an Area under the ROC Curve (AUC) score of 0.82 compared to the best\nresults of the existing literature. PMPI uses patient medical history, the time\nrelated to the events, and demographic information for prediction. The PMPI\nprediction framework has the potential to help medical teams in making better\ndecisions for treatment and care for ICU patients with PI to increase their\nlife expectancy.",
          "link": "http://arxiv.org/abs/2108.01267",
          "publishedOn": "2021-08-04T01:59:22.808Z",
          "wordCount": 607,
          "title": "Process Mining Model to Predict Mortality in Paralytic Ileus Patients. (arXiv:2108.01267v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tongxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1\">Songling Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jiye Qiu</a>",
          "description": "Recently, deep learning has been an area of intense researching. However, as\na kind of computing intensive task, deep learning highly relies on the scale of\nGPU memory, which is usually prohibitive and scarce. Although there are some\nextensive works have been proposed for dynamic GPU memory management, they are\nhard to be applied to systems with multiple dynamic workloads, such as\nin-database machine learning system.\n\nIn this paper, we demonstrated TENSILE, a method of managing GPU memory in\ntensor granularity to reduce the GPU memory peak, with taking the multiple\ndynamic workloads into consideration. As far as we know, TENSILE is the first\nmethod which is designed to manage multiple workloads' GPU memory using. We\nimplement TENSILE on a deep learning framework built by ourselves, and\nevaluated its performance. The experiment results show that TENSILE can save\nmore GPU memory with less extra time overhead than prior works in both single\nand multiple dynamic workloads scenarios.",
          "link": "http://arxiv.org/abs/2105.13336",
          "publishedOn": "2021-08-04T01:59:22.795Z",
          "wordCount": 663,
          "title": "TENSILE: A Tensor granularity dynamic GPU memory scheduling method towards multiple dynamic workloads system. (arXiv:2105.13336v3 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1\">Lei Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>",
          "description": "Controlling the style of natural language by disentangling the latent space\nis an important step towards interpretable machine learning. After the latent\nspace is disentangled, the style of a sentence can be transformed by tuning the\nstyle representation without affecting other features of the sentence. Previous\nworks usually use adversarial training to guarantee that disentangled vectors\ndo not affect each other. However, adversarial methods are difficult to train.\nEspecially when there are multiple features (e.g., sentiment, or tense, which\nwe call style types in this paper), each feature requires a separate\ndiscriminator for extracting a disentangled style vector corresponding to that\nfeature. In this paper, we propose a unified distribution-controlling method,\nwhich provides each specific style value (the value of style types, e.g.,\npositive sentiment, or past tense) with a unique representation. This method\ncontributes a solid theoretical basis to avoid adversarial training in\nmulti-type disentanglement. We also propose multiple loss functions to achieve\na style-content disentanglement as well as a disentanglement among multiple\nstyle types. In addition, we observe that if two different style types always\nhave some specific style values that occur together in the dataset, they will\naffect each other when transferring the style values. We call this phenomenon\ntraining bias, and we propose a loss function to alleviate such training bias\nwhile disentangling multiple types. We conduct experiments on two datasets\n(Yelp service reviews and Amazon product reviews) to evaluate the\nstyle-disentangling effect and the unsupervised style transfer performance on\ntwo style types: sentiment and tense. The experimental results show the\neffectiveness of our model.",
          "link": "http://arxiv.org/abs/2012.08883",
          "publishedOn": "2021-08-04T01:59:22.730Z",
          "wordCount": 724,
          "title": "Multi-type Disentanglement without Adversarial Training. (arXiv:2012.08883v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pfeil_T/0/1/0/all/0/1\">Thomas Pfeil</a>",
          "description": "Deep neural networks have usually to be compressed and accelerated for their\nusage in low-power, e.g. mobile, devices. Recently, massively-parallel hardware\naccelerators were developed that offer high throughput and low latency at low\npower by utilizing in-memory computation. However, to exploit these benefits\nthe computational graph of a neural network has to fit into the in-computation\nmemory of these hardware systems that is usually rather limited in size. In\nthis study, we introduce a class of network models that have a small memory\nfootprint in terms of their computational graphs. To this end, the graph is\ndesigned to contain loops by iteratively executing a single network building\nblock. Furthermore, the trade-off between accuracy and latency of these\nso-called iterative neural networks is improved by adding multiple intermediate\noutputs during both training and inference. We show state-of-the-art results\nfor semantic segmentation on the CamVid and Cityscapes datasets that are\nespecially demanding in terms of computational resources. In ablation studies,\nthe improvement of network training by intermediate network outputs as well as\nthe trade-off between weight sharing over iterations and the network size are\ninvestigated.",
          "link": "http://arxiv.org/abs/2101.08685",
          "publishedOn": "2021-08-04T01:59:22.724Z",
          "wordCount": 675,
          "title": "ItNet: iterative neural networks with small graphs for accurate, efficient and anytime semantic segmentation. (arXiv:2101.08685v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12740",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shengzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marwah_M/0/1/0/all/0/1\">Manish Marwah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arlitt_M/0/1/0/all/0/1\">Martin Arlitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_N/0/1/0/all/0/1\">Naren Ramakrishnan</a>",
          "description": "Deep learning models have achieved great success in recent years but progress\nin some domains like cybersecurity is stymied due to a paucity of realistic\ndatasets. Organizations are reluctant to share such data, even internally, due\nto privacy reasons. An alternative is to use synthetically generated data but\nexisting methods are limited in their ability to capture complex dependency\nstructures, between attributes and across time. This paper presents STAN\n(Synthetic network Traffic generation with Autoregressive Neural models), a\ntool to generate realistic synthetic network traffic datasets for subsequent\ndownstream applications. Our novel neural architecture captures both temporal\ndependencies and dependence between attributes at any given time. It integrates\nconvolutional neural layers with mixture density neural layers and softmax\nlayers, and models both continuous and discrete variables. We evaluate the\nperformance of STAN in terms of the quality of data generated, by training it\non both a simulated dataset and a real network traffic data set. Finally, to\nanswer the question - can real network traffic data be substituted with\nsynthetic data to train models of comparable accuracy? We train two anomaly\ndetection models based on self-supervision. The results show only a small\ndecline in the accuracy of models trained solely on synthetic data. While\ncurrent results are encouraging in terms of quality of data generated and\nabsence of any obvious data leakage from training data, in the future we plan\nto further validate this fact by conducting privacy attacks on the generated\ndata. Other future work includes validating capture of long term dependencies\nand making model training",
          "link": "http://arxiv.org/abs/2009.12740",
          "publishedOn": "2021-08-04T01:59:22.695Z",
          "wordCount": 721,
          "title": "STAN: Synthetic Network Traffic Generation with Generative Neural Models. (arXiv:2009.12740v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01584",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Galaris_E/0/1/0/all/0/1\">Evangelos Galaris</a>, <a href=\"http://arxiv.org/find/math/1/au:+Calabro_F/0/1/0/all/0/1\">Francesco Calabr&#xf2;</a>, <a href=\"http://arxiv.org/find/math/1/au:+Serafino_D/0/1/0/all/0/1\">Daniela di Serafino</a>, <a href=\"http://arxiv.org/find/math/1/au:+Siettos_C/0/1/0/all/0/1\">Constantinos Siettos</a>",
          "description": "We propose a numerical scheme based on Random Projection Neural Networks\n(RPNN) for the solution of Ordinary Differential Equations (ODEs) with a focus\non stiff problems. In particular, we use an Extreme Learning Machine, a\nsingle-hidden layer Feedforward Neural Network with Radial Basis Functions\nwhich widths are uniformly distributed random variables, while the values of\nthe weights between the input and the hidden layer are set equal to one. The\nnumerical solution is obtained by constructing a system of nonlinear algebraic\nequations, which is solved with respect to the output weights using the\nGauss-Newton method. For our illustrations, we apply the proposed machine\nlearning approach to solve two benchmark stiff problems, namely the Rober and\nthe van der Pol ones (the latter with large values of the stiffness parameter),\nand we perform a comparison with well-established methods such as the adaptive\nRunge-Kutta method based on the Dormand-Prince pair, and a variable-step\nvariable-order multistep solver based on numerical differentiation formulas, as\nimplemented in the \\texttt{ode45} and \\texttt{ode15s} MATLAB functions,\nrespectively. We show that our proposed scheme yields good numerical\napproximation accuracy without being affected by the stiffness, thus\noutperforming in same cases the \\texttt{ode45} and \\texttt{ode15s} functions.\nImportantly, upon training using a fixed number of collocation points, the\nproposed scheme approximates the solution in the whole domain in contrast to\nthe classical time integration methods.",
          "link": "http://arxiv.org/abs/2108.01584",
          "publishedOn": "2021-08-04T01:59:22.659Z",
          "wordCount": 669,
          "title": "Numerical Solution of Stiff Ordinary Differential Equations with Random Projection Neural Networks. (arXiv:2108.01584v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_D/0/1/0/all/0/1\">Ding Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_B/0/1/0/all/0/1\">Becky West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiquan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jinzhou Huang</a>",
          "description": "E-commerce sites strive to provide users the most timely relevant information\nin order to reduce shopping frictions and increase customer satisfaction. Multi\narmed bandit models (MAB) as a type of adaptive optimization algorithms provide\npossible approaches for such purposes. In this paper, we analyze using three\nclassic MAB algorithms, epsilon-greedy, Thompson sampling (TS), and upper\nconfidence bound 1 (UCB1) for dynamic content recommendations, and walk through\nthe process of developing these algorithms internally to solve a real world\ne-commerce use case. First, we analyze the three MAB algorithms using simulated\npurchasing datasets with non-stationary reward distributions to simulate the\npossible time-varying customer preferences, where the traffic allocation\ndynamics and the accumulative rewards of different algorithms are studied.\nSecond, we compare the accumulative rewards of the three MAB algorithms with\nmore than 1,000 trials using actual historical A/B test datasets. We find that\nthe larger difference between the success rates of competing recommendations\nthe more accumulative rewards the MAB algorithms can achieve. In addition, we\nfind that TS shows the highest average accumulative rewards under different\ntesting scenarios. Third, we develop a batch-updated MAB algorithm to overcome\nthe delayed reward issue in e-commerce and enable an online content\noptimization on our App homepage. For a state-of-the-art comparison, a real A/B\ntest among our batch-updated MAB algorithm, a third-party MAB solution, and the\ndefault business logic are conducted. The result shows that our batch-updated\nMAB algorithm outperforms the counterparts and achieves 6.13% relative\nclick-through rate (CTR) increase and 16.1% relative conversion rate (CVR)\nincrease compared to the default experience, and 2.9% relative CTR increase and\n1.4% relative CVR increase compared to the external MAB service.",
          "link": "http://arxiv.org/abs/2108.01440",
          "publishedOn": "2021-08-04T01:59:22.635Z",
          "wordCount": 719,
          "title": "Adaptively Optimize Content Recommendation Using Multi Armed Bandit Algorithms in E-commerce. (arXiv:2108.01440v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Han Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhanning Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Peiran Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lizhen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "Noisy labels are commonly found in real-world data, which cause performance\ndegradation of deep neural networks. Cleaning data manually is labour-intensive\nand time-consuming. Previous research mostly focuses on enhancing\nclassification models against noisy labels, while the robustness of deep metric\nlearning (DML) against noisy labels remains less well-explored. In this paper,\nwe bridge this important gap by proposing Probabilistic Ranking-based Instance\nSelection with Memory (PRISM) approach for DML. PRISM calculates the\nprobability of a label being clean, and filters out potentially noisy samples.\nSpecifically, we propose three methods to calculate this probability: 1)\nAverage Similarity Method (AvgSim), which calculates the average similarity\nbetween potentially noisy data and clean data; 2) Proxy Similarity Method\n(ProxySim), which replaces the centers maintained by AvgSim with the proxies\ntrained by proxy-based method; and 3) von Mises-Fisher Distribution Similarity\n(vMF-Sim), which estimates a von Mises-Fisher distribution for each data class.\nWith such a design, the proposed approach can deal with challenging DML\nsituations in which the majority of the samples are noisy. Extensive\nexperiments on both synthetic and real-world noisy dataset show that the\nproposed approach achieves up to 8.37% higher Precision@1 compared with the\nbest performing state-of-the-art baseline approaches, within reasonable\ntraining time.",
          "link": "http://arxiv.org/abs/2108.01431",
          "publishedOn": "2021-08-04T01:59:22.629Z",
          "wordCount": 657,
          "title": "Noise-Resistant Deep Metric Learning with Probabilistic Instance Filtering. (arXiv:2108.01431v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pathak_M/0/1/0/all/0/1\">Manish Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aditya Jain</a>",
          "description": "Recommendation engines are integral to the modern e-commerce experience, both\nfor the seller and the end user. Accurate recommendations lead to higher\nrevenue and better user experience. In this paper, we are presenting our\nsolution to ECML PKDD Farfetch Fashion Recommendation Challenge.The goal of\nthis challenge is to maximize the chances of a click when the users are\npresented with set of fashion items. We have approached this problem as a\nbinary classification problem. Our winning solution utilizes Catboost as the\nclassifier and Bayesian Optimization for hyper parameter tuning. Our baseline\nmodel achieved MRR of 0.5153 on the validation set. Bayesian optimization of\nhyper parameters improved the MRR to 0.5240 on the validation set. Our final\nsubmission on the test set achieved a MRR of 0.5257.",
          "link": "http://arxiv.org/abs/2108.01314",
          "publishedOn": "2021-08-04T01:59:22.567Z",
          "wordCount": 564,
          "title": "Solving Fashion Recommendation -- The Farfetch Challenge. (arXiv:2108.01314v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rezk_N/0/1/0/all/0/1\">Nesma M. Rezk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nordstrom_T/0/1/0/all/0/1\">Tomas Nordstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stathis_D/0/1/0/all/0/1\">Dimitrios Stathis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ul_Abdin_Z/0/1/0/all/0/1\">Zain Ul-Abdin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aksoy_E/0/1/0/all/0/1\">Eren Erdal Aksoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemani_A/0/1/0/all/0/1\">Ahmed Hemani</a>",
          "description": "The compression of deep learning models is of fundamental importance in\ndeploying such models to edge devices. Incorporating hardware model and\napplication constraints during compression maximizes the benefits but makes it\nspecifically designed for one case. Therefore, the compression needs to be\nautomated. Searching for the optimal compression method parameters is\nconsidered an optimization problem. This article introduces a Multi-Objective\nHardware-Aware Quantization (MOHAQ) method, which considers both hardware\nefficiency and inference error as objectives for mixed-precision quantization.\nThe proposed method makes the evaluation of candidate solutions in a large\nsearch space feasible by relying on two steps. First, post-training\nquantization is applied for fast solution evaluation. Second, we propose a\nsearch technique named \"beacon-based search\" to retrain selected solutions only\nin the search space and use them as beacons to know the effect of retraining on\nother solutions. To evaluate the optimization potential, we chose a speech\nrecognition model using the TIMIT dataset. The model is based on Simple\nRecurrent Unit (SRU) due to its considerable speedup over other recurrent\nunits. We applied our method to run on two platforms: SiLago and Bitfusion.\nExperimental evaluations showed that SRU can be compressed up to 8x by\npost-training quantization without any significant increase in the error and up\nto 12x with only a 1.5 percentage point increase in error. On SiLago, the\ninference-only search found solutions that achieve 80\\% and 64\\% of the maximum\npossible speedup and energy saving, respectively, with a 0.5 percentage point\nincrease in the error. On Bitfusion, with a constraint of a small SRAM size,\nbeacon-based search reduced the error gain of inference-only search by 4\npercentage points and increased the possible reached speedup to be 47x compared\nto the Bitfusion baseline.",
          "link": "http://arxiv.org/abs/2108.01192",
          "publishedOn": "2021-08-04T01:59:22.560Z",
          "wordCount": 732,
          "title": "Multi-objective Recurrent Neural Networks Optimization for the Edge -- a Quantization-based Approach. (arXiv:2108.01192v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mok_J/0/1/0/all/0/1\">Jisoo Mok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_B/0/1/0/all/0/1\">Byunggook Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_H/0/1/0/all/0/1\">Hyeokjun Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "Deep neural networks continue to awe the world with their remarkable\nperformance. Their predictions, however, are prone to be corrupted by\nadversarial examples that are imperceptible to humans. Current efforts to\nimprove the robustness of neural networks against adversarial examples are\nfocused on developing robust training methods, which update the weights of a\nneural network in a more robust direction. In this work, we take a step beyond\ntraining of the weight parameters and consider the problem of designing an\nadversarially robust neural architecture with high intrinsic robustness. We\npropose AdvRush, a novel adversarial robustness-aware neural architecture\nsearch algorithm, based upon a finding that independent of the training method,\nthe intrinsic robustness of a neural network can be represented with the\nsmoothness of its input loss landscape. Through a regularizer that favors a\ncandidate architecture with a smoother input loss landscape, AdvRush\nsuccessfully discovers an adversarially robust neural architecture. Along with\na comprehensive theoretical motivation for AdvRush, we conduct an extensive\namount of experiments to demonstrate the efficacy of AdvRush on various\nbenchmark datasets. Notably, on CIFAR-10, AdvRush achieves 55.91% robust\naccuracy under FGSM attack after standard training and 50.04% robust accuracy\nunder AutoAttack after 7-step PGD adversarial training.",
          "link": "http://arxiv.org/abs/2108.01289",
          "publishedOn": "2021-08-04T01:59:22.554Z",
          "wordCount": 630,
          "title": "AdvRush: Searching for Adversarially Robust Neural Architectures. (arXiv:2108.01289v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuhua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1\">Lexing Ying</a>",
          "description": "We introduce a class of variational actor-critic algorithms based on a\nvariational formulation over both the value function and the policy. The\nobjective function of the variational formulation consists of two parts: one\nfor maximizing the value function and the other for minimizing the Bellman\nresidual. Besides the vanilla gradient descent with both the value function and\nthe policy updates, we propose two variants, the clipping method and the\nflipping method, in order to speed up the convergence. We also prove that, when\nthe prefactor of the Bellman residual is sufficiently large, the fixed point of\nthe algorithm is close to the optimal policy.",
          "link": "http://arxiv.org/abs/2108.01215",
          "publishedOn": "2021-08-04T01:59:22.542Z",
          "wordCount": 525,
          "title": "Variational Actor-Critic Algorithms. (arXiv:2108.01215v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1\">Andrei-Marius Avram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1\">Vasile Pais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1\">Dan Tufis</a>",
          "description": "EuroVoc is a multilingual thesaurus that was built for organizing the\nlegislative documentary of the European Union institutions. It contains\nthousands of categories at different levels of specificity and its descriptors\nare targeted by legal texts in almost thirty languages. In this work we propose\na unified framework for EuroVoc classification on 22 languages by fine-tuning\nmodern Transformer-based pretrained language models. We study extensively the\nperformance of our trained models and show that they significantly improve the\nresults obtained by a similar tool - JEX - on the same dataset. The code and\nthe fine-tuned models were open sourced, together with a programmatic interface\nthat eases the process of loading the weights of a trained model and of\nclassifying a new document.",
          "link": "http://arxiv.org/abs/2108.01139",
          "publishedOn": "2021-08-04T01:59:22.532Z",
          "wordCount": 568,
          "title": "PyEuroVoc: A Tool for Multilingual Legal Document Classification with EuroVoc Descriptors. (arXiv:2108.01139v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1\">Matias Valdenegro-Toro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preciado_Grijalva_A/0/1/0/all/0/1\">Alan Preciado-Grijalva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wehbe_B/0/1/0/all/0/1\">Bilal Wehbe</a>",
          "description": "Machine learning and neural networks are now ubiquitous in sonar perception,\nbut it lags behind the computer vision field due to the lack of data and\npre-trained models specifically for sonar images. In this paper we present the\nMarine Debris Turntable dataset and produce pre-trained neural networks trained\non this dataset, meant to fill the gap of missing pre-trained models for sonar\nimages. We train Resnet 20, MobileNets, DenseNet121, SqueezeNet, MiniXception,\nand an Autoencoder, over several input image sizes, from 32 x 32 to 96 x 96, on\nthe Marine Debris turntable dataset. We evaluate these models using transfer\nlearning for low-shot classification in the Marine Debris Watertank and another\ndataset captured using a Gemini 720i sonar. Our results show that in both\ndatasets the pre-trained models produce good features that allow good\nclassification accuracy with low samples (10-30 samples per class). The Gemini\ndataset validates that the features transfer to other kinds of sonar sensors.\nWe expect that the community benefits from the public release of our\npre-trained models and the turntable dataset.",
          "link": "http://arxiv.org/abs/2108.01111",
          "publishedOn": "2021-08-04T01:59:22.513Z",
          "wordCount": 618,
          "title": "Pre-trained Models for Sonar Images. (arXiv:2108.01111v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schperberg_A/0/1/0/all/0/1\">Alexander Schperberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsuei_S/0/1/0/all/0/1\">Stephanie Tsuei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Dennis Hong</a>",
          "description": "We present an end-to-end online motion planning framework that uses a\ndata-driven approach to navigate a heterogeneous robot team towards a global\ngoal while avoiding obstacles in uncertain environments. First, we use\nstochastic model predictive control (SMPC) to calculate control inputs that\nsatisfy robot dynamics, and consider uncertainty during obstacle avoidance with\nchance constraints. Second, recurrent neural networks are used to provide a\nquick estimate of future state uncertainty considered in the SMPC finite-time\nhorizon solution, which are trained on uncertainty outputs of various\nsimultaneous localization and mapping algorithms. When two or more robots are\nin communication range, these uncertainties are then updated using a\ndistributed Kalman filtering approach. Lastly, a Deep Q-learning agent is\nemployed to serve as a high-level path planner, providing the SMPC with target\npositions that move the robots towards a desired global goal. Our complete\nmethods are demonstrated on a ground and aerial robot simultaneously (code\navailable at: https://github.com/AlexS28/SABER).",
          "link": "http://arxiv.org/abs/2108.01262",
          "publishedOn": "2021-08-04T01:59:22.440Z",
          "wordCount": 630,
          "title": "SABER: Data-Driven Motion Planner for Autonomously Navigating Heterogeneous Robots. (arXiv:2108.01262v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jooyoung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungbeom Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yonghyun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "Recent image generation models show remarkable generation performance.\nHowever, they mirror strong location preference in datasets, which we call\nspatial bias. Therefore, generators render poor samples at unseen locations and\nscales. We argue that the generators rely on their implicit positional encoding\nto render spatial content. From our observations, the generator's implicit\npositional encoding is translation-variant, making the generator spatially\nbiased. To address this issue, we propose injecting explicit positional\nencoding at each scale of the generator. By learning the spatially unbiased\ngenerator, we facilitate the robust use of generators in multiple tasks, such\nas GAN inversion, multi-scale generation, generation of arbitrary sizes and\naspect ratios. Furthermore, we show that our method can also be applied to\ndenoising diffusion probabilistic models.",
          "link": "http://arxiv.org/abs/2108.01285",
          "publishedOn": "2021-08-04T01:59:22.407Z",
          "wordCount": 554,
          "title": "Toward Spatially Unbiased Generative Models. (arXiv:2108.01285v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01220",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sidrane_C/0/1/0/all/0/1\">Chelsea Sidrane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maleki_A/0/1/0/all/0/1\">Amir Maleki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irfan_A/0/1/0/all/0/1\">Ahmed Irfan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>",
          "description": "Deep learning methods can be used to produce control policies, but certifying\ntheir safety is challenging. The resulting networks are nonlinear and often\nvery large. In response to this challenge, we present OVERT: a sound algorithm\nfor safety verification of nonlinear discrete-time closed loop dynamical\nsystems with neural network control policies. The novelty of OVERT lies in\ncombining ideas from the classical formal methods literature with ideas from\nthe newer neural network verification literature. The central concept of OVERT\nis to abstract nonlinear functions with a set of optimally tight piecewise\nlinear bounds. Such piecewise linear bounds are designed for seamless\nintegration into ReLU neural network verification tools. OVERT can be used to\nprove bounded-time safety properties by either computing reachable sets or\nsolving feasibility queries directly. We demonstrate various examples of safety\nverification for several classical benchmark examples. OVERT compares favorably\nto existing methods both in computation time and in tightness of the reachable\nset.",
          "link": "http://arxiv.org/abs/2108.01220",
          "publishedOn": "2021-08-04T01:59:22.401Z",
          "wordCount": 629,
          "title": "OVERT: An Algorithm for Safety Verification of Neural Network Control Policies for Nonlinear Systems. (arXiv:2108.01220v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01224",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>",
          "description": "We study a new challenging problem of efficient deployment for diverse tasks\nwith different resources, where the resource constraint and task of interest\ncorresponding to a group of classes are dynamically specified at testing time.\nPrevious NAS approaches seek to design architectures for all classes\nsimultaneously, which may not be optimal for some individual tasks. A\nstraightforward solution is to search an architecture from scratch for each\ndeployment scenario, which however is computation-intensive and impractical. To\naddress this, we present a novel and general framework, called Elastic\nArchitecture Search (EAS), permitting instant specializations at runtime for\ndiverse tasks with various resource constraints. To this end, we first propose\nto effectively train the over-parameterized network via a task dropout strategy\nto disentangle the tasks during training. In this way, the resulting model is\nrobust to the subsequent task dropping at inference time. Based on the\nwell-trained over-parameterized network, we then propose an efficient\narchitecture generator to obtain optimal architectures within a single forward\npass. Experiments on two image classification datasets show that EAS is able to\nfind more compact networks with better performance while remarkably being\norders of magnitude faster than state-of-the-art NAS methods. For example, our\nproposed EAS finds compact architectures within 0.1 second for 50 deployment\nscenarios.",
          "link": "http://arxiv.org/abs/2108.01224",
          "publishedOn": "2021-08-04T01:59:22.395Z",
          "wordCount": 660,
          "title": "Elastic Architecture Search for Diverse Tasks with Different Resources. (arXiv:2108.01224v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thornton_C/0/1/0/all/0/1\">Charles E. Thornton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buehrer_R/0/1/0/all/0/1\">R. Michael Buehrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martone_A/0/1/0/all/0/1\">Anthony F. Martone</a>",
          "description": "In tracking radar, the sensing environment often varies significantly over a\ntrack duration due to the target's trajectory and dynamic interference.\nAdapting the radar's waveform using partial information about the state of the\nscene has been shown to provide performance benefits in many practical\nscenarios. Moreover, radar measurements generally exhibit strong temporal\ncorrelation, allowing memory-based learning algorithms to effectively learn\nwaveform selection strategies. This work examines a radar system which builds a\ncompressed model of the radar-environment interface in the form of a\ncontext-tree. The radar uses this context tree-based model to select waveforms\nin a signal-dependent target channel, which may respond adversarially to the\nradar's strategy. This approach is guaranteed to asymptotically converge to the\naverage-cost optimal policy for any stationary target channel that can be\nrepresented as a Markov process of order U < $\\infty$, where the constant U is\nunknown to the radar. The proposed approach is tested in a simulation study,\nand is shown to provide tracking performance improvements over two\nstate-of-the-art waveform selection schemes.",
          "link": "http://arxiv.org/abs/2108.01181",
          "publishedOn": "2021-08-04T01:59:22.371Z",
          "wordCount": 628,
          "title": "Waveform Selection for Radar Tracking in Target Channels With Memory via Universal Learning. (arXiv:2108.01181v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01210",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ye_J/0/1/0/all/0/1\">Joel Ye</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pandarinath_C/0/1/0/all/0/1\">Chethan Pandarinath</a>",
          "description": "Neural population activity is theorized to reflect an underlying dynamical\nstructure. This structure can be accurately captured using state space models\nwith explicit dynamics, such as those based on recurrent neural networks\n(RNNs). However, using recurrence to explicitly model dynamics necessitates\nsequential processing of data, slowing real-time applications such as\nbrain-computer interfaces. Here we introduce the Neural Data Transformer (NDT),\na non-recurrent alternative. We test the NDT's ability to capture autonomous\ndynamical systems by applying it to synthetic datasets with known dynamics and\ndata from monkey motor cortex during a reaching task well-modeled by RNNs. The\nNDT models these datasets as well as state-of-the-art recurrent models.\nFurther, its non-recurrence enables 3.9ms inference, well within the loop time\nof real-time applications and more than 6 times faster than recurrent baselines\non the monkey reaching dataset. These results suggest that an explicit dynamics\nmodel is not necessary to model autonomous neural population dynamics. Code:\nhttps://github.com/snel-repo/neural-data-transformers",
          "link": "http://arxiv.org/abs/2108.01210",
          "publishedOn": "2021-08-04T01:59:22.365Z",
          "wordCount": 591,
          "title": "Representation learning for neural population activity with Neural Data Transformers. (arXiv:2108.01210v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01219",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Srinivasan_A/0/1/0/all/0/1\">Akshay Srinivasan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Todorov_E/0/1/0/all/0/1\">Emanuel Todorov</a>",
          "description": "Computing the Newton-step of a generic function with $N$ decision variables\ntakes $O(N^3)$ flops. In this paper, we show that given the computational graph\nof the function, this bound can be reduced to $O(m\\tau^3)$, where $\\tau, m$ are\nthe width and size of a tree-decomposition of the graph. The proposed algorithm\ngeneralizes nonlinear optimal-control methods based on LQR to general\noptimization problems and provides non-trivial gains in iteration-complexity\neven in cases where the Hessian is dense.",
          "link": "http://arxiv.org/abs/2108.01219",
          "publishedOn": "2021-08-04T01:59:22.359Z",
          "wordCount": 527,
          "title": "Computing the Newton-step faster than Hessian accumulation. (arXiv:2108.01219v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1\">Zadid Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Sakib Mahmud Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tine_J/0/1/0/all/0/1\">Jean Michel Tine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comert_A/0/1/0/all/0/1\">Ayse Turhan Comert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rice_D/0/1/0/all/0/1\">Diamon Rice</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comert_G/0/1/0/all/0/1\">Gurcan Comert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalaka_D/0/1/0/all/0/1\">Dimitra Michalaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mwakalonge_J/0/1/0/all/0/1\">Judith Mwakalonge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_R/0/1/0/all/0/1\">Reek Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Mashrur Chowdhury</a>",
          "description": "The efficiency and reliability of real-time incident detection models\ndirectly impact the affected corridors' traffic safety and operational\nconditions. The recent emergence of cloud-based quantum computing\ninfrastructure and innovations in noisy intermediate-scale quantum devices have\nrevealed a new era of quantum-enhanced algorithms that can be leveraged to\nimprove real-time incident detection accuracy. In this research, a hybrid\nmachine learning model, which includes classical and quantum machine learning\n(ML) models, is developed to identify incidents using the connected vehicle\n(CV) data. The incident detection performance of the hybrid model is evaluated\nagainst baseline classical ML models. The framework is evaluated using data\nfrom a microsimulation tool for different incident scenarios. The results\nindicate that a hybrid neural network containing a 4-qubit quantum layer\noutperforms all other baseline models when there is a lack of training data. We\nhave created three datasets; DS-1 with sufficient training data, and DS-2 and\nDS-3 with insufficient training data. The hybrid model achieves a recall of\n98.9%, 98.3%, and 96.6% for DS-1, DS-2, and DS-3, respectively. For DS-2 and\nDS-3, the average improvement in F2-score (measures model's performance to\ncorrectly identify incidents) achieved by the hybrid model is 1.9% and 7.8%,\nrespectively, compared to the classical models. It shows that with insufficient\ndata, which may be common for CVs, the hybrid ML model will perform better than\nthe classical models. With the continuing improvements of quantum computing\ninfrastructure, the quantum ML models could be a promising alternative for\nCV-related applications when the available data is insufficient.",
          "link": "http://arxiv.org/abs/2108.01127",
          "publishedOn": "2021-08-04T01:59:22.353Z",
          "wordCount": 709,
          "title": "Hybrid Quantum-Classical Neural Network for Incident Detection. (arXiv:2108.01127v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2005.06284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mirkes_E/0/1/0/all/0/1\">Evgeny M Mirkes</a>",
          "description": "Artificial Neural Networks (NN) are widely used for solving complex problems\nfrom medical diagnostics to face recognition. Despite notable successes, the\nmain disadvantages of NN are also well known: the risk of overfitting, lack of\nexplainability (inability to extract algorithms from trained NN), and high\nconsumption of computing resources. Determining the appropriate specific NN\nstructure for each problem can help overcome these difficulties: Too poor NN\ncannot be successfully trained, but too rich NN gives unexplainable results and\nmay have a high chance of overfitting. Reducing precision of NN parameters\nsimplifies the implementation of these NN, saves computing resources, and makes\nthe NN skills more transparent. This paper lists the basic NN simplification\nproblems and controlled pruning procedures to solve these problems. All the\ndescribed pruning procedures can be implemented in one framework. The developed\nprocedures, in particular, find the optimal structure of NN for each task,\nmeasure the influence of each input signal and NN parameter, and provide a\ndetailed verbal description of the algorithms and skills of NN. The described\nmethods are illustrated by a simple example: the generation of explicit\nalgorithms for predicting the results of the US presidential election.",
          "link": "http://arxiv.org/abs/2005.06284",
          "publishedOn": "2021-08-04T01:59:22.341Z",
          "wordCount": 656,
          "title": "Artificial Neural Network Pruning to Extract Knowledge. (arXiv:2005.06284v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01141",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abandah_G/0/1/0/all/0/1\">Gheith A. Abandah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suyyagh_A/0/1/0/all/0/1\">Ashraf Suyyagh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khedher_M/0/1/0/all/0/1\">Mohammed Z. Khedher</a>",
          "description": "Soft spelling errors are a class of spelling mistakes that is widespread\namong native Arabic speakers and foreign learners alike. Some of these errors\nare typographical in nature. They occur due to orthographic variations of some\nArabic letters and the complex rules that dictate their correct usage. Many\npeople forgo these rules, and given the identical phonetic sounds, they often\nconfuse such letters. In this paper, we propose a bidirectional long short-term\nmemory network that corrects this class of errors. We develop, train, evaluate,\nand compare a set of BiLSTM networks. We approach the spelling correction\nproblem at the character level. We handle Arabic texts from both classical and\nmodern standard Arabic. We treat the problem as a one-to-one sequence\ntranscription problem. Since the soft Arabic errors class encompasses omission\nand addition mistakes, to preserve the one-to-one sequence transcription, we\npropose a simple low-resource yet effective technique that maintains the\none-to-one sequencing and avoids using a costly encoder-decoder architecture.\nWe train the BiLSTM models to correct the spelling mistakes using transformed\ninput and stochastic error injection approaches. We recommend a configuration\nthat has two BiLSTM layers, uses the dropout regularization, and is trained\nusing the latter training approach with error injection rate of 40%. The best\nmodel corrects 96.4% of the injected errors and achieves a low character error\nrate of 1.28% on a real test set of soft spelling mistakes.",
          "link": "http://arxiv.org/abs/2108.01141",
          "publishedOn": "2021-08-04T01:59:22.303Z",
          "wordCount": 698,
          "title": "Correcting Arabic Soft Spelling Mistakes using BiLSTM-based Machine Learning. (arXiv:2108.01141v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Sakib Mahmud Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comert_G/0/1/0/all/0/1\">Gurcan Comert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Mashrur Chowdhury</a>",
          "description": "Connected vehicles (CVs), because of the external connectivity with other CVs\nand connected infrastructure, are vulnerable to cyberattacks that can instantly\ncompromise the safety of the vehicle itself and other connected vehicles and\nroadway infrastructure. One such cyberattack is the false information attack,\nwhere an external attacker injects inaccurate information into the connected\nvehicles and eventually can cause catastrophic consequences by compromising\nsafety-critical applications like the forward collision warning. The occurrence\nand target of such attack events can be very dynamic, making real-time and\nnear-real-time detection challenging. Change point models, can be used for\nreal-time anomaly detection caused by the false information attack. In this\npaper, we have evaluated three change point-based statistical models;\nExpectation Maximization, Cumulative Summation, and Bayesian Online Change\nPoint Algorithms for cyberattack detection in the CV data. Also, data-driven\nartificial intelligence (AI) models, which can be used to detect known and\nunknown underlying patterns in the dataset, have the potential of detecting a\nreal-time anomaly in the CV data. We have used six AI models to detect false\ninformation attacks and compared the performance for detecting the attacks with\nour developed change point models. Our study shows that change points models\nperformed better in real-time false information attack detection compared to\nthe performance of the AI models. Change point models having the advantage of\nno training requirements can be a feasible and computationally efficient\nalternative to AI models for false information attack detection in connected\nvehicles.",
          "link": "http://arxiv.org/abs/2108.01124",
          "publishedOn": "2021-08-04T01:59:22.297Z",
          "wordCount": 698,
          "title": "Efficacy of Statistical and Artificial Intelligence-based False Information Cyberattack Detection Models for Connected Vehicles. (arXiv:2108.01124v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thaker_P/0/1/0/all/0/1\">Parth K.Thaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_N/0/1/0/all/0/1\">Nikhil Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malu_M/0/1/0/all/0/1\">Mohit Malu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasarathy_G/0/1/0/all/0/1\">Gautam Dasarathy</a>",
          "description": "We study pure exploration in multi-armed bandits with graph side-information.\nIn particular, we consider the best arm (and near-best arm) identification\nproblem in the fixed confidence setting under the assumption that the arm\nrewards are smooth with respect to a given arbitrary graph. This captures a\nrange of real world pure-exploration scenarios where one often has information\nabout the similarity of the options or actions under consideration. We propose\na novel algorithm GRUB (GRaph based UcB) for this problem and provide a\ntheoretical characterization of its performance that elicits the benefit of the\ngraph-side information. We complement our theory with experimental results that\nshow that capitalizing on available graph side information yields significant\nimprovements over pure exploration methods that are unable to use this\ninformation.",
          "link": "http://arxiv.org/abs/2108.01152",
          "publishedOn": "2021-08-04T01:59:22.277Z",
          "wordCount": 564,
          "title": "Pure Exploration in Multi-armed Bandits with Graph Side Information. (arXiv:2108.01152v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Osanlou_K/0/1/0/all/0/1\">Kevin Osanlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guettier_C/0/1/0/all/0/1\">Christophe Guettier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1\">Andrei Bursuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cazenave_T/0/1/0/all/0/1\">Tristan Cazenave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacopin_E/0/1/0/all/0/1\">Eric Jacopin</a>",
          "description": "Learning-based methods are increasingly popular for search algorithms in\nsingle-criterion optimization problems. In contrast, for multiple-criteria\noptimization there are significantly fewer approaches despite the existence of\nnumerous applications. Constrained path-planning for Autonomous Ground Vehicles\n(AGV) is one such application, where an AGV is typically deployed in disaster\nrelief or search and rescue applications in off-road environments. The agent\ncan be faced with the following dilemma : optimize a source-destination path\naccording to a known criterion and an uncertain criterion under operational\nconstraints. The known criterion is associated to the cost of the path,\nrepresenting the distance. The uncertain criterion represents the feasibility\nof driving through the path without requiring human intervention. It depends on\nvarious external parameters such as the physics of the vehicle, the state of\nthe explored terrains or weather conditions. In this work, we leverage\nknowledge acquired through offline simulations by training a neural network\nmodel to predict the uncertain criterion. We integrate this model inside a\npath-planner which can solve problems online. Finally, we conduct experiments\non realistic AGV scenarios which illustrate that the proposed framework\nrequires human intervention less frequently, trading for a limited increase in\nthe path distance.",
          "link": "http://arxiv.org/abs/2108.01080",
          "publishedOn": "2021-08-04T01:59:22.270Z",
          "wordCount": 651,
          "title": "Learning-based Preference Prediction for Constrained Multi-Criteria Path-Planning. (arXiv:2108.01080v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_deMiguel_C/0/1/0/all/0/1\">Claudia Mart&#xed;nez-deMiguel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segura_Bedmar_I/0/1/0/all/0/1\">Isabel Segura-Bedmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chacon_Solano_E/0/1/0/all/0/1\">Esteban Chac&#xf3;n-Solano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_Aspizua_S/0/1/0/all/0/1\">Sara Guerrero-Aspizua</a>",
          "description": "The RareDis corpus contains more than 5,000 rare diseases and almost 6,000\nclinical manifestations are annotated. Moreover, the Inter Annotator Agreement\nevaluation shows a relatively high agreement (F1-measure equal to 83.5% under\nexact match criteria for the entities and equal to 81.3% for the relations).\nBased on these results, this corpus is of high quality, supposing a significant\nstep for the field since there is a scarcity of available corpus annotated with\nrare diseases. This could open the door to further NLP applications, which\nwould facilitate the diagnosis and treatment of these rare diseases and,\ntherefore, would improve dramatically the quality of life of these patients.",
          "link": "http://arxiv.org/abs/2108.01204",
          "publishedOn": "2021-08-04T01:59:22.264Z",
          "wordCount": 551,
          "title": "The RareDis corpus: a corpus annotated with rare diseases, their signs and symptoms. (arXiv:2108.01204v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01125",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Majumder_R/0/1/0/all/0/1\">Reek Majumder</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Khan_S/0/1/0/all/0/1\">Sakib Mahmud Khan</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ahmed_F/0/1/0/all/0/1\">Fahim Ahmed</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Khan_Z/0/1/0/all/0/1\">Zadid Khan</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ngeni_F/0/1/0/all/0/1\">Frank Ngeni</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Comert_G/0/1/0/all/0/1\">Gurcan Comert</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Mwakalonge_J/0/1/0/all/0/1\">Judith Mwakalonge</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Michalaka_D/0/1/0/all/0/1\">Dimitra Michalaka</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chowdhury_M/0/1/0/all/0/1\">Mashrur Chowdhury</a>",
          "description": "Image classification must work for autonomous vehicles (AV) operating on\npublic roads, and actions performed based on image misclassification can have\nserious consequences. Traffic sign images can be misclassified by an\nadversarial attack on machine learning models used by AVs for traffic sign\nrecognition. To make classification models resilient against adversarial\nattacks, we used a hybrid deep-learning model with both the quantum and\nclassical layers. Our goal is to study the hybrid deep-learning architecture\nfor classical-quantum transfer learning models to support the current era of\nintermediate-scale quantum technology. We have evaluated the impacts of various\nwhite box adversarial attacks on these hybrid models. The classical part of\nhybrid models includes a convolution network from the pre-trained Resnet18\nmodel, which extracts informative features from a high dimensional LISA traffic\nsign image dataset. The output from the classical processor is processed\nfurther through the quantum layer, which is composed of various quantum gates\nand provides support to various quantum mechanical features like entanglement\nand superposition. We have tested multiple combinations of quantum circuits to\nprovide better classification accuracy with decreasing training data and found\nbetter resiliency for our hybrid classical-quantum deep learning model during\nattacks compared to the classical-only machine learning models.",
          "link": "http://arxiv.org/abs/2108.01125",
          "publishedOn": "2021-08-04T01:59:22.257Z",
          "wordCount": 664,
          "title": "Hybrid Classical-Quantum Deep Learning Models for Autonomous Vehicle Traffic Image Classification Under Adversarial Attack. (arXiv:2108.01125v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01099",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponomareva_N/0/1/0/all/0/1\">Natalia Ponomareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perozzi_B/0/1/0/all/0/1\">Bryan Perozzi</a>",
          "description": "There has been a recent surge of interest in designing Graph Neural Networks\n(GNNs) for semi-supervised learning tasks. Unfortunately this work has assumed\nthat the nodes labeled for use in training were selected uniformly at random\n(i.e. are an IID sample). However in many real world scenarios gathering labels\nfor graph nodes is both expensive and inherently biased -- so this assumption\ncan not be met. GNNs can suffer poor generalization when this occurs, by\noverfitting to superfluous regularities present in the training data. In this\nwork we present a method, Shift-Robust GNN (SR-GNN), designed to account for\ndistributional differences between biased training data and the graph's true\ninference distribution. SR-GNN adapts GNN models for the presence of\ndistributional shifts between the nodes which have had labels provided for\ntraining and the rest of the dataset. We illustrate the effectiveness of SR-GNN\nin a variety of experiments with biased training datasets on common GNN\nbenchmark datasets for semi-supervised learning, where we see that SR-GNN\noutperforms other GNN baselines by accuracy, eliminating at least (~40%) of the\nnegative effects introduced by biased training data. On the largest dataset we\nconsider, ogb-arxiv, we observe an 2% absolute improvement over the baseline\nand reduce 30% of the negative effects.",
          "link": "http://arxiv.org/abs/2108.01099",
          "publishedOn": "2021-08-04T01:59:22.206Z",
          "wordCount": 640,
          "title": "Shift-Robust GNNs: Overcoming the Limitations of Localized Graph Training data. (arXiv:2108.01099v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lange_S/0/1/0/all/0/1\">Susanna Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helfrich_K/0/1/0/all/0/1\">Kyle Helfrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qiang Ye</a>",
          "description": "Batch normalization (BN) is a popular and ubiquitous method in deep learning\nthat has been shown to decrease training time and improve generalization\nperformance of neural networks. Despite its success, BN is not theoretically\nwell understood. It is not suitable for use with very small mini-batch sizes or\nonline learning. In this paper, we propose a new method called Batch\nNormalization Preconditioning (BNP). Instead of applying normalization\nexplicitly through a batch normalization layer as is done in BN, BNP applies\nnormalization by conditioning the parameter gradients directly during training.\nThis is designed to improve the Hessian matrix of the loss function and hence\nconvergence during training. One benefit is that BNP is not constrained on the\nmini-batch size and works in the online learning setting. Furthermore, its\nconnection to BN provides theoretical insights on how BN improves training and\nhow BN is applied to special architectures such as convolutional neural\nnetworks.",
          "link": "http://arxiv.org/abs/2108.01110",
          "publishedOn": "2021-08-04T01:59:22.179Z",
          "wordCount": 594,
          "title": "Batch Normalization Preconditioning for Neural Network Training. (arXiv:2108.01110v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1\">Ron Shmelkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1\">Tomer Friedlander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "A master face is a face image that passes face-based identity-authentication\nfor a large portion of the population. These faces can be used to impersonate,\nwith a high probability of success, any user, without having access to any user\ninformation. We optimize these faces, by using an evolutionary algorithm in the\nlatent embedding space of the StyleGAN face generator. Multiple evolutionary\nstrategies are compared, and we propose a novel approach that employs a neural\nnetwork in order to direct the search in the direction of promising samples,\nwithout adding fitness evaluations. The results we present demonstrate that it\nis possible to obtain a high coverage of the population (over 40%) with less\nthan 10 master faces, for three leading deep face recognition systems.",
          "link": "http://arxiv.org/abs/2108.01077",
          "publishedOn": "2021-08-04T01:59:22.140Z",
          "wordCount": 590,
          "title": "Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution. (arXiv:2108.01077v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Souza_J/0/1/0/all/0/1\">Jefferson Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "This work investigates the use of two-stage clustering methods. Four\ntechniques were proposed: SOMK, SOMAK, ASCAK and SOINAK. SOMK is composed of a\nSOM (Self-Organizing Maps) followed by the K-means algorithm, SOMAK is a\ncombination of SOM followed by the Ant K-means (AK) algorithm, ASCAK is\ncomposed by the ASCA (Ant System-based Clustering Algorithm) and AK algorithms,\nSOINAK is composed by the Self-Organizing Incremental Neural Network (SOINN)\nand AK. SOINAK presented a better performance among the four proposed\ntechniques when applied to pattern recognition problems.",
          "link": "http://arxiv.org/abs/2108.01123",
          "publishedOn": "2021-08-04T01:59:22.124Z",
          "wordCount": 522,
          "title": "Metodos de Agrupamentos em dois Estagios. (arXiv:2108.01123v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Siyang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harding_S/0/1/0/all/0/1\">Seth Austin Harding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haibin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shih-wei Liao</a>",
          "description": "Many complex multi-robot systems such as robot swarms control and autonomous\nvehicle coordination can be modeled as Multi-Agent Reinforcement Learning\n(MARL) tasks. QMIX, a widely popular MARL algorithm, has been used as a\nbaseline for the benchmark environments, e.g., Starcraft Multi-Agent Challenge\n(SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX\ntarget relaxing the monotonicity constraint of QMIX, allowing for performance\nimprovement in SMAC. In this paper, we investigate the code-level optimizations\nof these variants and the monotonicity constraint. (1) We find that such\nimprovements of the variants are significantly affected by various code-level\noptimizations. (2) The experiment results show that QMIX with normalized\noptimizations outperforms other works in SMAC; (3) beyond the common wisdom\nfrom these works, the monotonicity constraint can improve sample efficiency in\nSMAC and DEPP. We also discuss why monotonicity constraints work well in purely\ncooperative tasks with a theoretical analysis. We open-source the code at\n\\url{https://github.com/hijkzzz/pymarl2}.",
          "link": "http://arxiv.org/abs/2102.03479",
          "publishedOn": "2021-08-03T02:06:35.290Z",
          "wordCount": 731,
          "title": "Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2102.03479v13 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuying Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lei Bai Liang Lin</a>",
          "description": "Metro origin-destination prediction is a crucial yet challenging time-series\nanalysis task in intelligent transportation systems, which aims to accurately\nforecast two specific types of cross-station ridership, i.e.,\nOrigin-Destination (OD) one and Destination-Origin (DO) one. However, complete\nOD matrices of previous time intervals can not be obtained immediately in\nonline metro systems, and conventional methods only used limited information to\nforecast the future OD and DO ridership separately. In this work, we proposed a\nnovel neural network module termed Heterogeneous Information Aggregation\nMachine (HIAM), which fully exploits heterogeneous information of historical\ndata (e.g., incomplete OD matrices, unfinished order vectors, and DO matrices)\nto jointly learn the evolutionary patterns of OD and DO ridership.\nSpecifically, an OD modeling branch estimates the potential destinations of\nunfinished orders explicitly to complement the information of incomplete OD\nmatrices, while a DO modeling branch takes DO matrices as input to capture the\nspatial-temporal distribution of DO ridership. Moreover, a Dual Information\nTransformer is introduced to propagate the mutual information among OD features\nand DO features for modeling the OD-DO causality and correlation. Based on the\nproposed HIAM, we develop a unified Seq2Seq network to forecast the future OD\nand DO ridership simultaneously. Extensive experiments conducted on two\nlarge-scale benchmarks demonstrate the effectiveness of our method for online\nmetro origin-destination prediction.",
          "link": "http://arxiv.org/abs/2107.00946",
          "publishedOn": "2021-08-03T02:06:35.235Z",
          "wordCount": 688,
          "title": "Online Metro Origin-Destination Prediction via Heterogeneous Information Aggregation. (arXiv:2107.00946v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00781",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hodgkinson_L/0/1/0/all/0/1\">Liam Hodgkinson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1\">Umut &#x15e;im&#x15f;ekli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Khanna_R/0/1/0/all/0/1\">Rajiv Khanna</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>",
          "description": "Despite the ubiquitous use of stochastic optimization algorithms in machine\nlearning, the precise impact of these algorithms on generalization performance\nin realistic non-convex settings is still poorly understood. In this paper, we\nprovide an encompassing theoretical framework for investigating the\ngeneralization properties of stochastic optimizers, which is based on their\ndynamics. We first prove a generalization bound attributable to the optimizer\ndynamics in terms of the celebrated Fernique-Talagrand functional applied to\nthe trajectory of the optimizer. This data- and algorithm-dependent bound is\nshown to be the sharpest possible in the absence of further assumptions. We\nthen specialize this result by exploiting the Markovian structure of stochastic\noptimizers, deriving generalization bounds in terms of the (data-dependent)\ntransition kernels associated with the optimization algorithms. In line with\nrecent work that has revealed connections between generalization and\nheavy-tailed behavior in stochastic optimization, we link the generalization\nerror to the local tail behavior of the transition kernels. We illustrate that\nthe local power-law exponent of the kernel acts as an effective dimension,\nwhich decreases as the transitions become \"less Gaussian\". We support our\ntheory with empirical results from a variety of neural networks, and we show\nthat both the Fernique-Talagrand functional and the local power-law exponent\nare predictive of generalization performance.",
          "link": "http://arxiv.org/abs/2108.00781",
          "publishedOn": "2021-08-03T02:06:35.185Z",
          "wordCount": 647,
          "title": "Generalization Properties of Stochastic Optimizers via Trajectory Analysis. (arXiv:2108.00781v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01672",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Vlontzos_A/0/1/0/all/0/1\">Athanasios Vlontzos</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cao_Y/0/1/0/all/0/1\">Yueqi Cao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schmidtke_L/0/1/0/all/0/1\">Luca Schmidtke</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Monod_A/0/1/0/all/0/1\">Anthea Monod</a>",
          "description": "Appropriately representing elements in a database so that queries may be\naccurately matched is a central task in information retrieval; recently, this\nhas been achieved by embedding the graphical structure of the database into a\nmanifold in a hierarchy-preserving manner using a variety of metrics.\nPersistent homology is a tool commonly used in topological data analysis that\nis able to rigorously characterize a database in terms of both its hierarchy\nand connectivity structure. Computing persistent homology on a variety of\nembedded datasets reveals that some commonly used embeddings fail to preserve\nthe connectivity. We show that those embeddings which successfully retain the\ndatabase topology coincide in persistent homology by introducing two\ndilation-invariant comparative measures to capture this effect: in particular,\nthey address the issue of metric distortion on manifolds. We provide an\nalgorithm for their computation that exhibits greatly reduced time complexity\nover existing methods. We use these measures to perform the first instance of\ntopology-based information retrieval and demonstrate its increased performance\nover the standard bottleneck distance for persistent homology. We showcase our\napproach on databases of different data varieties including text, videos, and\nmedical images.",
          "link": "http://arxiv.org/abs/2104.01672",
          "publishedOn": "2021-08-03T02:06:35.179Z",
          "wordCount": 657,
          "title": "Topological Information Retrieval with Dilation-Invariant Bottleneck Comparative Measures. (arXiv:2104.01672v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.10743",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Luo_Y/0/1/0/all/0/1\">Yuetian Luo</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_A/0/1/0/all/0/1\">Anru R. Zhang</a>",
          "description": "This paper studies the statistical and computational limits of high-order\nclustering with planted structures. We focus on two clustering models, constant\nhigh-order clustering (CHC) and rank-one higher-order clustering (ROHC), and\nstudy the methods and theory for testing whether a cluster exists (detection)\nand identifying the support of cluster (recovery).\n\nSpecifically, we identify the sharp boundaries of signal-to-noise ratio for\nwhich CHC and ROHC detection/recovery are statistically possible. We also\ndevelop the tight computational thresholds: when the signal-to-noise ratio is\nbelow these thresholds, we prove that polynomial-time algorithms cannot solve\nthese problems under the computational hardness conjectures of hypergraphic\nplanted clique (HPC) detection and hypergraphic planted dense subgraph (HPDS)\nrecovery. We also propose polynomial-time tensor algorithms that achieve\nreliable detection and recovery when the signal-to-noise ratio is above these\nthresholds. Both sparsity and tensor structures yield the computational\nbarriers in high-order tensor clustering. The interplay between them results in\nsignificant differences between high-order tensor clustering and matrix\nclustering in literature in aspects of statistical and computational phase\ntransition diagrams, algorithmic approaches, hardness conjecture, and proof\ntechniques. To our best knowledge, we are the first to give a thorough\ncharacterization of the statistical and computational trade-off for such a\ndouble computational-barrier problem. Finally, we provide evidence for the\ncomputational hardness conjectures of HPC detection (via low-degree polynomial\nand Metropolis methods) and HPDS recovery (via low-degree polynomial method).",
          "link": "http://arxiv.org/abs/2005.10743",
          "publishedOn": "2021-08-03T02:06:35.173Z",
          "wordCount": 714,
          "title": "Tensor Clustering with Planted Structures: Statistical Optimality and Computational Limits. (arXiv:2005.10743v3 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Freund_Y/0/1/0/all/0/1\">Yoav Freund</a>",
          "description": "The backwards induction method due to Bellman~\\cite{bellman1952theory} is a\npopular approach to solving problems in optimiztion, optimal control, and many\nother areas of applied math. In this paper we analyze the backwords induction\napproach, under min/max conditions. We show that if the value function is has\nstrictly positive derivatives of order 1-4 then the optimal strategy for the\nadversary is Brownian motion. Using that fact we analyze different potential\nfunctions and show that the Normal-Hedge potential is optimal.",
          "link": "http://arxiv.org/abs/2106.10717",
          "publishedOn": "2021-08-03T02:06:35.154Z",
          "wordCount": 535,
          "title": "Strategies for convex potential games and an application to decision-theoretic online learning. (arXiv:2106.10717v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parry_H/0/1/0/all/0/1\">Hishan Parry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xun_L/0/1/0/all/0/1\">Lei Xun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_A/0/1/0/all/0/1\">Amin Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jia Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrett_G/0/1/0/all/0/1\">Geoff V. Merrett</a>",
          "description": "The Transformer architecture is widely used for machine translation tasks.\nHowever, its resource-intensive nature makes it challenging to implement on\nconstrained embedded devices, particularly where available hardware resources\ncan vary at run-time. We propose a dynamic machine translation model that\nscales the Transformer architecture based on the available resources at any\nparticular time. The proposed approach, 'Dynamic-HAT', uses a HAT\nSuperTransformer as the backbone to search for SubTransformers with different\naccuracy-latency trade-offs at design time. The optimal SubTransformers are\nsampled from the SuperTransformer at run-time, depending on latency\nconstraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses\ninherited SubTransformers sampled directly from the SuperTransformer with a\nswitching time of <1s. Using inherited SubTransformers results in a BLEU score\nloss of <1.5% because the SubTransformer configuration is not retrained from\nscratch after sampling. However, to recover this loss in performance, the\ndimensions of the design space can be reduced to tailor it to a family of\ntarget hardware. The new reduced design space results in a BLEU score increase\nof approximately 1% for sub-optimal models from the original design space, with\na wide range for performance scaling between 0.356s - 1.526s for the GPU and\n2.9s - 7.31s for the CPU.",
          "link": "http://arxiv.org/abs/2107.08199",
          "publishedOn": "2021-08-03T02:06:35.148Z",
          "wordCount": 680,
          "title": "Dynamic Transformer for Efficient Machine Translation on Embedded Devices. (arXiv:2107.08199v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Huimin Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jiahao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>",
          "description": "Randomized Smoothing (RS), being one of few provable defenses, has been\nshowing great effectiveness and scalability in terms of defending against\n$\\ell_2$-norm adversarial perturbations. However, the cost of MC sampling\nneeded in RS for evaluation is high and computationally expensive. To address\nthis issue, we investigate the possibility of performing randomized smoothing\nand establishing the robust certification in the latent space of a network, so\nthat the overall dimensionality of tensors involved in computation could be\ndrastically reduced. To this end, we propose Latent Space Randomized Smoothing.\nAnother important aspect is that we use orthogonal modules, whose Lipschitz\nproperty is known for free by design, to propagate the certified radius\nestimated in the latent space back to the input space, providing valid\ncertifiable regions for the test samples in the input space. Experiments on\nCIFAR10 and ImageNet show that our method achieves competitive certified\nrobustness but with a significant improvement of efficiency during the test\nphase.",
          "link": "http://arxiv.org/abs/2108.00491",
          "publishedOn": "2021-08-03T02:06:35.142Z",
          "wordCount": 593,
          "title": "Certified Defense via Latent Space Randomized Smoothing with Orthogonal Encoders. (arXiv:2108.00491v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.00351",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chung_M/0/1/0/all/0/1\">Moo K. Chung</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ombao_H/0/1/0/all/0/1\">Hernando Ombao</a>",
          "description": "Persistent homology has undergone significant development in recent years.\nHowever, one outstanding challenge is to build a coherent statistical inference\nprocedure on persistent diagrams. In this paper, we first present a new lattice\npath representation for persistent diagrams. We then develop a new exact\nstatistical inference procedure for lattice paths via combinatorial\nenumerations. The lattice path method is applied to the topological\ncharacterization of the protein structures of the COVID-19 virus. We\ndemonstrate that there are topological changes during the conformational change\nof spike proteins.",
          "link": "http://arxiv.org/abs/2105.00351",
          "publishedOn": "2021-08-03T02:06:35.136Z",
          "wordCount": 603,
          "title": "Lattice Paths for Persistent Diagrams. (arXiv:2105.00351v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00473",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Xu_Z/0/1/0/all/0/1\">Zi Xu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Shen_J/0/1/0/all/0/1\">Jingjing Shen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_Z/0/1/0/all/0/1\">Ziqi Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Dai_Y/0/1/0/all/0/1\">Yuhong Dai</a>",
          "description": "In this paper, we study zeroth-order algorithms for nonconvex-concave minimax\nproblems, which have attracted widely attention in machine learning, signal\nprocessing and many other fields in recent years. We propose a zeroth-order\nalternating randomized gradient projection (ZO-AGP) algorithm for smooth\nnonconvex-concave minimax problems, and its iteration complexity to obtain an\n$\\varepsilon$-stationary point is bounded by $\\mathcal{O}(\\varepsilon^{-4})$,\nand the number of function value estimation is bounded by\n$\\mathcal{O}(d_{x}\\varepsilon^{-4}+d_{y}\\varepsilon^{-6})$ per iteration.\nMoreover, we propose a zeroth-order block alternating randomized proximal\ngradient algorithm (ZO-BAPG) for solving block-wise nonsmooth nonconvex-concave\nminimax optimization problems, and the iteration complexity to obtain an\n$\\varepsilon$-stationary point is bounded by $\\mathcal{O}(\\varepsilon^{-4})$\nand the number of function value estimation per iteration is bounded by\n$\\mathcal{O}(K d_{x}\\varepsilon^{-4}+d_{y}\\varepsilon^{-6})$. To the best of\nour knowledge, this is the first time that zeroth-order algorithms with\niteration complexity gurantee are developed for solving both general smooth and\nblock-wise nonsmooth nonconvex-concave minimax problems. Numerical results on\ndata poisoning attack problem validate the efficiency of the proposed\nalgorithms.",
          "link": "http://arxiv.org/abs/2108.00473",
          "publishedOn": "2021-08-03T02:06:35.119Z",
          "wordCount": 608,
          "title": "Zeroth-Order Alternating Randomized Gradient Projection Algorithms for General Nonconvex-Concave Minimax Problems. (arXiv:2108.00473v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/1912.02620",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1\">Tian Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1\">Agisilaos Chartsias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chengjia Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>",
          "description": "How will my face look when I get older? Or, for a more challenging question:\nHow will my brain look when I get older? To answer this question one must\ndevise (and learn from data) a multivariate auto-regressive function which\ngiven an image and a desired target age generates an output image. While\ncollecting data for faces may be easier, collecting longitudinal brain data is\nnot trivial. We propose a deep learning-based method that learns to simulate\nsubject-specific brain ageing trajectories without relying on longitudinal\ndata. Our method synthesises images conditioned on two factors: age (a\ncontinuous variable), and status of Alzheimer's Disease (AD, an ordinal\nvariable). With an adversarial formulation we learn the joint distribution of\nbrain appearance, age and AD status, and define reconstruction losses to\naddress the challenging problem of preserving subject identity. We compare with\nseveral benchmarks using two widely used datasets. We evaluate the quality and\nrealism of synthesised images using ground-truth longitudinal data and a\npre-trained age predictor. We show that, despite the use of cross-sectional\ndata, our model learns patterns of gray matter atrophy in the middle temporal\ngyrus in patients with AD. To demonstrate generalisation ability, we train on\none dataset and evaluate predictions on the other. In conclusion, our model\nshows an ability to separate age, disease influence and anatomy using only 2D\ncross-sectional data that should should be useful in large studies into\nneurodegenerative disease, that aim to combine several data sources. To\nfacilitate such future studies by the community at large our code is made\navailable at https://github.com/xiat0616/BrainAgeing.",
          "link": "http://arxiv.org/abs/1912.02620",
          "publishedOn": "2021-08-03T02:06:35.113Z",
          "wordCount": 762,
          "title": "Learning to synthesise the ageing brain without longitudinal data. (arXiv:1912.02620v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00683",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haliem_M/0/1/0/all/0/1\">Marina Haliem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonjour_T/0/1/0/all/0/1\">Trevor Bonjour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alsalem_A/0/1/0/all/0/1\">Aala Alsalem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Shilpa Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1\">Vaneet Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhargava_B/0/1/0/all/0/1\">Bharat Bhargava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kejriwal_M/0/1/0/all/0/1\">Mayank Kejriwal</a>",
          "description": "Learning to adapt and make real-time informed decisions in a dynamic and\ncomplex environment is a challenging problem. Monopoly is a popular strategic\nboard game that requires players to make multiple decisions during the game.\nDecision-making in Monopoly involves many real-world elements such as\nstrategizing, luck, and modeling of opponent's policies. In this paper, we\npresent novel representations for the state and action space for the full\nversion of Monopoly and define an improved reward function. Using these, we\nshow that our deep reinforcement learning agent can learn winning strategies\nfor Monopoly against different fixed-policy agents. In Monopoly, players can\ntake multiple actions even if it is not their turn to roll the dice. Some of\nthese actions occur more frequently than others, resulting in a skewed\ndistribution that adversely affects the performance of the learning agent. To\ntackle the non-uniform distribution of actions, we propose a hybrid approach\nthat combines deep reinforcement learning (for frequent but complex decisions)\nwith a fixed policy approach (for infrequent but straightforward decisions).\nExperimental results show that our hybrid agent outperforms a standard deep\nreinforcement learning agent by 30% in the number of games won against\nfixed-policy agents.",
          "link": "http://arxiv.org/abs/2103.00683",
          "publishedOn": "2021-08-03T02:06:35.107Z",
          "wordCount": 672,
          "title": "Decision Making in Monopoly using a Hybrid Deep Reinforcement Learning Approach. (arXiv:2103.00683v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07626",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Chia Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1\">Marina Meil&#x103;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kevrekidis_I/0/1/0/all/0/1\">Ioannis G. Kevrekidis</a>",
          "description": "The manifold Helmholtzian (1-Laplacian) operator $\\Delta_1$ elegantly\ngeneralizes the Laplace-Beltrami operator to vector fields on a manifold\n$\\mathcal M$. In this work, we propose the estimation of the manifold\nHelmholtzian from point cloud data by a weighted 1-Laplacian $\\mathbf{\\mathcal\nL}_1$. While higher order Laplacians ave been introduced and studied, this work\nis the first to present a graph Helmholtzian constructed from a simplicial\ncomplex as an estimator for the continuous operator in a non-parametric\nsetting. Equipped with the geometric and topological information about\n$\\mathcal M$, the Helmholtzian is a useful tool for the analysis of flows and\nvector fields on $\\mathcal M$ via the Helmholtz-Hodge theorem. In addition, the\n$\\mathbf{\\mathcal L}_1$ allows the smoothing, prediction, and feature\nextraction of the flows. We demonstrate these possibilities on substantial sets\nof synthetic and real point cloud datasets with non-trivial topological\nstructures; and provide theoretical results on the limit of $\\mathbf{\\mathcal\nL}_1$ to $\\Delta_1$.",
          "link": "http://arxiv.org/abs/2103.07626",
          "publishedOn": "2021-08-03T02:06:35.101Z",
          "wordCount": 611,
          "title": "Helmholtzian Eigenmap: Topological feature discovery & edge flow learning from point cloud data. (arXiv:2103.07626v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leonardos_S/0/1/0/all/0/1\">Stefanos Leonardos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Overman_W/0/1/0/all/0/1\">Will Overman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panageas_I/0/1/0/all/0/1\">Ioannis Panageas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1\">Georgios Piliouras</a>",
          "description": "Potential games are arguably one of the most important and widely studied\nclasses of normal form games. They define the archetypal setting of multi-agent\ncoordination as all agent utilities are perfectly aligned with each other via a\ncommon potential function. Can this intuitive framework be transplanted in the\nsetting of Markov Games? What are the similarities and differences between\nmulti-agent coordination with and without state dependence? We present a novel\ndefinition of Markov Potential Games (MPG) that generalizes prior attempts at\ncapturing complex stateful multi-agent coordination. Counter-intuitively,\ninsights from normal-form potential games do not carry over as MPGs can consist\nof settings where state-games can be zero-sum games. In the opposite direction,\nMarkov games where every state-game is a potential game are not necessarily\nMPGs. Nevertheless, MPGs showcase standard desirable properties such as the\nexistence of deterministic Nash policies. In our main technical result, we\nprove fast convergence of independent policy gradient to Nash policies by\nadapting recent gradient dominance property arguments developed for single\nagent MDPs to multi-agent learning settings.",
          "link": "http://arxiv.org/abs/2106.01969",
          "publishedOn": "2021-08-03T02:06:35.094Z",
          "wordCount": 652,
          "title": "Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games. (arXiv:2106.01969v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhongjie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingye Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1\">Martin Trapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skryagin_A/0/1/0/all/0/1\">Arseny Skryagin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>",
          "description": "Inspired by recent advances in the field of expert-based approximations of\nGaussian processes (GPs), we present an expert-based approach to large-scale\nmulti-output regression using single-output GP experts. Employing a deeply\nstructured mixture of single-output GPs encoded via a probabilistic circuit\nallows us to capture correlations between multiple output dimensions\naccurately. By recursively partitioning the covariate space and the output\nspace, posterior inference in our model reduces to inference on single-output\nGP experts, which only need to be conditioned on a small subset of the\nobservations. We show that inference can be performed exactly and efficiently\nin our model, that it can capture correlations between output dimensions and,\nhence, often outperforms approaches that do not incorporate inter-output\ncorrelations, as demonstrated on several data sets in terms of the negative log\npredictive density.",
          "link": "http://arxiv.org/abs/2106.08687",
          "publishedOn": "2021-08-03T02:06:35.088Z",
          "wordCount": 606,
          "title": "Leveraging Probabilistic Circuits for Nonparametric Multi-Output Regression. (arXiv:2106.08687v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balali_A/0/1/0/all/0/1\">Ali Balali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadpour_M/0/1/0/all/0/1\">Masoud Asadpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_S/0/1/0/all/0/1\">Seyed Hossein Jafari</a>",
          "description": "Data is published on the web over time in great volumes, but majority of the\ndata is unstructured, making it hard to understand and difficult to interpret.\nInformation Extraction (IE) methods extract structured information from\nunstructured data. One of the challenging IE tasks is Event Extraction (EE)\nwhich seeks to derive information about specific incidents and their actors\nfrom the text. EE is useful in many domains such as building a knowledge base,\ninformation retrieval, summarization and online monitoring systems. In the past\ndecades, some event ontologies like ACE, CAMEO and ICEWS were developed to\ndefine event forms, actors and dimensions of events observed in the text. These\nevent ontologies still have some shortcomings such as covering only a few\ntopics like political events, having inflexible structure in defining argument\nroles, lack of analytical dimensions, and complexity in choosing event\nsub-types. To address these concerns, we propose an event ontology, namely\nCOfEE, that incorporates both expert domain knowledge, previous ontologies and\na data-driven approach for identifying events from text. COfEE consists of two\nhierarchy levels (event types and event sub-types) that include new categories\nrelating to environmental issues, cyberspace, criminal activity and natural\ndisasters which need to be monitored instantly. Also, dynamic roles according\nto each event sub-type are defined to capture various dimensions of events. In\na follow-up experiment, the proposed ontology is evaluated on Wikipedia events,\nand it is shown to be general and comprehensive. Moreover, in order to\nfacilitate the preparation of gold-standard data for event extraction, a\nlanguage-independent online tool is presented based on COfEE.",
          "link": "http://arxiv.org/abs/2107.10326",
          "publishedOn": "2021-08-03T02:06:35.069Z",
          "wordCount": 733,
          "title": "COfEE: A Comprehensive Ontology for Event Extraction from text, with an online annotation tool. (arXiv:2107.10326v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.10572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianqian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1\">Walid Saad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1\">Mehdi Bennis</a>",
          "description": "In this paper, a novel framework is proposed to optimize the downlink\nmulti-user communication of a millimeter wave base station, which is assisted\nby a reconfigurable intelligent reflector (IR). In particular, a channel\nestimation approach is developed to measure the channel state information (CSI)\nin real-time. First, for a perfect CSI scenario, the precoding transmission of\nthe BS and the reflection coefficient of the IR are jointly optimized, via an\niterative approach, so as to maximize the sum of downlink rates towards\nmultiple users. Next, in the imperfect CSI scenario, a distributional\nreinforcement learning (DRL) approach is proposed to learn the optimal IR\nreflection and maximize the expectation of downlink capacity. In order to model\nthe transmission rate's probability distribution, a learning algorithm, based\non quantile regression (QR), is developed, and the proposed QR-DRL method is\nproved to converge to a stable distribution of downlink transmission rate.\nSimulation results show that, in the error-free CSI scenario, the proposed\napproach yields over 30% and 2-fold increase in the downlink sum-rate, compared\nwith a fixed IR reflection scheme and direct transmission scheme, respectively.\nSimulation results also show that by deploying more IR elements, the downlink\nsum-rate can be significantly improved. However, as the number of IR components\nincreases, more time is required for channel estimation, and the slope of\nincrease in the IR-aided transmission rate will become smaller. Furthermore,\nunder limited knowledge of CSI, simulation results show that the proposed\nQR-DRL method, which learns a full distribution of the downlink rate, yields a\nbetter prediction accuracy and improves the downlink rate by 10% for online\ndeployments, compared with a Q-learning baseline.",
          "link": "http://arxiv.org/abs/2002.10572",
          "publishedOn": "2021-08-03T02:06:35.062Z",
          "wordCount": 750,
          "title": "Millimeter Wave Communications with an Intelligent Reflector: Performance Optimization and Distributional Reinforcement Learning. (arXiv:2002.10572v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1\">Andrea Cossu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1\">Antonio Carta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1\">Vincenzo Lomonaco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>",
          "description": "Learning continuously during all model lifetime is fundamental to deploy\nmachine learning solutions robust to drifts in the data distribution. Advances\nin Continual Learning (CL) with recurrent neural networks could pave the way to\na large number of applications where incoming data is non stationary, like\nnatural language processing and robotics. However, the existing body of work on\nthe topic is still fragmented, with approaches which are application-specific\nand whose assessment is based on heterogeneous learning protocols and datasets.\nIn this paper, we organize the literature on CL for sequential data processing\nby providing a categorization of the contributions and a review of the\nbenchmarks. We propose two new benchmarks for CL with sequential data based on\nexisting datasets, whose characteristics resemble real-world applications. We\nalso provide a broad empirical evaluation of CL and Recurrent Neural Networks\nin class-incremental scenario, by testing their ability to mitigate forgetting\nwith a number of different strategies which are not specific to sequential data\nprocessing. Our results highlight the key role played by the sequence length\nand the importance of a clear specification of the CL scenario.",
          "link": "http://arxiv.org/abs/2103.07492",
          "publishedOn": "2021-08-03T02:06:35.055Z",
          "wordCount": 681,
          "title": "Continual Learning for Recurrent Neural Networks: an Empirical Evaluation. (arXiv:2103.07492v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_P/0/1/0/all/0/1\">Pablo G&#xf3;mez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meoni_G/0/1/0/all/0/1\">Gabriele Meoni</a>",
          "description": "Supervised learning techniques are at the center of many tasks in remote\nsensing. Unfortunately, these methods, especially recent deep learning methods,\noften require large amounts of labeled data for training. Even though\nsatellites acquire large amounts of data, labeling the data is often tedious,\nexpensive and requires expert knowledge. Hence, improved methods that require\nfewer labeled samples are needed. We present MSMatch, the first semi-supervised\nlearning approach competitive with supervised methods on scene classification\non the EuroSAT and UC Merced Land Use benchmark datasets. We test both RGB and\nmultispectral images of EuroSAT and perform various ablation studies to\nidentify the critical parts of the model. The trained neural network achieves\nstate-of-the-art results on EuroSAT with an accuracy that is up to 19.76%\nbetter than previous methods depending on the number of labeled training\nexamples. With just five labeled examples per class, we reach 94.53% and 95.86%\naccuracy on the EuroSAT RGB and multispectral datasets, respectively. On the UC\nMerced Land Use dataset, we outperform previous works by up to 5.59% and reach\n90.71% with five labeled examples. Our results show that MSMatch is capable of\ngreatly reducing the requirements for labeled data. It translates well to\nmultispectral data and should enable various applications that are currently\ninfeasible due to a lack of labeled data. We provide the source code of MSMatch\nonline to enable easy reproduction and quick adoption.",
          "link": "http://arxiv.org/abs/2103.10368",
          "publishedOn": "2021-08-03T02:06:35.048Z",
          "wordCount": 713,
          "title": "MSMatch: Semi-Supervised Multispectral Scene Classification with Few Labels. (arXiv:2103.10368v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.04695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitrevski_B/0/1/0/all/0/1\">Blagoj Mitrevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filipovic_M/0/1/0/all/0/1\">Milena Filipovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1\">Emma Lejal Glaude</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>",
          "description": "Multi-objective gradient methods are becoming the standard for solving\nmulti-objective problems. Among others, they show promising results in\ndeveloping multi-objective recommender systems with both correlated and\nconflicting objectives. Classic multi-gradient descent usually relies on the\ncombination of the gradients, not including the computation of first and second\nmoments of the gradients. This leads to a brittle behavior and misses important\nareas in the solution space. In this work, we create a multi-objective\nmodel-agnostic Adamize method that leverages the benefits of the Adam optimizer\nin single-objective problems. This corrects and stabilizes the gradients of\nevery objective before calculating a common gradient descent vector that\noptimizes all the objectives simultaneously. We evaluate the benefits of\nmulti-objective Adamize on two multi-objective recommender systems and for\nthree different objective combinations, both correlated or conflicting. We\nreport significant improvements, measured with three different Pareto front\nmetrics: hypervolume, coverage, and spacing. Finally, we show that the Adamized\nPareto front strictly dominates the previous one on multiple objective pairs.",
          "link": "http://arxiv.org/abs/2009.04695",
          "publishedOn": "2021-08-03T02:06:35.040Z",
          "wordCount": 643,
          "title": "Momentum-based Gradient Methods in Multi-Objective Recommendation. (arXiv:2009.04695v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.",
          "link": "http://arxiv.org/abs/2107.08264",
          "publishedOn": "2021-08-03T02:06:35.025Z",
          "wordCount": 730,
          "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caro_M/0/1/0/all/0/1\">Matthias C. Caro</a>",
          "description": "Machine learning researchers and practitioners steadily enlarge the multitude\nof successful learning models. They achieve this through in-depth theoretical\nanalyses and experiential heuristics. However, there is no known\ngeneral-purpose procedure for rigorously evaluating whether newly proposed\nmodels indeed successfully learn from data. We show that such a procedure\ncannot exist. For PAC binary classification, uniform and universal online\nlearning, and exact learning through teacher-learner interactions, learnability\nis in general undecidable, both in the sense of independence of the axioms in a\nformal system and in the sense of uncomputability. Our proofs proceed via\ncomputable constructions of function classes that encode the consistency\nproblem for formal systems and the halting problem for Turing machines into\ncomplexity measures that characterize learnability. Our work shows that\nundecidability appears in the theoretical foundations of machine learning:\nThere is no one-size-fits-all algorithm for deciding whether a machine learning\nmodel can be successful. We cannot in general automatize the process of\nassessing new learning models.",
          "link": "http://arxiv.org/abs/2106.01382",
          "publishedOn": "2021-08-03T02:06:35.018Z",
          "wordCount": 641,
          "title": "Undecidability of Learnability. (arXiv:2106.01382v2 [cs.CC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.09335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goh_S/0/1/0/all/0/1\">Sim Kuan Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Narendra Pratap Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_Z/0/1/0/all/0/1\">Zhi Jun Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1\">Sameer Alam</a>",
          "description": "Approach and landing accidents have resulted in a significant number of hull\nlosses worldwide. Technologies (e.g., instrument landing system) and procedures\n(e.g., stabilized approach criteria) have been developed to reduce the risks.\nIn this paper, we propose a data-driven method to learn and interpret flight's\napproach and landing parameters to facilitate comprehensible and actionable\ninsights into flight dynamics. Specifically, we develop two variants of tunnel\nGaussian process (TGP) models to elucidate aircraft's approach and landing\ndynamics using advanced surface movement guidance and control system (A-SMGCS)\ndata, which then indicates the stability of flight. TGP hybridizes the\nstrengths of sparse variational Gaussian process and polar Gaussian process to\nlearn from a large amount of data in cylindrical coordinates. We examine TGP\nqualitatively and quantitatively by synthesizing three complex trajectory\ndatasets and compared TGP against existing methods on trajectory learning.\nEmpirically, TGP demonstrates superior modeling performance. When applied to\noperational A-SMGCS data, TGP provides the generative probabilistic description\nof landing dynamics and interpretable tunnel views of approach and landing\nparameters. These probabilistic tunnel models can facilitate the analysis of\nprocedure adherence and augment existing aircrew and air traffic controllers'\ndisplays during the approach and landing procedures, enabling necessary\ncorrective actions.",
          "link": "http://arxiv.org/abs/2011.09335",
          "publishedOn": "2021-08-03T02:06:35.012Z",
          "wordCount": 683,
          "title": "A Tunnel Gaussian Process Model for Learning Interpretable Flight's Landing Parameters. (arXiv:2011.09335v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00735",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Swijsen_L/0/1/0/all/0/1\">Lars Swijsen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Veken_J/0/1/0/all/0/1\">Joeri Van der Veken</a>, <a href=\"http://arxiv.org/find/math/1/au:+Vannieuwenhoven_N/0/1/0/all/0/1\">Nick Vannieuwenhoven</a>",
          "description": "We propose a Riemannian conjugate gradient (CG) optimization method for\nfinding low rank approximations of incomplete tensors. Our main contribution\nconsists of an explicit expression of the geodesics on the Segre manifold.\nThese are exploited in our algorithm to perform the retractions. We apply our\nmethod to movie rating predictions in a recommender system for the MovieLens\ndataset, and identification of pure fluorophores via fluorescent spectroscopy\nwith missing data. In this last application, we recover the tensor\ndecomposition from less than $10\\%$ of the data.",
          "link": "http://arxiv.org/abs/2108.00735",
          "publishedOn": "2021-08-03T02:06:35.005Z",
          "wordCount": 530,
          "title": "Tensor completion using geodesics on Segre manifolds. (arXiv:2108.00735v1 [math.DG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.05266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valerio_L/0/1/0/all/0/1\">Lorenzo Valerio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passarella_A/0/1/0/all/0/1\">Andrea Passarella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Marco Conti</a>",
          "description": "The exponential growth of devices and data at the edges of the Internet is\nrising scalability and privacy concerns on approaches based exclusively on\nremote cloud platforms. Data gravity, a fundamental concept in Fog Computing,\npoints towards decentralisation of computation for data analysis, as a viable\nalternative to address those concerns. Decentralising AI tasks on several\ncooperative devices means identifying the optimal set of locations or\nCollection Points (CP for short) to use, in the continuum between full\ncentralisation (i.e., all data on a single device) and full decentralisation\n(i.e., data on source locations). We propose an analytical framework able to\nfind the optimal operating point in this continuum, linking the accuracy of the\nlearning task with the corresponding network and computational cost for moving\ndata and running the distributed training at the CPs. We show through\nsimulations that the model accurately predicts the optimal trade-off, quite\noften an intermediate point between full centralisation and full\ndecentralisation, showing also a significant cost saving w.r.t. both of them.\nFinally, the analytical model admits closed-form or numeric solutions, making\nit not only a performance evaluation instrument but also a design tool to\nconfigure a given distributed learning task optimally before its deployment.",
          "link": "http://arxiv.org/abs/2012.05266",
          "publishedOn": "2021-08-03T02:06:34.987Z",
          "wordCount": 696,
          "title": "Optimising cost vs accuracy of decentralised analytics in fog computing environments. (arXiv:2012.05266v3 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_I/0/1/0/all/0/1\">Ivan D. Jimenez Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosolia_U/0/1/0/all/0/1\">Ugo Rosolia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ames_A/0/1/0/all/0/1\">Aaron D. Ames</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yisong Yue</a>",
          "description": "We present a straightforward and efficient way to control unstable robotic\nsystems using an estimated dynamics model. Specifically, we show how to exploit\nthe differentiability of Gaussian Processes to create a state-dependent\nlinearized approximation of the true continuous dynamics that can be integrated\nwith model predictive control. Our approach is compatible with most Gaussian\nprocess approaches for system identification, and can learn an accurate model\nusing modest amounts of training data. We validate our approach by learning the\ndynamics of an unstable system such as a segway with a 7-D state space and 2-D\ninput space (using only one minute of data), and we show that the resulting\ncontroller is robust to unmodelled dynamics and disturbances, while\nstate-of-the-art control methods based on nominal models can fail under small\nperturbations. Code is open sourced at\nhttps://github.com/learning-and-control/core .",
          "link": "http://arxiv.org/abs/2103.04548",
          "publishedOn": "2021-08-03T02:06:34.981Z",
          "wordCount": 631,
          "title": "Learning to Control an Unstable System with One Minute of Data: Leveraging Gaussian Process Differentiation in Predictive Control. (arXiv:2103.04548v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianqian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferdowsi_A/0/1/0/all/0/1\">Aidin Ferdowsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1\">Walid Saad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1\">Mehdi Bennis</a>",
          "description": "In this paper, a novel framework is proposed to perform data-driven\nair-to-ground (A2G) channel estimation for millimeter wave (mmWave)\ncommunications in an unmanned aerial vehicle (UAV) wireless network. First, an\neffective channel estimation approach is developed to collect mmWave channel\ninformation, allowing each UAV to train a stand-alone channel model via a\nconditional generative adversarial network (CGAN) along each beamforming\ndirection. Next, in order to expand the application scenarios of the trained\nchannel model into a broader spatial-temporal domain, a cooperative framework,\nbased on a distributed CGAN architecture, is developed, allowing each UAV to\ncollaboratively learn the mmWave channel distribution in a fully-distributed\nmanner. To guarantee an efficient learning process, necessary and sufficient\nconditions for the optimal UAV network topology that maximizes the learning\nrate for cooperative channel modeling are derived, and the optimal CGAN\nlearning solution per UAV is subsequently characterized, based on the\ndistributed network structure. Simulation results show that the proposed\ndistributed CGAN approach is robust to the local training error at each UAV.\nMeanwhile, a larger airborne network size requires more communication resources\nper UAV to guarantee an efficient learning rate. The results also show that,\ncompared with a stand-alone CGAN without information sharing and two other\ndistributed schemes, namely: A multi-discriminator CGAN and a federated CGAN\nmethod, the proposed distributed CGAN approach yields a higher modeling\naccuracy while learning the environment, and it achieves a larger average data\nrate in the online performance of UAV downlink mmWave communications.",
          "link": "http://arxiv.org/abs/2102.01751",
          "publishedOn": "2021-08-03T02:06:34.976Z",
          "wordCount": 720,
          "title": "Distributed Conditional Generative Adversarial Networks (GANs) for Data-Driven Millimeter Wave Communications in UAV Networks. (arXiv:2102.01751v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zakovskis_R/0/1/0/all/0/1\">Ronalds Zakovskis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Draguns_A/0/1/0/all/0/1\">Andis Draguns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaile_E/0/1/0/all/0/1\">Eliza Gaile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozolins_E/0/1/0/all/0/1\">Emils Ozolins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freivalds_K/0/1/0/all/0/1\">Karlis Freivalds</a>",
          "description": "Recurrent neural networks have flourished in many areas. Consequently, we can\nsee new RNN cells being developed continuously, usually by creating or using\ngates in a new, original way. But what if we told you that gates in RNNs are\nredundant? In this paper, we propose a new recurrent cell called Residual\nRecurrent Unit (RRU) which beats traditional cells and does not employ a single\ngate. It is based on the residual shortcut connection together with linear\ntransformations, ReLU, and normalization. To evaluate our cell's effectiveness,\nwe compare its performance against the widely-used GRU and LSTM cells and the\nrecently proposed Mogrifier LSTM on several tasks including, polyphonic music\nmodeling, language modeling, and sentiment analysis. Our experiments show that\nRRU outperforms the traditional gated units on most of these tasks. Also, it\nhas better robustness to parameter selection, allowing immediate application in\nnew tasks without much tuning. We have implemented the RRU in TensorFlow, and\nthe code is made available at https://github.com/LUMII-Syslab/RRU .",
          "link": "http://arxiv.org/abs/2108.00527",
          "publishedOn": "2021-08-03T02:06:34.968Z",
          "wordCount": 602,
          "title": "Gates are not what you need in RNNs. (arXiv:2108.00527v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01333",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1\">Shuyan Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Spirtes_P/0/1/0/all/0/1\">Peter Spirtes</a>",
          "description": "Kalisch and B\\\"{u}hlmann (2007) showed that for linear Gaussian models, under\nthe Causal Markov Assumption, the Strong Causal Faithfulness Assumption, and\nthe assumption of causal sufficiency, the PC algorithm is a uniformly\nconsistent estimator of the Markov Equivalence Class of the true causal DAG for\nlinear Gaussian models; it follows from this that for the identifiable causal\neffects in the Markov Equivalence Class, there are uniformly consistent\nestimators of causal effects as well. The $k$-Triangle-Faithfulness Assumption\nis a strictly weaker assumption that avoids some implausible implications of\nthe Strong Causal Faithfulness Assumption and also allows for uniformly\nconsistent estimates of Markov Equivalence Classes (in a weakened sense), and\nof identifiable causal effects. However, both of these assumptions are\nrestricted to linear Gaussian models. We propose the Generalized $k$-Triangle\nFaithfulness, which can be applied to any smooth distribution. In addition,\nunder the Generalized $k$-Triangle Faithfulness Assumption, we describe the\nEdge Estimation Algorithm that provides uniformly consistent estimates of\ncausal effects in some cases (and otherwise outputs \"can't tell\"), and the\n\\textit{Very Conservative }$SGS$ Algorithm that (in a slightly weaker sense) is\na uniformly consistent estimator of the Markov equivalence class of the true\nDAG.",
          "link": "http://arxiv.org/abs/2107.01333",
          "publishedOn": "2021-08-03T02:06:34.962Z",
          "wordCount": 651,
          "title": "A Uniformly Consistent Estimator of non-Gaussian Causal Effects Under the k-Triangle-Faithfulness Assumption. (arXiv:2107.01333v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.04026",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wilson_J/0/1/0/all/0/1\">James T. Wilson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Borovitskiy_V/0/1/0/all/0/1\">Viacheslav Borovitskiy</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Terenin_A/0/1/0/all/0/1\">Alexander Terenin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mostowsky_P/0/1/0/all/0/1\">Peter Mostowsky</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Deisenroth_M/0/1/0/all/0/1\">Marc Peter Deisenroth</a>",
          "description": "As Gaussian processes are used to answer increasingly complex questions,\nanalytic solutions become scarcer and scarcer. Monte Carlo methods act as a\nconvenient bridge for connecting intractable mathematical expressions with\nactionable estimates via sampling. Conventional approaches for simulating\nGaussian process posteriors view samples as draws from marginal distributions\nof process values at finite sets of input locations. This distribution-centric\ncharacterization leads to generative strategies that scale cubically in the\nsize of the desired random vector. These methods are prohibitively expensive in\ncases where we would, ideally, like to draw high-dimensional vectors or even\ncontinuous sample paths. In this work, we investigate a different line of\nreasoning: rather than focusing on distributions, we articulate Gaussian\nconditionals at the level of random variables. We show how this pathwise\ninterpretation of conditioning gives rise to a general family of approximations\nthat lend themselves to efficiently sampling Gaussian process posteriors.\nStarting from first principles, we derive these methods and analyze the\napproximation errors they introduce. We, then, ground these results by\nexploring the practical implications of pathwise conditioning in various\napplied settings, such as global optimization and reinforcement learning.",
          "link": "http://arxiv.org/abs/2011.04026",
          "publishedOn": "2021-08-03T02:06:34.956Z",
          "wordCount": 657,
          "title": "Pathwise Conditioning of Gaussian Processes. (arXiv:2011.04026v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeevan_P/0/1/0/all/0/1\">Pranav Jeevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1\">Amit Sethi</a> (Indian Institute of Technology Bombay)",
          "description": "We propose three improvements to vision transformers (ViT) to reduce the\nnumber of trainable parameters without compromising classification accuracy. We\naddress two shortcomings of the early ViT architectures -- quadratic bottleneck\nof the attention mechanism and the lack of an inductive bias in their\narchitectures that rely on unrolling the two-dimensional image structure.\nLinear attention mechanisms overcome the bottleneck of quadratic complexity,\nwhich restricts application of transformer models in vision tasks. We modify\nthe ViT architecture to work on longer sequence data by replacing the quadratic\nattention with efficient transformers, such as Performer, Linformer and\nNystr\\\"omformer of linear complexity creating Vision X-formers (ViX). We show\nthat all three versions of ViX may be more accurate than ViT for image\nclassification while using far fewer parameters and computational resources. We\nalso compare their performance with FNet and multi-layer perceptron (MLP)\nmixer. We further show that replacing the initial linear embedding layer by\nconvolutional layers in ViX further increases their performance. Furthermore,\nour tests on recent vision transformer models, such as LeViT, Convolutional\nvision Transformer (CvT), Compact Convolutional Transformer (CCT) and\nPooling-based Vision Transformer (PiT) show that replacing the attention with\nNystr\\\"omformer or Performer saves GPU usage and memory without deteriorating\nthe classification accuracy. We also show that replacing the standard learnable\n1D position embeddings in ViT with Rotary Position Embedding (RoPE) give\nfurther improvements in accuracy. Incorporating these changes can democratize\ntransformers by making them accessible to those with limited data and computing\nresources.",
          "link": "http://arxiv.org/abs/2107.02239",
          "publishedOn": "2021-08-03T02:06:34.939Z",
          "wordCount": 733,
          "title": "Vision Xformers: Efficient Attention for Image Classification. (arXiv:2107.02239v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10845",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_H/0/1/0/all/0/1\">Hanrui Wang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ding_Y/0/1/0/all/0/1\">Yongshan Ding</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Gu_J/0/1/0/all/0/1\">Jiaqi Gu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lin_Y/0/1/0/all/0/1\">Yujun Lin</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pan_D/0/1/0/all/0/1\">David Z. Pan</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chong_F/0/1/0/all/0/1\">Frederic T. Chong</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>",
          "description": "Quantum noise is the key challenge in Noisy Intermediate-Scale Quantum (NISQ)\ncomputers. Previous work for mitigating noise has primarily focused on\ngate-level or pulse-level noise-adaptive compilation. However, limited research\nefforts have explored a higher level of optimization by making the quantum\ncircuits themselves resilient to noise.\n\nWe propose QuantumNAS, a comprehensive framework for noise-adaptive co-search\nof the variational circuit and qubit mapping. Variational quantum circuits are\na promising approach for constructing QML and quantum simulation. However,\nfinding the best variational circuit and its optimal parameters is challenging\ndue to the large design space and parameter training cost. We propose to\ndecouple the circuit search and parameter training by introducing a novel\nSuperCircuit. The SuperCircuit is constructed with multiple layers of\npre-defined parameterized gates and trained by iteratively sampling and\nupdating the parameter subsets (SubCircuits) of it. It provides an accurate\nestimation of SubCircuits performance trained from scratch. Then we perform an\nevolutionary co-search of SubCircuit and its qubit mapping. The SubCircuit\nperformance is estimated with parameters inherited from SuperCircuit and\nsimulated with real device noise models. Finally, we perform iterative gate\npruning and finetuning to remove redundant gates.\n\nExtensively evaluated with 12 QML and VQE benchmarks on 10 quantum comput,\nQuantumNAS significantly outperforms baselines. For QML, QuantumNAS is the\nfirst to demonstrate over 95% 2-class, 85% 4-class, and 32% 10-class\nclassification accuracy on real QC. It also achieves the lowest eigenvalue for\nVQE tasks on H2, H2O, LiH, CH4, BeH2 compared with UCCSD. We also open-source\nQuantumEngine (https://github.com/mit-han-lab/pytorch-quantum) for fast\ntraining of parameterized quantum circuits to facilitate future research.",
          "link": "http://arxiv.org/abs/2107.10845",
          "publishedOn": "2021-08-03T02:06:34.932Z",
          "wordCount": 731,
          "title": "QuantumNAS: Noise-Adaptive Search for Robust Quantum Circuits. (arXiv:2107.10845v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manapragada_C/0/1/0/all/0/1\">Chaitanya Manapragada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_H/0/1/0/all/0/1\">Heitor M Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mahsa Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bifet_A/0/1/0/all/0/1\">Albert Bifet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1\">Geoffrey I Webb</a>",
          "description": "Decision tree ensembles are widely used in practice. In this work, we study\nin ensemble settings the effectiveness of replacing the split strategy for the\nstate-of-the-art online tree learner, Hoeffding Tree, with a rigorous but more\neager splitting strategy that we had previously published as Hoeffding AnyTime\nTree. Hoeffding AnyTime Tree (HATT), uses the Hoeffding Test to determine\nwhether the current best candidate split is superior to the current split, with\nthe possibility of revision, while Hoeffding Tree aims to determine whether the\ntop candidate is better than the second best and if a test is selected, fixes\nit for all posterity. HATT converges to the ideal batch tree while Hoeffding\nTree does not. We find that HATT is an efficacious base learner for online\nbagging and online boosting ensembles. On UCI and synthetic streams, HATT as a\nbase learner outperforms HT within a 0.05 significance level for the majority\nof tested ensembles on what we believe is the largest and most comprehensive\nset of testbenches in the online learning literature. Our results indicate that\nHATT is a superior alternative to Hoeffding Tree in a large number of ensemble\nsettings.",
          "link": "http://arxiv.org/abs/2010.10935",
          "publishedOn": "2021-08-03T02:06:34.925Z",
          "wordCount": 667,
          "title": "An Eager Splitting Strategy for Online Decision Trees. (arXiv:2010.10935v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_R/0/1/0/all/0/1\">Rachit Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiwelekar_A/0/1/0/all/0/1\">Arvind W Kiwelekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netak_L/0/1/0/all/0/1\">Laxman D Netak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodake_A/0/1/0/all/0/1\">Akshay Ghodake</a>",
          "description": "Although most logistics and freight forwarding organizations, in one way or\nanother, claim to have core values. The engagement of employees is a vast\nstructure that affects almost every part of the company's core environmental\nvalues. There is little theoretical knowledge about the relationship between\nfirms and the engagement of employees. Based on research literature, this paper\naims to provide a novel approach for insight around employee engagement in a\nlogistics organization by implementing deep natural language processing\nconcepts. The artificial intelligence-enabled solution named Intelligent Pulse\n(I-Pulse) can evaluate hundreds and thousands of pulse survey comments and\nprovides the actionable insights and gist of employee feedback. I-Pulse allows\nthe stakeholders to think in new ways in their organization, helping them to\nhave a powerful influence on employee engagement, retention, and efficiency.\nThis study is of corresponding interest to researchers and practitioners.",
          "link": "http://arxiv.org/abs/2106.07341",
          "publishedOn": "2021-08-03T02:06:34.897Z",
          "wordCount": null,
          "title": "i-Pulse: A NLP based novel approach for employee engagement in logistics organization. (arXiv:2106.07341v1 [cs.SI] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pattanayak_K/0/1/0/all/0/1\">Kunal Pattanayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_V/0/1/0/all/0/1\">Vikram Krishnamurthy</a>",
          "description": "Are deep convolutional neural networks (CNNs) for image classification\nexplainable by utility maximization with information acquisition costs? We\ndemonstrate that deep CNNs behave equivalently (in terms of necessary and\nsufficient conditions) to rationally inattentive utility maximizers, a\ngenerative model used extensively in economics for human decision making. Our\nclaim is based by extensive experiments on 200 deep CNNs from 5 popular\narchitectures. The parameters of our interpretable model are computed\nefficiently via convex feasibility algorithms. As an application, we show that\nour economics-based interpretable model can predict the classification\nperformance of deep CNNs trained with arbitrary parameters with accuracy\nexceeding 94% . This eliminates the need to re-train the deep CNNs for image\nclassification. The theoretical foundation of our approach lies in Bayesian\nrevealed preference studied in micro-economics. All our results are on GitHub\nand completely reproducible.",
          "link": "http://arxiv.org/abs/2102.04594",
          "publishedOn": "2021-08-03T02:06:34.896Z",
          "wordCount": null,
          "title": "Rationally Inattentive Utility Maximization for Interpretable Deep Image Classification. (arXiv:2102.04594v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junda Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xupin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "While the long-term effects of COVID-19 are yet to be determined, its\nimmediate impact on crowdfunding is nonetheless significant. This study takes a\ncomputational approach to more deeply comprehend this change. Using a unique\ndata set of all the campaigns published over the past two years on GoFundMe, we\nexplore the factors that have led to the successful funding of a crowdfunding\nproject. In particular, we study a corpus of crowdfunded projects, analyzing\ncover images and other variables commonly present on crowdfunding sites.\nFurthermore, we construct a classifier and a regression model to assess the\nsignificance of features based on XGBoost. In addition, we employ\ncounterfactual analysis to investigate the causality between features and the\nsuccess of crowdfunding. More importantly, sentiment analysis and the paired\nsample t-test are performed to examine the differences in crowdfunding\ncampaigns before and after the COVID-19 outbreak that started in March 2020.\nFirst, we note that there is significant racial disparity in crowdfunding\nsuccess. Second, we find that sad emotion expressed through the campaign's\ndescription became significant after the COVID-19 outbreak. Considering all\nthese factors, our findings shed light on the impact of COVID-19 on\ncrowdfunding campaigns.",
          "link": "http://arxiv.org/abs/2106.09981",
          "publishedOn": "2021-08-03T02:06:34.896Z",
          "wordCount": null,
          "title": "How COVID-19 Has Changed Crowdfunding: Evidence From GoFundMe. (arXiv:2106.09981v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Awasthi_P/0/1/0/all/0/1\">Pranjal Awasthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_A/0/1/0/all/0/1\">Alex Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijayaraghavan_A/0/1/0/all/0/1\">Aravindan Vijayaraghavan</a>",
          "description": "We present polynomial time and sample efficient algorithms for learning an\nunknown depth-2 feedforward neural network with general ReLU activations, under\nmild non-degeneracy assumptions. In particular, we consider learning an unknown\nnetwork of the form $f(x) = {a}^{\\mathsf{T}}\\sigma({W}^\\mathsf{T}x+b)$, where\n$x$ is drawn from the Gaussian distribution, and $\\sigma(t) := \\max(t,0)$ is\nthe ReLU activation. Prior works for learning networks with ReLU activations\nassume that the bias $b$ is zero. In order to deal with the presence of the\nbias terms, our proposed algorithm consists of robustly decomposing multiple\nhigher order tensors arising from the Hermite expansion of the function $f(x)$.\nUsing these ideas we also establish identifiability of the network parameters\nunder minimal assumptions.",
          "link": "http://arxiv.org/abs/2107.10209",
          "publishedOn": "2021-08-03T02:06:34.896Z",
          "wordCount": null,
          "title": "Efficient Algorithms for Learning Depth-2 Neural Networks with General ReLU Activations. (arXiv:2107.10209v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xixi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yun Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yanfei Kang</a>",
          "description": "One of the most significant differences of M5 over previous forecasting\ncompetitions is that it was held on Kaggle, an online platform of data\nscientists and machine learning practitioners. Kaggle provides a gathering\nplace, or virtual community, for web users who are interested in the M5\ncompetition. Users can share code, models, features, loss functions, etc.\nthrough online notebooks and discussion forums. This paper aims to study the\nsocial influence of virtual community on user behaviors in the M5 competition.\nWe first research the content of the M5 virtual community by topic modeling and\ntrend analysis. Further, we perform social media analysis to identify the\npotential relationship network of the virtual community. We study the roles and\ncharacteristics of some key participants that promote the diffusion of\ninformation within the M5 virtual community. Overall, this study provides\nin-depth insights into the mechanism of the virtual community's influence on\nthe participants and has potential implications for future online competitions.",
          "link": "http://arxiv.org/abs/2103.00501",
          "publishedOn": "2021-08-03T02:06:34.895Z",
          "wordCount": null,
          "title": "Exploring the social influence of Kaggle virtual community on the M5 competition. (arXiv:2103.00501v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06295",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ramli_A/0/1/0/all/0/1\">Albara Ah Ramli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Huanle Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1\">Jiahui Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1\">Rex Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nicorici_A/0/1/0/all/0/1\">Alina Nicorici</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aranki_D/0/1/0/all/0/1\">Daniel Aranki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Owens_C/0/1/0/all/0/1\">Corey Owens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasad_P/0/1/0/all/0/1\">Poonam Prasad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McDonald_C/0/1/0/all/0/1\">Craig McDonald</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Henricson_E/0/1/0/all/0/1\">Erik Henricson</a>",
          "description": "Differences in gait patterns of children with Duchenne muscular dystrophy\n(DMD) and typically developing (TD) peers are visible to the eye, but\nquantification of those differences outside of the gait laboratory has been\nelusive. We measured vertical, mediolateral, and anteroposterior acceleration\nusing a waist-worn iPhone accelerometer during ambulation across a typical\nrange of velocities. Six TD and six DMD children from 3-15 years of age\nunderwent seven walking/running tasks, including five 25m walk/run tests at a\nslow walk to running speeds, a 6-minute walk test (6MWT), and a\n100-meter-run/walk (100MRW). We extracted temporospatial clinical gait features\n(CFs) and applied multiple Artificial Intelligence (AI) tools to differentiate\nbetween DMD and TD control children using extracted features and raw data.\nExtracted CFs showed reduced step length and a greater mediolateral component\nof total power (TP) consistent with shorter strides and Trendelenberg-like gait\ncommonly observed in DMD. AI methods using CFs and raw data varied\nineffectiveness at differentiating between DMD and TD controls at different\nspeeds, with an accuracy of some methods exceeding 91%. We demonstrate that by\nusing AI tools with accelerometer data from a consumer-level smartphone, we can\nidentify DMD gait disturbance in toddlers to early teens.",
          "link": "http://arxiv.org/abs/2105.06295",
          "publishedOn": "2021-08-03T02:06:34.895Z",
          "wordCount": null,
          "title": "Gait Characterization in Duchenne Muscular Dystrophy (DMD) Using a Single-Sensor Accelerometer: Classical Machine Learning and Deep Learning Approaches. (arXiv:2105.06295v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02521",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kunar_A/0/1/0/all/0/1\">Aditya Kunar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birke_R/0/1/0/all/0/1\">Robert Birke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zilong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lydia Chen</a>",
          "description": "Tabular generative adversarial networks (TGAN) have recently emerged to cater\nto the need of synthesizing tabular data -- the most widely used data format.\nWhile synthetic tabular data offers the advantage of complying with privacy\nregulations, there still exists a risk of privacy leakage via inference attacks\ndue to interpolating the properties of real data during training. Differential\nprivate (DP) training algorithms provide theoretical guarantees for training\nmachine learning models by injecting statistical noise to prevent privacy\nleaks. However, the challenges of applying DP on TGAN are to determine the most\noptimal framework (i.e., PATE/DP-SGD) and neural network (i.e.,\nGenerator/Discriminator)to inject noise such that the data utility is well\nmaintained under a given privacy guarantee. In this paper, we propose DTGAN, a\nnovel conditional Wasserstein tabular GAN that comes in two variants DTGAN_G\nand DTGAN_D, for providing a detailed comparison of tabular GANs trained using\nDP-SGD for the generator vs discriminator, respectively. We elicit the privacy\nanalysis associated with training the generator with complex loss functions\n(i.e., classification and information losses) needed for high quality tabular\ndata synthesis. Additionally, we rigorously evaluate the theoretical privacy\nguarantees offered by DP empirically against membership and attribute inference\nattacks. Our results on 3 datasets show that the DP-SGD framework is superior\nto PATE and that a DP discriminator is more optimal for training convergence.\nThus, we find (i) DTGAN_D is capable of maintaining the highest data utility\nacross 4 ML models by up to 18% in terms of the average precision score for a\nstrict privacy budget, epsilon = 1, as compared to the prior studies and (ii)\nDP effectively prevents privacy loss against inference attacks by restricting\nthe success probability of membership attacks to be close to 50%.",
          "link": "http://arxiv.org/abs/2107.02521",
          "publishedOn": "2021-08-03T02:06:34.895Z",
          "wordCount": null,
          "title": "DTGAN: Differential Private Training for Tabular GANs. (arXiv:2107.02521v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weijia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_L/0/1/0/all/0/1\">Lijun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hengshu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>",
          "description": "Real estate appraisal refers to the process of developing an unbiased opinion\nfor real property's market value, which plays a vital role in decision-making\nfor various players in the marketplace (e.g., real estate agents, appraisers,\nlenders, and buyers). However, it is a nontrivial task for accurate real estate\nappraisal because of three major challenges: (1) The complicated influencing\nfactors for property value; (2) The asynchronously spatiotemporal dependencies\namong real estate transactions; (3) The diversified correlations between\nresidential communities. To this end, we propose a Multi-Task Hierarchical\nGraph Representation Learning (MugRep) framework for accurate real estate\nappraisal. Specifically, by acquiring and integrating multi-source urban data,\nwe first construct a rich feature set to comprehensively profile the real\nestate from multiple perspectives (e.g., geographical distribution, human\nmobility distribution, and resident demographics distribution). Then, an\nevolving real estate transaction graph and a corresponding event graph\nconvolution module are proposed to incorporate asynchronously spatiotemporal\ndependencies among real estate transactions. Moreover, to further incorporate\nvaluable knowledge from the view of residential communities, we devise a\nhierarchical heterogeneous community graph convolution module to capture\ndiversified correlations between residential communities. Finally, an urban\ndistrict partitioned multi-task learning module is introduced to generate\ndifferently distributed value opinions for real estate. Extensive experiments\non two real-world datasets demonstrate the effectiveness of MugRep and its\ncomponents and features.",
          "link": "http://arxiv.org/abs/2107.05180",
          "publishedOn": "2021-08-03T02:06:34.895Z",
          "wordCount": null,
          "title": "MugRep: A Multi-Task Hierarchical Graph Representation Learning Framework for Real Estate Appraisal. (arXiv:2107.05180v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suarez_G/0/1/0/all/0/1\">Gabriel Suarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raful_J/0/1/0/all/0/1\">Juan Raful</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luque_M/0/1/0/all/0/1\">Maria A. Luque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valencia_C/0/1/0/all/0/1\">Carlos F. Valencia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correa_Bahnsen_A/0/1/0/all/0/1\">Alejandro Correa-Bahnsen</a>",
          "description": "This paper presents the advantages of alternative data from Super-Apps to\nenhance user' s income estimation models. It compares the performance of these\nalternative data sources with the performance of industry-accepted bureau\nincome estimators that takes into account only financial system information;\nsuccessfully showing that the alternative data manage to capture information\nthat bureau income estimators do not. By implementing the TreeSHAP method for\nStochastic Gradient Boosting Interpretation, this paper highlights which of the\ncustomer' s behavioral and transactional patterns within a Super-App have a\nstronger predictive power when estimating user' s income. Ultimately, this\npaper shows the incentive for financial institutions to seek to incorporate\nalternative data into constructing their risk profiles.",
          "link": "http://arxiv.org/abs/2104.05831",
          "publishedOn": "2021-08-03T02:06:34.894Z",
          "wordCount": null,
          "title": "Enhancing User' s Income Estimation with Super-App Alternative Data. (arXiv:2104.05831v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Assran_M/0/1/0/all/0/1\">Mahmoud Assran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1\">Mathilde Caron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1\">Piotr Bojanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1\">Nicolas Ballas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1\">Michael Rabbat</a>",
          "description": "This paper proposes a novel method of learning by predicting view assignments\nwith support samples (PAWS). The method trains a model to minimize a\nconsistency loss, which ensures that different views of the same unlabeled\ninstance are assigned similar pseudo-labels. The pseudo-labels are generated\nnon-parametrically, by comparing the representations of the image views to\nthose of a set of randomly sampled labeled images. The distance between the\nview representations and labeled representations is used to provide a weighting\nover class labels, which we interpret as a soft pseudo-label. By\nnon-parametrically incorporating labeled samples in this way, PAWS extends the\ndistance-metric loss used in self-supervised methods such as BYOL and SwAV to\nthe semi-supervised setting. Despite the simplicity of the approach, PAWS\noutperforms other semi-supervised methods across architectures, setting a new\nstate-of-the-art for a ResNet-50 on ImageNet trained with either 10% or 1% of\nthe labels, reaching 75.5% and 66.5% top-1 respectively. PAWS requires 4x to\n12x less training than the previous best methods.",
          "link": "http://arxiv.org/abs/2104.13963",
          "publishedOn": "2021-08-03T02:06:34.894Z",
          "wordCount": null,
          "title": "Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples. (arXiv:2104.13963v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1\">H. Vincent Poor</a>",
          "description": "Owing to the low communication costs and privacy-promoting capabilities,\nFederated Learning (FL) has become a promising tool for training effective\nmachine learning models among distributed clients. However, with the\ndistributed architecture, low quality models could be uploaded to the\naggregator server by unreliable clients, leading to a degradation or even a\ncollapse of training. In this paper, we model these unreliable behaviors of\nclients and propose a defensive mechanism to mitigate such a security risk.\nSpecifically, we first investigate the impact on the models caused by\nunreliable clients by deriving a convergence upper bound on the loss function\nbased on the gradient descent updates. Our theoretical bounds reveal that with\na fixed amount of total computational resources, there exists an optimal number\nof local training iterations in terms of convergence performance. We further\ndesign a novel defensive mechanism, named deep neural network based secure\naggregation (DeepSA). Our experimental results validate our theoretical\nanalysis. In addition, the effectiveness of DeepSA is verified by comparing\nwith other state-of-the-art defensive mechanisms.",
          "link": "http://arxiv.org/abs/2105.06256",
          "publishedOn": "2021-08-03T02:06:34.892Z",
          "wordCount": 645,
          "title": "Federated Learning with Unreliable Clients: Performance Analysis and Mechanism Design. (arXiv:2105.06256v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunguz_B/0/1/0/all/0/1\">Bojan Tunguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titericz_G/0/1/0/all/0/1\">Gilberto Titericz</a>",
          "description": "Black-box optimization is essential for tuning complex machine learning\nalgorithms which are easier to experiment with than to understand. In this\npaper, we show that a simple ensemble of black-box optimization algorithms can\noutperform any single one of them. However, searching for such an optimal\nensemble requires a large number of experiments. We propose a\nMulti-GPU-optimized framework to accelerate a brute force search for the\noptimal ensemble of black-box optimization algorithms by running many\nexperiments in parallel. The lightweight optimizations are performed by CPU\nwhile expensive model training and evaluations are assigned to GPUs. We\nevaluate 15 optimizers by training 2.7 million models and running 541,440\noptimizations. On a DGX-1, the search time is reduced from more than 10 days on\ntwo 20-core CPUs to less than 24 hours on 8-GPUs. With the optimal ensemble\nfound by GPU-accelerated exhaustive search, we won the 2nd place of NeurIPS\n2020 black-box optimization challenge.",
          "link": "http://arxiv.org/abs/2012.04201",
          "publishedOn": "2021-08-03T02:06:34.876Z",
          "wordCount": 648,
          "title": "GPU Accelerated Exhaustive Search for Optimal Ensemble of Black-Box Optimization Algorithms. (arXiv:2012.04201v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.09394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junwoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Youngwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Haneol Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>",
          "description": "Question Answering (QA) is a widely-used framework for developing and\nevaluating an intelligent machine. In this light, QA on Electronic Health\nRecords (EHR), namely EHR QA, can work as a crucial milestone towards\ndeveloping an intelligent agent in healthcare. EHR data are typically stored in\na relational database, which can also be converted to a directed acyclic graph,\nallowing two approaches for EHR QA: Table-based QA and Knowledge Graph-based\nQA. We hypothesize that the graph-based approach is more suitable for EHR QA as\ngraphs can represent relations between entities and values more naturally\ncompared to tables, which essentially require JOIN operations. In this paper,\nwe propose a graph-based EHR QA where natural language queries are converted to\nSPARQL instead of SQL. To validate our hypothesis, we create four EHR QA\ndatasets (graph-based VS table-based, and simplified database schema VS\noriginal database schema), based on a table-based dataset MIMICSQL. We test\nboth a simple Seq2Seq model and a state-of-the-art EHR QA model on all datasets\nwhere the graph-based datasets facilitated up to 34% higher accuracy than the\ntable-based dataset without any modification to the model architectures.\nFinally, all datasets are open-sourced to encourage further EHR QA research in\nboth directions.",
          "link": "http://arxiv.org/abs/2010.09394",
          "publishedOn": "2021-08-03T02:06:34.868Z",
          "wordCount": 675,
          "title": "Knowledge Graph-based Question Answering with Electronic Health Records. (arXiv:2010.09394v2 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nasibullah/0/1/0/all/0/1\">Nasibullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanta_P/0/1/0/all/0/1\">Partha Pratim Mohanta</a>",
          "description": "Video captioning is one of the challenging problems at the intersection of\nvision and language, having many real-life applications in video retrieval,\nvideo surveillance, assisting visually challenged people, Human-machine\ninterface, and many more. Recent deep learning-based methods have shown\npromising results but are still on the lower side than other vision tasks (such\nas image classification, object detection). A significant drawback with\nexisting video captioning methods is that they are optimized over cross-entropy\nloss function, which is uncorrelated to the de facto evaluation metrics (BLEU,\nMETEOR, CIDER, ROUGE).In other words, cross-entropy is not a proper surrogate\nof the true loss function for video captioning. This paper addresses the\ndrawback by introducing a dynamic loss network (DLN), which provides an\nadditional feedback signal that directly reflects the evaluation metrics. Our\nresults on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to\nText (MSRVTT) datasets outperform previous methods.",
          "link": "http://arxiv.org/abs/2107.11707",
          "publishedOn": "2021-08-03T02:06:34.862Z",
          "wordCount": 609,
          "title": "Boosting Video Captioning with Dynamic Loss Network. (arXiv:2107.11707v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04007",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Legarreta_J/0/1/0/all/0/1\">Jon Haitz Legarreta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petit_L/0/1/0/all/0/1\">Laurent Petit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rheault_F/0/1/0/all/0/1\">Fran&#xe7;ois Rheault</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Theaud_G/0/1/0/all/0/1\">Guillaume Theaud</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lemaire_C/0/1/0/all/0/1\">Carl Lemaire</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Descoteaux_M/0/1/0/all/0/1\">Maxime Descoteaux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>",
          "description": "Current brain white matter fiber tracking techniques show a number of\nproblems, including: generating large proportions of streamlines that do not\naccurately describe the underlying anatomy; extracting streamlines that are not\nsupported by the underlying diffusion signal; and under-representing some fiber\npopulations, among others. In this paper, we describe a novel autoencoder-based\nlearning method to filter streamlines from diffusion MRI tractography, and\nhence, to obtain more reliable tractograms. Our method, dubbed FINTA (Filtering\nin Tractography using Autoencoders) uses raw, unlabeled tractograms to train\nthe autoencoder, and to learn a robust representation of brain streamlines.\nSuch an embedding is then used to filter undesired streamline samples using a\nnearest neighbor algorithm. Our experiments on both synthetic and in vivo human\nbrain diffusion MRI tractography data obtain accuracy scores exceeding the 90\\%\nthreshold on the test set. Results reveal that FINTA has a superior filtering\nperformance compared to conventional, anatomy-based methods, and the\nRecoBundles state-of-the-art method. Additionally, we demonstrate that FINTA\ncan be applied to partial tractograms without requiring changes to the\nframework. We also show that the proposed method generalizes well across\ndifferent tracking methods and datasets, and shortens significantly the\ncomputation time for large (>1 M streamlines) tractograms. Together, this work\nbrings forward a new deep learning framework in tractography based on\nautoencoders, which offers a flexible and powerful method for white matter\nfiltering and bundling that could enhance tractometry and connectivity\nanalyses.",
          "link": "http://arxiv.org/abs/2010.04007",
          "publishedOn": "2021-08-03T02:06:34.855Z",
          "wordCount": 748,
          "title": "Filtering in tractography using autoencoders (FINTA). (arXiv:2010.04007v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>",
          "description": "Modeling complex physical dynamics is a fundamental task in science and\nengineering. Traditional physics-based models are interpretable but rely on\nrigid assumptions. And the direct numerical approximation is usually\ncomputationally intensive, requiring significant computational resources and\nexpertise. While deep learning (DL) provides novel alternatives for efficiently\nrecognizing complex patterns and emulating nonlinear dynamics, it does not\nnecessarily obey the governing laws of physical systems, nor do they generalize\nwell across different systems. Thus, the study of physics-guided DL emerged and\nhas gained great progress. It aims to take the best from both physics-based\nmodeling and state-of-the-art DL models to better solve scientific problems. In\nthis paper, we provide a structured overview of existing methodologies of\nintegrating prior physical knowledge or physics-based modeling into DL and\ndiscuss the emerging opportunities.",
          "link": "http://arxiv.org/abs/2107.01272",
          "publishedOn": "2021-08-03T02:06:34.848Z",
          "wordCount": 579,
          "title": "Physics-Guided Deep Learning for Dynamical Systems: A survey. (arXiv:2107.01272v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.06606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Arghyadip Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1\">R. Srikant</a>",
          "description": "In the regret-based formulation of multi-armed bandit (MAB) problems, except\nin rare instances, much of the literature focuses on arms with i.i.d. rewards.\nIn this paper, we consider the problem of obtaining regret guarantees for MAB\nproblems in which the rewards of each arm form a Markov chain which may not\nbelong to a single parameter exponential family. To achieve logarithmic regret\nin such problems is not difficult: a variation of standard KL-UCB does the job.\nHowever, the constants obtained from such an analysis are poor for the\nfollowing reason: i.i.d. rewards are a special case of Markov rewards and it is\ndifficult to design an algorithm that works well independent of whether the\nunderlying model is truly Markovian or i.i.d. To overcome this issue, we\nintroduce a novel algorithm that identifies whether the rewards from each arm\nare truly Markovian or i.i.d. using a Hellinger distance-based test. Our\nalgorithm then switches from using a standard KL-UCB to a specialized version\nof KL-UCB when it determines that the arm reward is Markovian, thus resulting\nin low regret for both i.i.d. and Markovian settings.",
          "link": "http://arxiv.org/abs/2009.06606",
          "publishedOn": "2021-08-03T02:06:34.842Z",
          "wordCount": 649,
          "title": "Adaptive KL-UCB based Bandit Algorithms for Markovian and i.i.d. Settings. (arXiv:2009.06606v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Si_N/0/1/0/all/0/1\">Nian Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhengyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanchet_J/0/1/0/all/0/1\">Jose Blanchet</a>",
          "description": "Policy learning using historical observational data is an important problem\nthat has found widespread applications. Examples include selecting offers,\nprices, advertisements to send to customers, as well as selecting which\nmedication to prescribe to a patient. However, existing literature rests on the\ncrucial assumption that the future environment where the learned policy will be\ndeployed is the same as the past environment that has generated the data--an\nassumption that is often false or too coarse an approximation. In this paper,\nwe lift this assumption and aim to learn a distributional robust policy with\nincomplete (bandit) observational data. We propose a novel learning algorithm\nthat is able to learn a robust policy to adversarial perturbations and unknown\ncovariate shifts. We first present a policy evaluation procedure in the\nambiguous environment and then give a performance guarantee based on the theory\nof uniform convergence. Additionally, we also give a heuristic algorithm to\nsolve the distributional robust policy learning problems efficiently. Finally,\nwe demonstrate the robustness of our methods in the synthetic and real-world\ndatasets.",
          "link": "http://arxiv.org/abs/2006.05630",
          "publishedOn": "2021-08-03T02:06:34.822Z",
          "wordCount": 660,
          "title": "Distributional Robust Batch Contextual Bandits. (arXiv:2006.05630v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Siqi Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiangjing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>",
          "description": "Federated learning (FL) is a machine learning field in which researchers try\nto facilitate model learning process among multiparty without violating privacy\nprotection regulations. Considerable effort has been invested in FL\noptimization and communication related researches. In this work, we introduce\nFedLab, a lightweight open-source framework for FL simulation. The design of\nFedLab focuses on FL algorithm effectiveness and communication efficiency.\nAlso, FedLab is scalable in different deployment scenario. We hope FedLab could\nprovide flexible API as well as reliable baseline implementations, and relieve\nthe burden of implementing novel approaches for researchers in FL community.",
          "link": "http://arxiv.org/abs/2107.11621",
          "publishedOn": "2021-08-03T02:06:34.816Z",
          "wordCount": 550,
          "title": "FedLab: A Flexible Federated Learning Framework. (arXiv:2107.11621v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Illarionov_E/0/1/0/all/0/1\">E. Illarionov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Temirchev_P/0/1/0/all/0/1\">P. Temirchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voloskov_D/0/1/0/all/0/1\">D. Voloskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostoev_R/0/1/0/all/0/1\">R. Kostoev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonov_M/0/1/0/all/0/1\">M. Simonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pissarenko_D/0/1/0/all/0/1\">D. Pissarenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orlov_D/0/1/0/all/0/1\">D. Orlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koroteev_D/0/1/0/all/0/1\">D. Koroteev</a>",
          "description": "Reservoir simulation and adaptation (also known as history matching) are\ntypically considered as separate problems. While a set of models are aimed at\nthe solution of the forward simulation problem assuming all initial geological\nparameters are known, the other set of models adjust geological parameters\nunder the fixed forward simulation model to fit production data. This results\nin many difficulties for both reservoir engineers and developers of new\nefficient computation schemes. We present a unified approach to reservoir\nsimulation and adaptation problems. A single neural network model allows a\nforward pass from initial geological parameters of the 3D reservoir model\nthrough dynamic state variables to well's production rates and backward\ngradient propagation to any model inputs and variables. The model fitting and\ngeological parameters adaptation both become the optimization problem over\nspecific parts of the same neural network model. Standard gradient-based\noptimization schemes can be used to find the optimal solution. Using real-world\noilfield model and historical production rates we demonstrate that the\nsuggested approach allows reservoir simulation and history matching with a\nbenefit of several orders of magnitude simulation speed-up. Finally, to\npropagate this research we open-source a Python-based framework DeepField that\nallows standard processing of reservoir models and reproducing the approach\npresented in this paper.",
          "link": "http://arxiv.org/abs/2102.10304",
          "publishedOn": "2021-08-03T02:06:34.809Z",
          "wordCount": null,
          "title": "End-to-end neural network approach to 3D reservoir simulation and adaptation. (arXiv:2102.10304v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09543",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1\">Richard Osuala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1\">Kaisar Kushibar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garrucho_L/0/1/0/all/0/1\">Lidia Garrucho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1\">Akis Linardos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szafranowska_Z/0/1/0/all/0/1\">Zuzanna Szafranowska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1\">Stefan Klein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1\">Oliver Diaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>",
          "description": "Despite technological and medical advances, the detection, interpretation,\nand treatment of cancer based on imaging data continue to pose significant\nchallenges. These include high inter-observer variability, difficulty of\nsmall-sized lesion detection, nodule interpretation and malignancy\ndetermination, inter- and intra-tumour heterogeneity, class imbalance,\nsegmentation inaccuracies, and treatment effect uncertainty. The recent\nadvancements in Generative Adversarial Networks (GANs) in computer vision as\nwell as in medical imaging may provide a basis for enhanced capabilities in\ncancer detection and analysis. In this review, we assess the potential of GANs\nto address a number of key challenges of cancer imaging, including data\nscarcity and imbalance, domain and dataset shifts, data access and privacy,\ndata annotation and quantification, as well as cancer detection, tumour\nprofiling and treatment planning. We provide a critical appraisal of the\nexisting literature of GANs applied to cancer imagery, together with\nsuggestions on future research directions to address these challenges. We\nanalyse and discuss 163 papers that apply adversarial training techniques in\nthe context of cancer imaging and elaborate their methodologies, advantages and\nlimitations. With this work, we strive to bridge the gap between the needs of\nthe clinical cancer imaging community and the current and prospective research\non GANs in the artificial intelligence community.",
          "link": "http://arxiv.org/abs/2107.09543",
          "publishedOn": "2021-08-03T02:06:34.809Z",
          "wordCount": null,
          "title": "A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions. (arXiv:2107.09543v1 [eess.IV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.05260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baharani_M/0/1/0/all/0/1\">Mohammadreza Baharani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1\">Hamed Tabkhi</a>",
          "description": "This paper presents a scalable deep learning model called Agile Temporal\nConvolutional Network (ATCN) for high-accurate fast classification and time\nseries prediction in resource-constrained embedded systems. ATCN is a family of\ncompact networks with formalized hyperparameters that enable\napplication-specific adjustments to be made to the model architecture. It is\nprimarily designed for embedded edge devices with very limited performance and\nmemory, such as wearable biomedical devices and real-time reliability\nmonitoring systems. ATCN makes fundamental improvements over the mainstream\ntemporal convolutional neural networks, including residual connections as time\nattention machines to increase the network depth and accuracy and the\nincorporation of separable depth-wise convolution to reduce the computational\ncomplexity of the model. As part of the present work, three ATCN families,\nnamely T0, T1, and T2, are also presented and evaluated on different ranges of\nembedded processors - Cortex-M7 and Cortex-A57 processor. An evaluation of the\nATCN models against the best-in-class InceptionTime shows that ATCN improves\nboth accuracy and execution time on a broad range of embedded and\ncyber-physical applications with demand for real-time processing on the\nembedded edge. At the same time, in contrast to existing solutions, ATCN is the\nfirst deep learning-based approach that can be run on embedded microcontrollers\n(Cortex-M7) with limited computational performance and memory capacity while\ndelivering state-of-the-art accuracy.",
          "link": "http://arxiv.org/abs/2011.05260",
          "publishedOn": "2021-08-03T02:06:34.808Z",
          "wordCount": null,
          "title": "ATCN: Resource-Efficient Processing of Time Series on Edge. (arXiv:2011.05260v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07405",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1\">Wu Lin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1\">Frank Nielsen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Emtiyaz Khan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1\">Mark Schmidt</a>",
          "description": "Natural-gradient descent (NGD) on structured parameter spaces (e.g., low-rank\ncovariances) is computationally challenging due to difficult Fisher-matrix\ncomputations. We address this issue by using \\emph{local-parameter coordinates}\nto obtain a flexible and efficient NGD method that works well for a\nwide-variety of structured parameterizations. We show four applications where\nour method (1) generalizes the exponential natural evolutionary strategy, (2)\nrecovers existing Newton-like algorithms, (3) yields new structured\nsecond-order algorithms via matrix groups, and (4) gives new algorithms to\nlearn covariances of Gaussian and Wishart-based distributions. We show results\non a range of problems from deep learning, variational inference, and evolution\nstrategies. Our work opens a new direction for scalable structured geometric\nmethods.",
          "link": "http://arxiv.org/abs/2102.07405",
          "publishedOn": "2021-08-03T02:06:34.808Z",
          "wordCount": null,
          "title": "Tractable structured natural gradient descent using local parameterizations. (arXiv:2102.07405v7 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Nan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zepeng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianfeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askari_H/0/1/0/all/0/1\">Hassan Askari</a>",
          "description": "Autonomous vehicles are most concerned about safety control issues, and the\nslip ratio is critical to the safety of the vehicle control system. In this\npaper, different machine learning algorithms (Neural Networks, Gradient\nBoosting Machine, Random Forest, and Support Vector Machine) are used to train\nthe slip ratio estimation model based on the acceleration signals ($a_x$,\n$a_y$, and $a_z$) from the tri-axial Micro-Electro Mechanical System (MEMS)\naccelerometer utilized in the intelligent tire system, where the acceleration\nsignals are divided into four sets ($a_x/a_y/a_z$, $a_x/a_z$, $a_y/a_z$, and\n$a_z$) as algorithm inputs. The experimental data used in this study are\ncollected through the MTS Flat-Trac tire test platform. Performance of\ndifferent slip ratio estimation models is compared using the NRMS errors in\n10-fold cross-validation (CV). The results indicate that NN and GBM have more\npromising accuracy, and the $a_z$ input type has a better performance compared\nto other input types, with the best result being the estimation model of the NN\nalgorithm with $a_z$ as input, which results is 4.88\\%. The present study with\nthe fusion of intelligent tire system and machine learning paves the way for\nthe accurate estimation of tire slip ratio under different driving conditions,\nwhich will open up a new way of Autonomous vehicles, intelligent tires, and\ntire slip ratio estimation.",
          "link": "http://arxiv.org/abs/2106.08961",
          "publishedOn": "2021-08-03T02:06:34.808Z",
          "wordCount": null,
          "title": "Intelligent-Tire-Based Slip Ratio Estimation Using Machine Learning. (arXiv:2106.08961v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.01037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiamei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1\">Sebastian Lapuschkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1\">Wojciech Samek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1\">Alexander Binder</a>",
          "description": "This paper analyzes the predictions of image captioning models with attention\nmechanisms beyond visualizing the attention itself. We develop variants of\nlayer-wise relevance propagation (LRP) and gradient-based explanation methods,\ntailored to image captioning models with attention mechanisms. We compare the\ninterpretability of attention heatmaps systematically against the explanations\nprovided by explanation methods such as LRP, Grad-CAM, and Guided Grad-CAM. We\nshow that explanation methods provide simultaneously pixel-wise image\nexplanations (supporting and opposing pixels of the input image) and linguistic\nexplanations (supporting and opposing words of the preceding sequence) for each\nword in the predicted captions. We demonstrate with extensive experiments that\nexplanation methods 1) can reveal additional evidence used by the model to make\ndecisions compared to attention; 2) correlate to object locations with high\nprecision; 3) are helpful to \"debug\" the model, e.g. by analyzing the reasons\nfor hallucinated object words. With the observed properties of explanations, we\nfurther design an LRP-inference fine-tuning strategy that reduces the issue of\nobject hallucination in image captioning models, and meanwhile, maintains the\nsentence fluency. We conduct experiments with two widely used attention\nmechanisms: the adaptive attention mechanism calculated with the additive\nattention and the multi-head attention mechanism calculated with the scaled dot\nproduct.",
          "link": "http://arxiv.org/abs/2001.01037",
          "publishedOn": "2021-08-03T02:06:34.807Z",
          "wordCount": null,
          "title": "Explain and Improve: LRP-Inference Fine-Tuning for Image Captioning Models. (arXiv:2001.01037v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aqajari_S/0/1/0/all/0/1\">Seyed Amir Hossein Aqajari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zargari_A/0/1/0/all/0/1\">Amir Hosein Afandizadeh Zargari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1\">Amir M. Rahmani</a>",
          "description": "Respiratory rate (RR) is a clinical sign representing ventilation. An\nabnormal change in RR is often the first sign of health deterioration as the\nbody attempts to maintain oxygen delivery to its tissues. There has been a\ngrowing interest in remotely monitoring of RR in everyday settings which has\nmade photoplethysmography (PPG) monitoring wearable devices an attractive\nchoice. PPG signals are useful sources for RR extraction due to the presence of\nrespiration-induced modulations in them. The existing PPG-based RR estimation\nmethods mainly rely on hand-crafted rules and manual parameters tuning. An\nend-to-end deep learning approach was recently proposed, however, despite its\nautomatic nature, the performance of this method is not ideal using the real\nworld data. In this paper, we present an end-to-end and accurate pipeline for\nRR estimation using Cycle Generative Adversarial Networks (CycleGAN) to\nreconstruct respiratory signals from raw PPG signals. Our results demonstrate a\nhigher RR estimation accuracy of up to 2$\\times$ (mean absolute error of\n1.9$\\pm$0.3 using five fold cross validation) compared to the state-of-th-art\nusing a identical publicly available dataset. Our results suggest that CycleGAN\ncan be a valuable method for RR estimation from raw PPG signals.",
          "link": "http://arxiv.org/abs/2105.00594",
          "publishedOn": "2021-08-03T02:06:34.807Z",
          "wordCount": null,
          "title": "An End-to-End and Accurate PPG-based Respiratory Rate Estimation Approach Using Cycle Generative Adversarial Networks. (arXiv:2105.00594v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Amish Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1\">Sourav Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datar_A/0/1/0/all/0/1\">Arnhav Datar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadiwala_J/0/1/0/all/0/1\">Juned Kadiwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1\">Hrithwik Shalu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1\">Jimson Mathew</a>",
          "description": "Reliable detection of the prodromal stages of Alzheimer's disease (AD)\nremains difficult even today because, unlike other neurocognitive impairments,\nthere is no definitive diagnosis of AD in vivo. In this context, existing\nresearch has shown that patients often develop language impairment even in mild\nAD conditions. We propose a multimodal deep learning method that utilizes\nspeech and the corresponding transcript simultaneously to detect AD. For audio\nsignals, the proposed audio-based network, a convolutional neural network (CNN)\nbased model, predicts the diagnosis for multiple speech segments, which are\ncombined for the final prediction. Similarly, we use contextual embedding\nextracted from BERT concatenated with a CNN-generated embedding for classifying\nthe transcript. The individual predictions of the two models are then combined\nto make the final classification. We also perform experiments to analyze the\nmodel performance when Automated Speech Recognition (ASR) system generated\ntranscripts are used instead of manual transcription in the text-based model.\nThe proposed method achieves 85.3% 10-fold cross-validation accuracy when\ntrained and evaluated on the Dementiabank Pitt corpus.",
          "link": "http://arxiv.org/abs/2012.00096",
          "publishedOn": "2021-08-03T02:06:34.806Z",
          "wordCount": null,
          "title": "Multi-Modal Detection of Alzheimer's Disease from Speech and Text. (arXiv:2012.00096v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kunhong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yucheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yahong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingshuai Li</a>",
          "description": "In recent years, researchers have been paying increasing attention to the\nthreats brought by deep learning models to data security and privacy,\nespecially in the field of domain adaptation. Existing unsupervised domain\nadaptation (UDA) methods can achieve promising performance without transferring\ndata from source domain to target domain. However, UDA with representation\nalignment or self-supervised pseudo-labeling relies on the transferred source\nmodels. In many data-critical scenarios, methods based on model transferring\nmay suffer from membership inference attacks and expose private data. In this\npaper, we aim to overcome a challenging new setting where the source models are\nonly queryable but cannot be transferred to the target domain. We propose\nBlack-box Probe Domain Adaptation (BPDA), which adopts query mechanism to probe\nand refine information from source model using third-party dataset. In order to\ngain more informative query results, we further propose Distributionally\nAdversarial Training (DAT) to align the distribution of third-party data with\nthat of target data. BPDA uses public third-party dataset and adversarial\nexamples based on DAT as the information carrier between source and target\ndomains, dispensing with transferring source data or model. Experimental\nresults on benchmarks of Digit-Five, Office-Caltech, Office-31, Office-Home,\nand DomainNet demonstrate the feasibility of BPDA without model transferring.",
          "link": "http://arxiv.org/abs/2107.10174",
          "publishedOn": "2021-08-03T02:06:34.806Z",
          "wordCount": null,
          "title": "Black-box Probe for Unsupervised Domain Adaptation without Model Transferring. (arXiv:2107.10174v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.08708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1\">Uttaran Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roncal_C/0/1/0/all/0/1\">Christian Roncal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_T/0/1/0/all/0/1\">Trisha Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohan Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapsaskis_K/0/1/0/all/0/1\">Kyra Kapsaskis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_K/0/1/0/all/0/1\">Kurt Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1\">Aniket Bera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>",
          "description": "We present an autoencoder-based semi-supervised approach to classify\nperceived human emotions from walking styles obtained from videos or\nmotion-captured data and represented as sequences of 3D poses. Given the motion\non each joint in the pose at each time step extracted from 3D pose sequences,\nwe hierarchically pool these joint motions in a bottom-up manner in the\nencoder, following the kinematic chains in the human body. We also constrain\nthe latent embeddings of the encoder to contain the space of\npsychologically-motivated affective features underlying the gaits. We train the\ndecoder to reconstruct the motions per joint per time step in a top-down manner\nfrom the latent embeddings. For the annotated data, we also train a classifier\nto map the latent embeddings to emotion labels. Our semi-supervised approach\nachieves a mean average precision of 0.84 on the Emotion-Gait benchmark\ndataset, which contains both labeled and unlabeled gaits collected from\nmultiple sources. We outperform current state-of-art algorithms for both\nemotion recognition and action recognition from 3D gaits by 7%--23% on the\nabsolute. More importantly, we improve the average precision by 10%--50% on the\nabsolute on classes that each makes up less than 25% of the labeled part of the\nEmotion-Gait benchmark dataset.",
          "link": "http://arxiv.org/abs/1911.08708",
          "publishedOn": "2021-08-03T02:06:34.805Z",
          "wordCount": null,
          "title": "Take an Emotion Walk: Perceiving Emotions from Gaits Using Hierarchical Attention Pooling and Affective Mapping. (arXiv:1911.08708v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenmann_M/0/1/0/all/0/1\">Matthias Eisenmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarikaya_D/0/1/0/all/0/1\">Duygu Sarikaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marz_K/0/1/0/all/0/1\">Keno M&#xe4;rz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_T/0/1/0/all/0/1\">Toby Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malpani_A/0/1/0/all/0/1\">Anand Malpani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallert_J/0/1/0/all/0/1\">Johannes Fallert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feussner_H/0/1/0/all/0/1\">Hubertus Feussner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannarou_S/0/1/0/all/0/1\">Stamatia Giannarou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1\">Pietro Mascagni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakawala_H/0/1/0/all/0/1\">Hirenkumar Nakawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_A/0/1/0/all/0/1\">Adrian Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pugh_C/0/1/0/all/0/1\">Carla Pugh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedula_S/0/1/0/all/0/1\">Swaroop S. Vedula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cleary_K/0/1/0/all/0/1\">Kevin Cleary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fichtinger_G/0/1/0/all/0/1\">Gabor Fichtinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forestier_G/0/1/0/all/0/1\">Germain Forestier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibaud_B/0/1/0/all/0/1\">Bernard Gibaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grantcharov_T/0/1/0/all/0/1\">Teodor Grantcharov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashizume_M/0/1/0/all/0/1\">Makoto Hashizume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckmann_Notzel_D/0/1/0/all/0/1\">Doreen Heckmann-N&#xf6;tzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenngott_H/0/1/0/all/0/1\">Hannes G. Kenngott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kikinis_R/0/1/0/all/0/1\">Ron Kikinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mundermann_L/0/1/0/all/0/1\">Lars M&#xfc;ndermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onogur_S/0/1/0/all/0/1\">Sinan Onogur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1\">Raphael Sznitman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1\">Russell H. Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu D. Tizabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1\">Martin Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1\">Gregory D. Hager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumuth_T/0/1/0/all/0/1\">Thomas Neumuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_J/0/1/0/all/0/1\">Justin Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gockel_I/0/1/0/all/0/1\">Ines Gockel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goedeke_J/0/1/0/all/0/1\">Jan Goedeke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_D/0/1/0/all/0/1\">Daniel A. Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joyeux_L/0/1/0/all/0/1\">Luc Joyeux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1\">Kyle Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leff_D/0/1/0/all/0/1\">Daniel R. Leff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madani_A/0/1/0/all/0/1\">Amin Madani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_H/0/1/0/all/0/1\">Hani J. Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meireles_O/0/1/0/all/0/1\">Ozanan Meireles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seitel_A/0/1/0/all/0/1\">Alexander Seitel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teber_D/0/1/0/all/0/1\">Dogu Teber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uckert_F/0/1/0/all/0/1\">Frank &#xdc;ckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Stich_B/0/1/0/all/0/1\">Beat P. M&#xfc;ller-Stich</a>, et al. (2 additional authors not shown)",
          "description": "Recent developments in data science in general and machine learning in\nparticular have transformed the way experts envision the future of surgery.\nSurgical Data Science (SDS) is a new research field that aims to improve the\nquality of interventional healthcare through the capture, organization,\nanalysis and modeling of data. While an increasing number of data-driven\napproaches and clinical applications have been studied in the fields of\nradiological and clinical data science, translational success stories are still\nlacking in surgery. In this publication, we shed light on the underlying\nreasons and provide a roadmap for future advances in the field. Based on an\ninternational workshop involving leading researchers in the field of SDS, we\nreview current practice, key achievements and initiatives as well as available\nstandards and tools for a number of topics relevant to the field, namely (1)\ninfrastructure for data acquisition, storage and access in the presence of\nregulatory constraints, (2) data annotation and sharing and (3) data analytics.\nWe further complement this technical perspective with (4) a review of currently\navailable SDS products and the translational progress from academia and (5) a\nroadmap for faster clinical translation and exploitation of the full potential\nof SDS, based on an international multi-round Delphi process.",
          "link": "http://arxiv.org/abs/2011.02284",
          "publishedOn": "2021-08-03T02:06:34.804Z",
          "wordCount": null,
          "title": "Surgical Data Science -- from Concepts toward Clinical Translation. (arXiv:2011.02284v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.01651",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Chia Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1\">Marina Meil&#x103;</a>",
          "description": "Many manifold embedding algorithms fail apparently when the data manifold has\na large aspect ratio (such as a long, thin strip). Here, we formulate success\nand failure in terms of finding a smooth embedding, showing also that the\nproblem is pervasive and more complex than previously recognized.\nMathematically, success is possible under very broad conditions, provided that\nembedding is done by carefully selected eigenfunctions of the Laplace-Beltrami\noperator $\\Delta$. Hence, we propose a bicriterial Independent Eigencoordinate\nSelection (IES) algorithm that selects smooth embeddings with few eigenvectors.\nThe algorithm is grounded in theory, has low computational overhead, and is\nsuccessful on synthetic and large real data.",
          "link": "http://arxiv.org/abs/1907.01651",
          "publishedOn": "2021-08-03T02:06:34.802Z",
          "wordCount": 565,
          "title": "Selecting the independent coordinates of manifolds with large aspect ratios. (arXiv:1907.01651v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.01437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tianyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "We present several new complexity results for the algorithms that\napproximately solve the optimal transport (OT) problem between two discrete\nprobability measures with at most $n$ atoms. First, we improve the complexity\nbound of a greedy variant of the Sinkhorn algorithm, known as\n\\textit{Greenkhorn} algorithm, from $\\widetilde{O}(n^2\\varepsilon^{-3})$ to\n$\\widetilde{O}(n^2\\varepsilon^{-2})$. Notably, this matches the best known\ncomplexity bound of the Sinkhorn algorithm and sheds the light to superior\npractical performance of the Greenkhorn algorithm. Second, we generalize an\nadaptive primal-dual accelerated gradient descent (APDAGD)\nalgorithm~\\citep{Dvurechensky-2018-Computational} with mirror mapping $\\phi$\nand prove that the resulting APDAMD algorithm achieves the complexity bound of\n$\\widetilde{O}(n^2\\sqrt{\\delta}\\varepsilon^{-1})$ where $\\delta>0$ refers to\nthe regularity of $\\phi$. We demonstrate that the complexity bound of\n$\\widetilde{O}(\\min\\{n^{9/4}\\varepsilon^{-1}, n^2\\varepsilon^{-2}\\})$ is\ninvalid for the APDAGD algorithm and establish a new complexity bound of\n$\\widetilde{O}(n^{5/2}\\varepsilon^{-1})$. Moreover, we propose a\n\\textit{deterministic} accelerated Sinkhorn algorithm and prove that it\nachieves the complexity bound of $\\widetilde{O}(n^{7/3}\\varepsilon^{-4/3})$ by\nincorporating an estimate sequence. Therefore, the accelerated Sinkhorn\nalgorithm outperforms the Sinkhorn and Greenkhorn algorithms in terms of\n$1/\\varepsilon$ and the APDAGD and accelerated alternating\nminimization~\\citep{Guminov-2021-Combination} algorithms in terms of $n$.\nFinally, we conduct experiments on synthetic data and real images with the\nproposed algorithms in the paper and demonstrate their efficiency via numerical\nresults.",
          "link": "http://arxiv.org/abs/1906.01437",
          "publishedOn": "2021-08-03T02:06:34.795Z",
          "wordCount": 763,
          "title": "On the Efficiency of Sinkhorn and Greenkhorn and Their Acceleration for Optimal Transport. (arXiv:1906.01437v7 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.04005",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Maddalena_E/0/1/0/all/0/1\">Emilio T. Maddalena</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scharnhorst_P/0/1/0/all/0/1\">Paul Scharnhorst</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jones_C/0/1/0/all/0/1\">Colin N. Jones</a>",
          "description": "We consider the problem of reconstructing a function from a finite set of\nnoise-corrupted samples. Two kernel algorithms are analyzed, namely kernel\nridge regression and $\\varepsilon$-support vector regression. By assuming the\nground-truth function belongs to the reproducing kernel Hilbert space of the\nchosen kernel, and the measurement noise affecting the dataset is bounded, we\nadopt an approximation theory viewpoint to establish \\textit{deterministic},\nfinite-sample error bounds for the two models. Finally, we discuss their\nconnection with Gaussian processes and two numerical examples are provided. In\nestablishing our inequalities, we hope to help bring the fields of\nnon-parametric kernel learning and system identification for robust control\ncloser to each other.",
          "link": "http://arxiv.org/abs/2008.04005",
          "publishedOn": "2021-08-03T02:06:34.780Z",
          "wordCount": 584,
          "title": "Deterministic error bounds for kernel-based learning techniques under bounded noise. (arXiv:2008.04005v3 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1\">Anusua Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1\">Mohit Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1\">Nikhil Kumar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinsche_M/0/1/0/all/0/1\">Markus Hinsche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Prashant Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matiaschek_M/0/1/0/all/0/1\">Markus Matiaschek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behrens_T/0/1/0/all/0/1\">Tristan Behrens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Militeri_M/0/1/0/all/0/1\">Mirco Militeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birge_C/0/1/0/all/0/1\">Cameron Birge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaushik_S/0/1/0/all/0/1\">Shivangi Kaushik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_A/0/1/0/all/0/1\">Archisman Mohapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_R/0/1/0/all/0/1\">Rita Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1\">Rahul Dodhia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1\">Juan Lavista Ferres</a>",
          "description": "Malnutrition is a global health crisis and is the leading cause of death\namong children under five. Detecting malnutrition requires anthropometric\nmeasurements of weight, height, and middle-upper arm circumference. However,\nmeasuring them accurately is a challenge, especially in the global south, due\nto limited resources. In this work, we propose a CNN-based approach to estimate\nthe height of standing children under five years from depth images collected\nusing a smart-phone. According to the SMART Methodology Manual [5], the\nacceptable accuracy for height is less than 1.4 cm. On training our deep\nlearning model on 87131 depth images, our model achieved an average mean\nabsolute error of 1.64% on 57064 test images. For 70.3% test images, we\nestimated height accurately within the acceptable 1.4 cm range. Thus, our\nproposed solution can accurately detect stunting (low height-for-age) in\nstanding children below five years of age.",
          "link": "http://arxiv.org/abs/2105.01688",
          "publishedOn": "2021-08-03T02:06:34.773Z",
          "wordCount": 641,
          "title": "Height Estimation of Children under Five Years using Depth Images. (arXiv:2105.01688v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06396",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Sengupta_U/0/1/0/all/0/1\">Ushnish Sengupta</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Waxenegger_Wilfing_G/0/1/0/all/0/1\">G&#xfc;nther Waxenegger-Wilfing</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Martin_J/0/1/0/all/0/1\">Jan Martin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hardi_J/0/1/0/all/0/1\">Justin Hardi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Juniper_M/0/1/0/all/0/1\">Matthew P. Juniper</a>",
          "description": "The 100 MW cryogenic liquid oxygen/hydrogen multi-injector combustor BKD\noperated by the DLR Institute of Space Propulsion is a research platform that\nallows the study of thermoacoustic instabilities under realistic conditions,\nrepresentative of small upper stage rocket engines. We use data from BKD\nexperimental campaigns in which the static chamber pressure and fuel-oxidizer\nratio are varied such that the first tangential mode of the combustor is\nexcited under some conditions. We train an autoregressive Bayesian neural\nnetwork model to forecast the amplitude of the dynamic pressure time series,\ninputting multiple sensor measurements (injector pressure/ temperature\nmeasurements, static chamber pressure, high-frequency dynamic pressure\nmeasurements, high-frequency OH* chemiluminescence measurements) and future\nflow rate control signals. The Bayesian nature of our algorithms allows us to\nwork with a dataset whose size is restricted by the expense of each\nexperimental run, without making overconfident extrapolations. We find that the\nnetworks are able to accurately forecast the evolution of the pressure\namplitude and anticipate instability events on unseen experimental runs 500\nmilliseconds in advance. We compare the predictive accuracy of multiple models\nusing different combinations of sensor inputs. We find that the high-frequency\ndynamic pressure signal is particularly informative. We also use the technique\nof integrated gradients to interpret the influence of different sensor inputs\non the model prediction. The negative log-likelihood of data points in the test\ndataset indicates that predictive uncertainties are well-characterized by our\nBayesian model and simulating a sensor failure event results as expected in a\ndramatic increase in the epistemic component of the uncertainty.",
          "link": "http://arxiv.org/abs/2107.06396",
          "publishedOn": "2021-08-03T02:06:34.765Z",
          "wordCount": 724,
          "title": "Forecasting Thermoacoustic Instabilities in Liquid Propellant Rocket Engines Using Multimodal Bayesian Deep Learning. (arXiv:2107.06396v2 [physics.flu-dyn] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.13086",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goldsteen_A/0/1/0/all/0/1\">Abigail Goldsteen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezov_G/0/1/0/all/0/1\">Gilad Ezov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1\">Ron Shmelkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moffie_M/0/1/0/all/0/1\">Micha Moffie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farkash_A/0/1/0/all/0/1\">Ariel Farkash</a>",
          "description": "There is a known tension between the need to analyze personal data to drive\nbusiness and privacy concerns. Many data protection regulations, including the\nEU General Data Protection Regulation (GDPR) and the California Consumer\nProtection Act (CCPA), set out strict restrictions and obligations on the\ncollection and processing of personal data. Moreover, machine learning models\nthemselves can be used to derive personal information, as demonstrated by\nrecent membership and attribute inference attacks. Anonymized data, however, is\nexempt from the obligations set out in these regulations. It is therefore\ndesirable to be able to create models that are anonymized, thus also exempting\nthem from those obligations, in addition to providing better protection against\nattacks.\n\nLearning on anonymized data typically results in significant degradation in\naccuracy. In this work, we propose a method that is able to achieve better\nmodel accuracy by using the knowledge encoded within the trained model, and\nguiding our anonymization process to minimize the impact on the model's\naccuracy, a process we call accuracy-guided anonymization. We demonstrate that\nby focusing on the model's accuracy rather than generic information loss\nmeasures, our method outperforms state of the art k-anonymity methods in terms\nof the achieved utility, in particular with high values of k and large numbers\nof quasi-identifiers.\n\nWe also demonstrate that our approach has a similar, and sometimes even\nbetter ability to prevent membership inference attacks as approaches based on\ndifferential privacy, while averting some of their drawbacks such as\ncomplexity, performance overhead and model-specific implementations. This makes\nmodel-guided anonymization a legitimate substitute for such methods and a\npractical approach to creating privacy-preserving models.",
          "link": "http://arxiv.org/abs/2007.13086",
          "publishedOn": "2021-08-03T02:06:34.716Z",
          "wordCount": 735,
          "title": "Anonymizing Machine Learning Models. (arXiv:2007.13086v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+K_P/0/1/0/all/0/1\">Priyadarshini K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Siddhartha Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borkar_V/0/1/0/all/0/1\">Vivek Borkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Subhasis Chaudhuri</a>",
          "description": "Active metric learning is the problem of incrementally selecting high-utility\nbatches of training data (typically, ordered triplets) to annotate, in order to\nprogressively improve a learned model of a metric over some input domain as\nrapidly as possible. Standard approaches, which independently assess the\ninformativeness of each triplet in a batch, are susceptible to highly\ncorrelated batches with many redundant triplets and hence low overall utility.\nWhile a recent work \\cite{kumari2020batch} proposes batch-decorrelation\nstrategies for metric learning, they rely on ad hoc heuristics to estimate the\ncorrelation between two triplets at a time. We present a novel batch active\nmetric learning method that leverages the Maximum Entropy Principle to learn\nthe least biased estimate of triplet distribution for a given set of prior\nconstraints. To avoid redundancy between triplets, our method collectively\nselects batches with maximum joint entropy, which simultaneously captures both\ninformativeness and diversity. We take advantage of the submodularity of the\njoint entropy function to construct a tractable solution using an efficient\ngreedy algorithm based on Gram-Schmidt orthogonalization that is provably\n$\\left( 1 - \\frac{1}{e} \\right)$-optimal. Our approach is the first batch\nactive metric learning method to define a unified score that balances\ninformativeness and diversity for an entire batch of triplets. Experiments with\nseveral real-world datasets demonstrate that our algorithm is robust,\ngeneralizes well to different applications and input modalities, and\nconsistently outperforms the state-of-the-art.",
          "link": "http://arxiv.org/abs/2102.07365",
          "publishedOn": "2021-08-03T02:06:34.708Z",
          "wordCount": 736,
          "title": "A Unified Batch Selection Policy for Active Metric Learning. (arXiv:2102.07365v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00774",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Goulart_J/0/1/0/all/0/1\">Jos&#xe9; Henrique de Morais Goulart</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Couillet_R/0/1/0/all/0/1\">Romain Couillet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Comon_P/0/1/0/all/0/1\">Pierre Comon</a>",
          "description": "Tensor models play an increasingly prominent role in many fields, notably in\nmachine learning. In several applications of such models, such as community\ndetection, topic modeling and Gaussian mixture learning, one must estimate a\nlow-rank signal from a noisy tensor. Hence, understanding the fundamental\nlimits and the attainable performance of estimators of that signal inevitably\ncalls for the study of random tensors. Substantial progress has been achieved\non this subject thanks to recent efforts, under the assumption that the tensor\ndimensions grow large. Yet, some of the most significant among these\nresults--in particular, a precise characterization of the abrupt phase\ntransition (in terms of signal-to-noise ratio) that governs the performance of\nthe maximum likelihood (ML) estimator of a symmetric rank-one model with\nGaussian noise--were derived on the basis of statistical physics ideas, which\nare not easily accessible to non-experts.\n\nIn this work, we develop a sharply distinct approach, relying instead on\nstandard but powerful tools brought by years of advances in random matrix\ntheory. The key idea is to study the spectra of random matrices arising from\ncontractions of a given random tensor. We show how this gives access to\nspectral properties of the random tensor itself. In the specific case of a\nsymmetric rank-one model with Gaussian noise, our technique yields a hitherto\nunknown characterization of the local maximum of the ML problem that is global\nabove the phase transition threshold. This characterization is in terms of a\nfixed-point equation satisfied by a formula that had only been previously\nobtained via statistical physics methods. Moreover, our analysis sheds light on\ncertain properties of the landscape of the ML problem in the large-dimensional\nsetting. Our approach is versatile and can be extended to other models, such as\nasymmetric, non-Gaussian and higher-order ones.",
          "link": "http://arxiv.org/abs/2108.00774",
          "publishedOn": "2021-08-03T02:06:34.679Z",
          "wordCount": 737,
          "title": "A Random Matrix Perspective on Random Tensors. (arXiv:2108.00774v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_B/0/1/0/all/0/1\">Bin Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xiaoyu Ge</a>",
          "description": "This paper seeks to tackle the bin packing problem (BPP) through a learning\nperspective. Building on self-attention-based encoding and deep reinforcement\nlearning algorithms, we propose a new end-to-end learning model for this task\nof interest. By decomposing the combinatorial action space, as well as\nutilizing a new training technique denoted as prioritized oversampling, which\nis a general scheme to speed up on-policy learning, we achieve state-of-the-art\nperformance in a range of experimental settings. Moreover, although the\nproposed approach attend2pack targets offline-BPP, we strip our method down to\nthe strict online-BPP setting where it is also able to achieve state-of-the-art\nperformance. With a set of ablation studies as well as comparisons against a\nrange of previous works, we hope to offer as a valid baseline approach to this\nfield of study.",
          "link": "http://arxiv.org/abs/2107.04333",
          "publishedOn": "2021-08-03T02:06:34.443Z",
          "wordCount": 608,
          "title": "Attend2Pack: Bin Packing through Deep Reinforcement Learning with Attention. (arXiv:2107.04333v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_X/0/1/0/all/0/1\">Xuming Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Keyin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1\">Chen Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haiyan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quanying Liu</a>",
          "description": "Machine learning is playing an increasingly important role in medical image\nanalysis, spawning new advances in the clinical application of neuroimaging.\nThere have been some reviews of machine learning and epilepsy before, but they\nmainly focused on electrophysiological signals such as\nelectroencephalography(EEG) or stereo electroencephalography(SEEG), while\nignoring the potential of neuroimaging in epilepsy research. Neuroimaging has\nits important advantages in confirming the range of epileptic region, which\nmeans a lot in presurgical evaluation and assessment after surgery. However,\nEEG is difficult to locate the epilepsy lesion region in the brain. In this\nreview, we emphasize the interaction between neuroimaging and machine learning\nin the context of the epilepsy diagnosis and prognosis. We start with an\noverview of typical neuroimaging modalities used in epilepsy clinics, MRI, DTI,\nfMRI, and PET. Then, we introduce three approaches for applying machine\nlearning methods to neuroimaging data: i) the two-step compositional approach\ncombining feature engineering and machine learning classifiers, ii) the\nend-to-end approach, which is usually toward deep learning, and iii) the hybrid\napproach using the advantages of the two methods. Subsequently, the application\nof machine learning on epilepsy neuroimaging, such as segmentation,\nlocalization and lateralization tasks, as well as tasks directly related to\ndiagnosis and prognosis are introduced in detail. Finally, we discuss the\ncurrent achievements, challenges, and potential future directions in this\nfield, hoping to pave the way for computer-aided diagnosis and prognosis of\nepilepsy.",
          "link": "http://arxiv.org/abs/2102.03336",
          "publishedOn": "2021-08-03T02:06:34.419Z",
          "wordCount": 707,
          "title": "Machine Learning Applications on Neuroimaging for Diagnosis and Prognosis of Epilepsy: A Review. (arXiv:2102.03336v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gottwald_M/0/1/0/all/0/1\">Martin Gottwald</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gronauer_S/0/1/0/all/0/1\">Sven Gronauer</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hao Shen</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Diepold_K/0/1/0/all/0/1\">Klaus Diepold</a> (1) ((1) Technical University of Munich, (2) fortiss)",
          "description": "Recent development of Deep Reinforcement Learning has demonstrated superior\nperformance of neural networks in solving challenging problems with large or\neven continuous state spaces. One specific approach is to deploy neural\nnetworks to approximate value functions by minimising the Mean Squared Bellman\nError function. Despite great successes of Deep Reinforcement Learning,\ndevelopment of reliable and efficient numerical algorithms to minimise the\nBellman Error is still of great scientific interest and practical demand. Such\na challenge is partially due to the underlying optimisation problem being\nhighly non-convex or using incorrect gradient information as done in\nSemi-Gradient algorithms. In this work, we analyse the Mean Squared Bellman\nError from a smooth optimisation perspective combined with a Residual Gradient\nformulation. Our contribution is two-fold.\n\nFirst, we analyse critical points of the error function and provide technical\ninsights on the optimisation procure and design choices for neural networks.\nWhen the existence of global minima is assumed and the objective fulfils\ncertain conditions we can eliminate suboptimal local minima when using\nover-parametrised neural networks. We can construct an efficient Approximate\nNewton's algorithm based on our analysis and confirm theoretical properties of\nthis algorithm such as being locally quadratically convergent to a global\nminimum numerically.\n\nSecond, we demonstrate feasibility and generalisation capabilities of the\nproposed algorithm empirically using continuous control problems and provide a\nnumerical verification of our critical point analysis. We outline the short\ncoming of Semi-Gradients. To benefit from an approximate Newton's algorithm\ncomplete derivatives of the Mean Squared Bellman error must be considered\nduring training.",
          "link": "http://arxiv.org/abs/2106.08774",
          "publishedOn": "2021-08-03T02:06:34.410Z",
          "wordCount": 742,
          "title": "Analysis and Optimisation of Bellman Residual Errors with Neural Function Approximation. (arXiv:2106.08774v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02941",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Theodorou_E/0/1/0/all/0/1\">Evangelos Theodorou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1\">Shengjie Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kang_Y/0/1/0/all/0/1\">Yanfei Kang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Spiliotis_E/0/1/0/all/0/1\">Evangelos Spiliotis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Makridakis_S/0/1/0/all/0/1\">Spyros Makridakis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Assimakopoulos_V/0/1/0/all/0/1\">Vassilios Assimakopoulos</a>",
          "description": "The main objective of the M5 competition, which focused on forecasting the\nhierarchical unit sales of Walmart, was to evaluate the accuracy and\nuncertainty of forecasting methods in the field in order to identify best\npractices and highlight their practical implications. However, whether the\nfindings of the M5 competition can be generalized and exploited by retail firms\nto better support their decisions and operation depends on the extent to which\nthe M5 data is sufficiently similar to unit sales data of retailers that\noperate in different regions, sell different types of products, and consider\ndifferent marketing strategies. To answer this question, we analyze the\ncharacteristics of the M5 time series and compare them with those of two\ngrocery retailers, namely Corporaci\\'on Favorita and a major Greek supermarket\nchain, using feature spaces. Our results suggest that there are only small\ndiscrepancies between the examined data sets, supporting the representativeness\nof the M5 data.",
          "link": "http://arxiv.org/abs/2103.02941",
          "publishedOn": "2021-08-03T02:06:34.396Z",
          "wordCount": 605,
          "title": "Exploring the representativeness of the M5 competition data. (arXiv:2103.02941v2 [stat.AP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Archetti_A/0/1/0/all/0/1\">Alberto Archetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cannici_M/0/1/0/all/0/1\">Marco Cannici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1\">Matteo Matteucci</a>",
          "description": "Recently, the trend of incorporating differentiable algorithms into deep\nlearning architectures arose in machine learning research, as the fusion of\nneural layers and algorithmic layers has been beneficial for handling\ncombinatorial data, such as shortest paths on graphs. Recent works related to\ndata-driven planning aim at learning either cost functions or heuristic\nfunctions, but not both. We propose Neural Weighted A*, a differentiable\nanytime planner able to produce improved representations of planar maps as\ngraph costs and heuristics. Training occurs end-to-end on raw images with\ndirect supervision on planning examples, thanks to a differentiable A* solver\nintegrated into the architecture. More importantly, the user can trade off\nplanning accuracy for efficiency at run-time, using a single, real-valued\nparameter. The solution suboptimality is constrained within a linear bound\nequal to the optimal path cost multiplied by the tradeoff parameter. We\nexperimentally show the validity of our claims by testing Neural Weighted A*\nagainst several baselines, introducing a novel, tile-based navigation dataset.\nWe outperform similar architectures in planning accuracy and efficiency.",
          "link": "http://arxiv.org/abs/2105.01480",
          "publishedOn": "2021-08-03T02:06:34.373Z",
          "wordCount": 631,
          "title": "Neural Weighted A*: Learning Graph Costs and Heuristics with Differentiable Anytime A*. (arXiv:2105.01480v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07963",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gassner_A/0/1/0/all/0/1\">Arthur Gassner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rusu_A/0/1/0/all/0/1\">Alexandru Rusu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burg_A/0/1/0/all/0/1\">Andreas Burg</a>",
          "description": "Many applications require accurate indoor localization. Fingerprint-based\nlocalization methods propose a solution to this problem, but rely on a radio\nmap that is effort-intensive to acquire. We automate the radio map acquisition\nphase using a software-defined radio (SDR) and a wheeled robot. Furthermore, we\nopen-source a radio map acquired with our automated tool for a 3GPP Long-Term\nEvolution (LTE) wireless link. To the best of our knowledge, this is the first\npublicly available radio map containing channel state information (CSI).\nFinally, we describe first localization experiments on this radio map using a\nconvolutional neural network to regress for location coordinates.",
          "link": "http://arxiv.org/abs/2104.07963",
          "publishedOn": "2021-08-03T02:06:34.359Z",
          "wordCount": 558,
          "title": "OpenCSI: An Open-Source Dataset for Indoor Localization Using CSI-Based Fingerprinting. (arXiv:2104.07963v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1\">Guangyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_G/0/1/0/all/0/1\">Guanhong Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1\">Shengwei An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiuling Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shiqing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>",
          "description": "Back-door attack poses a severe threat to deep learning systems. It injects\nhidden malicious behaviors to a model such that any input stamped with a\nspecial pattern can trigger such behaviors. Detecting back-door is hence of\npressing need. Many existing defense techniques use optimization to generate\nthe smallest input pattern that forces the model to misclassify a set of benign\ninputs injected with the pattern to a target label. However, the complexity is\nquadratic to the number of class labels such that they can hardly handle models\nwith many classes. Inspired by Multi-Arm Bandit in Reinforcement Learning, we\npropose a K-Arm optimization method for backdoor detection. By iteratively and\nstochastically selecting the most promising labels for optimization with the\nguidance of an objective function, we substantially reduce the complexity,\nallowing to handle models with many classes. Moreover, by iteratively refining\nthe selection of labels to optimize, it substantially mitigates the uncertainty\nin choosing the right labels, improving detection accuracy. At the time of\nsubmission, the evaluation of our method on over 4000 models in the IARPA\nTrojAI competition from round 1 to the latest round 4 achieves top performance\non the leaderboard. Our technique also supersedes three state-of-the-art\ntechniques in terms of accuracy and the scanning time needed.",
          "link": "http://arxiv.org/abs/2102.05123",
          "publishedOn": "2021-08-03T02:06:34.339Z",
          "wordCount": null,
          "title": "Backdoor Scanning for Deep Neural Networks through K-Arm Optimization. (arXiv:2102.05123v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>",
          "description": "Weak supervision has shown promising results in many natural language\nprocessing tasks, such as Named Entity Recognition (NER). Existing work mainly\nfocuses on learning deep NER models only with weak supervision, i.e., without\nany human annotation, and shows that by merely using weakly labeled data, one\ncan achieve good performance, though still underperforms fully supervised NER\nwith manually/strongly labeled data. In this paper, we consider a more\npractical scenario, where we have both a small amount of strongly labeled data\nand a large amount of weakly labeled data. Unfortunately, we observe that\nweakly labeled data does not necessarily improve, or even deteriorate the model\nperformance (due to the extensive noise in the weak labels) when we train deep\nNER models over a simple or weighted combination of the strongly labeled and\nweakly labeled data. To address this issue, we propose a new multi-stage\ncomputational framework -- NEEDLE with three essential ingredients: (1) weak\nlabel completion, (2) noise-aware loss function, and (3) final fine-tuning over\nthe strongly labeled data. Through experiments on E-commerce query NER and\nBiomedical NER, we demonstrate that NEEDLE can effectively suppress the noise\nof the weak labels and outperforms existing methods. In particular, we achieve\nnew SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74,\nBC5CDR-disease 90.69, NCBI-disease 92.28.",
          "link": "http://arxiv.org/abs/2106.08977",
          "publishedOn": "2021-08-03T02:06:34.336Z",
          "wordCount": 705,
          "title": "Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data. (arXiv:2106.08977v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05082",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Katsnelson_M/0/1/0/all/0/1\">Mikhail I. Katsnelson</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Vanchurin_V/0/1/0/all/0/1\">Vitaly Vanchurin</a>",
          "description": "It was recently shown that the Madelung equations, that is, a hydrodynamic\nform of the Schr\\\"odinger equation, can be derived from a canonical ensemble of\nneural networks where the quantum phase was identified with the free energy of\nhidden variables. We consider instead a grand canonical ensemble of neural\nnetworks, by allowing an exchange of neurons with an auxiliary subsystem, to\nshow that the free energy must also be multivalued. By imposing the\nmultivaluedness condition on the free energy we derive the Schr\\\"odinger\nequation with \"Planck's constant\" determined by the chemical potential of\nhidden variables. This shows that quantum mechanics provides a correct\nstatistical description of the dynamics of the grand canonical ensemble of\nneural networks at the learning equilibrium. We also discuss implications of\nthe results for machine learning, fundamental physics and, in a more\nspeculative way, evolutionary biology.",
          "link": "http://arxiv.org/abs/2012.05082",
          "publishedOn": "2021-08-03T02:06:34.320Z",
          "wordCount": 603,
          "title": "Emergent Quantumness in Neural Networks. (arXiv:2012.05082v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zihan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "Binary neural networks (BNNs) have received increasing attention due to their\nsuperior reductions of computation and memory. Most existing works focus on\neither lessening the quantization error by minimizing the gap between the\nfull-precision weights and their binarization or designing a gradient\napproximation to mitigate the gradient mismatch, while leaving the \"dead\nweights\" untouched. This leads to slow convergence when training BNNs. In this\npaper, for the first time, we explore the influence of \"dead weights\" which\nrefer to a group of weights that are barely updated during the training of\nBNNs, and then introduce rectified clamp unit (ReCU) to revive the \"dead\nweights\" for updating. We prove that reviving the \"dead weights\" by ReCU can\nresult in a smaller quantization error. Besides, we also take into account the\ninformation entropy of the weights, and then mathematically analyze why the\nweight standardization can benefit BNNs. We demonstrate the inherent\ncontradiction between minimizing the quantization error and maximizing the\ninformation entropy, and then propose an adaptive exponential scheduler to\nidentify the range of the \"dead weights\". By considering the \"dead weights\",\nour method offers not only faster BNN training, but also state-of-the-art\nperformance on CIFAR-10 and ImageNet, compared with recent methods. Code can be\navailable at https://github.com/z-hXu/ReCU.",
          "link": "http://arxiv.org/abs/2103.12369",
          "publishedOn": "2021-08-03T02:06:34.310Z",
          "wordCount": 696,
          "title": "ReCU: Reviving the Dead Weights in Binary Neural Networks. (arXiv:2103.12369v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1\">Navid Kardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>",
          "description": "Deep Learning (DL) is the most widely used tool in the contemporary field of\ncomputer vision. Its ability to accurately solve complex problems is employed\nin vision research to learn deep neural models for a variety of tasks,\nincluding security critical applications. However, it is now known that DL is\nvulnerable to adversarial attacks that can manipulate its predictions by\nintroducing visually imperceptible perturbations in images and videos. Since\nthe discovery of this phenomenon in 2013~[1], it has attracted significant\nattention of researchers from multiple sub-fields of machine intelligence. In\n[2], we reviewed the contributions made by the computer vision community in\nadversarial attacks on deep learning (and their defenses) until the advent of\nyear 2018. Many of those contributions have inspired new directions in this\narea, which has matured significantly since witnessing the first generation\nmethods. Hence, as a legacy sequel of [2], this literature review focuses on\nthe advances in this area since 2018. To ensure authenticity, we mainly\nconsider peer-reviewed contributions published in the prestigious sources of\ncomputer vision and machine learning research. Besides a comprehensive\nliterature review, the article also provides concise definitions of technical\nterminologies for non-experts in this domain. Finally, this article discusses\nchallenges and future outlook of this direction based on the literature\nreviewed herein and [2].",
          "link": "http://arxiv.org/abs/2108.00401",
          "publishedOn": "2021-08-03T02:06:34.290Z",
          "wordCount": 673,
          "title": "Threat of Adversarial Attacks on Deep Learning in Computer Vision: Survey II. (arXiv:2108.00401v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.06308",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Deng_X/0/1/0/all/0/1\">Xiangwen Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1\">Junlin Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shangming Yang</a>",
          "description": "Emotion recognition based on EEG (electroencephalography) has been widely\nused in human-computer interaction, distance education and health care.\nHowever, the conventional methods ignore the adjacent and symmetrical\ncharacteristics of EEG signals, which also contain salient information related\nto emotion. In this paper, a spatial folding ensemble network (SFE-Net) is\npresented for EEG feature extraction and emotion recognition. Firstly, for the\nundetected area between EEG electrodes, an improved Bicubic-EEG interpolation\nalgorithm is developed for EEG channels information completion, which allows us\nto extract a wider range of adjacent space features. Then, motivated by the\nspatial symmetric mechanism of human brain, we fold the input EEG channels data\nwith five different symmetrical strategies, which enable the proposed network\nto extract the information of space features of EEG signals more effectively.\nFinally, a 3DCNN-based spatial, temporal extraction, and a multi-voting\nstrategy of ensemble learning are integrated to model a new neural network.\nWith this network, the spatial features of different symmetric folding signals\ncan be extracted simultaneously, which greatly improves the robustness and\naccuracy of emotion recognition. The experimental results on DEAP and SEED\ndatasets show that the proposed algorithm has comparable performance in terms\nof recognition accuracy.",
          "link": "http://arxiv.org/abs/2104.06308",
          "publishedOn": "2021-08-03T02:06:34.284Z",
          "wordCount": 673,
          "title": "SFE-Net: EEG-based Emotion Recognition with Symmetrical Spatial Feature Extraction. (arXiv:2104.06308v4 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1\">Qili Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shihang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wohlberg_B/0/1/0/all/0/1\">Brendt Wohlberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Youzuo Lin</a>",
          "description": "Seismic full-waveform inversion (FWI) techniques aim to find a\nhigh-resolution subsurface geophysical model provided with waveform data. Some\nrecent effort in data-driven FWI has shown some encouraging results in\nobtaining 2D velocity maps. However, due to high computational complexity and\nlarge memory consumption, the reconstruction of 3D high-resolution velocity\nmaps via deep networks is still a great challenge. In this paper, we present\nInversionNet3D, an efficient and scalable encoder-decoder network for 3D FWI.\nThe proposed method employs group convolution in the encoder to establish an\neffective hierarchy for learning information from multiple sources while\ncutting down unnecessary parameters and operations at the same time. The\nintroduction of invertible layers further reduces the memory consumption of\nintermediate features during training and thus enables the development of\ndeeper networks with more layers and higher capacity as required by different\napplication scenarios. Experiments on the 3D Kimberlina dataset demonstrate\nthat InversionNet3D achieves state-of-the-art reconstruction performance with\nlower computational cost and lower memory footprint compared to the baseline.",
          "link": "http://arxiv.org/abs/2103.14158",
          "publishedOn": "2021-08-03T02:06:34.278Z",
          "wordCount": 633,
          "title": "InversionNet3D: Efficient and Scalable Learning for 3D Full Waveform Inversion. (arXiv:2103.14158v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1\">Daniel Berleant</a>",
          "description": "The accuracy of DL classifiers is unstable in that it often changes\nsignificantly when retested on adversarial images, imperfect images, or\nperturbed images. This paper adds to the small but fundamental body of work on\nbenchmarking the robustness of DL classifiers on defective images. Unlike\nexisted single-factor digital perturbation work, we provide state-of-the-art\ntwo-factor perturbation that provides two natural perturbations on images\napplied in different sequences. The two-factor perturbation includes (1) two\ndigital perturbations (Salt & pepper noise and Gaussian noise) applied in both\nsequences. (2) one digital perturbation (salt & pepper noise) and a geometric\nperturbation (rotation) applied in different sequences. To measure robust DL\nclassifiers, previous scientists provided 15 types of single-factor corruption.\nWe created 69 benchmarking image sets, including a clean set, sets with single\nfactor perturbations, and sets with two-factor perturbation conditions. To be\nbest of our knowledge, this is the first report that two-factor perturbed\nimages improves both robustness and accuracy of DL classifiers. Previous\nresearch evaluating deep learning (DL) classifiers has often used top-1/top-5\naccuracy, so researchers have usually offered tables, line diagrams, and bar\ncharts to display accuracy of DL classifiers. But these existed approaches\ncannot quantitively evaluate robustness of DL classifiers. We innovate a new\ntwo-dimensional, statistical visualization tool, including mean accuracy and\ncoefficient of variation (CV), to benchmark the robustness of DL classifiers.\nAll source codes and related image sets are shared on websites\n(this http URL or\nhttps://github.com/daiweiworking/RobustDeepLearningUsingPerturbations ) to\nsupport future academic research and industry projects.",
          "link": "http://arxiv.org/abs/2103.03102",
          "publishedOn": "2021-08-03T02:06:34.272Z",
          "wordCount": 722,
          "title": "Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16336",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Goren_E/0/1/0/all/0/1\">Emily M. Goren</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1\">Ranjan Maitra</a>",
          "description": "Partially recorded data are frequently encountered in many applications and\nusually clustered by first removing incomplete cases or features with missing\nvalues, or by imputing missing values, followed by application of a clustering\nalgorithm to the resulting altered dataset. Here, we develop clustering\nmethodology through a model-based approach using the marginal density for the\nobserved values, assuming a finite mixture model of multivariate $t$\ndistributions. We compare our approximate algorithm to the corresponding full\nexpectation-maximization (EM) approach that considers the missing values in the\nincomplete data set and makes a missing at random (MAR) assumption, as well as\ncase deletion and imputation methods. Since only the observed values are\nutilized, our approach is computationally more efficient than imputation or\nfull EM. Simulation studies demonstrate that our approach has favorable\nrecovery of the true cluster partition compared to case deletion and imputation\nunder various missingness mechanisms, and is at least competitive with the full\nEM approach, even when MAR assumptions are violated. Our methodology is\ndemonstrated on a problem of clustering gamma-ray bursts and is implemented at\nhttps://github.com/emilygoren/MixtClust.",
          "link": "http://arxiv.org/abs/2103.16336",
          "publishedOn": "2021-08-03T02:06:34.265Z",
          "wordCount": 671,
          "title": "Fast model-based clustering of partial records. (arXiv:2103.16336v4 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1\">Siyuan Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Ailing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>",
          "description": "Heatmap-based methods dominate in the field of human pose estimation by\nmodelling the output distribution through likelihood heatmaps. In contrast,\nregression-based methods are more efficient but suffer from inferior\nperformance. In this work, we explore maximum likelihood estimation (MLE) to\ndevelop an efficient and effective regression-based methods. From the\nperspective of MLE, adopting different regression losses is making different\nassumptions about the output density function. A density function closer to the\ntrue distribution leads to a better regression performance. In light of this,\nwe propose a novel regression paradigm with Residual Log-likelihood Estimation\n(RLE) to capture the underlying output distribution. Concretely, RLE learns the\nchange of the distribution instead of the unreferenced underlying distribution\nto facilitate the training process. With the proposed reparameterization\ndesign, our method is compatible with off-the-shelf flow models. The proposed\nmethod is effective, efficient and flexible. We show its potential in various\nhuman pose estimation tasks with comprehensive experiments. Compared to the\nconventional regression paradigm, regression with RLE bring 12.4 mAP\nimprovement on MSCOCO without any test-time overhead. Moreover, for the first\ntime, especially on multi-person pose estimation, our regression method is\nsuperior to the heatmap-based methods. Our code is available at\nhttps://github.com/Jeff-sjtu/res-loglikelihood-regression",
          "link": "http://arxiv.org/abs/2107.11291",
          "publishedOn": "2021-08-03T02:06:34.247Z",
          "wordCount": 687,
          "title": "Human Pose Regression with Residual Log-likelihood Estimation. (arXiv:2107.11291v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linnan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_R/0/1/0/all/0/1\">Rodrigo Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tian Guo</a>",
          "description": "Efficient evaluation of a network architecture drawn from a large search\nspace remains a key challenge in Neural Architecture Search (NAS). Vanilla NAS\nevaluates each architecture by training from scratch, which gives the true\nperformance but is extremely time-consuming. Recently, one-shot NAS\nsubstantially reduces the computation cost by training only one supernetwork,\na.k.a. supernet, to approximate the performance of every architecture in the\nsearch space via weight-sharing. However, the performance estimation can be\nvery inaccurate due to the co-adaption among operations. In this paper, we\npropose few-shot NAS that uses multiple supernetworks, called sub-supernet,\neach covering different regions of the search space to alleviate the undesired\nco-adaption. Compared to one-shot NAS, few-shot NAS improves the accuracy of\narchitecture evaluation with a small increase of evaluation cost. With only up\nto 7 sub-supernets, few-shot NAS establishes new SoTAs: on ImageNet, it finds\nmodels that reach 80.5% top-1 accuracy at 600 MB FLOPS and 77.5% top-1 accuracy\nat 238 MFLOPS; on CIFAR10, it reaches 98.72% top-1 accuracy without using extra\ndata or transfer learning. In Auto-GAN, few-shot NAS outperforms the previously\npublished results by up to 20%. Extensive experiments show that few-shot NAS\nsignificantly improves various one-shot methods, including 4 gradient-based and\n6 search-based methods on 3 different tasks in NasBench-201 and\nNasBench1-shot-1.",
          "link": "http://arxiv.org/abs/2006.06863",
          "publishedOn": "2021-08-03T02:06:34.241Z",
          "wordCount": 734,
          "title": "Few-shot Neural Architecture Search. (arXiv:2006.06863v9 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Sibo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Handong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfu Liu</a>",
          "description": "Outlier detection is one of the most popular and continuously rising topics\nin the data mining field due to its crucial academic value and extensive\nindustrial applications. Among different settings, unsupervised outlier\ndetection is the most challenging and practical one, which attracts tremendous\nefforts from diverse perspectives. In this paper, we consider the score-based\noutlier detection category and point out that the performance of current\noutlier detection algorithms might be further boosted by score propagation.\nSpecifically, we propose Infinite Propagation of Outlier Factor (iPOF)\nalgorithm, an extremely and excitingly simple outlier detection booster via\ninfinite propagation. By employing score-based outlier detectors for\ninitialization, iPOF updates each data point's outlier score by averaging the\noutlier factors of its nearest common neighbors. Extensive experimental results\non numerous datasets in various domains demonstrate the effectiveness and\nefficiency of iPOF significantly over several classical and recent\nstate-of-the-art methods. We also provide the parameter analysis on the number\nof neighbors, the unique parameter in iPOF, and different initial outlier\ndetectors for general validation. It is worthy to note that iPOF brings in\npositive improvements ranging from 2% to 46% on the average level, and in some\ncases, iPOF boosts the performance over 3000% over the original outlier\ndetection algorithm.",
          "link": "http://arxiv.org/abs/2108.00360",
          "publishedOn": "2021-08-03T02:06:34.234Z",
          "wordCount": 639,
          "title": "IPOF: An Extremely and Excitingly Simple Outlier Detection Booster via Infinite Propagation. (arXiv:2108.00360v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.01959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bianchini_E/0/1/0/all/0/1\">Elizabeth Bibit Bianchini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Prateek Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salisbury_K/0/1/0/all/0/1\">Kenneth Salisbury</a>",
          "description": "Physical human-robot interactions (pHRI) are less efficient and communicative\nthan human-human interactions, and a key reason is a lack of informative sense\nof touch in robotic systems. Interpreting human touch gestures is a nuanced,\nchallenging task with extreme gaps between human and robot capability. Among\nprior works that demonstrate human touch recognition capability, differences in\nsensors, gesture classes, feature sets, and classification algorithms yield a\nconglomerate of non-transferable results and a glaring lack of a standard. To\naddress this gap, this work presents 1) four proposed touch gesture classes\nthat cover an important subset of the gesture characteristics identified in the\nliterature, 2) the collection of an extensive force dataset on a common pHRI\nrobotic arm with only its internal wrist force-torque sensor, and 3) an\nexhaustive performance comparison of combinations of feature sets and\nclassification algorithms on this dataset. We demonstrate high classification\naccuracies among our proposed gesture definitions on a test set, emphasizing\nthat neural net-work classifiers on the raw data outperform other combinations\nof feature sets and algorithms. The accompanying video is here:\nhttps://youtu.be/gJPVImNKU68",
          "link": "http://arxiv.org/abs/2012.01959",
          "publishedOn": "2021-08-03T02:06:34.206Z",
          "wordCount": 680,
          "title": "Towards Human Haptic Gesture Interpretation for Robotic Systems. (arXiv:2012.01959v4 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jyun-Li Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_W/0/1/0/all/0/1\">Wei Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shang-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_P/0/1/0/all/0/1\">Ping-Chun Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xi Liu</a>",
          "description": "Action-constrained reinforcement learning (RL) is a widely-used approach in\nvarious real-world applications, such as scheduling in networked systems with\nresource constraints and control of a robot with kinematic constraints. While\nthe existing projection-based approaches ensure zero constraint violation, they\ncould suffer from the zero-gradient problem due to the tight coupling of the\npolicy gradient and the projection, which results in sample-inefficient\ntraining and slow convergence. To tackle this issue, we propose a learning\nalgorithm that decouples the action constraints from the policy parameter\nupdate by leveraging state-wise Frank-Wolfe and a regression-based policy\nupdate scheme. Moreover, we show that the proposed algorithm enjoys convergence\nand policy improvement properties in the tabular case as well as generalizes\nthe popular DDPG algorithm for action-constrained RL in the general case.\nThrough experiments, we demonstrate that the proposed algorithm significantly\noutperforms the benchmark methods on a variety of control tasks.",
          "link": "http://arxiv.org/abs/2102.11055",
          "publishedOn": "2021-08-03T02:06:34.200Z",
          "wordCount": 618,
          "title": "Escaping from Zero Gradient: Revisiting Action-Constrained Reinforcement Learning via Frank-Wolfe Policy Optimization. (arXiv:2102.11055v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bock_M/0/1/0/all/0/1\">Marius Bock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoelzemann_A/0/1/0/all/0/1\">Alexander Hoelzemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1\">Michael Moeller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laerhoven_K/0/1/0/all/0/1\">Kristof Van Laerhoven</a>",
          "description": "Recent studies in Human Activity Recognition (HAR) have shown that Deep\nLearning methods are able to outperform classical Machine Learning algorithms.\nOne popular Deep Learning architecture in HAR is the DeepConvLSTM. In this\npaper we propose to alter the DeepConvLSTM architecture to employ a 1-layered\ninstead of a 2-layered LSTM. We validate our architecture change on 5 publicly\navailable HAR datasets by comparing the predictive performance with and without\nthe change employing varying hidden units within the LSTM layer(s). Results\nshow that across all datasets, our architecture consistently improves on the\noriginal one: Recognition performance increases up to 11.7% for the F1-score,\nand our architecture significantly decreases the amount of learnable\nparameters. This improvement over DeepConvLSTM decreases training time by as\nmuch as 48%. Our results stand in contrast to the belief that one needs at\nleast a 2-layered LSTM when dealing with sequential data. Based on our results\nwe argue that said claim might not be applicable to sensor-based HAR.",
          "link": "http://arxiv.org/abs/2108.00702",
          "publishedOn": "2021-08-03T02:06:34.169Z",
          "wordCount": 614,
          "title": "Improving Deep Learning for HAR with shallow LSTMs. (arXiv:2108.00702v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2101.08398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1\">Mustafa Hajij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batayneh_F/0/1/0/all/0/1\">Fawwaz Batayneh</a>",
          "description": "Topological Data Analysis (TDA) has emerged recently as a robust tool to\nextract and compare the structure of datasets. TDA identifies features in data\nsuch as connected components and holes and assigns a quantitative measure to\nthese features. Several studies reported that topological features extracted by\nTDA tools provide unique information about the data, discover new insights, and\ndetermine which feature is more related to the outcome. On the other hand, the\noverwhelming success of deep neural networks in learning patterns and\nrelationships has been proven on a vast array of data applications, images in\nparticular. To capture the characteristics of both powerful tools, we propose\n\\textit{TDA-Net}, a novel ensemble network that fuses topological and deep\nfeatures for the purpose of enhancing model generalizability and accuracy. We\napply the proposed \\textit{TDA-Net} to a critical application, which is the\nautomated detection of COVID-19 from CXR images. The experimental results\nshowed that the proposed network achieved excellent performance and suggests\nthe applicability of our method in practice.",
          "link": "http://arxiv.org/abs/2101.08398",
          "publishedOn": "2021-08-03T02:06:34.163Z",
          "wordCount": 718,
          "title": "TDA-Net: Fusion of Persistent Homology and Deep Learning Features for COVID-19 Detection in Chest X-Ray Images. (arXiv:2101.08398v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manneschi_L/0/1/0/all/0/1\">Luca Manneschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_M/0/1/0/all/0/1\">Matthew O. A. Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gigante_G/0/1/0/all/0/1\">Guido Gigante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1\">Andrew C. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giudice_P/0/1/0/all/0/1\">Paolo Del Giudice</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilaki_E/0/1/0/all/0/1\">Eleni Vasilaki</a>",
          "description": "Echo state networks (ESNs) are a powerful form of reservoir computing that\nonly require training of linear output weights whilst the internal reservoir is\nformed of fixed randomly connected neurons. With a correctly scaled\nconnectivity matrix, the neurons' activity exhibits the echo-state property and\nresponds to the input dynamics with certain timescales. Tuning the timescales\nof the network can be necessary for treating certain tasks, and some\nenvironments require multiple timescales for an efficient representation. Here\nwe explore the timescales in hierarchical ESNs, where the reservoir is\npartitioned into two smaller linked reservoirs with distinct properties. Over\nthree different tasks (NARMA10, a reconstruction task in a volatile\nenvironment, and psMNIST), we show that by selecting the hyper-parameters of\neach partition such that they focus on different timescales, we achieve a\nsignificant performance improvement over a single ESN. Through a linear\nanalysis, and under the assumption that the timescales of the first partition\nare much shorter than the second's (typically corresponding to optimal\noperating conditions), we interpret the feedforward coupling of the partitions\nin terms of an effective representation of the input signal, provided by the\nfirst partition to the second, whereby the instantaneous input signal is\nexpanded into a weighted combination of its time derivatives. Furthermore, we\npropose a data-driven approach to optimise the hyper-parameters through a\ngradient descent optimisation method that is an online approximation of\nbackpropagation through time. We demonstrate the application of the online\nlearning rule across all the tasks considered.",
          "link": "http://arxiv.org/abs/2101.04223",
          "publishedOn": "2021-08-03T02:06:34.152Z",
          "wordCount": 733,
          "title": "Exploiting Multiple Timescales in Hierarchical Echo State Networks. (arXiv:2101.04223v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ye Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brinker_F/0/1/0/all/0/1\">Frank Brinker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decking_W/0/1/0/all/0/1\">Winfried Decking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomin_S/0/1/0/all/0/1\">Sergey Tomin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlarb_H/0/1/0/all/0/1\">Holger Schlarb</a>",
          "description": "Modeling of large-scale research facilities is extremely challenging due to\ncomplex physical processes and engineering problems. Here, we adopt a\ndata-driven approach to model the longitudinal phase-space diagnostic beamline\nat the photoinector of the European XFEL with an encoder-decoder neural network\nmodel. A deep convolutional neural network (decoder) is used to build images\nmeasured on the screen from a small feature map generated by another neural\nnetwork (encoder). We demonstrate that the model trained only with experimental\ndata can make high-fidelity predictions of megapixel images for the\nlongitudinal phase-space measurement without any prior knowledge of\nphotoinjectors and electron beams. The prediction significantly outperforms\nexisting methods. We also show the scalability and interpretability of the\nmodel by sharing the same decoder with more than one encoder used for different\nsetups of the photoinjector, and propose a pragmatic way to model a facility\nwith various diagnostics and working points. This opens the door to a new way\nof accurately modeling a photoinjector using neural networks and experimental\ndata. The approach can possibly be extended to the whole accelerator and even\nother types of scientific facilities.",
          "link": "http://arxiv.org/abs/2101.10437",
          "publishedOn": "2021-08-03T02:06:34.138Z",
          "wordCount": 680,
          "title": "High-fidelity Prediction of Megapixel Longitudinal Phase-space Images of Electron Beams using Encoder-Decoder Neural Networks. (arXiv:2101.10437v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiawei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jinyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bin Cui</a>",
          "description": "Hyperparameter optimization (HPO) is a fundamental problem in automatic\nmachine learning (AutoML). However, due to the expensive evaluation cost of\nmodels (e.g., training deep learning models or training models on large\ndatasets), vanilla Bayesian optimization (BO) is typically computationally\ninfeasible. To alleviate this issue, Hyperband (HB) utilizes the early stopping\nmechanism to speed up configuration evaluations by terminating those\nbadly-performing configurations in advance. This leads to two kinds of quality\nmeasurements: (1) many low-fidelity measurements for configurations that get\nearly-stopped, and (2) few high-fidelity measurements for configurations that\nare evaluated without being early stopped. The state-of-the-art HB-style\nmethod, BOHB, aims to combine the benefits of both BO and HB. Instead of\nsampling configurations randomly in HB, BOHB samples configurations based on a\nBO surrogate model, which is constructed with the high-fidelity measurements\nonly. However, the scarcity of high-fidelity measurements greatly hampers the\nefficiency of BO to guide the configuration search. In this paper, we present\nMFES-HB, an efficient Hyperband method that is capable of utilizing both the\nhigh-fidelity and low-fidelity measurements to accelerate the convergence of\nHPO tasks. Designing MFES-HB is not trivial as the low-fidelity measurements\ncan be biased yet informative to guide the configuration search. Thus we\npropose to build a Multi- Fidelity Ensemble Surrogate (MFES) based on the\ngeneralized Product of Experts framework, which can integrate useful\ninformation from multi-fidelity measurements effectively. The empirical studies\non the real-world AutoML tasks demonstrate that MFES-HB can achieve 3.3-8.9x\nspeedups over the state-of-the-art approach - BOHB.",
          "link": "http://arxiv.org/abs/2012.03011",
          "publishedOn": "2021-08-03T02:06:34.125Z",
          "wordCount": 718,
          "title": "MFES-HB: Efficient Hyperband with Multi-Fidelity Quality Measurements. (arXiv:2012.03011v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.10420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1\">Gustavo Henrique de Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcao_A/0/1/0/all/0/1\">Alexandre Xavier Falc&#xe3;o</a>",
          "description": "Machine learning techniques have been paramount throughout the last years,\nbeing applied in a wide range of tasks, such as classification, object\nrecognition, person identification, and image segmentation. Nevertheless,\nconventional classification algorithms, e.g., Logistic Regression, Decision\nTrees, and Bayesian classifiers, might lack complexity and diversity, not\nsuitable when dealing with real-world data. A recent graph-inspired classifier,\nknown as the Optimum-Path Forest, has proven to be a state-of-the-art\ntechnique, comparable to Support Vector Machines and even surpassing it in some\ntasks. This paper proposes a Python-based Optimum-Path Forest framework,\ndenoted as OPFython, where all of its functions and classes are based upon the\noriginal C language implementation. Additionally, as OPFython is a Python-based\nlibrary, it provides a more friendly environment and a faster prototyping\nworkspace than the C language.",
          "link": "http://arxiv.org/abs/2001.10420",
          "publishedOn": "2021-08-03T02:06:34.103Z",
          "wordCount": 628,
          "title": "OPFython: A Python-Inspired Optimum-Path Forest Classifier. (arXiv:2001.10420v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10970",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Chia Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1\">Marina Meil&#x103;</a>",
          "description": "The null space of the $k$-th order Laplacian $\\mathbf{\\mathcal L}_k$, known\nas the {\\em $k$-th homology vector space}, encodes the non-trivial topology of\na manifold or a network. Understanding the structure of the homology embedding\ncan thus disclose geometric or topological information from the data. The study\nof the null space embedding of the graph Laplacian $\\mathbf{\\mathcal L}_0$ has\nspurred new research and applications, such as spectral clustering algorithms\nwith theoretical guarantees and estimators of the Stochastic Block Model. In\nthis work, we investigate the geometry of the $k$-th homology embedding and\nfocus on cases reminiscent of spectral clustering. Namely, we analyze the {\\em\nconnected sum} of manifolds as a perturbation to the direct sum of their\nhomology embeddings. We propose an algorithm to factorize the homology\nembedding into subspaces corresponding to a manifold's simplest topological\ncomponents. The proposed framework is applied to the {\\em shortest homologous\nloop detection} problem, a problem known to be NP-hard in general. Our spectral\nloop detection algorithm scales better than existing methods and is effective\non diverse data such as point clouds and images.",
          "link": "http://arxiv.org/abs/2107.10970",
          "publishedOn": "2021-08-03T02:06:34.089Z",
          "wordCount": 642,
          "title": "The decomposition of the higher-order homology embedding constructed from the $k$-Laplacian. (arXiv:2107.10970v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahapatra_D/0/1/0/all/0/1\">Debabrata Mahapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_V/0/1/0/all/0/1\">Vaibhav Rajan</a>",
          "description": "Multi-Task Learning (MTL) is a well-established paradigm for training deep\nneural network models for multiple correlated tasks. Often the task objectives\nconflict, requiring trade-offs between them during model building. In such\ncases, MTL models can use gradient-based multi-objective optimization (MOO) to\nfind one or more Pareto optimal solutions. A common requirement in MTL\napplications is to find an {\\it Exact} Pareto optimal (EPO) solution, which\nsatisfies user preferences with respect to task-specific objective functions.\nFurther, to improve model generalization, various constraints on the weights\nmay need to be enforced during training. Addressing these requirements is\nchallenging because it requires a search direction that allows descent not only\ntowards the Pareto front but also towards the input preference, within the\nconstraints imposed and in a manner that scales to high-dimensional gradients.\nWe design and theoretically analyze such search directions and develop the\nfirst scalable algorithm, with theoretical guarantees of convergence, to find\nan EPO solution, including when box and equality constraints are imposed. Our\nunique method combines multiple gradient descent with carefully controlled\nascent to traverse the Pareto front in a principled manner, making it robust to\ninitialization. This also facilitates systematic exploration of the Pareto\nfront, that we utilize to approximate the Pareto front for multi-criteria\ndecision-making. Empirical results show that our algorithm outperforms\ncompeting methods on benchmark MTL datasets and MOO problems.",
          "link": "http://arxiv.org/abs/2108.00597",
          "publishedOn": "2021-08-03T02:06:34.081Z",
          "wordCount": 660,
          "title": "Exact Pareto Optimal Search for Multi-Task Learning: Touring the Pareto Front. (arXiv:2108.00597v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Anindita Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheema_N/0/1/0/all/0/1\">Noshaba Cheema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_C/0/1/0/all/0/1\">Cennet Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slusallek_P/0/1/0/all/0/1\">Philipp Slusallek</a>",
          "description": "\"How can we animate 3D-characters from a movie script or move robots by\nsimply telling them what we would like them to do?\" \"How unstructured and\ncomplex can we make a sentence and still generate plausible movements from it?\"\nThese are questions that need to be answered in the long-run, as the field is\nstill in its infancy. Inspired by these problems, we present a new technique\nfor generating compositional actions, which handles complex input sentences.\nOur output is a 3D pose sequence depicting the actions in the input sentence.\nWe propose a hierarchical two-stream sequential model to explore a finer\njoint-level mapping between natural language sentences and 3D pose sequences\ncorresponding to the given motion. We learn two manifold representations of the\nmotion -- one each for the upper body and the lower body movements. Our model\ncan generate plausible pose sequences for short sentences describing single\nactions as well as long compositional sentences describing multiple sequential\nand superimposed actions. We evaluate our proposed model on the publicly\navailable KIT Motion-Language Dataset containing 3D pose data with\nhuman-annotated sentences. Experimental results show that our model advances\nthe state-of-the-art on text-based motion synthesis in objective evaluations by\na margin of 50%. Qualitative evaluations based on a user study indicate that\nour synthesized motions are perceived to be the closest to the ground-truth\nmotion captures for both short and compositional sentences.",
          "link": "http://arxiv.org/abs/2103.14675",
          "publishedOn": "2021-08-03T02:06:34.075Z",
          "wordCount": 723,
          "title": "Synthesis of Compositional Animations from Textual Descriptions. (arXiv:2103.14675v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.13203",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruishan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balsubramani_A/0/1/0/all/0/1\">Akshay Balsubramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>",
          "description": "Learning to align multiple datasets is an important problem with many\napplications, and it is especially useful when we need to integrate multiple\nexperiments or correct for confounding. Optimal transport (OT) is a principled\napproach to align datasets, but a key challenge in applying OT is that we need\nto specify a transport cost function that accurately captures how the two\ndatasets are related. Reliable cost functions are typically not available and\npractitioners often resort to using hand-crafted or Euclidean cost even if it\nmay not be appropriate. In this work, we investigate how to learn the cost\nfunction using a small amount of side information which is often available. The\nside information we consider captures subset correspondence -- i.e. certain\nsubsets of points in the two data sets are known to be related. For example, we\nmay have some images labeled as cars in both datasets; or we may have a common\nannotated cell type in single-cell data from two batches. We develop an\nend-to-end optimizer (OT-SI) that differentiates through the Sinkhorn algorithm\nand effectively learns the suitable cost function from side information. On\nsystematic experiments in images, marriage-matching and single-cell RNA-seq,\nour method substantially outperform state-of-the-art benchmarks.",
          "link": "http://arxiv.org/abs/1909.13203",
          "publishedOn": "2021-08-03T02:06:34.064Z",
          "wordCount": 656,
          "title": "Learning transport cost from subset correspondence. (arXiv:1909.13203v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09943",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Nikitin_O/0/1/0/all/0/1\">Oleg Nikitin</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lukyanova_O/0/1/0/all/0/1\">Olga Lukyanova</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kunin_A/0/1/0/all/0/1\">Alex Kunin</a>",
          "description": "Parallels between the signal processing tasks and biological neurons lead to\nan understanding of the principles of self-organized optimization of input\nsignal recognition. In the present paper, we discuss such similarities among\nbiological and technical systems. We propose adding the well-known STDP\nsynaptic plasticity rule to direct the weight modification towards the state\nassociated with the maximal difference between background noise and correlated\nsignals. We use the principle of physically constrained weight growth as a\nbasis for such weights' modification control. It is proposed that the existence\nand production of bio-chemical 'substances' needed for plasticity development\nrestrict a biological synaptic straight modification. In this paper, the\ninformation about the noise-to-signal ratio controls such a substances'\nproduction and storage and drives the neuron's synaptic pressures towards the\nstate with the best signal-to-noise ratio. We consider several experiments with\ndifferent input signal regimes to understand the functioning of the proposed\napproach.",
          "link": "http://arxiv.org/abs/2104.09943",
          "publishedOn": "2021-08-03T02:06:34.039Z",
          "wordCount": 639,
          "title": "The principle of weight divergence facilitation for unsupervised pattern recognition in spiking neural networks. (arXiv:2104.09943v2 [q-bio.NC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05842",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dwivedi_R/0/1/0/all/0/1\">Raaz Dwivedi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>",
          "description": "We introduce kernel thinning, a new procedure for compressing a distribution\n$\\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given\na suitable reproducing kernel $\\mathbf{k}$ and $\\mathcal{O}(n^2)$ time, kernel\nthinning compresses an $n$-point approximation to $\\mathbb{P}$ into a\n$\\sqrt{n}$-point approximation with comparable worst-case integration error\nacross the associated reproducing kernel Hilbert space. With high probability,\nthe maximum discrepancy in integration error is\n$\\mathcal{O}_d(n^{-\\frac{1}{2}}\\sqrt{\\log n})$ for compactly supported\n$\\mathbb{P}$ and $\\mathcal{O}_d(n^{-\\frac{1}{2}} \\sqrt{(\\log n)^{d+1}\\log\\log\nn})$ for sub-exponential $\\mathbb{P}$ on $\\mathbb{R}^d$. In contrast, an\nequal-sized i.i.d. sample from $\\mathbb{P}$ suffers $\\Omega(n^{-\\frac14})$\nintegration error. Our sub-exponential guarantees resemble the classical\nquasi-Monte Carlo error rates for uniform $\\mathbb{P}$ on $[0,1]^d$ but apply\nto general distributions on $\\mathbb{R}^d$ and a wide range of common kernels.\nWe use our results to derive explicit non-asymptotic maximum mean discrepancy\nbounds for Gaussian, Mat\\'ern, and B-spline kernels and present two vignettes\nillustrating the practical benefits of kernel thinning over i.i.d. sampling and\nstandard Markov chain Monte Carlo thinning, in dimensions $d=2$ through $100$.",
          "link": "http://arxiv.org/abs/2105.05842",
          "publishedOn": "2021-08-03T02:06:34.027Z",
          "wordCount": 644,
          "title": "Kernel Thinning. (arXiv:2105.05842v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_K/0/1/0/all/0/1\">Kalpit Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_V/0/1/0/all/0/1\">Vipul Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Sonu Kumar Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">Mohit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Sachchida Nand Tripathi</a>",
          "description": "Low-cost particulate matter sensors are transforming air quality monitoring\nbecause they have lower costs and greater mobility as compared to reference\nmonitors. Calibration of these low-cost sensors requires training data from\nco-deployed reference monitors. Machine Learning based calibration gives better\nperformance than conventional techniques, but requires a large amount of\ntraining data from the sensor, to be calibrated, co-deployed with a reference\nmonitor. In this work, we propose novel transfer learning methods for quick\ncalibration of sensors with minimal co-deployment with reference monitors.\nTransfer learning utilizes a large amount of data from other sensors along with\na limited amount of data from the target sensor. Our extensive experimentation\nfinds the proposed Model-Agnostic- Meta-Learning (MAML) based transfer learning\nmethod to be the most effective over other competitive baselines.",
          "link": "http://arxiv.org/abs/2108.00640",
          "publishedOn": "2021-08-03T02:06:34.021Z",
          "wordCount": 580,
          "title": "Few-shot calibration of low-cost air pollution (PM2.5) sensors using meta-learning. (arXiv:2108.00640v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jing Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Deqing Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shouhuai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yichen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hai Jin</a>",
          "description": "Automatically detecting software vulnerabilities in source code is an\nimportant problem that has attracted much attention. In particular, deep\nlearning-based vulnerability detectors, or DL-based detectors, are attractive\nbecause they do not need human experts to define features or patterns of\nvulnerabilities. However, such detectors' robustness is unclear. In this paper,\nwe initiate the study in this aspect by demonstrating that DL-based detectors\nare not robust against simple code transformations, dubbed attacks in this\npaper, as these transformations may be leveraged for malicious purposes. As a\nfirst step towards making DL-based detectors robust against such attacks, we\npropose an innovative framework, dubbed ZigZag, which is centered at (i)\ndecoupling feature learning and classifier learning and (ii) using a\nZigZag-style strategy to iteratively refine them until they converge to robust\nfeatures and robust classifiers. Experimental results show that the ZigZag\nframework can substantially improve the robustness of DL-based detectors.",
          "link": "http://arxiv.org/abs/2108.00669",
          "publishedOn": "2021-08-03T02:06:33.989Z",
          "wordCount": 589,
          "title": "Towards Making Deep Learning-based Vulnerability Detectors Robust. (arXiv:2108.00669v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00713",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1\">Brennan Nichyporuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardinell_J/0/1/0/all/0/1\">Jillian Cardinell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1\">Justin Szeto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mehta_R/0/1/0/all/0/1\">Raghav Mehta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios Tsaftaris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1\">Douglas L. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>",
          "description": "Many automatic machine learning models developed for focal pathology (e.g.\nlesions, tumours) detection and segmentation perform well, but do not\ngeneralize as well to new patient cohorts, impeding their widespread adoption\ninto real clinical contexts. One strategy to create a more diverse,\ngeneralizable training set is to naively pool datasets from different cohorts.\nSurprisingly, training on this \\it{big data} does not necessarily increase, and\nmay even reduce, overall performance and model generalizability, due to the\nexistence of cohort biases that affect label distributions. In this paper, we\npropose a generalized affine conditioning framework to learn and account for\ncohort biases across multi-source datasets, which we call Source-Conditioned\nInstance Normalization (SCIN). Through extensive experimentation on three\ndifferent, large scale, multi-scanner, multi-centre Multiple Sclerosis (MS)\nclinical trial MRI datasets, we show that our cohort bias adaptation method (1)\nimproves performance of the network on pooled datasets relative to naively\npooling datasets and (2) can quickly adapt to a new cohort by fine-tuning the\ninstance normalization parameters, thus learning the new cohort bias with only\n10 labelled samples.",
          "link": "http://arxiv.org/abs/2108.00713",
          "publishedOn": "2021-08-03T02:06:33.974Z",
          "wordCount": 638,
          "title": "Cohort Bias Adaptation in Aggregated Datasets for Lesion Segmentation. (arXiv:2108.00713v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Y/0/1/0/all/0/1\">Yen Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_B/0/1/0/all/0/1\">Bao Le</a>",
          "description": "Recent breakthroughs in the field of semi-supervised learning have achieved\nresults that match state-of-the-art traditional supervised learning methods.\nMost successful semi-supervised learning approaches in computer vision focus on\nleveraging huge amount of unlabeled data, learning the general representation\nvia data augmentation and transformation, creating pseudo labels, implementing\ndifferent loss functions, and eventually transferring this knowledge to more\ntask-specific smaller models. In this paper, we aim to conduct our analyses on\nthree different aspects of SimCLR, the current state-of-the-art semi-supervised\nlearning framework for computer vision. First, we analyze properties of\ncontrast learning on fine-tuning, as we understand that contrast learning is\nwhat makes this method so successful. Second, we research knowledge\ndistillation through teacher-forcing paradigm. We observe that when the teacher\nand the student share the same base model, knowledge distillation will achieve\nbetter result. Finally, we study how transfer learning works and its\nrelationship with the number of classes on different data sets. Our results\nindicate that transfer learning performs better when number of classes are\nsmaller.",
          "link": "http://arxiv.org/abs/2108.00587",
          "publishedOn": "2021-08-03T02:06:33.969Z",
          "wordCount": 606,
          "title": "Semi-Supervising Learning, Transfer Learning, and Knowledge Distillation with SimCLR. (arXiv:2108.00587v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.00120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuolin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhikuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1\">Hengzhi Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlas_B/0/1/0/all/0/1\">Bojan Karlas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Heng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>",
          "description": "As machine learning (ML) being applied to many mission-critical scenarios,\ncertifying ML model robustness becomes increasingly important. Many previous\nworks focuses on the robustness of independent ML and ensemble models, and can\nonly certify a very small magnitude of the adversarial perturbation. In this\npaper, we take a different viewpoint and improve learning robustness by going\nbeyond independent ML and ensemble models. We aim at promoting the generic\nSensing-Reasoning machine learning pipeline which contains both the sensing\n(e.g. deep neural networks) and reasoning (e.g. Markov logic networks (MLN))\ncomponents enriched with domain knowledge. Can domain knowledge help improve\nlearning robustness? Can we formally certify the end-to-end robustness of such\nan ML pipeline? We first theoretically analyze the computational complexity of\nchecking the provable robustness in the reasoning component. We then derive the\nprovable robustness bound for several concrete reasoning components. We show\nthat for reasoning components such as MLN and a specific family of Bayesian\nnetworks it is possible to certify the robustness of the whole pipeline even\nwith a large magnitude of perturbation which cannot be certified by existing\nwork. Finally, we conduct extensive real-world experiments on large scale\ndatasets to evaluate the certified robustness for Sensing-Reasoning ML\npipelines.",
          "link": "http://arxiv.org/abs/2003.00120",
          "publishedOn": "2021-08-03T02:06:33.948Z",
          "wordCount": 688,
          "title": "End-to-end Robustness for Sensing-Reasoning Machine Learning Pipelines. (arXiv:2003.00120v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.10933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhuo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Ximeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhongyuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jianfeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1\">Kui Ren</a>",
          "description": "Nowadays, machine learning models, especially neural networks, become\nprevalent in many real-world applications.These models are trained based on a\none-way trip from user data: as long as users contribute their data, there is\nno way to withdraw; and it is well-known that a neural network memorizes its\ntraining data. This contradicts the \"right to be forgotten\" clause of GDPR,\npotentially leading to law violations. To this end, machine unlearning becomes\na popular research topic, which allows users to eliminate memorization of their\nprivate data from a trained machine learning model.In this paper, we propose\nthe first uniform metric called for-getting rate to measure the effectiveness\nof a machine unlearning method. It is based on the concept of membership\ninference and describes the transformation rate of the eliminated data from\n\"memorized\" to \"unknown\" after conducting unlearning. We also propose a novel\nunlearning method calledForsaken. It is superior to previous work in either\nutility or efficiency (when achieving the same forgetting rate). We benchmark\nForsaken with eight standard datasets to evaluate its performance. The\nexperimental results show that it can achieve more than 90\\% forgetting rate on\naverage and only causeless than 5\\% accuracy loss.",
          "link": "http://arxiv.org/abs/2003.10933",
          "publishedOn": "2021-08-03T02:06:33.942Z",
          "wordCount": 681,
          "title": "Learn to Forget: Machine Unlearning via Neuron Masking. (arXiv:2003.10933v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00654",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tadayon_M/0/1/0/all/0/1\">Manie Tadayon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pottie_G/0/1/0/all/0/1\">Greg Pottie</a>",
          "description": "Educational systems have traditionally been evaluated using cross-sectional\nstudies, namely, examining a pretest, posttest, and single intervention.\nAlthough this is a popular approach, it does not model valuable information\nsuch as confounding variables, feedback to students, and other real-world\ndeviations of studies from ideal conditions. Moreover, learning inherently is a\nsequential process and should involve a sequence of interventions. In this\npaper, we propose various experimental and quasi-experimental designs for\neducational systems and quantify them using the graphical model and directed\nacyclic graph (DAG) language. We discuss the applications and limitations of\neach method in education. Furthermore, we propose to model the education system\nas time-varying treatments, confounders, and time-varying\ntreatments-confounders feedback. We show that if we control for a sufficient\nset of confounders and use appropriate inference techniques such as the inverse\nprobability of treatment weighting (IPTW) or g-formula, we can close the\nbackdoor paths and derive the unbiased causal estimate of joint interventions\non the outcome. Finally, we compare the g-formula and IPTW performance and\ndiscuss the pros and cons of using each method.",
          "link": "http://arxiv.org/abs/2108.00654",
          "publishedOn": "2021-08-03T02:06:33.934Z",
          "wordCount": 608,
          "title": "Causal Inference in Educational Systems: A Graphical Modeling Approach. (arXiv:2108.00654v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1\">Zhanghui Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aojun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jing-Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinjiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qingmin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wayne Zhang</a>",
          "description": "Network compression has been widely studied since it is able to reduce the\nmemory and computation cost during inference. However, previous methods seldom\ndeal with complicated structures like residual connections, group/depth-wise\nconvolution and feature pyramid network, where channels of multiple layers are\ncoupled and need to be pruned simultaneously. In this paper, we present a\ngeneral channel pruning approach that can be applied to various complicated\nstructures. Particularly, we propose a layer grouping algorithm to find coupled\nchannels automatically. Then we derive a unified metric based on Fisher\ninformation to evaluate the importance of a single channel and coupled\nchannels. Moreover, we find that inference speedup on GPUs is more correlated\nwith the reduction of memory rather than FLOPs, and thus we employ the memory\nreduction of each channel to normalize the importance. Our method can be used\nto prune any structures including those with coupled channels. We conduct\nextensive experiments on various backbones, including the classic ResNet and\nResNeXt, mobile-friendly MobileNetV2, and the NAS-based RegNet, both on image\nclassification and object detection which is under-explored. Experimental\nresults validate that our method can effectively prune sophisticated networks,\nboosting inference speed without sacrificing accuracy.",
          "link": "http://arxiv.org/abs/2108.00708",
          "publishedOn": "2021-08-03T02:06:33.928Z",
          "wordCount": 649,
          "title": "Group Fisher Pruning for Practical Network Compression. (arXiv:2108.00708v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1909.08610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Felicia Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "Improving the sample efficiency in reinforcement learning has been a\nlong-standing research problem. In this work, we aim to reduce the sample\ncomplexity of existing policy gradient methods. We propose a novel policy\ngradient algorithm called SRVR-PG, which only requires $O(1/\\epsilon^{3/2})$\nepisodes to find an $\\epsilon$-approximate stationary point of the nonconcave\nperformance function $J(\\boldsymbol{\\theta})$ (i.e., $\\boldsymbol{\\theta}$ such\nthat $\\|\\nabla J(\\boldsymbol{\\theta})\\|_2^2\\leq\\epsilon$). This sample\ncomplexity improves the existing result $O(1/\\epsilon^{5/3})$ for stochastic\nvariance reduced policy gradient algorithms by a factor of\n$O(1/\\epsilon^{1/6})$. In addition, we also propose a variant of SRVR-PG with\nparameter exploration, which explores the initial policy parameter from a prior\nprobability distribution. We conduct numerical experiments on classic control\nproblems in reinforcement learning to validate the performance of our proposed\nalgorithms.",
          "link": "http://arxiv.org/abs/1909.08610",
          "publishedOn": "2021-08-03T02:06:33.907Z",
          "wordCount": 610,
          "title": "Sample Efficient Policy Gradient Methods with Recursive Variance Reduction. (arXiv:1909.08610v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_N/0/1/0/all/0/1\">Ng Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochiai_H/0/1/0/all/0/1\">Hideya Ochiai</a>",
          "description": "An attack on deep learning systems where intelligent machines collaborate to\nsolve problems could cause a node in the network to make a mistake on a\ncritical judgment. At the same time, the security and privacy concerns of AI\nhave galvanized the attention of experts from multiple disciplines. In this\nresearch, we successfully mounted adversarial attacks on a federated learning\n(FL) environment using three different datasets. The attacks leveraged\ngenerative adversarial networks (GANs) to affect the learning process and\nstrive to reconstruct the private data of users by learning hidden features\nfrom shared local model parameters. The attack was target-oriented drawing data\nwith distinct class distribution from the CIFAR- 10, MNIST, and Fashion-MNIST\nrespectively. Moreover, by measuring the Euclidean distance between the real\ndata and the reconstructed adversarial samples, we evaluated the performance of\nthe adversary in the learning processes in various scenarios. At last, we\nsuccessfully reconstructed the real data of the victim from the shared global\nmodel parameters with all the applied datasets.",
          "link": "http://arxiv.org/abs/2108.00701",
          "publishedOn": "2021-08-03T02:06:33.900Z",
          "wordCount": 626,
          "title": "Information Stealing in Federated Learning Systems Based on Generative Adversarial Networks. (arXiv:2108.00701v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_K/0/1/0/all/0/1\">Kfir M. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sangwoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1\">Osvaldo Simeone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamai_S/0/1/0/all/0/1\">Shlomo Shamai</a> (Shitz)",
          "description": "Meta-learning, or learning to learn, offers a principled framework for\nfew-shot learning. It leverages data from multiple related learning tasks to\ninfer an inductive bias that enables fast adaptation on a new task. The\napplication of meta-learning was recently proposed for learning how to\ndemodulate from few pilots. The idea is to use pilots received and stored for\noffline use from multiple devices in order to meta-learn an adaptation\nprocedure with the aim of speeding up online training on new devices. Standard\nfrequentist learning, which can yield relatively accurate \"hard\" classification\ndecisions, is known to be poorly calibrated, particularly in the small-data\nregime. Poor calibration implies that the soft scores output by the demodulator\nare inaccurate estimates of the true probability of correct demodulation. In\nthis work, we introduce the use of Bayesian meta-learning via variational\ninference for the purpose of obtaining well-calibrated few-pilot demodulators.\nIn a Bayesian framework, each neural network weight is represented by a\ndistribution, capturing epistemic uncertainty. Bayesian meta-learning optimizes\nover the prior distribution of the weights. The resulting Bayesian ensembles\noffer better calibrated soft decisions, at the computational cost of running\nmultiple instances of the neural network for demodulation. Numerical results\nfor single-input single-output Rayleigh fading channels with transmitter's\nnon-linearities are provided that compare symbol error rate and expected\ncalibration error for both frequentist and Bayesian meta-learning, illustrating\nhow the latter is both more accurate and better-calibrated.",
          "link": "http://arxiv.org/abs/2108.00785",
          "publishedOn": "2021-08-03T02:06:33.888Z",
          "wordCount": 682,
          "title": "Learning to Learn to Demodulate with Uncertainty Quantification via Bayesian Meta-Learning. (arXiv:2108.00785v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00574",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Poderini_D/0/1/0/all/0/1\">Davide Poderini</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Polino_E/0/1/0/all/0/1\">Emanuele Polino</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Rodari_G/0/1/0/all/0/1\">Giovanni Rodari</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Suprano_A/0/1/0/all/0/1\">Alessia Suprano</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chaves_R/0/1/0/all/0/1\">Rafael Chaves</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sciarrino_F/0/1/0/all/0/1\">Fabio Sciarrino</a>",
          "description": "The violation of a Bell inequality is the paradigmatic example of\ndevice-independent quantum information: the nonclassicality of the data is\ncertified without the knowledge of the functioning of devices. In practice,\nhowever, all Bell experiments rely on the precise understanding of the\nunderlying physical mechanisms. Given that, it is natural to ask: Can one\nwitness nonclassical behaviour in a truly black-box scenario? Here we propose\nand implement, computationally and experimentally, a solution to this ab-initio\ntask. It exploits a robust automated optimization approach based on the\nStochastic Nelder-Mead algorithm. Treating preparation and measurement devices\nas black-boxes, and relying on the observed statistics only, our adaptive\nprotocol approaches the optimal Bell inequality violation after a limited\nnumber of iterations for a variety photonic states, measurement responses and\nBell scenarios. In particular, we exploit it for randomness certification from\nunknown states and measurements. Our results demonstrate the power of automated\nalgorithms, opening a new venue for the experimental implementation of\ndevice-independent quantum technologies.",
          "link": "http://arxiv.org/abs/2108.00574",
          "publishedOn": "2021-08-03T02:06:33.881Z",
          "wordCount": 593,
          "title": "Ab-initio experimental violation of Bell inequalities. (arXiv:2108.00574v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Inturrisi_J/0/1/0/all/0/1\">Jordan Inturrisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoo_S/0/1/0/all/0/1\">Sui Yang Khoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouzani_A/0/1/0/all/0/1\">Abbas Kouzani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagliarella_R/0/1/0/all/0/1\">Riccardo Pagliarella</a>",
          "description": "The activation function is at the heart of a deep neural networks\nnonlinearity; the choice of the function has great impact on the success of\ntraining. Currently, many practitioners prefer the Rectified Linear Unit (ReLU)\ndue to its simplicity and reliability, despite its few drawbacks. While most\nprevious functions proposed to supplant ReLU have been hand-designed, recent\nwork on learning the function during training has shown promising results. In\nthis paper we propose an adaptive piecewise linear activation function, the\nPiecewise Linear Unit (PiLU), which can be learned independently for each\ndimension of the neural network. We demonstrate how PiLU is a generalised\nrectifier unit and note its similarities with the Adaptive Piecewise Linear\nUnits, namely adaptive and piecewise linear. Across a distribution of 30\nexperiments, we show that for the same model architecture, hyperparameters, and\npre-processing, PiLU significantly outperforms ReLU: reducing classification\nerror by 18.53% on CIFAR-10 and 13.13% on CIFAR-100, for a minor increase in\nthe number of neurons. Further work should be dedicated to exploring\ngeneralised piecewise linear units, as well as verifying these results across\nother challenging domains and larger problems.",
          "link": "http://arxiv.org/abs/2108.00700",
          "publishedOn": "2021-08-03T02:06:33.875Z",
          "wordCount": 629,
          "title": "Piecewise Linear Units Improve Deep Neural Networks. (arXiv:2108.00700v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1912.02290",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kessler_S/0/1/0/all/0/1\">Samuel Kessler</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_V/0/1/0/all/0/1\">Vu Nguyen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zohren_S/0/1/0/all/0/1\">Stefan Zohren</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>",
          "description": "We place an Indian Buffet process (IBP) prior over the structure of a\nBayesian Neural Network (BNN), thus allowing the complexity of the BNN to\nincrease and decrease automatically. We further extend this model such that the\nprior on the structure of each hidden layer is shared globally across all\nlayers, using a Hierarchical-IBP (H-IBP). We apply this model to the problem of\nresource allocation in Continual Learning (CL) where new tasks occur and the\nnetwork requires extra resources. Our model uses online variational inference\nwith reparameterisation of the Bernoulli and Beta distributions, which\nconstitute the IBP and H-IBP priors. As we automatically learn the number of\nweights in each layer of the BNN, overfitting and underfitting problems are\nlargely overcome. We show empirically that our approach offers a competitive\nedge over existing methods in CL.",
          "link": "http://arxiv.org/abs/1912.02290",
          "publishedOn": "2021-08-03T02:06:33.856Z",
          "wordCount": 629,
          "title": "Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning. (arXiv:1912.02290v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vassilev_A/0/1/0/all/0/1\">Apostol Vassilev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Munawar Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Honglan Jin</a>",
          "description": "When people try to understand nuanced language they typically process\nmultiple input sensor modalities to complete this cognitive task. It turns out\nthe human brain has even a specialized neuron formation, called sagittal\nstratum, to help us understand sarcasm. We use this biological formation as the\ninspiration for designing a neural network architecture that combines\npredictions of different models on the same text to construct robust, accurate\nand computationally efficient classifiers for sentiment analysis and study\nseveral different realizations. Among them, we propose a systematic new\napproach to combining multiple predictions based on a dedicated neural network\nand develop mathematical analysis of it along with state-of-the-art\nexperimental results. We also propose a heuristic-hybrid technique for\ncombining models and back it up with experimental results on a representative\nbenchmark dataset and comparisons to other methods to show the advantages of\nthe new approaches.",
          "link": "http://arxiv.org/abs/2006.12958",
          "publishedOn": "2021-08-03T02:06:33.850Z",
          "wordCount": 640,
          "title": "Can you tell? SSNet -- a Sagittal Stratum-inspired Neural Network Framework for Sentiment Analysis. (arXiv:2006.12958v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00740",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Soh_Y/0/1/0/all/0/1\">Yong Sheng Soh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Varvitsiotis_A/0/1/0/all/0/1\">Antonios Varvitsiotis</a>",
          "description": "Given a matrix $X\\in \\mathbb{R}^{m\\times n}_+$ with non-negative entries, the\ncone factorization problem over a cone $\\mathcal{K}\\subseteq \\mathbb{R}^k$\nconcerns computing $\\{ a_1,\\ldots, a_{m} \\} \\subseteq \\mathcal{K}$ and $\\{\nb_1,\\ldots, b_{n} \\} \\subseteq~\\mathcal{K}^*$ belonging to its dual so that\n$X_{ij} = \\langle a_i, b_j \\rangle$ for all $i\\in [m], j\\in [n]$. Cone\nfactorizations are fundamental to mathematical optimization as they allow us to\nexpress convex bodies as feasible regions of linear conic programs. In this\npaper, we introduce and analyze the symmetric-cone multiplicative update (SCMU)\nalgorithm for computing cone factorizations when $\\mathcal{K}$ is symmetric;\ni.e., it is self-dual and homogeneous. Symmetric cones are of central interest\nin mathematical optimization as they provide a common language for studying\nlinear optimization over the nonnegative orthant (linear programs), over the\nsecond-order cone (second order cone programs), and over the cone of positive\nsemidefinite matrices (semidefinite programs). The SCMU algorithm is\nmultiplicative in the sense that the iterates are updated by applying a\nmeticulously chosen automorphism of the cone computed using a generalization of\nthe geometric mean to symmetric cones. Using an extension of Lieb's concavity\ntheorem and von Neumann's trace inequality to symmetric cones, we show that the\nsquared loss objective is non-decreasing along the trajectories of the SCMU\nalgorithm. Specialized to the nonnegative orthant, the SCMU algorithm\ncorresponds to the seminal algorithm by Lee and Seung for computing Nonnegative\nMatrix Factorizations.",
          "link": "http://arxiv.org/abs/2108.00740",
          "publishedOn": "2021-08-03T02:06:33.816Z",
          "wordCount": 668,
          "title": "Multiplicative updates for symmetric-cone factorizations. (arXiv:2108.00740v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/1912.04884",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_B/0/1/0/all/0/1\">Benjie Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Webb_S/0/1/0/all/0/1\">Stefan Webb</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1\">Tom Rainforth</a>",
          "description": "Despite their numerous successes, there are many scenarios where adversarial\nrisk metrics do not provide an appropriate measure of robustness. For example,\ntest-time perturbations may occur in a probabilistic manner rather than being\ngenerated by an explicit adversary, while the poor train--test generalization\nof adversarial metrics can limit their usage to simple problems. Motivated by\nthis, we develop a probabilistic robust risk framework, the statistically\nrobust risk (SRR), which considers pointwise corruption distributions, as\nopposed to worst-case adversaries. The SRR provides a distinct and\ncomplementary measure of robust performance, compared to natural and\nadversarial risk. We show that the SRR admits estimation and training schemes\nwhich are as simple and efficient as for the natural risk: these simply require\nnoising the inputs, but with a principled derivation for exactly how and why\nthis should be done. Furthermore, we demonstrate both theoretically and\nexperimentally that it can provide superior generalization performance compared\nwith adversarial risks, enabling application to high-dimensional datasets.",
          "link": "http://arxiv.org/abs/1912.04884",
          "publishedOn": "2021-08-03T02:06:33.811Z",
          "wordCount": 614,
          "title": "Statistically Robust Neural Network Classification. (arXiv:1912.04884v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00568",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guihong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1\">Sumit K. Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogras_U/0/1/0/all/0/1\">Umit Y. Ogras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marculescu_R/0/1/0/all/0/1\">Radu Marculescu</a>",
          "description": "Neural architecture search (NAS) is a promising technique to design efficient\nand high-performance deep neural networks (DNNs). As the performance\nrequirements of ML applications grow continuously, the hardware accelerators\nstart playing a central role in DNN design. This trend makes NAS even more\ncomplicated and time-consuming for most real applications. This paper proposes\nFLASH, a very fast NAS methodology that co-optimizes the DNN accuracy and\nperformance on a real hardware platform. As the main theoretical contribution,\nwe first propose the NN-Degree, an analytical metric to quantify the\ntopological characteristics of DNNs with skip connections (e.g., DenseNets,\nResNets, Wide-ResNets, and MobileNets). The newly proposed NN-Degree allows us\nto do training-free NAS within one second and build an accuracy predictor by\ntraining as few as 25 samples out of a vast search space with more than 63\nbillion configurations. Second, by performing inference on the target hardware,\nwe fine-tune and validate our analytical models to estimate the latency, area,\nand energy consumption of various DNN architectures while executing standard ML\ndatasets. Third, we construct a hierarchical algorithm based on simplicial\nhomology global optimization (SHGO) to optimize the model-architecture\nco-design process, while considering the area, latency, and energy consumption\nof the target hardware. We demonstrate that, compared to the state-of-the-art\nNAS approaches, our proposed hierarchical SHGO-based algorithm enables more\nthan four orders of magnitude speedup (specifically, the execution time of the\nproposed algorithm is about 0.1 seconds). Finally, our experimental evaluations\nshow that FLASH is easily transferable to different hardware architectures,\nthus enabling us to do NAS on a Raspberry Pi-3B processor in less than 3\nseconds.",
          "link": "http://arxiv.org/abs/2108.00568",
          "publishedOn": "2021-08-03T02:06:33.799Z",
          "wordCount": 711,
          "title": "FLASH: Fast Neural Architecture Search with Hardware Optimization. (arXiv:2108.00568v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wentao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bin Cui</a>",
          "description": "Data selection methods, such as active learning and core-set selection, are\nuseful tools for improving the data efficiency of deep learning models on\nlarge-scale datasets. However, recent deep learning models have moved forward\nfrom independent and identically distributed data to graph-structured data,\nsuch as social networks, e-commerce user-item graphs, and knowledge graphs.\nThis evolution has led to the emergence of Graph Neural Networks (GNNs) that go\nbeyond the models existing data selection methods are designed for. Therefore,\nwe present Grain, an efficient framework that opens up a new perspective\nthrough connecting data selection in GNNs with social influence maximization.\nBy exploiting the common patterns of GNNs, Grain introduces a novel feature\npropagation concept, a diversified influence maximization objective with novel\ninfluence and diversity functions, and a greedy algorithm with an approximation\nguarantee into a unified framework. Empirical studies on public datasets\ndemonstrate that Grain significantly improves both the performance and\nefficiency of data selection (including active learning and core-set selection)\nfor GNNs. To the best of our knowledge, this is the first attempt to bridge two\nlargely parallel threads of research, data selection, and social influence\nmaximization, in the setting of GNNs, paving new ways for improving data\nefficiency.",
          "link": "http://arxiv.org/abs/2108.00219",
          "publishedOn": "2021-08-03T02:06:33.771Z",
          "wordCount": 666,
          "title": "Grain: Improving Data Efficiency of Graph Neural Networks via Diversified Influence Maximization. (arXiv:2108.00219v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anshul Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_N/0/1/0/all/0/1\">Noveen Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sheshansh Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sumeet Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1\">Purushottam Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Manik Varma</a>",
          "description": "Deep extreme classification (XC) seeks to train deep architectures that can\ntag a data point with its most relevant subset of labels from an extremely\nlarge label set. The core utility of XC comes from predicting labels that are\nrarely seen during training. Such rare labels hold the key to personalized\nrecommendations that can delight and surprise a user. However, the large number\nof rare labels and small amount of training data per rare label offer\nsignificant statistical and computational challenges. State-of-the-art deep XC\nmethods attempt to remedy this by incorporating textual descriptions of labels\nbut do not adequately address the problem. This paper presents ECLARE, a\nscalable deep learning architecture that incorporates not only label text, but\nalso label correlations, to offer accurate real-time predictions within a few\nmilliseconds. Core contributions of ECLARE include a frugal architecture and\nscalable techniques to train deep models along with label correlation graphs at\nthe scale of millions of labels. In particular, ECLARE offers predictions that\nare 2 to 14% more accurate on both publicly available benchmark datasets as\nwell as proprietary datasets for a related products recommendation task sourced\nfrom the Bing search engine. Code for ECLARE is available at\nhttps://github.com/Extreme-classification/ECLARE.",
          "link": "http://arxiv.org/abs/2108.00261",
          "publishedOn": "2021-08-03T02:06:33.753Z",
          "wordCount": 655,
          "title": "ECLARE: Extreme Classification with Label Graph Correlations. (arXiv:2108.00261v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Joy T. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agu_N/0/1/0/all/0/1\">Nkechinyere N. Agu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1\">Ismini Lourentzou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Arjun Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paguio_J/0/1/0/all/0/1\">Joseph A. Paguio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jasper S. Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dee_E/0/1/0/all/0/1\">Edward C. Dee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_W/0/1/0/all/0/1\">William Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashyap_S/0/1/0/all/0/1\">Satyananda Kashyap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giovannini_A/0/1/0/all/0/1\">Andrea Giovannini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1\">Leo A. Celi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Mehdi Moradi</a>",
          "description": "Despite the progress in automatic detection of radiologic findings from chest\nX-ray (CXR) images in recent years, a quantitative evaluation of the\nexplainability of these models is hampered by the lack of locally labeled\ndatasets for different findings. With the exception of a few expert-labeled\nsmall-scale datasets for specific findings, such as pneumonia and pneumothorax,\nmost of the CXR deep learning models to date are trained on global \"weak\"\nlabels extracted from text reports, or trained via a joint image and\nunstructured text learning strategy. Inspired by the Visual Genome effort in\nthe computer vision community, we constructed the first Chest ImaGenome dataset\nwith a scene graph data structure to describe $242,072$ images. Local\nannotations are automatically produced using a joint rule-based natural\nlanguage processing (NLP) and atlas-based bounding box detection pipeline.\nThrough a radiologist constructed CXR ontology, the annotations for each CXR\nare connected as an anatomy-centered scene graph, useful for image-level\nreasoning and multimodal fusion applications. Overall, we provide: i) $1,256$\ncombinations of relation annotations between $29$ CXR anatomical locations\n(objects with bounding box coordinates) and their attributes, structured as a\nscene graph per image, ii) over $670,000$ localized comparison relations (for\nimproved, worsened, or no change) between the anatomical locations across\nsequential exams, as well as ii) a manually annotated gold standard scene graph\ndataset from $500$ unique patients.",
          "link": "http://arxiv.org/abs/2108.00316",
          "publishedOn": "2021-08-03T02:06:33.744Z",
          "wordCount": 697,
          "title": "Chest ImaGenome Dataset for Clinical Reasoning. (arXiv:2108.00316v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1\">Mithun Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1\">Punyajoy Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutt_R/0/1/0/all/0/1\">Ritam Dutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_B/0/1/0/all/0/1\">Binny Mathew</a>",
          "description": "Hate speech is regarded as one of the crucial issues plaguing the online\nsocial media. The current literature on hate speech detection leverages\nprimarily the textual content to find hateful posts and subsequently identify\nhateful users. However, this methodology disregards the social connections\nbetween users. In this paper, we run a detailed exploration of the problem\nspace and investigate an array of models ranging from purely textual to graph\nbased to finally semi-supervised techniques using Graph Neural Networks (GNN)\nthat utilize both textual and graph-based features. We run exhaustive\nexperiments on two datasets -- Gab, which is loosely moderated and Twitter,\nwhich is strictly moderated. Overall the AGNN model achieves 0.791 macro\nF1-score on the Gab dataset and 0.780 macro F1-score on the Twitter dataset\nusing only 5% of the labeled instances, considerably outperforming all the\nother models including the fully supervised ones. We perform detailed error\nanalysis on the best performing text and graph based models and observe that\nhateful users have unique network neighborhood signatures and the AGNN model\nbenefits by paying attention to these signatures. This property, as we observe,\nalso allows the model to generalize well across platforms in a zero-shot\nsetting. Lastly, we utilize the best performing GNN model to analyze the\nevolution of hateful users and their targets over time in Gab.",
          "link": "http://arxiv.org/abs/2108.00524",
          "publishedOn": "2021-08-03T02:06:33.712Z",
          "wordCount": 694,
          "title": "You too Brutus! Trapping Hateful Users in Social Media: Challenges, Solutions & Insights. (arXiv:2108.00524v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00065",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chee_J/0/1/0/all/0/1\">Jerry Chee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renz_M/0/1/0/all/0/1\">Megan Renz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damle_A/0/1/0/all/0/1\">Anil Damle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1\">Chris De Sa</a>",
          "description": "We introduce a principled approach to neural network pruning that casts the\nproblem as a structured low-rank matrix approximation. Our method uses a novel\napplication of a matrix factorization technique called the interpolative\ndecomposition to approximate the activation output of a network layer. This\ntechnique selects neurons or channels in the layer and propagates a corrective\ninterpolation matrix to the next layer, resulting in a dense, pruned network\nwith minimal degradation before fine tuning. We demonstrate how to prune a\nneural network by first building a set of primitives to prune a single fully\nconnected or convolution layer and then composing these primitives to prune\ndeep multi-layer networks. Theoretical guarantees are provided for pruning a\nsingle hidden layer fully connected network. Pruning with interpolative\ndecompositions achieves strong empirical results compared to the\nstate-of-the-art on multiple applications from one and two hidden layer\nnetworks on Fashion MNIST to VGG and ResNets on CIFAR-10. Notably, we achieve\nan accuracy of 93.62 $\\pm$ 0.36% using VGG-16 on CIFAR-10, with a 51% FLOPS\nreduction. This gains 0.02% from the full-sized model.",
          "link": "http://arxiv.org/abs/2108.00065",
          "publishedOn": "2021-08-03T02:06:33.692Z",
          "wordCount": 613,
          "title": "Pruning Neural Networks with Interpolative Decompositions. (arXiv:2108.00065v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00071",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Ajay Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_D/0/1/0/all/0/1\">Deri Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batarseh_F/0/1/0/all/0/1\">Feras A. Batarseh</a>",
          "description": "Dealing with imbalanced data is a prevalent problem while performing\nclassification on the datasets. Many times, this problem contributes to bias\nwhile making decisions or implementing policies. Thus, it is vital to\nunderstand the factors which cause imbalance in the data (or class imbalance).\nSuch hidden biases and imbalances can lead to data tyranny and a major\nchallenge to a data democracy. In this chapter, two essential statistical\nelements are resolved: the degree of class imbalance and the complexity of the\nconcept; solving such issues helps in building the foundations of a data\ndemocracy. Furthermore, statistical measures which are appropriate in these\nscenarios are discussed and implemented on a real-life dataset (car insurance\nclaims). In the end, popular data-level methods such as random oversampling,\nrandom undersampling, synthetic minority oversampling technique, Tomek link,\nand others are implemented in Python, and their performance is compared.",
          "link": "http://arxiv.org/abs/2108.00071",
          "publishedOn": "2021-08-03T02:06:33.685Z",
          "wordCount": 611,
          "title": "Foundations of data imbalance and solutions for a data democracy. (arXiv:2108.00071v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1\">Adityanarayanan Radhakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanakis_G/0/1/0/all/0/1\">George Stefanakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belkin_M/0/1/0/all/0/1\">Mikhail Belkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uhler_C/0/1/0/all/0/1\">Caroline Uhler</a>",
          "description": "Matrix completion problems arise in many applications including\nrecommendation systems, computer vision, and genomics. Increasingly larger\nneural networks have been successful in many of these applications, but at\nconsiderable computational costs. Remarkably, taking the width of a neural\nnetwork to infinity allows for improved computational performance. In this\nwork, we develop an infinite width neural network framework for matrix\ncompletion that is simple, fast, and flexible. Simplicity and speed come from\nthe connection between the infinite width limit of neural networks and kernels\nknown as neural tangent kernels (NTK). In particular, we derive the NTK for\nfully connected and convolutional neural networks for matrix completion. The\nflexibility stems from a feature prior, which allows encoding relationships\nbetween coordinates of the target matrix, akin to semi-supervised learning. The\neffectiveness of our framework is demonstrated through competitive results for\nvirtual drug screening and image inpainting/reconstruction. We also provide an\nimplementation in Python to make our framework accessible on standard hardware\nto a broad audience.",
          "link": "http://arxiv.org/abs/2108.00131",
          "publishedOn": "2021-08-03T02:06:33.672Z",
          "wordCount": 602,
          "title": "Simple, Fast, and Flexible Framework for Matrix Completion with Infinite Width Neural Networks. (arXiv:2108.00131v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1\">Ethel Ong</a>",
          "description": "In order to ensure quality and effective learning, fluency, and\ncomprehension, the proper identification of the difficulty levels of reading\nmaterials should be observed. In this paper, we describe the development of\nautomatic machine learning-based readability assessment models for educational\nFilipino texts using the most diverse set of linguistic features for the\nlanguage. Results show that using a Random Forest model obtained a high\nperformance of 62.7% in terms of accuracy, and 66.1% when using the optimal\ncombination of feature sets consisting of traditional and syllable\npattern-based predictors.",
          "link": "http://arxiv.org/abs/2108.00241",
          "publishedOn": "2021-08-03T02:06:33.666Z",
          "wordCount": 531,
          "title": "Diverse Linguistic Features for Assessing Reading Difficulty of Educational Filipino Texts. (arXiv:2108.00241v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2007.09060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noufi_C/0/1/0/all/0/1\">Camille Noufi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Prateek Verma</a>",
          "description": "In music and speech, meaning is derived at multiple levels of context.\nAffect, for example, can be inferred both by a short sound token and by sonic\npatterns over a longer temporal window such as an entire recording. In this\nletter, we focus on inferring meaning from this dichotomy of contexts. We show\nhow contextual representations of short sung vocal lines can be implicitly\nlearned from fundamental frequency ($F_0$) and thus be used as a meaningful\nfeature space for downstream Music Information Retrieval (MIR) tasks. We\npropose three self-supervised deep learning paradigms which leverage pseudotask\nlearning of these two levels of context to produce latent representation\nspaces. We evaluate the usefulness of these representations by embedding unseen\npitch contours into each space and conducting downstream classification tasks.\nOur results show that contextual representation can enhance downstream\nclassification by as much as 15\\% as compared to using traditional statistical\ncontour features.",
          "link": "http://arxiv.org/abs/2007.09060",
          "publishedOn": "2021-08-03T02:06:33.659Z",
          "wordCount": 632,
          "title": "Self-Supervised Learning of Context-Aware Pitch Prosody Representations. (arXiv:2007.09060v4 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_K/0/1/0/all/0/1\">Kaiyi Ji</a>",
          "description": "Bilevel optimization has become a powerful framework in various machine\nlearning applications including meta-learning, hyperparameter optimization, and\nnetwork architecture search. There are generally two classes of bilevel\noptimization formulations for machine learning: 1) problem-based bilevel\noptimization, whose inner-level problem is formulated as finding a minimizer of\na given loss function; and 2) algorithm-based bilevel optimization, whose\ninner-level solution is an output of a fixed algorithm. For the first class,\ntwo popular types of gradient-based algorithms have been proposed for\nhypergradient estimation via approximate implicit differentiation (AID) and\niterative differentiation (ITD). Algorithms for the second class include the\npopular model-agnostic meta-learning (MAML) and almost no inner loop (ANIL).\nHowever, the convergence rate and fundamental limitations of bilevel\noptimization algorithms have not been well explored.\n\nThis thesis provides a comprehensive convergence rate analysis for bilevel\nalgorithms in the aforementioned two classes. We further propose principled\nalgorithm designs for bilevel optimization with higher efficiency and\nscalability. For the problem-based formulation, we provide a convergence rate\nanalysis for AID- and ITD-based bilevel algorithms. We then develop\nacceleration bilevel algorithms, for which we provide shaper convergence\nanalysis with relaxed assumptions. We also provide the first lower bounds for\nbilevel optimization, and establish the optimality by providing matching upper\nbounds under certain conditions. We finally propose new stochastic bilevel\noptimization algorithms with lower complexity and higher efficiency in\npractice. For the algorithm-based formulation, we develop a theoretical\nconvergence for general multi-step MAML and ANIL, and characterize the impact\nof parameter selections and loss geometries on the their complexities.",
          "link": "http://arxiv.org/abs/2108.00330",
          "publishedOn": "2021-08-03T02:06:33.630Z",
          "wordCount": 705,
          "title": "Bilevel Optimization for Machine Learning: Algorithm Design and Convergence Analysis. (arXiv:2108.00330v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.04395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1\">Xiang Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Saizhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengfei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alex X. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chunming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>",
          "description": "While the celebrated graph neural networks yield effective representations\nfor individual nodes of a graph, there has been relatively less success in\nextending to the task of graph similarity learning. Recent work on graph\nsimilarity learning has considered either global-level graph-graph interactions\nor low-level node-node interactions, however ignoring the rich cross-level\ninteractions (e.g., between each node of one graph and the other whole graph).\nIn this paper, we propose a multi-level graph matching network (MGMN) framework\nfor computing the graph similarity between any pair of graph-structured objects\nin an end-to-end fashion. In particular, the proposed MGMN consists of a\nnode-graph matching network for effectively learning cross-level interactions\nbetween each node of one graph and the other whole graph, and a siamese graph\nneural network to learn global-level interactions between two input graphs.\nFurthermore, to compensate for the lack of standard benchmark datasets, we have\ncreated and collected a set of datasets for both the graph-graph classification\nand graph-graph regression tasks with different sizes in order to evaluate the\neffectiveness and robustness of our models. Comprehensive experiments\ndemonstrate that MGMN consistently outperforms state-of-the-art baseline models\non both the graph-graph classification and graph-graph regression tasks.\nCompared with previous work, MGMN also exhibits stronger robustness as the\nsizes of the two input graphs increase.",
          "link": "http://arxiv.org/abs/2007.04395",
          "publishedOn": "2021-08-03T02:06:33.589Z",
          "wordCount": 715,
          "title": "Multi-Level Graph Matching Networks for Deep Graph Similarity Learning. (arXiv:2007.04395v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_W/0/1/0/all/0/1\">Wei W. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiang Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Dan Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weishen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhou Jin</a>",
          "description": "One of the greatest challenges in IC design is the repeated executions of\ncomputationally expensive SPICE simulations, particularly when highly complex\nchip testing/verification is involved. Recently, pseudo transient analysis\n(PTA) has shown to be one of the most promising continuation SPICE solver.\nHowever, the PTA efficiency is highly influenced by the inserted\npseudo-parameters. In this work, we proposed BoA-PTA, a Bayesian optimization\naccelerated PTA that can substantially accelerate simulations and improve\nconvergence performance without introducing extra errors. Furthermore, our\nmethod does not require any pre-computation data or offline training. The\nacceleration framework can either be implemented to speed up ongoing repeated\nsimulations immediately or to improve new simulations of completely different\ncircuits. BoA-PTA is equipped with cutting-edge machine learning techniques,\ne.g., deep learning, Gaussian process, Bayesian optimization, non-stationary\nmonotonic transformation, and variational inference via parameterization. We\nassess BoA-PTA in 43 benchmark circuits against other SOTA SPICE solvers and\ndemonstrate an average 2.3x (maximum 3.5x) speed-up over the original CEPTA.",
          "link": "http://arxiv.org/abs/2108.00257",
          "publishedOn": "2021-08-03T02:06:33.583Z",
          "wordCount": 607,
          "title": "BoA-PTA, A Bayesian Optimization Accelerated Error-Free SPICE Solver. (arXiv:2108.00257v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kubotani_Y/0/1/0/all/0/1\">Yoshiki Kubotani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukuhara_Y/0/1/0/all/0/1\">Yoshihiro Fukuhara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morishima_S/0/1/0/all/0/1\">Shigeo Morishima</a>",
          "description": "A major challenge in the field of education is providing review schedules\nthat present learned items at appropriate intervals to each student so that\nmemory is retained over time. In recent years, attempts have been made to\nformulate item reviews as sequential decision-making problems to realize\nadaptive instruction based on the knowledge state of students. It has been\nreported previously that reinforcement learning can help realize mathematical\nmodels of students learning strategies to maintain a high memory rate. However,\noptimization using reinforcement learning requires a large number of\ninteractions, and thus it cannot be applied directly to actual students. In\nthis study, we propose a framework for optimizing teaching strategies by\nconstructing a virtual model of the student while minimizing the interaction\nwith the actual teaching target. In addition, we conducted an experiment\nconsidering actual instructions using the mathematical model and confirmed that\nthe model performance is comparable to that of conventional teaching methods.\nOur framework can directly substitute mathematical models used in experiments\nwith human students, and our results can serve as a buffer between theoretical\ninstructional optimization and practical applications in e-learning systems.",
          "link": "http://arxiv.org/abs/2108.00268",
          "publishedOn": "2021-08-03T02:06:33.576Z",
          "wordCount": 650,
          "title": "RLTutor: Reinforcement Learning Based Adaptive Tutoring System by Modeling Virtual Student with Fewer Interactions. (arXiv:2108.00268v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Ningyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xuefeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yi Xiong</a>",
          "description": "It has been recently shown in the literature that the sample averages from\nonline learning experiments are biased when used to estimate the mean reward.\nTo correct the bias, off-policy evaluation methods, including importance\nsampling and doubly robust estimators, typically calculate the propensity\nscore, which is unavailable in this setting due to unknown reward distribution\nand the adaptive policy. This paper provides a procedure to debias the samples\nusing bootstrap, which doesn't require the knowledge of the reward distribution\nat all. Numerical experiments demonstrate the effective bias reduction for\nsamples generated by popular multi-armed bandit algorithms such as\nExplore-Then-Commit (ETC), UCB, Thompson sampling and $\\epsilon$-greedy. We\nalso analyze and provide theoretical justifications for the procedure under the\nETC algorithm, including the asymptotic convergence of the bias decay rate in\nthe real and bootstrap worlds.",
          "link": "http://arxiv.org/abs/2108.00236",
          "publishedOn": "2021-08-03T02:06:33.560Z",
          "wordCount": 568,
          "title": "Debiasing Samples from Online Learning Using Bootstrap. (arXiv:2108.00236v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1\">Uttaran Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Childs_E/0/1/0/all/0/1\">Elizabeth Childs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rewkowski_N/0/1/0/all/0/1\">Nicholas Rewkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>",
          "description": "We present a generative adversarial network to synthesize 3D pose sequences\nof co-speech upper-body gestures with appropriate affective expressions. Our\nnetwork consists of two components: a generator to synthesize gestures from a\njoint embedding space of features encoded from the input speech and the seed\nposes, and a discriminator to distinguish between the synthesized pose\nsequences and real 3D pose sequences. We leverage the Mel-frequency cepstral\ncoefficients and the text transcript computed from the input speech in separate\nencoders in our generator to learn the desired sentiments and the associated\naffective cues. We design an affective encoder using multi-scale\nspatial-temporal graph convolutions to transform 3D pose sequences into latent,\npose-based affective features. We use our affective encoder in both our\ngenerator, where it learns affective features from the seed poses to guide the\ngesture synthesis, and our discriminator, where it enforces the synthesized\ngestures to contain the appropriate affective expressions. We perform extensive\nevaluations on two benchmark datasets for gesture synthesis from the speech,\nthe TED Gesture Dataset and the GENEA Challenge 2020 Dataset. Compared to the\nbest baselines, we improve the mean absolute joint error by 10--33%, the mean\nacceleration difference by 8--58%, and the Fr\\'echet Gesture Distance by\n21--34%. We also conduct a user study and observe that compared to the best\ncurrent baselines, around 15.28% of participants indicated our synthesized\ngestures appear more plausible, and around 16.32% of participants felt the\ngestures had more appropriate affective expressions aligned with the speech.",
          "link": "http://arxiv.org/abs/2108.00262",
          "publishedOn": "2021-08-03T02:06:33.553Z",
          "wordCount": 703,
          "title": "Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective Expression Learning. (arXiv:2108.00262v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuxin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_H/0/1/0/all/0/1\">Haoran Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianqiao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>",
          "description": "Nodule segmentation from breast ultrasound images is challenging yet\nessential for the diagnosis. Weakly-supervised segmentation (WSS) can help\nreduce time-consuming and cumbersome manual annotation. Unlike existing\nweakly-supervised approaches, in this study, we propose a novel and general WSS\nframework called Flip Learning, which only needs the box annotation.\nSpecifically, the target in the label box will be erased gradually to flip the\nclassification tag, and the erased region will be considered as the\nsegmentation result finally. Our contribution is three-fold. First, our\nproposed approach erases on superpixel level using a Multi-agent Reinforcement\nLearning framework to exploit the prior boundary knowledge and accelerate the\nlearning process. Second, we design two rewards: classification score and\nintensity distribution reward, to avoid under- and over-segmentation,\nrespectively. Third, we adopt a coarse-to-fine learning strategy to reduce the\nresidual errors and improve the segmentation performance. Extensively validated\non a large dataset, our proposed approach achieves competitive performance and\nshows great potential to narrow the gap between fully-supervised and\nweakly-supervised learning.",
          "link": "http://arxiv.org/abs/2108.00752",
          "publishedOn": "2021-08-03T02:06:33.541Z",
          "wordCount": 624,
          "title": "Flip Learning: Erase to Segment. (arXiv:2108.00752v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Rui Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hase_K/0/1/0/all/0/1\">Kazunori Hase</a>",
          "description": "To date, very few biomedical signals have transitioned from research\napplications to clinical applications. This is largely due to the lack of trust\nin the diagnostic ability of non-stationary signals. To reach the level of\nclinical diagnostic application, classification using high-quality signal\nfeatures is necessary. While there has been considerable progress in machine\nlearning in recent years, especially deep learning, progress has been quite\nlimited in the field of feature engineering. This study proposes a feature\nextraction algorithm based on group intelligence which we call a Plant Root\nSystem (PRS) algorithm. Importantly, the correlation between features produced\nby this PRS algorithm and traditional features is low, and the accuracy of\nseveral widely-used classifiers was found to be substantially improved with the\naddition of PRS features. It is expected that more biomedical signals can be\napplied to clinical diagnosis using the proposed algorithm.",
          "link": "http://arxiv.org/abs/2108.00214",
          "publishedOn": "2021-08-03T02:06:33.533Z",
          "wordCount": 584,
          "title": "A Plant Root System Algorithm Based on Swarm Intelligence for One-dimensional Biomedical Signal Feature Engineering. (arXiv:2108.00214v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kihwan Kim</a>",
          "description": "All learning algorithms for recommendations face inevitable and critical\ntrade-off between exploiting partial knowledge of a user's preferences for\nshort-term satisfaction and exploring additional user preferences for long-term\ncoverage. Although exploration is indispensable for long success of a\nrecommender system, the exploration has been considered as the risk to decrease\nuser satisfaction. The reason for the risk is that items chosen for exploration\nfrequently mismatch with the user's interests. To mitigate this risk,\nrecommender systems have mixed items chosen for exploration into a\nrecommendation list, disguising the items as recommendations to elicit feedback\non the items to discover the user's additional tastes. This mix-in approach has\nbeen widely used in many recommenders, but there is rare research, evaluating\nthe effectiveness of the mix-in approach or proposing a new approach for\neliciting user feedback without deceiving users. In this work, we aim to\npropose a new approach for feedback elicitation without any deception and\ncompare our approach to the conventional mix-in approach for evaluation. To\nthis end, we designed a recommender interface that reveals which items are for\nexploration and conducted a within-subject study with 94 MTurk workers. Our\nresults indicated that users left significantly more feedback on items chosen\nfor exploration with our interface. Besides, users evaluated that our new\ninterface is better than the conventional mix-in interface in terms of novelty,\ndiversity, transparency, trust, and satisfaction. Finally, path analysis show\nthat, in only our new interface, exploration caused to increase user-centric\nevaluation metrics. Our work paves the way for how to design an interface,\nwhich utilizes learning algorithm based on users' feedback signals, giving\nbetter user experience and gathering more feedback data.",
          "link": "http://arxiv.org/abs/2108.00151",
          "publishedOn": "2021-08-03T02:06:33.527Z",
          "wordCount": 708,
          "title": "An Empirical analysis on Transparent Algorithmic Exploration in Recommender Systems. (arXiv:2108.00151v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Folgoc_L/0/1/0/all/0/1\">Loic Le Folgoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baltatzis_V/0/1/0/all/0/1\">Vasileios Baltatzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alansary_A/0/1/0/all/0/1\">Amir Alansary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Sujal Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devaraj_A/0/1/0/all/0/1\">Anand Devaraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_S/0/1/0/all/0/1\">Sam Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzanera_O/0/1/0/all/0/1\">Octavio E. Martinez Manzanera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanavati_F/0/1/0/all/0/1\">Fahdi Kanavati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1\">Arjun Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia Schnabel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>",
          "description": "Datasets are rarely a realistic approximation of the target population. Say,\nprevalence is misrepresented, image quality is above clinical standards, etc.\nThis mismatch is known as sampling bias. Sampling biases are a major hindrance\nfor machine learning models. They cause significant gaps between model\nperformance in the lab and in the real world. Our work is a solution to\nprevalence bias. Prevalence bias is the discrepancy between the prevalence of a\npathology and its sampling rate in the training dataset, introduced upon\ncollecting data or due to the practioner rebalancing the training batches. This\npaper lays the theoretical and computational framework for training models, and\nfor prediction, in the presence of prevalence bias. Concretely a bias-corrected\nloss function, as well as bias-corrected predictive rules, are derived under\nthe principles of Bayesian risk minimization. The loss exhibits a direct\nconnection to the information gain. It offers a principled alternative to\nheuristic training losses and complements test-time procedures based on\nselecting an operating point from summary curves. It integrates seamlessly in\nthe current paradigm of (deep) learning using stochastic backpropagation and\nnaturally with Bayesian models.",
          "link": "http://arxiv.org/abs/2108.00250",
          "publishedOn": "2021-08-03T02:06:33.520Z",
          "wordCount": 655,
          "title": "Bayesian analysis of the prevalence bias: learning and predicting from imbalanced data. (arXiv:2108.00250v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00318",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Gupta_P/0/1/0/all/0/1\">Prachi Gupta</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bhat_H/0/1/0/all/0/1\">Harish S. Bhat</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ranka_K/0/1/0/all/0/1\">Karnamohit Ranka</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Isborn_C/0/1/0/all/0/1\">Christine M. Isborn</a>",
          "description": "We develop a statistical method to learn a molecular Hamiltonian matrix from\na time-series of electron density matrices. We extend our previous method to\nlarger molecular systems by incorporating physical properties to reduce\ndimensionality, while also exploiting regularization techniques like ridge\nregression for addressing multicollinearity. With the learned Hamiltonian we\ncan solve the Time-Dependent Hartree-Fock (TDHF) equation to propagate the\nelectron density in time, and predict its dynamics for field-free and field-on\nscenarios. We observe close quantitative agreement between the predicted\ndynamics and ground truth for both field-off trajectories similar to the\ntraining data, and field-on trajectories outside of the training data.",
          "link": "http://arxiv.org/abs/2108.00318",
          "publishedOn": "2021-08-03T02:06:33.514Z",
          "wordCount": 540,
          "title": "Statistical learning method for predicting density-matrix based electron dynamics. (arXiv:2108.00318v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">You-Wei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1\">Chuan-Xian Ren</a>",
          "description": "As a vital problem in classification-oriented transfer, unsupervised domain\nadaptation (UDA) has attracted widespread attention in recent years. Previous\nUDA methods assume the marginal distributions of different domains are shifted\nwhile ignoring the discriminant information in the label distributions. This\nleads to classification performance degeneration in real applications. In this\nwork, we focus on the conditional distribution shift problem which is of great\nconcern to current conditional invariant models. We aim to seek a kernel\ncovariance embedding for conditional distribution which remains yet unexplored.\nTheoretically, we propose the Conditional Kernel Bures (CKB) metric for\ncharacterizing conditional distribution discrepancy, and derive an empirical\nestimation for the CKB metric without introducing the implicit kernel feature\nmap. It provides an interpretable approach to understand the knowledge transfer\nmechanism. The established consistency theory of the empirical estimation\nprovides a theoretical guarantee for convergence. A conditional distribution\nmatching network is proposed to learn the conditional invariant and\ndiscriminative features for UDA. Extensive experiments and analysis show the\nsuperiority of our proposed model.",
          "link": "http://arxiv.org/abs/2108.00302",
          "publishedOn": "2021-08-03T02:06:33.503Z",
          "wordCount": 599,
          "title": "Conditional Bures Metric for Domain Adaptation. (arXiv:2108.00302v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rucker_M/0/1/0/all/0/1\">Mark Rucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_S/0/1/0/all/0/1\">Stephen Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_R/0/1/0/all/0/1\">Roy Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beling_P/0/1/0/all/0/1\">Peter A. Beling</a>",
          "description": "In adversarial environments, one side could gain an advantage by identifying\nthe opponent's strategy. For example, in combat games, if an opponents strategy\nis identified as overly aggressive, one could lay a trap that exploits the\nopponent's aggressive nature. However, an opponent's strategy is not always\napparent and may need to be estimated from observations of their actions. This\npaper proposes to use inverse reinforcement learning (IRL) to identify\nstrategies in adversarial environments. Specifically, the contributions of this\nwork are 1) the demonstration of this concept on gaming combat data generated\nfrom three pre-defined strategies and 2) the framework for using IRL to achieve\nstrategy identification. The numerical experiments demonstrate that the\nrecovered rewards can be identified using a variety of techniques. In this\npaper, the recovered reward are visually displayed, clustered using\nunsupervised learning, and classified using a supervised learner.",
          "link": "http://arxiv.org/abs/2108.00293",
          "publishedOn": "2021-08-03T02:06:33.471Z",
          "wordCount": 599,
          "title": "Inverse Reinforcement Learning for Strategy Identification. (arXiv:2108.00293v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colombelli_F/0/1/0/all/0/1\">Felipe Colombelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowalski_T/0/1/0/all/0/1\">Thayne Woycinck Kowalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recamonde_Mendoza_M/0/1/0/all/0/1\">Mariana Recamonde-Mendoza</a>",
          "description": "The discovery of disease biomarkers from gene expression data has been\ngreatly advanced by feature selection (FS) methods, especially using ensemble\nFS (EFS) strategies with perturbation at the data level (i.e., homogeneous,\nHom-EFS) or method level (i.e., heterogeneous, Het-EFS). Here we proposed a\nHybrid EFS (Hyb-EFS) design that explores both types of perturbation to improve\nthe stability and the predictive power of candidate biomarkers. With this,\nHyb-EFS aims to disrupt associations of good performance with a single dataset,\nsingle algorithm, or a specific combination of both, which is particularly\ninteresting for better reproducibility of genomic biomarkers. We investigated\nthe adequacy of our approach for microarray data related to four types of\ncancer, carrying out an extensive comparison with other ensemble and single FS\napproaches. Five FS methods were used in our experiments: Wx, Symmetrical\nUncertainty (SU), Gain Ratio (GR), Characteristic Direction (GeoDE), and\nReliefF. We observed that the Hyb-EFS and Het-EFS approaches attenuated the\nlarge performance variation observed for most single FS and Hom-EFS across\ndistinct datasets. Also, the Hyb-EFS improved upon the stability of the Het-EFS\nwithin our domain. Comparing the Hyb-EFS and Het-EFS composed of the\ntop-performing selectors (Wx, GR, and SU), our hybrid approach surpassed the\nequivalent heterogeneous design and the best Hom-EFS (Hom-Wx). Interestingly,\nthe rankings produced by our Hyb-EFS reached greater biological plausibility,\nwith a notably high enrichment for cancer-related genes and pathways. Thus, our\nexperiments suggest the potential of the proposed Hybrid EFS design in\ndiscovering candidate biomarkers from microarray data. Finally, we provide an\nopen-source framework to support similar analyses in other domains, both as a\nuser-friendly application and a plain Python package.",
          "link": "http://arxiv.org/abs/2108.00290",
          "publishedOn": "2021-08-03T02:06:33.440Z",
          "wordCount": 724,
          "title": "A Hybrid Ensemble Feature Selection Design for Candidate Biomarkers Discovery from Transcriptome Profiles. (arXiv:2108.00290v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Akshita Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinzamuri_B/0/1/0/all/0/1\">Bhanukiran Vinzamuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chandan K. Reddy</a>",
          "description": "With the growing interest in the machine learning community to solve\nreal-world problems, it has become crucial to uncover the hidden reasoning\nbehind their decisions by focusing on the fairness and auditing the predictions\nmade by these black-box models. In this paper, we propose a novel method to\naddress two key issues: (a) Can we simultaneously learn fair disentangled\nrepresentations while ensuring the utility of the learned representation for\ndownstream tasks, and (b)Can we provide theoretical insights into when the\nproposed approach will be both fair and accurate. To address the former, we\npropose the method FRIED, Fair Representation learning using Interpolation\nEnabled Disentanglement. In our architecture, by imposing a critic-based\nadversarial framework, we enforce the interpolated points in the latent space\nto be more realistic. This helps in capturing the data manifold effectively and\nenhances the utility of the learned representation for downstream prediction\ntasks. We address the latter question by developing a theory on\nfairness-accuracy trade-offs using classifier-based conditional mutual\ninformation estimation. We demonstrate the effectiveness of FRIED on datasets\nof different modalities - tabular, text, and image datasets. We observe that\nthe representations learned by FRIED are overall fairer in comparison to\nexisting baselines and also accurate for downstream prediction tasks.\nAdditionally, we evaluate FRIED on a real-world healthcare claims dataset where\nwe conduct an expert aided model auditing study providing useful insights into\nopioid ad-diction patterns.",
          "link": "http://arxiv.org/abs/2108.00295",
          "publishedOn": "2021-08-03T02:06:33.416Z",
          "wordCount": 657,
          "title": "Fair Representation Learning using Interpolation Enabled Disentanglement. (arXiv:2108.00295v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00099",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tazarv_A/0/1/0/all/0/1\">Ali Tazarv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levorato_M/0/1/0/all/0/1\">Marco Levorato</a>",
          "description": "Blood Pressure (BP) is one of the four primary vital signs indicating the\nstatus of the body's vital (life-sustaining) functions. BP is difficult to\ncontinuously monitor using a sphygmomanometer (i.e. a blood pressure cuff),\nespecially in everyday-setting. However, other health signals which can be\neasily and continuously acquired, such as photoplethysmography (PPG), show some\nsimilarities with the Aortic Pressure waveform. Based on these similarities, in\nrecent years several methods were proposed to predict BP from the PPG signal.\nBuilding on these results, we propose an advanced personalized data-driven\napproach that uses a three-layer deep neural network to estimate BP based on\nPPG signals. Different from previous work, the proposed model analyzes the PPG\nsignal in time-domain and automatically extracts the most critical features for\nthis specific application, then uses a variation of recurrent neural networks\ncalled Long-Short-Term-Memory (LSTM) to map the extracted features to the BP\nvalue associated with that time window. Experimental results on two separate\nstandard hospital datasets, yielded absolute errors mean and absolute error\nstandard deviation for systolic and diastolic BP values outperforming prior\nworks.",
          "link": "http://arxiv.org/abs/2108.00099",
          "publishedOn": "2021-08-03T02:06:33.398Z",
          "wordCount": 619,
          "title": "A Deep Learning Approach to Predict Blood Pressure from PPG Signals. (arXiv:2108.00099v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin-Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian-Xun Wang</a>",
          "description": "Model-based reinforcement learning (MBRL) is believed to have much higher\nsample efficiency compared to model-free algorithms by learning a predictive\nmodel of the environment. However, the performance of MBRL highly relies on the\nquality of the learned model, which is usually built in a black-box manner and\nmay have poor predictive accuracy outside of the data distribution. The\ndeficiencies of the learned model may prevent the policy from being fully\noptimized. Although some uncertainty analysis-based remedies have been proposed\nto alleviate this issue, model bias still poses a great challenge for MBRL. In\nthis work, we propose to leverage the prior knowledge of underlying physics of\nthe environment, where the governing laws are (partially) known. In particular,\nwe developed a physics-informed MBRL framework, where governing equations and\nphysical constraints are utilized to inform the model learning and policy\nsearch. By incorporating the prior information of the environment, the quality\nof the learned model can be notably improved, while the required interactions\nwith the environment are significantly reduced, leading to better sample\nefficiency and learning performance. The effectiveness and merit have been\ndemonstrated over a handful of classic control problems, where the environments\nare governed by canonical ordinary/partial differential equations.",
          "link": "http://arxiv.org/abs/2108.00128",
          "publishedOn": "2021-08-03T02:06:33.392Z",
          "wordCount": 637,
          "title": "Physics-informed Dyna-Style Model-Based Deep Reinforcement Learning for Dynamic Control. (arXiv:2108.00128v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dirksen_S/0/1/0/all/0/1\">Sjoerd Dirksen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genzel_M/0/1/0/all/0/1\">Martin Genzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacques_L/0/1/0/all/0/1\">Laurent Jacques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stollenwerk_A/0/1/0/all/0/1\">Alexander Stollenwerk</a>",
          "description": "Neural networks with random weights appear in a variety of machine learning\napplications, most prominently as the initialization of many deep learning\nalgorithms and as a computationally cheap alternative to fully learned neural\nnetworks. In the present article we enhance the theoretical understanding of\nrandom neural nets by addressing the following data separation problem: under\nwhat conditions can a random neural network make two classes $\\mathcal{X}^-,\n\\mathcal{X}^+ \\subset \\mathbb{R}^d$ (with positive distance) linearly\nseparable? We show that a sufficiently large two-layer ReLU-network with\nstandard Gaussian weights and uniformly distributed biases can solve this\nproblem with high probability. Crucially, the number of required neurons is\nexplicitly linked to geometric properties of the underlying sets\n$\\mathcal{X}^-, \\mathcal{X}^+$ and their mutual arrangement. This\ninstance-specific viewpoint allows us to overcome the usual curse of\ndimensionality (exponential width of the layers) in non-pathological situations\nwhere the data carries low-complexity structure. We quantify the relevant\nstructure of the data in terms of a novel notion of mutual complexity (based on\na localized version of Gaussian mean width), which leads to sound and\ninformative separation guarantees. We connect our result with related lines of\nwork on approximation, memorization, and generalization.",
          "link": "http://arxiv.org/abs/2108.00207",
          "publishedOn": "2021-08-03T02:06:33.385Z",
          "wordCount": 627,
          "title": "The Separation Capacity of Random Neural Networks. (arXiv:2108.00207v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schuster_D/0/1/0/all/0/1\">Daniel Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelst_S/0/1/0/all/0/1\">Sebastiaan J. van Zelst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aalst_W/0/1/0/all/0/1\">Wil M. P. van der Aalst</a>",
          "description": "Process discovery aims to learn a process model from observed process\nbehavior. From a user's perspective, most discovery algorithms work like a\nblack box. Besides parameter tuning, there is no interaction between the user\nand the algorithm. Interactive process discovery allows the user to exploit\ndomain knowledge and to guide the discovery process. Previously, an incremental\ndiscovery approach has been introduced where a model, considered to be under\nconstruction, gets incrementally extended by user-selected process behavior.\nThis paper introduces a novel approach that additionally allows the user to\nfreeze model parts within the model under construction. Frozen sub-models are\nnot altered by the incremental approach when new behavior is added to the\nmodel. The user can thus steer the discovery algorithm. Our experiments show\nthat freezing sub-models can lead to higher quality models.",
          "link": "http://arxiv.org/abs/2108.00215",
          "publishedOn": "2021-08-03T02:06:33.379Z",
          "wordCount": 600,
          "title": "Freezing Sub-Models During Incremental Process Discovery: Extended Version. (arXiv:2108.00215v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>",
          "description": "Transformers have made much progress in dealing with visual tasks. However,\nexisting vision transformers still do not possess an ability that is important\nto visual input: building the attention among features of different scales. The\nreasons for this problem are two-fold: (1) Input embeddings of each layer are\nequal-scale without cross-scale features; (2) Some vision transformers\nsacrifice the small-scale features of embeddings to lower the cost of the\nself-attention module. To make up this defect, we propose Cross-scale Embedding\nLayer (CEL) and Long Short Distance Attention (LSDA). In particular, CEL blends\neach embedding with multiple patches of different scales, providing the model\nwith cross-scale embeddings. LSDA splits the self-attention module into a\nshort-distance and long-distance one, also lowering the cost but keeping both\nsmall-scale and large-scale features in embeddings. Through these two designs,\nwe achieve cross-scale attention. Besides, we propose dynamic position bias for\nvision transformers to make the popular relative position bias apply to\nvariable-sized images. Based on these proposed modules, we construct our vision\narchitecture called CrossFormer. Experiments show that CrossFormer outperforms\nother transformers on several representative visual tasks, especially object\ndetection and segmentation. The code has been released:\nhttps://github.com/cheerss/CrossFormer.",
          "link": "http://arxiv.org/abs/2108.00154",
          "publishedOn": "2021-08-03T02:06:33.358Z",
          "wordCount": 648,
          "title": "CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention. (arXiv:2108.00154v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kallitsis_M/0/1/0/all/0/1\">Michalis Kallitsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honavar_V/0/1/0/all/0/1\">Vasant Honavar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prajapati_R/0/1/0/all/0/1\">Rupesh Prajapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dinghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_J/0/1/0/all/0/1\">John Yen</a>",
          "description": "Network telescopes or \"Darknets\" provide a unique window into Internet-wide\nmalicious activities associated with malware propagation, denial of service\nattacks, scanning performed for network reconnaissance, and others. Analyses of\nthe resulting data can provide actionable insights to security analysts that\ncan be used to prevent or mitigate cyber-threats. Large Darknets, however,\nobserve millions of nefarious events on a daily basis which makes the\ntransformation of the captured information into meaningful insights\nchallenging. We present a novel framework for characterizing Darknet behavior\nand its temporal evolution aiming to address this challenge. The proposed\nframework: (i) Extracts a high dimensional representation of Darknet events\ncomposed of features distilled from Darknet data and other external sources;\n(ii) Learns, in an unsupervised fashion, an information-preserving\nlow-dimensional representation of these events (using deep representation\nlearning) that is amenable to clustering; (iv) Performs clustering of the\nscanner data in the resulting representation space and provides interpretable\ninsights using optimal decision trees; and (v) Utilizes the clustering outcomes\nas \"signatures\" that can be used to detect structural changes in the Darknet\nactivities. We evaluate the proposed system on a large operational Network\nTelescope and demonstrate its ability to detect real-world, high-impact\ncybersecurity incidents.",
          "link": "http://arxiv.org/abs/2108.00079",
          "publishedOn": "2021-08-03T02:06:33.351Z",
          "wordCount": 653,
          "title": "Zooming Into the Darknet: Characterizing Internet Background Radiation and its Structural Changes. (arXiv:2108.00079v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00230",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Sentenac_F/0/1/0/all/0/1\">Flore Sentenac</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yi_J/0/1/0/all/0/1\">Jialin Yi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Calauzenes_C/0/1/0/all/0/1\">Cl&#xe9;ment Calauz&#xe8;nes</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Perchet_V/0/1/0/all/0/1\">Vianney Perchet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vojnovic_M/0/1/0/all/0/1\">Milan Vojnovic</a>",
          "description": "Finding an optimal matching in a weighted graph is a standard combinatorial\nproblem. We consider its semi-bandit version where either a pair or a full\nmatching is sampled sequentially. We prove that it is possible to leverage a\nrank-1 assumption on the adjacency matrix to reduce the sample complexity and\nthe regret of off-the-shelf algorithms up to reaching a linear dependency in\nthe number of vertices (up to poly log terms).",
          "link": "http://arxiv.org/abs/2108.00230",
          "publishedOn": "2021-08-03T02:06:33.344Z",
          "wordCount": 521,
          "title": "Pure Exploration and Regret Minimization in Matching Bandits. (arXiv:2108.00230v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_D/0/1/0/all/0/1\">Deming Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>",
          "description": "Learning with noisy labels is an important and challenging task for training\naccurate deep neural networks. Some commonly-used loss functions, such as Cross\nEntropy (CE), suffer from severe overfitting to noisy labels. Robust loss\nfunctions that satisfy the symmetric condition were tailored to remedy this\nproblem, which however encounter the underfitting effect. In this paper, we\ntheoretically prove that \\textbf{any loss can be made robust to noisy labels}\nby restricting the network output to the set of permutations over a fixed\nvector. When the fixed vector is one-hot, we only need to constrain the output\nto be one-hot, which however produces zero gradients almost everywhere and thus\nmakes gradient-based optimization difficult. In this work, we introduce the\nsparse regularization strategy to approximate the one-hot constraint, which is\ncomposed of network output sharpening operation that enforces the output\ndistribution of a network to be sharp and the $\\ell_p$-norm ($p\\le 1$)\nregularization that promotes the network output to be sparse. This simple\napproach guarantees the robustness of arbitrary loss functions while not\nhindering the fitting ability. Experimental results demonstrate that our method\ncan significantly improve the performance of commonly-used loss functions in\nthe presence of noisy labels and class imbalance, and outperform the\nstate-of-the-art methods. The code is available at\nhttps://github.com/hitcszx/lnl_sr.",
          "link": "http://arxiv.org/abs/2108.00192",
          "publishedOn": "2021-08-03T02:06:33.321Z",
          "wordCount": 657,
          "title": "Learning with Noisy Labels via Sparse Regularization. (arXiv:2108.00192v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karandikar_A/0/1/0/all/0/1\">Archit Karandikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cain_N/0/1/0/all/0/1\">Nicholas Cain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Dustin Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1\">Jonathon Shlens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1\">Michael C. Mozer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_B/0/1/0/all/0/1\">Becca Roelofs</a>",
          "description": "Optimal decision making requires that classifiers produce uncertainty\nestimates consistent with their empirical accuracy. However, deep neural\nnetworks are often under- or over-confident in their predictions. Consequently,\nmethods have been developed to improve the calibration of their predictive\nuncertainty both during training and post-hoc. In this work, we propose\ndifferentiable losses to improve calibration based on a soft (continuous)\nversion of the binning operation underlying popular calibration-error\nestimators. When incorporated into training, these soft calibration losses\nachieve state-of-the-art single-model ECE across multiple datasets with less\nthan 1% decrease in accuracy. For instance, we observe an 82% reduction in ECE\n(70% relative to the post-hoc rescaled ECE) in exchange for a 0.7% relative\ndecrease in accuracy relative to the cross entropy baseline on CIFAR-100. When\nincorporated post-training, the soft-binning-based calibration error objective\nimproves upon temperature scaling, a popular recalibration method. Overall,\nexperiments across losses and datasets demonstrate that using\ncalibration-sensitive procedures yield better uncertainty estimates under\ndataset shift than the standard practice of using a cross entropy loss and\npost-hoc recalibration methods.",
          "link": "http://arxiv.org/abs/2108.00106",
          "publishedOn": "2021-08-03T02:06:33.296Z",
          "wordCount": 630,
          "title": "Soft Calibration Objectives for Neural Networks. (arXiv:2108.00106v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parchami_M/0/1/0/all/0/1\">Mostafa Parchami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayed_S/0/1/0/all/0/1\">Saif Iftekar Sayed</a>",
          "description": "Feature tracking is the building block of many applications such as visual\nodometry, augmented reality, and target tracking. Unfortunately, the\nstate-of-the-art vision-based tracking algorithms fail in surgical images due\nto the challenges imposed by the nature of such environments. In this paper, we\nproposed a novel and unified deep learning-based approach that can learn how to\ntrack features reliably as well as learn how to detect such reliable features\nfor tracking purposes. The proposed network dubbed as Deep-PT, consists of a\ntracker network which is a convolutional neural network simulating\ncross-correlation in terms of deep learning and two fully connected networks\nthat operate on the output of intermediate layers of the tracker to detect\nfeatures and predict trackability of the detected points. The ability to detect\nfeatures based on the capabilities of the tracker distinguishes the proposed\nmethod from previous algorithms used in this area and improves the robustness\nof the algorithms against dynamics of the scene. The network is trained using\nmultiple datasets due to the lack of specialized dataset for feature tracking\ndatasets and extensive comparisons are conducted to compare the accuracy of\nDeep-PT against recent pixel tracking algorithms. As the experiments suggest,\nthe proposed deep architecture deliberately learns what to track and how to\ntrack and outperforms the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.00105",
          "publishedOn": "2021-08-03T02:06:33.289Z",
          "wordCount": 658,
          "title": "Deep Feature Tracker: A Novel Application for Deep Convolutional Neural Networks. (arXiv:2108.00105v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Llorente_F/0/1/0/all/0/1\">F. Llorente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_L/0/1/0/all/0/1\">L. Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1\">J. Read</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delgado_D/0/1/0/all/0/1\">D. Delgado</a>",
          "description": "This survey gives an overview of Monte Carlo methodologies using surrogate\nmodels, for dealing with densities which are intractable, costly, and/or noisy.\nThis type of problem can be found in numerous real-world scenarios, including\nstochastic optimization and reinforcement learning, where each evaluation of a\ndensity function may incur some computationally-expensive or even physical\n(real-world activity) cost, likely to give different results each time. The\nsurrogate model does not incur this cost, but there are important trade-offs\nand considerations involved in the choice and design of such methodologies. We\nclassify the different methodologies into three main classes and describe\nspecific instances of algorithms under a unified notation. A modular scheme\nwhich encompasses the considered methods is also presented. A range of\napplication scenarios is discussed, with special attention to the\nlikelihood-free setting and reinforcement learning. Several numerical\ncomparisons are also provided.",
          "link": "http://arxiv.org/abs/2108.00490",
          "publishedOn": "2021-08-03T02:06:32.927Z",
          "wordCount": 595,
          "title": "A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning. (arXiv:2108.00490v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00664",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Storchan_V/0/1/0/all/0/1\">Victor Storchan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyetrenko_S/0/1/0/all/0/1\">Svitlana Vyetrenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balch_T/0/1/0/all/0/1\">Tucker Balch</a>",
          "description": "In electronic trading markets often only the price or volume time series,\nthat result from interaction of multiple market participants, are directly\nobservable. In order to test trading strategies before deploying them to\nreal-time trading, multi-agent market environments calibrated so that the time\nseries that result from interaction of simulated agents resemble historical are\noften used. To ensure adequate testing, one must test trading strategies in a\nvariety of market scenarios -- which includes both scenarios that represent\nordinary market days as well as stressed markets (most recently observed due to\nthe beginning of COVID pandemic). In this paper, we address the problem of\nmulti-agent simulator parameter calibration to allow simulator capture\ncharacteristics of different market regimes. We propose a novel two-step method\nto train a discriminator that is able to distinguish between \"real\" and \"fake\"\nprice and volume time series as a part of GAN with self-attention, and then\nutilize it within an optimization framework to tune parameters of a simulator\nmodel with known agent archetypes to represent a market scenario. We conclude\nwith experimental results that demonstrate effectiveness of our method.",
          "link": "http://arxiv.org/abs/2108.00664",
          "publishedOn": "2021-08-03T02:06:32.921Z",
          "wordCount": 642,
          "title": "Learning who is in the market from time series: market participant discovery through adversarial calibration of multi-agent simulators. (arXiv:2108.00664v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00413",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Zheng_C/0/1/0/all/0/1\">Candi Zheng</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chen_S/0/1/0/all/0/1\">Shiyi Chen</a>",
          "description": "Macroscopic modeling of the gas dynamics across Knudsen numbers from dense\ngas region to rarefied gas region remains a great challenge. The reason is\nmacroscopic models lack accurate constitutive relations valid across different\nKnudsen numbers. To address this problem, we proposed a Data-driven, KnUdsen\nnumber Adaptive Linear constitutive relation model named DUAL. The DUAL model\nis accurate across a range of Knudsen numbers, from dense to rarefied, through\nlearning to adapt Knudsen number change from observed data. It is consistent\nwith the Navier-Stokes equation under the hydrodynamic limit, by utilizing a\nconstrained neural network. In addition, it naturally satisfies the second law\nof thermodynamics and is robust to noisy data. We test the DUAL model on the\ncalculation of Rayleigh scattering spectra. The DUAL model gives accurate\nspectra for various Knudsen numbers and is superior to traditional perturbation\nand moment expansion methods.",
          "link": "http://arxiv.org/abs/2108.00413",
          "publishedOn": "2021-08-03T02:06:32.916Z",
          "wordCount": 595,
          "title": "Data Driven Macroscopic Modeling across Knudsen Numbers for Rarefied Gas Dynamics and Application to Rayleigh Scattering. (arXiv:2108.00413v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Theis_J/0/1/0/all/0/1\">Julian Theis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darabi_H/0/1/0/all/0/1\">Houshang Darabi</a>",
          "description": "Decay Replay Mining is a deep learning method that utilizes process model\nnotations to predict the next event. However, this method does not intertwine\nthe neural network with the structure of the process model to its full extent.\nThis paper proposes an approach to further interlock the process model of Decay\nReplay Mining with its neural network for next event prediction. The approach\nuses a masking layer which is initialized based on the reachability graph of\nthe process model. Additionally, modifications to the neural network\narchitecture are proposed to increase the predictive performance. Experimental\nresults demonstrate the value of the approach and underscore the importance of\ndiscovering precise and generalized process models.",
          "link": "http://arxiv.org/abs/2108.00404",
          "publishedOn": "2021-08-03T02:06:32.910Z",
          "wordCount": 542,
          "title": "Masking Neural Networks Using Reachability Graphs to Predict Process Events. (arXiv:2108.00404v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00367",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+S_A/0/1/0/all/0/1\">Anu T S</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raveendran_T/0/1/0/all/0/1\">Tara Raveendran</a>",
          "description": "Non-Orthogonal Multiple Access (NOMA) schemes are being actively explored to\naddress some of the major challenges in 5th Generation (5G) Wireless\ncommunications. Channel estimation is exceptionally challenging in scenarios\nwhere NOMA schemes are integrated with millimeter wave (mmWave) massive\nmultiple-input multiple-output (MIMO) systems. An accurate estimation of the\nchannel is essential in exploiting the benefits of the pairing of the duo-NOMA\nand mmWave. This paper proposes a convolutional neural network (CNN) based\napproach to estimate the channel for NOMA based millimeter wave (mmWave)\nmassive multiple-input multiple-output (MIMO) systems built on a hybrid\narchitecture. Initially, users are grouped into different clusters based on\ntheir channel gains and beamforming technique is performed to maximize the\nsignal in the direction of desired cluster. A coarse estimation of the channel\nis first made from the received signal and this estimate is given as the input\nto CNN to fine estimate the channel coefficients. Numerical illustrations show\nthat the proposed method outperforms least square (LS) estimate, minimum mean\nsquare error (MMSE) estimate and are close to the Cramer-Rao Bound (CRB).",
          "link": "http://arxiv.org/abs/2108.00367",
          "publishedOn": "2021-08-03T02:06:32.896Z",
          "wordCount": 616,
          "title": "CNN based Channel Estimation using NOMA for mmWave Massive MIMO System. (arXiv:2108.00367v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00480",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Rahimikia_E/0/1/0/all/0/1\">Eghbal Rahimikia</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zohren_S/0/1/0/all/0/1\">Stefan Zohren</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Poon_S/0/1/0/all/0/1\">Ser-Huang Poon</a>",
          "description": "We develop FinText, a novel, state-of-the-art, financial word embedding from\nDow Jones Newswires Text News Feed Database. Incorporating this word embedding\nin a machine learning model produces a substantial increase in volatility\nforecasting performance on days with volatility jumps for 23 NASDAQ stocks from\n27 July 2007 to 18 November 2016. A simple ensemble model, combining our word\nembedding and another machine learning model that uses limit order book data,\nprovides the best forecasting performance for both normal and jump volatility\ndays. Finally, we use Integrated Gradients and SHAP (SHapley Additive\nexPlanations) to make the results more 'explainable' and the model comparisons\nmore transparent.",
          "link": "http://arxiv.org/abs/2108.00480",
          "publishedOn": "2021-08-03T02:06:32.875Z",
          "wordCount": 549,
          "title": "Realised Volatility Forecasting: Machine Learning via Financial Word Embedding. (arXiv:2108.00480v1 [q-fin.CP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00599",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yan_R/0/1/0/all/0/1\">Rong Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuxuan Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoyu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geng_G/0/1/0/all/0/1\">Guangchao Geng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_Q/0/1/0/all/0/1\">Quanyuan Jiang</a>",
          "description": "Real active distribution networks with associated smart meter (SM) data are\ncritical for power researchers. However, it is practically difficult for\nresearchers to obtain such comprehensive datasets from utilities due to privacy\nconcerns. To bridge this gap, an implicit generative model with Wasserstein GAN\nobjectives, namely unbalanced graph generative adversarial network (UG-GAN), is\ndesigned to generate synthetic three-phase unbalanced active distribution\nsystem connectivity. The basic idea is to learn the distribution of random\nwalks both over a real-world system and across each phase of line segments,\ncapturing the underlying local properties of an individual real-world\ndistribution network and generating specific synthetic networks accordingly.\nThen, to create a comprehensive synthetic test case, a network correction and\nextension process is proposed to obtain time-series nodal demands and standard\ndistribution grid components with realistic parameters, including distributed\nenergy resources (DERs) and capacity banks. A Midwest distribution system with\n1-year SM data has been utilized to validate the performance of our method.\nCase studies with several power applications demonstrate that synthetic active\nnetworks generated by the proposed framework can mimic almost all features of\nreal-world networks while avoiding the disclosure of confidential information.",
          "link": "http://arxiv.org/abs/2108.00599",
          "publishedOn": "2021-08-03T02:06:32.865Z",
          "wordCount": 644,
          "title": "Synthetic Active Distribution System Generation via Unbalanced Graph Generative Adversarial Network. (arXiv:2108.00599v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anshul Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahiya_K/0/1/0/all/0/1\">Kunal Dahiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sheshansh Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saini_D/0/1/0/all/0/1\">Deepak Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sumeet Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1\">Purushottam Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Manik Varma</a>",
          "description": "Extreme multi-label classification (XML) involves tagging a data point with\nits most relevant subset of labels from an extremely large label set, with\nseveral applications such as product-to-product recommendation with millions of\nproducts. Although leading XML algorithms scale to millions of labels, they\nlargely ignore label meta-data such as textual descriptions of the labels. On\nthe other hand, classical techniques that can utilize label metadata via\nrepresentation learning using deep networks struggle in extreme settings. This\npaper develops the DECAF algorithm that addresses these challenges by learning\nmodels enriched by label metadata that jointly learn model parameters and\nfeature representations using deep networks and offer accurate classification\nat the scale of millions of labels. DECAF makes specific contributions to model\narchitecture design, initialization, and training, enabling it to offer up to\n2-6% more accurate prediction than leading extreme classifiers on publicly\navailable benchmark product-to-product recommendation datasets, such as\nLF-AmazonTitles-1.3M. At the same time, DECAF was found to be up to 22x faster\nat inference than leading deep extreme classifiers, which makes it suitable for\nreal-time applications that require predictions within a few milliseconds. The\ncode for DECAF is available at the following URL\nhttps://github.com/Extreme-classification/DECAF.",
          "link": "http://arxiv.org/abs/2108.00368",
          "publishedOn": "2021-08-03T02:06:32.859Z",
          "wordCount": 654,
          "title": "DECAF: Deep Extreme Classification with Label Features. (arXiv:2108.00368v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duplyakov_V/0/1/0/all/0/1\">Viktor Duplyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morozov_A/0/1/0/all/0/1\">Anton Morozov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popkov_D/0/1/0/all/0/1\">Dmitriy Popkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shel_E/0/1/0/all/0/1\">Egor Shel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainshtein_A/0/1/0/all/0/1\">Albert Vainshtein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osiptsov_A/0/1/0/all/0/1\">Andrei Osiptsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paderin_G/0/1/0/all/0/1\">Grigory Paderin</a>",
          "description": "We describe a stacked model for predicting the cumulative fluid production\nfor an oil well with a multistage-fracture completion based on a combination of\nRidge Regression and CatBoost algorithms. The model is developed based on an\nextended digital field data base of reservoir, well and fracturing design\nparameters. The database now includes more than 5000 wells from 23 oilfields of\nWestern Siberia (Russia), with 6687 fracturing operations in total. Starting\nwith 387 parameters characterizing each well, including construction, reservoir\nproperties, fracturing design features and production, we end up with 38 key\nparameters used as input features for each well in the model training process.\nThe model demonstrates physically explainable dependencies plots of the target\non the design parameters (number of stages, proppant mass, average and final\nproppant concentrations and fluid rate). We developed a set of methods\nincluding those based on the use of Euclidean distance and clustering\ntechniques to perform similar (offset) wells search, which is useful for a\nfield engineer to analyze earlier fracturing treatments on similar wells. These\napproaches are also adapted for obtaining the optimization parameters\nboundaries for the particular pilot well, as part of the field testing campaign\nof the methodology. An inverse problem (selecting an optimum set of fracturing\ndesign parameters to maximize production) is formulated as optimizing a high\ndimensional black box approximation function constrained by boundaries and\nsolved with four different optimization methods: surrogate-based optimization,\nsequential least squares programming, particle swarm optimization and\ndifferential evolution. A recommendation system containing all the above\nmethods is designed to advise a production stimulation engineer on an optimized\nfracturing design.",
          "link": "http://arxiv.org/abs/2108.00751",
          "publishedOn": "2021-08-03T02:06:32.852Z",
          "wordCount": 729,
          "title": "Data-driven model for hydraulic fracturing design optimization. Part II: Inverse problem. (arXiv:2108.00751v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhoubo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Puqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raveaux_R/0/1/0/all/0/1\">Romain Raveaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huadong Liu</a>",
          "description": "Graph matching is an important problem that has received widespread\nattention, especially in the field of computer vision. Recently,\nstate-of-the-art methods seek to incorporate graph matching with deep learning.\nHowever, there is no research to explain what role the graph matching algorithm\nplays in the model. Therefore, we propose an approach integrating a MILP\nformulation of the graph matching problem. This formulation is solved to\noptimal and it provides inherent baseline. Meanwhile, similar approaches are\nderived by releasing the optimal guarantee of the graph matching solver and by\nintroducing a quality level. This quality level controls the quality of the\nsolutions provided by the graph matching solver. In addition, several\nrelaxations of the graph matching problem are put to the test. Our experimental\nevaluation gives several theoretical insights and guides the direction of deep\ngraph matching methods.",
          "link": "http://arxiv.org/abs/2108.00394",
          "publishedOn": "2021-08-03T02:06:32.836Z",
          "wordCount": 604,
          "title": "Deep graph matching meets mixed-integer linear programming: Relax at your own risk ?. (arXiv:2108.00394v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bashizade_R/0/1/0/all/0/1\">Ramin Bashizade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Sayan Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lebeck_A/0/1/0/all/0/1\">Alvin R. Lebeck</a>",
          "description": "Statistical machine learning has widespread application in various domains.\nThese methods include probabilistic algorithms, such as Markov Chain\nMonte-Carlo (MCMC), which rely on generating random numbers from probability\ndistributions. These algorithms are computationally expensive on conventional\nprocessors, yet their statistical properties, namely interpretability and\nuncertainty quantification (UQ) compared to deep learning, make them an\nattractive alternative approach. Therefore, hardware specialization can be\nadopted to address the shortcomings of conventional processors in running these\napplications.\n\nIn this paper, we propose a high-throughput accelerator for Markov Random\nField (MRF) inference, a powerful model for representing a wide range of\napplications, using MCMC with Gibbs sampling. We propose a tiled architecture\nwhich takes advantage of near-memory computing, and memory optimizations\ntailored to the semantics of MRF. Additionally, we propose a novel hybrid\non-chip/off-chip memory system and logging scheme to efficiently support UQ.\nThis memory system design is not specific to MRF models and is applicable to\napplications using probabilistic algorithms. In addition, it dramatically\nreduces off-chip memory bandwidth requirements.\n\nWe implemented an FPGA prototype of our proposed architecture using\nhigh-level synthesis tools and achieved 146MHz frequency for an accelerator\nwith 32 function units on an Intel Arria 10 FPGA. Compared to prior work on\nFPGA, our accelerator achieves 26X speedup. Furthermore, our proposed memory\nsystem and logging scheme to support UQ reduces off-chip bandwidth by 71% for\ntwo applications. ASIC analysis in 15nm shows our design with 2048 function\nunits running at 3GHz outperforms GPU implementations of motion estimation and\nstereo vision on Nvidia RTX2080Ti by 120X-210X, occupying only 7.7% of the\narea.",
          "link": "http://arxiv.org/abs/2108.00570",
          "publishedOn": "2021-08-03T02:06:32.830Z",
          "wordCount": 695,
          "title": "Accelerating Markov Random Field Inference with Uncertainty Quantification. (arXiv:2108.00570v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Choubo Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>",
          "description": "Existing anomaly detection paradigms overwhelmingly focus on training\ndetection models using exclusively normal data or unlabeled data (mostly normal\nsamples). One notorious issue with these approaches is that they are weak in\ndiscriminating anomalies from normal samples due to the lack of the knowledge\nabout the anomalies. Here, we study the problem of few-shot anomaly detection,\nin which we aim at using a few labeled anomaly examples to train\nsample-efficient discriminative detection models. To address this problem, we\nintroduce a novel weakly-supervised anomaly detection framework to train\ndetection models without assuming the examples illustrating all possible\nclasses of anomaly.\n\nSpecifically, the proposed approach learns discriminative normality\n(regularity) by leveraging the labeled anomalies and a prior probability to\nenforce expressive representations of normality and unbounded deviated\nrepresentations of abnormality. This is achieved by an end-to-end optimization\nof anomaly scores with a neural deviation learning, in which the anomaly scores\nof normal samples are imposed to approximate scalar scores drawn from the prior\nwhile that of anomaly examples is enforced to have statistically significant\ndeviations from these sampled scores in the upper tail. Furthermore, our model\nis optimized to learn fine-grained normality and abnormality by top-K\nmultiple-instance-learning-based feature subspace deviation learning, allowing\nmore generalized representations. Comprehensive experiments on nine real-world\nimage anomaly detection benchmarks show that our model is substantially more\nsample-efficient and robust, and performs significantly better than\nstate-of-the-art competing methods in both closed-set and open-set settings.\nOur model can also offer explanation capability as a result of its prior-driven\nanomaly score learning. Code and datasets are available at:\nhttps://git.io/DevNet.",
          "link": "http://arxiv.org/abs/2108.00462",
          "publishedOn": "2021-08-03T02:06:32.820Z",
          "wordCount": 714,
          "title": "Explainable Deep Few-shot Anomaly Detection with Deviation Networks. (arXiv:2108.00462v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jackie Shen</a>",
          "description": "The bucketed PCA neural network (PCA-NN) with transforms is developed here in\nan effort to benchmark deep neural networks (DNN's), for problems on supervised\nclassification. Most classical PCA models apply PCA to the entire training data\nset to establish a reductive representation and then employ non-network tools\nsuch as high-order polynomial classifiers. In contrast, the bucketed PCA-NN\napplies PCA to individual buckets which are constructed in two consecutive\nphases, as well as retains a genuine architecture of a neural network. This\nfacilitates a fair apple-to-apple comparison to DNN's, esp. to reveal that a\nmajor chunk of accuracy achieved by many impressive DNN's could possibly be\nexplained by the bucketed PCA-NN (e.g., 96% out of 98% for the MNIST data set\nas an example). Compared with most DNN's, the three building blocks of the\nbucketed PCA-NN are easier to comprehend conceptually - PCA, transforms, and\nbucketing for error correction. Furthermore, unlike the somewhat quasi-random\nneurons ubiquitously observed in DNN's, the PCA neurons resemble or mirror the\ninput signals and are more straightforward to decipher as a result.",
          "link": "http://arxiv.org/abs/2108.00605",
          "publishedOn": "2021-08-03T02:06:32.814Z",
          "wordCount": 619,
          "title": "Bucketed PCA Neural Networks with Neurons Mirroring Signals. (arXiv:2108.00605v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jinyuan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yupei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>",
          "description": "Self-supervised learning in computer vision aims to pre-train an image\nencoder using a large amount of unlabeled images or (image, text) pairs. The\npre-trained image encoder can then be used as a feature extractor to build\ndownstream classifiers for many downstream tasks with a small amount of or no\nlabeled training data. In this work, we propose BadEncoder, the first backdoor\nattack to self-supervised learning. In particular, our BadEncoder injects\nbackdoors into a pre-trained image encoder such that the downstream classifiers\nbuilt based on the backdoored image encoder for different downstream tasks\nsimultaneously inherit the backdoor behavior. We formulate our BadEncoder as an\noptimization problem and we propose a gradient descent based method to solve\nit, which produces a backdoored image encoder from a clean one. Our extensive\nempirical evaluation results on multiple datasets show that our BadEncoder\nachieves high attack success rates while preserving the accuracy of the\ndownstream classifiers. We also show the effectiveness of BadEncoder using two\npublicly available, real-world image encoders, i.e., Google's image encoder\npre-trained on ImageNet and OpenAI's Contrastive Language-Image Pre-training\n(CLIP) image encoder pre-trained on 400 million (image, text) pairs collected\nfrom the Internet. Moreover, we consider defenses including Neural Cleanse and\nMNTD (empirical defenses) as well as PatchGuard (a provable defense). Our\nresults show that these defenses are insufficient to defend against BadEncoder,\nhighlighting the needs for new defenses against our BadEncoder. Our code is\npublicly available at: https://github.com/jjy1994/BadEncoder.",
          "link": "http://arxiv.org/abs/2108.00352",
          "publishedOn": "2021-08-03T02:06:32.809Z",
          "wordCount": 693,
          "title": "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning. (arXiv:2108.00352v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanzhina_N/0/1/0/all/0/1\">Natalia Khanzhina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapenok_A/0/1/0/all/0/1\">Alexey Lapenok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1\">Andrey Filchenkov</a>",
          "description": "According to recent studies, commonly used computer vision datasets contain\nabout 4% of label errors. For example, the COCO dataset is known for its high\nlevel of noise in data labels, which limits its use for training robust neural\ndeep architectures in a real-world scenario. To model such a noise, in this\npaper we have proposed the homoscedastic aleatoric uncertainty estimation, and\npresent a series of novel loss functions to address the problem of image object\ndetection at scale. Specifically, the proposed functions are based on Bayesian\ninference and we have incorporated them into the common community-adopted\nobject detection deep learning architecture RetinaNet. We have also shown that\nmodeling of homoscedastic aleatoric uncertainty using our novel functions\nallows to increase the model interpretability and to improve the object\ndetection performance being evaluated on the COCO dataset.",
          "link": "http://arxiv.org/abs/2108.00784",
          "publishedOn": "2021-08-03T02:06:32.784Z",
          "wordCount": 587,
          "title": "Towards Robust Object Detection: Bayesian RetinaNet for Homoscedastic Aleatoric Uncertainty Modeling. (arXiv:2108.00784v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00559",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Pinchuk_P/0/1/0/all/0/1\">Pavlo Pinchuk</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Margot_J/0/1/0/all/0/1\">Jean-Luc Margot</a>",
          "description": "Radio frequency interference (RFI) mitigation remains a major challenge in\nthe search for radio technosignatures. Typical mitigation strategies include a\ndirection-of-origin (DoO) filter, where a signal is classified as RFI if it is\ndetected in multiple directions on the sky. These classifications generally\nrely on estimates of signal properties, such as frequency and frequency drift\nrate. Convolutional neural networks (CNNs) offer a promising complement to\nexisting filters because they can be trained to analyze dynamic spectra\ndirectly, instead of relying on inferred signal properties. In this work, we\ncompiled several data sets consisting of labeled pairs of images of dynamic\nspectra, and we designed and trained a CNN that can determine whether or not a\nsignal detected in one scan is also present in another scan. This CNN-based DoO\nfilter outperforms both a baseline 2D correlation model as well as existing DoO\nfilters over a range of metrics, with precision and recall values of 99.15% and\n97.81%, respectively. We found that the CNN reduces the number of signals\nrequiring visual inspection after the application of traditional DoO filters by\na factor of 6-16 in nominal situations.",
          "link": "http://arxiv.org/abs/2108.00559",
          "publishedOn": "2021-08-03T02:06:32.778Z",
          "wordCount": 662,
          "title": "A Machine-Learning-Based Direction-of-Origin Filter for the Identification of Radio Frequency Interference in the Search for Technosignatures. (arXiv:2108.00559v1 [astro-ph.IM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhixiong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seongjin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1\">Hwasoo Yeo</a>",
          "description": "In many trajectory-based applications, it is necessary to map raw GPS\ntrajectories onto road networks in digital maps, which is commonly referred to\nas a map-matching process. While most previous map-matching methods have\nfocused on using rule-based algorithms to deal with the map-matching problems,\nin this paper, we consider the map-matching task from the data perspective,\nproposing a deep learning-based map-matching model. We build a\nTransformer-based map-matching model with a transfer learning approach. We\ngenerate synthetic trajectory data to pre-train the Transformer model and then\nfine-tune the model with a limited number of ground-truth data to minimize the\nmodel development cost and reduce the real-to-virtual gap. Three metrics\n(Average Hamming Distance, F-score, and BLEU) at two levels (point and segment\nlevel) are used to evaluate the model performance. The results indicate that\nthe proposed model outperforms existing models. Furthermore, we use the\nattention weights of the Transformer to plot the map-matching process and find\nhow the model matches the road segments correctly.",
          "link": "http://arxiv.org/abs/2108.00439",
          "publishedOn": "2021-08-03T02:06:32.771Z",
          "wordCount": 602,
          "title": "Transformer-based Map Matching with Model Limited Ground-Truth Data using Transfer-Learning Approach. (arXiv:2108.00439v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baharani_M/0/1/0/all/0/1\">Mohammadreza Baharani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katariya_V/0/1/0/all/0/1\">Vinit Katariya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_N/0/1/0/all/0/1\">Nichole Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoghli_O/0/1/0/all/0/1\">Omidreza Shoghli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1\">Hamed Tabkhi</a>",
          "description": "Vehicle trajectory prediction is an essential task for enabling many\nintelligent transportation systems. While there have been some promising\nadvances in the field, there is a need for new agile algorithms with smaller\nmodel sizes and lower computational requirements. This article presents\nDeepTrack, a novel deep learning algorithm customized for real-time vehicle\ntrajectory prediction in highways. In contrast to previous methods, the vehicle\ndynamics are encoded using Agile Temporal Convolutional Networks (ATCNs) to\nprovide more robust time prediction with less computation. ATCN also uses\ndepthwise convolution, which reduces the complexity of models compared to\nexisting approaches in terms of model size and operations. Overall, our\nexperimental results demonstrate that DeepTrack achieves comparable accuracy to\nstate-of-the-art trajectory prediction models but with smaller model sizes and\nlower computational complexity, making it more suitable for real-world\ndeployment.",
          "link": "http://arxiv.org/abs/2108.00505",
          "publishedOn": "2021-08-03T02:06:32.757Z",
          "wordCount": 573,
          "title": "DeepTrack: Lightweight Deep Learning for Vehicle Path Prediction in Highways. (arXiv:2108.00505v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1\">Md Afzal Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meraz_M/0/1/0/all/0/1\">Md Meraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Pavan Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1\">Mohammed Javed</a>",
          "description": "In this paper, we present new feature encoding methods for Detection of 3D\nobjects in point clouds. We used a graph neural network (GNN) for Detection of\n3D objects namely cars, pedestrians, and cyclists. Feature encoding is one of\nthe important steps in Detection of 3D objects. The dataset used is point cloud\ndata which is irregular and unstructured and it needs to be encoded in such a\nway that ensures better feature encapsulation. Earlier works have used relative\ndistance as one of the methods to encode the features. These methods are not\nresistant to rotation variance problems in Graph Neural Networks. We have\nincluded angular-based measures while performing feature encoding in graph\nneural networks. Along with that, we have performed a comparison between other\nmethods like Absolute, Relative, Euclidean distances, and a combination of the\nAngle and Relative methods. The model is trained and evaluated on the subset of\nthe KITTI object detection benchmark dataset under resource constraints. Our\nresults demonstrate that a combination of angle measures and relative distance\nhas performed better than other methods. In comparison to the baseline\nmethod(relative), it achieved better performance. We also performed time\nanalysis of various feature encoding methods.",
          "link": "http://arxiv.org/abs/2108.00780",
          "publishedOn": "2021-08-03T02:06:32.734Z",
          "wordCount": 652,
          "title": "Angle Based Feature Learning in GNN for 3D Object Detection using Point Cloud. (arXiv:2108.00780v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pawelczyk_M/0/1/0/all/0/1\">Martin Pawelczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielawski_S/0/1/0/all/0/1\">Sascha Bielawski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heuvel_J/0/1/0/all/0/1\">Johannes van den Heuvel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richter_T/0/1/0/all/0/1\">Tobias Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1\">Gjergji Kasneci</a>",
          "description": "Counterfactual explanations provide means for prescriptive model explanations\nby suggesting actionable feature changes (e.g., increase income) that allow\nindividuals to achieve favorable outcomes in the future (e.g., insurance\napproval). Choosing an appropriate method is a crucial aspect for meaningful\ncounterfactual explanations. As documented in recent reviews, there exists a\nquickly growing literature with available methods. Yet, in the absence of\nwidely available opensource implementations, the decision in favor of certain\nmodels is primarily based on what is readily available. Going forward - to\nguarantee meaningful comparisons across explanation methods - we present CARLA\n(Counterfactual And Recourse LibrAry), a python library for benchmarking\ncounterfactual explanation methods across both different data sets and\ndifferent machine learning models. In summary, our work provides the following\ncontributions: (i) an extensive benchmark of 11 popular counterfactual\nexplanation methods, (ii) a benchmarking framework for research on future\ncounterfactual explanation methods, and (iii) a standardized set of integrated\nevaluation measures and data sets for transparent and extensive comparisons of\nthese methods. We have open-sourced CARLA and our experimental results on\nGithub, making them available as competitive baselines. We welcome\ncontributions from other research groups and practitioners.",
          "link": "http://arxiv.org/abs/2108.00783",
          "publishedOn": "2021-08-03T02:06:32.727Z",
          "wordCount": 643,
          "title": "CARLA: A Python Library to Benchmark Algorithmic Recourse and Counterfactual Explanation Algorithms. (arXiv:2108.00783v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00138",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Watson_T/0/1/0/all/0/1\">Thomas Watson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poudel_B/0/1/0/all/0/1\">Bibek Poudel</a>",
          "description": "Model free techniques have been successful at optimal control of complex\nsystems at an expense of copious amounts of data and computation. However, it\nis often desired to obtain a control policy in a short period of time with\nminimal data use and computational burden. To this end, we make use of the NFQ\nalgorithm for steering position control of a golf cart in both a real hardware\nand a simulated environment that was built from real-world interaction. The\ncontroller learns to apply a sequence of voltage signals in the presence of\nenvironmental uncertainties and inherent non-linearities that challenge the the\ncontrol task. We were able to increase the rate of successful control under\nfour minutes in simulation and under 11 minutes in real hardware.",
          "link": "http://arxiv.org/abs/2108.00138",
          "publishedOn": "2021-08-03T02:06:32.721Z",
          "wordCount": 570,
          "title": "Learning to Control Direct Current Motor for Steering in Real Time via Reinforcement Learning. (arXiv:2108.00138v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huangfu_Y/0/1/0/all/0/1\">Yourui Huangfu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yiqun Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>",
          "description": "The wireless network is undergoing a trend from \"onnection of things\" to\n\"connection of intelligence\". With data spread over the communication networks\nand computing capability enhanced on the devices, distributed learning becomes\na hot topic in both industrial and academic communities. Many frameworks, such\nas federated learning and federated distillation, have been proposed. However,\nfew of them takes good care of obstacles such as the time-varying topology\nresulted by the characteristics of wireless networks. In this paper, we propose\na distributed learning framework based on a scalable deep neural network (DNN)\ndesign. By exploiting the permutation equivalence and invariance properties of\nthe learning tasks, the DNNs with different scales for different clients can be\nbuilt up based on two basic parameter sub-matrices. Further, model aggregation\ncan also be conducted based on these two sub-matrices to improve the learning\nconvergence and performance. Finally, simulation results verify the benefits of\nthe proposed framework by compared with some baselines.",
          "link": "http://arxiv.org/abs/2108.00231",
          "publishedOn": "2021-08-03T02:06:32.711Z",
          "wordCount": 605,
          "title": "Distributed Learning for Time-varying Networks: A Scalable Design. (arXiv:2108.00231v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00259",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Q/0/1/0/all/0/1\">Qihan Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kim_J/0/1/0/all/0/1\">Junhyung Lyle Kim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "The lottery ticket hypothesis (LTH) claims that randomly-initialized, dense\nneural networks contain (sparse) subnetworks that, when trained an equal amount\nin isolation, can match the dense network's performance. Although LTH is useful\nfor discovering efficient network architectures, its three-step process --\npre-training, pruning, and re-training -- is computationally expensive, as the\ndense model must be fully pre-trained. Luckily, \"early-bird\" tickets can be\ndiscovered within neural networks that are minimally pre-trained, allowing for\nthe creation of efficient, LTH-inspired training procedures. Yet, no\ntheoretical foundation of this phenomenon exists. We derive an analytical bound\nfor the number of pre-training iterations that must be performed for a winning\nticket to be discovered, thus providing a theoretical understanding of when and\nwhy such early-bird tickets exist. By adopting a greedy forward selection\npruning strategy, we directly connect the pruned network's performance to the\nloss of the dense network from which it was derived, revealing a threshold in\nthe number of pre-training iterations beyond which high-performing subnetworks\nare guaranteed to exist. We demonstrate the validity of our theoretical results\nacross a variety of architectures and datasets, including multi-layer\nperceptrons (MLPs) trained on MNIST and several deep convolutional neural\nnetwork (CNN) architectures trained on CIFAR10 and ImageNet.",
          "link": "http://arxiv.org/abs/2108.00259",
          "publishedOn": "2021-08-03T02:06:32.704Z",
          "wordCount": 654,
          "title": "Provably Efficient Lottery Ticket Discovery. (arXiv:2108.00259v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ilboudo_W/0/1/0/all/0/1\">Wendyam Eric Lionel Ilboudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_T/0/1/0/all/0/1\">Taisuke Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_K/0/1/0/all/0/1\">Kenji Sugimoto</a>",
          "description": "Behavioral cloning (BC) bears a high potential for safe and direct transfer\nof human skills to robots. However, demonstrations performed by human operators\noften contain noise or imperfect behaviors that can affect the efficiency of\nthe imitator if left unchecked. In order to allow the imitators to effectively\nlearn from imperfect demonstrations, we propose to employ the robust t-momentum\noptimization algorithm. This algorithm builds on the Student's t-distribution\nin order to deal with heavy-tailed data and reduce the effect of outlying\nobservations. We extend the t-momentum algorithm to allow for an adaptive and\nautomatic robustness and show empirically how the algorithm can be used to\nproduce robust BC imitators against datasets with unknown heaviness. Indeed,\nthe imitators trained with the t-momentum-based Adam optimizers displayed\nrobustness to imperfect demonstrations on two different manipulation tasks with\ndifferent robots and revealed the capability to take advantage of the\nadditional data while reducing the adverse effect of non-optimal behaviors.",
          "link": "http://arxiv.org/abs/2108.00625",
          "publishedOn": "2021-08-03T02:06:32.682Z",
          "wordCount": 616,
          "title": "Adaptive t-Momentum-based Optimization for Unknown Ratio of Outliers in Amateur Data in Imitation Learning. (arXiv:2108.00625v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinyan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>",
          "description": "In this paper, we revisit the problem of Differentially Private Stochastic\nConvex Optimization (DP-SCO) and provide excess population risks for some\nspecial classes of functions that are faster than the previous results of\ngeneral convex and strongly convex functions. In the first part of the paper,\nwe study the case where the population risk function satisfies the Tysbakov\nNoise Condition (TNC) with some parameter $\\theta>1$. Specifically, we first\nshow that under some mild assumptions on the loss functions, there is an\nalgorithm whose output could achieve an upper bound of\n$\\tilde{O}((\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d\\log\n\\frac{1}{\\delta}}}{n\\epsilon})^\\frac{\\theta}{\\theta-1})$ for $(\\epsilon,\n\\delta)$-DP when $\\theta\\geq 2$, here $n$ is the sample size and $d$ is the\ndimension of the space. Then we address the inefficiency issue, improve the\nupper bounds by $\\text{Poly}(\\log n)$ factors and extend to the case where\n$\\theta\\geq \\bar{\\theta}>1$ for some known $\\bar{\\theta}$. Next we show that\nthe excess population risk of population functions satisfying TNC with\nparameter $\\theta>1$ is always lower bounded by\n$\\Omega((\\frac{d}{n\\epsilon})^\\frac{\\theta}{\\theta-1}) $ and\n$\\Omega((\\frac{\\sqrt{d\\log\n\\frac{1}{\\delta}}}{n\\epsilon})^\\frac{\\theta}{\\theta-1})$ for $\\epsilon$-DP and\n$(\\epsilon, \\delta)$-DP, respectively. In the second part, we focus on a\nspecial case where the population risk function is strongly convex. Unlike the\nprevious studies, here we assume the loss function is {\\em non-negative} and\n{\\em the optimal value of population risk is sufficiently small}. With these\nadditional assumptions, we propose a new method whose output could achieve an\nupper bound of\n$O(\\frac{d\\log\\frac{1}{\\delta}}{n^2\\epsilon^2}+\\frac{1}{n^{\\tau}})$ for any\n$\\tau\\geq 1$ in $(\\epsilon,\\delta)$-DP model if the sample size $n$ is\nsufficiently large.",
          "link": "http://arxiv.org/abs/2108.00331",
          "publishedOn": "2021-08-03T02:06:32.675Z",
          "wordCount": 686,
          "title": "Faster Rates of Differentially Private Stochastic Convex Optimization. (arXiv:2108.00331v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Sangwoo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hyunwoo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kihyuk Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Contrastive self-supervised learning has shown impressive results in learning\nvisual representations from unlabeled images by enforcing invariance against\ndifferent data augmentations. However, the learned representations are often\ncontextually biased to the spurious scene correlations of different objects or\nobject and background, which may harm their generalization on the downstream\ntasks. To tackle the issue, we develop a novel object-aware contrastive\nlearning framework that first (a) localizes objects in a self-supervised manner\nand then (b) debias scene correlations via appropriate data augmentations\nconsidering the inferred object locations. For (a), we propose the contrastive\nclass activation map (ContraCAM), which finds the most discriminative regions\n(e.g., objects) in the image compared to the other images using the\ncontrastively trained models. We further improve the ContraCAM to detect\nmultiple objects and entire shapes via an iterative refinement procedure. For\n(b), we introduce two data augmentations based on ContraCAM, object-aware\nrandom crop and background mixup, which reduce contextual and background biases\nduring contrastive self-supervised learning, respectively. Our experiments\ndemonstrate the effectiveness of our representation learning framework,\nparticularly when trained under multi-object images or evaluated under the\nbackground (and distribution) shifted images.",
          "link": "http://arxiv.org/abs/2108.00049",
          "publishedOn": "2021-08-03T02:06:32.654Z",
          "wordCount": 632,
          "title": "Object-aware Contrastive Learning for Debiased Scene Representation. (arXiv:2108.00049v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00354",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhu_B/0/1/0/all/0/1\">Botao Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bedeer_E/0/1/0/all/0/1\">Ebrahim Bedeer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha H. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barton_R/0/1/0/all/0/1\">Robert Barton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Henry_J/0/1/0/all/0/1\">Jerome Henry</a>",
          "description": "Unmanned aerial vehicles (UAVs) have emerged as a promising candidate\nsolution for data collection of large-scale wireless sensor networks (WSNs). In\nthis paper, we investigate a UAV-aided WSN, where cluster heads (CHs) receive\ndata from their member nodes, and a UAV is dispatched to collect data from CHs\nalong the planned trajectory. We aim to minimize the total energy consumption\nof the UAV-WSN system in a complete round of data collection. Toward this end,\nwe formulate the energy consumption minimization problem as a constrained\ncombinatorial optimization problem by jointly selecting CHs from nodes within\nclusters and planning the UAV's visiting order to the selected CHs. The\nformulated energy consumption minimization problem is NP-hard, and hence, hard\nto solve optimally. In order to tackle this challenge, we propose a novel deep\nreinforcement learning (DRL) technique, pointer network-A* (Ptr-A*), which can\nefficiently learn from experiences the UAV trajectory policy for minimizing the\nenergy consumption. The UAV's start point and the WSN with a set of\npre-determined clusters are fed into the Ptr-A*, and the Ptr-A* outputs a group\nof CHs and the visiting order to these CHs, i.e., the UAV's trajectory. The\nparameters of the Ptr-A* are trained on small-scale clusters problem instances\nfor faster training by using the actor-critic algorithm in an unsupervised\nmanner. At inference, three search strategies are also proposed to improve the\nquality of solutions. Simulation results show that the trained models based on\n20-clusters and 40-clusters have a good generalization ability to solve the\nUAV's trajectory planning problem in WSNs with different numbers of clusters,\nwithout the need to retrain the models. Furthermore, the results show that our\nproposed DRL algorithm outperforms two baseline techniques.",
          "link": "http://arxiv.org/abs/2108.00354",
          "publishedOn": "2021-08-03T02:06:32.646Z",
          "wordCount": 742,
          "title": "UAV Trajectory Planning in Wireless Sensor Networks for Energy Consumption Minimization by Deep Reinforcement Learning. (arXiv:2108.00354v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Albanese_A/0/1/0/all/0/1\">Andrea Albanese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nardello_M/0/1/0/all/0/1\">Matteo Nardello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunelli_D/0/1/0/all/0/1\">Davide Brunelli</a>",
          "description": "Artificial intelligence has smoothly penetrated several economic activities,\nespecially monitoring and control applications, including the agriculture\nsector. However, research efforts toward low-power sensing devices with fully\nfunctional machine learning (ML) on-board are still fragmented and limited in\nsmart farming. Biotic stress is one of the primary causes of crop yield\nreduction. With the development of deep learning in computer vision technology,\nautonomous detection of pest infestation through images has become an important\nresearch direction for timely crop disease diagnosis. This paper presents an\nembedded system enhanced with ML functionalities, ensuring continuous detection\nof pest infestation inside fruit orchards. The embedded solution is based on a\nlow-power embedded sensing system along with a Neural Accelerator able to\ncapture and process images inside common pheromone-based traps. Three different\nML algorithms have been trained and deployed, highlighting the capabilities of\nthe platform. Moreover, the proposed approach guarantees an extended battery\nlife thanks to the integration of energy harvesting functionalities. Results\nshow how it is possible to automate the task of pest infestation for unlimited\ntime without the farmer's intervention.",
          "link": "http://arxiv.org/abs/2108.00421",
          "publishedOn": "2021-08-03T02:06:32.612Z",
          "wordCount": 626,
          "title": "Automated Pest Detection with DNN on the Edge for Precision Agriculture. (arXiv:2108.00421v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dogan_M/0/1/0/all/0/1\">Mine Gokce Dogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezzeldin_Y/0/1/0/all/0/1\">Yahya H. Ezzeldin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragouli_C/0/1/0/all/0/1\">Christina Fragouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohannon_A/0/1/0/all/0/1\">Addison W. Bohannon</a>",
          "description": "We consider a source that wishes to communicate with a destination at a\ndesired rate, over a mmWave network where links are subject to blockage and\nnodes to failure (e.g., in a hostile military environment). To achieve\nresilience to link and node failures, we here explore a state-of-the-art Soft\nActor-Critic (SAC) deep reinforcement learning algorithm, that adapts the\ninformation flow through the network, without using knowledge of the link\ncapacities or network topology. Numerical evaluations show that our algorithm\ncan achieve the desired rate even in dynamic environments and it is robust\nagainst blockage.",
          "link": "http://arxiv.org/abs/2108.00548",
          "publishedOn": "2021-08-03T02:06:32.596Z",
          "wordCount": 534,
          "title": "A Reinforcement Learning Approach for Scheduling in mmWave Networks. (arXiv:2108.00548v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haitong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shuang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xia Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kaiyue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hongjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nizhuan Wang</a>",
          "description": "It is a challenging task to accurately perform semantic segmentation due to\nthe complexity of real picture scenes. Many semantic segmentation methods based\non traditional deep learning insufficiently captured the semantic and\nappearance information of images, which put limit on their generality and\nrobustness for various application scenes. In this paper, we proposed a novel\nstrategy that reformulated the popularly-used convolution operation to\nmulti-layer convolutional sparse coding block to ease the aforementioned\ndeficiency. This strategy can be possibly used to significantly improve the\nsegmentation performance of any semantic segmentation model that involves\nconvolutional operations. To prove the effectiveness of our idea, we chose the\nwidely-used U-Net model for the demonstration purpose, and we designed CSC-Unet\nmodel series based on U-Net. Through extensive analysis and experiments, we\nprovided credible evidence showing that the multi-layer convolutional sparse\ncoding block enables semantic segmentation model to converge faster, can\nextract finer semantic and appearance information of images, and improve the\nability to recover spatial detail information. The best CSC-Unet model\nsignificantly outperforms the results of the original U-Net on three public\ndatasets with different scenarios, i.e., 87.14% vs. 84.71% on DeepCrack\ndataset, 68.91% vs. 67.09% on Nuclei dataset, and 53.68% vs. 48.82% on CamVid\ndataset, respectively.",
          "link": "http://arxiv.org/abs/2108.00408",
          "publishedOn": "2021-08-03T02:06:32.562Z",
          "wordCount": 661,
          "title": "CSC-Unet: A Novel Convolutional Sparse Coding Strategy based Neural Network for Semantic Segmentation. (arXiv:2108.00408v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abhishek_G/0/1/0/all/0/1\">Guttu Sai Abhishek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingole_H/0/1/0/all/0/1\">Harshad Ingole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laturia_P/0/1/0/all/0/1\">Parth Laturia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorna_V/0/1/0/all/0/1\">Vineeth Dorna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1\">Ayush Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>",
          "description": "We present SPEAR, an open-source python library for data programming with\nsemi supervision. The package implements several recent data programming\napproaches including facility to programmatically label and build training\ndata. SPEAR facilitates weak supervision in the form of heuristics (or rules)\nand association of noisy labels to the training dataset. These noisy labels are\naggregated to assign labels to the unlabeled data for downstream tasks. We have\nimplemented several label aggregation approaches that aggregate the noisy\nlabels and then train using the noisily labeled set in a cascaded manner. Our\nimplementation also includes other approaches that jointly aggregate and train\nthe model. Thus, in our python package, we integrate several cascade and joint\ndata-programming approaches while also providing the facility of data\nprogramming by letting the user define labeling functions or rules. The code\nand tutorial notebooks are available at\n\\url{https://github.com/decile-team/spear}.",
          "link": "http://arxiv.org/abs/2108.00373",
          "publishedOn": "2021-08-03T02:06:32.510Z",
          "wordCount": 584,
          "title": "SPEAR : Semi-supervised Data Programming in Python. (arXiv:2108.00373v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00109",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Medrano_M/0/1/0/all/0/1\">Maria Medrano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_R/0/1/0/all/0/1\">Rui Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Politte_D/0/1/0/all/0/1\">David G. Politte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Williamson_J/0/1/0/all/0/1\">Jeffrey F. Williamson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+OSullivan_J/0/1/0/all/0/1\">Joseph A. O&#x27;Sullivan</a>",
          "description": "Dual-energy CT (DECT) has been widely investigated to generate more\ninformative and more accurate images in the past decades. For example,\nDual-Energy Alternating Minimization (DEAM) algorithm achieves sub-percentage\nuncertainty in estimating proton stopping-power mappings from experimental 3-mm\ncollimated phantom data. However, elapsed time of iterative DECT algorithms is\nnot clinically acceptable, due to their low convergence rate and the tremendous\ngeometry of modern helical CT scanners. A CNN-based initialization method is\nintroduced to reduce the computational time of iterative DECT algorithms. DEAM\nis used as an example of iterative DECT algorithms in this work. The simulation\nresults show that our method generates denoised images with greatly improved\nestimation accuracy for adipose, tonsils, and muscle tissue. Also, it reduces\nelapsed time by approximately 5-fold for DEAM to reach the same objective\nfunction value for both simulated and real data.",
          "link": "http://arxiv.org/abs/2108.00109",
          "publishedOn": "2021-08-03T02:06:32.504Z",
          "wordCount": 608,
          "title": "A Machine-learning Based Initialization for Joint Statistical Iterative Dual-energy CT with Application to Proton Therapy. (arXiv:2108.00109v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grushin_A/0/1/0/all/0/1\">Alexander Grushin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woods_W/0/1/0/all/0/1\">Walt Woods</a>",
          "description": "Reinforcement learning has recently shown promise as a technique for training\nan artificial neural network to parse sentences in some unknown format. A key\naspect of this approach is that rather than explicitly inferring a grammar that\ndescribes the format, the neural network learns to perform various parsing\nactions (such as merging two tokens) over a corpus of sentences, with the goal\nof maximizing the total reward, which is roughly based on the estimated\nfrequency of the resulting parse structures. This can allow the learning\nprocess to more easily explore different action choices, since a given choice\nmay change the optimality of the parse (as expressed by the total reward), but\nwill not result in the failure to parse a sentence. However, the approach also\nexhibits limitations: first, the neural network does not provide production\nrules for the grammar that it uses during parsing; second, because this neural\nnetwork can successfully parse any sentence, it cannot be directly used to\nidentify sentences that deviate from the format of the training sentences,\ni.e., that are anomalous. In this paper, we address these limitations by\npresenting procedures for extracting production rules from the neural network,\nand for using these rules to determine whether a given sentence is nominal or\nanomalous, when compared to structures observed within training data. In the\nlatter case, an attempt is made to identify the location of the anomaly.\nAdditionally, a two pass mechanism is presented for dealing with formats\ncontaining high-entropy information. We empirically evaluate the approach on\nartificial formats, demonstrating effectiveness, but also identifying\nlimitations. By further improving parser learning, and leveraging rule\nextraction and anomaly detection, one might begin to understand common errors,\neither benign or malicious, in practical formats.",
          "link": "http://arxiv.org/abs/2108.00103",
          "publishedOn": "2021-08-03T02:06:32.463Z",
          "wordCount": 743,
          "title": "Extracting Grammars from a Neural Network Parser for Anomaly Detection in Unknown Formats. (arXiv:2108.00103v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Che_C/0/1/0/all/0/1\">Chunjiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaoyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zibin Zheng</a>",
          "description": "Federated learning allows multiple participants to collaboratively train an\nefficient model without exposing data privacy. However, this distributed\nmachine learning training method is prone to attacks from Byzantine clients,\nwhich interfere with the training of the global model by modifying the model or\nuploading the false gradient. In this paper, we propose a novel serverless\nfederated learning framework Committee Mechanism based Federated Learning\n(CMFL), which can ensure the robustness of the algorithm with convergence\nguarantee. In CMFL, a committee system is set up to screen the uploaded local\ngradients. The committee system selects the local gradients rated by the\nelected members for the aggregation procedure through the selection strategy,\nand replaces the committee member through the election strategy. Based on the\ndifferent considerations of model performance and defense, two opposite\nselection strategies are designed for the sake of both accuracy and robustness.\nExtensive experiments illustrate that CMFL achieves faster convergence and\nbetter accuracy than the typical Federated Learning, in the meanwhile obtaining\nbetter robustness than the traditional Byzantine-tolerant algorithms, in the\nmanner of a decentralized approach. In addition, we theoretically analyze and\nprove the convergence of CMFL under different election and selection\nstrategies, which coincides with the experimental results.",
          "link": "http://arxiv.org/abs/2108.00365",
          "publishedOn": "2021-08-03T02:06:32.228Z",
          "wordCount": 649,
          "title": "A Decentralized Federated Learning Framework via Committee Mechanism with Convergence Guarantee. (arXiv:2108.00365v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cini_A/0/1/0/all/0/1\">Andrea Cini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marisca_I/0/1/0/all/0/1\">Ivan Marisca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alippi_C/0/1/0/all/0/1\">Cesare Alippi</a>",
          "description": "Dealing with missing values and incomplete time series is a labor-intensive\nand time-consuming inevitable task when handling data coming from real-world\napplications. Effective spatio-temporal representations would allow imputation\nmethods to reconstruct missing temporal data by exploiting information coming\nfrom sensors at different locations. However, standard methods fall short in\ncapturing the nonlinear time and space dependencies existing within networks of\ninterconnected sensors and do not take full advantage of the available - and\noften strong - relational information. Notably, most of state-of-the-art\nimputation methods based on deep learning do not explicitly model relational\naspects and, in any case, do not exploit processing frameworks able to\nadequately represent structured spatio-temporal data. Conversely, graph neural\nnetworks have recently surged in popularity as both expressive and scalable\ntools for processing sequential data with relational inductive biases. In this\nwork, we present the first assessment of graph neural networks in the context\nof multivariate time series imputation. In particular, we introduce a novel\ngraph neural network architecture, named GRIL, which aims at reconstructing\nmissing data in the different channels of a multivariate time series by\nlearning spatial-temporal representations through message passing. Preliminary\nempirical results show that our model outperforms state-of-the-art methods in\nthe imputation task on relevant benchmarks with mean absolute error\nimprovements often higher than 20%.",
          "link": "http://arxiv.org/abs/2108.00298",
          "publishedOn": "2021-08-03T02:06:32.175Z",
          "wordCount": 644,
          "title": "Multivariate Time Series Imputation by Graph Neural Networks. (arXiv:2108.00298v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alamri_F/0/1/0/all/0/1\">Faisal Alamri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1\">Anjan Dutta</a>",
          "description": "Zero-Shot Learning (ZSL) aims to recognise unseen object classes, which are\nnot observed during the training phase. The existing body of works on ZSL\nmostly relies on pretrained visual features and lacks the explicit attribute\nlocalisation mechanism on images. In this work, we propose an attention-based\nmodel in the problem settings of ZSL to learn attributes useful for unseen\nclass recognition. Our method uses an attention mechanism adapted from Vision\nTransformer to capture and learn discriminative attributes by splitting images\ninto small patches. We conduct experiments on three popular ZSL benchmarks\n(i.e., AWA2, CUB and SUN) and set new state-of-the-art harmonic mean results\n{on all the three datasets}, which illustrate the effectiveness of our proposed\nmethod.",
          "link": "http://arxiv.org/abs/2108.00045",
          "publishedOn": "2021-08-03T02:06:32.070Z",
          "wordCount": 562,
          "title": "Multi-Head Self-Attention via Vision Transformer for Zero-Shot Learning. (arXiv:2108.00045v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tazarv_A/0/1/0/all/0/1\">Ali Tazarv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labbaf_S/0/1/0/all/0/1\">Sina Labbaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reich_S/0/1/0/all/0/1\">Stephanie M. Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutt_N/0/1/0/all/0/1\">Nikil Dutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1\">Amir M. Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levorato_M/0/1/0/all/0/1\">Marco Levorato</a>",
          "description": "Since stress contributes to a broad range of mental and physical health\nproblems, the objective assessment of stress is essential for behavioral and\nphysiological studies. Although several studies have evaluated stress levels in\ncontrolled settings, objective stress assessment in everyday settings is still\nlargely under-explored due to challenges arising from confounding contextual\nfactors and limited adherence for self-reports. In this paper, we explore the\nobjective prediction of stress levels in everyday settings based on heart rate\n(HR) and heart rate variability (HRV) captured via low-cost and easy-to-wear\nphotoplethysmography (PPG) sensors that are widely available on newer smart\nwearable devices. We present a layered system architecture for personalized\nstress monitoring that supports a tunable collection of data samples for\nlabeling, and present a method for selecting informative samples from the\nstream of real-time data for labeling. We captured the stress levels of\nfourteen volunteers through self-reported questionnaires over periods of\nbetween 1-3 months, and explored binary stress detection based on HR and HRV\nusing Machine Learning Methods. We observe promising preliminary results given\nthat the dataset is collected in the challenging environments of everyday\nsettings. The binary stress detector is fairly accurate and can detect\nstressful vs non-stressful samples with a macro-F1 score of up to \\%76. Our\nstudy lays the groundwork for more sophisticated labeling strategies that\ngenerate context-aware, personalized models that will empower health\nprofessionals to provide personalized interventions.",
          "link": "http://arxiv.org/abs/2108.00144",
          "publishedOn": "2021-08-03T02:06:32.061Z",
          "wordCount": 682,
          "title": "Personalized Stress Monitoring using Wearable Sensors in Everyday Settings. (arXiv:2108.00144v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yeung_Y/0/1/0/all/0/1\">Yu-Hong Yeung</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Barajas_Solano_D/0/1/0/all/0/1\">David A. Barajas-Solano</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Tartakovsky_A/0/1/0/all/0/1\">Alexandre M. Tartakovsky</a> (1 and 2) ((1) Physical and Computational Sciences Directorate, Pacific Northwest National Laboratory, (2) Department of Civil and Environmental Engineering, University of Illinois Urbana-Champaign)",
          "description": "We develop a physics-informed machine learning approach for large-scale data\nassimilation and parameter estimation and apply it for estimating\ntransmissivity and hydraulic head in the two-dimensional steady-state\nsubsurface flow model of the Hanford Site given synthetic measurements of said\nvariables. In our approach, we extend the physics-informed conditional\nKarhunen-Lo\\'{e}ve expansion (PICKLE) method for modeling subsurface flow with\nunknown flux (Neumann) and varying head (Dirichlet) boundary conditions. We\ndemonstrate that the PICKLE method is comparable in accuracy with the standard\nmaximum a posteriori (MAP) method, but is significantly faster than MAP for\nlarge-scale problems. Both methods use a mesh to discretize the computational\ndomain. In MAP, the parameters and states are discretized on the mesh;\ntherefore, the size of the MAP parameter estimation problem directly depends on\nthe mesh size. In PICKLE, the mesh is used to evaluate the residuals of the\ngoverning equation, while the parameters and states are approximated by the\ntruncated conditional Karhunen-Lo\\'{e}ve expansions with the number of\nparameters controlled by the smoothness of the parameter and state fields, and\nnot by the mesh size. For a considered example, we demonstrate that the\ncomputational cost of PICKLE increases near linearly (as $N_{FV}^{1.15}$) with\nthe number of grid points $N_{FV}$, while that of MAP increases much faster as\n$N_{FV}^{3.28}$. We demonstrated that once trained for one set of Dirichlet\nboundary conditions (i.e., one river stage), the PICKLE method provides\naccurate estimates of the hydraulic head for any value of the Dirichlet\nboundary conditions (i.e., for any river stage).",
          "link": "http://arxiv.org/abs/2108.00037",
          "publishedOn": "2021-08-03T02:06:32.041Z",
          "wordCount": 728,
          "title": "Physics-Informed Machine Learning Method for Large-Scale Data Assimilation Problems. (arXiv:2108.00037v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00069",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lejarza_F/0/1/0/all/0/1\">Fernando Lejarza</a>, <a href=\"http://arxiv.org/find/math/1/au:+Baldea_M/0/1/0/all/0/1\">Michael Baldea</a>",
          "description": "Discovering the governing laws underpinning physical and chemical phenomena\nis a key step towards understanding and ultimately controlling systems in\nscience and engineering. We introduce Discovery of Dynamical Systems via Moving\nHorizon Optimization (DySMHO), a scalable machine learning framework for\nidentifying governing laws in the form of differential equations from\nlarge-scale noisy experimental data sets. DySMHO consists of a novel moving\nhorizon dynamic optimization strategy that sequentially learns the underlying\ngoverning equations from a large dictionary of basis functions. The sequential\nnature of DySMHO allows leveraging statistical arguments for eliminating\nirrelevant basis functions, avoiding overfitting to recover accurate and\nparsimonious forms of the governing equations. Canonical nonlinear dynamical\nsystem examples are used to demonstrate that DySMHO can accurately recover the\ngoverning laws, is robust to high levels of measurement noise and that it can\nhandle challenges such as multiple time scale dynamics.",
          "link": "http://arxiv.org/abs/2108.00069",
          "publishedOn": "2021-08-03T02:06:31.892Z",
          "wordCount": 586,
          "title": "DySMHO: Data-Driven Discovery of Governing Equations for Dynamical Systems via Moving Horizon Optimization. (arXiv:2108.00069v1 [math.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Massart_E/0/1/0/all/0/1\">Estelle Massart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrol_V/0/1/0/all/0/1\">Vinayak Abrol</a>",
          "description": "We propose to use stochastic Riemannian coordinate descent on the orthogonal\ngroup for recurrent neural network training. The algorithm rotates successively\ntwo columns of the recurrent matrix, an operation that can be efficiently\nimplemented as a multiplication by a Givens matrix. In the case when the\ncoordinate is selected uniformly at random at each iteration, we prove the\nconvergence of the proposed algorithm under standard assumptions on the loss\nfunction, stepsize and minibatch noise. In addition, we numerically demonstrate\nthat the Riemannian gradient in recurrent neural network training has an\napproximately sparse structure. Leveraging this observation, we propose a\nfaster variant of the proposed algorithm that relies on the Gauss-Southwell\nrule. Experiments on a benchmark recurrent neural network training problem are\npresented to demonstrate the effectiveness of the proposed algorithm.",
          "link": "http://arxiv.org/abs/2108.00051",
          "publishedOn": "2021-08-03T02:06:31.886Z",
          "wordCount": 568,
          "title": "Coordinate descent on the orthogonal group for recurrent neural network training. (arXiv:2108.00051v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Gary Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wessler_B/0/1/0/all/0/1\">Benjamin Wessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1\">Michael C. Hughes</a>",
          "description": "Semi-supervised image classification has shown substantial progress in\nlearning from limited labeled data, but recent advances remain largely untested\nfor clinical applications. Motivated by the urgent need to improve timely\ndiagnosis of life-threatening heart conditions, especially aortic stenosis, we\ndevelop a benchmark dataset to assess semi-supervised approaches to two tasks\nrelevant to cardiac ultrasound (echocardiogram) interpretation: view\nclassification and disease severity classification. We find that a\nstate-of-the-art method called MixMatch achieves promising gains in heldout\naccuracy on both tasks, learning from a large volume of truly unlabeled images\nas well as a labeled set collected at great expense to achieve better\nperformance than is possible with the labeled set alone. We further pursue\npatient-level diagnosis prediction, which requires aggregating across hundreds\nof images of diverse view types, most of which are irrelevant, to make a\ncoherent prediction. The best patient-level performance is achieved by new\nmethods that prioritize diagnosis predictions from images that are predicted to\nbe clinically-relevant views and transfer knowledge from the view task to the\ndiagnosis task. We hope our released Tufts Medical Echocardiogram Dataset and\nevaluation framework inspire further improvements in multi-task semi-supervised\nlearning for clinical applications.",
          "link": "http://arxiv.org/abs/2108.00080",
          "publishedOn": "2021-08-03T02:06:31.848Z",
          "wordCount": 674,
          "title": "A New Semi-supervised Learning Benchmark for Classifying View and Diagnosing Aortic Stenosis from Echocardiograms. (arXiv:2108.00080v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00002",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Kotthoff_L/0/1/0/all/0/1\">Lars Kotthoff</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Wahab_H/0/1/0/all/0/1\">Hud Wahab</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Johnson_P/0/1/0/all/0/1\">Patrick Johnson</a>",
          "description": "Bayesian optimization is used in many areas of AI for the optimization of\nblack-box processes and has achieved impressive improvements of the state of\nthe art for a lot of applications. It intelligently explores large and complex\ndesign spaces while minimizing the number of evaluations of the expensive\nunderlying process to be optimized. Materials science considers the problem of\noptimizing materials' properties given a large design space that defines how to\nsynthesize or process them, with evaluations requiring expensive experiments or\nsimulations -- a very similar setting. While Bayesian optimization is also a\npopular approach to tackle such problems, there is almost no overlap between\nthe two communities that are investigating the same concepts. We present a\nsurvey of Bayesian optimization approaches in materials science to increase\ncross-fertilization and avoid duplication of work. We highlight common\nchallenges and opportunities for joint research efforts.",
          "link": "http://arxiv.org/abs/2108.00002",
          "publishedOn": "2021-08-03T02:06:31.842Z",
          "wordCount": 573,
          "title": "Bayesian Optimization in Materials Science: A Survey. (arXiv:2108.00002v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Novikov_G/0/1/0/all/0/1\">Georgii S. Novikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panov_M/0/1/0/all/0/1\">Maxim E. Panov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1\">Ivan V. Oseledets</a>",
          "description": "Estimation of probability density function from samples is one of the central\nproblems in statistics and machine learning. Modern neural network-based models\ncan learn high dimensional distributions but have problems with hyperparameter\nselection and are often prone to instabilities during training and inference.\nWe propose a new efficient tensor train-based model for density estimation\n(TTDE). Such density parametrization allows exact sampling, calculation of\ncumulative and marginal density functions, and partition function. It also has\nvery intuitive hyperparameters. We develop an efficient non-adversarial\ntraining procedure for TTDE based on the Riemannian optimization. Experimental\nresults demonstrate the competitive performance of the proposed method in\ndensity estimation and sampling tasks, while TTDE significantly outperforms\ncompetitors in training speed.",
          "link": "http://arxiv.org/abs/2108.00089",
          "publishedOn": "2021-08-03T02:06:31.835Z",
          "wordCount": 561,
          "title": "Tensor-Train Density Estimation. (arXiv:2108.00089v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00043",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Ziegler_J/0/1/0/all/0/1\">Joshua Ziegler</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+McJunkin_T/0/1/0/all/0/1\">Thomas McJunkin</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Joseph_E/0/1/0/all/0/1\">E. S. Joseph</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kalantre_S/0/1/0/all/0/1\">Sandesh S. Kalantre</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Harpt_B/0/1/0/all/0/1\">Benjamin Harpt</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Savage_D/0/1/0/all/0/1\">D. E. Savage</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lagally_M/0/1/0/all/0/1\">M. G. Lagally</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Eriksson_M/0/1/0/all/0/1\">M. A. Eriksson</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Taylor_J/0/1/0/all/0/1\">Jacob M. Taylor</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zwolak_J/0/1/0/all/0/1\">Justyna P. Zwolak</a>",
          "description": "The current autotuning approaches for quantum dot (QD) devices, while showing\nsome success, lack an assessment of data reliability. This leads to unexpected\nfailures when noisy data is processed by an autonomous system. In this work, we\npropose a framework for robust autotuning of QD devices that combines a machine\nlearning (ML) state classifier with a data quality control module. The data\nquality control module acts as a ``gatekeeper'' system, ensuring that only\nreliable data is processed by the state classifier. Lower data quality results\nin either device recalibration or termination. To train both ML systems, we\nenhance the QD simulation by incorporating synthetic noise typical of QD\nexperiments. We confirm that the inclusion of synthetic noise in the training\nof the state classifier significantly improves the performance, resulting in an\naccuracy of 95.1(7) % when tested on experimental data. We then validate the\nfunctionality of the data quality control module by showing the state\nclassifier performance deteriorates with decreasing data quality, as expected.\nOur results establish a robust and flexible ML framework for autonomous tuning\nof noisy QD devices.",
          "link": "http://arxiv.org/abs/2108.00043",
          "publishedOn": "2021-08-03T02:06:31.802Z",
          "wordCount": 637,
          "title": "Toward Robust Autotuning of Noisy Quantum Dot Devices. (arXiv:2108.00043v1 [quant-ph])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}