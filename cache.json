{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2105.07148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiyan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wenming Xiao</a>",
          "description": "Lexicon information and pre-trained models, such as BERT, have been combined\nto explore Chinese sequence labelling tasks due to their respective strengths.\nHowever, existing methods solely fuse lexicon features via a shallow and random\ninitialized sequence layer and do not integrate them into the bottom layers of\nBERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese\nsequence labelling, which integrates external lexicon knowledge into BERT\nlayers directly by a Lexicon Adapter layer. Compared with the existing methods,\nour model facilitates deep lexicon knowledge fusion at the lower layers of\nBERT. Experiments on ten Chinese datasets of three tasks including Named Entity\nRecognition, Word Segmentation, and Part-of-Speech tagging, show that LEBERT\nachieves the state-of-the-art results.",
          "link": "http://arxiv.org/abs/2105.07148",
          "publishedOn": "2021-05-23T06:08:17.061Z",
          "wordCount": 566,
          "title": "Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter. (arXiv:2105.07148v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>",
          "description": "Offensive content is pervasive in social media and a reason for concern to\ncompanies and government organizations. Several studies have been recently\npublished investigating methods to detect the various forms of such content\n(e.g. hate speech, cyberbullying, and cyberaggression). The clear majority of\nthese studies deal with English partially because most annotated datasets\navailable contain English data. In this paper, we take advantage of available\nEnglish datasets by applying cross-lingual contextual word embeddings and\ntransfer learning to make predictions in low-resource languages. We project\npredictions on comparable data in Arabic, Bengali, Danish, Greek, Hindi,\nSpanish, and Turkish. We report results of 0.8415 F1 macro for Bengali in\nTRAC-2 shared task, 0.8532 F1 macro for Danish and 0.8701 F1 macro for Greek in\nOffensEval 2020, 0.8568 F1 macro for Hindi in HASOC 2019 shared task and 0.7513\nF1 macro for Spanish in in SemEval-2019 Task 5 (HatEval) showing that our\napproach compares favourably to the best systems submitted to recent shared\ntasks on these three languages. Additionally, we report competitive performance\non Arabic, and Turkish using the training and development sets of OffensEval\n2020 shared task. The results for all languages confirm the robustness of\ncross-lingual contextual embeddings and transfer learning for this task.",
          "link": "http://arxiv.org/abs/2105.05996",
          "publishedOn": "2021-05-23T06:08:17.026Z",
          "wordCount": 695,
          "title": "Multilingual Offensive Language Identification for Low-resource Languages. (arXiv:2105.05996v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06965",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_G/0/1/0/all/0/1\">Grusha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>",
          "description": "When language models process syntactically complex sentences, do they use\nabstract syntactic information present in these sentences in a manner that is\nconsistent with the grammar of English, or do they rely solely on a set of\nheuristics? We propose a method to tackle this question, AlterRep. For any\nlinguistic feature in the sentence, AlterRep allows us to generate\ncounterfactual representations by altering how this feature is encoded, while\nleaving all other aspects of the original representation intact. Then, by\nmeasuring the change in a models' word prediction with these counterfactual\nrepresentations in different sentences, we can draw causal conclusions about\nthe contexts in which the model uses the linguistic feature (if any). Applying\nthis method to study how BERT uses relative clause (RC) span information, we\nfound that BERT uses information about RC spans during agreement prediction\nusing the linguistically correct strategy. We also found that counterfactual\nrepresentations generated for a specific RC subtype influenced the number\nprediction in sentences with other RC subtypes, suggesting that information\nabout RC boundaries was encoded abstractly in BERT's representation.",
          "link": "http://arxiv.org/abs/2105.06965",
          "publishedOn": "2021-05-23T06:08:16.967Z",
          "wordCount": 645,
          "title": "Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction. (arXiv:2105.06965v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zili Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1\">Donal Landers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>",
          "description": "This paper describes N-XKT (Neural encoding based on eXplanatory Knowledge\nTransfer), a novel method for the automatic transfer of explanatory knowledge\nthrough neural encoding mechanisms. We demonstrate that N-XKT is able to\nimprove accuracy and generalization on science Question Answering (QA).\nSpecifically, by leveraging facts from background explanatory knowledge\ncorpora, the N-XKT model shows a clear improvement on zero-shot QA.\nFurthermore, we show that N-XKT can be fine-tuned on a target QA dataset,\nenabling faster convergence and more accurate results. A systematic analysis is\nconducted to quantitatively analyze the performance of the N-XKT model and the\nimpact of different categories of knowledge on the zero-shot generalization\ntask.",
          "link": "http://arxiv.org/abs/2105.05737",
          "publishedOn": "2021-05-23T06:08:16.959Z",
          "wordCount": 554,
          "title": "Encoding Explanatory Knowledge for Zero-shot Science Question Answering. (arXiv:2105.05737v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang-Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Ed Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Error correction techniques have been used to refine the output sentences\nfrom automatic speech recognition (ASR) models and achieve a lower word error\nrate (WER) than original ASR outputs. Previous works usually use a\nsequence-to-sequence model to correct an ASR output sentence autoregressively,\nwhich causes large latency and cannot be deployed in online ASR services. A\nstraightforward solution to reduce latency, inspired by non-autoregressive\n(NAR) neural machine translation, is to use an NAR sequence generation model\nfor ASR error correction, which, however, comes at the cost of significantly\nincreased ASR error rate. In this paper, observing distinctive error patterns\nand correction operations (i.e., insertion, deletion, and substitution) in ASR,\nwe propose FastCorrect, a novel NAR error correction model based on edit\nalignment. In training, FastCorrect aligns each source token from an ASR output\nsentence to the target tokens from the corresponding ground-truth sentence\nbased on the edit distance between the source and target sentences, and\nextracts the number of target tokens corresponding to each source token during\nedition/correction, which is then used to train a length predictor and to\nadjust the source tokens to match the length of the target sentence for\nparallel generation. In inference, the token number predicted by the length\npredictor is used to adjust the source tokens for target sequence generation.\nExperiments on the public AISHELL-1 dataset and an internal industrial-scale\nASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)\nit speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER\nreduction) compared with the autoregressive correction model; and 2) it\noutperforms the accuracy of popular NAR models adopted in neural machine\ntranslation by a large margin.",
          "link": "http://arxiv.org/abs/2105.03842",
          "publishedOn": "2021-05-23T06:08:16.945Z",
          "wordCount": 751,
          "title": "FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition. (arXiv:2105.03842v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_V/0/1/0/all/0/1\">Vanessa Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_R/0/1/0/all/0/1\">Rihao Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>",
          "description": "Learning prerequisite chains is an essential task for efficiently acquiring\nknowledge in both known and unknown domains. For example, one may be an expert\nin the natural language processing (NLP) domain but want to determine the best\norder to learn new concepts in an unfamiliar Computer Vision domain (CV). Both\ndomains share some common concepts, such as machine learning basics and deep\nlearning models. In this paper, we propose unsupervised cross-domain concept\nprerequisite chain learning using an optimized variational graph autoencoder.\nOur model learns to transfer concept prerequisite relations from an\ninformation-rich domain (source domain) to an information-poor domain (target\ndomain), substantially surpassing other baseline models. Also, we expand an\nexisting dataset by introducing two new domains: CV and Bioinformatics (BIO).\nThe annotated data and resources, as well as the code, will be made publicly\navailable.",
          "link": "http://arxiv.org/abs/2105.03505",
          "publishedOn": "2021-05-23T06:08:16.936Z",
          "wordCount": 588,
          "title": "Unsupervised Cross-Domain Prerequisite Chain Learning using Variational Graph Autoencoders. (arXiv:2105.03505v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengbao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araki_J/0/1/0/all/0/1\">Jun Araki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Haibo Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "Recent works have shown that language models (LM) capture different types of\nknowledge regarding facts or common sense. However, because no model is\nperfect, they still fail to provide appropriate answers in many cases. In this\npaper, we ask the question \"how can we know when language models know, with\nconfidence, the answer to a particular query?\" We examine this question from\nthe point of view of calibration, the property of a probabilistic model's\npredicted probabilities actually being well correlated with the probabilities\nof correctness. We examine three strong generative models -- T5, BART, and\nGPT-2 -- and study whether their probabilities on QA tasks are well calibrated,\nfinding the answer is a relatively emphatic no. We then examine methods to\ncalibrate such models to make their confidence scores correlate better with the\nlikelihood of correctness through fine-tuning, post-hoc probability\nmodification, or adjustment of the predicted outputs or inputs. Experiments on\na diverse range of datasets demonstrate the effectiveness of our methods. We\nalso perform analysis to study the strengths and limitations of these methods,\nshedding light on further improvements that may be made in methods for\ncalibrating LMs. We have released the code at\nhttps://github.com/jzbjyb/lm-calibration.",
          "link": "http://arxiv.org/abs/2012.00955",
          "publishedOn": "2021-05-23T06:08:16.916Z",
          "wordCount": 672,
          "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering. (arXiv:2012.00955v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Can Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Guocheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>",
          "description": "Existed pre-training methods either focus on single-modal tasks or\nmulti-modal tasks, and cannot effectively adapt to each other. They can only\nutilize single-modal data (i.e. text or image) or limited multi-modal data\n(i.e. image-text pairs). In this work, we propose a unified-modal pre-training\narchitecture, namely UNIMO, which can effectively adapt to both single-modal\nand multi-modal understanding and generation tasks. Large scale of free text\ncorpus and image collections can be utilized to improve the capability of\nvisual and textual understanding, and cross-modal contrastive learning (CMCL)\nis leveraged to align the textual and visual information into a unified\nsemantic space over a corpus of image-text pairs. As the non-paired\nsingle-modal data is very rich, our model can utilize much larger scale of data\nto learn more generalizable representations. Moreover, the textual knowledge\nand visual knowledge can enhance each other in the unified semantic space. The\nexperimental results show that UNIMO significantly improves the performance of\nseveral single-modal and multi-modal downstream tasks. Our code and pre-trained\nmodels are public at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO",
          "link": "http://arxiv.org/abs/2012.15409",
          "publishedOn": "2021-05-23T06:08:16.907Z",
          "wordCount": 665,
          "title": "UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning. (arXiv:2012.15409v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1\">Mohit Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pailla_D/0/1/0/all/0/1\">Dheeraj Pailla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_H/0/1/0/all/0/1\">Himanshu Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchawala_A/0/1/0/all/0/1\">Aadilmehdi Sanchawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1\">Manish Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1\">Ponnurangam Kumaraguru</a>",
          "description": "The exponential rise of online social media has enabled the creation,\ndistribution, and consumption of information at an unprecedented rate. However,\nit has also led to the burgeoning of various forms of online abuse. Increasing\ncases of online antisemitism have become one of the major concerns because of\nits socio-political consequences. Unlike other major forms of online abuse like\nracism, sexism, etc., online antisemitism has not been studied much from a\nmachine learning perspective. To the best of our knowledge, we present the\nfirst work in the direction of automated multimodal detection of online\nantisemitism. The task poses multiple challenges that include extracting\nsignals across multiple modalities, contextual references, and handling\nmultiple aspects of antisemitism. Unfortunately, there does not exist any\npublicly available benchmark corpus for this critical task. Hence, we collect\nand label two datasets with 3,102 and 3,509 social media posts from Twitter and\nGab respectively. Further, we present a multimodal deep learning system that\ndetects the presence of antisemitic content and its specific antisemitism\ncategory using text and images from posts. We perform an extensive set of\nexperiments on the two datasets to evaluate the efficacy of the proposed\nsystem. Finally, we also present a qualitative analysis of our study.",
          "link": "http://arxiv.org/abs/2104.05947",
          "publishedOn": "2021-05-23T06:08:16.899Z",
          "wordCount": 668,
          "title": "\"Subverting the Jewtocracy\": Online Antisemitism Detection Using Multimodal Deep Learning. (arXiv:2104.05947v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hazra_R/0/1/0/all/0/1\">Rishi Hazra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_S/0/1/0/all/0/1\">Sonu Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1\">Sayambhu Sen</a>",
          "description": "Human language has been described as a system that makes \\textit{use of\nfinite means to express an unlimited array of thoughts}. Of particular interest\nis the aspect of compositionality, whereby, the meaning of a compound language\nexpression can be deduced from the meaning of its constituent parts. If\nartificial agents can develop compositional communication protocols akin to\nhuman language, they can be made to seamlessly generalize to unseen\ncombinations. However, the real question is, how do we induce compositionality\nin emergent communication? Studies have recognized the role of curiosity in\nenabling linguistic development in children. It is this same intrinsic urge\nthat drives us to master complex tasks with decreasing amounts of explicit\nreward. In this paper, we seek to use this intrinsic feedback in inducing a\nsystematic and unambiguous protolanguage in artificial agents. We show how\nthese rewards can be leveraged in training agents to induce compositionality in\nabsence of any external feedback. Additionally, we introduce gComm, an\nenvironment for investigating grounded language acquisition in 2D-grid\nenvironments. Using this, we demonstrate how compositionality can enable agents\nto not only interact with unseen objects but also transfer skills from one task\nto another in a zero-shot setting: \\textit{Can an agent, trained to `pull' and\n`push twice', `pull twice'?}.",
          "link": "http://arxiv.org/abs/2012.05011",
          "publishedOn": "2021-05-23T06:08:16.830Z",
          "wordCount": 674,
          "title": "Infinite use of finite means: Zero-Shot Generalization using Compositional Emergent Protocols. (arXiv:2012.05011v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1\">Jan Christian Blaise Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resabal_J/0/1/0/all/0/1\">Jose Kristian Resabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">James Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velasco_D/0/1/0/all/0/1\">Dan John Velasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Charibeth Cheng</a>",
          "description": "Transformers represent the state-of-the-art in Natural Language Processing\n(NLP) in recent years, proving effective even in tasks done in low-resource\nlanguages. While pretrained transformers for these languages can be made, it is\nchallenging to measure their true performance and capacity due to the lack of\nhard benchmark datasets, as well as the difficulty and cost of producing them.\nIn this paper, we present three contributions: First, we propose a methodology\nfor automatically producing Natural Language Inference (NLI) benchmark datasets\nfor low-resource languages using published news articles. Through this, we\ncreate and release NewsPH-NLI, the first sentence entailment benchmark dataset\nin the low-resource Filipino language. Second, we produce new pretrained\ntransformers based on the ELECTRA technique to further alleviate the resource\nscarcity in Filipino, benchmarking them on our dataset against other\ncommonly-used transfer learning techniques. Lastly, we perform analyses on\ntransfer learning techniques to shed light on their true performance when\noperating in low-data domains through the use of degradation tests.",
          "link": "http://arxiv.org/abs/2010.11574",
          "publishedOn": "2021-05-23T06:08:16.822Z",
          "wordCount": 649,
          "title": "Exploiting News Article Structure for Automatic Corpus Generation. (arXiv:2010.11574v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1\">Mantas Mazeika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Akul Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_E/0/1/0/all/0/1\">Ethan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burns_C/0/1/0/all/0/1\">Collin Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puranik_S/0/1/0/all/0/1\">Samir Puranik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Horace He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>",
          "description": "While programming is one of the most broadly applicable skills in modern\nsociety, modern machine learning models still cannot code solutions to basic\nproblems. It can be difficult to accurately assess code generation performance,\nand there has been surprisingly little work on evaluating code generation in a\nway that is both flexible and rigorous. To meet this challenge, we introduce\nAPPS, a benchmark for code generation. Unlike prior work in more restricted\nsettings, our benchmark measures the ability of models to take an arbitrary\nnatural language specification and generate Python code fulfilling this\nspecification. Similar to how companies assess candidate software developers,\nwe then evaluate models by checking their generated code on test cases. Our\nbenchmark includes 10,000 problems, which range from having simple one-line\nsolutions to being substantial algorithmic challenges. We fine-tune large\nlanguage models on both GitHub and our training set, and we find that the\nprevalence of syntax errors is decreasing exponentially. Recent models such as\nGPT-Neo can pass approximately 15% of the test cases of introductory problems,\nso we find that machine learning models are beginning to learn how to code. As\nthe social significance of automatic code generation increases over the coming\nyears, our benchmark can provide an important measure for tracking\nadvancements.",
          "link": "http://arxiv.org/abs/2105.09938",
          "publishedOn": "2021-05-23T06:08:16.792Z",
          "wordCount": 662,
          "title": "Measuring Coding Challenge Competence With APPS. (arXiv:2105.09938v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2006.16362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cordonnier_J/0/1/0/all/0/1\">Jean-Baptiste Cordonnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loukas_A/0/1/0/all/0/1\">Andreas Loukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>",
          "description": "Attention layers are widely used in natural language processing (NLP) and are\nbeginning to influence computer vision architectures. Training very large\ntransformer models allowed significant improvement in both fields, but once\ntrained, these networks show symptoms of over-parameterization. For instance,\nit is known that many attention heads can be pruned without impacting accuracy.\nThis work aims to enhance current understanding on how multiple heads interact.\nMotivated by the observation that attention heads learn redundant key/query\nprojections, we propose a collaborative multi-head attention layer that enables\nheads to learn shared projections. Our scheme decreases the number of\nparameters in an attention layer and can be used as a drop-in replacement in\nany transformer architecture. Our experiments confirm that sharing key/query\ndimensions can be exploited in language understanding, machine translation and\nvision. We also show that it is possible to re-parametrize a pre-trained\nmulti-head attention layer into our collaborative attention layer.\nCollaborative multi-head attention reduces the size of the key and query\nprojections by 4 for same accuracy and speed. Our code is public.",
          "link": "http://arxiv.org/abs/2006.16362",
          "publishedOn": "2021-05-23T06:08:16.782Z",
          "wordCount": 628,
          "title": "Multi-Head Attention: Collaborate Instead of Concatenate. (arXiv:2006.16362v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Michalopoulos_G/0/1/0/all/0/1\">George Michalopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuanxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaka_H/0/1/0/all/0/1\">Hussam Kaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Helen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>",
          "description": "Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have\nachieved state-of-the-art results in biomedical natural language processing\ntasks by focusing their pre-training process on domain-specific corpora.\nHowever, such models do not take into consideration expert domain knowledge.\n\nIn this work, we introduced UmlsBERT, a contextual embedding model that\nintegrates domain knowledge during the pre-training process via a novel\nknowledge augmentation strategy. More specifically, the augmentation on\nUmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus was\nperformed in two ways: i) connecting words that have the same underlying\n`concept' in UMLS, and ii) leveraging semantic group knowledge in UMLS to\ncreate clinically meaningful input embeddings. By applying these two\nstrategies, UmlsBERT can encode clinical domain knowledge into word embeddings\nand outperform existing domain-specific models on common named-entity\nrecognition (NER) and clinical natural language inference clinical NLP tasks.",
          "link": "http://arxiv.org/abs/2010.10391",
          "publishedOn": "2021-05-23T06:08:16.766Z",
          "wordCount": 642,
          "title": "UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus. (arXiv:2010.10391v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.07414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nejadgholi_I/0/1/0/all/0/1\">Isar Nejadgholi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiritchenko_S/0/1/0/all/0/1\">Svetlana Kiritchenko</a>",
          "description": "NLP research has attained high performances in abusive language detection as\na supervised classification task. While in research settings, training and test\ndatasets are usually obtained from similar data samples, in practice systems\nare often applied on data that are different from the training set in topic and\nclass distributions. Also, the ambiguity in class definitions inherited in this\ntask aggravates the discrepancies between source and target datasets. We\nexplore the topic bias and the task formulation bias in cross-dataset\ngeneralization. We show that the benign examples in the Wikipedia Detox dataset\nare biased towards platform-specific topics. We identify these examples using\nunsupervised topic modeling and manual inspection of topics' keywords. Removing\nthese topics increases cross-dataset generalization, without reducing in-domain\nclassification performance. For a robust dataset design, we suggest applying\ninexpensive unsupervised methods to inspect the collected data and downsize the\nnon-generalizable content before manually annotating for class labels.",
          "link": "http://arxiv.org/abs/2010.07414",
          "publishedOn": "2021-05-23T06:08:16.758Z",
          "wordCount": 628,
          "title": "On Cross-Dataset Generalization in Automatic Detection of Online Abuse. (arXiv:2010.07414v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.06269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hettiarachchi_H/0/1/0/all/0/1\">Hansi Hettiarachchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>",
          "description": "This paper presents the team BRUMS submission to SemEval-2020 Task 3: Graded\nWord Similarity in Context. The system utilises state-of-the-art contextualised\nword embeddings, which have some task-specific adaptations, including stacked\nembeddings and average embeddings. Overall, the approach achieves good\nevaluation scores across all the languages, while maintaining simplicity.\nFollowing the final rankings, our approach is ranked within the top 5 solutions\nof each language while preserving the 1st position of Finnish subtask 2.",
          "link": "http://arxiv.org/abs/2010.06269",
          "publishedOn": "2021-05-23T06:08:16.745Z",
          "wordCount": 556,
          "title": "BRUMS at SemEval-2020 Task 3: Contextualised Embeddings for Predicting the (Graded) Effect of Context in Word Similarity. (arXiv:2010.06269v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sodhi_S/0/1/0/all/0/1\">Sukhdeep S. Sodhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chio_E/0/1/0/all/0/1\">Ellie Ka-In Chio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jash_A/0/1/0/all/0/1\">Ambarish Jash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apte_A/0/1/0/all/0/1\">Ajit Apte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ankit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeje_A/0/1/0/all/0/1\">Ayooluwakunmi Jeje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuzmin_D/0/1/0/all/0/1\">Dima Kuzmin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_H/0/1/0/all/0/1\">Harry Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Heng-Tze Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Effrat_J/0/1/0/all/0/1\">Jon Effrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bali_T/0/1/0/all/0/1\">Tarush Bali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jindal_N/0/1/0/all/0/1\">Nitin Jindal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1\">Pei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sarvjeet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Senqiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1\">Tameen Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wankhede_A/0/1/0/all/0/1\">Amol Wankhede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alzantot_M/0/1/0/all/0/1\">Moustafa Alzantot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Allen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_T/0/1/0/all/0/1\">Tushar Chandra</a>",
          "description": "As more and more online search queries come from voice, automatic speech\nrecognition becomes a key component to deliver relevant search results. Errors\nintroduced by automatic speech recognition (ASR) lead to irrelevant search\nresults returned to the user, thus causing user dissatisfaction. In this paper,\nwe introduce an approach, Mondegreen, to correct voice queries in text space\nwithout depending on audio signals, which may not always be available due to\nsystem constraints or privacy or bandwidth (for example, some ASR systems run\non-device) considerations. We focus on voice queries transcribed via several\nproprietary commercial ASR systems. These queries come from users making\ninternet, or online service search queries. We first present an analysis\nshowing how different the language distribution coming from user voice queries\nis from that in traditional text corpora used to train off-the-shelf ASR\nsystems. We then demonstrate that Mondegreen can achieve significant\nimprovements in increased user interaction by correcting user voice queries in\none of the largest search systems in Google. Finally, we see Mondegreen as\ncomplementing existing highly-optimized production ASR systems, which may not\nbe frequently retrained and thus lag behind due to vocabulary drifts.",
          "link": "http://arxiv.org/abs/2105.09930",
          "publishedOn": "2021-05-23T06:08:16.721Z",
          "wordCount": 678,
          "title": "Mondegreen: A Post-Processing Solution to Speech Recognition Error Correction for Voice Search Queries. (arXiv:2105.09930v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2005.01107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lopez_L/0/1/0/all/0/1\">Luis Enrico Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_D/0/1/0/all/0/1\">Diane Kathryn Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1\">Jan Christian Blaise Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Charibeth Cheng</a>",
          "description": "Question generation (QG) is a natural language generation task where a model\nis trained to ask questions corresponding to some input text. Most recent\napproaches frame QG as a sequence-to-sequence problem and rely on additional\nfeatures and mechanisms to increase performance; however, these often increase\nmodel complexity, and can rely on auxiliary data unavailable in practical use.\nA single Transformer-based unidirectional language model leveraging transfer\nlearning can be used to produce high quality questions while disposing of\nadditional task-specific complexity. Our QG model, finetuned from GPT-2 Small,\noutperforms several paragraph-level QG baselines on the SQuAD dataset by 0.95\nMETEOR points. Human evaluators rated questions as easy to answer, relevant to\ntheir context paragraph, and corresponding well to natural human speech. Also\nintroduced is a new set of baseline scores on the RACE dataset, which has not\npreviously been used for QG tasks. Further experimentation with varying model\ncapacities and datasets with non-identification type questions is recommended\nin order to further verify the robustness of pretrained Transformer-based LMs\nas question generators.",
          "link": "http://arxiv.org/abs/2005.01107",
          "publishedOn": "2021-05-23T06:08:16.708Z",
          "wordCount": 646,
          "title": "Simplifying Paragraph-level Question Generation via Transformer Language Models. (arXiv:2005.01107v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09867",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scontras_G/0/1/0/all/0/1\">Gregory Scontras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tessler_M/0/1/0/all/0/1\">Michael Henry Tessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franke_M/0/1/0/all/0/1\">Michael Franke</a>",
          "description": "Recent advances in computational cognitive science (i.e., simulation-based\nprobabilistic programs) have paved the way for significant progress in formal,\nimplementable models of pragmatics. Rather than describing a pragmatic\nreasoning process in prose, these models formalize and implement one, deriving\nboth qualitative and quantitative predictions of human behavior -- predictions\nthat consistently prove correct, demonstrating the viability and value of the\nframework. The current paper provides a practical introduction to and critical\nassessment of the Bayesian Rational Speech Act modeling framework, unpacking\ntheoretical foundations, exploring technological innovations, and drawing\nconnections to issues beyond current applications.",
          "link": "http://arxiv.org/abs/2105.09867",
          "publishedOn": "2021-05-23T06:08:16.699Z",
          "wordCount": 522,
          "title": "A practical introduction to the Rational Speech Act modeling framework. (arXiv:2105.09867v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1\">Patrick Lumban Tobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "This paper presents a low-latency real-time (LLRT) non-parallel voice\nconversion (VC) framework based on cyclic variational autoencoder (CycleVAE)\nand multiband WaveRNN with data-driven linear prediction (MWDLP). CycleVAE is a\nrobust non-parallel multispeaker spectral model, which utilizes a\nspeaker-independent latent space and a speaker-dependent code to generate\nreconstructed/converted spectral features given the spectral features of an\ninput speaker. On the other hand, MWDLP is an efficient and a high-quality\nneural vocoder that can handle multispeaker data and generate speech waveform\nfor LLRT applications with CPU. To accommodate LLRT constraint with CPU, we\npropose a novel CycleVAE framework that utilizes mel-spectrogram as spectral\nfeatures and is built with a sparse network architecture. Further, to improve\nthe modeling performance, we also propose a novel fine-tuning procedure that\nrefines the frame-rate CycleVAE network by utilizing the waveform loss from the\nMWDLP network. The experimental results demonstrate that the proposed framework\nachieves high-performance VC, while allowing for LLRT usage with a single-core\nof $2.1$--$2.7$~GHz CPU on a real-time factor of $0.87$--$0.95$, including\ninput/output, feature extraction, on a frame shift of $10$ ms, a window length\nof $27.5$ ms, and $2$ lookup frames.",
          "link": "http://arxiv.org/abs/2105.09858",
          "publishedOn": "2021-05-23T06:08:16.671Z",
          "wordCount": 646,
          "title": "Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear Prediction. (arXiv:2105.09858v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1\">Patrick Lumban Tobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "This paper presents a novel high-fidelity and low-latency universal neural\nvocoder framework based on multiband WaveRNN with data-driven linear prediction\nfor discrete waveform modeling (MWDLP). MWDLP employs a coarse-fine bit WaveRNN\narchitecture for 10-bit mu-law waveform modeling. A sparse gated recurrent unit\nwith a relatively large size of hidden units is utilized, while the multiband\nmodeling is deployed to achieve real-time low-latency usage. A novel technique\nfor data-driven linear prediction (LP) with discrete waveform modeling is\nproposed, where the LP coefficients are estimated in a data-driven manner.\nMoreover, a novel loss function using short-time Fourier transform (STFT) for\ndiscrete waveform modeling with Gumbel approximation is also proposed. The\nexperimental results demonstrate that the proposed MWDLP framework generates\nhigh-fidelity synthetic speech for seen and unseen speakers and/or language on\n300 speakers training data including clean and noisy/reverberant conditions,\nwhere the number of training utterances is limited to 60 per speaker, while\nallowing for real-time low-latency processing using a single core of $\\sim\\!$\n2.1--2.7~GHz CPU with $\\sim\\!$ 0.57--0.64 real-time factor including\ninput/output and feature extraction.",
          "link": "http://arxiv.org/abs/2105.09856",
          "publishedOn": "2021-05-23T06:08:16.662Z",
          "wordCount": 635,
          "title": "High-Fidelity and Low-Latency Universal Neural Vocoder based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling. (arXiv:2105.09856v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1\">Sebastian Hofst&#xe4;tter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_B/0/1/0/all/0/1\">Bhaskar Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Craswell_N/0/1/0/all/0/1\">Nick Craswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>",
          "description": "An emerging recipe for achieving state-of-the-art effectiveness in neural\ndocument re-ranking involves utilizing large pre-trained language models -\ne.g., BERT - to evaluate all individual passages in the document and then\naggregating the outputs by pooling or additional Transformer layers. A major\ndrawback of this approach is high query latency due to the cost of evaluating\nevery passage in the document with BERT. To make matters worse, this high\ninference cost and latency varies based on the length of the document, with\nlonger documents requiring more time and computation. To address this\nchallenge, we adopt an intra-document cascading strategy, which prunes passages\nof a candidate document using a less expensive model, called ESM, before\nrunning a scoring model that is more expensive and effective, called ETM. We\nfound it best to train ESM (short for Efficient Student Model) via knowledge\ndistillation from the ETM (short for Effective Teacher Model) e.g., BERT. This\npruning allows us to only run the ETM model on a smaller set of passages whose\nsize does not vary by document length. Our experiments on the MS MARCO and TREC\nDeep Learning Track benchmarks suggest that the proposed Intra-Document\nCascaded Ranking Model (IDCM) leads to over 400% lower query latency by\nproviding essentially the same effectiveness as the state-of-the-art BERT-based\ndocument ranking models.",
          "link": "http://arxiv.org/abs/2105.09816",
          "publishedOn": "2021-05-23T06:08:16.636Z",
          "wordCount": 659,
          "title": "Intra-Document Cascading: Learning to Select Passages for Neural Document Ranking. (arXiv:2105.09816v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Junru Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parnow_K/0/1/0/all/0/1\">Kevin Parnow</a>",
          "description": "Constituent and dependency parsing, the two classic forms of syntactic\nparsing, have been found to benefit from joint training and decoding under a\nuniform formalism, Head-driven Phrase Structure Grammar (HPSG). However,\ndecoding this unified grammar has a higher time complexity ($O(n^5)$) than\ndecoding either form individually ($O(n^3)$) since more factors have to be\nconsidered during decoding. We thus propose an improved head scorer that helps\nachieve a novel performance-preserved parser in $O$($n^3$) time complexity.\nFurthermore, on the basis of this proposed practical HPSG parser, we\ninvestigated the strengths of HPSG-based parsing and explored the general\nmethod of training an HPSG-based parser from only a constituent or dependency\nannotations in a multilingual scenario. We thus present a more effective, more\nin-depth, and general work on HPSG parsing.",
          "link": "http://arxiv.org/abs/2105.09835",
          "publishedOn": "2021-05-23T06:08:16.627Z",
          "wordCount": 552,
          "title": "Head-driven Phrase Structure Parsing in O($n^3$) Time Complexity. (arXiv:2105.09835v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungjoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jihyung Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1\">Won Ik Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiyoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jangwon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chisung Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junseong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yongsook Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Taehwan Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joohong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Juhyun Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Sungwon Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Younghoon Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Inkwon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Sangwoo Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongjun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Myeonghwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1\">Seongbo Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_S/0/1/0/all/0/1\">Seungwon Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunkyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">Kyungtae Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jongwon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyumin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jamin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seonghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_L/0/1/0/all/0/1\">Lucy Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jungwoo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho Alice Oh Jungwoo Ha Kyunghyun Cho</a>",
          "description": "We introduce Korean Language Understanding Evaluation (KLUE) benchmark. KLUE\nis a collection of 8 Korean natural language understanding (NLU) tasks,\nincluding Topic Classification, Semantic Textual Similarity, Natural Language\nInference, Named Entity Recognition, Relation Extraction, Dependency Parsing,\nMachine Reading Comprehension, and Dialogue State Tracking. We build all of the\ntasks from scratch from diverse source corpora while respecting copyrights, to\nensure accessibility for anyone without any restrictions. With ethical\nconsiderations in mind, we carefully design annotation protocols. Along with\nthe benchmark tasks and data, we provide suitable evaluation metrics and\nfine-tuning recipes for pretrained language models for each task. We\nfurthermore release the pretrained language models (PLM), KLUE-BERT and\nKLUE-RoBERTa, to help reproduce baseline models on KLUE and thereby facilitate\nfuture research. We make a few interesting observations from the preliminary\nexperiments using the proposed KLUE benchmark suite, already demonstrating the\nusefulness of this new benchmark suite. First, we find KLUE-RoBERTa-large\noutperforms other baselines, including multilingual PLMs and existing\nopen-source Korean PLMs. Second, we see minimal degradation in performance even\nwhen we replace personally identifiable information from the pretraining\ncorpus, suggesting that privacy and NLU capability are not at odds with each\nother. Lastly, we find that using BPE tokenization in combination with\nmorpheme-level pre-tokenization is effective in tasks involving morpheme-level\ntagging, detection and generation. In addition to accelerating Korean NLP\nresearch, our comprehensive documentation on creating KLUE will facilitate\ncreating similar resources for other languages in the future. KLUE is available\nat this https URL (https://klue-benchmark.com/).",
          "link": "http://arxiv.org/abs/2105.09680",
          "publishedOn": "2021-05-23T06:08:16.503Z",
          "wordCount": 735,
          "title": "KLUE: Korean Language Understanding Evaluation. (arXiv:2105.09680v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_K/0/1/0/all/0/1\">Keyue Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuzhuo Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhiyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Distantly supervised (DS) relation extraction (RE) has attracted much\nattention in the past few years as it can utilize large-scale auto-labeled\ndata. However, its evaluation has long been a problem: previous works either\ntook costly and inconsistent methods to manually examine a small sample of\nmodel predictions, or directly test models on auto-labeled data -- which, by\nour check, produce as much as 53% wrong labels at the entity pair level in the\npopular NYT10 dataset. This problem has not only led to inaccurate evaluation,\nbut also made it hard to understand where we are and what's left to improve in\nthe research of DS-RE. To evaluate DS-RE models in a more credible way, we\nbuild manually-annotated test sets for two DS-RE datasets, NYT10 and Wiki20,\nand thoroughly evaluate several competitive models, especially the latest\npre-trained ones. The experimental results show that the manual evaluation can\nindicate very different conclusions from automatic ones, especially some\nunexpected observations, e.g., pre-trained models can achieve dominating\nperformance while being more susceptible to false-positives compared to\nprevious methods. We hope that both our manual test sets and novel observations\ncan help advance future DS-RE research.",
          "link": "http://arxiv.org/abs/2105.09543",
          "publishedOn": "2021-05-23T06:08:16.493Z",
          "wordCount": 643,
          "title": "Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction. (arXiv:2105.09543v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09660",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1\">Felix Hamborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donnay_K/0/1/0/all/0/1\">Karsten Donnay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>",
          "description": "Extensive research on target-dependent sentiment classification (TSC) has led\nto strong classification performances in domains where authors tend to\nexplicitly express sentiment about specific entities or topics, such as in\nreviews or on social media. We investigate TSC in news articles, a much less\nresearched domain, despite the importance of news as an essential information\nsource in individual and societal decision making. This article introduces\nNewsTSC, a manually annotated dataset to explore TSC on news articles.\nInvestigating characteristics of sentiment in news and contrasting them to\npopular TSC domains, we find that sentiment in the news is expressed less\nexplicitly, is more dependent on context and readership, and requires a greater\ndegree of interpretation. In an extensive evaluation, we find that the state of\nthe art in TSC performs worse on news articles than on other domains (average\nrecall AvgRec = 69.8 on NewsTSC compared to AvgRev = [75.6, 82.2] on\nestablished TSC datasets). Reasons include incorrectly resolved relation of\ntarget and sentiment-bearing phrases and off-context dependence. As a major\nimprovement over previous news TSC, we find that BERT's natural language\nunderstanding capabilities capture the less explicit sentiment used in news\narticles.",
          "link": "http://arxiv.org/abs/2105.09660",
          "publishedOn": "2021-05-23T06:08:16.482Z",
          "wordCount": 621,
          "title": "Towards Target-dependent Sentiment Classification in News Articles. (arXiv:2105.09660v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Profitlich_H/0/1/0/all/0/1\">Hans-J&#xfc;rgen Profitlich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1\">Daniel Sonntag</a>",
          "description": "We describe our work on information extraction in medical documents written\nin German, especially detecting negations using an architecture based on the\nUIMA pipeline. Based on our previous work on software modules to cover medical\nconcepts like diagnoses, examinations, etc. we employ a version of the NegEx\nregular expression algorithm with a large set of triggers as a baseline. We\nshow how a significantly smaller trigger set is sufficient to achieve similar\nresults, in order to reduce adaptation times to new text types. We elaborate on\nthe question whether dependency parsing (based on the Stanford CoreNLP model)\nis a good alternative and describe the potentials and shortcomings of both\napproaches.",
          "link": "http://arxiv.org/abs/2105.09702",
          "publishedOn": "2021-05-23T06:08:16.455Z",
          "wordCount": 571,
          "title": "A Case Study on Pros and Cons of Regular Expression Detection and Dependency Parsing for Negation Extraction from German Medical Documents. Technical Report. (arXiv:2105.09702v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09742",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Aashish Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zesch_T/0/1/0/all/0/1\">Torsten Zesch</a>",
          "description": "When evaluating the performance of automatic speech recognition models,\nusually word error rate within a certain dataset is used. Special care must be\ntaken in understanding the dataset in order to report realistic performance\nnumbers. We argue that many performance numbers reported probably underestimate\nthe expected error rate. We conduct experiments controlling for selection bias,\ngender as well as overlap (between training and test data) in content, voices,\nand recording conditions. We find that content overlap has the biggest impact,\nbut other factors like gender also play a role.",
          "link": "http://arxiv.org/abs/2105.09742",
          "publishedOn": "2021-05-23T06:08:16.444Z",
          "wordCount": 532,
          "title": "Robustness of end-to-end Automatic Speech Recognition Models -- A Case Study using Mozilla DeepSpeech. (arXiv:2105.09742v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1\">P. Gloor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1\">A. Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giacomelli_G/0/1/0/all/0/1\">G. Giacomelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saran_T/0/1/0/all/0/1\">T. Saran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grippa_F/0/1/0/all/0/1\">F. Grippa</a>",
          "description": "We investigate the impact of a novel method called \"virtual mirroring\" to\npromote employee self-reflection and impact customer satisfaction. The method\nis based on measuring communication patterns, through social network and\nsemantic analysis, and mirroring them back to the individual. Our goal is to\ndemonstrate that self-reflection can trigger a change in communication\nbehaviors, which lead to increased customer satisfaction. We illustrate and\ntest our approach analyzing e-mails of a large global services company by\ncomparing changes in customer satisfaction associated with team leaders exposed\nto virtual mirroring (the experimental group). We find an increase in customer\nsatisfaction in the experimental group and a decrease in the control group\n(team leaders not involved in the virtual mirroring process). With regard to\nthe individual communication indicators, we find that customer satisfaction is\nhigher when employees are more responsive, use a simpler language, are embedded\nin less centralized communication networks, and show more stable leadership\npatterns.",
          "link": "http://arxiv.org/abs/2105.09571",
          "publishedOn": "2021-05-23T06:08:16.435Z",
          "wordCount": 611,
          "title": "The impact of virtual mirroring on customer satisfaction. (arXiv:2105.09571v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lenci_A/0/1/0/all/0/1\">Alessandro Lenci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahlgren_M/0/1/0/all/0/1\">Magnus Sahlgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeuniaux_P/0/1/0/all/0/1\">Patrick Jeuniaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyllensten_A/0/1/0/all/0/1\">Amaru Cuba Gyllensten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miliani_M/0/1/0/all/0/1\">Martina Miliani</a>",
          "description": "Distributional semantics has deeply changed in the last decades. First,\npredict models stole the thunder from traditional count ones, and more recently\nboth of them were replaced in many NLP applications by contextualized vectors\nproduced by Transformer neural language models. Although an extensive body of\nresearch has been devoted to Distributional Semantic Model (DSM) evaluation, we\nstill lack a thorough comparison with respect to tested models, semantic tasks,\nand benchmark datasets. Moreover, previous work has mostly focused on\ntask-driven evaluation, instead of exploring the differences between the way\nmodels represent the lexical semantic space. In this paper, we perform a\ncomprehensive evaluation of type distributional vectors, either produced by\nstatic DSMs or obtained by averaging the contextualized vectors generated by\nBERT. First of all, we investigate the performance of embeddings in several\nsemantic tasks, carrying out an in-depth statistical analysis to identify the\nmajor factors influencing the behavior of DSMs. The results show that i.) the\nalleged superiority of predict based models is more apparent than real, and\nsurely not ubiquitous and ii.) static DSMs surpass contextualized\nrepresentations in most out-of-context semantic tasks and datasets.\nFurthermore, we borrow from cognitive neuroscience the methodology of\nRepresentational Similarity Analysis (RSA) to inspect the semantic spaces\ngenerated by distributional models. RSA reveals important differences related\nto the frequency and part-of-speech of lexical items.",
          "link": "http://arxiv.org/abs/2105.09825",
          "publishedOn": "2021-05-23T06:08:16.424Z",
          "wordCount": 663,
          "title": "A comprehensive comparative evaluation and analysis of Distributional Semantic Models. (arXiv:2105.09825v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dessi_D/0/1/0/all/0/1\">Danilo Dessi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helaoui_R/0/1/0/all/0/1\">Rim Helaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vivek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1\">Diego Reforgiato Recupero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riboni_D/0/1/0/all/0/1\">Daniele Riboni</a>",
          "description": "Today, we are seeing an ever-increasing number of clinical notes that contain\nclinical results, images, and textual descriptions of patient's health state.\nAll these data can be analyzed and employed to cater novel services that can\nhelp people and domain experts with their common healthcare tasks. However,\nmany technologies such as Deep Learning and tools like Word Embeddings have\nstarted to be investigated only recently, and many challenges remain open when\nit comes to healthcare domain applications. To address these challenges, we\npropose the use of Deep Learning and Word Embeddings for identifying sixteen\nmorbidity types within textual descriptions of clinical records. For this\npurpose, we have used a Deep Learning model based on Bidirectional Long-Short\nTerm Memory (LSTM) layers which can exploit state-of-the-art vector\nrepresentations of data such as Word Embeddings. We have employed pre-trained\nWord Embeddings namely GloVe and Word2Vec, and our own Word Embeddings trained\non the target domain. Furthermore, we have compared the performances of the\ndeep learning approaches against the traditional tf-idf using Support Vector\nMachine and Multilayer perceptron (our baselines). From the obtained results it\nseems that the latter outperforms the combination of Deep Learning approaches\nusing any word embeddings. Our preliminary results indicate that there are\nspecific features that make the dataset biased in favour of traditional machine\nlearning approaches.",
          "link": "http://arxiv.org/abs/2105.09632",
          "publishedOn": "2021-05-23T06:08:16.414Z",
          "wordCount": 680,
          "title": "TF-IDF vs Word Embeddings for Morbidity Identification in Clinical Notes: An Initial Study. (arXiv:2105.09632v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zixiu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helaoui_R/0/1/0/all/0/1\">Rim Helaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vivek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1\">Diego Reforgiato Recupero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riboni_D/0/1/0/all/0/1\">Daniele Riboni</a>",
          "description": "Empathetic response from the therapist is key to the success of clinical\npsychotherapy, especially motivational interviewing. Previous work on\ncomputational modelling of empathy in motivational interviewing has focused on\noffline, session-level assessment of therapist empathy, where empathy captures\nall efforts that the therapist makes to understand the client's perspective and\nconvey that understanding to the client. In this position paper, we propose a\nnovel task of turn-level detection of client need for empathy. Concretely, we\npropose to leverage pre-trained language models and empathy-related general\nconversation corpora in a unique labeller-detector framework, where the\nlabeller automatically annotates a motivational interviewing conversation\ncorpus with empathy labels to train the detector that determines the need for\ntherapist empathy. We also lay out our strategies of extending the detector\nwith additional-input and multi-task setups to improve its detection and\nexplainability.",
          "link": "http://arxiv.org/abs/2105.09649",
          "publishedOn": "2021-05-23T06:08:16.391Z",
          "wordCount": 586,
          "title": "Towards Detecting Need for Empathetic Response in Motivational Interviewing. (arXiv:2105.09649v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09458",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lou_D/0/1/0/all/0/1\">Dongfang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1\">Zhilin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>",
          "description": "We consider the problem of collectively detecting multiple events,\nparticularly in cross-sentence settings. The key to dealing with the problem is\nto encode semantic information and model event inter-dependency at a\ndocument-level. In this paper, we reformulate it as a Seq2Seq task and propose\na Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level\nassociation of events and semantic information simultaneously. Specifically, a\nbidirectional decoder is firstly devised to model event inter-dependency within\na sentence when decoding the event tag vector sequence. Secondly, an\ninformation aggregation module is employed to aggregate sentence-level semantic\nand event tag information. Finally, we stack multiple bidirectional decoders\nand feed cross-sentence information, forming a multi-layer bidirectional\ntagging architecture to iteratively propagate information across sentences. We\nshow that our approach provides significant improvement in performance compared\nto the current state-of-the-art results.",
          "link": "http://arxiv.org/abs/2105.09458",
          "publishedOn": "2021-05-23T06:08:16.366Z",
          "wordCount": 569,
          "title": "MLBiNet: A Cross-Sentence Collective Event Detection Network. (arXiv:2105.09458v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bestgen_Y/0/1/0/all/0/1\">Yves Bestgen</a>",
          "description": "This paper describes the system developed by the Laboratoire d'analyse\nstatistique des textes (LAST) for the Lexical Complexity Prediction shared task\nat SemEval-2021. The proposed system is made up of a LightGBM model fed with\nfeatures obtained from many word frequency lists, published lexical norms and\npsychometric data. For tackling the specificity of the multi-word task, it uses\nbigram association measures. Despite that the only contextual feature used was\nsentence length, the system achieved an honorable performance in the multi-word\ntask, but poorer in the single word task. The bigram association measures were\nfound useful, but to a limited extent.",
          "link": "http://arxiv.org/abs/2105.09653",
          "publishedOn": "2021-05-23T06:08:16.352Z",
          "wordCount": 533,
          "title": "LAST at SemEval-2021 Task 1: Improving Multi-Word Complexity Prediction Using Bigram Association Measures. (arXiv:2105.09653v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Existing multilingual machine translation approaches mainly focus on\nEnglish-centric directions, while the non-English directions still lag behind.\nIn this work, we aim to build a many-to-many translation system with an\nemphasis on the quality of non-English language directions. Our intuition is\nbased on the hypothesis that a universal cross-language representation leads to\nbetter multilingual translation performance. To this end, we propose \\method, a\ntraining method to obtain a single unified multilingual translation model.\nmCOLT is empowered by two techniques: (i) a contrastive learning scheme to\nclose the gap among representations of different languages, and (ii) data\naugmentation on both multiple parallel and monolingual data to further align\ntoken representations. For English-centric directions, mCOLT achieves\ncompetitive or even better performance than a strong pre-trained model mBART on\ntens of WMT benchmarks. For non-English directions, mCOLT achieves an\nimprovement of average 10+ BLEU compared with the multilingual baseline.",
          "link": "http://arxiv.org/abs/2105.09501",
          "publishedOn": "2021-05-23T06:08:16.338Z",
          "wordCount": 576,
          "title": "Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lahlou_C/0/1/0/all/0/1\">Chuhong Lahlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crayton_A/0/1/0/all/0/1\">Ancil Crayton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trier_C/0/1/0/all/0/1\">Caroline Trier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willett_E/0/1/0/all/0/1\">Evan Willett</a>",
          "description": "In 2019, The Centers for Medicare and Medicaid Services (CMS) launched an\nArtificial Intelligence (AI) Health Outcomes Challenge seeking solutions to\npredict risk in value-based care for incorporation into CMS Innovation Center\npayment and service delivery models. Recently, modern language models have\nplayed key roles in a number of health related tasks. This paper presents, to\nthe best of our knowledge, the first application of these models to patient\nreadmission prediction. To facilitate this, we create a dataset of 1.2 million\nmedical history samples derived from the Limited Dataset (LDS) issued by CMS.\nMoreover, we propose a comprehensive modeling solution centered on a deep\nlearning framework for this data. To demonstrate the framework, we train an\nattention-based Transformer to learn Medicare semantics in support of\nperforming downstream prediction tasks thereby achieving 0.91 AUC and 0.91\nrecall on readmission classification. We also introduce a novel data\npre-processing pipeline and discuss pertinent deployment considerations\nsurrounding model explainability and bias.",
          "link": "http://arxiv.org/abs/2105.09428",
          "publishedOn": "2021-05-23T06:08:16.214Z",
          "wordCount": 592,
          "title": "Explainable Health Risk Predictor with Transformer-based Medicare Claim Encoder. (arXiv:2105.09428v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09611",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Gonzalez_D/0/1/0/all/0/1\">Daniel Fern&#xe1;ndez-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>",
          "description": "Dependency parsing is a crucial step towards deep language understanding and,\ntherefore, widely demanded by numerous Natural Language Processing\napplications. In particular, left-to-right and top-down transition-based\nalgorithms that rely on Pointer Networks are among the most accurate approaches\nfor performing dependency parsing. Additionally, it has been observed for the\ntop-down algorithm that Pointer Networks' sequential decoding can be improved\nby implementing a hierarchical variant, more adequate to model dependency\nstructures. Considering all this, we develop a bottom-up-oriented Hierarchical\nPointer Network for the left-to-right parser and propose two novel\ntransition-based alternatives: an approach that parses a sentence in\nright-to-left order and a variant that does it from the outside in. We\nempirically test the proposed neural architecture with the different algorithms\non a wide variety of languages, outperforming the original approach in\npractically all of them and setting new state-of-the-art results on the English\nand Chinese Penn Treebanks for non-contextualized and BERT-based embeddings.",
          "link": "http://arxiv.org/abs/2105.09611",
          "publishedOn": "2021-05-23T06:08:16.202Z",
          "wordCount": 583,
          "title": "Dependency Parsing with Bottom-up Hierarchical Pointer Networks. (arXiv:2105.09611v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>",
          "description": "Neural network approaches have been applied to computational morphology with\ngreat success, improving the performance of most tasks by a large margin and\nproviding new perspectives for modeling. This paper starts with a brief\nintroduction to computational morphology, followed by a review of recent work\non computational morphology with neural network approaches, to provide an\noverview of the area. In the end, we will analyze the advantages and problems\nof neural network approaches to computational morphology, and point out some\ndirections to be explored by future research and study.",
          "link": "http://arxiv.org/abs/2105.09404",
          "publishedOn": "2021-05-23T06:08:16.168Z",
          "wordCount": 505,
          "title": "Computational Morphology with Neural Network Approaches. (arXiv:2105.09404v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atri_Y/0/1/0/all/0/1\">Yash Kumar Atri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1\">Shraman Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1\">Vikram Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>",
          "description": "In recent years, abstractive text summarization with multimodal inputs has\nstarted drawing attention due to its ability to accumulate information from\ndifferent source modalities and generate a fluent textual summary. However,\nexisting methods use short videos as the visual modality and short summary as\nthe ground-truth, therefore, perform poorly on lengthy videos and long\nground-truth summary. Additionally, there exists no benchmark dataset to\ngeneralize this task on videos of varying lengths. In this paper, we introduce\nAVIATE, the first large-scale dataset for abstractive text summarization with\nvideos of diverse duration, compiled from presentations in well-known academic\nconferences like NDSS, ICML, NeurIPS, etc. We use the abstract of corresponding\nresearch papers as the reference summaries, which ensure adequate quality and\nuniformity of the ground-truth. We then propose {\\name}, a factorized\nmulti-modal Transformer based decoder-only language model, which inherently\ncaptures the intra-modal and inter-modal dynamics within various input\nmodalities for the text summarization task. {\\name} utilizes an increasing\nnumber of self-attentions to capture multimodality and performs significantly\nbetter than traditional encoder-decoder based networks. Extensive experiments\nillustrate that {\\name} achieves significant improvement over the baselines in\nboth qualitative and quantitative evaluations on the existing How2 dataset for\nshort videos and newly introduced AVIATE dataset for videos with diverse\nduration, beating the best baseline on the two datasets by $1.39$ and $2.74$\nROUGE-L points respectively.",
          "link": "http://arxiv.org/abs/2105.09601",
          "publishedOn": "2021-05-23T06:08:16.148Z",
          "wordCount": 667,
          "title": "See, Hear, Read: Leveraging Multimodality with Guided Attention for Abstractive Text Summarization. (arXiv:2105.09601v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mai_G/0/1/0/all/0/1\">Gengchen Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janowicz_K/0/1/0/all/0/1\">Krzysztof Janowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Ling Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1\">Ni Lao</a>",
          "description": "As an important part of Artificial Intelligence (AI), Question Answering (QA)\naims at generating answers to questions phrased in natural language. While\nthere has been substantial progress in open-domain question answering, QA\nsystems are still struggling to answer questions which involve geographic\nentities or concepts and that require spatial operations. In this paper, we\ndiscuss the problem of geographic question answering (GeoQA). We first\ninvestigate the reasons why geographic questions are difficult to answer by\nanalyzing challenges of geographic questions. We discuss the uniqueness of\ngeographic questions compared to general QA. Then we review existing work on\nGeoQA and classify them by the types of questions they can address. Based on\nthis survey, we provide a generic classification framework for geographic\nquestions. Finally, we conclude our work by pointing out unique future research\ndirections for GeoQA.",
          "link": "http://arxiv.org/abs/2105.09392",
          "publishedOn": "2021-05-23T06:08:16.136Z",
          "wordCount": 595,
          "title": "Geographic Question Answering: Challenges, Uniqueness, Classification, and Future Directions. (arXiv:2105.09392v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09509",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Shirong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongtong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sheng Bi</a>",
          "description": "Event detection (ED) aims at detecting event trigger words in sentences and\nclassifying them into specific event types. In real-world applications, ED\ntypically does not have sufficient labelled data, thus can be formulated as a\nfew-shot learning problem. To tackle the issue of low sample diversity in\nfew-shot ED, we propose a novel knowledge-based few-shot event detection method\nwhich uses a definition-based encoder to introduce external event knowledge as\nthe knowledge prior of event types. Furthermore, as external knowledge\ntypically provides limited and imperfect coverage of event types, we introduce\nan adaptive knowledge-enhanced Bayesian meta-learning method to dynamically\nadjust the knowledge prior of event types. Experiments show our method\nconsistently and substantially outperforms a number of baselines by at least 15\nabsolute F1 points under the same few-shot settings.",
          "link": "http://arxiv.org/abs/2105.09509",
          "publishedOn": "2021-05-23T06:08:16.101Z",
          "wordCount": 568,
          "title": "Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event Detection. (arXiv:2105.09509v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lianwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yuan Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yuqian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Ling Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhaoyin Qi</a>",
          "description": "Recent studies constructing direct interactions between the claim and each\nsingle user response (a comment or a relevant article) to capture evidence have\nshown remarkable success in interpretable claim verification. Owing to\ndifferent single responses convey different cognition of individual users\n(i.e., audiences), the captured evidence belongs to the perspective of\nindividual cognition. However, individuals' cognition of social things is not\nalways able to truly reflect the objective. There may be one-sided or biased\nsemantics in their opinions on a claim. The captured evidence correspondingly\ncontains some unobjective and biased evidence fragments, deteriorating task\nperformance. In this paper, we propose a Dual-view model based on the views of\nCollective and Individual Cognition (CICD) for interpretable claim\nverification. From the view of the collective cognition, we not only capture\nthe word-level semantics based on individual users, but also focus on\nsentence-level semantics (i.e., the overall responses) among all users and\nadjust the proportion between them to generate global evidence. From the view\nof individual cognition, we select the top-$k$ articles with high degree of\ndifference and interact with the claim to explore the local key evidence\nfragments. To weaken the bias of individual cognition-view evidence, we devise\ninconsistent loss to suppress the divergence between global and local evidence\nfor strengthening the consistent shared evidence between the both. Experiments\non three benchmark datasets confirm that CICD achieves state-of-the-art\nperformance.",
          "link": "http://arxiv.org/abs/2105.09567",
          "publishedOn": "2021-05-23T06:08:16.080Z",
          "wordCount": 672,
          "title": "Unified Dual-view Cognitive Model for Interpretable Claim Verification. (arXiv:2105.09567v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2102.03848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hamada_M/0/1/0/all/0/1\">Mohamed A. Hamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_A/0/1/0/all/0/1\">Abdelrahman Abdallah</a>",
          "description": "Many computer systems for calculating the proper organization of memory are\namong the most critical issues. Using a tier cache memory (along with branching\nprediction) is an effective means of increasing modern multi-core processors'\nperformance. Designing high-performance processors is a complex task and\nrequires preliminary verification and analysis of the model level, usually used\nin analytical and simulation modeling. The refinement of extreme programming is\nan unfortunate challenge. Few experts disagree with the synthesis of access\npoints. This article demonstrates that Internet QoS and 16-bit architectures\nare always incompatible, but it's the same situation for write-back caches. The\nsolution to this problem can be implemented by analyzing simulation models of\ndifferent complexity in combination with the analytical evaluation of\nindividual algorithms. This work is devoted to designing a multi-parameter\nsimulation model of a multi-process for evaluating the performance of cache\nmemory algorithms and the optimality of the structure. Optimization of the\nstructures and algorithms of the cache memory allows you to accelerate the\ninteraction of the memory process and improve the performance of the entire\nsystem.",
          "link": "http://arxiv.org/abs/2102.03848",
          "publishedOn": "2021-05-23T06:08:17.168Z",
          "wordCount": 650,
          "title": "Estimate The Efficiency Of Multiprocessor's Cash Memory Work Algorithms. (arXiv:2102.03848v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1812.00002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_S/0/1/0/all/0/1\">Sen Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Congzhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>",
          "description": "Interactive news recommendation has been launched and attracted much\nattention recently. In this scenario, user's behavior evolves from single click\nbehavior to multiple behaviors including like, comment, share etc. However,\nmost of the existing methods still use single click behavior as the unique\ncriterion of judging user's preferences. Further, although heterogeneous graphs\nhave been applied in different areas, a proper way to construct a heterogeneous\ngraph for interactive news data with an appropriate learning mechanism on it is\nstill desired. To address the above concerns, we propose a graph-based\nbehavior-aware network, which simultaneously considers six different types of\nbehaviors as well as user's demand on the news diversity. We have three main\nsteps. First, we build an interaction behavior graph for multi-level and\nmulti-category data. Second, we apply DeepWalk on the behavior graph to obtain\nentity semantics, then build a graph-based convolutional neural network called\nG-CNN to learn news representations, and an attention-based LSTM to learn\nbehavior sequence representations. Third, we introduce core and coritivity\nfeatures for the behavior graph, which measure the concentration degree of\nuser's interests. These features affect the trade-off between accuracy and\ndiversity of our personalized recommendation system. Taking these features into\naccount, our system finally achieves recommending news to different users at\ntheir different levels of concentration degrees.",
          "link": "http://arxiv.org/abs/1812.00002",
          "publishedOn": "2021-05-23T06:08:17.152Z",
          "wordCount": 678,
          "title": "The Graph-Based Behavior-Aware Recommendation for Interactive News. (arXiv:1812.00002v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bolin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>",
          "description": "Conversational recommender systems (CRS) enable the traditional recommender\nsystems to explicitly acquire user preferences towards items and attributes\nthrough interactive conversations. Reinforcement learning (RL) is widely\nadopted to learn conversational recommendation policies to decide what\nattributes to ask, which items to recommend, and when to ask or recommend, at\neach conversation turn. However, existing methods mainly target at solving one\nor two of these three decision-making problems in CRS with separated\nconversation and recommendation components, which restrict the scalability and\ngenerality of CRS and fall short of preserving a stable training procedure. In\nthe light of these challenges, we propose to formulate these three\ndecision-making problems in CRS as a unified policy learning task. In order to\nsystematically integrate conversation and recommendation components, we develop\na dynamic weighted graph based RL method to learn a policy to select the action\nat each conversation turn, either asking an attribute or recommending items.\nFurther, to deal with the sample efficiency issue, we propose two action\nselection strategies for reducing the candidate action space according to the\npreference and entropy information. Experimental results on two benchmark CRS\ndatasets and a real-world E-Commerce application show that the proposed method\nnot only significantly outperforms state-of-the-art methods but also enhances\nthe scalability and stability of CRS.",
          "link": "http://arxiv.org/abs/2105.09710",
          "publishedOn": "2021-05-23T06:08:17.142Z",
          "wordCount": 642,
          "title": "Unified Conversational Recommendation Policy Learning via Graph-based Reinforcement Learning. (arXiv:2105.09710v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Proper_H/0/1/0/all/0/1\">H. A. Proper</a>",
          "description": "Effective information disclosure in the context of databases with a large\nconceptual schema is known to be a non-trivial problem. In particular the\nformulation of ad-hoc queries is a major problem in such contexts. Existing\napproaches for tackling this problem include graphical query interfaces, query\nby navigation, query by construction, and point to point queries. In this\nreport we propose an adoption of the query by navigation mechanism that is\nespecially geared towards the InfoAssistant product. Query by navigation is\nbased on ideas from the information retrieval world, in particular on the\nstratified hypermedia architecture. When using our approach to the formulations\nof queries, a user will first formulate a number of simple queries\ncorresponding to linear paths through the information structure. The\nformulation of the linear paths is the result of the {\\em explorative phase} of\nthe query formulation. Once users have specified a number of these linear\npaths, they may combine them to form more complex queries. Examples of such\ncombinations are: concatenation, union, intersection and selection. This last\nprocess is referred to as {\\em query by construction}, and is the {\\em\nconstructive phase} of the query formulation process.",
          "link": "http://arxiv.org/abs/2105.09562",
          "publishedOn": "2021-05-23T06:08:17.118Z",
          "wordCount": 609,
          "title": "Interactive Query Formulation using Query By Navigation. (arXiv:2105.09562v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bountrogiannis_K/0/1/0/all/0/1\">Konstantinos Bountrogiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzagkarakis_G/0/1/0/all/0/1\">George Tzagkarakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsakalides_P/0/1/0/all/0/1\">Panagiotis Tsakalides</a>",
          "description": "Due to the importance of the lower bounding distances and the attractiveness\nof symbolic representations, the family of symbolic aggregate approximations\n(SAX) has been used extensively for encoding time series data. However, typical\nSAX-based methods rely on two restrictive assumptions; the Gaussian\ndistribution and equiprobable symbols. This paper proposes two novel\ndata-driven SAX-based symbolic representations, distinguished by their\ndiscretization steps. The first representation, oriented for general data\ncompaction and indexing scenarios, is based on the combination of kernel\ndensity estimation and Lloyd-Max quantization to minimize the information loss\nand mean squared error in the discretization step. The second method, oriented\nfor high-level mining tasks, employs the Mean-Shift clustering method and is\nshown to enhance anomaly detection in the lower-dimensional space. Besides, we\nverify on a theoretical basis a previously observed phenomenon of the intrinsic\nprocess that results in a lower than the expected variance of the intermediate\npiecewise aggregate approximation. This phenomenon causes an additional\ninformation loss but can be avoided with a simple modification. The proposed\nrepresentations possess all the attractive properties of the conventional SAX\nmethod. Furthermore, experimental evaluation on real-world datasets\ndemonstrates their superiority compared to the traditional SAX and an\nalternative data-driven SAX variant.",
          "link": "http://arxiv.org/abs/2105.09592",
          "publishedOn": "2021-05-23T06:08:17.108Z",
          "wordCount": 634,
          "title": "Distribution Agnostic Symbolic Representations for Time Series Dimensionality Reduction and Online Anomaly Detection. (arXiv:2105.09592v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanxiong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>",
          "description": "Recommender systems are gaining increasing and critical impacts on human and\nsociety since a growing number of users use them for information seeking and\ndecision making. Therefore, it is crucial to address the potential unfairness\nproblems in recommendations. Just like users have personalized preferences on\nitems, users' demands for fairness are also personalized in many scenarios.\nTherefore, it is important to provide personalized fair recommendations for\nusers to satisfy their personalized fairness demands. Besides, previous works\non fair recommendation mainly focus on association-based fairness. However, it\nis important to advance from associative fairness notions to causal fairness\nnotions for assessing fairness more properly in recommender systems. Based on\nthe above considerations, this paper focuses on achieving personalized\ncounterfactual fairness for users in recommender systems. To this end, we\nintroduce a framework for achieving counterfactually fair recommendations\nthrough adversary learning by generating feature-independent user embeddings\nfor recommendation. The framework allows recommender systems to achieve\npersonalized fairness for users while also covering non-personalized\nsituations. Experiments on two real-world datasets with shallow and deep\nrecommendation algorithms show that our method can generate fairer\nrecommendations for users with a desirable recommendation performance.",
          "link": "http://arxiv.org/abs/2105.09829",
          "publishedOn": "2021-05-23T06:08:17.079Z",
          "wordCount": 630,
          "title": "Towards Personalized Fairness based on Causal Notion. (arXiv:2105.09829v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aditi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanya_S/0/1/0/all/0/1\">Suhas Jayaram Subramanya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_R/0/1/0/all/0/1\">Ravishankar Krishnaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simhadri_H/0/1/0/all/0/1\">Harsha Vardhan Simhadri</a>",
          "description": "Approximate nearest neighbor search (ANNS) is a fundamental building block in\ninformation retrieval with graph-based indices being the current\nstate-of-the-art and widely used in the industry. Recent advances in\ngraph-based indices have made it possible to index and search billion-point\ndatasets with high recall and millisecond-level latency on a single commodity\nmachine with an SSD.\n\nHowever, existing graph algorithms for ANNS support only static indices that\ncannot reflect real-time changes to the corpus required by many key real-world\nscenarios (e.g. index of sentences in documents, email, or a news index). To\novercome this drawback, the current industry practice for manifesting updates\ninto such indices is to periodically re-build these indices, which can be\nprohibitively expensive.\n\nIn this paper, we present the first graph-based ANNS index that reflects\ncorpus updates into the index in real-time without compromising on search\nperformance. Using update rules for this index, we design FreshDiskANN, a\nsystem that can index over a billion points on a workstation with an SSD and\nlimited memory, and support thousands of concurrent real-time inserts, deletes\nand searches per second each, while retaining $>95\\%$ 5-recall@5. This\nrepresents a 5-10x reduction in the cost of maintaining freshness in indices\nwhen compared to existing methods.",
          "link": "http://arxiv.org/abs/2105.09613",
          "publishedOn": "2021-05-23T06:08:17.044Z",
          "wordCount": 643,
          "title": "FreshDiskANN: A Fast and Accurate Graph-Based ANN Index for Streaming Similarity Search. (arXiv:2105.09613v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1\">Sebastian Hofst&#xe4;tter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_B/0/1/0/all/0/1\">Bhaskar Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Craswell_N/0/1/0/all/0/1\">Nick Craswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>",
          "description": "An emerging recipe for achieving state-of-the-art effectiveness in neural\ndocument re-ranking involves utilizing large pre-trained language models -\ne.g., BERT - to evaluate all individual passages in the document and then\naggregating the outputs by pooling or additional Transformer layers. A major\ndrawback of this approach is high query latency due to the cost of evaluating\nevery passage in the document with BERT. To make matters worse, this high\ninference cost and latency varies based on the length of the document, with\nlonger documents requiring more time and computation. To address this\nchallenge, we adopt an intra-document cascading strategy, which prunes passages\nof a candidate document using a less expensive model, called ESM, before\nrunning a scoring model that is more expensive and effective, called ETM. We\nfound it best to train ESM (short for Efficient Student Model) via knowledge\ndistillation from the ETM (short for Effective Teacher Model) e.g., BERT. This\npruning allows us to only run the ETM model on a smaller set of passages whose\nsize does not vary by document length. Our experiments on the MS MARCO and TREC\nDeep Learning Track benchmarks suggest that the proposed Intra-Document\nCascaded Ranking Model (IDCM) leads to over 400% lower query latency by\nproviding essentially the same effectiveness as the state-of-the-art BERT-based\ndocument ranking models.",
          "link": "http://arxiv.org/abs/2105.09816",
          "publishedOn": "2021-05-23T06:08:17.014Z",
          "wordCount": 659,
          "title": "Intra-Document Cascading: Learning to Select Passages for Neural Document Ranking. (arXiv:2105.09816v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1\">Xin Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1\">Joemon Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fuli Feng</a>",
          "description": "Learning from implicit feedback is one of the most common cases in the\napplication of recommender systems. Generally speaking, interacted examples are\nconsidered as positive while negative examples are sampled from uninteracted\nones. However, noisy examples are prevalent in real-world implicit feedback. A\nnoisy positive example could be interacted but it actually leads to negative\nuser preference. A noisy negative example which is uninteracted because of\nunawareness of the user could also denote potential positive user preference.\nConventional training methods overlook these noisy examples, leading to\nsub-optimal recommendation. In this work, we propose probabilistic and\nvariational recommendation denoising for implicit feedback. Through an\nempirical study, we find that different models make relatively similar\npredictions on clean examples which denote the real user preference, while the\npredictions on noisy examples vary much more across different models. Motivated\nby this observation, we propose denoising with probabilistic inference (DPI)\nwhich aims to minimize the KL-divergence between the real user preference\ndistributions parameterized by two recommendation models while maximize the\nlikelihood of data observation. We then show that DPI recovers the evidence\nlower bound of an variational auto-encoder when the real user preference is\nconsidered as the latent variables. This leads to our second learning framework\ndenoising with variational autoencoder (DVAE). We employ the proposed DPI and\nDVAE on four state-of-the-art recommendation models and conduct experiments on\nthree datasets. Experimental results demonstrate that DPI and DVAE\nsignificantly improve recommendation performance compared with normal training\nand other denoising methods. Codes will be open-sourced.",
          "link": "http://arxiv.org/abs/2105.09605",
          "publishedOn": "2021-05-23T06:08:16.990Z",
          "wordCount": 678,
          "title": "Probabilistic and Variational Recommendation Denoising. (arXiv:2105.09605v1 [cs.IR])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2012.00641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>",
          "description": "The content based image retrieval aims to find the similar images from a\nlarge scale dataset against a query image. Generally, the similarity between\nthe representative features of the query image and dataset images is used to\nrank the images for retrieval. In early days, various hand designed feature\ndescriptors have been investigated based on the visual cues such as color,\ntexture, shape, etc. that represent the images. However, the deep learning has\nemerged as a dominating alternative of hand-designed feature engineering from a\ndecade. It learns the features automatically from the data. This paper presents\na comprehensive survey of deep learning based developments in the past decade\nfor content based image retrieval. The categorization of existing\nstate-of-the-art methods from different perspectives is also performed for\ngreater understanding of the progress. The taxonomy used in this survey covers\ndifferent supervision, different networks, different descriptor type and\ndifferent retrieval type. A performance analysis is also performed using the\nstate-of-the-art methods. The insights are also presented for the benefit of\nthe researchers to observe the progress and to make the best choices. The\nsurvey presented in this paper will help in further research progress in image\nretrieval using deep learning.",
          "link": "http://arxiv.org/abs/2012.00641",
          "publishedOn": "2021-05-23T06:08:15.920Z",
          "wordCount": 678,
          "title": "A Decade Survey of Content Based Image Retrieval using Deep Learning. (arXiv:2012.00641v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1\">Mohit Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pailla_D/0/1/0/all/0/1\">Dheeraj Pailla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_H/0/1/0/all/0/1\">Himanshu Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchawala_A/0/1/0/all/0/1\">Aadilmehdi Sanchawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1\">Manish Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1\">Ponnurangam Kumaraguru</a>",
          "description": "The exponential rise of online social media has enabled the creation,\ndistribution, and consumption of information at an unprecedented rate. However,\nit has also led to the burgeoning of various forms of online abuse. Increasing\ncases of online antisemitism have become one of the major concerns because of\nits socio-political consequences. Unlike other major forms of online abuse like\nracism, sexism, etc., online antisemitism has not been studied much from a\nmachine learning perspective. To the best of our knowledge, we present the\nfirst work in the direction of automated multimodal detection of online\nantisemitism. The task poses multiple challenges that include extracting\nsignals across multiple modalities, contextual references, and handling\nmultiple aspects of antisemitism. Unfortunately, there does not exist any\npublicly available benchmark corpus for this critical task. Hence, we collect\nand label two datasets with 3,102 and 3,509 social media posts from Twitter and\nGab respectively. Further, we present a multimodal deep learning system that\ndetects the presence of antisemitic content and its specific antisemitism\ncategory using text and images from posts. We perform an extensive set of\nexperiments on the two datasets to evaluate the efficacy of the proposed\nsystem. Finally, we also present a qualitative analysis of our study.",
          "link": "http://arxiv.org/abs/2104.05947",
          "publishedOn": "2021-05-23T06:08:15.873Z",
          "wordCount": 668,
          "title": "\"Subverting the Jewtocracy\": Online Antisemitism Detection Using Multimodal Deep Learning. (arXiv:2104.05947v2 [cs.MM] UPDATED)"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2104.14066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Min Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_B/0/1/0/all/0/1\">Bo Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zihao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Junxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Degang Sun</a>",
          "description": "Object detection can be regarded as a pixel clustering task, and its boundary\nis determined by four extreme points (leftmost, top, rightmost, and bottom).\nHowever, most studies focus on the center or corner points of the object, which\nare actually conditional results of the extreme points. In this paper, we\npresent an Extreme-Point-Prediction-Based object detector (EPP-Net), which\ndirectly regresses the relative displacement vector between each pixel and the\nfour extreme points. We also propose a new metric to measure the similarity\nbetween two groups of extreme points, namely, Extreme Intersection over Union\n(EIoU), and incorporate this EIoU as a new regression loss. Moreover, we\npropose a novel branch to predict the EIoU between the ground-truth and the\nprediction results, and combine it with the classification confidence as the\nranking keyword in non-maximum suppression. On the MS-COCO dataset, our method\nachieves an average precision (AP) of 44.0% with ResNet-50 and an AP of 48.3%\nwith ResNeXt-101-DCN. The proposed EPP-Net provides a new method to detect\nobjects and outperforms state-of-the-art anchor-free detectors.",
          "link": "http://arxiv.org/abs/2104.14066",
          "publishedOn": "2021-05-23T06:08:18.236Z",
          "wordCount": 613,
          "title": "Objects as Extreme Points. (arXiv:2104.14066v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1\">Bj&#xf6;rn L&#xfc;tjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leshchinskiy_B/0/1/0/all/0/1\">Brandon Leshchinskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Requena_Mesa_C/0/1/0/all/0/1\">Christian Requena-Mesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chishtie_F/0/1/0/all/0/1\">Farrukh Chishtie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_Rodriguez_N/0/1/0/all/0/1\">Natalia D&#xed;az-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulais_O/0/1/0/all/0/1\">Oc&#xe9;ane Boulais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1\">Aruna Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pina_A/0/1/0/all/0/1\">Aaron Pi&#xf1;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raissi_C/0/1/0/all/0/1\">Chedy Ra&#xef;ssi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavin_A/0/1/0/all/0/1\">Alexander Lavin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newman_D/0/1/0/all/0/1\">Dava Newman</a>",
          "description": "As climate change increases the intensity of natural disasters, society needs\nbetter tools for adaptation. Floods, for example, are the most frequent natural\ndisaster, and better tools for flood risk communication could increase the\nsupport for flood-resilient infrastructure development. Our work aims to enable\nmore visual communication of large-scale climate impacts via visualizing the\noutput of coastal flood models as satellite imagery. We propose the first deep\nlearning pipeline to ensure physical-consistency in synthetic visual satellite\nimagery. We advanced a state-of-the-art GAN called pix2pixHD, such that it\nproduces imagery that is physically-consistent with the output of an\nexpert-validated storm surge model (NOAA SLOSH). By evaluating the imagery\nrelative to physics-based flood maps, we find that our proposed framework\noutperforms baseline models in both physical-consistency and photorealism. We\nenvision our work to be the first step towards a global visualization of how\nclimate change shapes our landscape. Continuing on this path, we show that the\nproposed pipeline generalizes to visualize arctic sea ice melt. We also publish\na dataset of over 25k labelled image-pairs to study image-to-image translation\nin Earth observation.",
          "link": "http://arxiv.org/abs/2104.04785",
          "publishedOn": "2021-05-23T06:08:18.215Z",
          "wordCount": 685,
          "title": "Physically-Consistent Generative Adversarial Networks for Coastal Flood Visualization. (arXiv:2104.04785v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13482",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_K/0/1/0/all/0/1\">Kang Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yirui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaoyun Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1\">Fakai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1\">Le Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_C/0/1/0/all/0/1\">Chihung Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1\">Lingyun Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1\">Chang-Fu Kuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miao_S/0/1/0/all/0/1\">Shun Miao</a>",
          "description": "Bone mineral density (BMD) is a clinically critical indicator of\nosteoporosis, usually measured by dual-energy X-ray absorptiometry (DEXA). Due\nto the limited accessibility of DEXA machines and examinations, osteoporosis is\noften under-diagnosed and under-treated, leading to increased fragility\nfracture risks. Thus it is highly desirable to obtain BMDs with alternative\ncost-effective and more accessible medical imaging examinations such as X-ray\nplain films. In this work, we formulate the BMD estimation from plain hip X-ray\nimages as a regression problem. Specifically, we propose a new semi-supervised\nself-training algorithm to train the BMD regression model using images coupled\nwith DEXA measured BMDs and unlabeled images with pseudo BMDs. Pseudo BMDs are\ngenerated and refined iteratively for unlabeled images during self-training. We\nalso present a novel adaptive triplet loss to improve the model's regression\naccuracy. On an in-house dataset of 1,090 images (819 unique patients), our BMD\nestimation method achieves a high Pearson correlation coefficient of 0.8805 to\nground-truth BMDs. It offers good feasibility to use the more accessible and\ncheaper X-ray imaging for opportunistic osteoporosis screening.",
          "link": "http://arxiv.org/abs/2103.13482",
          "publishedOn": "2021-05-23T06:08:18.206Z",
          "wordCount": 649,
          "title": "Semi-Supervised Learning for Bone Mineral Density Estimation in Hip X-ray Images. (arXiv:2103.13482v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ruimin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiayi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baofeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jie Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuting Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunlei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jie Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hongjiang Wei</a>",
          "description": "Quantitative susceptibility mapping (QSM) has demonstrated great potential in\nquantifying tissue susceptibility in various brain diseases. However, the\nintrinsic ill-posed inverse problem relating the tissue phase to the underlying\nsusceptibility distribution affects the accuracy for quantifying tissue\nsusceptibility. Recently, deep learning has shown promising results to improve\naccuracy by reducing the streaking artifacts. However, there exists a mismatch\nbetween the observed phase and the theoretical forward phase estimated by the\nsusceptibility label. In this study, we proposed a model-based deep learning\narchitecture that followed the STI (susceptibility tensor imaging) physical\nmodel, referred to as MoDL-QSM. Specifically, MoDL-QSM accounts for the\nrelationship between STI-derived phase contrast induced by the susceptibility\ntensor terms (ki13,ki23,ki33) and the acquired single-orientation phase. The\nconvolution neural networks are embedded into the physical model to learn a\nregularization term containing prior information. ki33 and phase induced by\nki13 and ki23 terms were used as the labels for network training. Quantitative\nevaluation metrics (RSME, SSIM, and HFEN) were compared with recently developed\ndeep learning QSM methods. The results showed that MoDL-QSM achieved superior\nperformance, demonstrating its potential for future applications.",
          "link": "http://arxiv.org/abs/2101.08413",
          "publishedOn": "2021-05-23T06:08:18.199Z",
          "wordCount": 660,
          "title": "MoDL-QSM: Model-based Deep Learning for Quantitative Susceptibility Mapping. (arXiv:2101.08413v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bie_X/0/1/0/all/0/1\">Xiaoyu Bie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>",
          "description": "Human motion prediction aims to forecast future human poses given a sequence\nof past 3D skeletons. While this problem has recently received increasing\nattention, it has mostly been tackled for single humans in isolation. In this\npaper we explore this problem from a novel perspective, involving humans\nperforming collaborative tasks. We assume that the input of our system are two\nsequences of past skeletons for two interacting persons, and we aim to predict\nthe future motion for each of them. For this purpose, we devise a novel cross\ninteraction attention mechanism that exploits historical information of both\npersons and learns to predict cross dependencies between self poses and the\nposes of the other person in spite of their spatial or temporal distance. Since\nno dataset to train such interactive situations is available, we have captured\nExPI (Extreme Pose Interaction), a new lab-based person interaction dataset of\nprofessional dancers performing acrobatics. ExPI contains 115 sequences with\n30k frames and 60k instances with annotated 3D body poses and shapes. We\nthoroughly evaluate our cross-interaction network on this dataset and show that\nboth in short-term and long-term predictions, it consistently outperforms\nbaselines that independently reason for each person. We plan to release our\ncode jointly with the dataset and the train/test splits to spur future research\non the topic.",
          "link": "http://arxiv.org/abs/2105.08825",
          "publishedOn": "2021-05-23T06:08:18.192Z",
          "wordCount": 661,
          "title": "Multi-Person Extreme Motion Prediction with Cross-Interaction Attention. (arXiv:2105.08825v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>",
          "description": "The content based image retrieval aims to find the similar images from a\nlarge scale dataset against a query image. Generally, the similarity between\nthe representative features of the query image and dataset images is used to\nrank the images for retrieval. In early days, various hand designed feature\ndescriptors have been investigated based on the visual cues such as color,\ntexture, shape, etc. that represent the images. However, the deep learning has\nemerged as a dominating alternative of hand-designed feature engineering from a\ndecade. It learns the features automatically from the data. This paper presents\na comprehensive survey of deep learning based developments in the past decade\nfor content based image retrieval. The categorization of existing\nstate-of-the-art methods from different perspectives is also performed for\ngreater understanding of the progress. The taxonomy used in this survey covers\ndifferent supervision, different networks, different descriptor type and\ndifferent retrieval type. A performance analysis is also performed using the\nstate-of-the-art methods. The insights are also presented for the benefit of\nthe researchers to observe the progress and to make the best choices. The\nsurvey presented in this paper will help in further research progress in image\nretrieval using deep learning.",
          "link": "http://arxiv.org/abs/2012.00641",
          "publishedOn": "2021-05-23T06:08:18.175Z",
          "wordCount": 678,
          "title": "A Decade Survey of Content Based Image Retrieval using Deep Learning. (arXiv:2012.00641v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08147",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ramesh_V/0/1/0/all/0/1\">Vignav Ramesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rister_B/0/1/0/all/0/1\">Blaine Rister</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel L. Rubin</a>",
          "description": "Chest X-rays of coronavirus disease 2019 (COVID-19) patients are frequently\nobtained to determine the extent of lung disease and are a valuable source of\ndata for creating artificial intelligence models. Most work to date assessing\ndisease severity on chest imaging has focused on segmenting computed tomography\n(CT) images; however, given that CTs are performed much less frequently than\nchest X-rays for COVID-19 patients, automated lung lesion segmentation on chest\nX-rays could be clinically valuable. There currently exists a universal\nshortage of chest X-rays with ground truth COVID-19 lung lesion annotations,\nand manually contouring lung opacities is a tedious, labor-intensive task. To\naccelerate severity detection and augment the amount of publicly available\nchest X-ray training data for supervised deep learning (DL) models, we leverage\nexisting annotated CT images to generate frontal projection \"chest X-ray\"\nimages for training COVID-19 chest X-ray models. In this paper, we propose an\nautomated pipeline for segmentation of COVID-19 lung lesions on chest X-rays\ncomprised of a Mask R-CNN trained on a mixed dataset of open-source chest\nX-rays and coronal X-ray projections computed from annotated volumetric CTs. On\na test set containing 40 chest X-rays of COVID-19 positive patients, our model\nachieved IoU scores of 0.81 $\\pm$ 0.03 and 0.79 $\\pm$ 0.03 when trained on a\ndataset of 60 chest X-rays and on a mixed dataset of 10 chest X-rays and 50\nprojections from CTs, respectively. Our model far outperforms current baselines\nwith limited supervised training and may assist in automated COVID-19 severity\nquantification on chest X-rays.",
          "link": "http://arxiv.org/abs/2105.08147",
          "publishedOn": "2021-05-23T06:08:18.169Z",
          "wordCount": 779,
          "title": "COVID-19 Lung Lesion Segmentation Using a Sparsely Supervised Mask R-CNN on Chest X-rays Automatically Computed from Volumetric CTs. (arXiv:2105.08147v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aaditya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hingane_S/0/1/0/all/0/1\">Shreeshail Hingane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xinyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>",
          "description": "Artistic style transfer aims to transfer the style characteristics of one\nimage onto another image while retaining its content. Existing approaches\ncommonly leverage various normalization techniques, although these face\nlimitations in adequately transferring diverse textures to different spatial\nlocations. Self-Attention-based approaches have tackled this issue with partial\nsuccess but suffer from unwanted artifacts. Motivated by these observations,\nthis paper aims to combine the best of both worlds: self-attention and\nnormalization. That yields a new plug-and-play module that we name\nSelf-Attentive Factorized Instance Normalization (SAFIN). SAFIN is essentially\na spatially adaptive normalization module whose parameters are inferred through\nattention on the content and style image. We demonstrate that plugging SAFIN\ninto the base network of another state-of-the-art method results in enhanced\nstylization. We also develop a novel base network composed of Wavelet Transform\nfor multi-scale style transfer, which when combined with SAFIN, produces\nvisually appealing results with lesser unwanted textures.",
          "link": "http://arxiv.org/abs/2105.06129",
          "publishedOn": "2021-05-23T06:08:18.162Z",
          "wordCount": 616,
          "title": "SAFIN: Arbitrary Style Transfer With Self-Attentive Factorized Instance Normalization. (arXiv:2105.06129v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Speth_J/0/1/0/all/0/1\">Jeremy Speth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vance_N/0/1/0/all/0/1\">Nathan Vance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flynn_P/0/1/0/all/0/1\">Patrick Flynn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1\">Kevin Bowyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czajka_A/0/1/0/all/0/1\">Adam Czajka</a>",
          "description": "Remote photoplethysmography (rPPG), a family of techniques for monitoring\nblood volume changes, may be especially useful for widespread contactless\nhealth monitoring using face video from consumer-grade visible-light cameras.\nThe COVID-19 pandemic has caused the widespread use of protective face masks.\nWe found that occlusions from cloth face masks increased the mean absolute\nerror of heart rate estimation by more than 80\\% when deploying methods\ndesigned on unmasked faces. We show that augmenting unmasked face videos by\nadding patterned synthetic face masks forces the model to attend to the\nperiocular and forehead regions, improving performance and closing the gap\nbetween masked and unmasked pulse estimation. To our knowledge, this paper is\nthe first to analyse the impact of face masks on the accuracy of pulse\nestimation and offers several novel contributions: (a) 3D CNN-based method\ndesigned for remote photoplethysmography in a presence of face masks, (b) two\npublicly available pulse estimation datasets acquired from 86 unmasked and 61\nmasked subjects, (c) evaluations of handcrafted algorithms and a 3D CNN trained\non videos of unmasked faces and with masks synthetically added, and (d) data\naugmentation method to add a synthetic mask to a face video.",
          "link": "http://arxiv.org/abs/2101.04096",
          "publishedOn": "2021-05-23T06:08:18.154Z",
          "wordCount": 714,
          "title": "Remote Pulse Estimation in the Presence of Face Masks. (arXiv:2101.04096v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xinya Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaisiyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wayne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Feng Xu</a>",
          "description": "Despite previous success in generating audio-driven talking heads, most of\nthe previous studies focus on the correlation between speech content and the\nmouth shape. Facial emotion, which is one of the most important features on\nnatural human faces, is always neglected in their methods. In this work, we\npresent Emotional Video Portraits (EVP), a system for synthesizing high-quality\nvideo portraits with vivid emotional dynamics driven by audios. Specifically,\nwe propose the Cross-Reconstructed Emotion Disentanglement technique to\ndecompose speech into two decoupled spaces, i.e., a duration-independent\nemotion space and a duration dependent content space. With the disentangled\nfeatures, dynamic 2D emotional facial landmarks can be deduced. Then we propose\nthe Target-Adaptive Face Synthesis technique to generate the final high-quality\nvideo portraits, by bridging the gap between the deduced landmarks and the\nnatural head poses of target videos. Extensive experiments demonstrate the\neffectiveness of our method both qualitatively and quantitatively.",
          "link": "http://arxiv.org/abs/2104.07452",
          "publishedOn": "2021-05-23T06:08:18.147Z",
          "wordCount": 612,
          "title": "Audio-Driven Emotional Video Portraits. (arXiv:2104.07452v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samani_E/0/1/0/all/0/1\">Ekta U. Samani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xingjian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Ashis G. Banerjee</a>",
          "description": "Object recognition in unseen indoor environments remains a challenging\nproblem for visual perception of mobile robots. In this letter, we propose the\nuse of topologically persistent features, which rely on the objects' shape\ninformation, to address this challenge. In particular, we extract two kinds of\nfeatures, namely, sparse persistence image (PI) and amplitude, by applying\npersistent homology to multi-directional height function-based filtrations of\nthe cubical complexes representing the object segmentation maps. The features\nare then used to train a fully connected network for recognition. For\nperformance evaluation, in addition to a widely used shape dataset and a\nbenchmark indoor scenes dataset, we collect a new dataset, comprising scene\nimages from two different environments, namely, a living room and a mock\nwarehouse. The scenes are captured using varying camera poses under different\nillumination conditions and include up to five different objects from a given\nset of fourteen objects. On the benchmark indoor scenes dataset, sparse PI\nfeatures show better recognition performance in unseen environments than the\nfeatures learned using the widely used ResNetV2-56 and EfficientNet-B4 models.\nFurther, they provide slightly higher recall and accuracy values than Faster\nR-CNN, an end-to-end object detection method, and its state-of-the-art variant,\nDomain Adaptive Faster R-CNN. The performance of our methods also remains\nrelatively unchanged from the training environment (living room) to the unseen\nenvironment (mock warehouse) in the new dataset. In contrast, the performance\nof the object detection methods drops substantially. We also implement the\nproposed method on a real-world robot to demonstrate its usefulness.",
          "link": "http://arxiv.org/abs/2010.03196",
          "publishedOn": "2021-05-23T06:08:18.140Z",
          "wordCount": 761,
          "title": "Visual Object Recognition in Indoor Environments Using Topologically Persistent Features. (arXiv:2010.03196v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08506",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_S/0/1/0/all/0/1\">Sara Atito Ali Ahmed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yavuz_M/0/1/0/all/0/1\">Mehmet Can Yavuz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sen_M/0/1/0/all/0/1\">Mehmet Umut Sen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gulsen_F/0/1/0/all/0/1\">Fatih Gulsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tutar_O/0/1/0/all/0/1\">Onur Tutar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korkmazer_B/0/1/0/all/0/1\">Bora Korkmazer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samanci_C/0/1/0/all/0/1\">Cesur Samanci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sirolu_S/0/1/0/all/0/1\">Sabri Sirolu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamid_R/0/1/0/all/0/1\">Rauf Hamid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eryurekli_A/0/1/0/all/0/1\">Ali Ergun Eryurekli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mammadov_T/0/1/0/all/0/1\">Toghrul Mammadov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yanikoglu_B/0/1/0/all/0/1\">Berrin Yanikoglu</a>",
          "description": "Detecting COVID-19 in computed tomography (CT) or radiography images has been\nproposed as a supplement to the definitive RT-PCR test. We present a deep\nlearning ensemble for detecting COVID-19 infection, combining slice-based (2D)\nand volume-based (3D) approaches. The 2D system detects the infection on each\nCT slice independently, combining them to obtain the patient-level decision via\ndifferent methods (averaging and long-short term memory networks). The 3D\nsystem takes the whole CT volume to arrive to the patient-level decision in one\nstep. A new high resolution chest CT scan dataset, called the IST-C dataset, is\nalso collected in this work. The proposed ensemble, called IST-CovNet, obtains\n90.80% accuracy and 0.95 AUC score overall on the IST-C dataset in detecting\nCOVID-19 among normal controls and other types of lung pathologies; and 93.69%\naccuracy and 0.99 AUC score on the publicly available MosMed dataset that\nconsists of COVID-19 scans and normal controls only. The system is deployed at\nIstanbul University Cerrahpasa School of Medicine.",
          "link": "http://arxiv.org/abs/2105.08506",
          "publishedOn": "2021-05-23T06:08:18.131Z",
          "wordCount": 693,
          "title": "COVID-19 Detection in Computed Tomography Images with 2D and 3D Approaches. (arXiv:2105.08506v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10762",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1\">Robert A. Murphy</a>",
          "description": "We show how to use random field theory in a supervised, energy-based model\nfor multiple pseudo image classification of 2D integer matrices. In the model,\neach row of a 2D integer matrix is a pseudo image where a local receptive field\nfocuses on multiple portions of individual rows for simultaneous learning. The\nmodel is used for a classification task consisting of presence of patient\nbiomarkers indicative of a particular disease.",
          "link": "http://arxiv.org/abs/2104.10762",
          "publishedOn": "2021-05-23T06:08:18.110Z",
          "wordCount": 540,
          "title": "Multiple Simultaneous Pseudo Image Classification with Random Fields and a Deep Belief Network for Disease Indication. (arXiv:2104.10762v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1\">Masaya Ueda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimura_A/0/1/0/all/0/1\">Akisato Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>",
          "description": "Various fonts give different impressions, such as legible, rough, and\ncomic-text.This paper aims to analyze the correlation between the local shapes,\nor parts, and the impression of fonts. By focusing on local shapes instead of\nthe whole letter shape, we can realize letter-shape independent and more\ngeneral analysis. The analysis is performed by newly combining SIFT and\nDeepSets, to extract an arbitrary number of essential parts from a particular\nfont and aggregate them to infer the font impressions by nonlinear regression.\nOur qualitative and quantitative analyses prove that (1)fonts with similar\nparts have similar impressions, (2)many impressions, such as legible and rough,\nlargely depend on specific parts, (3)several impressions are very irrelevant to\nparts.",
          "link": "http://arxiv.org/abs/2103.14216",
          "publishedOn": "2021-05-23T06:08:18.103Z",
          "wordCount": 569,
          "title": "Which Parts determine the Impression of the Font?. (arXiv:2103.14216v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Wei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Spiking Neural Networks (SNNs) have been attached great importance due to\ntheir biological plausibility and high energy-efficiency on neuromorphic chips.\nAs these chips are usually resource-constrained, the compression of SNNs is\nthus crucial along the road of practical use of SNNs. Most existing methods\ndirectly apply pruning approaches in artificial neural networks (ANNs) to SNNs,\nwhich ignore the difference between ANNs and SNNs, thus limiting the\nperformance of the pruned SNNs. Besides, these methods are only suitable for\nshallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination\nin the neural system, we propose gradient rewiring (Grad R), a joint learning\nalgorithm of connectivity and weight for SNNs, that enables us to seamlessly\noptimize network structure without retrain. Our key innovation is to redefine\nthe gradient to a new synaptic parameter, allowing better exploration of\nnetwork structures by taking full advantage of the competition between pruning\nand regrowth of connections. The experimental results show that the proposed\nmethod achieves minimal loss of SNNs' performance on MNIST and CIFAR-10 dataset\nso far. Moreover, it reaches a $\\sim$3.5% accuracy loss under unprecedented\n0.73% connectivity, which reveals remarkable structure refining capability in\nSNNs. Our work suggests that there exists extremely high redundancy in deep\nSNNs. Our codes are available at\nhttps://github.com/Yanqi-Chen/Gradient-Rewiring .",
          "link": "http://arxiv.org/abs/2105.04916",
          "publishedOn": "2021-05-23T06:08:18.096Z",
          "wordCount": 696,
          "title": "Pruning of Deep Spiking Neural Networks through Gradient Rewiring. (arXiv:2105.04916v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paletta_Q/0/1/0/all/0/1\">Quentin Paletta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>",
          "description": "Improving irradiance forecasting is critical to further increase the share of\nsolar in the energy mix. On a short time scale, fish-eye cameras on the ground\nare used to capture cloud displacements causing the local variability of the\nelectricity production. As most of the solar radiation comes directly from the\nSun, current forecasting approaches use its position in the image as a\nreference to interpret the cloud cover dynamics. However, existing Sun tracking\nmethods rely on external data and a calibration of the camera, which requires\naccess to the device. To address these limitations, this study introduces an\nimage-based Sun tracking algorithm to localise the Sun in the image when it is\nvisible and interpolate its daily trajectory from past observations. We\nvalidate the method on a set of sky images collected over a year at SIRTA's\nlab. Experimental results show that the proposed method provides robust smooth\nSun trajectories with a mean absolute error below 1% of the image size.",
          "link": "http://arxiv.org/abs/2012.01059",
          "publishedOn": "2021-05-23T06:08:18.079Z",
          "wordCount": 634,
          "title": "A Temporally Consistent Image-based Sun Tracking Algorithm for Solar Energy Forecasting Applications. (arXiv:2012.01059v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuqing Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watkins_O/0/1/0/all/0/1\">Olivia Watkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>",
          "description": "Policies trained in simulation often fail when transferred to the real world\ndue to the `reality gap' where the simulator is unable to accurately capture\nthe dynamics and visual properties of the real world. Current approaches to\ntackle this problem, such as domain randomization, require prior knowledge and\nengineering to determine how much to randomize system parameters in order to\nlearn a policy that is robust to sim-to-real transfer while also not being too\nconservative. We propose a method for automatically tuning simulator system\nparameters to match the real world using only raw RGB images of the real world\nwithout the need to define rewards or estimate state. Our key insight is to\nreframe the auto-tuning of parameters as a search problem where we iteratively\nshift the simulation system parameters to approach the real-world system\nparameters. We propose a Search Param Model (SPM) that, given a sequence of\nobservations and actions and a set of system parameters, predicts whether the\ngiven parameters are higher or lower than the true parameters used to generate\nthe observations. We evaluate our method on multiple robotic control tasks in\nboth sim-to-sim and sim-to-real transfer, demonstrating significant improvement\nover naive domain randomization. Project videos and code at\nhttps://yuqingd.github.io/autotuned-sim2real/",
          "link": "http://arxiv.org/abs/2104.07662",
          "publishedOn": "2021-05-23T06:08:18.072Z",
          "wordCount": 681,
          "title": "Auto-Tuned Sim-to-Real Transfer. (arXiv:2104.07662v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Aditya Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girishekar_E/0/1/0/all/0/1\">Eshwar Shamanna Girishekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_P/0/1/0/all/0/1\">Padmakar Anil Deshpande</a>",
          "description": "Automated image captioning is one of the applications of Deep Learning which\ninvolves fusion of work done in computer vision and natural language\nprocessing, and it is typically performed using Encoder-Decoder architectures.\nIn this project, we have implemented and experimented with various flavors of\nmulti-modal image captioning networks where ResNet101, DenseNet121 and VGG19\nbased CNN Encoders and Attention based LSTM Decoders were explored. We have\nstudied the effect of beam size and the use of pretrained word embeddings and\ncompared them to baseline CNN encoder and RNN decoder architecture. The goal is\nto analyze the performance of each approach using various evaluation metrics\nincluding BLEU, CIDEr, ROUGE and METEOR. We have also explored model\nexplainability using Visual Attention Maps (VAM) to highlight parts of the\nimages which has maximum contribution for predicting each word of the generated\ncaption.",
          "link": "http://arxiv.org/abs/2105.09906",
          "publishedOn": "2021-05-23T06:08:18.065Z",
          "wordCount": 582,
          "title": "Empirical Analysis of Image Caption Generation using Deep Learning. (arXiv:2105.09906v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">YiMin Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianbing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yingjie Xi</a>",
          "description": "Detection faults in seismic data is a crucial step for seismic structural\ninterpretation, reservoir characterization and well placement. Some recent\nworks regard it as an image segmentation task. The task of image segmentation\nrequires huge labels, especially 3D seismic data, which has a complex structure\nand lots of noise. Therefore, its annotation requires expert experience and a\nhuge workload. In this study, we present {\\lambda}-BCE and {\\lambda}-smooth\nL1loss to effectively train 3D-CNN by some slices from 3D seismic data, so that\nthe model can learn the segmentation of 3D seismic data from a few 2D slices.\nIn order to fully extract information from limited data and suppress seismic\nnoise, we propose an attention module that can be used for active supervision\ntraining and embedded in the network. The attention heatmap target is generated\nby the original label, and letting it supervise the attention module using the\n{\\lambda}-smooth L1loss. The experiment proves the effectiveness of our loss\nfunction and attention module, it also shows that our method can extract 3D\nseismic features from a few 2D slices labels, and the segmentation effect\nachieves state-of-the-art. We only use 3.3% of the all labels, and we can\nachieve similar performance as using all labels. This work has been submitted\nto the IEEE for possible publication. Copyright may be transferred without\nnotice, after which this version may no longer be accessible.",
          "link": "http://arxiv.org/abs/2105.03857",
          "publishedOn": "2021-05-23T06:08:18.058Z",
          "wordCount": 724,
          "title": "Seismic Fault Segmentation via 3D-CNN Training by a Few 2D Slices Labels. (arXiv:2105.03857v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung-Nghia Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yubo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tan-Cong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1\">Minh-Quan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khanh-Duy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Thanh-Toan Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tam V. Nguyen</a>",
          "description": "This paper pushes the envelope on camouflaged regions to decompose them into\nmeaningful components, namely, camouflaged instances. To promote the new task\nof camouflaged instance segmentation in-the-wild, we introduce a new dataset,\nnamely CAMO++, by extending our preliminary CAMO dataset (camouflaged object\nsegmentation) in terms of quantity and diversity. The new dataset substantially\nincreases the number of images with hierarchical pixel-wise ground-truths. We\nalso provide a benchmark suite for the task of camouflaged instance\nsegmentation. In particular, we conduct extensive evaluation of\nstate-of-the-art instance segmentation methods on our newly constructed CAMO++\ndataset in various scenarios. We also propose Camouflage Fusion Learning (CFL)\nframework for camouflaged instance segmentation to further improve the\nstate-of-the-art performance. The dataset, model, evaluation suite, and\nbenchmark will be publicly available at our project page.\n\\url{https://sites.google.com/view/ltnghia/research/camo\\_plus\\_plus}",
          "link": "http://arxiv.org/abs/2103.17123",
          "publishedOn": "2021-05-23T06:08:18.024Z",
          "wordCount": 611,
          "title": "Camouflaged Instance Segmentation In-The-Wild: Dataset And Benchmark Suite. (arXiv:2103.17123v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hornauer_S/0/1/0/all/0/1\">Sascha Hornauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffarzadegan_S/0/1/0/all/0/1\">Shabnam Ghaffarzadegan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liu Ren</a>",
          "description": "Recent progress in network-based audio event classification has shown the\nbenefit of pre-training models on visual data such as ImageNet. While this\nprocess allows knowledge transfer across different domains, training a model on\nlarge-scale visual datasets is time consuming. On several audio event\nclassification benchmarks, we show a fast and effective alternative that\npre-trains the model unsupervised, only on audio data and yet delivers on-par\nperformance with ImageNet pre-training. Furthermore, we show that our\ndiscriminative audio learning can be used to transfer knowledge across audio\ndatasets and optionally include ImageNet pre-training.",
          "link": "http://arxiv.org/abs/2105.09279",
          "publishedOn": "2021-05-23T06:08:18.017Z",
          "wordCount": 573,
          "title": "Unsupervised Discriminative Learning of Sounds for Audio Event Classification. (arXiv:2105.09279v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03814",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Autthasan_P/0/1/0/all/0/1\">Phairot Autthasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chaisaen_R/0/1/0/all/0/1\">Rattanaphon Chaisaen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sudhawiyangkul_T/0/1/0/all/0/1\">Thapanun Sudhawiyangkul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rangpong_P/0/1/0/all/0/1\">Phurin Rangpong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiatthaveephong_S/0/1/0/all/0/1\">Suktipol Kiatthaveephong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dilokthanakul_N/0/1/0/all/0/1\">Nat Dilokthanakul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhakdisongkhram_G/0/1/0/all/0/1\">Gun Bhakdisongkhram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phan_H/0/1/0/all/0/1\">Huy Phan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1\">Theerawit Wilaiprasitporn</a>",
          "description": "Advances in the motor imagery (MI)-based brain-computer interfaces (BCIs)\nallow control of several applications by decoding neurophysiological phenomena,\nwhich are usually recorded by electroencephalography (EEG) using a non-invasive\ntechnique. Despite great advances in MI-based BCI, EEG rhythms are specific to\na subject and various changes over time. These issues point to significant\nchallenges to enhance the classification performance, especially in a\nsubject-independent manner. To overcome these challenges, we propose MIN2Net, a\nnovel end-to-end multi-task learning to tackle this task. We integrate deep\nmetric learning into a multi-task autoencoder to learn a compact and\ndiscriminative latent representation from EEG and perform classification\nsimultaneously. This approach reduces the complexity in pre-processing, results\nin significant performance improvement on EEG classification. Experimental\nresults in a subject-independent manner show that MIN2Net outperforms the\nstate-of-the-art techniques, achieving an F1-score improvement of 6.72%, and\n2.23% on the SMR-BCI, and OpenBMI datasets, respectively. We demonstrate that\nMIN2Net improves discriminative information in the latent representation. This\nstudy indicates the possibility and practicality of using this model to develop\nMI-based BCI applications for new users without the need for calibration.",
          "link": "http://arxiv.org/abs/2102.03814",
          "publishedOn": "2021-05-23T06:08:18.005Z",
          "wordCount": 664,
          "title": "MIN2Net: End-to-End Multi-Task Learning for Subject-Independent Motor Imagery EEG Classification. (arXiv:2102.03814v3 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09750",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Du_Z/0/1/0/all/0/1\">Zongcai Du</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>",
          "description": "Along with the rapid development of real-world applications, higher\nrequirements on the accuracy and efficiency of image super-resolution (SR) are\nbrought forward. Though existing methods have achieved remarkable success, the\nmajority of them demand plenty of computational resources and large amount of\nRAM, and thus they can not be well applied to mobile device. In this paper, we\naim at designing efficient architecture for 8-bit quantization and deploy it on\nmobile device. First, we conduct an experiment about meta-node latency by\ndecomposing lightweight SR architectures, which determines the portable\noperations we can utilize. Then, we dig deeper into what kind of architecture\nis beneficial to 8-bit quantization and propose anchor-based plain net (ABPN).\nFinally, we adopt quantization-aware training strategy to further boost the\nperformance. Our model can outperform 8-bit quantized FSRCNN by nearly 2dB in\nterms of PSNR, while satisfying realistic needs at the same time. Code is\navaliable at https://github.com/NJU- Jet/SR_Mobile_Quantization.",
          "link": "http://arxiv.org/abs/2105.09750",
          "publishedOn": "2021-05-23T06:08:17.998Z",
          "wordCount": 596,
          "title": "Anchor-based Plain Net for Mobile Image Super-Resolution. (arXiv:2105.09750v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhijian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Alexander Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Sibo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karaman_S/0/1/0/all/0/1\">Sertac Karaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>",
          "description": "Deep learning has been used to demonstrate end-to-end neural network learning\nfor autonomous vehicle control from raw sensory input. While LiDAR sensors\nprovide reliably accurate information, existing end-to-end driving solutions\nare mainly based on cameras since processing 3D data requires a large memory\nfootprint and computation cost. On the other hand, increasing the robustness of\nthese systems is also critical; however, even estimating the model's\nuncertainty is very challenging due to the cost of sampling-based methods. In\nthis paper, we present an efficient and robust LiDAR-based end-to-end\nnavigation framework. We first introduce Fast-LiDARNet that is based on sparse\nconvolution kernel optimization and hardware-aware model design. We then\npropose Hybrid Evidential Fusion that directly estimates the uncertainty of the\nprediction from only a single forward pass and then fuses the control\npredictions intelligently. We evaluate our system on a full-scale vehicle and\ndemonstrate lane-stable as well as navigation capabilities. In the presence of\nout-of-distribution events (e.g., sensor failures), our system significantly\nimproves robustness and reduces the number of takeovers in the real world.",
          "link": "http://arxiv.org/abs/2105.09932",
          "publishedOn": "2021-05-23T06:08:17.961Z",
          "wordCount": 618,
          "title": "Efficient and Robust LiDAR-Based End-to-End Navigation. (arXiv:2105.09932v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pahwa_R/0/1/0/all/0/1\">Ramanpreet S Pahwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1\">Soon Wee Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ren Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_R/0/1/0/all/0/1\">Richard Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_O/0/1/0/all/0/1\">Oo Zaw Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_W/0/1/0/all/0/1\">Wang Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_V/0/1/0/all/0/1\">Vempati Srinivasa Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nwe_T/0/1/0/all/0/1\">Tin Lay Nwe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanjing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_J/0/1/0/all/0/1\">Jens Timo Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pichumani_R/0/1/0/all/0/1\">Ramani Pichumani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregorich_T/0/1/0/all/0/1\">Thomas Gregorich</a>",
          "description": "For over 40 years lithographic silicon scaling has driven circuit integration\nand performance improvement in the semiconductor industry. As silicon scaling\nslows down, the industry is increasingly dependent on IC package technologies\nto contribute to further circuit integration and performance improvements. This\nis a paradigm shift and requires the IC package industry to reduce the size and\nincrease the density of internal interconnects on a scale which has never been\ndone before. Traditional package characterization and process optimization\nrelies on destructive techniques such as physical cross-sections and delayering\nto extract data from internal package features. These destructive techniques\nare not practical with today's advanced packages. In this paper we will\ndemonstrate how data acquired non-destructively with a 3D X-ray microscope can\nbe enhanced and optimized using machine learning, and can then be used to\nmeasure, characterize and optimize the design and production of buried\ninterconnects in advanced IC packages. Test vehicles replicating 2.5D and HBM\nconstruction were designed and fabricated, and digital data was extracted from\nthese test vehicles using 3D X-ray and machine learning techniques. The\nextracted digital data was used to characterize and optimize the design and\nproduction of the interconnects and demonstrates a superior alternative to\ndestructive physical analysis. We report an mAP of 0.96 for 3D object\ndetection, a dice score of 0.92 for 3D segmentation, and an average of 2.1um\nerror for 3D metrology on the test dataset. This paper is the first part of a\nmulti-part report.",
          "link": "http://arxiv.org/abs/2103.04838",
          "publishedOn": "2021-05-23T06:08:17.947Z",
          "wordCount": 754,
          "title": "Machine-learning based methodologies for 3d x-ray measurement, characterization and optimization for buried structures in advanced ic packages. (arXiv:2103.04838v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bouhsain_S/0/1/0/all/0/1\">Smail Ait Bouhsain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadatnejad_S/0/1/0/all/0/1\">Saeed Saadatnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1\">Alexandre Alahi</a>",
          "description": "In order to be globally deployed, autonomous cars must guarantee the safety\nof pedestrians. This is the reason why forecasting pedestrians' intentions\nsufficiently in advance is one of the most critical and challenging tasks for\nautonomous vehicles. This work tries to solve this problem by jointly\npredicting the intention and visual states of pedestrians. In terms of visual\nstates, whereas previous work focused on x-y coordinates, we will also predict\nthe size and indeed the whole bounding box of the pedestrian. The method is a\nrecurrent neural network in a multi-task learning approach. It has one head\nthat predicts the intention of the pedestrian for each one of its future\nposition and another one predicting the visual states of the pedestrian.\nExperiments on the JAAD dataset show the superiority of the performance of our\nmethod compared to previous works for intention prediction. Also, although its\nsimple architecture (more than 2 times faster), the performance of the bounding\nbox prediction is comparable to the ones yielded by much more complex\narchitectures. Our code is available online.",
          "link": "http://arxiv.org/abs/2010.10270",
          "publishedOn": "2021-05-23T06:08:17.928Z",
          "wordCount": 647,
          "title": "Pedestrian Intention Prediction: A Multi-task Perspective. (arXiv:2010.10270v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanli Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lake_B/0/1/0/all/0/1\">Brenden M. Lake</a>",
          "description": "Humans are highly efficient learners, with the ability to grasp the meaning\nof a new concept from just a few examples. Unlike popular computer vision\nsystems, humans can flexibly leverage the compositional structure of the visual\nworld, understanding new concepts as combinations of existing concepts. In the\ncurrent paper, we study how people learn different types of visual\ncompositions, using abstract visual forms with rich relational structure. We\nfind that people can make meaningful compositional generalizations from just a\nfew examples in a variety of scenarios, and we develop a Bayesian program\ninduction model that provides a close fit to the behavioral data. Unlike past\nwork examining special cases of compositionality, our work shows how a single\ncomputational approach can account for many distinct types of compositional\ngeneralization.",
          "link": "http://arxiv.org/abs/2105.09848",
          "publishedOn": "2021-05-23T06:08:17.922Z",
          "wordCount": 587,
          "title": "Flexible Compositional Learning of Structured Visual Concepts. (arXiv:2105.09848v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1\">Andrew Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalogeiton_V/0/1/0/all/0/1\">Vicky Kalogeiton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>",
          "description": "The objective of this work is person-clustering in videos -- grouping\ncharacters according to their identity. Previous methods focus on the narrower\ntask of face-clustering, and for the most part ignore other cues such as the\nperson's voice, their overall appearance (hair, clothes, posture), and the\nediting structure of the videos. Similarly, most current datasets evaluate only\nthe task of face-clustering, rather than person-clustering. This limits their\napplicability to downstream applications such as story understanding which\nrequire person-level, rather than only face-level, reasoning. In this paper we\nmake contributions to address both these deficiencies: first, we introduce a\nMulti-Modal High-Precision Clustering algorithm for person-clustering in videos\nusing cues from several modalities (face, body, and voice). Second, we\nintroduce a Video Person-Clustering dataset, for evaluating multi-modal\nperson-clustering. It contains body-tracks for each annotated character,\nface-tracks when visible, and voice-tracks when speaking, with their associated\nfeatures. The dataset is by far the largest of its kind, and covers films and\nTV-shows representing a wide range of demographics. Finally, we show the\neffectiveness of using multiple modalities for person-clustering, explore the\nuse of this new broad task for story understanding through character\nco-occurrences, and achieve a new state of the art on all available datasets\nfor face and person-clustering.",
          "link": "http://arxiv.org/abs/2105.09939",
          "publishedOn": "2021-05-23T06:08:17.901Z",
          "wordCount": 633,
          "title": "Face, Body, Voice: Video Person-Clustering with Multiple Modalities. (arXiv:2105.09939v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weidler_T/0/1/0/all/0/1\">Tonio Weidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehnen_J/0/1/0/all/0/1\">Julian Lehnen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_Q/0/1/0/all/0/1\">Quinton Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebok_D/0/1/0/all/0/1\">D&#xe1;vid Seb&#x151;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1\">Gerhard Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driessens_K/0/1/0/all/0/1\">Kurt Driessens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senden_M/0/1/0/all/0/1\">Mario Senden</a>",
          "description": "Lateral connections play an important role for sensory processing in visual\ncortex by supporting discriminable neuronal responses even to highly similar\nfeatures. In the present work, we show that establishing a biologically\ninspired Mexican hat lateral connectivity profile along the filter domain can\nsignificantly improve the classification accuracy of a variety of lightweight\nconvolutional neural networks without the addition of trainable network\nparameters. Moreover, we demonstrate that it is possible to analytically\ndetermine the stationary distribution of modulated filter activations and\nthereby avoid using recurrence for modeling temporal dynamics. We furthermore\nreveal that the Mexican hat connectivity profile has the effect of ordering\nfilters in a sequence resembling the topographic organization of feature\nselectivity in early visual cortex. In an ordered filter sequence, this profile\nthen sharpens the filters' tuning curves.",
          "link": "http://arxiv.org/abs/2105.09830",
          "publishedOn": "2021-05-23T06:08:17.895Z",
          "wordCount": 573,
          "title": "Biologically Inspired Semantic Lateral Connectivity for Convolutional Neural Networks. (arXiv:2105.09830v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipayan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Saumik Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1\">Umapada Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1\">Sukalpa Chanda</a>",
          "description": "Reservoir Computing (RC) offers a viable option to deploy AI algorithms on\nlow-end embedded system platforms. Liquid State Machine (LSM) is a bio-inspired\nRC model that mimics the cortical microcircuits and uses spiking neural\nnetworks (SNN) that can be directly realized on neuromorphic hardware. In this\npaper, we present a novel Parallelized LSM (PLSM) architecture that\nincorporates spatio-temporal read-out layer and semantic constraints on model\noutput. To the best of our knowledge, such a formulation has been done for the\nfirst time in literature, and it offers a computationally lighter alternative\nto traditional deep-learning models. Additionally, we also present a\ncomprehensive algorithm for the implementation of parallelizable SNNs and LSMs\nthat are GPU-compatible. We implement the PLSM model to classify\nunintentional/accidental video clips, using the Oops dataset. From the\nexperimental results on detecting unintentional action in video, it can be\nobserved that our proposed model outperforms a self-supervised model and a\nfully supervised traditional deep learning model. All the implemented codes can\nbe found at our repository\nhttps://github.com/anonymoussentience2020/Parallelized_LSM_for_Unintentional_Action_Recognition.",
          "link": "http://arxiv.org/abs/2105.09909",
          "publishedOn": "2021-05-23T06:08:17.884Z",
          "wordCount": 613,
          "title": "PLSM: A Parallelized Liquid State Machine for Unintentional Action Detection. (arXiv:2105.09909v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agu_N/0/1/0/all/0/1\">Nkechinyere N. Agu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Joy T. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1\">Hanqing Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1\">Ismini Lourentzou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Arjun Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Mehdi Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1\">Pingkun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendler_J/0/1/0/all/0/1\">James Hendler</a>",
          "description": "Radiologists usually observe anatomical regions of chest X-ray images as well\nas the overall image before making a decision. However, most existing deep\nlearning models only look at the entire X-ray image for classification, failing\nto utilize important anatomical information. In this paper, we propose a novel\nmulti-label chest X-ray classification model that accurately classifies the\nimage finding and also localizes the findings to their correct anatomical\nregions. Specifically, our model consists of two modules, the detection module\nand the anatomical dependency module. The latter utilizes graph convolutional\nnetworks, which enable our model to learn not only the label dependency but\nalso the relationship between the anatomical regions in the chest X-ray. We\nfurther utilize a method to efficiently create an adjacency matrix for the\nanatomical regions using the correlation of the label across the different\nregions. Detailed experiments and analysis of our results show the\neffectiveness of our method when compared to the current state-of-the-art\nmulti-label chest X-ray image classification methods while also providing\naccurate location information.",
          "link": "http://arxiv.org/abs/2105.09937",
          "publishedOn": "2021-05-23T06:08:17.877Z",
          "wordCount": 619,
          "title": "AnaXNet: Anatomy Aware Multi-label Finding Classification in Chest X-ray. (arXiv:2105.09937v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wangyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Abraham Noah Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1\">Filip Biljecki</a>",
          "description": "There is a prevailing trend to study urban morphology quantitatively thanks\nto the growing accessibility to various forms of spatial big data, increasing\ncomputing power, and use cases benefiting from such information. The methods\ndeveloped up to now measure urban morphology with numerical indices describing\ndensity, proportion, and mixture, but they do not directly represent\nmorphological features from human's visual and intuitive perspective. We take\nthe first step to bridge the gap by proposing a deep learning-based technique\nto automatically classify road networks into four classes on a visual basis.\nThe method is implemented by generating an image of the street network (Colored\nRoad Hierarchy Diagram), which we introduce in this paper, and classifying it\nusing a deep convolutional neural network (ResNet-34). The model achieves an\noverall classification accuracy of 0.875. Nine cities around the world are\nselected as the study areas and their road networks are acquired from\nOpenStreetMap. Latent subgroups among the cities are uncovered through a\nclustering on the percentage of each road network category. In the subsequent\npart of the paper, we focus on the usability of such classification: the\neffectiveness of our human perception augmentation is examined by a case study\nof urban vitality prediction. An advanced tree-based regression model is for\nthe first time designated to establish the relationship between morphological\nindices and vitality indicators. A positive effect of human perception\naugmentation is detected in the comparative experiment of baseline model and\naugmented model. This work expands the toolkit of quantitative urban morphology\nstudy with new techniques, supporting further studies in the future.",
          "link": "http://arxiv.org/abs/2105.09908",
          "publishedOn": "2021-05-23T06:08:17.864Z",
          "wordCount": 703,
          "title": "Classification of Urban Morphology with Deep Learning: Application on Urban Vitality. (arXiv:2105.09908v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.01165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1\">Ilke Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciftci_U/0/1/0/all/0/1\">Umur A. Ciftci</a>",
          "description": "Following the recent initiatives for the democratization of AI, deep fake\ngenerators have become increasingly popular and accessible, causing dystopian\nscenarios towards social erosion of trust. A particular domain, such as\nbiological signals, attracted attention towards detection methods that are\ncapable of exploiting authenticity signatures in real videos that are not yet\nfaked by generative approaches. In this paper, we first propose several\nprominent eye and gaze features that deep fakes exhibit differently. Second, we\ncompile those features into signatures and analyze and compare those of real\nand fake videos, formulating geometric, visual, metric, temporal, and spectral\nvariations. Third, we generalize this formulation to the deep fake detection\nproblem by a deep neural network, to classify any video in the wild as fake or\nreal. We evaluate our approach on several deep fake datasets, achieving 92.48%\naccuracy on FaceForensics++, 80.0% on Deep Fakes (in the wild), 88.35% on\nCelebDF, and 99.27% on DeeperForensics datasets. Our approach outperforms most\ndeep and biological fake detectors with complex network architectures without\nthe proposed gaze signatures. We conduct ablation studies involving different\nfeatures, architectures, sequence durations, and post-processing artifacts.",
          "link": "http://arxiv.org/abs/2101.01165",
          "publishedOn": "2021-05-23T06:08:17.845Z",
          "wordCount": 660,
          "title": "Where Do Deep Fakes Look? Synthetic Face Detection via Gaze Tracking. (arXiv:2101.01165v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Ran Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mingkun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rujun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bo Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhuoling Xiao</a>",
          "description": "The technology for Visual Odometry (VO) that estimates the position and\norientation of the moving object through analyzing the image sequences captured\nby on-board cameras, has been well investigated with the rising interest in\nautonomous driving. This paper studies monocular VO from the perspective of\nDeep Learning (DL). Unlike most current learning-based methods, our approach,\ncalled DeepAVO, is established on the intuition that features contribute\ndiscriminately to different motion patterns. Specifically, we present a novel\nfour-branch network to learn the rotation and translation by leveraging\nConvolutional Neural Networks (CNNs) to focus on different quadrants of optical\nflow input. To enhance the ability of feature selection, we further introduce\nan effective channel-spatial attention mechanism to force each branch to\nexplicitly distill related information for specific Frame to Frame (F2F) motion\nestimation. Experiments on various datasets involving outdoor driving and\nindoor walking scenarios show that the proposed DeepAVO outperforms the\nstate-of-the-art monocular methods by a large margin, demonstrating competitive\nperformance to the stereo VO algorithm and verifying promising potential for\ngeneralization.",
          "link": "http://arxiv.org/abs/2105.09899",
          "publishedOn": "2021-05-23T06:08:17.838Z",
          "wordCount": 619,
          "title": "DeepAVO: Efficient Pose Refining with Feature Distilling for Deep Visual Odometry. (arXiv:2105.09899v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2005.13934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hug_R/0/1/0/all/0/1\">Ronny Hug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1\">Stefan Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubner_W/0/1/0/all/0/1\">Wolfgang H&#xfc;bner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1\">Michael Arens</a>",
          "description": "Methods to quantify the complexity of trajectory datasets are still a missing\npiece in benchmarking human trajectory prediction models. In order to gain a\nbetter understanding of the complexity of trajectory prediction tasks and\nfollowing the intuition, that more complex datasets contain more information,\nan approach for quantifying the amount of information contained in a dataset\nfrom a prototype-based dataset representation is proposed. The dataset\nrepresentation is obtained by first employing a non-trivial spatial sequence\nalignment, which enables a subsequent learning vector quantization (LVQ) stage.\nA large-scale complexity analysis is conducted on several human trajectory\nprediction benchmarking datasets, followed by a brief discussion on indications\nfor human trajectory prediction and benchmarking.",
          "link": "http://arxiv.org/abs/2005.13934",
          "publishedOn": "2021-05-23T06:08:17.832Z",
          "wordCount": 600,
          "title": "Quantifying the Complexity of Standard Benchmarking Datasets for Long-Term Human Trajectory Prediction. (arXiv:2005.13934v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.01383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siebert_J/0/1/0/all/0/1\">Jan Paul Siebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangrong Xu</a>",
          "description": "This paper proposes a novel automatically generating image masks method for\nthe state-of-the-art Mask R-CNN deep learning method. The Mask R-CNN method\nachieves the best results in object detection until now, however, it is very\ntime-consuming and laborious to get the object Masks for training, the proposed\nmethod is composed by a two-stage design, to automatically generating image\nmasks, the first stage implements a fully convolutional networks (FCN) based\nsegmentation network, the second stage network, a Mask R-CNN based object\ndetection network, which is trained on the object image masks from FCN output,\nthe original input image, and additional label information. Through\nexperimentation, our proposed method can obtain the image masks automatically\nto train Mask R-CNN, and it can achieve very high classification accuracy with\nan over 90% mean of average precision (mAP) for segmentation",
          "link": "http://arxiv.org/abs/2003.01383",
          "publishedOn": "2021-05-23T06:08:17.823Z",
          "wordCount": 603,
          "title": "Fully Convolutional Networks for Automatically Generating Image Masks to Train Mask R-CNN. (arXiv:2003.01383v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.02161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_N/0/1/0/all/0/1\">Nivedita Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>",
          "description": "Automated Parking is becoming a standard feature in modern vehicles. Existing\nparking systems build a local map to be able to plan for maneuvering towards a\ndetected slot. Next generation parking systems have an use case where they\nbuild a persistent map of the environment where the car is frequently parked,\nsay for example, home parking or office parking. The pre-built map helps in\nre-localizing the vehicle better when its trying to park the next time. This is\nachieved by augmenting the parking system with a Visual SLAM pipeline and the\nfeature is called trained trajectory parking in the automotive industry. In\nthis paper, we discuss the use cases, design and implementation of a trained\ntrajectory automated parking system. The proposed system is deployed on\ncommercial vehicles and the consumer application is illustrated in\n\\url{https://youtu.be/nRWF5KhyJZU}. The focus of this paper is on the\napplication and the details of vision algorithms are kept at high level.",
          "link": "http://arxiv.org/abs/2001.02161",
          "publishedOn": "2021-05-23T06:08:17.817Z",
          "wordCount": 642,
          "title": "Trained Trajectory based Automated Parking System using Visual SLAM on Surround View Cameras. (arXiv:2001.02161v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.13693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pares_F/0/1/0/all/0/1\">Ferran Par&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arias_Duart_A/0/1/0/all/0/1\">Anna Arias-Duart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Gasulla_D/0/1/0/all/0/1\">Dario Garcia-Gasulla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campo_Frances_G/0/1/0/all/0/1\">Gema Campo-Franc&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viladrich_N/0/1/0/all/0/1\">Nina Viladrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayguade_E/0/1/0/all/0/1\">Eduard Ayguad&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labarta_J/0/1/0/all/0/1\">Jes&#xfa;s Labarta</a>",
          "description": "In the image classification task, the most common approach is to resize all\nimages in a dataset to a unique shape, while reducing their precision to a size\nwhich facilitates experimentation at scale. This practice has benefits from a\ncomputational perspective, but it entails negative side-effects on performance\ndue to loss of information and image deformation. In this work we introduce the\nMAMe dataset, an image classification dataset with remarkable high resolution\nand variable shape properties. The goal of MAMe is to provide a tool for\nstudying the impact of such properties in image classification, while\nmotivating research in the field. The MAMe dataset contains thousands of\nartworks from three different museums, and proposes a classification task\nconsisting on differentiating between 29 mediums (i.e. materials and\ntechniques) supervised by art experts. After reviewing the singularity of MAMe\nin the context of current image classification tasks, a thorough description of\nthe task is provided, together with dataset statistics. Experiments are\nconducted to evaluate the impact of using high resolution images, variable\nshape inputs and both properties at the same time. Results illustrate the\npositive impact in performance when using high resolution images, while\nhighlighting the lack of solutions to exploit variable shapes. An additional\nexperiment exposes the distinctiveness between the MAMe dataset and the\nprototypical ImageNet dataset. Finally, the baselines are inspected using\nexplainability methods and expert knowledge, to gain insights on the challenges\nthat remain ahead.",
          "link": "http://arxiv.org/abs/2007.13693",
          "publishedOn": "2021-05-23T06:08:17.797Z",
          "wordCount": 725,
          "title": "The MAMe Dataset: On the relevance of High Resolution and Variable Shape image properties. (arXiv:2007.13693v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Thai_B/0/1/0/all/0/1\">Binh Nguyen-Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1\">Vuong Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_C/0/1/0/all/0/1\">Catherine Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badawi_N/0/1/0/all/0/1\">Nadia Badawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>",
          "description": "The absence or abnormality of fidgety movements of joints or limbs is\nstrongly indicative of cerebral palsy in infants. Developing computer-based\nmethods for assessing infant movements in videos is pivotal for improved\ncerebral palsy screening. Most existing methods use appearance-based features\nand are thus sensitive to strong but irrelevant signals caused by background\nclutter or a moving camera. Moreover, these features are computed over the\nwhole frame, thus they measure gross whole body movements rather than specific\njoint/limb motion.\n\nAddressing these challenges, we develop and validate a new method for fidgety\nmovement assessment from consumer-grade videos using human poses extracted from\nshort clips. Human poses capture only relevant motion profiles of joints and\nlimbs and are thus free from irrelevant appearance artifacts. The dynamics and\ncoordination between joints are modeled using spatio-temporal graph\nconvolutional networks. Frames and body parts that contain discriminative\ninformation about fidgety movements are selected through a spatio-temporal\nattention mechanism. We validate the proposed model on the cerebral palsy\nscreening task using a real-life consumer-grade video dataset collected at an\nAustralian hospital through the Cerebral Palsy Alliance, Australia. Our\nexperiments show that the proposed method achieves the ROC-AUC score of 81.87%,\nsignificantly outperforming existing competing methods with better\ninterpretability.",
          "link": "http://arxiv.org/abs/2105.09783",
          "publishedOn": "2021-05-23T06:08:17.790Z",
          "wordCount": 657,
          "title": "A Spatio-temporal Attention-based Model for Infant Movement Assessment from Videos. (arXiv:2105.09783v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1\">Xiaoguang Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiankun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_W/0/1/0/all/0/1\">Wenjie Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "In real-world scenarios, many factors may harm face recognition performance,\ne.g., large pose, bad illumination,low resolution, blur and noise. To address\nthese challenges, previous efforts usually first restore the low-quality faces\nto high-quality ones and then perform face recognition. However, most of these\nmethods are stage-wise, which is sub-optimal and deviates from the reality. In\nthis paper, we address all these challenges jointly for unconstrained face\nrecognition. We propose an Multi-Degradation Face Restoration (MDFR) model to\nrestore frontalized high-quality faces from the given low-quality ones under\narbitrary facial poses, with three distinct novelties. First, MDFR is a\nwell-designed encoder-decoder architecture which extracts feature\nrepresentation from an input face image with arbitrary low-quality factors and\nrestores it to a high-quality counterpart. Second, MDFR introduces a pose\nresidual learning strategy along with a 3D-based Pose Normalization Module\n(PNM), which can perceive the pose gap between the input initial pose and its\nreal-frontal pose to guide the face frontalization. Finally, MDFR can generate\nfrontalized high-quality face images by a single unified network, showing a\nstrong capability of preserving face identity. Qualitative and quantitative\nexperiments on both controlled and in-the-wild benchmarks demonstrate the\nsuperiority of MDFR over state-of-the-art methods on both face frontalization\nand face restoration.",
          "link": "http://arxiv.org/abs/2105.09907",
          "publishedOn": "2021-05-23T06:08:17.782Z",
          "wordCount": 647,
          "title": "Joint Face Image Restoration and Frontalization for Recognition. (arXiv:2105.09907v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Clever_H/0/1/0/all/0/1\">Henry M. Clever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grady_P/0/1/0/all/0/1\">Patrick Grady</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turk_G/0/1/0/all/0/1\">Greg Turk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemp_C/0/1/0/all/0/1\">Charles C. Kemp</a>",
          "description": "Contact pressure between the human body and its surroundings has important\nimplications. For example, it plays a role in comfort, safety, posture, and\nhealth. We present a method that infers contact pressure between a human body\nand a mattress from a depth image. Specifically, we focus on using a depth\nimage from a downward facing camera to infer pressure on a body at rest in bed\noccluded by bedding, which is directly applicable to the prevention of pressure\ninjuries in healthcare. Our approach involves augmenting a real dataset with\nsynthetic data generated via a soft-body physics simulation of a human body, a\nmattress, a pressure sensing mat, and a blanket. We introduce a novel deep\nnetwork that we trained on an augmented dataset and evaluated with real data.\nThe network contains an embedded human body mesh model and uses a white-box\nmodel of depth and pressure image generation. Our network successfully infers\nbody pose, outperforming prior work. It also infers contact pressure across a\n3D mesh model of the human body, which is a novel capability, and does so in\nthe presence of occlusion from blankets.",
          "link": "http://arxiv.org/abs/2105.09936",
          "publishedOn": "2021-05-23T06:08:17.774Z",
          "wordCount": 633,
          "title": "BodyPressure -- Inferring Body Pose and Contact Pressure from a Depth Image. (arXiv:2105.09936v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2002.08797",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hayou_S/0/1/0/all/0/1\">Soufiane Hayou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ton_J/0/1/0/all/0/1\">Jean-Francois Ton</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>",
          "description": "Overparameterized Neural Networks (NN) display state-of-the-art performance.\nHowever, there is a growing need for smaller, energy-efficient, neural networks\ntobe able to use machine learning applications on devices with limited\ncomputational resources. A popular approach consists of using pruning\ntechniques. While these techniques have traditionally focused on pruning\npre-trained NN (LeCun et al.,1990; Hassibi et al., 1993), recent work by Lee et\nal. (2018) has shown promising results when pruning at initialization. However,\nfor Deep NNs, such procedures remain unsatisfactory as the resulting pruned\nnetworks can be difficult to train and, for instance, they do not prevent one\nlayer from being fully pruned. In this paper, we provide a comprehensive\ntheoretical analysis of Magnitude and Gradient based pruning at initialization\nand training of sparse architectures. This allows us to propose novel\nprincipled approaches which we validate experimentally on a variety of NN\narchitectures.",
          "link": "http://arxiv.org/abs/2002.08797",
          "publishedOn": "2021-05-23T06:08:17.767Z",
          "wordCount": 619,
          "title": "Robust Pruning at Initialization. (arXiv:2002.08797v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsotsos_J/0/1/0/all/0/1\">John K. Tsotsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jun Luo</a>",
          "description": "Learned networks in the domain of visual recognition and cognition impress in\npart because even though they are trained with datasets many orders of\nmagnitude smaller than the full population of possible images, they exhibit\nsufficient generalization to be applicable to new and previously unseen data.\nAlthough many have examined issues regarding generalization from several\nperspectives, we wondered If a network is trained with a biased dataset that\nmisses particular samples corresponding to some defining domain attribute, can\nit generalize to the full domain from which that training dataset was\nextracted? It is certainly true that in vision, no current training set fully\ncaptures all visual information and this may lead to Selection Bias. Here, we\ntry a novel approach in the tradition of the Thought Experiment. We run this\nthought experiment on a real domain of visual objects that we can fully\ncharacterize and look at specific gaps in training data and their impact on\nperformance requirements. Our thought experiment points to three conclusions:\nfirst, that generalization behavior is dependent on how sufficiently the\nparticular dimensions of the domain are represented during training; second,\nthat the utility of any generalization is completely dependent on the\nacceptable system error; and third, that specific visual features of objects,\nsuch as pose orientations out of the imaging plane or colours, may not be\nrecoverable if not represented sufficiently in a training set. Any currently\nobserved generalization in modern deep learning networks may be more the result\nof coincidental alignments and whose utility needs to be confirmed with respect\nto a system's performance specification. Our Thought Experiment Probe approach,\ncoupled with the resulting Bias Breakdown can be very informative towards\nunderstanding the impact of biases.",
          "link": "http://arxiv.org/abs/2105.09934",
          "publishedOn": "2021-05-23T06:08:17.749Z",
          "wordCount": 730,
          "title": "Probing the Effect of Selection Bias on NN Generalization with a Thought Experiment. (arXiv:2105.09934v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arnavaz_K/0/1/0/all/0/1\">Kasra Arnavaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1\">Oswin Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krivokapic_J/0/1/0/all/0/1\">Jelena M. Krivokapic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilmann_S/0/1/0/all/0/1\">Silja Heilmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baerentzen_J/0/1/0/all/0/1\">Jakob Andreas B&#xe6;rentzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyeng_P/0/1/0/all/0/1\">Pia Nyeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1\">Aasa Feragen</a>",
          "description": "Motivated by a challenging tubular network segmentation task, this paper\ntackles two commonly encountered problems in biomedical imaging: Topological\nconsistency of the segmentation, and limited annotations. We propose a\ntopological score which measures both topological and geometric consistency\nbetween the predicted and ground truth segmentations, applied for model\nselection and validation. We apply our topological score in three scenarios: i.\na U-net ii. a U-net pretrained on an autoencoder, and iii. a semisupervised\nU-net architecture, which offers a straightforward approach to jointly training\nthe network both as an autoencoder and a segmentation algorithm. This allows us\nto utilize un-annotated data for training a representation that generalizes\nacross test data variability, in spite of our annotated training data having\nvery limited variation. Our contributions are validated on a challenging\nsegmentation task, locating tubular structures in the fetal pancreas from noisy\nlive imaging confocal microscopy.",
          "link": "http://arxiv.org/abs/2105.09737",
          "publishedOn": "2021-05-23T06:08:17.742Z",
          "wordCount": 590,
          "title": "Semi-supervised, Topology-Aware Segmentation of Tubular Structures from Live Imaging 3D Microscopy. (arXiv:2105.09737v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kothari_R/0/1/0/all/0/1\">Rakshit Kothari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1\">Shalini De Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_U/0/1/0/all/0/1\">Umar Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byeon_W/0/1/0/all/0/1\">Wonmin Byeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seonwook Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>",
          "description": "A major challenge for physically unconstrained gaze estimation is acquiring\ntraining data with 3D gaze annotations for in-the-wild and outdoor scenarios.\nIn contrast, videos of human interactions in unconstrained environments are\nabundantly available and can be much more easily annotated with frame-level\nactivity labels. In this work, we tackle the previously unexplored problem of\nweakly-supervised gaze estimation from videos of human interactions. We\nleverage the insight that strong gaze-related geometric constraints exist when\npeople perform the activity of \"looking at each other\" (LAEO). To acquire\nviable 3D gaze supervision from LAEO labels, we propose a training algorithm\nalong with several novel loss functions especially designed for the task. With\nweak supervision from two large scale CMU-Panoptic and AVA-LAEO activity\ndatasets, we show significant improvements in (a) the accuracy of\nsemi-supervised gaze estimation and (b) cross-domain generalization on the\nstate-of-the-art physically unconstrained in-the-wild Gaze360 gaze estimation\nbenchmark. We open source our code at\nhttps://github.com/NVlabs/weakly-supervised-gaze.",
          "link": "http://arxiv.org/abs/2105.09803",
          "publishedOn": "2021-05-23T06:08:17.735Z",
          "wordCount": 588,
          "title": "Weakly-Supervised Physically Unconstrained Gaze Estimation. (arXiv:2105.09803v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_M/0/1/0/all/0/1\">Manav Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakob_P/0/1/0/all/0/1\">Peter Jakob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_Schirling_T/0/1/0/all/0/1\">Tobias Schmid-Schirling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>",
          "description": "Multi-view classification is inspired by the behavior of humans, especially\nwhen fine-grained features or in our case rarely occurring anomalies are to be\ndetected. Current contributions point to the problem of how high-dimensional\ndata can be fused. In this work, we build upon the deep support vector data\ndescription algorithm and address multi-perspective anomaly detection using\nthree different fusion techniques i.e. early fusion, late fusion, and late\nfusion with multiple decoders. We employ different augmentation techniques with\na denoising process to deal with scarce one-class data, which further improves\nthe performance (ROC AUC = 80\\%). Furthermore, we introduce the dices dataset\nthat consists of over 2000 grayscale images of falling dices from multiple\nperspectives, with 5\\% of the images containing rare anomalies (e.g. drill\nholes, sawing, or scratches). We evaluate our approach on the new dices dataset\nusing images from two different perspectives and also benchmark on the standard\nMNIST dataset. Extensive experiments demonstrate that our proposed approach\nexceeds the state-of-the-art on both the MNIST and dices datasets. To the best\nof our knowledge, this is the first work that focuses on addressing\nmulti-perspective anomaly detection in images by jointly using different\nperspectives together with one single objective function for anomaly detection.",
          "link": "http://arxiv.org/abs/2105.09903",
          "publishedOn": "2021-05-23T06:08:17.679Z",
          "wordCount": 633,
          "title": "Multi-Perspective Anomaly Detection. (arXiv:2105.09903v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fonder_M/0/1/0/all/0/1\">Micha&#xeb;l Fonder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_D/0/1/0/all/0/1\">Damien Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droogenbroeck_M/0/1/0/all/0/1\">Marc Van Droogenbroeck</a>",
          "description": "Getting the distance to objects is crucial for autonomous vehicles. In\ninstances where depth sensors cannot be used, this distance has to be estimated\nfrom RGB cameras. As opposed to cars, the task of estimating depth from\non-board mounted cameras is made complex on drones because of the lack of\nconstrains on motion during flights. %In the case of drones, this task is even\nmore complex than for car-mounted cameras since the camera motion is\nunconstrained. In this paper, we present a method to estimate the distance of\nobjects seen by an on-board mounted camera by using its RGB video stream and\ndrone motion information. Our method is built upon a pyramidal convolutional\nneural network architecture and uses time recurrence in pair with geometric\nconstraints imposed by motion to produce pixel-wise depth maps. %from a RGB\nvideo stream of a camera attached to the drone In our architecture, each level\nof the pyramid is designed to produce its own depth estimate based on past\nobservations and information provided by the previous level in the pyramid. We\nintroduce a spatial reprojection layer to maintain the spatio-temporal\nconsistency of the data between the levels. We analyse the performance of our\napproach on Mid-Air, a public drone dataset featuring synthetic drone\ntrajectories recorded in a wide variety of unstructured outdoor environments.\nOur experiments show that our network outperforms state-of-the-art depth\nestimation methods and that the use of motion information is the main\ncontributing factor for this improvement. The code of our method is publicly\navailable on GitHub; see\n$\\href{https://github.com/michael-fonder/M4Depth}{\\text{https://github.com/michael-fonder/M4Depth}}$",
          "link": "http://arxiv.org/abs/2105.09847",
          "publishedOn": "2021-05-23T06:08:17.646Z",
          "wordCount": 705,
          "title": "M4Depth: A motion-based approach for monocular depth estimation on video sequences. (arXiv:2105.09847v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09913",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Perera_S/0/1/0/all/0/1\">Shehan Perera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adhikari_S/0/1/0/all/0/1\">Srikar Adhikari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yilmaz_A/0/1/0/all/0/1\">Alper Yilmaz</a>",
          "description": "The rapid and seemingly endless expansion of COVID-19 can be traced back to\nthe inefficiency and shortage of testing kits that offer accurate results in a\ntimely manner. An emerging popular technique, which adopts improvements made in\nmobile ultrasound technology, allows for healthcare professionals to conduct\nrapid screenings on a large scale. We present an image-based solution that aims\nat automating the testing process which allows for rapid mass testing to be\nconducted with or without a trained medical professional that can be applied to\nrural environments and third world countries. Our contributions towards rapid\nlarge-scale testing include a novel deep learning architecture capable of\nanalyzing ultrasound data that can run in real-time and significantly improve\nthe current state-of-the-art detection accuracies using image-based COVID-19\ndetection.",
          "link": "http://arxiv.org/abs/2105.09913",
          "publishedOn": "2021-05-23T06:08:17.622Z",
          "wordCount": 617,
          "title": "POCFormer: A Lightweight Transformer Architecture for Detection of COVID-19 Using Point of Care Ultrasound. (arXiv:2105.09913v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McNally_W/0/1/0/all/0/1\">William McNally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walters_P/0/1/0/all/0/1\">Pascale Walters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vats_K/0/1/0/all/0/1\">Kanav Vats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McPhee_J/0/1/0/all/0/1\">John McPhee</a>",
          "description": "Existing multi-camera solutions for automatic scorekeeping in steel-tip darts\nare very expensive and thus inaccessible to most players. Motivated to develop\na more accessible low-cost solution, we present a new approach to keypoint\ndetection and apply it to predict dart scores from a single image taken from\nany camera angle. This problem involves detecting multiple keypoints that may\nbe of the same class and positioned in close proximity to one another. The\nwidely adopted framework for regressing keypoints using heatmaps is not\nwell-suited for this task. To address this issue, we instead propose to model\nkeypoints as objects. We develop a deep convolutional neural network around\nthis idea and use it to predict dart locations and dartboard calibration points\nwithin an overall pipeline for automatic dart scoring, which we call DeepDarts.\nAdditionally, we propose several task-specific data augmentation strategies to\nimprove the generalization of our method. As a proof of concept, two datasets\ncomprising 16k images originating from two different dartboard setups were\nmanually collected and annotated to evaluate the system. In the primary dataset\ncontaining 15k images captured from a face-on view of the dartboard using a\nsmartphone, DeepDarts predicted the total score correctly in 94.7% of the test\nimages. In a second more challenging dataset containing limited training data\n(830 images) and various camera angles, we utilize transfer learning and\nextensive data augmentation to achieve a test accuracy of 84.0%. Because\nDeepDarts relies only on single images, it has the potential to be deployed on\nedge devices, giving anyone with a smartphone access to an automatic dart\nscoring system for steel-tip darts. The code and datasets are available.",
          "link": "http://arxiv.org/abs/2105.09880",
          "publishedOn": "2021-05-23T06:08:17.580Z",
          "wordCount": 718,
          "title": "DeepDarts: Modeling Keypoints as Objects for Automatic Scorekeeping in Darts using a Single Camera. (arXiv:2105.09880v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borkar_J/0/1/0/all/0/1\">Jaydeep Borkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>",
          "description": "There has been a rise in the use of Machine Learning as a Service (MLaaS)\nVision APIs as they offer multiple services including pre-built models and\nalgorithms, which otherwise take a huge amount of resources if built from\nscratch. As these APIs get deployed for high-stakes applications, it's very\nimportant that they are robust to different manipulations. Recent works have\nonly focused on typical adversarial attacks when evaluating the robustness of\nvision APIs. We propose two new aspects of adversarial image generation methods\nand evaluate them on the robustness of Google Cloud Vision API's optical\ncharacter recognition service and object detection APIs deployed in real-world\nsettings such as sightengine.com, picpurify.com, Google Cloud Vision API, and\nMicrosoft Azure's Computer Vision API. Specifically, we go beyond the\nconventional small-noise adversarial attacks and introduce secret embedding and\ntransparent adversarial examples as a simpler way to evaluate robustness. These\nmethods are so straightforward that even non-specialists can craft such\nattacks. As a result, they pose a serious threat where APIs are used for\nhigh-stakes applications. Our transparent adversarial examples successfully\nevade state-of-the art object detections APIs such as Azure Cloud Vision\n(attack success rate 52%) and Google Cloud Vision (attack success rate 36%).\n90% of the images have a secret embedded text that successfully fools the\nvision of time-limited humans but is detected by Google Cloud Vision API's\noptical character recognition. Complementing to current research, our results\nprovide simple but unconventional methods on robustness evaluation.",
          "link": "http://arxiv.org/abs/2105.09685",
          "publishedOn": "2021-05-23T06:08:17.573Z",
          "wordCount": 694,
          "title": "Simple Transparent Adversarial Examples. (arXiv:2105.09685v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xianzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jianyang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yiqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shuting He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>",
          "description": "This paper introduces our solution for the Track2 in AI City Challenge 2021\n(AICITY21). The Track2 is a vehicle re-identification (ReID) task with both the\nreal-world data and synthetic data. We mainly focus on four points, i.e.\ntraining data, unsupervised domain-adaptive (UDA) training, post-processing,\nmodel ensembling in this challenge. (1) Both cropping training data and using\nsynthetic data can help the model learn more discriminative features. (2) Since\nthere is a new scenario in the test set that dose not appear in the training\nset, UDA methods perform well in the challenge. (3) Post-processing techniques\nincluding re-ranking, image-to-track retrieval, inter-camera fusion, etc,\nsignificantly improve final performance. (4) We ensemble CNN-based models and\ntransformer-based models which provide different representation diversity. With\naforementioned techniques, our method finally achieves 0.7445 mAP score,\nyielding the first place in the competition. Codes are available at\nhttps://github.com/michuanhaohao/AICITY2021_Track2_DMT.",
          "link": "http://arxiv.org/abs/2105.09701",
          "publishedOn": "2021-05-23T06:08:17.562Z",
          "wordCount": 610,
          "title": "An Empirical Study of Vehicle Re-Identification on the AI City Challenge. (arXiv:2105.09701v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yukai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jinghui Qin</a>",
          "description": "Deep convolutional networks have attracted great attention in image\nrestoration and enhancement. Generally, restoration quality has been improved\nby building more and more convolutional block. However, these methods mostly\nlearn a specific model to handle all images and ignore difficulty diversity. In\nother words, an area in the image with high frequency tend to lose more\ninformation during compressing while an area with low frequency tends to lose\nless. In this article, we adrress the efficiency issue in image SR by\nincorporating a patch-wise rolling network(PRN) to content-adaptively recover\nimages according to difficulty levels. In contrast to existing studies that\nignore difficulty diversity, we adopt different stage of a neural network to\nperform image restoration. In addition, we propose a rolling strategy that\nutilizes the parameters of each stage more flexible. Extensive experiments\ndemonstrate that our model not only shows a significant acceleration but also\nmaintain state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2105.09645",
          "publishedOn": "2021-05-23T06:08:17.554Z",
          "wordCount": 572,
          "title": "Content-adaptive Representation Learning for Fast Image Super-resolution. (arXiv:2105.09645v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09511",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shaohua Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sui_X/0/1/0/all/0/1\">Xiuchao Sui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xiangde Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xinxing Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goh_R/0/1/0/all/0/1\">Rick Siow Mong Goh</a>",
          "description": "Medical image segmentation is important for computer-aided diagnosis. Good\nsegmentation demands the model to see the big picture and fine details\nsimultaneously, i.e., to learn image features that incorporate large context\nwhile keep high spatial resolutions. To approach this goal, the most widely\nused methods -- U-Net and variants, extract and fuse multi-scale features.\nHowever, the fused features still have small \"effective receptive fields\" with\na focus on local image cues, limiting their performance. In this work, we\npropose Segtran, an alternative segmentation framework based on transformers,\nwhich have unlimited \"effective receptive fields\" even at high feature\nresolutions. The core of Segtran is a novel Squeeze-and-Expansion transformer:\na squeezed attention block regularizes the self attention of transformers, and\nan expansion block learns diversified representations. Additionally, we propose\na new positional encoding scheme for transformers, imposing a continuity\ninductive bias for images. Experiments were performed on 2D and 3D medical\nimage segmentation tasks: optic disc/cup segmentation in fundus images\n(REFUGE'20 challenge), polyp segmentation in colonoscopy images, and brain\ntumor segmentation in MRI scans (BraTS'19 challenge). Compared with\nrepresentative existing methods, Segtran consistently achieved the highest\nsegmentation accuracy, and exhibited good cross-domain generalization\ncapabilities.",
          "link": "http://arxiv.org/abs/2105.09511",
          "publishedOn": "2021-05-23T06:08:17.533Z",
          "wordCount": 638,
          "title": "Medical Image Segmentation using Squeeze-and-Expansion Transformers. (arXiv:2105.09511v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09720",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mudiyanselage_T/0/1/0/all/0/1\">Thosini Bamunu Mudiyanselage</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Senanayake_N/0/1/0/all/0/1\">Nipuna Senanayake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_C/0/1/0/all/0/1\">Chunyan Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1\">Yi Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanqing Zhang</a>",
          "description": "The novel corona virus (Covid-19) has introduced significant challenges due\nto its rapid spreading nature through respiratory transmission. As a result,\nthere is a huge demand for Artificial Intelligence (AI) based quick disease\ndiagnosis methods as an alternative to high demand tests such as Polymerase\nChain Reaction (PCR). Chest X-ray (CXR) Image analysis is such cost-effective\nradiography technique due to resource availability and quick screening. But, a\nsufficient and systematic data collection that is required by complex deep\nleaning (DL) models is more difficult and hence there are recent efforts that\nutilize transfer learning to address this issue. Still these transfer learnt\nmodels suffer from lack of generalization and increased bias to the training\ndataset resulting poor performance for unseen data. Limited correlation of the\ntransferred features from the pre-trained model to a specific medical imaging\ndomain like X-ray and overfitting on fewer data can be reasons for this\ncircumstance. In this work, we propose a novel Graph Convolution Neural Network\n(GCN) that is capable of identifying bio-markers of Covid-19 pneumonia from CXR\nimages and meta information about patients. The proposed method exploits\nimportant relational knowledge between data instances and their features using\ngraph representation and applies convolution to learn the graph data which is\nnot possible with conventional convolution on Euclidean domain. The results of\nextensive experiments of proposed model on binary (Covid vs normal) and three\nclass (Covid, normal, other pneumonia) classification problems outperform\ndifferent benchmark transfer learnt models, hence overcoming the aforementioned\ndrawbacks.",
          "link": "http://arxiv.org/abs/2105.09720",
          "publishedOn": "2021-05-23T06:08:17.526Z",
          "wordCount": 747,
          "title": "Covid-19 Detection from Chest X-ray and Patient Metadata using Graph Convolutional Neural Networks. (arXiv:2105.09720v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09683",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cheng_B/0/1/0/all/0/1\">Bo Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_R/0/1/0/all/0/1\">Ruhui Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Hang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1\">Laili Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiang_W/0/1/0/all/0/1\">Wei Xiang</a>",
          "description": "Background and Objective: The new type of coronavirus is also called\nCOVID-19. It began to spread at the end of 2019 and has now spread across the\nworld. Until October 2020, It has infected around 37 million people and claimed\nabout 1 million lives. We propose a deep learning model that can help\nradiologists and clinicians use chest X-rays to diagnose COVID-19 cases and\nshow the diagnostic features of pneumonia. Methods: The approach in this study\nis: 1) we propose a data enhancement method to increase the diversity of the\ndata set, thereby improving the generalization performance of the model. 2) Our\ndeep convolution neural network model DPN-SE adds a self-attention mechanism to\nthe DPN network. The addition of a self-attention mechanism has greatly\nimproved the performance of the network. 3) Use the Lime interpretable library\nto mark the feature regions on the X-ray medical image that helps doctors more\nquickly diagnose COVID-19 in people. Results: Under the same network model, the\ndata with and without data enhancement is put into the model for training\nrespectively. At last, comparing two experimental results: among the 10 network\nmodels with different structures, 7 network models have improved their effects\nafter using data enhancement, with an average improvement of 1% in recognition\naccuracy. We propose that the accuracy and recall rates of the DPN-SE network\nare 93% and 98% of cases (COVID vs. pneumonia bacteria vs. viral pneumonia vs.\nnormal). Compared with the original DPN, the respective accuracy is improved by\n2%. Conclusion: The data augmentation method we used has achieved effective\nresults on a small amount of data set, showing that a reasonable data\naugmentation method can improve the recognition accuracy without changing the\nsample size and model structure. Overall, the proposed method and model can\neffectively become a very useful tool for clinical radiologists.",
          "link": "http://arxiv.org/abs/2105.09683",
          "publishedOn": "2021-05-23T06:08:17.519Z",
          "wordCount": 807,
          "title": "DPN-SENet:A self-attention mechanism neural network for detection and diagnosis of COVID-19 from chest x-ray images. (arXiv:2105.09683v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09600",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hu_Z/0/1/0/all/0/1\">Zhihao Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_G/0/1/0/all/0/1\">Guo Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Dong Xu</a>",
          "description": "Learning based video compression attracts increasing attention in the past\nfew years. The previous hybrid coding approaches rely on pixel space operations\nto reduce spatial and temporal redundancy, which may suffer from inaccurate\nmotion estimation or less effective motion compensation. In this work, we\npropose a feature-space video coding network (FVC) by performing all major\noperations (i.e., motion estimation, motion compression, motion compensation\nand residual compression) in the feature space. Specifically, in the proposed\ndeformable compensation module, we first apply motion estimation in the feature\nspace to produce motion information (i.e., the offset maps), which will be\ncompressed by using the auto-encoder style network. Then we perform motion\ncompensation by using deformable convolution and generate the predicted\nfeature. After that, we compress the residual feature between the feature from\nthe current frame and the predicted feature from our deformable compensation\nmodule. For better frame reconstruction, the reference features from multiple\nprevious reconstructed frames are also fused by using the non-local attention\nmechanism in the multi-frame feature fusion module. Comprehensive experimental\nresults demonstrate that the proposed framework achieves the state-of-the-art\nperformance on four benchmark datasets including HEVC, UVG, VTL and MCL-JCV.",
          "link": "http://arxiv.org/abs/2105.09600",
          "publishedOn": "2021-05-23T06:08:17.511Z",
          "wordCount": 635,
          "title": "FVC: A New Framework towards Deep Video Compression in Feature Space. (arXiv:2105.09600v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kowalczyk_M/0/1/0/all/0/1\">Marcin Kowalczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryjak_T/0/1/0/all/0/1\">Tomasz Kryjak</a>",
          "description": "This work describes the hardware implementation of a connected component\nlabelling (CCL) module in reprogammable logic. The main novelty of the design\nis the \"full\", i.e. without any simplifications, support of a 4 pixel per clock\nformat (4 ppc) and real-time processing of a 4K/UltraHD video stream (3840 x\n2160 pixels) at 60 frames per second. To achieve this, a special labelling\nmethod was designed and a functionality that stops the input data stream in\norder to process pixel groups which require writing more than one merger into\nthe equivalence table. The proposed module was verified in simulation and in\nhardware on the Xilinx Zynq Ultrascale+ MPSoC chip on the ZCU104 evaluation\nboard.",
          "link": "http://arxiv.org/abs/2105.09658",
          "publishedOn": "2021-05-23T06:08:17.504Z",
          "wordCount": 559,
          "title": "A Connected Component Labelling algorithm for multi-pixel per clock cycle video strea. (arXiv:2105.09658v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09624",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Grohl_J/0/1/0/all/0/1\">Janek Gr&#xf6;hl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schellenberg_M/0/1/0/all/0/1\">Melanie Schellenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dreher_K/0/1/0/all/0/1\">Kris Dreher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Holzwarth_N/0/1/0/all/0/1\">Niklas Holzwarth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu D. Tizabi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seitel_A/0/1/0/all/0/1\">Alexander Seitel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a>",
          "description": "Photoacoustic imaging has the potential to revolutionise healthcare due to\nthe valuable information on tissue physiology that is contained in\nmultispectral photoacoustic measurements. Clinical translation of the\ntechnology requires conversion of the high-dimensional acquired data into\nclinically relevant and interpretable information. In this work, we present a\ndeep learning-based approach to semantic segmentation of multispectral\nphotoacoustic images to facilitate the interpretability of recorded images.\nManually annotated multispectral photoacoustic imaging data are used as gold\nstandard reference annotations and enable the training of a deep learning-based\nsegmentation algorithm in a supervised manner. Based on a validation study with\nexperimentally acquired data of healthy human volunteers, we show that\nautomatic tissue segmentation can be used to create powerful analyses and\nvisualisations of multispectral photoacoustic images. Due to the intuitive\nrepresentation of high-dimensional information, such a processing algorithm\ncould be a valuable means to facilitate the clinical translation of\nphotoacoustic imaging.",
          "link": "http://arxiv.org/abs/2105.09624",
          "publishedOn": "2021-05-23T06:08:17.485Z",
          "wordCount": 608,
          "title": "Semantic segmentation of multispectral photoacoustic images using deep learning. (arXiv:2105.09624v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barakat_B/0/1/0/all/0/1\">Berat Kurar Barakat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droby_A/0/1/0/all/0/1\">Ahmad Droby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saabni_R/0/1/0/all/0/1\">Raid Saabni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sana_J/0/1/0/all/0/1\">Jihad El-Sana</a>",
          "description": "Despite recent advances in the field of supervised deep learning for text\nline segmentation, unsupervised deep learning solutions are beginning to gain\npopularity. In this paper, we present an unsupervised deep learning method that\nembeds document image patches to a compact Euclidean space where distances\ncorrespond to a coarse text line pattern similarity. Once this space has been\nproduced, text line segmentation can be easily implemented using standard\ntechniques with the embedded feature vectors. To train the model, we extract\nrandom pairs of document image patches with the assumption that neighbour\npatches contain a similar coarse trend of text lines, whereas if one of them is\nrotated, they contain different coarse trends of text lines. Doing well on this\ntask requires the model to learn to recognize the text lines and their salient\nparts. The benefit of our approach is zero manual labelling effort. We evaluate\nthe method qualitatively and quantitatively on several variants of text line\nsegmentation datasets to demonstrate its effectivity.",
          "link": "http://arxiv.org/abs/2105.09405",
          "publishedOn": "2021-05-23T06:08:17.479Z",
          "wordCount": 596,
          "title": "Unsupervised learning of text line segmentationby differentiating coarse patterns. (arXiv:2105.09405v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shijie Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tong Lin</a>",
          "description": "Recently, collaborative learning proposed by Song and Chai has achieved\nremarkable improvements in image classification tasks by simultaneously\ntraining multiple classifier heads. However, huge memory footprints required by\nsuch multi-head structures may hinder the training of large-capacity baseline\nmodels. The natural question is how to achieve collaborative learning within a\nsingle network without duplicating any modules. In this paper, we propose four\nways of collaborative learning among different parts of a single network with\nnegligible engineering efforts. To improve the robustness of the network, we\nleverage the consistency of the output layer and intermediate layers for\ntraining under the collaborative learning framework. Besides, the similarity of\nintermediate representation and convolution kernel is also introduced to reduce\nthe reduce redundant in a neural network. Compared to the method of Song and\nChai, our framework further considers the collaboration inside a single model\nand takes smaller overhead. Extensive experiments on Cifar-10, Cifar-100,\nImageNet32 and STL-10 corroborate the effectiveness of these four ways\nseparately while combining them leads to further improvements. In particular,\ntest errors on the STL-10 dataset are decreased by $9.28\\%$ and $5.45\\%$ for\nResNet-18 and VGG-16 respectively. Moreover, our method is proven to be robust\nto label noise with experiments on Cifar-10 dataset. For example, our method\nhas $3.53\\%$ higher performance under $50\\%$ noise ratio setting.",
          "link": "http://arxiv.org/abs/2105.09590",
          "publishedOn": "2021-05-23T06:08:17.472Z",
          "wordCount": 638,
          "title": "Intra-Model Collaborative Learning of Neural Networks. (arXiv:2105.09590v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhibo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuchen Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>",
          "description": "Recently few-shot object detection is widely adopted to deal with\ndata-limited situations. While most previous works merely focus on the\nperformance on few-shot categories, we claim that detecting all classes is\ncrucial as test samples may contain any instances in realistic applications,\nwhich requires the few-shot detector to learn new concepts without forgetting.\nThrough analysis on transfer learning based methods, some neglected but\nbeneficial properties are utilized to design a simple yet effective few-shot\ndetector, Retentive R-CNN. It consists of Bias-Balanced RPN to debias the\npretrained RPN and Re-detector to find few-shot class objects without\nforgetting previous knowledge. Extensive experiments on few-shot detection\nbenchmarks show that Retentive R-CNN significantly outperforms state-of-the-art\nmethods on overall performance among all settings as it can achieve competitive\nresults on few-shot classes and does not degrade the base class performance at\nall. Our approach has demonstrated that the long desired never-forgetting\nlearner is available in object detection.",
          "link": "http://arxiv.org/abs/2105.09491",
          "publishedOn": "2021-05-23T06:08:17.466Z",
          "wordCount": 583,
          "title": "Generalized Few-Shot Object Detection without Forgetting. (arXiv:2105.09491v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Heming Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>",
          "description": "Object goal navigation aims to steer an agent towards a target object based\non observations of the agent. It is of pivotal importance to design effective\nvisual representations of the observed scene in determining navigation actions.\nIn this paper, we introduce a Visual Transformer Network (VTNet) for learning\ninformative visual representation in navigation. VTNet is a highly effective\nstructure that embodies two key properties for visual representations: First,\nthe relationships among all the object instances in a scene are exploited;\nSecond, the spatial locations of objects and image regions are emphasized so\nthat directional navigation signals can be learned. Furthermore, we also\ndevelop a pre-training scheme to associate the visual representations with\nnavigation signals, and thus facilitate navigation policy learning. In a\nnutshell, VTNet embeds object and region features with their location cues as\nspatial-aware descriptors and then incorporates all the encoded descriptors\nthrough attention operations to achieve informative representation for\nnavigation. Given such visual representations, agents are able to explore the\ncorrelations between visual observations and navigation actions. For example,\nan agent would prioritize \"turning right\" over \"turning left\" when the visual\nrepresentation emphasizes on the right side of activation map. Experiments in\nthe artificial environment AI2-Thor demonstrate that VTNet significantly\noutperforms state-of-the-art methods in unseen testing environments.",
          "link": "http://arxiv.org/abs/2105.09447",
          "publishedOn": "2021-05-23T06:08:17.458Z",
          "wordCount": 642,
          "title": "VTNet: Visual Transformer Network for Object Goal Navigation. (arXiv:2105.09447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haoyue Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Song Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">S.-H. Gary Chan</a>",
          "description": "Labeled crowd scene images are expensive and scarce. To significantly reduce\nthe requirement of the labeled images, we propose ColorCount, a novel CNN-based\napproach by combining self-supervised transfer colorization learning and global\nprior classification to leverage the abundantly available unlabeled data. The\nself-supervised colorization branch learns the semantics and surface texture of\nthe image by using its color components as pseudo labels. The classification\nbranch extracts global group priors by learning correlations among image\nclusters. Their fused resultant discriminative features (global priors,\nsemantics and textures) provide ample priors for counting, hence significantly\nreducing the requirement of labeled images. We conduct extensive experiments on\nfour challenging benchmarks. ColorCount achieves much better performance as\ncompared with other unsupervised approaches. Its performance is close to the\nsupervised baseline with substantially less labeled data (10\\% of the original\none).",
          "link": "http://arxiv.org/abs/2105.09684",
          "publishedOn": "2021-05-23T06:08:17.452Z",
          "wordCount": 570,
          "title": "Crowd Counting by Self-supervised Transfer Colorization Learning and Global Prior Classification. (arXiv:2105.09684v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1\">Fausto Giunchiglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_M/0/1/0/all/0/1\">Mayukh Bagchi</a>",
          "description": "We assume that substances in the world are represented by two types of\nconcepts, namely substance concepts and classification concepts, the former\ninstrumental to (visual) perception, the latter to (language based)\nclassification. Based on this distinction, we introduce a general methodology\nfor building lexico-semantic hierarchies of substance concepts, where nodes are\nannotated with the media, e.g.,videos or photos, from which substance concepts\nare extracted, and are associated with the corresponding classification\nconcepts. The methodology is based on Ranganathan's original faceted approach,\ncontextualized to the problem of classifying substance concepts. The key\nnovelty is that the hierarchy is built exploiting the visual properties of\nsubstance concepts, while the linguistically defined properties of\nclassification concepts are only used to describe substance concepts. The\nvalidity of the approach is exemplified by providing some highlights of an\nongoing project whose goal is to build a large scale multimedia multilingual\nconcept hierarchy.",
          "link": "http://arxiv.org/abs/2105.09422",
          "publishedOn": "2021-05-23T06:08:17.426Z",
          "wordCount": 567,
          "title": "Classifying concepts via visual properties. (arXiv:2105.09422v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1\">Pengxiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>",
          "description": "Joint relation modeling is a curial component in human motion prediction.\nMost existing methods tend to design skeletal-based graphs to build the\nrelations among joints, where local interactions between joint pairs are well\nlearned. However, the global coordination of all joints, which reflects human\nmotion's balance property, is usually weakened because it is learned from part\nto whole progressively and asynchronously. Thus, the final predicted motions\nare sometimes unnatural. To tackle this issue, we learn a medium, called\nbalance attractor (BA), from the spatiotemporal features of motion to\ncharacterize the global motion features, which is subsequently used to build\nnew joint relations. Through the BA, all joints are related synchronously, and\nthus the global coordination of all joints can be better learned. Based on the\nBA, we propose our framework, referred to Attractor-Guided Neural Network,\nmainly including Attractor-Based Joint Relation Extractor (AJRE) and\nMulti-timescale Dynamics Extractor (MTDE). The AJRE mainly includes Global\nCoordination Extractor (GCE) and Local Interaction Extractor (LIE). The former\npresents the global coordination of all joints, and the latter encodes local\ninteractions between joint pairs. The MTDE is designed to extract dynamic\ninformation from raw position information for effective prediction. Extensive\nexperiments show that the proposed framework outperforms state-of-the-art\nmethods in both short and long-term predictions in H3.6M, CMU-Mocap, and 3DPW.",
          "link": "http://arxiv.org/abs/2105.09711",
          "publishedOn": "2021-05-23T06:08:17.418Z",
          "wordCount": 642,
          "title": "An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction. (arXiv:2105.09711v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung-Nghia Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tam V. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Z/0/1/0/all/0/1\">Zhongliang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_A/0/1/0/all/0/1\">Akihiro Sugimoto</a>",
          "description": "Camouflaged objects attempt to conceal their texture into the background and\ndiscriminating them from the background is hard even for human beings. The main\nobjective of this paper is to explore the camouflaged object segmentation\nproblem, namely, segmenting the camouflaged object(s) for a given image. This\nproblem has not been well studied in spite of a wide range of potential\napplications including the preservation of wild animals and the discovery of\nnew species, surveillance systems, search-and-rescue missions in the event of\nnatural disasters such as earthquakes, floods or hurricanes. This paper\naddresses a new challenging problem of camouflaged object segmentation. To\naddress this problem, we provide a new image dataset of camouflaged objects for\nbenchmarking purposes. In addition, we propose a general end-to-end network,\ncalled the Anabranch Network, that leverages both classification and\nsegmentation tasks. Different from existing networks for segmentation, our\nproposed network possesses the second branch for classification to predict the\nprobability of containing camouflaged object(s) in an image, which is then\nfused into the main branch for segmentation to boost up the segmentation\naccuracy. Extensive experiments conducted on the newly built dataset\ndemonstrate the effectiveness of our network using various fully convolutional\nnetworks. \\url{https://sites.google.com/view/ltnghia/research/camo}",
          "link": "http://arxiv.org/abs/2105.09451",
          "publishedOn": "2021-05-23T06:08:17.411Z",
          "wordCount": 649,
          "title": "Anabranch Network for Camouflaged Object Segmentation. (arXiv:2105.09451v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rundi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changxi Zheng</a>",
          "description": "Deep generative models of 3D shapes have received a great deal of research\ninterest. Yet, almost all of them generate discrete shape representations, such\nas voxels, point clouds, and polygon meshes. We present the first 3D generative\nmodel for a drastically different shape representation -- describing a shape as\na sequence of computer-aided design (CAD) operations. Unlike meshes and point\nclouds, CAD models encode the user creation process of 3D shapes, widely used\nin numerous industrial and engineering design tasks. However, the sequential\nand irregular structure of CAD operations poses significant challenges for\nexisting 3D generative models. Drawing an analogy between CAD operations and\nnatural language, we propose a CAD generative network based on the Transformer.\nWe demonstrate the performance of our model for both shape autoencoding and\nrandom shape generation. To train our network, we create a new CAD dataset\nconsisting of 179,133 models and their CAD construction sequences. We have made\nthis dataset publicly available to promote future research on this topic.",
          "link": "http://arxiv.org/abs/2105.09492",
          "publishedOn": "2021-05-23T06:08:17.404Z",
          "wordCount": 608,
          "title": "DeepCAD: A Deep Generative Network for Computer-Aided Design Models. (arXiv:2105.09492v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1\">Dengqiang Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shangqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qunlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xinzhe Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>",
          "description": "Registration networks have shown great application potentials in medical\nimage analysis. However, supervised training methods have a great demand for\nlarge and high-quality labeled datasets, which is time-consuming and sometimes\nimpractical due to data sharing issues. Unsupervised image registration\nalgorithms commonly employ intensity-based similarity measures as loss\nfunctions without any manual annotations. These methods estimate the\nparameterized transformations between pairs of moving and fixed images through\nthe optimization of the network parameters during training. However, these\nmethods become less effective when the image quality varies, e.g., some images\nare corrupted by substantial noise or artifacts. In this work, we propose a\nnovel approach based on a low-rank representation, i.e., Regnet-LRR, to tackle\nthe problem. We project noisy images into a noise-free low-rank space, and then\ncompute the similarity between the images. Based on the low-rank similarity\nmeasure, we train the registration network to predict the dense deformation\nfields of noisy image pairs. We highlight that the low-rank projection is\nreformulated in a way that the registration network can successfully update\ngradients. With two tasks, i.e., cardiac and abdominal intra-modality\nregistration, we demonstrate that the low-rank representation can boost the\ngeneralization ability and robustness of models as well as bring significant\nimprovements in noisy data registration scenarios.",
          "link": "http://arxiv.org/abs/2105.09548",
          "publishedOn": "2021-05-23T06:08:17.397Z",
          "wordCount": 645,
          "title": "A low-rank representation for unsupervised registration of medical images. (arXiv:2105.09548v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jianbo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Rui Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Larry Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>",
          "description": "Attention mechanisms have been widely applied to cross-modal tasks such as\nimage captioning and information retrieval, and have achieved remarkable\nimprovements due to its capability to learn fine-grained relevance across\ndifferent modalities. However, existing attention models could be sub-optimal\nand lack preciseness because there is no direct supervision involved during\ntraining. In this work, we propose Contrastive Content Re-sourcing (CCR) and\nContrastive Content Swapping (CCS) constraints to address such limitation.\nThese constraints supervise the training of attention models in a contrastive\nlearning manner without requiring explicit attention annotations. Additionally,\nwe introduce three metrics, namely Attention Precision, Recall and F1-Score, to\nquantitatively evaluate the attention quality. We evaluate the proposed\nconstraints with cross-modal retrieval (image-text matching) task. The\nexperiments on both Flickr30k and MS-COCO datasets demonstrate that integrating\nthese attention constraints into two state-of-the-art attention-based models\nimproves the model performance in terms of both retrieval accuracy and\nattention metrics.",
          "link": "http://arxiv.org/abs/2105.09597",
          "publishedOn": "2021-05-23T06:08:17.378Z",
          "wordCount": 587,
          "title": "More Than Just Attention: Learning Cross-Modal Attentions with Contrastive Constraints. (arXiv:2105.09597v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1\">Tirtharaj Dash</a>",
          "description": "Superpixels are higher-order perceptual groups of pixels in an image, often\ncarrying much more information than raw pixels. There is an inherent relational\nstructure to the relationship among different superpixels of an image. This\nrelational information can convey some form of domain information about the\nimage, e.g. relationship between superpixels representing two eyes in a cat\nimage. Our interest in this paper is to construct computer vision models,\nspecifically those based on Deep Neural Networks (DNNs) to incorporate these\nsuperpixels information. We propose a methodology to construct a hybrid model\nthat leverages (a) Convolutional Neural Network (CNN) to deal with spatial\ninformation in an image, and (b) Graph Neural Network (GNN) to deal with\nrelational superpixel information in the image. The proposed deep model is\nlearned using a generic hybrid loss function that we call a `hybrid' loss. We\nevaluate the predictive performance of our proposed hybrid vision model on four\npopular image classification datasets: MNIST, FMNIST, CIFAR-10 and CIFAR-100.\nMoreover, we evaluate our method on three real-world classification tasks:\nCOVID-19 X-Ray Detection, LFW Face Recognition, and SOCOFing Fingerprint\nIdentification. The results demonstrate that the relational superpixel\ninformation provided via a GNN could improve the performance of standard\nCNN-based vision systems.",
          "link": "http://arxiv.org/abs/2105.09448",
          "publishedOn": "2021-05-23T06:08:17.371Z",
          "wordCount": 693,
          "title": "Superpixel-based Domain-Knowledge Infusion in Computer Vision. (arXiv:2105.09448v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yongxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiaolin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuncong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lu Li</a>",
          "description": "Recently, plenty of work has tried to introduce transformers into computer\nvision tasks, with good results. Unlike classic convolution networks, which\nextract features within a local receptive field, transformers can adaptively\naggregate similar features from a global view using self-attention mechanism.\nFor object detection, Feature Pyramid Network (FPN) proposes feature\ninteraction across layers and proves its extremely importance. However, its\ninteraction is still in a local manner, which leaves a lot of room for\nimprovement. Since transformer was originally designed for NLP tasks, adapting\nprocessing subject directly from text to image will cause unaffordable\ncomputation and space overhead. In this paper, we utilize a linearized\nattention function to overcome above problems and build a novel architecture,\nnamed Content-Augmented Feature Pyramid Network (CA-FPN), which proposes a\nglobal content extraction module and deeply combines with FPN through light\nlinear transformers. What's more, light transformers can further make the\napplication of multi-head attention mechanism easier. Most importantly, our\nCA-FPN can be readily plugged into existing FPN-based models. Extensive\nexperiments on the challenging COCO object detection dataset demonstrated that\nour CA-FPN significantly outperforms competitive baselines without bells and\nwhistles. Code will be made publicly available.",
          "link": "http://arxiv.org/abs/2105.09464",
          "publishedOn": "2021-05-23T06:08:17.363Z",
          "wordCount": 634,
          "title": "Content-Augmented Feature Pyramid Network with Light Linear Transformers. (arXiv:2105.09464v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wei Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_R/0/1/0/all/0/1\">Ruhui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_K/0/1/0/all/0/1\">Kaida Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Laili Zhu</a>",
          "description": "Recently, the anchor-free object detection model has shown great potential\nfor accuracy and speed to exceed anchor-based object detection. Therefore, two\nissues are mainly studied in this article: (1) How to let the backbone network\nin the anchor-free object detection model learn feature extraction? (2) How to\nmake better use of the feature pyramid network? In order to solve the above\nproblems, Experiments show that our model has a certain improvement in accuracy\ncompared with the current popular detection models on the COCO dataset, the\ndesigned attention mechanism module can capture contextual information well,\nimprove detection accuracy, and use sepc network to help balance abstract and\ndetailed information, and reduce the problem of semantic gap in the feature\npyramid network. Whether it is anchor-based network model YOLOv3, Faster RCNN,\nor anchor-free network model Foveabox, FSAF, FCOS. Our optimal model can get\n39.5% COCO AP under the background of ResNet50.",
          "link": "http://arxiv.org/abs/2105.09596",
          "publishedOn": "2021-05-23T06:08:17.355Z",
          "wordCount": 596,
          "title": "AGSFCOS: Based on attention mechanism and Scale-Equalizing pyramid network of object detection. (arXiv:2105.09596v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lingni Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somasundaram_K/0/1/0/all/0/1\">Kiran Somasundaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1\">James M. Rehg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>",
          "description": "Given a video captured from a first person perspective and recorded in a\nfamiliar environment, can we recognize what the person is doing and identify\nwhere the action occurs in the 3D space? We address this challenging problem of\njointly recognizing and localizing actions of a mobile user on a known 3D map\nfrom egocentric videos. To this end, we propose a novel deep probabilistic\nmodel. Our model takes the inputs of a Hierarchical Volumetric Representation\n(HVR) of the environment and an egocentric video, infers the 3D action location\nas a latent variable, and recognizes the action based on the video and\ncontextual cues surrounding its potential locations. To evaluate our model, we\nconduct extensive experiments on a newly collected egocentric video dataset, in\nwhich both human naturalistic actions and photo-realistic 3D environment\nreconstructions are captured. Our method demonstrates strong results on both\naction recognition and 3D action localization across seen and unseen\nenvironments. We believe our work points to an exciting research direction in\nthe intersection of egocentric vision, and 3D scene understanding.",
          "link": "http://arxiv.org/abs/2105.09544",
          "publishedOn": "2021-05-23T06:08:17.347Z",
          "wordCount": 612,
          "title": "Egocentric Activity Recognition and Localization on a 3D Map. (arXiv:2105.09544v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gangeh_M/0/1/0/all/0/1\">Mehrdad J Gangeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plata_M/0/1/0/all/0/1\">Marcin Plata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motahari_H/0/1/0/all/0/1\">Hamid Motahari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duffy_N/0/1/0/all/0/1\">Nigel P Duffy</a>",
          "description": "Removing noise from scanned pages is a vital step before their submission to\noptical character recognition (OCR) system. Most available image denoising\nmethods are supervised where the pairs of noisy/clean pages are required.\nHowever, this assumption is rarely met in real settings. Besides, there is no\nsingle model that can remove various noise types from documents. Here, we\npropose a unified end-to-end unsupervised deep learning model, for the first\ntime, that can effectively remove multiple types of noise, including salt \\&\npepper noise, blurred and/or faded text, as well as watermarks from documents\nat various levels of intensity. We demonstrate that the proposed model\nsignificantly improves the quality of scanned images and the OCR of the pages\non several test datasets.",
          "link": "http://arxiv.org/abs/2105.09437",
          "publishedOn": "2021-05-23T06:08:17.320Z",
          "wordCount": 551,
          "title": "End-to-End Unsupervised Document Image Blind Denoising. (arXiv:2105.09437v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yada Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingrui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>",
          "description": "With the advent of big data across multiple high-impact applications, we are\noften facing the challenge of complex heterogeneity. The newly collected data\nusually consist of multiple modalities and characterized with multiple labels,\nthus exhibiting the co-existence of multiple types of heterogeneity. Although\nstate-of-the-art techniques are good at modeling the complex heterogeneity with\nsufficient label information, such label information can be quite expensive to\nobtain in real applications, leading to sub-optimal performance using these\ntechniques. Inspired by the capability of contrastive learning to utilize rich\nunlabeled data for improving performance, in this paper, we propose a unified\nheterogeneous learning framework, which combines both weighted unsupervised\ncontrastive loss and weighted supervised contrastive loss to model multiple\ntypes of heterogeneity. We also provide theoretical analyses showing that the\nproposed weighted supervised contrastive loss is the lower bound of the mutual\ninformation of two samples from the same class and the weighted unsupervised\ncontrastive loss is the lower bound of the mutual information between the\nhidden representation of two views of the same sample. Experimental results on\nreal-world data sets demonstrate the effectiveness and the efficiency of the\nproposed method modeling multiple types of heterogeneity.",
          "link": "http://arxiv.org/abs/2105.09401",
          "publishedOn": "2021-05-23T06:08:17.301Z",
          "wordCount": 613,
          "title": "Heterogeneous Contrastive Learning. (arXiv:2105.09401v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolotouros_N/0/1/0/all/0/1\">Nikos Kolotouros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badger_M/0/1/0/all/0/1\">Marc Badger</a>",
          "description": "Animals are diverse in shape, but building a deformable shape model for a new\nspecies is not always possible due to the lack of 3D data. We present a method\nto capture new species using an articulated template and images of that\nspecies. In this work, we focus mainly on birds. Although birds represent\nalmost twice the number of species as mammals, no accurate shape model is\navailable. To capture a novel species, we first fit the articulated template to\neach training sample. By disentangling pose and shape, we learn a shape space\nthat captures variation both among species and within each species from image\nevidence. We learn models of multiple species from the CUB dataset, and\ncontribute new species-specific and multi-species shape models that are useful\nfor downstream reconstruction tasks. Using a low-dimensional embedding, we show\nthat our learned 3D shape space better reflects the phylogenetic relationships\namong birds than learned perceptual features.",
          "link": "http://arxiv.org/abs/2105.09396",
          "publishedOn": "2021-05-23T06:08:17.274Z",
          "wordCount": 596,
          "title": "Birds of a Feather: Capturing Avian Shape Models from Images. (arXiv:2105.09396v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09378",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gadjimuradov_F/0/1/0/all/0/1\">Fasil Gadjimuradov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benkert_T/0/1/0/all/0/1\">Thomas Benkert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nickel_M/0/1/0/all/0/1\">Marcel Dominik Nickel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>",
          "description": "Purpose: To develop an algorithm for robust partial Fourier (PF)\nreconstruction applicable to diffusion-weighted (DW) images with non-smooth\nphase variations.\n\nMethods: Based on an unrolled proximal splitting algorithm, a neural network\narchitecture is derived which alternates between data consistency operations\nand regularization implemented by recurrent convolutions. In order to exploit\ncorrelations, multiple repetitions of the same slice are jointly reconstructed\nunder consideration of permutation-equivariance. The proposed method is trained\non DW liver data of 60 volunteers and evaluated on retrospectively and\nprospectively sub-sampled data of different anatomies and resolutions. In\naddition, the benefits of using a recurrent network over other unrolling\nstrategies is investigated.\n\nResults: Conventional PF techniques can be significantly outperformed in\nterms of quantitative measures as well as perceptual image quality. The\nproposed method is able to generalize well to brain data with contrasts and\nresolution not present in the training set. The reduction in echo time (TE)\nassociated with prospective PF-sampling enables DW imaging with higher signal.\nAlso, the TE increase in acquisitions with higher resolution can be compensated\nfor. It can be shown that unrolling by means of a recurrent network produced\nbetter results than using a weight-shared network or a cascade of networks.\n\nConclusion: This work demonstrates that robust PF reconstruction of DW data\nis feasible even at strong PF factors in applications with severe phase\nvariations. Since the proposed method does not rely on smoothness priors of the\nphase but uses learned recurrent convolutions instead, artifacts of\nconventional PF methods can be avoided.",
          "link": "http://arxiv.org/abs/2105.09378",
          "publishedOn": "2021-05-23T06:08:17.262Z",
          "wordCount": 705,
          "title": "Robust partial Fourier reconstruction for diffusion-weighted imaging using a recurrent convolutional neural network. (arXiv:2105.09378v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09374",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halperin_T/0/1/0/all/0/1\">Tavi Halperin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakim_H/0/1/0/all/0/1\">Hanit Hakim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vantzos_O/0/1/0/all/0/1\">Orestis Vantzos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hochman_G/0/1/0/all/0/1\">Gershon Hochman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benaim_N/0/1/0/all/0/1\">Netai Benaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sassy_L/0/1/0/all/0/1\">Lior Sassy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kupchik_M/0/1/0/all/0/1\">Michael Kupchik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_O/0/1/0/all/0/1\">Ofir Bibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_O/0/1/0/all/0/1\">Ohad Fried</a>",
          "description": "We present an algorithm for producing a seamless animated loop from a single\nimage. The algorithm detects periodic structures, such as the windows of a\nbuilding or the steps of a staircase, and generates a non-trivial displacement\nvector field that maps each segment of the structure onto a neighboring segment\nalong a user- or auto-selected main direction of motion. This displacement\nfield is used, together with suitable temporal and spatial smoothing, to warp\nthe image and produce the frames of a continuous animation loop. Our\ncinemagraphs are created in under a second on a mobile device. Over 140,000\nusers downloaded our app and exported over 350,000 cinemagraphs. Moreover, we\nconducted two user studies that show that users prefer our method for creating\nsurreal and structured cinemagraphs compared to more manual approaches and\ncompared to previous methods.",
          "link": "http://arxiv.org/abs/2105.09374",
          "publishedOn": "2021-05-23T06:08:17.231Z",
          "wordCount": 608,
          "title": "Endless Loops: Detecting and Animating Periodic Patterns in Still Images. (arXiv:2105.09374v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09365",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Uysal_E/0/1/0/all/0/1\">Enes Sadi Uysal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bilici_M/0/1/0/all/0/1\">M.&#x15e;afak Bilici</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zaza_B/0/1/0/all/0/1\">B. Selin Zaza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozgenc_M/0/1/0/all/0/1\">M. Yi&#x11f;it &#xd6;zgen&#xe7;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boyar_O/0/1/0/all/0/1\">Onur Boyar</a>",
          "description": "Retinal Vessel Segmentation is important for diagnosis of various diseases.\nThe research on retinal vessel segmentation focuses mainly on improvement of\nthe segmentation model which is usually based on U-Net architecture. In our\nstudy we use the U-Net architecture and we rely on heavy data augmentation in\norder to achieve better performance. The success of the data augmentation\nrelies on successfully addressing the problem of input images. By analyzing\ninput images and performing the augmentation accordingly we show that the\nperformance of the U-Net model can be increased dramatically. Results are\nreported using the most widely used retina dataset, DRIVE.",
          "link": "http://arxiv.org/abs/2105.09365",
          "publishedOn": "2021-05-23T06:08:17.203Z",
          "wordCount": 564,
          "title": "Exploring The Limits Of Data Augmentation For Retinal Vessel Segmentation. (arXiv:2105.09365v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karnan_H/0/1/0/all/0/1\">Haresh Karnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1\">Garrett Warnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuesu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1\">Peter Stone</a>",
          "description": "While imitation learning for vision based autonomous mobile robot navigation\nhas recently received a great deal of attention in the research community,\nexisting approaches typically require state action demonstrations that were\ngathered using the deployment platform. However, what if one cannot easily\noutfit their platform to record these demonstration signals or worse yet the\ndemonstrator does not have access to the platform at all? Is imitation learning\nfor vision based autonomous navigation even possible in such scenarios? In this\nwork, we hypothesize that the answer is yes and that recent ideas from the\nImitation from Observation (IfO) literature can be brought to bear such that a\nrobot can learn to navigate using only ego centric video collected by a\ndemonstrator, even in the presence of viewpoint mismatch. To this end, we\nintroduce a new algorithm, Visual Observation only Imitation Learning for\nAutonomous navigation (VOILA), that can successfully learn navigation policies\nfrom a single video demonstration collected from a physically different agent.\nWe evaluate VOILA in the photorealistic AirSim simulator and show that VOILA\nnot only successfully imitates the expert, but that it also learns navigation\npolicies that can generalize to novel environments. Further, we demonstrate the\neffectiveness of VOILA in a real world setting by showing that it allows a\nwheeled Jackal robot to successfully imitate a human walking in an environment\nusing a video recorded using a mobile phone camera.",
          "link": "http://arxiv.org/abs/2105.09371",
          "publishedOn": "2021-05-23T06:08:17.176Z",
          "wordCount": 671,
          "title": "VOILA: Visual-Observation-Only Imitation Learning for Autonomous Navigation. (arXiv:2105.09371v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1\">Seyed Saeed Changiz Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Fred X. Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1\">Mohammad Salameh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1\">Keith Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1\">Shuo Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1\">Shangling Jui</a>",
          "description": "Despite the empirical success of neural architecture search (NAS) in deep\nlearning applications, the optimality, reproducibility and cost of NAS schemes\nremain hard to assess. In this paper, we propose Generative Adversarial NAS\n(GA-NAS) with theoretically provable convergence guarantees, promoting\nstability and reproducibility in neural architecture search. Inspired by\nimportance sampling, GA-NAS iteratively fits a generator to previously\ndiscovered top architectures, thus increasingly focusing on important parts of\na large search space. Furthermore, we propose an efficient adversarial learning\napproach, where the generator is trained by reinforcement learning based on\nrewards provided by a discriminator, thus being able to explore the search\nspace without evaluating a large number of architectures. Extensive experiments\nshow that GA-NAS beats the best published results under several cases on three\npublic NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search\nconstraints and search spaces. We show that GA-NAS can be used to improve\nalready optimized baselines found by other NAS methods, including EfficientNet\nand ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in\ntheir original search space.",
          "link": "http://arxiv.org/abs/2105.09356",
          "publishedOn": "2021-05-23T06:08:17.161Z",
          "wordCount": 621,
          "title": "Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v1 [cs.LG])"
        }
      ]
    }
  ],
  "cliVersion": "1.8.1"
}