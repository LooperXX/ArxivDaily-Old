{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.12488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahdaouy_A/0/1/0/all/0/1\">Abdelkader El Mahdaouy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekki_A/0/1/0/all/0/1\">Abdellah El Mekki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Essefar_K/0/1/0/all/0/1\">Kabil Essefar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamoun_N/0/1/0/all/0/1\">Nabil El Mamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrada_I/0/1/0/all/0/1\">Ismail Berrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoumsi_A/0/1/0/all/0/1\">Ahmed Khoumsi</a>",
          "description": "The prominence of figurative language devices, such as sarcasm and irony,\nposes serious challenges for Arabic Sentiment Analysis (SA). While previous\nresearch works tackle SA and sarcasm detection separately, this paper\nintroduces an end-to-end deep Multi-Task Learning (MTL) model, allowing\nknowledge interaction between the two tasks. Our MTL model's architecture\nconsists of a Bidirectional Encoder Representation from Transformers (BERT)\nmodel, a multi-task attention interaction module, and two task classifiers. The\noverall obtained results show that our proposed model outperforms its\nsingle-task counterparts on both SA and sarcasm detection sub-tasks.",
          "link": "http://arxiv.org/abs/2106.12488",
          "publishedOn": "2021-06-24T01:51:43.048Z",
          "wordCount": 536,
          "title": "Deep Multi-Task Model for Sarcasm Detection and Sentiment Analysis in Arabic Language. (arXiv:2106.12488v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_O/0/1/0/all/0/1\">Oliver Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buschek_D/0/1/0/all/0/1\">Daniel Buschek</a>",
          "description": "We present CharacterChat, a concept and chatbot to support writers in\ncreating fictional characters. Concretely, writers progressively turn the bot\ninto their imagined character through conversation. We iteratively developed\nCharacterChat in a user-centred approach, starting with a survey on character\ncreation with writers (N=30), followed by two qualitative user studies (N=7 and\nN=8). Our prototype combines two modes: (1) Guided prompts help writers define\ncharacter attributes (e.g. User: \"Your name is Jane.\"), including suggestions\nfor attributes (e.g. Bot: \"What is my main motivation?\") and values, realised\nas a rule-based system with a concept network. (2) Open conversation with the\nchatbot helps writers explore their character and get inspiration, realised\nwith a language model that takes into account the defined character attributes.\nOur user studies reveal benefits particularly for early stages of character\ncreation, and challenges due to limited conversational capabilities. We\nconclude with lessons learned and ideas for future work.",
          "link": "http://arxiv.org/abs/2106.12314",
          "publishedOn": "2021-06-24T01:51:42.964Z",
          "wordCount": 611,
          "title": "CharacterChat: Supporting the Creation of Fictional Characters through Conversation and Progressive Manifestation with a Chatbot. (arXiv:2106.12314v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Ruixiang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>",
          "description": "Broad-coverage meaning representations in NLP mostly focus on explicitly\nexpressed content. More importantly, the scarcity of datasets annotating\ndiverse implicit roles limits empirical studies into their linguistic nuances.\nFor example, in the web review \"Great service!\", the provider and consumer are\nimplicit arguments of different types. We examine an annotated corpus of\nfine-grained implicit arguments (Cui and Hershcovich, 2020) by carefully\nre-annotating it, resolving several inconsistencies. Subsequently, we present\nthe first transition-based neural parser that can handle implicit arguments\ndynamically, and experiment with two different transition systems on the\nimproved dataset. We find that certain types of implicit arguments are more\ndifficult to parse than others and that the simpler system is more accurate in\nrecovering implicit arguments, despite having a lower overall parsing score,\nattesting current reasoning limitations of NLP models. This work will\nfacilitate a better understanding of implicit and underspecified language, by\nincorporating it holistically into meaning representations.",
          "link": "http://arxiv.org/abs/2106.02561",
          "publishedOn": "2021-06-24T01:51:42.958Z",
          "wordCount": 599,
          "title": "Great Service! Fine-grained Parsing of Implicit Arguments. (arXiv:2106.02561v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1\">Mikhail Galkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiapeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denis_E/0/1/0/all/0/1\">Etienne Denis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1\">William L. Hamilton</a>",
          "description": "Conventional representation learning algorithms for knowledge graphs (KG) map\neach entity to a unique embedding vector. Such a shallow lookup results in a\nlinear growth of memory consumption for storing the embedding matrix and incurs\nhigh computational costs when working with real-world KGs. Drawing parallels\nwith subword tokenization commonly used in NLP, we explore the landscape of\nmore parameter-efficient node embedding strategies with possibly sublinear\nmemory requirements. To this end, we propose NodePiece, an anchor-based\napproach to learn a fixed-size entity vocabulary. In NodePiece, a vocabulary of\nsubword/sub-entity units is constructed from anchor nodes in a graph with known\nrelation types. Given such a fixed-size vocabulary, it is possible to bootstrap\nan encoding and embedding for any entity, including those unseen during\ntraining. Experiments show that NodePiece performs competitively in node\nclassification, link prediction, and relation prediction tasks while retaining\nless than 10% of explicit nodes in a graph as anchors and often having 10x\nfewer parameters.",
          "link": "http://arxiv.org/abs/2106.12144",
          "publishedOn": "2021-06-24T01:51:42.903Z",
          "wordCount": 600,
          "title": "NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs. (arXiv:2106.12144v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trentin_E/0/1/0/all/0/1\">Edmondo Trentin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gretter_R/0/1/0/all/0/1\">Roberto Gretter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matassoni_M/0/1/0/all/0/1\">Marco Matassoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falavigna_D/0/1/0/all/0/1\">Daniele Falavigna</a>",
          "description": "The paper copes with the task of automatic assessment of second language\nproficiency from the language learners' spoken responses to test prompts. The\ntask has significant relevance to the field of computer assisted language\nlearning. The approach presented in the paper relies on two separate modules:\n(1) an automatic speech recognition system that yields text transcripts of the\nspoken interactions involved, and (2) a multiple classifier system based on\ndeep learners that ranks the transcripts into proficiency classes. Different\ndeep neural network architectures (both feed-forward and recurrent) are\nspecialized over diverse representations of the texts in terms of: a reference\ngrammar, the outcome of probabilistic language models, several word embeddings,\nand two bag-of-word models. Combination of the individual classifiers is\nrealized either via a probabilistic pseudo-joint model, or via a neural mixture\nof experts. Using the data of the third Spoken CALL Shared Task challenge, the\nhighest values to date were obtained in terms of three popular evaluation\nmetrics.",
          "link": "http://arxiv.org/abs/2106.12475",
          "publishedOn": "2021-06-24T01:51:42.862Z",
          "wordCount": 602,
          "title": "Mixtures of Deep Neural Experts for Automated Speech Scoring. (arXiv:2106.12475v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Jiajie Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Peiqing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Cheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xunyi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Nai Ding</a>",
          "description": "Pre-trained language models achieves high performance on machine reading\ncomprehension (MRC) tasks but the results are hard to explain. An appealing\napproach to make models explainable is to provide rationales for its decision.\nTo facilitate supervised learning of human rationales, here we present PALRACE\n(Pruned And Labeled RACE), a new MRC dataset with human labeled rationales for\n800 passages selected from the RACE dataset. We further classified the question\nto each passage into 6 types. Each passage was read by at least 26\nparticipants, who labeled their rationales to answer the question. Besides, we\nconducted a rationale evaluation session in which participants were asked to\nanswering the question solely based on labeled rationales, confirming that the\nlabeled rationales were of high quality and can sufficiently support question\nanswering.",
          "link": "http://arxiv.org/abs/2106.12373",
          "publishedOn": "2021-06-24T01:51:42.857Z",
          "wordCount": 570,
          "title": "PALRACE: Reading Comprehension Dataset with Human Data and Labeled Rationales. (arXiv:2106.12373v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2003.08271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yige Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_N/0/1/0/all/0/1\">Ning Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>",
          "description": "Recently, the emergence of pre-trained models (PTMs) has brought natural\nlanguage processing (NLP) to a new era. In this survey, we provide a\ncomprehensive review of PTMs for NLP. We first briefly introduce language\nrepresentation learning and its research progress. Then we systematically\ncategorize existing PTMs based on a taxonomy with four perspectives. Next, we\ndescribe how to adapt the knowledge of PTMs to the downstream tasks. Finally,\nwe outline some potential directions of PTMs for future research. This survey\nis purposed to be a hands-on guide for understanding, using, and developing\nPTMs for various NLP tasks.",
          "link": "http://arxiv.org/abs/2003.08271",
          "publishedOn": "2021-06-24T01:51:42.735Z",
          "wordCount": 603,
          "title": "Pre-trained Models for Natural Language Processing: A Survey. (arXiv:2003.08271v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_E/0/1/0/all/0/1\">Emily Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Premkumar Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>",
          "description": "Technology for language generation has advanced rapidly, spurred by\nadvancements in pre-training large models on massive amounts of data and the\nneed for intelligent agents to communicate in a natural manner. While\ntechniques can effectively generate fluent text, they can also produce\nundesirable societal biases that can have a disproportionately negative impact\non marginalized populations. Language generation presents unique challenges for\nbiases in terms of direct user interaction and the structure of decoding\ntechniques. To better understand these challenges, we present a survey on\nsocietal biases in language generation, focusing on how data and techniques\ncontribute to biases and progress towards reducing biases. Motivated by a lack\nof studies on biases from decoding techniques, we also conduct experiments to\nquantify the effects of these techniques. By further discussing general trends\nand open challenges, we call to attention promising directions for research and\nthe importance of fairness and inclusivity considerations for language\ngeneration applications.",
          "link": "http://arxiv.org/abs/2105.04054",
          "publishedOn": "2021-06-24T01:51:42.706Z",
          "wordCount": 631,
          "title": "Societal Biases in Language Generation: Progress and Challenges. (arXiv:2105.04054v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zujie Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yining Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1\">Fan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>",
          "description": "Arguably, the visual perception of conversational agents to the physical\nworld is a key way for them to exhibit the human-like intelligence.\nImage-grounded conversation is thus proposed to address this challenge.\nExisting works focus on exploring the multimodal dialog models that ground the\nconversation on a given image. In this paper, we take a step further to study\nimage-grounded conversation under a fully open-ended setting where no paired\ndialog and image are assumed available. Specifically, we present Maria, a\nneural conversation agent powered by the visual world experiences which are\nretrieved from a large-scale image index. Maria consists of three flexible\ncomponents, i.e., text-to-image retriever, visual concept detector and\nvisual-knowledge-grounded response generator. The retriever aims to retrieve a\ncorrelated image to the dialog from an image index, while the visual concept\ndetector extracts rich visual knowledge from the image. Then, the response\ngenerator is grounded on the extracted visual knowledge and dialog context to\ngenerate the target response. Extensive experiments demonstrate Maria\noutperforms previous state-of-the-art methods on automatic metrics and human\nevaluation, and can generate informative responses that have some visual\ncommonsense of the physical world.",
          "link": "http://arxiv.org/abs/2105.13073",
          "publishedOn": "2021-06-24T01:51:42.643Z",
          "wordCount": 666,
          "title": "Maria: A Visual Experience Powered Conversational Agent. (arXiv:2105.13073v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1\">Charaf Eddine Benarab</a>",
          "description": "Knowledge is acquired by humans through experience, and no boundary is set\nbetween the kinds of knowledge or skill levels we can achieve on different\ntasks at the same time. When it comes to Neural Networks, that is not the case,\nthe major breakthroughs in the field are extremely task and domain specific.\nVision and language are dealt with in separate manners, using separate methods\nand different datasets. In this work, we propose to use knowledge acquired by\nbenchmark Vision Models which are trained on ImageNet to help a much smaller\narchitecture learn to classify text. After transforming the textual data\ncontained in the IMDB dataset to gray scale images. An analysis of different\ndomains and the Transfer Learning method is carried out. Despite the challenge\nposed by the very different datasets, promising results are achieved. The main\ncontribution of this work is a novel approach which links large pretrained\nmodels on both language and vision to achieve state-of-the-art results in\ndifferent sub-fields from the original task. Without needing high compute\ncapacity resources. Specifically, Sentiment Analysis is achieved after\ntransferring knowledge between vision and language models. BERT embeddings are\ntransformed into grayscale images, these images are then used as training\nexamples for pretrained vision models such as VGG16 and ResNet\n\nIndex Terms: Natural language, Vision, BERT, Transfer Learning, CNN, Domain\nAdaptation.",
          "link": "http://arxiv.org/abs/2106.12479",
          "publishedOn": "2021-06-24T01:51:42.624Z",
          "wordCount": 675,
          "title": "Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Olaleye_K/0/1/0/all/0/1\">Kayode Olaleye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>",
          "description": "Visually grounded speech models learn from images paired with spoken\ncaptions. By tagging images with soft text labels using a trained visual\nclassifier with a fixed vocabulary, previous work has shown that it is possible\nto train a model that can detect whether a particular text keyword occurs in\nspeech utterances or not. Here we investigate whether visually grounded speech\nmodels can also do keyword localisation: predicting where, within an utterance,\na given textual keyword occurs without any explicit text-based or alignment\nsupervision. We specifically consider whether incorporating attention into a\nconvolutional model is beneficial for localisation. Although absolute\nlocalisation performance with visually supervised models is still modest\n(compared to using unordered bag-of-word text labels for supervision), we show\nthat attention provides a large gain in performance over previous visually\ngrounded models. As in many other speech-image studies, we find that many of\nthe incorrect localisations are due to semantic confusions, e.g. locating the\nword 'backstroke' for the query keyword 'swimming'.",
          "link": "http://arxiv.org/abs/2106.08859",
          "publishedOn": "2021-06-24T01:51:42.618Z",
          "wordCount": 621,
          "title": "Attention-Based Keyword Localisation in Speech using Visual Grounding. (arXiv:2106.08859v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12495",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mekki_A/0/1/0/all/0/1\">Abdellah El Mekki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdaouy_A/0/1/0/all/0/1\">Abdelkader El Mahdaouy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Essefar_K/0/1/0/all/0/1\">Kabil Essefar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamoun_N/0/1/0/all/0/1\">Nabil El Mamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrada_I/0/1/0/all/0/1\">Ismail Berrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoumsi_A/0/1/0/all/0/1\">Ahmed Khoumsi</a>",
          "description": "Dialect and standard language identification are crucial tasks for many\nArabic natural language processing applications. In this paper, we present our\ndeep learning-based system, submitted to the second NADI shared task for\ncountry-level and province-level identification of Modern Standard Arabic (MSA)\nand Dialectal Arabic (DA). The system is based on an end-to-end deep Multi-Task\nLearning (MTL) model to tackle both country-level and province-level MSA/DA\nidentification. The latter MTL model consists of a shared Bidirectional Encoder\nRepresentation Transformers (BERT) encoder, two task-specific attention layers,\nand two classifiers. Our key idea is to leverage both the task-discriminative\nand the inter-task shared features for country and province MSA/DA\nidentification. The obtained results show that our MTL model outperforms\nsingle-task models on most subtasks.",
          "link": "http://arxiv.org/abs/2106.12495",
          "publishedOn": "2021-06-24T01:51:42.599Z",
          "wordCount": 573,
          "title": "BERT-based Multi-Task Model for Country and Province Level Modern Standard Arabic and Dialectal Arabic Identification. (arXiv:2106.12495v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2003.11561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polo_F/0/1/0/all/0/1\">Felipe Maia Polo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciochetti_I/0/1/0/all/0/1\">Itamar Ciochetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertolo_E/0/1/0/all/0/1\">Emerson Bertolo</a>",
          "description": "The objective of this paper is to develop predictive models to classify\nBrazilian legal proceedings in three possible classes of status: (i) archived\nproceedings, (ii) active proceedings, and (iii) suspended proceedings. This\nproblem's resolution is intended to assist public and private institutions in\nmanaging large portfolios of legal proceedings, providing gains in scale and\nefficiency. In this paper, legal proceedings are made up of sequences of short\ntexts called \"motions.\" We combined several natural language processing (NLP)\nand machine learning techniques to solve the problem. Although working with\nPortuguese NLP, which can be challenging due to lack of resources, our\napproaches performed remarkably well in the classification task, achieving\nmaximum accuracy of .93 and top average F1 Scores of .89 (macro) and .93\n(weighted). Furthermore, we could extract and interpret the patterns learned by\none of our models besides quantifying how those patterns relate to the\nclassification task. The interpretability step is important among machine\nlearning legal applications and gives us an exciting insight into how black-box\nmodels make decisions.",
          "link": "http://arxiv.org/abs/2003.11561",
          "publishedOn": "2021-06-24T01:51:42.584Z",
          "wordCount": 677,
          "title": "Predicting Legal Proceedings Status: Approaches Based on Sequential Text Data. (arXiv:2003.11561v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zujie Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haifeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiaying Zhu</a>",
          "description": "Most existing Visual Question Answering (VQA) systems tend to overly rely on\nlanguage bias and hence fail to reason from the visual clue. To address this\nissue, we propose a novel Language-Prior Feedback (LPF) objective function, to\nre-balance the proportion of each answer's loss value in the total VQA loss.\nThe LPF firstly calculates a modulating factor to determine the language bias\nusing a question-only branch. Then, the LPF assigns a self-adaptive weight to\neach training sample in the training process. With this reweighting mechanism,\nthe LPF ensures that the total VQA loss can be reshaped to a more balanced\nform. By this means, the samples that require certain visual information to\npredict will be efficiently used during training. Our method is simple to\nimplement, model-agnostic, and end-to-end trainable. We conduct extensive\nexperiments and the results show that the LPF (1) brings a significant\nimprovement over various VQA models, (2) achieves competitive performance on\nthe bias-sensitive VQA-CP v2 benchmark.",
          "link": "http://arxiv.org/abs/2105.14300",
          "publishedOn": "2021-06-24T01:51:42.572Z",
          "wordCount": 629,
          "title": "LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering. (arXiv:2105.14300v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1\">Rui Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thaker_K/0/1/0/all/0/1\">Khushboo Thaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yue Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xingdi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Daqing He</a>",
          "description": "Faceted summarization provides briefings of a document from different\nperspectives. Readers can quickly comprehend the main points of a long document\nwith the help of a structured outline. However, little research has been\nconducted on this subject, partially due to the lack of large-scale faceted\nsummarization datasets. In this study, we present FacetSum, a faceted\nsummarization benchmark built on Emerald journal articles, covering a diverse\nrange of domains. Different from traditional document-summary pairs, FacetSum\nprovides multiple summaries, each targeted at specific sections of a long\ndocument, including the purpose, method, findings, and value. Analyses and\nempirical results on our dataset reveal the importance of bringing structure\ninto summaries. We believe FacetSum will spur further advances in summarization\nresearch and foster the development of NLP systems that can leverage the\nstructured information in both long texts and summaries.",
          "link": "http://arxiv.org/abs/2106.00130",
          "publishedOn": "2021-06-24T01:51:42.566Z",
          "wordCount": 603,
          "title": "Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents. (arXiv:2106.00130v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jon_J/0/1/0/all/0/1\">Josef Jon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aires_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Aires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varis_D/0/1/0/all/0/1\">Du&#x161;an Vari&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>",
          "description": "Lexically constrained machine translation allows the user to manipulate the\noutput sentence by enforcing the presence or absence of certain words and\nphrases. Although current approaches can enforce terms to appear in the\ntranslation, they often struggle to make the constraint word form agree with\nthe rest of the generated output. Our manual analysis shows that 46% of the\nerrors in the output of a baseline constrained model for English to Czech\ntranslation are related to agreement. We investigate mechanisms to allow neural\nmachine translation to infer the correct word inflection given lemmatized\nconstraints. In particular, we focus on methods based on training the model\nwith constraints provided as part of the input sequence. Our experiments on the\nEnglish-Czech language pair show that this approach improves the translation of\nconstrained terms in both automatic and manual evaluation by reducing errors in\nagreement. Our approach thus eliminates inflection errors, without introducing\nnew errors or decreasing the overall quality of the translation.",
          "link": "http://arxiv.org/abs/2106.12398",
          "publishedOn": "2021-06-24T01:51:42.503Z",
          "wordCount": 594,
          "title": "End-to-End Lexically Constrained Machine Translation for Morphologically Rich Languages. (arXiv:2106.12398v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.09807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bingqing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1\">Jacopo Tagliabue</a>",
          "description": "Word embeddings (e.g., word2vec) have been applied successfully to eCommerce\nproducts through~\\textit{prod2vec}. Inspired by the recent performance\nimprovements on several NLP tasks brought by contextualized embeddings, we\npropose to transfer BERT-like architectures to eCommerce: our model --\n~\\textit{Prod2BERT} -- is trained to generate representations of products\nthrough masked session modeling. Through extensive experiments over multiple\nshops, different tasks, and a range of design choices, we systematically\ncompare the accuracy of~\\textit{Prod2BERT} and~\\textit{prod2vec} embeddings:\nwhile~\\textit{Prod2BERT} is found to be superior in several scenarios, we\nhighlight the importance of resources and hyperparameters in the best\nperforming models. Finally, we provide guidelines to practitioners for training\nembeddings under a variety of computational and data constraints.",
          "link": "http://arxiv.org/abs/2012.09807",
          "publishedOn": "2021-06-24T01:51:42.485Z",
          "wordCount": 589,
          "title": "BERT Goes Shopping: Comparing Distributional Models for Product Representations. (arXiv:2012.09807v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Riley_P/0/1/0/all/0/1\">Parker Riley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mandy Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Girish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uthus_D/0/1/0/all/0/1\">David Uthus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_Z/0/1/0/all/0/1\">Zarana Parekh</a>",
          "description": "We present a novel approach to the problem of text style transfer. Unlike\nprevious approaches requiring style-labeled training data, our method makes use\nof readily-available unlabeled text by relying on the implicit connection in\nstyle between adjacent sentences, and uses labeled data only at inference time.\nWe adapt T5 (Raffel et al., 2020), a strong pretrained text-to-text model, to\nextract a style vector from text and use it to condition the decoder to perform\nstyle transfer. As our label-free training results in a style vector space\nencoding many facets of style, we recast transfers as \"targeted restyling\"\nvector operations that adjust specific attributes of the input while preserving\nothers. We demonstrate that training on unlabeled Amazon reviews data results\nin a model that is competitive on sentiment transfer, even compared to models\ntrained fully on labeled data. Furthermore, applying our novel method to a\ndiverse corpus of unlabeled web text results in a single model capable of\ntransferring along multiple dimensions of style (dialect, emotiveness,\nformality, politeness, sentiment) despite no additional training and using only\na handful of exemplars at inference time.",
          "link": "http://arxiv.org/abs/2010.03802",
          "publishedOn": "2021-06-24T01:51:42.479Z",
          "wordCount": 662,
          "title": "TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling. (arXiv:2010.03802v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.05297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cathcart_C/0/1/0/all/0/1\">Chundra Aroor Cathcart</a>",
          "description": "This paper addresses a series of complex and unresolved issues in the\nhistorical phonology of West Iranian languages. The West Iranian languages\n(Persian, Kurdish, Balochi, and other languages) display a high degree of\nnon-Lautgesetzlich behavior. Most of this irregularity is undoubtedly due to\nlanguage contact; we argue, however, that an oversimplified view of the\nprocesses at work has prevailed in the literature on West Iranian dialectology,\nwith specialists assuming that deviations from an expected outcome in a given\nnon-Persian language are due to lexical borrowing from some chronological stage\nof Persian. It is demonstrated that this qualitative approach yields at times\nproblematic conclusions stemming from the lack of explicit probabilistic\ninferences regarding the distribution of the data: Persian may not be the sole\ndonor language; additionally, borrowing at the lexical level is not always the\nmechanism that introduces irregularity. In many cases, the possibility that\nWest Iranian languages show different reflexes in different conditioning\nenvironments remains under-explored. We employ a novel Bayesian approach\ndesigned to overcome these problems and tease apart the different determinants\nof irregularity in patterns of West Iranian sound change. Our methodology\nallows us to provisionally resolve a number of outstanding questions in the\nliterature on West Iranian dialectology concerning the dialectal affiliation of\ncertain sound changes. We outline future directions for work of this sort.",
          "link": "http://arxiv.org/abs/2001.05297",
          "publishedOn": "2021-06-24T01:51:42.453Z",
          "wordCount": 683,
          "title": "Dialectal Layers in West Iranian: a Hierarchical Dirichlet Process Approach to Linguistic Relationships. (arXiv:2001.05297v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aithal_M/0/1/0/all/0/1\">Madhusudhan Aithal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>",
          "description": "Prior work has revealed that positive words occur more frequently than\nnegative words in human expressions, which is typically attributed to\npositivity bias, a tendency for people to report positive views of reality. But\nwhat about the language used in negative reviews? Consistent with prior work,\nwe show that English negative reviews tend to contain more positive words than\nnegative words, using a variety of datasets. We reconcile this observation with\nprior findings on the pragmatics of negation, and show that negations are\ncommonly associated with positive words in negative reviews. Furthermore, in\nnegative reviews, the majority of sentences with positive words express\nnegative opinions based on sentiment classifiers, indicating some form of\nnegation.",
          "link": "http://arxiv.org/abs/2106.12056",
          "publishedOn": "2021-06-24T01:51:42.447Z",
          "wordCount": 553,
          "title": "On Positivity Bias in Negative Reviews. (arXiv:2106.12056v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryabinin_M/0/1/0/all/0/1\">Max Ryabinin</a>",
          "description": "Commonsense reasoning is one of the key problems in natural language\nprocessing, but the relative scarcity of labeled data holds back the progress\nfor languages other than English. Pretrained cross-lingual models are a source\nof powerful language-agnostic representations, yet their inherent reasoning\ncapabilities are still actively studied. In this work, we design a simple\napproach to commonsense reasoning which trains a linear classifier with weights\nof multi-head attention as features. To evaluate this approach, we create a\nmultilingual Winograd Schema corpus by processing several datasets from prior\nwork within a standardized pipeline and measure cross-lingual generalization\nability in terms of out-of-sample performance. The method performs\ncompetitively with recent supervised and unsupervised approaches for\ncommonsense reasoning, even when applied to other languages in a zero-shot\nmanner. Also, we demonstrate that most of the performance is given by the same\nsmall subset of attention heads for all studied languages, which provides\nevidence of universal reasoning capabilities in multilingual encoders.",
          "link": "http://arxiv.org/abs/2106.12066",
          "publishedOn": "2021-06-24T01:51:42.424Z",
          "wordCount": 622,
          "title": "It's All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning. (arXiv:2106.12066v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2005.07920",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Naowarat_B/0/1/0/all/0/1\">Burin Naowarat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kongthaworn_T/0/1/0/all/0/1\">Thananchai Kongthaworn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karunratanakul_K/0/1/0/all/0/1\">Korrawe Karunratanakul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1\">Sheng Hui Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chuangsuwanich_E/0/1/0/all/0/1\">Ekapol Chuangsuwanich</a>",
          "description": "Code-Switching (CS) remains a challenge for Automatic Speech Recognition\n(ASR), especially character-based models. With the combined choice of\ncharacters from multiple languages, the outcome from character-based models\nsuffers from phoneme duplication, resulting in language-inconsistent spellings.\nWe propose Contextualized Connectionist Temporal Classification (CCTC) loss to\nencourage spelling consistencies of a character-based non-autoregressive ASR\nwhich allows for faster inference. The CCTC loss conditions the main prediction\non the predicted contexts to ensure language consistency in the spellings. In\ncontrast to existing CTC-based approaches, CCTC loss does not require\nframe-level alignments, since the context ground truth is obtained from the\nmodel's estimated path. Compared to the same model trained with regular CTC\nloss, our method consistently improved the ASR performance on both CS and\nmonolingual corpora.",
          "link": "http://arxiv.org/abs/2005.07920",
          "publishedOn": "2021-06-24T01:51:42.416Z",
          "wordCount": 605,
          "title": "Reducing Spelling Inconsistencies in Code-Switching ASR using Contextualized CTC Loss. (arXiv:2005.07920v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1\">Yuanxing Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>",
          "description": "Event extraction is a fundamental task for natural language processing.\nFinding the roles of event arguments like event participants is essential for\nevent extraction. However, doing so for real-life event descriptions is\nchallenging because an argument's role often varies in different contexts.\nWhile the relationship and interactions between multiple arguments are useful\nfor settling the argument roles, such information is largely ignored by\nexisting approaches. This paper presents a better approach for event extraction\nby explicitly utilizing the relationships of event arguments. We achieve this\nthrough a carefully designed task-oriented dialogue system. To model the\nargument relation, we employ reinforcement learning and incremental learning to\nextract multiple arguments via a multi-turned, iterative process. Our approach\nleverages knowledge of the already extracted arguments of the same sentence to\ndetermine the role of arguments that would be difficult to decide individually.\nIt then uses the newly obtained information to improve the decisions of\npreviously extracted arguments. This two-way feedback process allows us to\nexploit the argument relations to effectively settle argument roles, leading to\nbetter sentence understanding and event extraction. Experimental results show\nthat our approach consistently outperforms seven state-of-the-art event\nextraction methods for the classification of events and argument role and\nargument identification.",
          "link": "http://arxiv.org/abs/2106.12384",
          "publishedOn": "2021-06-24T01:51:42.409Z",
          "wordCount": 645,
          "title": "Reinforcement Learning-based Dialogue Guided Event Extraction to Exploit Argument Relations. (arXiv:2106.12384v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shengjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shanda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Tianle Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Dinglan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuxin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1\">Guolin Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "The attention module, which is a crucial component in Transformer, cannot\nscale efficiently to long sequences due to its quadratic complexity. Many works\nfocus on approximating the dot-then-exponentiate softmax function in the\noriginal attention, leading to sub-quadratic or even linear-complexity\nTransformer architectures. However, we show that these methods cannot be\napplied to more powerful attention modules that go beyond the\ndot-then-exponentiate style, e.g., Transformers with relative positional\nencoding (RPE). Since in many state-of-the-art models, relative positional\nencoding is used as default, designing efficient Transformers that can\nincorporate RPE is appealing. In this paper, we propose a novel way to\naccelerate attention calculation for Transformers with RPE on top of the\nkernelized attention. Based upon the observation that relative positional\nencoding forms a Toeplitz matrix, we mathematically show that kernelized\nattention with RPE can be calculated efficiently using Fast Fourier Transform\n(FFT). With FFT, our method achieves $\\mathcal{O}(n\\log n)$ time complexity.\nInterestingly, we further demonstrate that properly using relative positional\nencoding can mitigate the training instability problem of vanilla kernelized\nattention. On a wide range of tasks, we empirically show that our models can be\ntrained from scratch without any optimization issues. The learned model\nperforms better than many efficient Transformer variants and is faster than\nstandard Transformer in the long-sequence regime.",
          "link": "http://arxiv.org/abs/2106.12566",
          "publishedOn": "2021-06-24T01:51:42.400Z",
          "wordCount": 670,
          "title": "Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding. (arXiv:2106.12566v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junxia Lin</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ledolter_J/0/1/0/all/0/1\">Johannes Ledolter</a> (2) ((1) Georgetown University Medical Center, Georgetown University, (2) Tippie College of Business, University of Iowa)",
          "description": "The focus of our paper is the identification and correction of non-word\nerrors in OCR text. Such errors may be the result of incorrect insertion,\ndeletion, or substitution of a character, or the transposition of two adjacent\ncharacters within a single word. Or, it can be the result of word boundary\nproblems that lead to run-on errors and incorrect-split errors. The traditional\nN-gram correction methods can handle single-word errors effectively. However,\nthey show limitations when dealing with split and merge errors. In this paper,\nwe develop an unsupervised method that can handle both errors. The method we\ndevelop leads to a sizable improvement in the correction rates. This tutorial\npaper addresses very difficult word correction problems - namely incorrect\nrun-on and split errors - and illustrates what needs to be considered when\naddressing such problems. We outline a possible approach and assess its success\non a limited study.",
          "link": "http://arxiv.org/abs/2106.12030",
          "publishedOn": "2021-06-24T01:51:42.391Z",
          "wordCount": 603,
          "title": "A Simple and Practical Approach to Improve Misspellings in OCR Text. (arXiv:2106.12030v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozdil_U/0/1/0/all/0/1\">Umut &#xd6;zdil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arslan_B/0/1/0/all/0/1\">B&#xfc;&#x15f;ra Arslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tasar_D/0/1/0/all/0/1\">D. Emre Ta&#x15f;ar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polat_G/0/1/0/all/0/1\">G&#xf6;k&#xe7;e Polat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozan_S/0/1/0/all/0/1\">&#x15e;&#xfc;kr&#xfc; Ozan</a>",
          "description": "In this study, a natural language processing-based (NLP-based) method is\nproposed for the sector-wise automatic classification of ad texts created on\nonline advertising platforms. Our data set consists of approximately 21,000\nlabeled advertising texts from 12 different sectors. In the study, the\nBidirectional Encoder Representations from Transformers (BERT) model, which is\na transformer-based language model that is recently used in fields such as text\nclassification in the natural language processing literature, was used. The\nclassification efficiencies obtained using a pre-trained BERT model for the\nTurkish language are shown in detail.",
          "link": "http://arxiv.org/abs/2106.10899",
          "publishedOn": "2021-06-24T01:51:42.366Z",
          "wordCount": 554,
          "title": "Ad Text Classification with Transformer-Based Natural Language Processing Methods. (arXiv:2106.10899v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yanjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting-hao/0/1/0/all/0/1\">Ting-hao</a> (Kenneth) <a href=\"http://arxiv.org/find/cs/1/au:+Huang/0/1/0/all/0/1\">Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1\">Rebecca J. Passonneau</a>",
          "description": "Atomic clauses are fundamental text units for understanding complex\nsentences. Identifying the atomic sentences within complex sentences is\nimportant for applications such as summarization, argument mining, discourse\nanalysis, discourse parsing, and question answering. Previous work mainly\nrelies on rule-based methods dependent on parsing. We propose a new task to\ndecompose each complex sentence into simple sentences derived from the tensed\nclauses in the source, and a novel problem formulation as a graph edit task.\nOur neural model learns to Accept, Break, Copy or Drop elements of a graph that\ncombines word adjacency and grammatical dependencies. The full processing\npipeline includes modules for graph construction, graph editing, and sentence\ngeneration from the output graph. We introduce DeSSE, a new dataset designed to\ntrain and evaluate complex sentence decomposition, and MinWiki, a subset of\nMinWikiSplit. ABCD achieves comparable performance as two parsing baselines on\nMinWiki. On DeSSE, which has a more even balance of complex sentence types, our\nmodel achieves higher accuracy on the number of atomic sentences than an\nencoder-decoder baseline. Results include a detailed error analysis.",
          "link": "http://arxiv.org/abs/2106.12027",
          "publishedOn": "2021-06-24T01:51:42.350Z",
          "wordCount": 649,
          "title": "ABCD: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences. (arXiv:2106.12027v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiang Dai</a>",
          "description": "The growth rate in the amount of biomedical documents is staggering.\nUnlocking information trapped in these documents can enable researchers and\npractitioners to operate confidently in the information world. Biomedical NER,\nthe task of recognising biomedical names, is usually employed as the first step\nof the NLP pipeline. Standard NER models, based on sequence tagging technique,\nare good at recognising short entity mentions in the generic domain. However,\nthere are several open challenges of applying these models to recognise\nbiomedical names: 1) Biomedical names may contain complex inner structure\n(discontinuity and overlapping) which cannot be recognised using standard\nsequence tagging technique; 2) The training of NER models usually requires\nlarge amount of labelled data, which are difficult to obtain in the biomedical\ndomain; and, 3) Commonly used language representation models are pre-trained on\ngeneric data; a domain shift therefore exists between these models and target\nbiomedical data. To deal with these challenges, we explore several research\ndirections and make the following contributions: 1) we propose a\ntransition-based NER model which can recognise discontinuous mentions; 2) We\ndevelop a cost-effective approach that nominates the suitable pre-training\ndata; and, 3) We design several data augmentation methods for NER. Our\ncontributions have obvious practical implications, especially when new\nbiomedical applications are needed. Our proposed data augmentation methods can\nhelp the NER model achieve decent performance, requiring only a small amount of\nlabelled data. Our investigation regarding selecting pre-training data can\nimprove the model by incorporating language representation models, which are\npre-trained using in-domain data. Finally, our proposed transition-based NER\nmodel can further improve the performance by recognising discontinuous\nmentions.",
          "link": "http://arxiv.org/abs/2106.12230",
          "publishedOn": "2021-06-24T01:51:42.344Z",
          "wordCount": 693,
          "title": "Recognising Biomedical Names: Challenges and Solutions. (arXiv:2106.12230v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2003.07723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haider_T/0/1/0/all/0/1\">Thomas Haider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Evgeny Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menninghaus_W/0/1/0/all/0/1\">Winfried Menninghaus</a>",
          "description": "Most approaches to emotion analysis of social media, literature, news, and\nother domains focus exclusively on basic emotion categories as defined by Ekman\nor Plutchik. However, art (such as literature) enables engagement in a broader\nrange of more complex and subtle emotions. These have been shown to also\ninclude mixed emotional responses. We consider emotions in poetry as they are\nelicited in the reader, rather than what is expressed in the text or intended\nby the author. Thus, we conceptualize a set of aesthetic emotions that are\npredictive of aesthetic appreciation in the reader, and allow the annotation of\nmultiple labels per line to capture mixed emotions within their context. We\nevaluate this novel setting in an annotation experiment both with carefully\ntrained experts and via crowdsourcing. Our annotation with experts leads to an\nacceptable agreement of kappa = .70, resulting in a consistent dataset for\nfuture large scale analysis. Finally, we conduct first emotion classification\nexperiments based on BERT, showing that identifying aesthetic emotions is\nchallenging in our data, with up to .52 F1-micro on the German subset. Data and\nresources are available at https://github.com/tnhaider/poetry-emotion",
          "link": "http://arxiv.org/abs/2003.07723",
          "publishedOn": "2021-06-24T01:51:42.337Z",
          "wordCount": 692,
          "title": "PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry. (arXiv:2003.07723v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>",
          "description": "A growing effort in NLP aims to build datasets of human explanations.\nHowever, the term explanation encompasses a broad range of notions, each with\ndifferent properties and ramifications. Our goal is to provide an overview of\ndiverse types of explanations and human limitations, and discuss implications\nfor collecting and using explanations in NLP. Inspired by prior work in\npsychology and cognitive sciences, we group existing human explanations in NLP\ninto three categories: proximal mechanism, evidence, and procedure. These three\ntypes differ in nature and have implications for the resultant explanations.\nFor instance, procedure is not considered explanations in psychology and\nconnects with a rich body of work on learning from instructions. The diversity\nof explanations is further evidenced by proxy questions that are needed for\nannotators to interpret and answer open-ended why questions. Finally,\nexplanations may require different, often deeper, understandings than\npredictions, which casts doubt on whether humans can provide useful\nexplanations in some tasks.",
          "link": "http://arxiv.org/abs/2106.11988",
          "publishedOn": "2021-06-24T01:51:42.329Z",
          "wordCount": 595,
          "title": "On the Diversity and Limits of Human Explanations. (arXiv:2106.11988v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ihori_M/0/1/0/all/0/1\">Mana Ihori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1\">Naoki Makishima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Tomohiro Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1\">Akihiko Takashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orihashi_S/0/1/0/all/0/1\">Shota Orihashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1\">Ryo Masumura</a>",
          "description": "In this paper, we propose a novel spoken-text-style conversion method that\ncan simultaneously execute multiple style conversion modules such as\npunctuation restoration and disfluency deletion without preparing matched\ndatasets. In practice, transcriptions generated by automatic speech recognition\nsystems are not highly readable because they often include many disfluencies\nand do not include punctuation marks. To improve their readability, multiple\nspoken-text-style conversion modules that individually model a single\nconversion task are cascaded because matched datasets that simultaneously\nhandle multiple conversion tasks are often unavailable. However, the cascading\nis unstable against the order of tasks because of the chain of conversion\nerrors. Besides, the computation cost of the cascading must be higher than the\nsingle conversion. To execute multiple conversion tasks simultaneously without\npreparing matched datasets, our key idea is to distinguish individual\nconversion tasks using the on-off switch. In our proposed zero-shot joint\nmodeling, we switch the individual tasks using multiple switching tokens,\nenabling us to utilize a zero-shot learning approach to executing simultaneous\nconversions. Our experiments on joint modeling of disfluency deletion and\npunctuation restoration demonstrate the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2106.12131",
          "publishedOn": "2021-06-24T01:51:42.304Z",
          "wordCount": 628,
          "title": "Zero-Shot Joint Modeling of Multiple Spoken-Text-Style Conversion Tasks using Switching Tokens. (arXiv:2106.12131v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14152",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qiujia Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1\">Liangliang Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>",
          "description": "End-to-end models with auto-regressive decoders have shown impressive results\nfor automatic speech recognition (ASR). These models formulate the\nsequence-level probability as a product of the conditional probabilities of all\nindividual tokens given their histories. However, the performance of locally\nnormalised models can be sub-optimal because of factors such as exposure bias.\nConsequently, the model distribution differs from the underlying data\ndistribution. In this paper, the residual energy-based model (R-EBM) is\nproposed to complement the auto-regressive ASR model to close the gap between\nthe two distributions. Meanwhile, R-EBMs can also be regarded as\nutterance-level confidence estimators, which may benefit many downstream tasks.\nExperiments on a 100hr LibriSpeech dataset show that R-EBMs can reduce the word\nerror rates (WERs) by 8.2%/6.7% while improving areas under precision-recall\ncurves of confidence scores by 12.6%/28.4% on test-clean/test-other sets.\nFurthermore, on a state-of-the-art model using self-supervised learning\n(wav2vec 2.0), R-EBMs still significantly improves both the WER and confidence\nestimation performance.",
          "link": "http://arxiv.org/abs/2103.14152",
          "publishedOn": "2021-06-24T01:51:42.294Z",
          "wordCount": 629,
          "title": "Residual Energy-Based Models for End-to-End Speech Recognition. (arXiv:2103.14152v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1\">Nawshad Farruque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1\">Randy Goebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivapalan_S/0/1/0/all/0/1\">Sudhakar Sivapalan</a>",
          "description": "We focus on exploring various approaches of Zero-Shot Learning (ZSL) and\ntheir explainability for a challenging yet important supervised learning task\nnotorious for training data scarcity, i.e. Depression Symptoms Detection (DSD)\nfrom text. We start with a comprehensive synthesis of different components of\nour ZSL modeling and analysis of our ground truth samples and Depression\nsymptom clues curation process with the help of a practicing clinician. We next\nanalyze the accuracy of various state-of-the-art ZSL models and their potential\nenhancements for our task. Further, we sketch a framework for the use of ZSL\nfor hierarchical text-based explanation mechanism, which we call, Syntax\nTree-Guided Semantic Explanation (STEP). Finally, we summarize experiments from\nwhich we conclude that we can use ZSL models and achieve reasonable accuracy\nand explainability, measured by a proposed Explainability Index (EI). This work\nis, to our knowledge, the first work to exhaustively explore the efficacy of\nZSL models for DSD task, both in terms of accuracy and explainability.",
          "link": "http://arxiv.org/abs/2106.10928",
          "publishedOn": "2021-06-24T01:51:42.283Z",
          "wordCount": 640,
          "title": "STEP-EZ: Syntax Tree guided semantic ExPlanation for Explainable Zero-shot modeling of clinical depression symptoms from text. (arXiv:2106.10928v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>",
          "description": "We consider repair tasks: given a critic (e.g., compiler) that assesses the\nquality of an input, the goal is to train a fixer that converts a bad example\n(e.g., code with syntax errors) into a good one (e.g., code with no syntax\nerrors). Existing works create training data consisting of (bad, good) pairs by\ncorrupting good examples using heuristics (e.g., dropping tokens). However,\nfixers trained on this synthetically-generated data do not extrapolate well to\nthe real distribution of bad inputs. To bridge this gap, we propose a new\ntraining approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use\nthe critic to check a fixer's output on real bad inputs and add good (fixed)\noutputs to the training data, and (ii) we train a breaker to generate realistic\nbad code from good code. Based on these ideas, we iteratively update the\nbreaker and the fixer while using them in conjunction to generate more paired\ndata. We evaluate BIFI on two code repair datasets: GitHub-Python, a new\ndataset we introduce where the goal is to repair Python code with AST parse\nerrors; and DeepFix, where the goal is to repair C code with compiler errors.\nBIFI outperforms existing methods, obtaining 90.5% repair accuracy on\nGitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%). Notably, BIFI does not\nrequire any labeled data; we hope it will be a strong starting point for\nunsupervised learning of various repair tasks.",
          "link": "http://arxiv.org/abs/2106.06600",
          "publishedOn": "2021-06-23T01:48:38.902Z",
          "wordCount": 693,
          "title": "Break-It-Fix-It: Unsupervised Learning for Program Repair. (arXiv:2106.06600v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Finlayson_M/0/1/0/all/0/1\">Matthew Finlayson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_A/0/1/0/all/0/1\">Aaron Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shieber_S/0/1/0/all/0/1\">Stuart Shieber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>",
          "description": "Targeted syntactic evaluations have demonstrated the ability of language\nmodels to perform subject-verb agreement given difficult contexts. To elucidate\nthe mechanisms by which the models accomplish this behavior, this study applies\ncausal mediation analysis to pre-trained neural language models. We investigate\nthe magnitude of models' preferences for grammatical inflections, as well as\nwhether neurons process subject-verb agreement similarly across sentences with\ndifferent syntactic structures. We uncover similarities and differences across\narchitectures and model sizes -- notably, that larger models do not necessarily\nlearn stronger preferences. We also observe two distinct mechanisms for\nproducing subject-verb agreement depending on the syntactic structure of the\ninput sentence. Finally, we find that language models rely on similar sets of\nneurons when given sentences with similar syntactic structure.",
          "link": "http://arxiv.org/abs/2106.06087",
          "publishedOn": "2021-06-23T01:48:38.764Z",
          "wordCount": 600,
          "title": "Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models. (arXiv:2106.06087v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1\">Weizhen Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bolun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bartuer Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Biao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiusheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>",
          "description": "Now, the pre-training technique is ubiquitous in natural language processing\nfield. ProphetNet is a pre-training based natural language generation method\nwhich shows powerful performance on English text summarization and question\ngeneration tasks. In this paper, we extend ProphetNet into other domains and\nlanguages, and present the ProphetNet family pre-training models, named\nProphetNet-X, where X can be English, Chinese, Multi-lingual, and so on. We\npre-train a cross-lingual generation model ProphetNet-Multi, a Chinese\ngeneration model ProphetNet-Zh, two open-domain dialog generation models\nProphetNet-Dialog-En and ProphetNet-Dialog-Zh. And also, we provide a PLG\n(Programming Language Generation) model ProphetNet-Code to show the generation\nperformance besides NLG (Natural Language Generation) tasks. In our\nexperiments, ProphetNet-X models achieve new state-of-the-art performance on 10\nbenchmarks. All the models of ProphetNet-X share the same model structure,\nwhich allows users to easily switch between different models. We make the code\nand models publicly available, and we will keep updating more pre-training\nmodels and finetuning scripts.",
          "link": "http://arxiv.org/abs/2104.08006",
          "publishedOn": "2021-06-23T01:48:38.731Z",
          "wordCount": 648,
          "title": "ProphetNet-X: Large-Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code Generation. (arXiv:2104.08006v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>",
          "description": "This paper describes the system submitted to the IWSLT 2021 Multilingual\nSpeech Translation (MultiST) task from Huawei Noah's Ark Lab. We use a unified\ntransformer architecture for our MultiST model, so that the data from different\nmodalities (i.e., speech and text) and different tasks (i.e., Speech\nRecognition, Machine Translation, and Speech Translation) can be exploited to\nenhance the model's ability. Specifically, speech and text inputs are firstly\nfed to different feature extractors to extract acoustic and textual features,\nrespectively. Then, these features are processed by a shared encoder--decoder\narchitecture. We apply several training techniques to improve the performance,\nincluding multi-task learning, task-level curriculum learning, data\naugmentation, etc. Our final system achieves significantly better results than\nbilingual baselines on supervised language pairs and yields reasonable results\non zero-shot language pairs.",
          "link": "http://arxiv.org/abs/2106.00197",
          "publishedOn": "2021-06-23T01:48:38.717Z",
          "wordCount": 599,
          "title": "Multilingual Speech Translation with Unified Transformer: Huawei Noah's Ark Lab at IWSLT 2021. (arXiv:2106.00197v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>",
          "description": "Language models trained on billions of tokens have recently led to\nunprecedented results on many NLP tasks. This success raises the question of\nwhether, in principle, a system can ever ``understand'' raw text without access\nto some form of grounding. We formally investigate the abilities of ungrounded\nsystems to acquire meaning. Our analysis focuses on the role of ``assertions'':\ntextual contexts that provide indirect clues about the underlying semantics. We\nstudy whether assertions enable a system to emulate representations preserving\nsemantic relations like equivalence. We find that assertions enable semantic\nemulation of languages that satisfy a strong notion of semantic transparency.\nHowever, for classes of languages where the same expression can take different\nvalues in different contexts, we show that emulation can become uncomputable.\nFinally, we discuss differences between our formal model and natural language,\nexploring how our results generalize to a modal setting and other semantic\nrelations. Together, our results suggest that assertions in code or language do\nnot provide sufficient signal to fully emulate semantic representations. We\nformalize ways in which ungrounded language models appear to be fundamentally\nlimited in their ability to ``understand''.",
          "link": "http://arxiv.org/abs/2104.10809",
          "publishedOn": "2021-06-23T01:48:38.643Z",
          "wordCount": 674,
          "title": "Provable Limitations of Acquiring Meaning from Ungrounded Form: What Will Future Language Models Understand?. (arXiv:2104.10809v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>",
          "description": "Most previous studies integrate cognitive language processing signals (e.g.,\neye-tracking or EEG data) into neural models of natural language processing\n(NLP) just by directly concatenating word embeddings with cognitive features,\nignoring the gap between the two modalities (i.e., textual vs. cognitive) and\nnoise in cognitive features. In this paper, we propose a CogAlign approach to\nthese issues, which learns to align textual neural representations to cognitive\nfeatures. In CogAlign, we use a shared encoder equipped with a modality\ndiscriminator to alternatively encode textual and cognitive inputs to capture\ntheir differences and commonalities. Additionally, a text-aware attention\nmechanism is proposed to detect task-related information and to avoid using\nnoise in cognitive features. Experimental results on three NLP tasks, namely\nnamed entity recognition, sentiment analysis and relation extraction, show that\nCogAlign achieves significant improvements with multiple cognitive features\nover state-of-the-art models on public datasets. Moreover, our model is able to\ntransfer cognitive information to other datasets that do not have any cognitive\nprocessing signals.",
          "link": "http://arxiv.org/abs/2106.05544",
          "publishedOn": "2021-06-23T01:48:38.636Z",
          "wordCount": 615,
          "title": "CogAlign: Learning to Align Textual Neural Representations to Cognitive Language Processing Signals. (arXiv:2106.05544v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stankevicius_L/0/1/0/all/0/1\">Lukas Stankevi&#x10d;ius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukosevicius_M/0/1/0/all/0/1\">Mantas Luko&#x161;evi&#x10d;ius</a>",
          "description": "In this work, we train the first monolingual Lithuanian transformer model on\na relatively large corpus of Lithuanian news articles and compare various\noutput decoding algorithms for abstractive news summarization. We achieve an\naverage ROUGE-2 score 0.163, generated summaries are coherent and look\nimpressive at first glance. However, some of them contain misleading\ninformation that is not so easy to spot. We describe all the technical details\nand share our trained model and accompanying code in an online open-source\nrepository, as well as some characteristic samples of the generated summaries.",
          "link": "http://arxiv.org/abs/2105.03279",
          "publishedOn": "2021-06-23T01:48:38.597Z",
          "wordCount": 569,
          "title": "Generating abstractive summaries of Lithuanian news articles using a transformer model. (arXiv:2105.03279v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1\">Baban Gain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1\">Dibyanayan Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1\">Tanik Saikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>",
          "description": "Natural Language Processing (NLP) and Information Retrieval (IR) in the\njudicial domain is an essential task. With the advent of availability\ndomain-specific data in electronic form and aid of different Artificial\nintelligence (AI) technologies, automated language processing becomes more\ncomfortable, and hence it becomes feasible for researchers and developers to\nprovide various automated tools to the legal community to reduce human burden.\nThe Competition on Legal Information Extraction/Entailment (COLIEE-2019) run in\nassociation with the International Conference on Artificial Intelligence and\nLaw (ICAIL)-2019 has come up with few challenging tasks. The shared defined\nfour sub-tasks (i.e. Task1, Task2, Task3 and Task4), which will be able to\nprovide few automated systems to the judicial system. The paper presents our\nworking note on the experiments carried out as a part of our participation in\nall the sub-tasks defined in this shared task. We make use of different\nInformation Retrieval(IR) and deep learning based approaches to tackle these\nproblems. We obtain encouraging results in all these four sub-tasks.",
          "link": "http://arxiv.org/abs/2104.08653",
          "publishedOn": "2021-06-23T01:48:38.589Z",
          "wordCount": 649,
          "title": "IITP@COLIEE 2019: Legal Information Retrieval using BM25 and BERT. (arXiv:2104.08653v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1\">Arun S. Maiya</a>",
          "description": "The vast majority of existing methods and systems for causal inference assume\nthat all variables under consideration are categorical or numerical (e.g.,\ngender, price, blood pressure, enrollment). In this paper, we present\nCausalNLP, a toolkit for inferring causality from observational data that\nincludes text in addition to traditional numerical and categorical variables.\nCausalNLP employs the use of meta-learners for treatment effect estimation and\nsupports using raw text and its linguistic properties as both a treatment and a\n\"controlled-for\" variable (e.g., confounder). The library is open-source and\navailable at: https://github.com/amaiya/causalnlp.",
          "link": "http://arxiv.org/abs/2106.08043",
          "publishedOn": "2021-06-23T01:48:38.581Z",
          "wordCount": 544,
          "title": "CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kunwoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhufeng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>",
          "description": "Understanding who blames or supports whom in news text is a critical research\nquestion in computational social science. Traditional methods and datasets for\nsentiment analysis are, however, not suitable for the domain of political text\nas they do not consider the direction of sentiments expressed between entities.\nIn this paper, we propose a novel NLP task of identifying directed sentiment\nrelationship between political entities from a given news document, which we\ncall directed sentiment extraction. From a million-scale news corpus, we\nconstruct a dataset of news sentences where sentiment relations of political\nentities are manually annotated. We present a simple but effective approach for\nutilizing a pretrained transformer, which infers the target class by predicting\nmultiple question-answering tasks and combining the outcomes. We demonstrate\nthe utility of our proposed method for social science research questions by\nanalyzing positive and negative opinions between political entities in two\nmajor events: 2016 U.S. presidential election and COVID-19. The newly proposed\nproblem, data, and method will facilitate future studies on interdisciplinary\nNLP methods and applications.",
          "link": "http://arxiv.org/abs/2106.01033",
          "publishedOn": "2021-06-23T01:48:38.573Z",
          "wordCount": 698,
          "title": "Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment Extraction in News Text. (arXiv:2106.01033v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.10875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carbone_G/0/1/0/all/0/1\">Ginevra Carbone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarti_G/0/1/0/all/0/1\">Gabriele Sarti</a>",
          "description": "Plug-and-play language models (PPLMs) enable topic-conditioned natural\nlanguage generation by pairing large pre-trained generators with attribute\nmodels used to steer the predicted token distribution towards the selected\ntopic. Despite their computational efficiency, PPLMs require large amounts of\nlabeled texts to effectively balance generation fluency and proper\nconditioning, making them unsuitable for low-resource settings. We present\nETC-NLG, an approach leveraging topic modeling annotations to enable\nfully-unsupervised End-to-end Topic-Conditioned Natural Language Generation\nover emergent topics in unlabeled document collections. We first test the\neffectiveness of our approach in a low-resource setting for Italian, evaluating\nthe conditioning for both topic models and gold annotations. We then perform a\ncomparative evaluation of ETC-NLG for Italian and English using a parallel\ncorpus. Finally, we propose an automatic approach to estimate the effectiveness\nof conditioning on the generated utterances.",
          "link": "http://arxiv.org/abs/2008.10875",
          "publishedOn": "2021-06-23T01:48:38.422Z",
          "wordCount": 599,
          "title": "ETC-NLG: End-to-end Topic-Conditioned Natural Language Generation. (arXiv:2008.10875v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Md Mahfuz ibn Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1\">Laurent Besacier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galle_M/0/1/0/all/0/1\">Matthias Gall&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>",
          "description": "As neural machine translation (NMT) systems become an important part of\nprofessional translator pipelines, a growing body of work focuses on combining\nNMT with terminologies. In many scenarios and particularly in cases of domain\nadaptation, one expects the MT output to adhere to the constraints provided by\na terminology. In this work, we propose metrics to measure the consistency of\nMT output with regards to a domain terminology. We perform studies on the\nCOVID-19 domain over 5 languages, also performing terminology-targeted human\nevaluation. We open-source the code for computing all proposed metrics:\nhttps://github.com/mahfuzibnalam/terminology_evaluation",
          "link": "http://arxiv.org/abs/2106.11891",
          "publishedOn": "2021-06-23T01:48:38.396Z",
          "wordCount": 584,
          "title": "On the Evaluation of Machine Translation for Terminology Consistency. (arXiv:2106.11891v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.09147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaojiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>",
          "description": "Most of the existing works for dialogue generation are data-driven models\ntrained directly on corpora crawled from websites. They mainly focus on\nimproving the model architecture to produce better responses but pay little\nattention to considering the quality of the training data contrastively. In\nthis paper, we propose a multi-level contrastive learning paradigm to model the\nfine-grained quality of the responses with respect to the query. A Rank-aware\nCalibration (RC) network is designed to construct the multi-level contrastive\noptimization objectives. Since these objectives are calculated based on the\nsentence level, which may erroneously encourage/suppress the generation of\nuninformative/informative words. To tackle this incidental issue, on one hand,\nwe design an exquisite token-level strategy for estimating the instance loss\nmore accurately. On the other hand, we build a Knowledge Inference (KI)\ncomponent to capture the keyword knowledge from the reference during training\nand exploit such information to encourage the generation of informative words.\nWe evaluate the proposed model on a carefully annotated dialogue dataset and\nthe results suggest that our model can generate more relevant and diverse\nresponses compared to the baseline models.",
          "link": "http://arxiv.org/abs/2009.09147",
          "publishedOn": "2021-06-23T01:48:38.347Z",
          "wordCount": 644,
          "title": "Enhancing Dialogue Generation via Multi-Level Contrastive Learning. (arXiv:2009.09147v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Silin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1\">Ryuichi Takanobu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>",
          "description": "Current task-oriented dialog (TOD) systems mostly manage structured knowledge\n(e.g. databases and tables) to guide the goal-oriented conversations. However,\nthey fall short of handling dialogs which also involve unstructured knowledge\n(e.g. reviews and documents). In this paper, we formulate a task of modeling\nTOD grounded on a fusion of structured and unstructured knowledge. To address\nthis task, we propose a TOD system with semi-structured knowledge management,\nSeKnow, which extends the belief state to manage knowledge with both structured\nand unstructured contents. Furthermore, we introduce two implementations of\nSeKnow based on a non-pretrained sequence-to-sequence model and a pretrained\nlanguage model, respectively. Both implementations use the end-to-end manner to\njointly optimize dialog modeling grounded on structured and unstructured\nknowledge. We conduct experiments on the modified version of MultiWOZ 2.1\ndataset, where dialogs are processed to involve semi-structured knowledge.\nExperimental results show that SeKnow has strong performances in both\nend-to-end dialog and intermediate knowledge management, compared to existing\nTOD systems and their extensions with pipeline knowledge management schemes.",
          "link": "http://arxiv.org/abs/2106.11796",
          "publishedOn": "2021-06-23T01:48:38.337Z",
          "wordCount": 610,
          "title": "End-to-End Task-Oriented Dialog Modeling with Semi-Structured Knowledge Management. (arXiv:2106.11796v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yi-Ling Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekiroglu_S/0/1/0/all/0/1\">Serra Sinem Tekiroglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerini_M/0/1/0/all/0/1\">Marco Guerini</a>",
          "description": "Tackling online hatred using informed textual responses - called counter\nnarratives - has been brought under the spotlight recently. Accordingly, a\nresearch line has emerged to automatically generate counter narratives in order\nto facilitate the direct intervention in the hate discussion and to prevent\nhate content from further spreading. Still, current neural approaches tend to\nproduce generic/repetitive responses and lack grounded and up-to-date evidence\nsuch as facts, statistics, or examples. Moreover, these models can create\nplausible but not necessarily true arguments. In this paper we present the\nfirst complete knowledge-bound counter narrative generation pipeline, grounded\nin an external knowledge repository that can provide more informative content\nto fight online hatred. Together with our approach, we present a series of\nexperiments that show its feasibility to produce suitable and informative\ncounter narratives in in-domain and cross-domain settings.",
          "link": "http://arxiv.org/abs/2106.11783",
          "publishedOn": "2021-06-23T01:48:38.327Z",
          "wordCount": 590,
          "title": "Towards Knowledge-Grounded Counter Narrative Generation for Hate Speech. (arXiv:2106.11783v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tejaswin_P/0/1/0/all/0/1\">Priyam Tejaswin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_D/0/1/0/all/0/1\">Dhruv Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>",
          "description": "State-of-the-art summarization systems are trained and evaluated on massive\ndatasets scraped from the web. Despite their prevalence, we know very little\nabout the underlying characteristics (data noise, summarization complexity,\netc.) of these datasets, and how these affect system performance and the\nreliability of automatic metrics like ROUGE. In this study, we manually analyze\n600 samples from three popular summarization datasets. Our study is driven by a\nsix-class typology which captures different noise types (missing facts,\nentities) and degrees of summarization difficulty (extractive, abstractive). We\nfollow with a thorough analysis of 27 state-of-the-art summarization models and\n5 popular metrics, and report our key insights: (1) Datasets have distinct data\nquality and complexity distributions, which can be traced back to their\ncollection process. (2) The performance of models and reliability of metrics is\ndependent on sample complexity. (3) Faithful summaries often receive low scores\nbecause of the poor diversity of references. We release the code, annotated\ndata and model outputs.",
          "link": "http://arxiv.org/abs/2106.11388",
          "publishedOn": "2021-06-23T01:48:38.283Z",
          "wordCount": 598,
          "title": "How well do you know your summarization datasets?. (arXiv:2106.11388v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Staniek_M/0/1/0/all/0/1\">Michael Staniek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>",
          "description": "In semantic parsing of geographical queries against real-world databases such\nas OpenStreetMap (OSM), unique correct answers do not necessarily exist.\nInstead, the truth might be lying in the eye of the user, who needs to enter an\ninteractive setup where ambiguities can be resolved and parsing mistakes can be\ncorrected. Our work presents an approach to interactive semantic parsing where\nan explicit error detection is performed, and a clarification question is\ngenerated that pinpoints the suspected source of ambiguity or error and\ncommunicates it to the human user. Our experimental results show that a\ncombination of entropy-based uncertainty detection and beam search, together\nwith multi-source training on clarification question, initial parse, and user\nanswer, results in improvements of 1.2% F1 score on a parser that already\nperforms at 90.26% on the NLMaps dataset for OSM semantic parsing.",
          "link": "http://arxiv.org/abs/2106.11739",
          "publishedOn": "2021-06-23T01:48:38.270Z",
          "wordCount": 566,
          "title": "Error-Aware Interactive Semantic Parsing of OpenStreetMap. (arXiv:2106.11739v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>",
          "description": "Inspired by evidence that pretrained language models (LMs) encode commonsense\nknowledge, recent work has applied LMs to automatically populate commonsense\nknowledge graphs (CKGs). However, there is a lack of understanding on their\ngeneralization to multiple CKGs, unseen relations, and novel entities. This\npaper analyzes the ability of LMs to perform generalizable commonsense\ninference, in terms of knowledge capacity, transferability, and induction. Our\nexperiments with these three aspects show that: (1) LMs can adapt to different\nschemas defined by multiple CKGs but fail to reuse the knowledge to generalize\nto new relations. (2) Adapted LMs generalize well to unseen subjects, but less\nso on novel objects. Future work should investigate how to improve the\ntransferability and induction of commonsense mining from LMs.",
          "link": "http://arxiv.org/abs/2106.11533",
          "publishedOn": "2021-06-23T01:48:38.262Z",
          "wordCount": 564,
          "title": "Do Language Models Perform Generalizable Commonsense Inference?. (arXiv:2106.11533v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruotian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yaqian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>",
          "description": "Distant supervision for relation extraction provides uniform bag labels for\neach sentence inside the bag, while accurate sentence labels are important for\ndownstream applications that need the exact relation type. Directly using bag\nlabels for sentence-level training will introduce much noise, thus severely\ndegrading performance. In this work, we propose the use of negative training\n(NT), in which a model is trained using complementary labels regarding that\n``the instance does not belong to these complementary labels\". Since the\nprobability of selecting a true label as a complementary label is low, NT\nprovides less noisy information. Furthermore, the model trained with NT is able\nto separate the noisy data from the training data. Based on NT, we propose a\nsentence-level framework, SENT, for distant relation extraction. SENT not only\nfilters the noisy data to construct a cleaner dataset, but also performs a\nre-labeling process to transform the noisy data into useful training data, thus\nfurther benefiting the model's performance. Experimental results show the\nsignificant improvement of the proposed method over previous methods on\nsentence-level evaluation and de-noise effect.",
          "link": "http://arxiv.org/abs/2106.11566",
          "publishedOn": "2021-06-23T01:48:38.253Z",
          "wordCount": 618,
          "title": "SENT: Sentence-level Distant Relation Extraction via Negative Training. (arXiv:2106.11566v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>",
          "description": "Recently, the development of pre-trained language models has brought natural\nlanguage processing (NLP) tasks to the new state-of-the-art. In this paper we\nexplore the efficiency of various pre-trained language models. We pre-train a\nlist of transformer-based models with the same amount of text and the same\ntraining steps. The experimental results shows that the most improvement upon\nthe origin BERT is adding the RNN-layer to capture more contextual information\nfor the transformer-encoder layers.",
          "link": "http://arxiv.org/abs/2106.11483",
          "publishedOn": "2021-06-23T01:48:38.245Z",
          "wordCount": 501,
          "title": "A Comprehensive Exploration of Pre-training Language Models. (arXiv:2106.11483v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1\">Devamanyu Hazarika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>",
          "description": "The majority of existing methods for empathetic response generation rely on\nthe emotion of the context to generate empathetic responses. However, empathy\nis much more than generating responses with an appropriate emotion. It also\noften entails subtle expressions of understanding and personal resonance with\nthe situation of the other interlocutor. Unfortunately, such qualities are\ndifficult to quantify and the datasets lack the relevant annotations. To\naddress this issue, in this paper we propose an approach that relies on\nexemplars to cue the generative model on fine stylistic properties that signal\nempathy to the interlocutor. To this end, we employ dense passage retrieval to\nextract relevant exemplary responses from the training set. Three elements of\nhuman communication -- emotional presence, interpretation, and exploration, and\nsentiment are additionally introduced using synthetic labels to guide the\ngeneration towards empathy. The human evaluation is also extended by these\nelements of human communication. We empirically show that these approaches\nyield significant improvements in empathetic response quality in terms of both\nautomated and human-evaluated metrics. The implementation is available at\nhttps://github.com/declare-lab/exemplary-empathy.",
          "link": "http://arxiv.org/abs/2106.11791",
          "publishedOn": "2021-06-23T01:48:38.223Z",
          "wordCount": 622,
          "title": "Exemplars-guided Empathetic Response Generation Controlled by the Elements of Human Communication. (arXiv:2106.11791v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11531",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Suhang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>",
          "description": "Routing methods in capsule networks often learn a hierarchical relationship\nfor capsules in successive layers, but the intra-relation between capsules in\nthe same layer is less studied, while this intra-relation is a key factor for\nthe semantic understanding in text data. Therefore, in this paper, we introduce\na new capsule network with graph routing to learn both relationships, where\ncapsules in each layer are treated as the nodes of a graph. We investigate\nstrategies to yield adjacency and degree matrix with three different distances\nfrom a layer of capsules, and propose the graph routing mechanism between those\ncapsules. We validate our approach on five text classification datasets, and\nour findings suggest that the approach combining bottom-up routing and top-down\nattention performs the best. Such an approach demonstrates generalization\ncapability across datasets. Compared to the state-of-the-art routing methods,\nthe improvements in accuracy in the five datasets we used were 0.82, 0.39,\n0.07, 1.01, and 0.02, respectively.",
          "link": "http://arxiv.org/abs/2106.11531",
          "publishedOn": "2021-06-23T01:48:38.211Z",
          "wordCount": 592,
          "title": "Graph Routing between Capsules. (arXiv:2106.11531v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chia-Hsuan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polozov_O/0/1/0/all/0/1\">Oleksandr Polozov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardson_M/0/1/0/all/0/1\">Matthew Richardson</a>",
          "description": "The goal of database question answering is to enable natural language\nquerying of real-life relational databases in diverse application domains.\nRecently, large-scale datasets such as Spider and WikiSQL facilitated novel\nmodeling techniques for text-to-SQL parsing, improving zero-shot generalization\nto unseen databases. In this work, we examine the challenges that still prevent\nthese techniques from practical deployment. First, we present KaggleDBQA, a new\ncross-domain evaluation dataset of real Web databases, with domain-specific\ndata types, original formatting, and unrestricted questions. Second, we\nre-examine the choice of evaluation tasks for text-to-SQL parsers as applied in\nreal-life settings. Finally, we augment our in-domain evaluation task with\ndatabase documentation, a naturally occurring source of implicit domain\nknowledge. We show that KaggleDBQA presents a challenge to state-of-the-art\nzero-shot parsers but a more realistic evaluation setting and creative use of\nassociated database documentation boosts their accuracy by over 13.2%, doubling\ntheir performance.",
          "link": "http://arxiv.org/abs/2106.11455",
          "publishedOn": "2021-06-23T01:48:38.072Z",
          "wordCount": 591,
          "title": "KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers. (arXiv:2106.11455v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11520",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weizhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>",
          "description": "A wide variety of NLP applications, such as machine translation,\nsummarization, and dialog, involve text generation. One major challenge for\nthese applications is how to evaluate whether such generated texts are actually\nfluent, accurate, or effective. In this work, we conceptualize the evaluation\nof generated text as a text generation problem, modeled using pre-trained\nsequence-to-sequence models. The general idea is that models trained to convert\nthe generated text to/from a reference output or the source text will achieve\nhigher scores when the generated text is better. We operationalize this idea\nusing BART, an encoder-decoder based pre-trained model, and propose a metric\nBARTScore with a number of variants that can be flexibly applied in an\nunsupervised fashion to evaluation of text from different perspectives (e.g.\ninformativeness, fluency, or factuality). BARTScore is conceptually simple and\nempirically effective. It can outperform existing top-scoring metrics in 16 of\n22 test settings, covering evaluation of 16 datasets (e.g., machine\ntranslation, text summarization) and 7 different perspectives (e.g.,\ninformativeness, factuality). Code to calculate BARTScore is available at\nhttps://github.com/neulab/BARTScore, and we have released an interactive\nleaderboard for meta-evaluation at\nthis http URL on the ExplainaBoard\nplatform, which allows us to interactively understand the strengths,\nweaknesses, and complementarity of each metric.",
          "link": "http://arxiv.org/abs/2106.11520",
          "publishedOn": "2021-06-23T01:48:38.063Z",
          "wordCount": 638,
          "title": "BARTScore: Evaluating Generated Text as Text Generation. (arXiv:2106.11520v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11740",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weihao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zihang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qibin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "Modern pre-trained language models are mostly built upon backbones stacking\nself-attention and feed-forward layers in an interleaved order. In this paper,\nbeyond this stereotyped layer pattern, we aim to improve pre-trained models by\nexploiting layer variety from two aspects: the layer type set and the layer\norder. Specifically, besides the original self-attention and feed-forward\nlayers, we introduce convolution into the layer type set, which is\nexperimentally found beneficial to pre-trained models. Furthermore, beyond the\noriginal interleaved order, we explore more layer orders to discover more\npowerful architectures. However, the introduced layer variety leads to a large\narchitecture space of more than billions of candidates, while training a single\ncandidate model from scratch already requires huge computation cost, making it\nnot affordable to search such a space by directly training large amounts of\ncandidate models. To solve this problem, we first pre-train a supernet from\nwhich the weights of all candidate models can be inherited, and then adopt an\nevolutionary algorithm guided by pre-training accuracy to find the optimal\narchitecture. Extensive experiments show that LV-BERT model obtained by our\nmethod outperforms BERT and its variants on various downstream tasks. For\nexample, LV-BERT-small achieves 78.8 on the GLUE testing set, 1.8 higher than\nthe strong baseline ELECTRA-small.",
          "link": "http://arxiv.org/abs/2106.11740",
          "publishedOn": "2021-06-23T01:48:38.047Z",
          "wordCount": 661,
          "title": "LV-BERT: Exploiting Layer Variety for BERT. (arXiv:2106.11740v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11759",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mitra_V/0/1/0/all/0/1\">Vikramjit Mitra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zifang Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lea_C/0/1/0/all/0/1\">Colin Lea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tooley_L/0/1/0/all/0/1\">Lauren Tooley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1\">Sarah Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Botten_D/0/1/0/all/0/1\">Darren Botten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Palekar_A/0/1/0/all/0/1\">Ashwini Palekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thelapurath_S/0/1/0/all/0/1\">Shrinath Thelapurath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Georgiou_P/0/1/0/all/0/1\">Panayiotis Georgiou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kajarekar_S/0/1/0/all/0/1\">Sachin Kajarekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bigham_J/0/1/0/all/0/1\">Jefferey Bigham</a>",
          "description": "Dysfluencies and variations in speech pronunciation can severely degrade\nspeech recognition performance, and for many individuals with\nmoderate-to-severe speech disorders, voice operated systems do not work.\nCurrent speech recognition systems are trained primarily with data from fluent\nspeakers and as a consequence do not generalize well to speech with\ndysfluencies such as sound or word repetitions, sound prolongations, or audible\nblocks. The focus of this work is on quantitative analysis of a consumer speech\nrecognition system on individuals who stutter and production-oriented\napproaches for improving performance for common voice assistant tasks (i.e.,\n\"what is the weather?\"). At baseline, this system introduces a significant\nnumber of insertion and substitution errors resulting in intended speech Word\nError Rates (isWER) that are 13.64\\% worse (absolute) for individuals with\nfluency disorders. We show that by simply tuning the decoding parameters in an\nexisting hybrid speech recognition system one can improve isWER by 24\\%\n(relative) for individuals with fluency disorders. Tuning these parameters\ntranslates to 3.6\\% better domain recognition and 1.7\\% better intent\nrecognition relative to the default setup for the 18 study participants across\nall stuttering severities.",
          "link": "http://arxiv.org/abs/2106.11759",
          "publishedOn": "2021-06-23T01:48:38.037Z",
          "wordCount": 671,
          "title": "Analysis and Tuning of a Voice Assistant System for Dysfluent Speech. (arXiv:2106.11759v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leo_J/0/1/0/all/0/1\">Justin Leo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1\">Jugal Kalita</a>",
          "description": "Most modern neural networks for classification fail to take into account the\nconcept of the unknown. Trained neural networks are usually tested in an\nunrealistic scenario with only examples from a closed set of known classes. In\nan attempt to develop a more realistic model, the concept of working in an open\nset environment has been introduced. This in turn leads to the concept of\nincremental learning where a model with its own architecture and initial\ntrained set of data can identify unknown classes during the testing phase and\nautonomously update itself if evidence of a new class is detected. Some\nproblems that arise in incremental learning are inefficient use of resources to\nretrain the classifier repeatedly and the decrease of classification accuracy\nas multiple classes are added over time. This process of instantiating new\nclasses is repeated as many times as necessary, accruing errors. To address\nthese problems, this paper proposes the Classification Confidence Threshold\napproach to prime neural networks for incremental learning to keep accuracies\nhigh by limiting forgetting. A lean method is also used to reduce resources\nused in the retraining of the neural network. The proposed method is based on\nthe idea that a network is able to incrementally learn a new class even when\nexposed to a limited number samples associated with the new class. This method\ncan be applied to most existing neural networks with minimal changes to network\narchitecture.",
          "link": "http://arxiv.org/abs/2106.11437",
          "publishedOn": "2021-06-23T01:48:38.013Z",
          "wordCount": 683,
          "title": "Incremental Deep Neural Network Learning using Classification Confidence Thresholding. (arXiv:2106.11437v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>",
          "description": "Cross-lingual adaptation with multilingual pre-trained language models\n(mPTLMs) mainly consists of two lines of works: zero-shot approach and\ntranslation-based approach, which have been studied extensively on the\nsequence-level tasks. We further verify the efficacy of these cross-lingual\nadaptation approaches by evaluating their performances on more fine-grained\nsequence tagging tasks. After re-examining their strengths and drawbacks, we\npropose a novel framework to consolidate the zero-shot approach and the\ntranslation-based approach for better adaptation performance. Instead of simply\naugmenting the source data with the machine-translated data, we tailor-make a\nwarm-up mechanism to quickly update the mPTLMs with the gradients estimated on\na few translated data. Then, the adaptation approach is applied to the refined\nparameters and the cross-lingual transfer is performed in a warm-start way. The\nexperimental results on nine target languages demonstrate that our method is\nbeneficial to the cross-lingual adaptation of various sequence tagging tasks.",
          "link": "http://arxiv.org/abs/2010.12405",
          "publishedOn": "2021-06-23T01:48:38.000Z",
          "wordCount": 617,
          "title": "Unsupervised Cross-lingual Adaptation for Sequence Tagging and Beyond. (arXiv:2010.12405v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>",
          "description": "This open-source book represents our attempt to make deep learning\napproachable, teaching readers the concepts, the context, and the code. The\nentire book is drafted in Jupyter notebooks, seamlessly integrating exposition\nfigures, math, and interactive examples with self-contained code. Our goal is\nto offer a resource that could (i) be freely available for everyone; (ii) offer\nsufficient technical depth to provide a starting point on the path to actually\nbecoming an applied machine learning scientist; (iii) include runnable code,\nshowing readers how to solve problems in practice; (iv) allow for rapid\nupdates, both by us and also by the community at large; (v) be complemented by\na forum for interactive discussion of technical details and to answer\nquestions.",
          "link": "http://arxiv.org/abs/2106.11342",
          "publishedOn": "2021-06-23T01:48:37.985Z",
          "wordCount": 565,
          "title": "Dive into Deep Learning. (arXiv:2106.11342v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siriwardhana_S/0/1/0/all/0/1\">Shamane Siriwardhana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weerasekera_R/0/1/0/all/0/1\">Rivindu Weerasekera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_E/0/1/0/all/0/1\">Elliott Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanayakkara_S/0/1/0/all/0/1\">Suranga Nanayakkara</a>",
          "description": "In this paper, we illustrate how to fine-tune the entire Retrieval Augment\nGeneration (RAG) architecture in an end-to-end manner. We highlighted the main\nengineering challenges that needed to be addressed to achieve this objective.\nWe also compare how end-to-end RAG architecture outperforms the original RAG\narchitecture for the task of question answering. We have open-sourced our\nimplementation in the HuggingFace Transformers library.",
          "link": "http://arxiv.org/abs/2106.11517",
          "publishedOn": "2021-06-23T01:48:37.975Z",
          "wordCount": 509,
          "title": "Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering. (arXiv:2106.11517v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mokrii_I/0/1/0/all/0/1\">Iurii Mokrii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boytsov_L/0/1/0/all/0/1\">Leonid Boytsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braslavski_P/0/1/0/all/0/1\">Pavel Braslavski</a>",
          "description": "Due to high annotation costs making the best use of existing human-created\ntraining data is an important research direction. We, therefore, carry out a\nsystematic evaluation of transferability of BERT-based neural ranking models\nacross five English datasets. Previous studies focused primarily on zero-shot\nand few-shot transfer from a large dataset to a dataset with a small number of\nqueries. In contrast, each of our collections has a substantial number of\nqueries, which enables a full-shot evaluation mode and improves reliability of\nour results. Furthermore, since source datasets licences often prohibit\ncommercial use, we compare transfer learning to training on pseudo-labels\ngenerated by a BM25 scorer. We find that training on pseudo-labels -- possibly\nwith subsequent fine-tuning using a modest number of annotated queries -- can\nproduce a competitive or better model compared to transfer learning. Yet, it is\nnecessary to improve the stability and/or effectiveness of the few-shot\ntraining, which, sometimes, can degrade performance of a pretrained model.",
          "link": "http://arxiv.org/abs/2103.03335",
          "publishedOn": "2021-06-23T01:48:37.967Z",
          "wordCount": 653,
          "title": "A Systematic Evaluation of Transfer Learning and Pseudo-labeling with BERT-based Ranking Models. (arXiv:2103.03335v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blodgett_S/0/1/0/all/0/1\">Su Lin Blodgett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waseem_Z/0/1/0/all/0/1\">Zeerak Waseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>",
          "description": "Despite inextricable ties between race and language, little work has\nconsidered race in NLP research and development. In this work, we survey 79\npapers from the ACL anthology that mention race. These papers reveal various\ntypes of race-related bias in all stages of NLP model development, highlighting\nthe need for proactive consideration of how NLP systems can uphold racial\nhierarchies. However, persistent gaps in research on race and NLP remain: race\nhas been siloed as a niche topic and remains ignored in many NLP tasks; most\nwork operationalizes race as a fixed single-dimensional variable with a\nground-truth label, which risks reinforcing differences produced by historical\nracism; and the voices of historically marginalized people are nearly absent in\nNLP literature. By identifying where and how NLP literature has and has not\nconsidered race, especially in comparison to related fields, our work calls for\ninclusion and racial justice in NLP research practices.",
          "link": "http://arxiv.org/abs/2106.11410",
          "publishedOn": "2021-06-23T01:48:37.958Z",
          "wordCount": 590,
          "title": "A Survey of Race, Racism, and Anti-Racism in NLP. (arXiv:2106.11410v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11403",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yefeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yunpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>",
          "description": "Objective: The objective of this study is to develop a deep learning pipeline\nto detect signals on dietary supplement-related adverse events (DS AEs) from\nTwitter. Material and Methods: We obtained 247,807 tweets ranging from 2012 to\n2018 that mentioned both DS and AE. We annotated biomedical entities and\nrelations on 2,000 randomly selected tweets. For the concept extraction task,\nwe compared the performance of traditional word embeddings with SVM, CRF and\nLSTM-CRF classifiers to BERT models. For the relation extraction task, we\ncompared GloVe vectors with CNN classifiers to BERT models. We chose the best\nperforming models in each task to assemble an end-to-end deep learning pipeline\nto detect DS AE signals and compared the results to the known DS AEs from a DS\nknowledge base (i.e., iDISK). Results: In both tasks, the BERT-based models\noutperformed traditional word embeddings. The best performing concept\nextraction model is the BioBERT model that can identify supplement, symptom,\nand body organ entities with F1-scores of 0.8646, 0.8497, and 0.7104,\nrespectively. The best performing relation extraction model is the BERT model\nthat can identify purpose and AE relations with F1-scores of 0.8335 and 0.7538,\nrespectively. The end-to-end pipeline was able to extract DS indication and DS\nAEs with an F1-score of 0.7459 and 0,7414, respectively. Comparing to the\niDISK, we could find both known and novel DS-AEs. Conclusion: We have\ndemonstrated the feasibility of detecting DS AE signals from Twitter with a\nBioBERT-based deep learning pipeline.",
          "link": "http://arxiv.org/abs/2106.11403",
          "publishedOn": "2021-06-23T01:48:37.932Z",
          "wordCount": 691,
          "title": "Deep Learning Models in Detection of Dietary Supplement Adverse Event Signals from Twitter. (arXiv:2106.11403v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1\">Saeed Mahloujifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1\">Huseyin A. Inan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chase_M/0/1/0/all/0/1\">Melissa Chase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_E/0/1/0/all/0/1\">Esha Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_M/0/1/0/all/0/1\">Marcello Hasegawa</a>",
          "description": "In the text processing context, most ML models are built on word embeddings.\nThese embeddings are themselves trained on some datasets, potentially\ncontaining sensitive data. In some cases this training is done independently,\nin other cases, it occurs as part of training a larger, task-specific model. In\neither case, it is of interest to consider membership inference attacks based\non the embedding layer as a way of understanding sensitive information leakage.\nBut, somewhat surprisingly, membership inference attacks on word embeddings and\ntheir effect in other natural language processing (NLP) tasks that use these\nembeddings, have remained relatively unexplored.\n\nIn this work, we show that word embeddings are vulnerable to black-box\nmembership inference attacks under realistic assumptions. Furthermore, we show\nthat this leakage persists through two other major NLP applications:\nclassification and text-generation, even when the embedding layer is not\nexposed to the attacker. We show that our MI attack achieves high attack\naccuracy against a classifier model and an LSTM-based language model. Indeed,\nour attack is a cheaper membership inference attack on text-generative models,\nwhich does not require the knowledge of the target model or any expensive\ntraining of text-generative models as shadow models.",
          "link": "http://arxiv.org/abs/2106.11384",
          "publishedOn": "2021-06-23T01:48:37.921Z",
          "wordCount": 640,
          "title": "Membership Inference on Word Embedding and Beyond. (arXiv:2106.11384v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "Neural machine translation (NMT) is sensitive to domain shift. In this paper,\nwe address this problem in an active learning setting where we can spend a\ngiven budget on translating in-domain data, and gradually fine-tune a\npre-trained out-of-domain NMT model on the newly translated data. Existing\nactive learning methods for NMT usually select sentences based on uncertainty\nscores, but these methods require costly translation of full sentences even\nwhen only one or two key phrases within the sentence are informative. To\naddress this limitation, we re-examine previous work from the phrase-based\nmachine translation (PBMT) era that selected not full sentences, but rather\nindividual phrases. However, while incorporating these phrases into PBMT\nsystems was relatively simple, it is less trivial for NMT systems, which need\nto be trained on full sequences to capture larger structural properties of\nsentences unique to the new domain. To overcome these hurdles, we propose to\nselect both full sentences and individual phrases from unlabelled data in the\nnew domain for routing to human translators. In a German-English translation\ntask, our active learning approach achieves consistent improvements over\nuncertainty-based sentence selection methods, improving up to 1.2 BLEU score\nover strong active learning baselines.",
          "link": "http://arxiv.org/abs/2106.11375",
          "publishedOn": "2021-06-23T01:48:37.904Z",
          "wordCount": 626,
          "title": "Phrase-level Active Learning for Neural Machine Translation. (arXiv:2106.11375v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gangwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jungsoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>",
          "description": "One of the main challenges in conversational question answering (CQA) is to\nresolve the conversational dependency, such as anaphora and ellipsis. However,\nexisting approaches do not explicitly train QA models on how to resolve the\ndependency, and thus these models are limited in understanding human dialogues.\nIn this paper, we propose a novel framework, ExCorD (Explicit guidance on how\nto resolve Conversational Dependency) to enhance the abilities of QA models in\ncomprehending conversational context. ExCorD first generates self-contained\nquestions that can be understood without the conversation history, then trains\na QA model with the pairs of original and self-contained questions using a\nconsistency-based regularizer. In our experiments, we demonstrate that ExCorD\nsignificantly improves the QA models' performance by up to 1.2 F1 on QuAC, and\n5.2 F1 on CANARD, while addressing the limitations of the existing approaches.",
          "link": "http://arxiv.org/abs/2106.11575",
          "publishedOn": "2021-06-23T01:48:37.891Z",
          "wordCount": 588,
          "title": "Learn to Resolve Conversational Dependency: A Consistency Training Framework for Conversational Question Answering. (arXiv:2106.11575v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.08106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lingyun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>",
          "description": "Despite pre-trained language models such as BERT have achieved appealing\nperformance in a wide range of natural language processing tasks, they are\ncomputationally expensive to be deployed in real-time applications. A typical\nmethod is to adopt knowledge distillation to compress these large pre-trained\nmodels (teacher models) to small student models. However, for a target domain\nwith scarce training data, the teacher can hardly pass useful knowledge to the\nstudent, which yields performance degradation for the student models. To tackle\nthis problem, we propose a method to learn to augment for data-scarce domain\nBERT knowledge distillation, by learning a cross-domain manipulation scheme\nthat automatically augments the target with the help of resource-rich source\ndomains. Specifically, the proposed method generates samples acquired from a\nstationary distribution near the target data and adopts a reinforced selector\nto automatically refine the augmentation strategy according to the performance\nof the student. Extensive experiments demonstrate that the proposed method\nsignificantly outperforms state-of-the-art baselines on four different tasks,\nand for the data-scarce domains, the compressed student models even perform\nbetter than the original large teacher model, with much fewer parameters (only\n${\\sim}13.3\\%$) when only a few labeled examples available.",
          "link": "http://arxiv.org/abs/2101.08106",
          "publishedOn": "2021-06-22T01:57:10.313Z",
          "wordCount": 658,
          "title": "Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation. (arXiv:2101.08106v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01933",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gaddy_D/0/1/0/all/0/1\">David Gaddy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>",
          "description": "In this paper, we present an improved model for voicing silent speech, where\naudio is synthesized from facial electromyography (EMG) signals. To give our\nmodel greater flexibility to learn its own input features, we directly use EMG\nsignals as input in the place of hand-designed features used by prior work. Our\nmodel uses convolutional layers to extract features from the signals and\nTransformer layers to propagate information across longer distances. To provide\nbetter signal for learning, we also introduce an auxiliary task of predicting\nphoneme labels in addition to predicting speech audio features. On an open\nvocabulary intelligibility evaluation, our model improves the state of the art\nfor this task by an absolute 25.8%.",
          "link": "http://arxiv.org/abs/2106.01933",
          "publishedOn": "2021-06-22T01:57:10.220Z",
          "wordCount": 578,
          "title": "An Improved Model for Voicing Silent Speech. (arXiv:2106.01933v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>",
          "description": "We investigate the problem of Chinese Grammatical Error Correction (CGEC) and\npresent a new framework named Tail-to-Tail (\\textbf{TtT}) non-autoregressive\nsequence prediction to address the deep issues hidden in CGEC. Considering that\nmost tokens are correct and can be conveyed directly from source to target, and\nthe error positions can be estimated and corrected based on the bidirectional\ncontext information, thus we employ a BERT-initialized Transformer Encoder as\nthe backbone model to conduct information modeling and conveying. Considering\nthat only relying on the same position substitution cannot handle the\nvariable-length correction cases, various operations such substitution,\ndeletion, insertion, and local paraphrasing are required jointly. Therefore, a\nConditional Random Fields (CRF) layer is stacked on the up tail to conduct\nnon-autoregressive sequence prediction by modeling the token dependencies.\nSince most tokens are correct and easily to be predicted/conveyed to the\ntarget, then the models may suffer from a severe class imbalance issue. To\nalleviate this problem, focal loss penalty strategies are integrated into the\nloss functions. Moreover, besides the typical fix-length error correction\ndatasets, we also construct a variable-length corpus to conduct experiments.\nExperimental results on standard datasets, especially on the variable-length\ndatasets, demonstrate the effectiveness of TtT in terms of sentence-level\nAccuracy, Precision, Recall, and F1-Measure on tasks of error Detection and\nCorrection.",
          "link": "http://arxiv.org/abs/2106.01609",
          "publishedOn": "2021-06-22T01:57:09.997Z",
          "wordCount": 676,
          "title": "Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction. (arXiv:2106.01609v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zeqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yongliang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>",
          "description": "Named entity recognition (NER) is a widely studied task in natural language\nprocessing. Recently, a growing number of studies have focused on the nested\nNER. The span-based methods, considering the entity recognition as a span\nclassification task, can deal with nested entities naturally. But they suffer\nfrom the huge search space and the lack of interactions between entities. To\naddress these issues, we propose a novel sequence-to-set neural network for\nnested NER. Instead of specifying candidate spans in advance, we provide a\nfixed set of learnable vectors to learn the patterns of the valuable spans. We\nutilize a non-autoregressive decoder to predict the final set of entities in\none pass, in which we are able to capture dependencies between entities.\nCompared with the sequence-to-sequence method, our model is more suitable for\nsuch unordered recognition task as it is insensitive to the label order. In\naddition, we utilize the loss function based on bipartite matching to compute\nthe overall training loss. Experimental results show that our proposed model\nachieves state-of-the-art on three nested NER corpora: ACE 2004, ACE 2005 and\nKBP 2017. The code is available at\nhttps://github.com/zqtan1024/sequence-to-set.",
          "link": "http://arxiv.org/abs/2105.08901",
          "publishedOn": "2021-06-22T01:57:09.986Z",
          "wordCount": 657,
          "title": "A Sequence-to-Set Network for Nested Named Entity Recognition. (arXiv:2105.08901v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meek_C/0/1/0/all/0/1\">Christopher Meek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polozov_O/0/1/0/all/0/1\">Oleksandr Polozov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardson_M/0/1/0/all/0/1\">Matthew Richardson</a>",
          "description": "Learning to capture text-table alignment is essential for tasks like\ntext-to-SQL. A model needs to correctly recognize natural language references\nto columns and values and to ground them in the given database schema. In this\npaper, we present a novel weakly supervised Structure-Grounded pretraining\nframework (StruG) for text-to-SQL that can effectively learn to capture\ntext-table alignment based on a parallel text-table corpus. We identify a set\nof novel prediction tasks: column grounding, value grounding and column-value\nmapping, and leverage them to pretrain a text-table encoder. Additionally, to\nevaluate different methods under more realistic text-table alignment settings,\nwe create a new evaluation set Spider-Realistic based on Spider dev set with\nexplicit mentions of column names removed, and adopt eight existing text-to-SQL\ndatasets for cross-database evaluation. STRUG brings significant improvement\nover BERT-LARGE in all settings. Compared with existing pretraining methods\nsuch as GRAPPA, STRUG achieves similar performance on Spider, and outperforms\nall baselines on more realistic sets. All the code and data used in this work\nis public available at https://aka.ms/strug.",
          "link": "http://arxiv.org/abs/2010.12773",
          "publishedOn": "2021-06-22T01:57:09.935Z",
          "wordCount": 653,
          "title": "Structure-Grounded Pretraining for Text-to-SQL. (arXiv:2010.12773v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1\">James Lee-Thorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_I/0/1/0/all/0/1\">Ilya Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Ontanon</a>",
          "description": "We show that Transformer encoder architectures can be massively sped up, with\nlimited accuracy costs, by replacing the self-attention sublayers with simple\nlinear transformations that \"mix\" input tokens. These linear transformations,\nalong with standard nonlinearities in feed-forward layers, prove competent at\nmodeling semantic relationships in several text classification tasks. Most\nsurprisingly, we find that replacing the self-attention sublayer in a\nTransformer encoder with a standard, unparameterized Fourier Transform achieves\n92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains\nnearly seven times faster on GPUs and twice as fast on TPUs. The resulting\nmodel, FNet, also scales very efficiently to long inputs. Specifically, when\ncompared to the \"efficient\" Transformers on the Long Range Arena benchmark,\nFNet matches the accuracy of the most accurate models, but is faster than the\nfastest models across all sequence lengths on GPUs (and across relatively\nshorter lengths on TPUs). Finally, FNet has a light memory footprint and is\nparticularly efficient at smaller model sizes: for a fixed speed and accuracy\nbudget, small FNet models outperform Transformer counterparts.",
          "link": "http://arxiv.org/abs/2105.03824",
          "publishedOn": "2021-06-22T01:57:09.793Z",
          "wordCount": 639,
          "title": "FNet: Mixing Tokens with Fourier Transforms. (arXiv:2105.03824v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01065",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yujian Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiuping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodward_J/0/1/0/all/0/1\">John R. Woodward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jinxia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Pengsheng Huang</a>",
          "description": "Recently, there has been significant progress in studying neural networks to\ntranslate text descriptions into SQL queries. Despite achieving good\nperformance on some public benchmarks, existing text-to-SQL models typically\nrely on the lexical matching between words in natural language (NL) questions\nand tokens in table schemas, which may render the models vulnerable to attacks\nthat break the schema linking mechanism. In this work, we investigate the\nrobustness of text-to-SQL models to synonym substitution. In particular, we\nintroduce Spider-Syn, a human-curated dataset based on the Spider benchmark for\ntext-to-SQL translation. NL questions in Spider-Syn are modified from Spider,\nby replacing their schema-related words with manually selected synonyms that\nreflect real-world question paraphrases. We observe that the accuracy\ndramatically drops by eliminating such explicit correspondence between NL\nquestions and table schemas, even if the synonyms are not adversarially\nselected to conduct worst-case adversarial attacks. Finally, we present two\ncategories of approaches to improve the model robustness. The first category of\napproaches utilizes additional synonym annotations for table schemas by\nmodifying the model input, while the second category is based on adversarial\ntraining. We demonstrate that both categories of approaches significantly\noutperform their counterparts without the defense, and the first category of\napproaches are more effective.",
          "link": "http://arxiv.org/abs/2106.01065",
          "publishedOn": "2021-06-22T01:57:09.754Z",
          "wordCount": 665,
          "title": "Towards Robustness of Text-to-SQL Models against Synonym Substitution. (arXiv:2106.01065v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaobin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>",
          "description": "Recently, considerable literature has grown up around the theme of few-shot\nnamed entity recognition (NER), but little published benchmark data\nspecifically focused on the practical and challenging task. Current approaches\ncollect existing supervised NER datasets and re-organize them to the few-shot\nsetting for empirical study. These strategies conventionally aim to recognize\ncoarse-grained entity types with few examples, while in practice, most unseen\nentity types are fine-grained. In this paper, we present Few-NERD, a\nlarge-scale human-annotated few-shot NER dataset with a hierarchy of 8\ncoarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238\nsentences from Wikipedia, 4,601,160 words are included and each is annotated as\ncontext or a part of a two-level entity type. To the best of our knowledge,\nthis is the first few-shot NER dataset and the largest human-crafted NER\ndataset. We construct benchmark tasks with different emphases to\ncomprehensively assess the generalization capability of models. Extensive\nempirical results and analysis show that Few-NERD is challenging and the\nproblem requires further research. We make Few-NERD public at\nhttps://ningding97.github.io/fewnerd/.",
          "link": "http://arxiv.org/abs/2105.07464",
          "publishedOn": "2021-06-22T01:57:09.712Z",
          "wordCount": 683,
          "title": "Few-NERD: A Few-Shot Named Entity Recognition Dataset. (arXiv:2105.07464v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tjandra_A/0/1/0/all/0/1\">Andros Tjandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Ruoming Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karita_S/0/1/0/all/0/1\">Shigeki Karita</a>",
          "description": "We present an approach for unsupervised learning of speech representation\ndisentangling contents and styles. Our model consists of: (1) a local encoder\nthat captures per-frame information; (2) a global encoder that captures\nper-utterance information; and (3) a conditional decoder that reconstructs\nspeech given local and global latent variables. Our experiments show that (1)\nthe local latent variables encode speech contents, as reconstructed speech can\nbe recognized by ASR with low word error rates (WER), even with a different\nglobal encoding; (2) the global latent variables encode speaker style, as\nreconstructed speech shares speaker identity with the source utterance of the\nglobal encoding. Additionally, we demonstrate an useful application from our\npre-trained model, where we can train a speaker recognition model from the\nglobal latent variables and achieve high accuracy by fine-tuning with as few\ndata as one label per speaker.",
          "link": "http://arxiv.org/abs/2010.12973",
          "publishedOn": "2021-06-22T01:57:09.674Z",
          "wordCount": 618,
          "title": "Unsupervised Learning of Disentangled Speech Content and Style Representation. (arXiv:2010.12973v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_B/0/1/0/all/0/1\">Bowen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor W. Tsang</a>",
          "description": "Aspect-based sentiment analysis (ABSA) aims to predict the sentiment\nexpressed in a review with respect to a given aspect. The core of ABSA is to\nmodel the interaction between the context and given aspect to extract the\naspect-related information. In prior work, attention mechanisms and dependency\ngraph networks are commonly adopted to capture the relations between the\ncontext and given aspect. And the weighted sum of context hidden states is used\nas the final representation fed to the classifier. However, the information\nrelated to the given aspect may be already discarded and adverse information\nmay be retained in the context modeling processes of existing models. This\nproblem cannot be solved by subsequent modules and there are two reasons:\nfirst, their operations are conducted on the encoder-generated context hidden\nstates, whose value cannot change after the encoder; second, existing encoders\nonly consider the context while not the given aspect. To address this problem,\nwe argue the given aspect should be considered as a new clue out of context in\nthe context modeling process. As for solutions, we design several aspect-aware\ncontext encoders based on different backbones: an aspect-aware LSTM and three\naspect-aware BERTs. They are dedicated to generate aspect-aware hidden states\nwhich are tailored for ABSA task. In these aspect-aware context encoders, the\nsemantics of the given aspect is used to regulate the information flow.\nConsequently, the aspect-related information can be retained and\naspect-irrelevant information can be excluded in the generated hidden states.\nWe conduct extensive experiments on several benchmark datasets with empirical\nanalysis, demonstrating the efficacies and advantages of our proposed\naspect-aware context encoders.",
          "link": "http://arxiv.org/abs/2106.10816",
          "publishedOn": "2021-06-22T01:57:09.653Z",
          "wordCount": 707,
          "title": "Out of Context: A New Clue for Context Modeling of Aspect-based Sentiment Analysis. (arXiv:2106.10816v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.11384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pergola_G/0/1/0/all/0/1\">Gabriele Pergola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Lin Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>",
          "description": "The flexibility of the inference process in Variational Autoencoders (VAEs)\nhas recently led to revising traditional probabilistic topic models giving rise\nto Neural Topic Models (NTMs). Although these approaches have achieved\nsignificant results, surprisingly very little work has been done on how to\ndisentangle the latent topics. Existing topic models when applied to reviews\nmay extract topics associated with writers' subjective opinions mixed with\nthose related to factual descriptions such as plot summaries in movie and book\nreviews. It is thus desirable to automatically separate opinion topics from\nplot/neutral ones enabling a better interpretability. In this paper, we propose\na neural topic model combined with adversarial training to disentangle opinion\ntopics from plot and neutral ones. We conduct an extensive experimental\nassessment introducing a new collection of movie and book reviews paired with\ntheir plots, namely MOBO dataset, showing an improved coherence and variety of\ntopics, a consistent disentanglement rate, and sentiment classification\nperformance superior to other supervised topic models.",
          "link": "http://arxiv.org/abs/2010.11384",
          "publishedOn": "2021-06-22T01:57:09.643Z",
          "wordCount": 653,
          "title": "A Disentangled Adversarial Neural Topic Model for Separating Opinions from Plots in User Reviews. (arXiv:2010.11384v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Palma_I/0/1/0/all/0/1\">Ignacio Tampe Palma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendoza_M/0/1/0/all/0/1\">Marcelo Mendoza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milios_E/0/1/0/all/0/1\">Evangelos Milios</a>",
          "description": "Summarization has usually relied on gold standard summaries to train\nextractive or abstractive models. Social media brings a hurdle to summarization\ntechniques since it requires addressing a multi-document multi-author approach.\nWe address this challenging task by introducing a novel method that generates\nabstractive summaries of online news discussions. Our method extends a\nBERT-based architecture, including an attention encoding that fed comments'\nlikes during the training stage. To train our model, we define a task which\nconsists of reconstructing high impact comments based on popularity (likes).\nAccordingly, our model learns to summarize online discussions based on their\nmost relevant comments. Our novel approach provides a summary that represents\nthe most relevant aspects of a news item that users comment on, incorporating\nthe social context as a source of information to summarize texts in online\nsocial networks. Our model is evaluated using ROUGE scores between the\ngenerated summary and each comment on the thread. Our model, including the\nsocial attention encoding, significantly outperforms both extractive and\nabstractive summarization methods based on such evaluation.",
          "link": "http://arxiv.org/abs/2106.03953",
          "publishedOn": "2021-06-22T01:57:09.617Z",
          "wordCount": 623,
          "title": "Neural Abstractive Unsupervised Summarization of Online News Discussions. (arXiv:2106.03953v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramnath_K/0/1/0/all/0/1\">Kiran Ramnath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1\">Mark Hasegawa-Johnson</a>",
          "description": "Fact-based Visual Question Answering (FVQA), a challenging variant of VQA,\nrequires a QA-system to include facts from a diverse knowledge graph (KG) in\nits reasoning process to produce an answer. Large KGs, especially common-sense\nKGs, are known to be incomplete, i.e., not all non-existent facts are always\nincorrect. Therefore, being able to reason over incomplete KGs for QA is a\ncritical requirement in real-world applications that has not been addressed\nextensively in the literature. We develop a novel QA architecture that allows\nus to reason over incomplete KGs, something current FVQA state-of-the-art\n(SOTA) approaches lack due to their critical reliance on fact retrieval. We use\nKG Embeddings, a technique widely used for KG completion, for the downstream\ntask of FVQA. We also employ a new image representation technique we call\n'Image-as-Knowledge' to enable this capability, alongside a simple one-step\nCoAttention mechanism to attend to text and image during QA. Our FVQA\narchitecture is faster during inference time, being O(m), as opposed to\nexisting FVQA SOTA methods which are O(N log N), where m = number of vertices,\nN = number of edges = O(m^2). KG embeddings are shown to hold complementary\ninformation to word embeddings: a combination of both metrics permits\nperformance comparable to SOTA methods in the standard answer retrieval task,\nand significantly better (26% absolute) in the proposed missing-edge reasoning\ntask.",
          "link": "http://arxiv.org/abs/2012.15484",
          "publishedOn": "2021-06-22T01:57:09.568Z",
          "wordCount": 689,
          "title": "Seeing is Knowing! Fact-based Visual Question Answering using Knowledge Graph Embeddings. (arXiv:2012.15484v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09624",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Park_T/0/1/0/all/0/1\">Tae Jin Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dimitriadis_D/0/1/0/all/0/1\">Dimitrios Dimitriadis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_K/0/1/0/all/0/1\">Kyu J. Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>",
          "description": "Speaker diarization is a task to label audio or video recordings with classes\nthat correspond to speaker identity, or in short, a task to identify \"who spoke\nwhen\". In the early years, speaker diarization algorithms were developed for\nspeech recognition on multispeaker audio recordings to enable speaker adaptive\nprocessing. These algorithms also gained their own value as a standalone\napplication over time to provide speaker-specific metainformation for\ndownstream tasks such as audio retrieval. More recently, with the emergence of\ndeep learning technology, which has driven revolutionary changes in research\nand practices across speech application domains, rapid advancements have been\nmade for speaker diarization. In this paper, we review not only the historical\ndevelopment of speaker diarization technology but also the recent advancements\nin neural speaker diarization approaches. Furthermore, we discuss how speaker\ndiarization systems have been integrated with speech recognition applications\nand how the recent surge of deep learning is leading the way of jointly\nmodeling these two components to be complementary to each other. By considering\nsuch exciting technical trends, we believe that this paper is a valuable\ncontribution to the community to provide a survey work by consolidating the\nrecent developments with neural methods and thus facilitating further progress\ntoward a more efficient speaker diarization.",
          "link": "http://arxiv.org/abs/2101.09624",
          "publishedOn": "2021-06-22T01:57:09.562Z",
          "wordCount": 680,
          "title": "A Review of Speaker Diarization: Recent Advances with Deep Learning. (arXiv:2101.09624v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haochen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jian Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>",
          "description": "With recent advances in distantly supervised (DS) relation extraction (RE),\nconsiderable attention is attracted to leverage multi-instance learning (MIL)\nto distill high-quality supervision from the noisy DS. Here, we go beyond label\nnoise and identify the key bottleneck of DS-MIL to be its low data utilization:\nas high-quality supervision being refined by MIL, MIL abandons a large amount\nof training instances, which leads to a low data utilization and hinders model\ntraining from having abundant supervision. In this paper, we propose\ncollaborative adversarial training to improve the data utilization, which\ncoordinates virtual adversarial training (VAT) and adversarial training (AT) at\ndifferent levels. Specifically, since VAT is label-free, we employ the\ninstance-level VAT to recycle instances abandoned by MIL. Besides, we deploy AT\nat the bag-level to unleash the full potential of the high-quality supervision\ngot by MIL. Our proposed method brings consistent improvements (~ 5 absolute\nAUC score) to the previous state of the art, which verifies the importance of\nthe data utilization issue and the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2106.10835",
          "publishedOn": "2021-06-22T01:57:09.547Z",
          "wordCount": 615,
          "title": "Empower Distantly Supervised Relation Extraction with Collaborative Adversarial Training. (arXiv:2106.10835v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1910.09796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>",
          "description": "Fact Verification requires fine-grained natural language inference capability\nthat finds subtle clues to identify the syntactical and semantically correct\nbut not well-supported claims. This paper presents Kernel Graph Attention\nNetwork (KGAT), which conducts more fine-grained fact verification with\nkernel-based attentions. Given a claim and a set of potential evidence\nsentences that form an evidence graph, KGAT introduces node kernels, which\nbetter measure the importance of the evidence node, and edge kernels, which\nconduct fine-grained evidence propagation in the graph, into Graph Attention\nNetworks for more accurate fact verification. KGAT achieves a 70.38% FEVER\nscore and significantly outperforms existing fact verification models on FEVER,\na large-scale benchmark for fact verification. Our analyses illustrate that,\ncompared to dot-product attentions, the kernel-based attention concentrates\nmore on relevant evidence sentences and meaningful clues in the evidence graph,\nwhich is the main source of KGAT's effectiveness.",
          "link": "http://arxiv.org/abs/1910.09796",
          "publishedOn": "2021-06-22T01:57:09.534Z",
          "wordCount": 629,
          "title": "Fine-grained Fact Verification with Kernel Graph Attention Network. (arXiv:1910.09796v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03287",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neely_M/0/1/0/all/0/1\">Michael Neely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schouten_S/0/1/0/all/0/1\">Stefan F. Schouten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bleeker_M/0/1/0/all/0/1\">Maurits J. R. Bleeker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_A/0/1/0/all/0/1\">Ana Lucic</a>",
          "description": "By computing the rank correlation between attention weights and\nfeature-additive explanation methods, previous analyses either invalidate or\nsupport the role of attention-based explanations as a faithful and plausible\nmeasure of salience. To investigate whether this approach is appropriate, we\ncompare LIME, Integrated Gradients, DeepLIFT, Grad-SHAP, Deep-SHAP, and\nattention-based explanations, applied to two neural architectures trained on\nsingle- and pair-sequence language tasks. In most cases, we find that none of\nour chosen methods agree. Based on our empirical observations and theoretical\nobjections, we conclude that rank correlation does not measure the quality of\nfeature-additive methods. Practitioners should instead use the numerous and\nrigorous diagnostic methods proposed by the community.",
          "link": "http://arxiv.org/abs/2105.03287",
          "publishedOn": "2021-06-22T01:57:09.524Z",
          "wordCount": 595,
          "title": "Order in the Court: Explainable AI Methods Prone to Disagreement. (arXiv:2105.03287v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenghao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yudong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>",
          "description": "Social media has become a valuable resource for the study of suicidal\nideation and the assessment of suicide risk. Among social media platforms,\nReddit has emerged as the most promising one due to its anonymity and its focus\non topic-based communities (subreddits) that can be indicative of someone's\nstate of mind or interest regarding mental health disorders such as\nr/SuicideWatch, r/Anxiety, r/depression. A challenge for previous work on\nsuicide risk assessment has been the small amount of labeled data. We propose\nan empirical investigation into several classes of weakly-supervised\napproaches, and show that using pseudo-labeling based on related issues around\nmental health (e.g., anxiety, depression) helps improve model performance for\nsuicide risk assessment.",
          "link": "http://arxiv.org/abs/2106.02792",
          "publishedOn": "2021-06-22T01:57:09.518Z",
          "wordCount": 577,
          "title": "Weakly-Supervised Methods for Suicide Risk Assessment: Role of Related Domains. (arXiv:2106.02792v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.08937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frantz_C/0/1/0/all/0/1\">Christopher K. Frantz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddiki_S/0/1/0/all/0/1\">Saba N. Siddiki</a>",
          "description": "The Grammar of Institutions, or Institutional Grammar, is an established\napproach to encode policy information in terms of institutional statements\nbased on a set of pre-defined syntactic components. This codebook provides\ncoding guidelines for a revised version of the Institutional Grammar, the\nInstitutional Grammar 2.0 (IG 2.0). IG 2.0 is a specification that aims at\nfacilitating the encoding of policy to meet varying analytical objectives. To\nthis end, it revises the grammar with respect to comprehensiveness,\nflexibility, and specificity by offering multiple levels of expressiveness (IG\nCore, IG Extended, IG Logico). In addition to the encoding of regulative\nstatements, it further introduces the encoding of constitutive institutional\nstatements, as well as statements that exhibit both constitutive and regulative\ncharacteristics. Introducing those aspects, the codebook initially covers\nfundamental concepts of IG 2.0, before providing an overview of pre-coding\nsteps relevant for document preparation. Detailed coding guidelines are\nprovided for both regulative and constitutive statements across all levels of\nexpressiveness, along with the encoding guidelines for statements of mixed form\n-- hybrid and polymorphic institutional statements. The document further\nprovides an overview of taxonomies used in the encoding process and referred to\nthroughout the codebook. The codebook concludes with a summary and discussion\nof relevant considerations to facilitate the coding process. An initial\nReader's Guide helps the reader tailor the content to her interest.\n\nNote that this codebook specifically focuses on operational aspects of IG 2.0\nin the context of policy coding. Links to additional resources such as the\nunderlying scientific literature (that offers a comprehensive treatment of the\nunderlying theoretical concepts) are referred to in the concluding section of\nthe codebook.",
          "link": "http://arxiv.org/abs/2008.08937",
          "publishedOn": "2021-06-22T01:57:09.503Z",
          "wordCount": 756,
          "title": "Institutional Grammar 2.0 Codebook. (arXiv:2008.08937v3 [cs.MA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.09112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bjerva_J/0/1/0/all/0/1\">Johannes Bjerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>",
          "description": "Cross-lingual representations have the potential to make NLP techniques\navailable to the vast majority of languages in the world. However, they\ncurrently require large pretraining corpora or access to typologically similar\nlanguages. In this work, we address these obstacles by removing language\nidentity signals from multilingual embeddings. We examine three approaches for\nthis: (i) re-aligning the vector spaces of target languages (all together) to a\npivot source language; (ii) removing language-specific means and variances,\nwhich yields better discriminativeness of embeddings as a by-product; and (iii)\nincreasing input similarity across languages by removing morphological\ncontractions and sentence reordering. We evaluate on XNLI and reference-free MT\nacross 19 typologically diverse languages. Our findings expose the limitations\nof these approaches -- unlike vector normalization, vector space re-alignment\nand text normalization do not achieve consistent gains across encoders and\nlanguages. Due to the approaches' additive effects, their combination decreases\nthe cross-lingual transfer gap by 8.9 points (m-BERT) and 18.2 points (XLM-R)\non average across all tasks and languages, however. Our code and models are\npublicly available.",
          "link": "http://arxiv.org/abs/2008.09112",
          "publishedOn": "2021-06-22T01:57:09.486Z",
          "wordCount": 629,
          "title": "Inducing Language-Agnostic Multilingual Representations. (arXiv:2008.09112v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zejiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lucy Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>",
          "description": "Classifying the core textual components of a scientific paper-title, author,\nbody text, etc.-is a critical first step in automated scientific document\nunderstanding. Previous work has shown how using elementary layout information,\ni.e., each token's 2D position on the page, leads to more accurate\nclassification. We introduce new methods for incorporating VIsual LAyout (VILA)\nstructures, e.g., the grouping of page texts into text lines or text blocks,\ninto language models to further improve performance. We show that the I-VILA\napproach, which simply adds special tokens denoting the boundaries of layout\nstructures into model inputs, can lead to 1.9% Macro F1 improvements for token\nclassification. Moreover, we design a hierarchical model, H-VILA, that encodes\nthe text based on layout structures and record an up-to 47% inference time\nreduction with less than 1.5% Macro F1 loss for the text classification models.\nExperiments are conducted on a newly curated evaluation suite, S2-VLUE, with a\nnovel metric measuring classification uniformity within visual groups and a new\ndataset of gold annotations covering papers from 19 scientific disciplines.\nPre-trained weights, benchmark datasets, and source code will be available at\nhttps://github.com/allenai/VILA.",
          "link": "http://arxiv.org/abs/2106.00676",
          "publishedOn": "2021-06-22T01:57:09.481Z",
          "wordCount": 653,
          "title": "Incorporating Visual Layout Structures for Scientific Text Classification. (arXiv:2106.00676v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>",
          "description": "Assessing an AI agent that can converse in human language and understand\nvisual content is challenging. Generation metrics, such as BLEU scores favor\ncorrect syntax over semantics. Hence a discriminative approach is often used,\nwhere an agent ranks a set of candidate options. The mean reciprocal rank (MRR)\nmetric evaluates the model performance by taking into account the rank of a\nsingle human-derived answer. This approach, however, raises a new challenge:\nthe ambiguity and synonymy of answers, for instance, semantic equivalence\n(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative\ngain (NDCG) metric has been used to capture the relevance of all the correct\nanswers via dense annotations. However, the NDCG metric favors the usually\napplicable uncertain answers such as `I don't know. Crafting a model that\nexcels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should\nanswer a human-like reply and validate the correctness of any answer. To\naddress this issue, we describe a two-step non-parametric ranking approach that\ncan merge strong MRR and NDCG models. Using our approach, we manage to keep\nmost MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG\nstate-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won\nthe recent Visual Dialog 2020 challenge. Source code is available at\nhttps://github.com/idansc/mrr-ndcg.",
          "link": "http://arxiv.org/abs/2104.07511",
          "publishedOn": "2021-06-22T01:57:09.472Z",
          "wordCount": 688,
          "title": "Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1\">Prasanna Parthasarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelsalam_M/0/1/0/all/0/1\">Mohamed Abdelsalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>",
          "description": "Neural models trained for next utterance generation in dialogue task learn to\nmimic the n-gram sequences in the training set with training objectives like\nnegative log-likelihood (NLL) or cross-entropy. Such commonly used training\nobjectives do not foster generating alternate responses to a context. But, the\neffects of minimizing an alternate training objective that fosters a model to\ngenerate alternate response and score it on semantic similarity has not been\nwell studied. We hypothesize that a language generation model can improve on\nits diversity by learning to generate alternate text during training and\nminimizing a semantic loss as an auxiliary objective. We explore this idea on\ntwo different sized data sets on the task of next utterance generation in goal\noriented dialogues. We make two observations (1) minimizing a semantic\nobjective improved diversity in responses in the smaller data set (Frames) but\nonly as-good-as minimizing the NLL in the larger data set (MultiWoZ) (2) large\nlanguage model embeddings can be more useful as a semantic loss objective than\nas initialization for token embeddings.",
          "link": "http://arxiv.org/abs/2106.10619",
          "publishedOn": "2021-06-22T01:57:09.465Z",
          "wordCount": 622,
          "title": "A Brief Study on the Effects of Training Generative Dialogue Models with a Semantic loss. (arXiv:2106.10619v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.01051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_P/0/1/0/all/0/1\">Po-Han Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Cheng-I Jeff Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yist Y. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Andy T. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guan-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tzu-Hsien Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_W/0/1/0/all/0/1\">Wei-Cheng Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Ko-tik Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Da-Rong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zili Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shuyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>",
          "description": "Self-supervised learning (SSL) has proven vital for advancing research in\nnatural language processing (NLP) and computer vision (CV). The paradigm\npretrains a shared model on large volumes of unlabeled data and achieves\nstate-of-the-art (SOTA) for various tasks with minimal adaptation. However, the\nspeech processing community lacks a similar setup to systematically explore the\nparadigm. To bridge this gap, we introduce Speech processing Universal\nPERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the\nperformance of a shared model across a wide range of speech processing tasks\nwith minimal architecture changes and labeled data. Among multiple usages of\nthe shared model, we especially focus on extracting the representation learned\nfrom SSL due to its preferable re-usability. We present a simple framework to\nsolve SUPERB tasks by learning task-specialized lightweight prediction heads on\ntop of the frozen shared model. Our results demonstrate that the framework is\npromising as SSL representations show competitive generalizability and\naccessibility across SUPERB tasks. We release SUPERB as a challenge with a\nleaderboard and a benchmark toolkit to fuel the research in representation\nlearning and general speech processing.",
          "link": "http://arxiv.org/abs/2105.01051",
          "publishedOn": "2021-06-22T01:57:09.459Z",
          "wordCount": 691,
          "title": "SUPERB: Speech processing Universal PERformance Benchmark. (arXiv:2105.01051v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.01663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ilahi_I/0/1/0/all/0/1\">Inaam Ilahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zia_H/0/1/0/all/0/1\">Hafiz Muhammad Abdullah Zia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahsan_M/0/1/0/all/0/1\">Muhammad Ahtazaz Ahsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabassam_R/0/1/0/all/0/1\">Rauf Tabassam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Armaghan Ahmed</a>",
          "description": "Recent advancements in deep learning have created many opportunities to solve\nreal-world problems that remained unsolved for more than a decade. Automatic\ncaption generation is a major research field, and the research community has\ndone a lot of work on it in most common languages like English. Urdu is the\nnational language of Pakistan and also much spoken and understood in the\nsub-continent region of Pakistan-India, and yet no work has been done for Urdu\nlanguage caption generation. Our research aims to fill this gap by developing\nan attention-based deep learning model using techniques of sequence modeling\nspecialized for the Urdu language. We have prepared a dataset in the Urdu\nlanguage by translating a subset of the \"Flickr8k\" dataset containing 700 'man'\nimages. We evaluate our proposed technique on this dataset and show that it can\nachieve a BLEU score of 0.83 in the Urdu language. We improve on the previous\nstate-of-the-art by using better CNN architectures and optimization techniques.\nFurthermore, we provide a discussion on how the generated captions can be made\ncorrect grammar-wise.",
          "link": "http://arxiv.org/abs/2008.01663",
          "publishedOn": "2021-06-22T01:57:09.443Z",
          "wordCount": 677,
          "title": "Efficient Urdu Caption Generation using Attention based LSTM. (arXiv:2008.01663v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.11213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Casanova_E/0/1/0/all/0/1\">Edresson Casanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1\">Arnaldo Candido Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shulby_C/0/1/0/all/0/1\">Christopher Shulby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_F/0/1/0/all/0/1\">Frederico Santos de Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gris_L/0/1/0/all/0/1\">Lucas Rafael Stefanel Gris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_H/0/1/0/all/0/1\">Hamilton Pereira da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aluisio_S/0/1/0/all/0/1\">Sandra Maria Aluisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_M/0/1/0/all/0/1\">Moacir Antonelli Ponti</a>",
          "description": "In this paper we present an efficient method for training models for speaker\nrecognition using small or under-resourced datasets. This method requires less\ndata than other SOTA (State-Of-The-Art) methods, e.g. the Angular Prototypical\nand GE2E loss functions, while achieving similar results to those methods. This\nis done using the knowledge of the reconstruction of a phoneme in the speaker's\nvoice. For this purpose, a new dataset was built, composed of 40 male speakers,\nwho read sentences in Portuguese, totaling approximately 3h. We compare the\nthree best architectures trained using our method to select the best one, which\nis the one with a shallow architecture. Then, we compared this model with the\nSOTA method for the speaker recognition task: the Fast ResNet-34 trained with\napproximately 2,000 hours, using the loss functions Angular Prototypical and\nGE2E. Three experiments were carried out with datasets in different languages.\nAmong these three experiments, our model achieved the second best result in two\nexperiments and the best result in one of them. This highlights the importance\nof our method, which proved to be a great competitor to SOTA speaker\nrecognition models, with 500x less data and a simpler approach.",
          "link": "http://arxiv.org/abs/2002.11213",
          "publishedOn": "2021-06-22T01:57:09.436Z",
          "wordCount": 690,
          "title": "Speech2Phone: A Novel and Efficient Method for Training Speaker Recognition Models. (arXiv:2002.11213v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1\">Mohit Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pailla_D/0/1/0/all/0/1\">Dheeraj Pailla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_H/0/1/0/all/0/1\">Himanshu Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchawala_A/0/1/0/all/0/1\">Aadilmehdi Sanchawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1\">Manish Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1\">Ponnurangam Kumaraguru</a>",
          "description": "The exponential rise of online social media has enabled the creation,\ndistribution, and consumption of information at an unprecedented rate. However,\nit has also led to the burgeoning of various forms of online abuse. Increasing\ncases of online antisemitism have become one of the major concerns because of\nits socio-political consequences. Unlike other major forms of online abuse like\nracism, sexism, etc., online antisemitism has not been studied much from a\nmachine learning perspective. To the best of our knowledge, we present the\nfirst work in the direction of automated multimodal detection of online\nantisemitism. The task poses multiple challenges that include extracting\nsignals across multiple modalities, contextual references, and handling\nmultiple aspects of antisemitism. Unfortunately, there does not exist any\npublicly available benchmark corpus for this critical task. Hence, we collect\nand label two datasets with 3,102 and 3,509 social media posts from Twitter and\nGab respectively. Further, we present a multimodal deep learning system that\ndetects the presence of antisemitic content and its specific antisemitism\ncategory using text and images from posts. We perform an extensive set of\nexperiments on the two datasets to evaluate the efficacy of the proposed\nsystem. Finally, we also present a qualitative analysis of our study.",
          "link": "http://arxiv.org/abs/2104.05947",
          "publishedOn": "2021-06-22T01:57:09.429Z",
          "wordCount": 683,
          "title": "\"Subverting the Jewtocracy\": Online Antisemitism Detection Using Multimodal Deep Learning. (arXiv:2104.05947v3 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pruksachatkun_Y/0/1/0/all/0/1\">Yada Pruksachatkun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_S/0/1/0/all/0/1\">Satyapriya Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamala_J/0/1/0/all/0/1\">Jwala Dhamala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>",
          "description": "Existing bias mitigation methods to reduce disparities in model outcomes\nacross cohorts have focused on data augmentation, debiasing model embeddings,\nor adding fairness-based optimization objectives during training. Separately,\ncertified word substitution robustness methods have been developed to decrease\nthe impact of spurious features and synonym substitutions on model predictions.\nWhile their end goals are different, they both aim to encourage models to make\nthe same prediction for certain changes in the input. In this paper, we\ninvestigate the utility of certified word substitution robustness methods to\nimprove equality of odds and equality of opportunity on multiple text\nclassification tasks. We observe that certified robustness methods improve\nfairness, and using both robustness and bias mitigation methods in training\nresults in an improvement in both fronts",
          "link": "http://arxiv.org/abs/2106.10826",
          "publishedOn": "2021-06-22T01:57:09.398Z",
          "wordCount": 576,
          "title": "Does Robustness Improve Fairness? Approaching Fairness with Word Substitution Robustness Methods for Text Classification. (arXiv:2106.10826v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.09954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Runzhe Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>",
          "description": "In this paper, we explore the ability to model and infer personality types of\nopponents, predict their responses, and use this information to adapt a dialog\nagent's high-level strategy in negotiation tasks. Inspired by the idea of\nincorporating a theory of mind (ToM) into machines, we introduce a\nprobabilistic formulation to encapsulate the opponent's personality type during\nboth learning and inference. We test our approach on the CraigslistBargain\ndataset and show that our method using ToM inference achieves a 20% higher\ndialog agreement rate compared to baselines on a mixed population of opponents.\nWe also find that our model displays diverse negotiation behavior with\ndifferent types of opponents.",
          "link": "http://arxiv.org/abs/2010.09954",
          "publishedOn": "2021-06-22T01:57:09.393Z",
          "wordCount": 580,
          "title": "Improving Dialog Systems for Negotiation with Personality Modeling. (arXiv:2010.09954v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.05092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>",
          "description": "Dialogue relation extraction (DRE) aims to detect the relation between two\nentities mentioned in a multi-party dialogue. It plays an important role in\nconstructing knowledge graphs from conversational data increasingly abundant on\nthe internet and facilitating intelligent dialogue system development. The\nprior methods of DRE do not meaningfully leverage speaker information-they just\nprepend the utterances with the respective speaker names. Thus, they fail to\nmodel the crucial inter-speaker relations that may give additional context to\nrelevant argument entities through pronouns and triggers. We, however, present\na graph attention network-based method for DRE where a graph, that contains\nmeaningfully connected speaker, entity, entity-type, and utterance nodes, is\nconstructed. This graph is fed to a graph attention network for context\npropagation among relevant nodes, which effectively captures the dialogue\ncontext. We empirically show that this graph-based approach quite effectively\ncaptures the relations between different entity pairs in a dialogue as it\noutperforms the state-of-the-art approaches by a significant margin on the\nbenchmark dataset DialogRE. Our code is released at:\nhttps://github.com/declare-lab/dialog-HGAT",
          "link": "http://arxiv.org/abs/2009.05092",
          "publishedOn": "2021-06-22T01:57:09.387Z",
          "wordCount": 643,
          "title": "Dialogue Relation Extraction with Document-level Heterogeneous Graph Attention Networks. (arXiv:2009.05092v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1\">Igor L. Markov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jacqueline Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vagner_A/0/1/0/all/0/1\">Adam Vagner</a>",
          "description": "Text classifiers are at the core of many NLP applications and use a variety\nof algorithmic approaches and software. This paper introduces infrastructure\nand methodologies for text classifiers based on large-scale regular\nexpressions. In particular, we describe how Facebook determines if a given\npiece of text - anything from a hashtag to a post - belongs to a narrow topic\nsuch as COVID-19. To fully define a topic and evaluate classifier performance\nwe employ human-guided iterations of keyword discovery, but do not require\nlabeled data. For COVID-19, we build two sets of regular expressions: (1) for\n66 languages, with 99% precision and recall >50%, (2) for the 11 most common\nlanguages, with precision >90% and recall >90%. Regular expressions enable\nlow-latency queries from multiple platforms. Response to challenges like\nCOVID-19 is fast and so are revisions. Comparisons to a DNN classifier show\nexplainable results, higher precision and recall, and less overfitting. Our\nlearnings can be applied to other narrow-topic classifiers.",
          "link": "http://arxiv.org/abs/2102.09507",
          "publishedOn": "2021-06-22T01:57:09.382Z",
          "wordCount": 694,
          "title": "Regular Expressions for Fast-response COVID-19 Text Classification. (arXiv:2102.09507v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.04626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbado_A/0/1/0/all/0/1\">Alberto Barbado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fresno_V/0/1/0/all/0/1\">V&#xed;ctor Fresno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riesco_A/0/1/0/all/0/1\">&#xc1;ngeles Manjarr&#xe9;s Riesco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ros_S/0/1/0/all/0/1\">Salvador Ros</a>",
          "description": "Nowadays, there are many applications of text mining over corpora from\ndifferent languages. However, most of them are based on texts in prose, lacking\napplications that work with poetry texts. An example of an application of text\nmining in poetry is the usage of features derived from their individual words\nin order to capture the lexical, sublexical and interlexical meaning, and infer\nthe General Affective Meaning (GAM) of the text. However, even though this\nproposal has been proved as useful for poetry in some languages, there is a\nlack of studies for both Spanish poetry and for highly-structured poetic\ncompositions such as sonnets. This article presents a study over an annotated\ncorpus of Spanish sonnets, in order to analyse if it is possible to build\nfeatures from their individual words for predicting their GAM. The purpose of\nthis is to model sonnets at an affective level. The article also analyses the\nrelationship between the GAM of the sonnets and the content itself. For this,\nwe consider the content from a psychological perspective, identifying with tags\nwhen a sonnet is related to a specific term. Then, we study how GAM changes\naccording to each of those psychological terms.\n\nThe corpus used contains 274 Spanish sonnets from authors of different\ncenturies, from 15th to 19th. This corpus was annotated by different domain\nexperts. The experts annotated the poems with affective and lexico-semantic\nfeatures, as well as with domain concepts that belong to psychology. Thanks to\nthis, the corpus of sonnets can be used in different applications, such as\npoetry recommender systems, personality text mining studies of the authors, or\nthe usage of poetry for therapeutic purposes.",
          "link": "http://arxiv.org/abs/2007.04626",
          "publishedOn": "2021-06-22T01:57:09.342Z",
          "wordCount": 762,
          "title": "DISCO PAL: Diachronic Spanish Sonnet Corpus with Psychological and Affective Labels. (arXiv:2007.04626v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.06934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lun Yiu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Cuiyun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhicong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>",
          "description": "Automatic generation of high-quality commit messages for code commits can\nsubstantially facilitate software developers' works and coordination. However,\nthe semantic gap between source code and natural language poses a major\nchallenge for the task. Several studies have been proposed to alleviate the\nchallenge but none explicitly involves code contextual information during\ncommit message generation. Specifically, existing research adopts static\nembedding for code tokens, which maps a token to the same vector regardless of\nits context. In this paper, we propose a novel Contextualized code\nrepresentation learning strategy for commit message Generation (CoreGen).\nCoreGen first learns contextualized code representations which exploit the\ncontextual information behind code commit sequences. The learned\nrepresentations of code commits built upon Transformer are then fine-tuned for\ndownstream commit message generation. Experiments on the benchmark dataset\ndemonstrate the superior effectiveness of our model over the baseline models\nwith at least 28.18% improvement in terms of BLEU-4 score. Furthermore, we also\nhighlight the future opportunities in training contextualized code\nrepresentations on larger code corpus as a solution to low-resource tasks and\nadapting the contextualized code representation framework to other code-to-text\ngeneration tasks.",
          "link": "http://arxiv.org/abs/2007.06934",
          "publishedOn": "2021-06-22T01:57:09.313Z",
          "wordCount": 673,
          "title": "CoreGen: Contextualized Code Representation Learning for Commit Message Generation. (arXiv:2007.06934v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Voropaev_P/0/1/0/all/0/1\">Pavel Voropaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sopilnyak_O/0/1/0/all/0/1\">Olga Sopilnyak</a>",
          "description": "In this paper, we explore various multilingual and Russian pre-trained\ntransformer-based models for the Dialogue Evaluation 2021 shared task on\nheadline selection. Our experiments show that the combined approach is superior\nto individual multilingual and monolingual models. We present an analysis of a\nnumber of ways to obtain sentence embeddings and learn a ranking model on top\nof them. We achieve the result of 87.28% and 86.60% accuracy for the public and\nprivate test sets respectively.",
          "link": "http://arxiv.org/abs/2106.10487",
          "publishedOn": "2021-06-22T01:57:09.306Z",
          "wordCount": 510,
          "title": "Transformers for Headline Selection for Russian News Clusters. (arXiv:2106.10487v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10745",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alyafeai_Z/0/1/0/all/0/1\">Zaid Alyafeai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_shaibani_M/0/1/0/all/0/1\">Maged S. Al-shaibani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaleb_M/0/1/0/all/0/1\">Mustafa Ghaleb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Wajih_Y/0/1/0/all/0/1\">Yousif Ahmed Al-Wajih</a>",
          "description": "Calligraphy is an essential part of the Arabic heritage and culture. It has\nbeen used in the past for the decoration of houses and mosques. Usually, such\ncalligraphy is designed manually by experts with aesthetic insights. In the\npast few years, there has been a considerable effort to digitize such type of\nart by either taking a photo of decorated buildings or drawing them using\ndigital devices. The latter is considered an online form where the drawing is\ntracked by recording the apparatus movement, an electronic pen for instance, on\na screen. In the literature, there are many offline datasets collected with a\ndiversity of Arabic styles for calligraphy. However, there is no available\nonline dataset for Arabic calligraphy. In this paper, we illustrate our\napproach for the collection and annotation of an online dataset for Arabic\ncalligraphy called Calliar that consists of 2,500 sentences. Calliar is\nannotated for stroke, character, word and sentence level prediction.",
          "link": "http://arxiv.org/abs/2106.10745",
          "publishedOn": "2021-06-22T01:57:09.218Z",
          "wordCount": 590,
          "title": "Calliar: An Online Handwritten Dataset for Arabic Calligraphy. (arXiv:2106.10745v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renshen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1\">Siyang Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popat_A/0/1/0/all/0/1\">Ashok Popat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>",
          "description": "Natural reading orders of words are crucial for information extraction from\nform-like documents. Despite recent advances in Graph Convolutional Networks\n(GCNs) on modeling spatial layout patterns of documents, they have limited\nability to capture reading orders of given word-level node representations in a\ngraph. We propose Reading Order Equivariant Positional Encoding (ROPE), a new\npositional encoding technique designed to apprehend the sequential presentation\nof words in documents. ROPE generates unique reading order codes for\nneighboring words relative to the target word given a word-level graph\nconnectivity. We study two fundamental document entity extraction tasks\nincluding word labeling and word grouping on the public FUNSD dataset and a\nlarge-scale payment dataset. We show that ROPE consistently improves existing\nGCNs with a margin up to 8.4% F1-score.",
          "link": "http://arxiv.org/abs/2106.10786",
          "publishedOn": "2021-06-22T01:57:09.197Z",
          "wordCount": 580,
          "title": "ROPE: Reading Order Equivariant Positional Encoding for Graph-based Document Information Extraction. (arXiv:2106.10786v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuxian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shengqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaojun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenbo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jian Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1\">Pei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yanzheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guoyang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhixing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wentao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoyan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>",
          "description": "In recent years, the size of pre-trained language models (PLMs) has grown by\nleaps and bounds. However, efficiency issues of these large-scale PLMs limit\ntheir utilization in real-world scenarios. We present a suite of cost-effective\ntechniques for the use of PLMs to deal with the efficiency issues of\npre-training, fine-tuning, and inference. (1) We introduce knowledge\ninheritance to accelerate the pre-training process by exploiting existing PLMs\ninstead of training models from scratch. (2) We explore the best practice of\nprompt tuning with large-scale PLMs. Compared with conventional fine-tuning,\nprompt tuning significantly reduces the number of task-specific parameters. (3)\nWe implement a new inference toolkit, namely InfMoE, for using large-scale PLMs\nwith limited computational resources. Based on our cost-effective pipeline, we\npre-train two models: an encoder-decoder bilingual model with 11 billion\nparameters (CPM-2) and its corresponding MoE version with 198 billion\nparameters. In our experiments, we compare CPM-2 with mT5 on downstream tasks.\nExperimental results show that CPM-2 has excellent general language\nintelligence. Moreover, we validate the efficiency of InfMoE when conducting\ninference of large-scale models having tens of billions of parameters on a\nsingle GPU. All source code and model parameters are available at\nhttps://github.com/TsinghuaAI/CPM.",
          "link": "http://arxiv.org/abs/2106.10715",
          "publishedOn": "2021-06-22T01:57:09.158Z",
          "wordCount": 655,
          "title": "CPM-2: Large-scale Cost-effective Pre-trained Language Models. (arXiv:2106.10715v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saadany_H/0/1/0/all/0/1\">Hadeel Saadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orasan_C/0/1/0/all/0/1\">Constantin Orasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quintana_R/0/1/0/all/0/1\">Rocio Caro Quintana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmo_F/0/1/0/all/0/1\">Felix do Carmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zilio_L/0/1/0/all/0/1\">Leonardo Zilio</a>",
          "description": "Although emotions are universal concepts, transferring the different shades\nof emotion from one language to another may not always be straightforward for\nhuman translators, let alone for machine translation systems. Moreover, the\ncognitive states are established by verbal explanations of experience which is\nshaped by both the verbal and cultural contexts. There are a number of verbal\ncontexts where expression of emotions constitutes the pivotal component of the\nmessage. This is particularly true for User-Generated Content (UGC) which can\nbe in the form of a review of a product or a service, a tweet, or a social\nmedia post. Recently, it has become common practice for multilingual websites\nsuch as Twitter to provide an automatic translation of UGC to reach out to\ntheir linguistically diverse users. In such scenarios, the process of\ntranslating the user's emotion is entirely automatic with no human\nintervention, neither for post-editing nor for accuracy checking. In this\nresearch, we assess whether automatic translation tools can be a successful\nreal-life utility in transferring emotion in user-generated multilingual data\nsuch as tweets. We show that there are linguistic phenomena specific of Twitter\ndata that pose a challenge in translation of emotions in different languages.\nWe summarise these challenges in a list of linguistic features and show how\nfrequent these features are in different language pairs. We also assess the\ncapacity of commonly used methods for evaluating the performance of an MT\nsystem with respect to the preservation of emotion in the source text.",
          "link": "http://arxiv.org/abs/2106.10719",
          "publishedOn": "2021-06-22T01:57:09.150Z",
          "wordCount": 693,
          "title": "Challenges in Translation of Emotions in Multilingual User-Generated Content: Twitter as a Case Study. (arXiv:2106.10719v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zihan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Low_C/0/1/0/all/0/1\">Charles Low</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_M/0/1/0/all/0/1\">Mengqiu Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1\">Daniel E. Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krass_M/0/1/0/all/0/1\">Mark S. Krass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grabmair_M/0/1/0/all/0/1\">Matthias Grabmair</a>",
          "description": "Lawyers and judges spend a large amount of time researching the proper legal\nauthority to cite while drafting decisions. In this paper, we develop a\ncitation recommendation tool that can help improve efficiency in the process of\nopinion drafting. We train four types of machine learning models, including a\ncitation-list based method (collaborative filtering) and three context-based\nmethods (text similarity, BiLSTM and RoBERTa classifiers). Our experiments show\nthat leveraging local textual context improves recommendation, and that deep\nneural models achieve decent performance. We show that non-deep text-based\nmethods benefit from access to structured case metadata, but deep models only\nbenefit from such access when predicting from context of insufficient length.\nWe also find that, even after extensive training, RoBERTa does not outperform a\nrecurrent neural model, despite its benefits of pretraining. Our behavior\nanalysis of the RoBERTa model further shows that predictive performance is\nstable across time and citation classes.",
          "link": "http://arxiv.org/abs/2106.10776",
          "publishedOn": "2021-06-22T01:57:09.143Z",
          "wordCount": 615,
          "title": "Context-Aware Legal Citation Recommendation using Deep Learning. (arXiv:2106.10776v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolk_A/0/1/0/all/0/1\">Agnieszka Wo&#x142;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chlasta_K/0/1/0/all/0/1\">Karol Chlasta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holas_P/0/1/0/all/0/1\">Pawe&#x142; Holas</a>",
          "description": "Sentiment and lexical analyses are widely used to detect depression or\nanxiety disorders. It has been documented that there are significant\ndifferences in the language used by a person with emotional disorders in\ncomparison to a healthy individual. Still, the effectiveness of these lexical\napproaches could be improved further because the current analysis focuses on\nwhat the social media entries are about, and not how they are written. In this\nstudy, we focus on aspects in which these short texts are similar to each\nother, and how they were created. We present an innovative approach to the\ndepression screening problem by applying Collgram analysis, which is a known\neffective method of obtaining linguistic information from texts. We compare\nthese results with sentiment analysis based on the BERT architecture. Finally,\nwe create a hybrid model achieving a diagnostic accuracy of 71%.",
          "link": "http://arxiv.org/abs/2106.10485",
          "publishedOn": "2021-06-22T01:57:09.134Z",
          "wordCount": 602,
          "title": "Hybrid approach to detecting symptoms of depression in social media entries. (arXiv:2106.10485v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>",
          "description": "Select-then-compress is a popular hybrid, framework for text summarization\ndue to its high efficiency. This framework first selects salient sentences and\nthen independently condenses each of the selected sentences into a concise\nversion. However, compressing sentences separately ignores the context\ninformation of the document, and is therefore prone to delete salient\ninformation. To address this limitation, we propose a novel\ncondense-then-select framework for text summarization. Our framework first\nconcurrently condenses each document sentence. Original document sentences and\ntheir compressed versions then become the candidates for extraction. Finally,\nan extractor utilizes the context information of the document to select\ncandidates and assembles them into a summary. If salient information is deleted\nduring condensing, the extractor can select an original sentence to retain the\ninformation. Thus, our framework helps to avoid the loss of salient\ninformation, while preserving the high efficiency of sentence-level\ncompression. Experiment results on the CNN/DailyMail, DUC-2002, and Pubmed\ndatasets demonstrate that our framework outperforms the select-then-compress\nframework and other strong baselines.",
          "link": "http://arxiv.org/abs/2106.10468",
          "publishedOn": "2021-06-22T01:57:09.103Z",
          "wordCount": 597,
          "title": "A Condense-then-Select Strategy for Text Summarization. (arXiv:2106.10468v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1\">Pei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Haozhe Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_Y/0/1/0/all/0/1\">Yu Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linfeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoyan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>",
          "description": "Existing pre-trained models for knowledge-graph-to-text (KG-to-text)\ngeneration simply fine-tune text-to-text pre-trained models such as BART or T5\non KG-to-text datasets, which largely ignore the graph structure during\nencoding and lack elaborate pre-training tasks to explicitly model graph-text\nalignments. To tackle these problems, we propose a graph-text joint\nrepresentation learning model called JointGT. During encoding, we devise a\nstructure-aware semantic aggregation module which is plugged into each\nTransformer layer to preserve the graph structure. Furthermore, we propose\nthree new pre-training tasks to explicitly enhance the graph-text alignment\nincluding respective text / graph reconstruction, and graph-text alignment in\nthe embedding space via Optimal Transport. Experiments show that JointGT\nobtains new state-of-the-art performance on various KG-to-text datasets.",
          "link": "http://arxiv.org/abs/2106.10502",
          "publishedOn": "2021-06-22T01:57:09.095Z",
          "wordCount": 568,
          "title": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs. (arXiv:2106.10502v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1\">Viraj Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Shruti Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>",
          "description": "We present TweeNLP, a one-stop portal that organizes Twitter's natural\nlanguage processing (NLP) data and builds a visualization and exploration\nplatform. It curates 19,395 tweets (as of April 2021) from various NLP\nconferences and general NLP discussions. It supports multiple features such as\nTweetExplorer to explore tweets by topics, visualize insights from Twitter\nactivity throughout the organization cycle of conferences, discover popular\nresearch papers and researchers. It also builds a timeline of conference and\nworkshop submission deadlines. We envision TweeNLP to function as a collective\nmemory unit for the NLP community by integrating the tweets pertaining to\nresearch papers with the NLPExplorer scientific literature search engine. The\ncurrent system is hosted at this http URL .",
          "link": "http://arxiv.org/abs/2106.10512",
          "publishedOn": "2021-06-22T01:57:09.083Z",
          "wordCount": 558,
          "title": "TweeNLP: A Twitter Exploration Portal for Natural Language Processing. (arXiv:2106.10512v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Juyong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1\">Pradeep Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>",
          "description": "Compositional generalization is the ability to generalize systematically to a\nnew data distribution by combining known components. Although humans seem to\nhave a great ability to generalize compositionally, state-of-the-art neural\nmodels struggle to do so. In this work, we study compositional generalization\nin classification tasks and present two main contributions. First, we study\nways to convert a natural language sequence-to-sequence dataset to a\nclassification dataset that also requires compositional generalization. Second,\nwe show that providing structural hints (specifically, providing parse trees\nand entity links as attention masks for a Transformer model) helps\ncompositional generalization.",
          "link": "http://arxiv.org/abs/2106.10434",
          "publishedOn": "2021-06-22T01:57:09.061Z",
          "wordCount": 540,
          "title": "Improving Compositional Generalization in Classification Tasks via Structure Annotations. (arXiv:2106.10434v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10454",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>",
          "description": "Question generation (QG) is to generate natural and grammatical questions\nthat can be answered by a specific answer for a given context. Previous\nsequence-to-sequence models suffer from a problem that asking high-quality\nquestions requires commonsense knowledge as backgrounds, which in most cases\ncan not be learned directly from training data, resulting in unsatisfactory\nquestions deprived of knowledge. In this paper, we propose a multi-task\nlearning framework to introduce commonsense knowledge into question generation\nprocess. We first retrieve relevant commonsense knowledge triples from mature\ndatabases and select triples with the conversion information from source\ncontext to question. Based on these informative knowledge triples, we design\ntwo auxiliary tasks to incorporate commonsense knowledge into the main QG\nmodel, where one task is Concept Relation Classification and the other is Tail\nConcept Generation. Experimental results on SQuAD show that our proposed\nmethods are able to noticeably improve the QG performance on both automatic and\nhuman evaluation metrics, demonstrating that incorporating external commonsense\nknowledge with multi-task learning can help the model generate human-like and\nhigh-quality questions.",
          "link": "http://arxiv.org/abs/2106.10454",
          "publishedOn": "2021-06-22T01:57:09.019Z",
          "wordCount": 604,
          "title": "Enhancing Question Generation with Commonsense Knowledge. (arXiv:2106.10454v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Solaiman_I/0/1/0/all/0/1\">Irene Solaiman</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Dennison_C/0/1/0/all/0/1\">Christy Dennison</a> (1) ((1) OpenAI)",
          "description": "Language models can generate harmful and biased outputs and exhibit\nundesirable behavior. We propose a Process for Adapting Language Models to\nSociety (PALMS) with Values-Targeted Datasets, an iterative process to\nsignificantly change model behavior by crafting and fine-tuning on a dataset\nthat reflects a predetermined set of target values. We evaluate our process\nusing three metrics: quantitative metrics with human evaluations that score\noutput adherence to a target value, and toxicity scoring on outputs; and\nqualitative metrics analyzing the most common word associated with a given\nsocial category. Through each iteration, we add additional training dataset\nexamples based on observed shortcomings from evaluations. PALMS performs\nsignificantly better on all metrics compared to baseline and control models for\na broad range of GPT-3 language model sizes without compromising capability\nintegrity. We find that the effectiveness of PALMS increases with model size.\nWe show that significantly adjusting language model behavior is feasible with a\nsmall, hand-curated dataset.",
          "link": "http://arxiv.org/abs/2106.10328",
          "publishedOn": "2021-06-22T01:57:09.009Z",
          "wordCount": 605,
          "title": "Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets. (arXiv:2106.10328v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10608",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundin_J/0/1/0/all/0/1\">Jessica Lundin</a>",
          "description": "Text-style transfer aims to convert text given in one domain into another by\nparaphrasing the sentence or substituting the keywords without altering the\ncontent. By necessity, state-of-the-art methods have evolved to accommodate\nnonparallel training data, as it is frequently the case there are multiple data\nsources of unequal size, with a mixture of labeled and unlabeled sentences.\nMoreover, the inherent style defined within each source might be distinct. A\ngeneric bidirectional (e.g., formal $\\Leftrightarrow$ informal) style transfer\nregardless of different groups may not generalize well to different\napplications. In this work, we developed a task adaptive meta-learning\nframework that can simultaneously perform a multi-pair text-style transfer\nusing a single model. The proposed method can adaptively balance the difference\nof meta-knowledge across multiple tasks. Results show that our method leads to\nbetter quantitative performance as well as coherent style variations. Common\nchallenges of unbalanced data and mismatched domains are handled well by this\nmethod.",
          "link": "http://arxiv.org/abs/2106.10608",
          "publishedOn": "2021-06-22T01:57:08.999Z",
          "wordCount": 592,
          "title": "Multi-Pair Text Style Transfer on Unbalanced Data. (arXiv:2106.10608v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>",
          "description": "Multi-head attention has each of the attention heads collect salient\ninformation from different parts of an input sequence, making it a powerful\nmechanism for sequence modeling. Multilingual and multi-domain learning are\ncommon scenarios for sequence modeling, where the key challenge is to maximize\npositive transfer and mitigate negative transfer across languages and domains.\nIn this paper, we find that non-selective attention sharing is sub-optimal for\nachieving good generalization across all languages and domains. We further\npropose attention sharing strategies to facilitate parameter sharing and\nspecialization in multilingual and multi-domain sequence modeling. Our approach\nautomatically learns shared and specialized attention heads for different\nlanguages and domains to mitigate their interference. Evaluated in various\ntasks including speech recognition, text-to-text and speech-to-text\ntranslation, the proposed attention sharing strategies consistently bring gains\nto sequence models built upon multi-head attention. For speech-to-text\ntranslation, our approach yields an average of $+2.0$ BLEU over $13$ language\ndirections in multilingual setting and $+2.0$ BLEU over $3$ domains in\nmulti-domain setting.",
          "link": "http://arxiv.org/abs/2106.10840",
          "publishedOn": "2021-06-22T01:57:08.985Z",
          "wordCount": 607,
          "title": "Pay Better Attention to Attention: Head Selection in Multilingual and Multi-Domain Sequence Modeling. (arXiv:2106.10840v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1\">Prasanna Parthasarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>",
          "description": "Predicting the next utterance in dialogue is contingent on encoding of users'\ninput text to generate appropriate and relevant response in data-driven\napproaches. Although the semantic and syntactic quality of the language\ngenerated is evaluated, more often than not, the encoded representation of\ninput is not evaluated. As the representation of the encoder is essential for\npredicting the appropriate response, evaluation of encoder representation is a\nchallenging yet important problem. In this work, we showcase evaluating the\ntext generated through human or automatic metrics is not sufficient to\nappropriately evaluate soundness of the language understanding of dialogue\nmodels and, to that end, propose a set of probe tasks to evaluate encoder\nrepresentation of different language encoders commonly used in dialogue models.\nFrom experiments, we observe that some of the probe tasks are easier and some\nare harder for even sophisticated model architectures to learn. And, through\nexperiments we observe that RNN based architectures have lower performance on\nautomatic metrics on text generation than transformer model but perform better\nthan the transformer model on the probe tasks indicating that RNNs might\npreserve task information better than the Transformers.",
          "link": "http://arxiv.org/abs/2106.10622",
          "publishedOn": "2021-06-22T01:57:08.609Z",
          "wordCount": 641,
          "title": "Do Encoder Representations of Generative Dialogue Models Encode Sufficient Information about the Task ?. (arXiv:2106.10622v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>",
          "description": "Transformer has been widely adopted in Neural Machine Translation (NMT)\nbecause of its large capacity and parallel training of sequence generation.\nHowever, the deployment of Transformer is challenging because different\nscenarios require models of different complexities and scales. Naively training\nmultiple Transformers is redundant in terms of both computation and memory. In\nthis paper, we propose a novel Scalable Transformers, which naturally contains\nsub-Transformers of different scales and have shared parameters. Each\nsub-Transformer can be easily obtained by cropping the parameters of the\nlargest Transformer. A three-stage training scheme is proposed to tackle the\ndifficulty of training the Scalable Transformers, which introduces additional\nsupervisions from word-level and sequence-level self-distillation. Extensive\nexperiments were conducted on WMT EN-De and En-Fr to validate our proposed\nScalable Transformers.",
          "link": "http://arxiv.org/abs/2106.02242",
          "publishedOn": "2021-06-21T02:07:37.440Z",
          "wordCount": 582,
          "title": "Scalable Transformers for Neural Machine Translation. (arXiv:2106.02242v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hahn_V/0/1/0/all/0/1\">Vanessa Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1\">Dana Ruiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinbauer_T/0/1/0/all/0/1\">Thomas Kleinbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>",
          "description": "Hate speech and profanity detection suffer from data sparsity, especially for\nlanguages other than English, due to the subjective nature of the tasks and the\nresulting annotation incompatibility of existing corpora. In this study, we\nidentify profane subspaces in word and sentence representations and explore\ntheir generalization capability on a variety of similar and distant target\ntasks in a zero-shot setting. This is done monolingually (German) and\ncross-lingually to closely-related (English), distantly-related (French) and\nnon-related (Arabic) tasks. We observe that, on both similar and distant target\ntasks and across all languages, the subspace-based representations transfer\nmore effectively than standard BERT representations in the zero-shot setting,\nwith improvements between F1 +10.9 and F1 +42.9 over the baselines across all\ntested monolingual and cross-lingual scenarios.",
          "link": "http://arxiv.org/abs/2106.07505",
          "publishedOn": "2021-06-21T02:07:37.433Z",
          "wordCount": 596,
          "title": "Modeling Profanity and Hate Speech in Social Media with Semantic Subspaces. (arXiv:2106.07505v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "End-to-end speech translation models have become a new trend in research due\nto their potential of reducing error propagation. However, these models still\nsuffer from the challenge of data scarcity. How to effectively use unlabeled or\nother parallel corpora from machine translation is promising but still an open\nproblem. In this paper, we propose Cross Speech-Text Network (XSTNet), an\nend-to-end model for speech-to-text translation. XSTNet takes both speech and\ntext as input and outputs both transcription and translation text. The model\nbenefits from its three key design aspects: a self-supervised pre-trained\nsub-network as the audio encoder, a multi-task training objective to exploit\nadditional parallel bilingual text, and a progressive training procedure. We\nevaluate the performance of XSTNet and baselines on the MuST-C En-X and\nLibriSpeech En-Fr datasets. In particular, XSTNet achieves state-of-the-art\nresults on all language directions with an average BLEU of 28.8, outperforming\nthe previous best method by 3.2 BLEU. Code, models, cases, and more detailed\nanalysis are available at https://github.com/ReneeYe/XSTNet.",
          "link": "http://arxiv.org/abs/2104.10380",
          "publishedOn": "2021-06-21T02:07:37.426Z",
          "wordCount": 623,
          "title": "End-to-end Speech Translation via Cross-modal Progressive Training. (arXiv:2104.10380v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01547",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhuoyuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhendong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1\">Xin Lei</a>",
          "description": "In this paper, we propose an open source, production first, and production\nready speech recognition toolkit called WeNet in which a new two-pass approach\nis implemented to unify streaming and non-streaming end-to-end (E2E) speech\nrecognition in a single model. The main motivation of WeNet is to close the gap\nbetween the research and the production of E2E speechrecognition models. WeNet\nprovides an efficient way to ship ASR applications in several real-world\nscenarios, which is the main difference and advantage to other open source E2E\nspeech recognition toolkits. In our toolkit, a new two-pass method is\nimplemented. Our method propose a dynamic chunk-based attention strategy of the\nthe transformer layers to allow arbitrary right context length modifies in\nhybrid CTC/attention architecture. The inference latency could be easily\ncontrolled by only changing the chunk size. The CTC hypotheses are then\nrescored by the attention decoder to get the final result. Our experiments on\nthe AISHELL-1 dataset using WeNet show that, our model achieves 5.03\\% relative\ncharacter error rate (CER) reduction in non-streaming ASR compared to a\nstandard non-streaming transformer. After model quantification, our model\nperform reasonable RTF and latency.",
          "link": "http://arxiv.org/abs/2102.01547",
          "publishedOn": "2021-06-21T02:07:37.376Z",
          "wordCount": 690,
          "title": "WeNet: Production oriented Streaming and Non-streaming End-to-End Speech Recognition Toolkit. (arXiv:2102.01547v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mysore_S/0/1/0/all/0/1\">Sheshera Mysore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OGorman_T/0/1/0/all/0/1\">Tim O&#x27;Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>",
          "description": "Query by Example is a well-known information retrieval task in which a\ndocument is chosen by the user as the search query and the goal is to retrieve\nrelevant documents from a large collection. However, a document often covers\nmultiple aspects of a topic. To address this scenario we introduce the task of\nfaceted Query by Example in which users can also specify a finer grained aspect\nin addition to the input query document. We focus on the application of this\ntask in scientific literature search. We envision models which are able to\nretrieve scientific papers analogous to a query scientific paper along\nspecifically chosen rhetorical structure elements as one solution to this\nproblem. In this work, the rhetorical structure elements, which we refer to as\nfacets, indicate backgrounds, methods, or results of a scientific paper. We\nintroduce and describe an expert annotated test collection to evaluate models\ntrained to perform this task. Our test collection consists of a diverse set of\n50 query documents, drawn from computational linguistics and machine learning\nvenues. We carefully followed the annotation guideline used by TREC for depth-k\npooling (k = 100 or 250) and the resulting data collection consists of graded\nrelevance scores with high annotation agreement. The data is freely available\nfor research purposes.",
          "link": "http://arxiv.org/abs/2103.12906",
          "publishedOn": "2021-06-21T02:07:37.356Z",
          "wordCount": 700,
          "title": "CSFCube -- A Test Collection of Computer Science Research Articles for Faceted Query by Example. (arXiv:2103.12906v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10259",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tomanek_K/0/1/0/all/0/1\">Katrin Tomanek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cattiau_J/0/1/0/all/0/1\">Julie Cattiau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandorkar_A/0/1/0/all/0/1\">Angad Chandorkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>",
          "description": "While current state-of-the-art Automatic Speech Recognition (ASR) systems\nachieve high accuracy on typical speech, they suffer from significant\nperformance degradation on disordered speech and other atypical speech\npatterns. Personalization of ASR models, a commonly applied solution to this\nproblem, is usually performed in a server-based training environment posing\nproblems around data privacy, delayed model-update times, and communication\ncost for copying data and models between mobile device and server\ninfrastructure. In this paper, we present an approach to on-device based ASR\npersonalization with very small amounts of speaker-specific data. We test our\napproach on a diverse set of 100 speakers with disordered speech and find\nmedian relative word error rate improvement of 71% with only 50 short\nutterances required per speaker. When tested on a voice-controlled home\nautomation platform, on-device personalized models show a median task success\nrate of 81%, compared to only 40% of the unadapted models.",
          "link": "http://arxiv.org/abs/2106.10259",
          "publishedOn": "2021-06-21T02:07:37.348Z",
          "wordCount": 604,
          "title": "On-Device Personalization of Automatic Speech Recognition Models for Disordered Speech. (arXiv:2106.10259v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mengzhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>",
          "description": "Punctuation prediction for automatic speech recognition (ASR) output\ntranscripts plays a crucial role for improving the readability of the ASR\ntranscripts and for improving the performance of downstream natural language\nprocessing applications. However, achieving good performance on punctuation\nprediction often requires large amounts of labeled speech transcripts, which is\nexpensive and laborious. In this paper, we propose a Discriminative\nSelf-Training approach with weighted loss and discriminative label smoothing to\nexploit unlabeled speech transcripts. Experimental results on the English\nIWSLT2011 benchmark test set and an internal Chinese spoken language dataset\ndemonstrate that the proposed approach achieves significant improvement on\npunctuation prediction accuracy over strong baselines including BERT, RoBERTa,\nand ELECTRA models. The proposed Discriminative Self-Training approach\noutperforms the vanilla self-training approach. We establish a new\nstate-of-the-art (SOTA) on the IWSLT2011 test set, outperforming the current\nSOTA model by 1.3% absolute gain on F$_1$.",
          "link": "http://arxiv.org/abs/2104.10339",
          "publishedOn": "2021-06-21T02:07:37.341Z",
          "wordCount": 601,
          "title": "Discriminative Self-training for Punctuation Prediction. (arXiv:2104.10339v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>",
          "description": "In the traditional cascading architecture for spoken language understanding\n(SLU), it has been observed that automatic speech recognition errors could be\ndetrimental to the performance of natural language understanding. End-to-end\n(E2E) SLU models have been proposed to directly map speech input to desired\nsemantic frame with a single model, hence mitigating ASR error propagation.\nRecently, pre-training technologies have been explored for these E2E models. In\nthis paper, we propose a novel joint textual-phonetic pre-training approach for\nlearning spoken language representations, aiming at exploring the full\npotentials of phonetic information to improve SLU robustness to ASR errors. We\nexplore phoneme labels as high-level speech features, and design and compare\npre-training tasks based on conditional masked language model objectives and\ninter-sentence relation objectives. We also investigate the efficacy of\ncombining textual and phonetic information during fine-tuning. Experimental\nresults on spoken language understanding benchmarks, Fluent Speech Commands and\nSNIPS, show that the proposed approach significantly outperforms strong\nbaseline models and improves robustness of spoken language understanding to ASR\nerrors.",
          "link": "http://arxiv.org/abs/2104.10357",
          "publishedOn": "2021-06-21T02:07:37.334Z",
          "wordCount": 637,
          "title": "Pre-training for Spoken Language Understanding with Joint Textual and Phonetic Representation Learning. (arXiv:2104.10357v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.01183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shengfei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Linghao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_H/0/1/0/all/0/1\">Huixiong Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huanhuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "In recent years, great success has been achieved in the field of natural\nlanguage processing (NLP), thanks in part to the considerable amount of\nannotated resources. For named entity recognition (NER), most languages do not\nhave such an abundance of labeled data as English, so the performances of those\nlanguages are relatively lower. To improve the performance, we propose a\ngeneral approach called Back Attention Network (BAN). BAN uses a translation\nsystem to translate other language sentences into English and then applies a\nnew mechanism named back attention knowledge transfer to obtain task-specific\ninformation from pre-trained high-resource languages NER model. This strategy\ncan transfer high-layer features of well-trained model and enrich the semantic\nrepresentations of the original language. Experiments on three different\nlanguage datasets indicate that the proposed approach outperforms other\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/1906.01183",
          "publishedOn": "2021-06-21T02:07:37.325Z",
          "wordCount": 608,
          "title": "Back Attention Knowledge Transfer for Low-Resource Named Entity Recognition. (arXiv:1906.01183v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01287",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dookun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1\">Julia Kiseleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Bum Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungjin Lee</a>",
          "description": "Digital assistants are experiencing rapid growth due to their ability to\nassist users with day-to-day tasks where most dialogues are happening\nmulti-turn. However, evaluating multi-turn dialogues remains challenging,\nespecially at scale. We suggest a context-sensitive method to estimate the\nturn-level satisfaction for dialogue considering various types of user\npreferences. The costs of interactions between users and dialogue systems are\nformulated using a budget consumption concept. We assume users have an initial\ninteraction budget for a dialogue formed based on the task complexity and that\neach turn has a cost. When the task is completed, or the budget has been\nexhausted, users quit the dialogue. We demonstrate our method's effectiveness\nby extensive experimentation with a simulated dialogue platform and real\nmulti-turn dialogues.",
          "link": "http://arxiv.org/abs/2103.01287",
          "publishedOn": "2021-06-21T02:07:37.307Z",
          "wordCount": 594,
          "title": "DEUS: A Data-driven Approach to Estimate User Satisfaction in Multi-turn Dialogues. (arXiv:2103.01287v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10169",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruirui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1\">Chelsea J.-T. Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1\">Hongda Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elibol_O/0/1/0/all/0/1\">Oguz Elibol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>",
          "description": "By implicitly recognizing a user based on his/her speech input, speaker\nidentification enables many downstream applications, such as personalized\nsystem behavior and expedited shopping checkouts. Based on whether the speech\ncontent is constrained or not, both text-dependent (TD) and text-independent\n(TI) speaker recognition models may be used. We wish to combine the advantages\nof both types of models through an ensemble system to make more reliable\npredictions. However, any such combined approach has to be robust to incomplete\ninputs, i.e., when either TD or TI input is missing. As a solution we propose a\nfusion of embeddings network foenet architecture, combining joint learning with\nneural attention. We compare foenet with four competitive baseline methods on a\ndataset of voice assistant inputs, and show that it achieves higher accuracy\nthan the baseline and score fusion methods, especially in the presence of\nincomplete inputs.",
          "link": "http://arxiv.org/abs/2106.10169",
          "publishedOn": "2021-06-21T02:07:37.247Z",
          "wordCount": 603,
          "title": "Fusion of Embeddings Networks for Robust Combination of Text Dependent and Independent Speaker Recognition. (arXiv:2106.10169v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zaken_E/0/1/0/all/0/1\">Elad Ben Zaken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>",
          "description": "We show that with small-to-medium training data, fine-tuning only the bias\nterms (or a subset of the bias terms) of pre-trained BERT models is competitive\nwith (and sometimes better than) fine-tuning the entire model. For larger data,\nbias-only fine-tuning is competitive with other sparse fine-tuning methods.\nBesides their practical utility, these findings are relevant for the question\nof understanding the commonly-used process of finetuning: they support the\nhypothesis that finetuning is mainly about exposing knowledge induced by\nlanguage-modeling training, rather than learning new task-specific linguistic\nknowledge.",
          "link": "http://arxiv.org/abs/2106.10199",
          "publishedOn": "2021-06-21T02:07:37.239Z",
          "wordCount": 520,
          "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. (arXiv:2106.10199v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_A/0/1/0/all/0/1\">Atsushi Fujita</a>",
          "description": "In deep neural network modeling, the most common practice is to stack a\nnumber of recurrent, convolutional, or feed-forward layers in order to obtain\nhigh-quality continuous space representations which in turn improves the\nquality of the network's prediction. Conventionally, each layer in the stack\nhas its own parameters which leads to a significant increase in the number of\nmodel parameters. In this paper, we propose to share parameters across all\nlayers thereby leading to a recurrently stacked neural network model. We report\non an extensive case study on neural machine translation (NMT), where we apply\nour proposed method to an encoder-decoder based neural network model, i.e., the\nTransformer model, and experiment with three Japanese--English translation\ndatasets. We empirically demonstrate that the translation quality of a model\nthat recurrently stacks a single layer 6 times, despite having significantly\nfewer parameters, approaches that of a model that stacks 6 layers where each\nlayer has different parameters. We also explore the limits of recurrent\nstacking where we train extremely deep NMT models. This paper also examines the\nutility of our recurrently stacked model as a student model through transfer\nlearning via leveraging pre-trained parameters and knowledge distillation, and\nshows that it compensates for the performance drops in translation quality that\nthe direct training of recurrently stacked model brings. We also show how\ntransfer learning helps in faster decoding on top of the already reduced number\nof parameters due to recurrent stacking. Finally, we analyze the effects of\nrecurrently stacked layers by visualizing the attentions of models that use\nrecurrently stacked layers and models that do not.",
          "link": "http://arxiv.org/abs/2106.10002",
          "publishedOn": "2021-06-21T02:07:37.141Z",
          "wordCount": 740,
          "title": "Recurrent Stacking of Layers in Neural Networks: An Application to Neural Machine Translation. (arXiv:2106.10002v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litvak_M/0/1/0/all/0/1\">Marina Litvak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanetik_N/0/1/0/all/0/1\">Natalia Vanetik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiacheng Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1\">Siya Qi</a>",
          "description": "Due to the subjectivity of the summarization, it is a good practice to have\nmore than one gold summary for each training document. However, many modern\nlarge-scale abstractive summarization datasets have only one-to-one samples\nwritten by different human with different styles. The impact of this phenomenon\nis understudied. We formulate the differences among possible multiple\nexpressions summarizing the same content as subjective bias and examine the\nrole of this bias in the context of abstractive summarization. In this paper a\nlightweight and effective method to extract the feature embeddings of\nsubjective styles is proposed. Results of summarization models trained on\nstyle-clustered datasets show that there are certain types of styles that lead\nto better convergence, abstraction and generalization. The reproducible code\nand generated summaries are available online.",
          "link": "http://arxiv.org/abs/2106.10084",
          "publishedOn": "2021-06-21T02:07:37.133Z",
          "wordCount": 565,
          "title": "Subjective Bias in Abstractive Summarization. (arXiv:2106.10084v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rego_R/0/1/0/all/0/1\">Rosana C. B. Rego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1\">Ver&#xf4;nica M. L. Silva</a>",
          "description": "Predicting gender by the name is not a simple task. In many applications,\nespecially in the natural language processing (NLP) field, this task may be\nnecessary, mainly when considering foreign names. Some machine learning\nalgorithms can satisfactorily perform the prediction. In this paper, we\nexamined and implemented feedforward and recurrent deep neural network models,\nsuch as MLP, RNN, GRU, CNN, and BiLSTM, to classify gender through the first\nname. A dataset of Brazilian names is used to train and evaluate the models. We\nanalyzed the accuracy, recall, precision, and confusion matrix to measure the\nmodels' performances. The results indicate that the gender prediction can be\nperformed from the feature extraction strategy looking at the names as a set of\nstrings. Some models accurately predict the gender in more than 90% of the\ncases. The recurrent models overcome the feedforward models in this binary\nclassification problem.",
          "link": "http://arxiv.org/abs/2106.10156",
          "publishedOn": "2021-06-21T02:07:37.091Z",
          "wordCount": 589,
          "title": "Predicting gender of Brazilian names using deep learning. (arXiv:2106.10156v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1\">Vivek Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>",
          "description": "Code-mixing is a frequent communication style among multilingual speakers\nwhere they mix words and phrases from two different languages in the same\nutterance of text or speech. Identifying and filtering code-mixed text is a\nchallenging task due to its co-existence with monolingual and noisy text. Over\nthe years, several code-mixing metrics have been extensively used to identify\nand validate code-mixed text quality. This paper demonstrates several inherent\nlimitations of code-mixing metrics with examples from the already existing\ndatasets that are popularly used across various experiments.",
          "link": "http://arxiv.org/abs/2106.10123",
          "publishedOn": "2021-06-21T02:07:37.059Z",
          "wordCount": 520,
          "title": "Challenges and Limitations with the Metrics Measuring the Complexity of Code-Mixed Text. (arXiv:2106.10123v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jian Zhu</a>",
          "description": "Generating synthesised singing voice with models trained on speech data has\nmany advantages due to the models' flexibility and controllability. However,\nsince the information about the temporal relationship between segments and\nbeats are lacking in speech training data, the synthesised singing may sound\noff-beat at times. Therefore, the availability of the information on the\ntemporal relationship between speech segments and music beats is crucial. The\ncurrent study investigated the segment-beat synchronisation in singing data,\nwith hypotheses formed based on the linguistics theories of P-centre and\nsonority hierarchy. A Mandarin corpus and an English corpus of professional\nsinging data were manually annotated and analysed. The results showed that the\npresence of musical beats was more dependent on segment duration than sonority.\nHowever, the sonority hierarchy and the P-centre theory were highly related to\nthe location of beats. Mandarin and English demonstrated cross-linguistic\nvariations despite exhibiting common patterns.",
          "link": "http://arxiv.org/abs/2106.10045",
          "publishedOn": "2021-06-21T02:07:37.051Z",
          "wordCount": 597,
          "title": "Synchronising speech segments with musical beats in Mandarin and English singing. (arXiv:2106.10045v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hieu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1\">Long Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Truong-Son Nguyen</a>",
          "description": "We aim to create an unprecedented attempt to build an end-to-end Question\nAnswering (QA) over Knowledge Graphs (KGs), which can construct SPARQL queries\nfrom natural language questions and generate a verbalized answer to its\nqueries. Hence, we introduce SPBERT, a Transformer-based language model\npre-trained on massive SPARQL query logs. By incorporating masked language\nmodelling objective and word structural objective, SPBERT can learn\ngeneral-purpose representations in both natural language and SPARQL query\nlanguage and make the most of the sequential order of words that are crucial\nfor structured language like SPARQL. In this paper, we investigate how SPBERT\nand encoder-decoder architecture can be adapted for Knowledge-based QA corpora.\nWe conduct exhaustive experiments on two auxiliary tasks, including SPARQL\nQuery Construction and Answer Verbalization Generation. Results show that\nSPBERT obtains promising performance and achieves state-of-the-art results on\nseveral of these tasks.",
          "link": "http://arxiv.org/abs/2106.09997",
          "publishedOn": "2021-06-21T02:07:37.037Z",
          "wordCount": 580,
          "title": "SPBERT: Pre-training BERT on SPARQL Queries for End-to-end Question Answering over Knowledge Graphs. (arXiv:2106.09997v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Terblanche_M/0/1/0/all/0/1\">Michelle Terblanche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>",
          "description": "Sentiment analysis as a sub-field of natural language processing has received\nincreased attention in the past decade enabling organisations to more\neffectively manage their reputation through online media monitoring. Many\ndrivers impact reputation, however, this thesis focuses only the aspect of\nfinancial performance and explores the gap with regards to financial sentiment\nanalysis in a South African context. Results showed that pre-trained sentiment\nanalysers are least effective for this task and that traditional lexicon-based\nand machine learning approaches are best suited to predict financial sentiment\nof news articles. The evaluated methods produced accuracies of 84\\%-94\\%. The\npredicted sentiments correlated quite well with share price and highlighted the\npotential use of sentiment as an indicator of financial performance. A main\ncontribution of the study was updating an existing sentiment dictionary for\nfinancial sentiment analysis. Model generalisation was less acceptable due to\nthe limited amount of training data used. Future work includes expanding the\ndata set to improve general usability and contribute to an open-source\nfinancial sentiment analyser for South African data.",
          "link": "http://arxiv.org/abs/2106.10004",
          "publishedOn": "2021-06-21T02:07:37.028Z",
          "wordCount": 609,
          "title": "Towards Financial Sentiment Analysis in a South African Landscape. (arXiv:2106.10004v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_G/0/1/0/all/0/1\">Georgi V. Georgiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1\">Danko D. Georgiev</a>",
          "description": "Human creativity generates novel ideas to solve real-world problems. This\nthereby grants us the power to transform the surrounding world and extend our\nhuman attributes beyond what is currently possible. Creative ideas are not just\nnew and unexpected, but are also successful in providing solutions that are\nuseful, efficient and valuable. Thus, creativity optimizes the use of available\nresources and increases wealth. The origin of human creativity, however, is\npoorly understood, and semantic measures that could predict the success of\ngenerated ideas are currently unknown. Here, we analyze a dataset of design\nproblem-solving conversations in real-world settings by using 49 semantic\nmeasures based on WordNet 3.1 and demonstrate that a divergence of semantic\nsimilarity, an increased information content, and a decreased polysemy predict\nthe success of generated ideas. The first feedback from clients also enhances\ninformation content and leads to a divergence of successful ideas in creative\nproblem solving. These results advance cognitive science by identifying\nreal-world processes in human problem solving that are relevant to the success\nof produced solutions and provide tools for real-time monitoring of problem\nsolving, student training and skill acquisition. A selected subset of\ninformation content (IC S\\'anchez-Batet) and semantic similarity\n(Lin/S\\'anchez-Batet) measures, which are both statistically powerful and\ncomputationally fast, could support the development of technologies for\ncomputer-assisted enhancements of human creativity or for the implementation of\ncreativity in machines endowed with general artificial intelligence.",
          "link": "http://arxiv.org/abs/2106.10131",
          "publishedOn": "2021-06-21T02:07:37.005Z",
          "wordCount": 677,
          "title": "Enhancing user creativity: Semantic measures for idea generation. (arXiv:2106.10131v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingbing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zelong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_H/0/1/0/all/0/1\">Haining An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>",
          "description": "One of the key problems in multi-label text classification is how to take\nadvantage of the correlation among labels. However, it is very challenging to\ndirectly model the correlations among labels in a complex and unknown label\nspace. In this paper, we propose a Label Mask multi-label text classification\nmodel (LM-MTC), which is inspired by the idea of cloze questions of language\nmodel. LM-MTC is able to capture implicit relationships among labels through\nthe powerful ability of pre-train language models. On the basis, we assign a\ndifferent token to each potential label, and randomly mask the token with a\ncertain probability to build a label based Masked Language Model (MLM). We\ntrain the MTC and MLM together, further improving the generalization ability of\nthe model. A large number of experiments on multiple datasets demonstrate the\neffectiveness of our method.",
          "link": "http://arxiv.org/abs/2106.10076",
          "publishedOn": "2021-06-21T02:07:36.996Z",
          "wordCount": 579,
          "title": "Label Mask for Multi-Label Text Classification. (arXiv:2106.10076v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seonwoo_Y/0/1/0/all/0/1\">Yeon Seonwoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Ji-Hoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>",
          "description": "In multi-hop QA, answering complex questions entails iterative document\nretrieval for finding the missing entity of the question. The main steps of\nthis process are sub-question detection, document retrieval for the\nsub-question, and generation of a new query for the final document retrieval.\nHowever, building a dataset that contains complex questions with sub-questions\nand their corresponding documents requires costly human annotation. To address\nthe issue, we propose a new method for weakly supervised multi-hop retriever\npre-training without human efforts. Our method includes 1) a pre-training task\nfor generating vector representations of complex questions, 2) a scalable data\ngeneration method that produces the nested structure of question and\nsub-question as weak supervision for pre-training, and 3) a pre-training model\nstructure based on dense encoders. We conduct experiments to compare the\nperformance of our pre-trained retriever with several state-of-the-art models\non end-to-end multi-hop QA as well as document retrieval. The experimental\nresults show that our pre-trained retriever is effective and also robust on\nlimited data and computational resources.",
          "link": "http://arxiv.org/abs/2106.09983",
          "publishedOn": "2021-06-21T02:07:36.944Z",
          "wordCount": 599,
          "title": "Weakly Supervised Pre-Training for Multi-Hop Retriever. (arXiv:2106.09983v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Makino_K/0/1/0/all/0/1\">Kohei Makino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miwa_M/0/1/0/all/0/1\">Makoto Miwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_Y/0/1/0/all/0/1\">Yutaka Sasaki</a>",
          "description": "In this paper, we propose a novel edge-editing approach to extract relation\ninformation from a document. We treat the relations in a document as a relation\ngraph among entities in this approach. The relation graph is iteratively\nconstructed by editing edges of an initial graph, which might be a graph\nextracted by another system or an empty graph. The way to edit edges is to\nclassify them in a close-first manner using the document and\ntemporally-constructed graph information; each edge is represented with a\ndocument context information by a pretrained transformer model and a graph\ncontext information by a graph convolutional neural network model. We evaluate\nour approach on the task to extract material synthesis procedures from\nmaterials science texts. The experimental results show the effectiveness of our\napproach in editing the graphs initialized by our in-house rule-based system\nand empty graphs.",
          "link": "http://arxiv.org/abs/2106.09900",
          "publishedOn": "2021-06-21T02:07:36.926Z",
          "wordCount": 595,
          "title": "A Neural Edge-Editing Approach for Document-Level Relation Graph Extraction. (arXiv:2106.09900v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ash_J/0/1/0/all/0/1\">Jordan T. Ash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1\">Surbhi Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1\">Akshay Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1\">Dipendra Misra</a>",
          "description": "Noise contrastive learning is a popular technique for unsupervised\nrepresentation learning. In this approach, a representation is obtained via\nreduction to supervised learning, where given a notion of semantic similarity,\nthe learner tries to distinguish a similar (positive) example from a collection\nof random (negative) examples. The success of modern contrastive learning\npipelines relies on many parameters such as the choice of data augmentation,\nthe number of negative examples, and the batch size; however, there is limited\nunderstanding as to how these parameters interact and affect downstream\nperformance. We focus on disambiguating the role of one of these parameters:\nthe number of negative examples. Theoretically, we show the existence of a\ncollision-coverage trade-off suggesting that the optimal number of negative\nexamples should scale with the number of underlying concepts in the data.\nEmpirically, we scrutinize the role of the number of negatives in both NLP and\nvision tasks. In the NLP task, we find that the results broadly agree with our\ntheory, while our vision experiments are murkier with performance sometimes\neven being insensitive to the number of negatives. We discuss plausible\nexplanations for this behavior and suggest future directions to better align\ntheory and practice.",
          "link": "http://arxiv.org/abs/2106.09943",
          "publishedOn": "2021-06-21T02:07:36.915Z",
          "wordCount": 639,
          "title": "Investigating the Role of Negatives in Contrastive Representation Learning. (arXiv:2106.09943v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurajada_S/0/1/0/all/0/1\">Sairam Gurajada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qiuhao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neelam_S/0/1/0/all/0/1\">Sumit Neelam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popa_L/0/1/0/all/0/1\">Lucian Popa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_P/0/1/0/all/0/1\">Prithviraj Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_A/0/1/0/all/0/1\">Alexander Gray</a>",
          "description": "Entity linking (EL), the task of disambiguating mentions in text by linking\nthem to entities in a knowledge graph, is crucial for text understanding,\nquestion answering or conversational systems. Entity linking on short text\n(e.g., single sentence or question) poses particular challenges due to limited\ncontext. While prior approaches use either heuristics or black-box neural\nmethods, here we propose LNN-EL, a neuro-symbolic approach that combines the\nadvantages of using interpretable rules based on first-order logic with the\nperformance of neural learning. Even though constrained to using rules, LNN-EL\nperforms competitively against SotA black-box neural approaches, with the added\nbenefits of extensibility and transferability. In particular, we show that we\ncan easily blend existing rule templates given by a human expert, with multiple\ntypes of features (priors, BERT encodings, box embeddings, etc), and even\nscores resulting from previous EL methods, thus improving on such methods. For\ninstance, on the LC-QuAD-1.0 dataset, we show more than $4$\\% increase in F1\nscore over previous SotA. Finally, we show that the inductive bias offered by\nusing logic results in learned rules that transfer well across datasets, even\nwithout fine tuning, while maintaining high accuracy.",
          "link": "http://arxiv.org/abs/2106.09795",
          "publishedOn": "2021-06-21T02:07:36.895Z",
          "wordCount": 643,
          "title": "LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking. (arXiv:2106.09795v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hengyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_R/0/1/0/all/0/1\">Rui Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yifan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Ming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Joint extraction of entities and relations from unstructured texts is a\ncrucial task in information extraction. Recent methods achieve considerable\nperformance but still suffer from some inherent limitations, such as redundancy\nof relation prediction, poor generalization of span-based extraction and\ninefficiency. In this paper, we decompose this task into three subtasks,\nRelation Judgement, Entity Extraction and Subject-object Alignment from a novel\nperspective and then propose a joint relational triple extraction framework\nbased on Potential Relation and Global Correspondence (PRGC). Specifically, we\ndesign a component to predict potential relations, which constrains the\nfollowing entity extraction to the predicted relation subset rather than all\nrelations; then a relation-specific sequence tagging component is applied to\nhandle the overlapping problem between subjects and objects; finally, a global\ncorrespondence component is designed to align the subject and object into a\ntriple with low-complexity. Extensive experiments show that PRGC achieves\nstate-of-the-art performance on public benchmarks with higher efficiency and\ndelivers consistent performance gain on complex scenarios of overlapping\ntriples.",
          "link": "http://arxiv.org/abs/2106.09895",
          "publishedOn": "2021-06-21T02:07:36.885Z",
          "wordCount": 617,
          "title": "PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction. (arXiv:2106.09895v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haisong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>",
          "description": "Quotations are crucial for successful explanations and persuasions in\ninterpersonal communications. However, finding what to quote in a conversation\nis challenging for both humans and machines. This work studies automatic\nquotation generation in an online conversation and explores how language\nconsistency affects whether a quotation fits the given context. Here, we\ncapture the contextual consistency of a quotation in terms of latent topics,\ninteractions with the dialogue history, and coherence to the query turn's\nexisting content. Further, an encoder-decoder neural framework is employed to\ncontinue the context with a quotation via language generation. Experiment\nresults on two large-scale datasets in English and Chinese demonstrate that our\nquotation generation model outperforms the state-of-the-art models. Further\nanalysis shows that topic, interaction, and query consistency are all helpful\nto learn how to quote in online conversations.",
          "link": "http://arxiv.org/abs/2106.09896",
          "publishedOn": "2021-06-21T02:07:36.876Z",
          "wordCount": 584,
          "title": "Continuity of Topic, Interaction, and Query: Learning to Quote in Online Conversations. (arXiv:2106.09896v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jingli Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weihua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yongchareon_S/0/1/0/all/0/1\">Sira Yongchareon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1\">Quan Bai</a>",
          "description": "Public concern detection provides potential guidance to the authorities for\ncrisis management before or during a pandemic outbreak. Detecting people's\nconcerns and attention from online social media platforms has been widely\nacknowledged as an effective approach to relieve public panic and prevent a\nsocial crisis. However, detecting concerns in time from massive information in\nsocial media turns out to be a big challenge, especially when sufficient\nmanually labeled data is in the absence of public health emergencies, e.g.,\nCOVID-19. In this paper, we propose a novel end-to-end deep learning model to\nidentify people's concerns and the corresponding relations based on Graph\nConvolutional Network and Bi-directional Long Short Term Memory integrated with\nConcern Graph. Except for the sequential features from BERT embeddings, the\nregional features of tweets can be extracted by the Concern Graph module, which\nnot only benefits the concern detection but also enables our model to be high\nnoise-tolerant. Thus, our model can address the issue of insufficient manually\nlabeled data. We conduct extensive experiments to evaluate the proposed model\nby using both manually labeled tweets and automatically labeled tweets. The\nexperimental results show that our model can outperform the state-of-art models\non real-world datasets.",
          "link": "http://arxiv.org/abs/2106.09929",
          "publishedOn": "2021-06-21T02:07:36.859Z",
          "wordCount": 682,
          "title": "Graph-based Joint Pandemic Concern and Relation Extraction on Twitter. (arXiv:2106.09929v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boucher_N/0/1/0/all/0/1\">Nicholas Boucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1\">Ilia Shumailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_R/0/1/0/all/0/1\">Ross Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1\">Nicolas Papernot</a>",
          "description": "Several years of research have shown that machine-learning systems are\nvulnerable to adversarial examples, both in theory and in practice. Until now,\nsuch attacks have primarily targeted visual models, exploiting the gap between\nhuman and machine perception. Although text-based models have also been\nattacked with adversarial examples, such attacks struggled to preserve semantic\nmeaning and indistinguishability. In this paper, we explore a large class of\nadversarial examples that can be used to attack text-based models in a\nblack-box setting without making any human-perceptible visual modification to\ninputs. We use encoding-specific perturbations that are imperceptible to the\nhuman eye to manipulate the outputs of a wide range of Natural Language\nProcessing (NLP) systems from neural machine-translation pipelines to web\nsearch engines. We find that with a single imperceptible encoding injection --\nrepresenting one invisible character, homoglyph, reordering, or deletion -- an\nattacker can significantly reduce the performance of vulnerable models, and\nwith three injections most models can be functionally broken. Our attacks work\nagainst currently-deployed commercial systems, including those produced by\nMicrosoft and Google, in addition to open source models published by Facebook\nand IBM. This novel series of attacks presents a significant threat to many\nlanguage processing systems: an attacker can affect systems in a targeted\nmanner without any assumptions about the underlying model. We conclude that\ntext-based NLP systems require careful input sanitization, just like\nconventional applications, and that given such systems are now being deployed\nrapidly at scale, the urgent attention of architects and operators is required.",
          "link": "http://arxiv.org/abs/2106.09898",
          "publishedOn": "2021-06-21T02:07:36.850Z",
          "wordCount": 683,
          "title": "Bad Characters: Imperceptible NLP Attacks. (arXiv:2106.09898v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Lin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_E/0/1/0/all/0/1\">Edward Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Lei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Huaishao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharti_T/0/1/0/all/0/1\">Taroon Bharti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacheti_A/0/1/0/all/0/1\">Arun Sacheti</a>",
          "description": "In this paper, we present GEM as a General Evaluation benchmark for\nMultimodal tasks. Different from existing datasets such as GLUE, SuperGLUE,\nXGLUE and XTREME that mainly focus on natural language tasks, GEM is a\nlarge-scale vision-language benchmark, which consists of GEM-I for\nimage-language tasks and GEM-V for video-language tasks. Comparing with\nexisting multimodal datasets such as MSCOCO and Flicker30K for image-language\ntasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the\nlargest vision-language dataset covering image-language tasks and\nvideo-language tasks at the same time, but also labeled in multiple languages.\nWe also provide two baseline models for this benchmark. We will release the\ndataset, code and baseline models, aiming to advance the development of\nmultilingual multimodal research.",
          "link": "http://arxiv.org/abs/2106.09889",
          "publishedOn": "2021-06-21T02:07:36.841Z",
          "wordCount": 581,
          "title": "GEM: A General Evaluation Benchmark for Multimodal Tasks. (arXiv:2106.09889v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Turcan_E/0/1/0/all/0/1\">Elsbeth Turcan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anubhai_R/0/1/0/all/0/1\">Rishita Anubhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_K/0/1/0/all/0/1\">Kasturi Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Onaizan_Y/0/1/0/all/0/1\">Yaser Al-Onaizan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>",
          "description": "Detecting what emotions are expressed in text is a well-studied problem in\nnatural language processing. However, research on finer grained emotion\nanalysis such as what causes an emotion is still in its infancy. We present\nsolutions that tackle both emotion recognition and emotion cause detection in a\njoint fashion. Considering that common-sense knowledge plays an important role\nin understanding implicitly expressed emotions and the reasons for those\nemotions, we propose novel methods that combine common-sense knowledge via\nadapted knowledge models with multi-task learning to perform joint emotion\nclassification and emotion cause tagging. We show performance improvement on\nboth tasks when including common-sense reasoning and a multitask framework. We\nprovide a thorough analysis to gain insights into model performance.",
          "link": "http://arxiv.org/abs/2106.09790",
          "publishedOn": "2021-06-21T02:07:36.699Z",
          "wordCount": 569,
          "title": "Multi-Task Learning and Adapted Knowledge Models for Emotion-Cause Extraction. (arXiv:2106.09790v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09760",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1\">Kwangyoun Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_F/0/1/0/all/0/1\">Felix Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sridhar_P/0/1/0/all/0/1\">Prashant Sridhar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_K/0/1/0/all/0/1\">Kyu J. Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>",
          "description": "Automatic speech recognition (ASR) models make fewer errors when more\nsurrounding speech information is presented as context. Unfortunately,\nacquiring a larger future context leads to higher latency. There exists an\ninevitable trade-off between speed and accuracy. Naively, to fit different\nlatency requirements, people have to store multiple models and pick the best\none under the constraints. Instead, a more desirable approach is to have a\nsingle model that can dynamically adjust its latency based on different\nconstraints, which we refer to as Multi-mode ASR. A Multi-mode ASR model can\nfulfill various latency requirements during inference -- when a larger latency\nbecomes acceptable, the model can process longer future context to achieve\nhigher accuracy and when a latency budget is not flexible, the model can be\nless dependent on future context but still achieve reliable accuracy. In\npursuit of Multi-mode ASR, we propose Stochastic Future Context, a simple\ntraining procedure that samples one streaming configuration in each iteration.\nThrough extensive experiments on AISHELL-1 and LibriSpeech datasets, we show\nthat a Multi-mode ASR model rivals, if not surpasses, a set of competitive\nstreaming baselines trained with different latency budgets.",
          "link": "http://arxiv.org/abs/2106.09760",
          "publishedOn": "2021-06-21T02:07:36.655Z",
          "wordCount": 639,
          "title": "Multi-mode Transformer Transducer with Stochastic Future Context. (arXiv:2106.09760v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Mustafizur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_D/0/1/0/all/0/1\">Dinesh Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murthy_D/0/1/0/all/0/1\">Dhiraj Murthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutlu_M/0/1/0/all/0/1\">Mucahid Kutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1\">Matthew Lease</a>",
          "description": "Building a benchmark dataset for hate speech detection presents several\nchallenges. Firstly, because hate speech is relatively rare -- e.g., less than\n3\\% of Twitter posts are hateful \\citep{founta2018large} -- random sampling of\ntweets to annotate is inefficient in capturing hate speech. A common practice\nis to only annotate tweets containing known ``hate words'', but this risks\nyielding a biased benchmark that only partially captures the real-world\nphenomenon of interest. A second challenge is that definitions of hate speech\ntend to be highly variable and subjective. Annotators having diverse prior\nnotions of hate speech may not only disagree with one another but also struggle\nto conform to specified labeling guidelines. Our key insight is that the rarity\nand subjectivity of hate speech are akin to that of relevance in information\nretrieval (IR). This connection suggests that well-established methodologies\nfor creating IR test collections might also be usefully applied to create\nbetter benchmark datasets for hate speech detection. Firstly, to intelligently\nand efficiently select which tweets to annotate, we apply established IR\ntechniques of {\\em pooling} and {\\em active learning}. Secondly, to improve\nboth consistency and value of annotations, we apply {\\em task decomposition}\n\\cite{Zhang-sigir14} and {\\em annotator rationale} \\cite{mcdonnell16-hcomp}\ntechniques. Using the above techniques, we create and share a new benchmark\ndataset\\footnote{We will release the dataset upon publication.} for hate speech\ndetection with broader coverage than prior datasets. We also show a dramatic\ndrop in accuracy of existing detection models when tested on these broader\nforms of hate. Collected annotator rationales not only provide documented\nsupport for labeling decisions but also create exciting future work\nopportunities for dual-supervision and/or explanation generation in modeling.",
          "link": "http://arxiv.org/abs/2106.09775",
          "publishedOn": "2021-06-21T02:07:36.643Z",
          "wordCount": 726,
          "title": "An Information Retrieval Approach to Building Datasets for Hate Speech Detection. (arXiv:2106.09775v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.10928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1\">Nawshad Farruque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1\">Randy Goebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivapalan_S/0/1/0/all/0/1\">Sudhakar Sivapalan</a>",
          "description": "We focus on exploring various approaches of Zero-Shot Learning (ZSL) and\ntheir explainability for a challenging yet important supervised learning task\nnotorious for training data scarcity, i.e. Depression Symptoms Detection (DSD)\nfrom text. We start with a comprehensive synthesis of different components of\nour ZSL modeling and analysis of our ground truth samples and Depression\nsymptom clues curation process with the help of a practicing clinician. We next\nanalyze the accuracy of various state-of-the-art ZSL models and their potential\nenhancements for our task. Further, we sketch a framework for the use of ZSL\nfor hierarchical text-based explanation mechanism, which we call, Syntax\nTree-Guided Semantic Explanation (STEP). Finally, we summarize experiments from\nwhich we conclude that we can use ZSL models and achieve reasonable accuracy\nand explainability, measured by a proposed Explainability Index (EI). This work\nis, to our knowledge, the first work to exhaustively explore the efficacy of\nZSL models for DSD task, both in terms of accuracy and explainability.",
          "link": "http://arxiv.org/abs/2106.10928",
          "publishedOn": "2021-06-24T01:51:42.471Z",
          "wordCount": 640,
          "title": "STEP-EZ: Syntax Tree guided semantic ExPlanation for Explainable Zero-shot modeling of clinical depression symptoms from text. (arXiv:2106.10928v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shenghao Xu</a>",
          "description": "Multi-armed bandits (MAB) provide a principled online learning approach to\nattain the balance between exploration and exploitation. Due to the superior\nperformance and low feedback learning without the learning to act in multiple\nsituations, Multi-armed Bandits drawing widespread attention in applications\nranging such as recommender systems. Likewise, within the recommender system,\ncollaborative filtering (CF) is arguably the earliest and most influential\nmethod in the recommender system. Crucially, new users and an ever-changing\npool of recommended items are the challenges that recommender systems need to\naddress. For collaborative filtering, the classical method is training the\nmodel offline, then perform the online testing, but this approach can no longer\nhandle the dynamic changes in user preferences which is the so-called cold\nstart. So how to effectively recommend items to users in the absence of\neffective information? To address the aforementioned problems, a multi-armed\nbandit based collaborative filtering recommender system has been proposed,\nnamed BanditMF. BanditMF is designed to address two challenges in the\nmulti-armed bandits algorithm and collaborative filtering: (1) how to solve the\ncold start problem for collaborative filtering under the condition of scarcity\nof valid information, (2) how to solve the sub-optimal problem of bandit\nalgorithms in strong social relations domains caused by independently\nestimating unknown parameters associated with each user and ignoring\ncorrelations between users.",
          "link": "http://arxiv.org/abs/2106.10898",
          "publishedOn": "2021-06-24T01:51:42.312Z",
          "wordCount": 664,
          "title": "BanditMF: Multi-Armed Bandit Based Matrix Factorization Recommender System. (arXiv:2106.10898v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Muyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Huasheng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>",
          "description": "One of the key challenges in Sequential Recommendation (SR) is how to extract\nand represent user preferences. Traditional SR methods rely on the next item as\nthe supervision signal to guide preference extraction and representation. We\npropose a novel learning strategy, named preference editing. The idea is to\nforce the SR model to discriminate the common and unique preferences in\ndifferent sequences of interactions between users and the recommender system.\nBy doing so, the SR model is able to learn how to identify common and unique\nuser preferences, and thereby do better user preference extraction and\nrepresentation. We propose a transformer based SR model, named MrTransformer\n(Multi-preference Transformer), that concatenates some special tokens in front\nof the sequence to represent multiple user preferences and makes sure they\ncapture different aspects through a preference coverage mechanism. Then, we\ndevise a preference editing-based self-supervised learning mechanism for\ntraining MrTransformer which contains two main operations: preference\nseparation and preference recombination. The former separates the common and\nunique user preferences for a given pair of sequences. The latter swaps the\ncommon preferences to obtain recombined user preferences for each sequence.\nBased on the preference separation and preference recombination operations, we\ndefine two types of SSL loss that require that the recombined preferences are\nsimilar to the original ones, and the common preferences are close to each\nother.\n\nWe carry out extensive experiments on two benchmark datasets. MrTransformer\nwith preference editing significantly outperforms state-of-the-art SR methods\nin terms of Recall, MRR and NDCG. We find that long sequences whose user\npreferences are harder to extract and represent benefit most from preference\nediting.",
          "link": "http://arxiv.org/abs/2106.12120",
          "publishedOn": "2021-06-24T01:51:42.106Z",
          "wordCount": 706,
          "title": "Improving Transformer-based Sequential Recommenders through Preference Editing. (arXiv:2106.12120v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Delianidi_M/0/1/0/all/0/1\">Marina Delianidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salampasis_M/0/1/0/all/0/1\">Michail Salampasis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diamantaras_K/0/1/0/all/0/1\">Konstantinos Diamantaras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siomos_T/0/1/0/all/0/1\">Theodosios Siomos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsalis_A/0/1/0/all/0/1\">Alkiviadis Katsalis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karaveli_I/0/1/0/all/0/1\">Iphigenia Karaveli</a>",
          "description": "We present a graph-based approach for the data management tasks and the\nefficient operation of a system for session-based next-item recommendations.\nThe proposed method can collect data continuously and incrementally from an\necommerce web site, thus seemingly prepare the necessary data infrastructure\nfor the recommendation algorithm to operate without any excessive training\nphase. Our work aims at developing a recommender method that represents a\nbalance between data processing and management efficiency requirements and the\neffectiveness of the recommendations produced. We use the Neo4j graph database\nto implement a prototype of such a system. Furthermore, we use an industry\ndataset corresponding to a typical e-commerce session-based scenario, and we\nreport on experiments using our graph-based approach and other state-of-the-art\nmachine learning and deep learning methods.",
          "link": "http://arxiv.org/abs/2106.12085",
          "publishedOn": "2021-06-24T01:51:42.093Z",
          "wordCount": 580,
          "title": "A Graph-based Method for Session-based Recommendations. (arXiv:2106.12085v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2102.02964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sunouchi_M/0/1/0/all/0/1\">Motohiro Sunouchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_M/0/1/0/all/0/1\">Masaharu Yoshioka</a>",
          "description": "This paper proposes new acoustic feature signatures based on the multiscale\nfractal dimension (MFD), which are robust against the diversity of\nenvironmental sounds, for the content-based similarity search. The diversity of\nsound sources and acoustic compositions is a typical feature of environmental\nsounds. Several acoustic features have been proposed for environmental sounds.\nAmong them is the widely-used Mel-Frequency Cepstral Coefficients (MFCCs),\nwhich describes frequency-domain features. However, in addition to these\nfeatures in the frequency domain, environmental sounds have other important\nfeatures in the time domain with various time scales. In our previous paper, we\nproposed enhanced multiscale fractal dimension signature (EMFD) for\nenvironmental sounds. This paper extends EMFD by using the kernel density\nestimation method, which results in better performance of the similarity search\ntasks. Furthermore, it newly proposes another acoustic feature signature based\non MFD, namely very-long-range multiscale fractal dimension signature (MFD-VL).\nThe MFD-VL signature describes several features of the time-varying envelope\nfor long periods of time. The MFD-VL signature has stability and robustness\nagainst background noise and small fluctuations in the parameters of sound\nsources, which are produced in field recordings. We discuss the effectiveness\nof these signatures in the similarity sound search by comparing with acoustic\nfeatures proposed in the DCASE 2018 challenges. Due to the unique\ndescriptiveness of our proposed signatures, we confirmed the signatures are\neffective when they are used with other acoustic features.",
          "link": "http://arxiv.org/abs/2102.02964",
          "publishedOn": "2021-06-24T01:51:42.072Z",
          "wordCount": 707,
          "title": "Diversity-Robust Acoustic Feature Signatures Based on Multiscale Fractal Dimension for Similarity Search of Environmental Sounds. (arXiv:2102.02964v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tam_C/0/1/0/all/0/1\">Charmaine Tam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_S/0/1/0/all/0/1\">Simon Poon</a>",
          "description": "The insights revealed from process mining heavily rely on the quality of\nevent logs. Activities extracted from healthcare information systems with the\nfree-text nature may lead to inconsistent labels. Such inconsistency would then\nlead to redundancy of activity labels, which refer to labels that have\ndifferent syntax but share the same behaviours. The identifications of these\nlabels from data-driven process discovery are difficult and rely heavily on\nresource-intensive human review. Existing work achieves low accuracy either\nredundant activity labels are in low occurrence frequency or the existence of\nnumerical data values as attributes in event logs. However, these phenomena are\ncommonly observed in healthcare information systems. In this paper, we propose\nan approach to detect redundant activity labels using control-flow relations\nand numerical data values from event logs. Natural Language Processing is also\nintegrated into our method to assess semantic similarity between labels, which\nprovides users with additional insights. We have evaluated our approach through\nsynthetic logs generated from the real-life Sepsis log and a case study using\nthe MIMIC-III data set. The results demonstrate that our approach can\nsuccessfully detect redundant activity labels. This approach can add value to\nthe preprocessing step to generate more representative event logs for process\nmining tasks in the healthcare domain.",
          "link": "http://arxiv.org/abs/2103.16061",
          "publishedOn": "2021-06-24T01:51:42.012Z",
          "wordCount": 677,
          "title": "A Novel Approach to Detect Redundant Activity Labels For More Representative Event Logs. (arXiv:2103.16061v2 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bingqing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1\">Jacopo Tagliabue</a>",
          "description": "Word embeddings (e.g., word2vec) have been applied successfully to eCommerce\nproducts through~\\textit{prod2vec}. Inspired by the recent performance\nimprovements on several NLP tasks brought by contextualized embeddings, we\npropose to transfer BERT-like architectures to eCommerce: our model --\n~\\textit{Prod2BERT} -- is trained to generate representations of products\nthrough masked session modeling. Through extensive experiments over multiple\nshops, different tasks, and a range of design choices, we systematically\ncompare the accuracy of~\\textit{Prod2BERT} and~\\textit{prod2vec} embeddings:\nwhile~\\textit{Prod2BERT} is found to be superior in several scenarios, we\nhighlight the importance of resources and hyperparameters in the best\nperforming models. Finally, we provide guidelines to practitioners for training\nembeddings under a variety of computational and data constraints.",
          "link": "http://arxiv.org/abs/2012.09807",
          "publishedOn": "2021-06-24T01:51:41.975Z",
          "wordCount": 589,
          "title": "BERT Goes Shopping: Comparing Distributional Models for Product Representations. (arXiv:2012.09807v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iana_A/0/1/0/all/0/1\">Andreea Iana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>",
          "description": "In today's academic publishing model, especially in Computer Science,\nconferences commonly constitute the main platforms for releasing the latest\npeer-reviewed advancements in their respective fields. However, choosing a\nsuitable academic venue for publishing one's research can represent a\nchallenging task considering the plethora of available conferences,\nparticularly for those at the start of their academic careers, or for those\nseeking to publish outside of their usual domain. In this paper, we propose\nGraphConfRec, a conference recommender system which combines SciGraph and graph\nneural networks, to infer suggestions based not only on title and abstract, but\nalso on co-authorship and citation relationships. GraphConfRec achieves a\nrecall@10 of up to 0.580 and a MAP of up to 0.336 with a graph attention\nnetwork-based recommendation model. A user study with 25 subjects supports the\npositive results.",
          "link": "http://arxiv.org/abs/2106.12340",
          "publishedOn": "2021-06-24T01:51:41.960Z",
          "wordCount": 577,
          "title": "GraphConfRec: A Graph Neural Network-Based Conference Recommender System. (arXiv:2106.12340v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boukhers_Z/0/1/0/all/0/1\">Zeyd Boukhers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayr_P/0/1/0/all/0/1\">Philipp Mayr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peroni_S/0/1/0/all/0/1\">Silvio Peroni</a>",
          "description": "Automatic processing of bibliographic data becomes very important in digital\nlibraries, data science and machine learning due to its importance in keeping\npace with the significant increase of published papers every year from one side\nand to the inherent challenges from the other side. This processing has several\naspects including but not limited to I) Automatic extraction of references from\nPDF documents, II) Building an accurate citation graph, III) Author name\ndisambiguation, etc. Bibliographic data is heterogeneous by nature and occurs\nin both structured (e.g. citation graph) and unstructured (e.g. publications)\nformats. Therefore, it requires data science and machine learning techniques to\nbe processed and analysed. Here we introduce BiblioDAP'21: The 1st Workshop on\nBibliographic Data Analysis and Processing.",
          "link": "http://arxiv.org/abs/2106.12320",
          "publishedOn": "2021-06-24T01:51:41.930Z",
          "wordCount": 572,
          "title": "BiblioDAP: The 1st Workshop on Bibliographic Data Analysis and Processing. (arXiv:2106.12320v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12460",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leonhardt_J/0/1/0/all/0/1\">Jurek Leonhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudra_K/0/1/0/all/0/1\">Koustav Rudra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Avishek Anand</a>",
          "description": "Machine learning models for the ad-hoc retrieval of documents and passages\nhave recently shown impressive improvements due to better language\nunderstanding using large pre-trained language models. However, these\nover-parameterized models are inherently non-interpretable and do not provide\nany information on the parts of the documents that were used to arrive at a\ncertain prediction.\n\nIn this paper we introduce the select and rank paradigm for document ranking,\nwhere interpretability is explicitly ensured when scoring longer documents.\nSpecifically, we first select sentences in a document based on the input query\nand then predict the query-document score based only on the selected sentences,\nacting as an explanation. We treat sentence selection as a latent variable\ntrained jointly with the ranker from the final output. We conduct extensive\nexperiments to demonstrate that our inherently interpretable select-and-rank\napproach is competitive in comparison to other state-of-the-art methods and\nsometimes even outperforms them. This is due to our novel end-to-end training\napproach based on weighted reservoir sampling that manages to train the\nselector despite the stochastic sentence selection. We also show that our\nsentence selection approach can be used to provide explanations for models that\noperate on only parts of the document, such as BERT.",
          "link": "http://arxiv.org/abs/2106.12460",
          "publishedOn": "2021-06-24T01:51:41.910Z",
          "wordCount": 626,
          "title": "Learnt Sparsity for Effective and Interpretable Document Ranking. (arXiv:2106.12460v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulyono_H/0/1/0/all/0/1\">Hermawan Mulyono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1\">Desmond Chan</a>",
          "description": "Climate change has largely impacted our daily lives. As one of its\nconsequences, we are experiencing more wildfires. In the year 2020, wildfires\nburned a record number of 8,888,297 acres in the US. To awaken people's\nattention to climate change, and to visualize the current risk of wildfires, We\ndeveloped RtFPS, \"Real-Time Fire Prediction System\". It provides a real-time\nprediction visualization of wildfire risk at specific locations base on a\nMachine Learning model. It also provides interactive map features that show the\nhistorical wildfire events with environmental info.",
          "link": "http://arxiv.org/abs/2105.10880",
          "publishedOn": "2021-06-23T01:48:37.718Z",
          "wordCount": 577,
          "title": "RtFPS: An Interactive Map that Visualizes and Predicts Wildfires in the US. (arXiv:2105.10880v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1\">Anastasios Nentidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1\">Anastasia Krithara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1\">Georgios Paliouras</a>",
          "description": "The Medical Subject Headings (MeSH) thesaurus is a controlled vocabulary\nwidely used in biomedical knowledge systems, particularly for semantic indexing\nof scientific literature. As the MeSH hierarchy evolves through annual version\nupdates, some new descriptors are introduced that were not previously\navailable. This paper explores the conceptual provenance of these new\ndescriptors. In particular, we investigate whether such new descriptors have\nbeen previously covered by older descriptors and what is their current relation\nto them. To this end, we propose a framework to categorize new descriptors\nbased on their current relation to older descriptors. Based on the proposed\nclassification scheme, we quantify, analyse and present the different types of\nnew descriptors introduced in MeSH during the last fifteen years. The results\nshow that only about 25% of new MeSH descriptors correspond to new emerging\nconcepts, whereas the rest were previously covered by one or more existing\ndescriptors, either implicitly or explicitly. Most of them were covered by a\nsingle existing descriptor and they usually end up as descendants of it in the\ncurrent hierarchy, gradually leading towards a more fine-grained MeSH\nvocabulary. These insights about the dynamics of the thesaurus are useful for\nthe retrospective study of scientific articles annotated with MeSH, but could\nalso be used to inform the policy of updating the thesaurus in the future.",
          "link": "http://arxiv.org/abs/2101.08293",
          "publishedOn": "2021-06-23T01:48:37.688Z",
          "wordCount": 706,
          "title": "What is all this new MeSH about? Exploring the semantic provenance of new descriptors in the MeSH thesaurus. (arXiv:2101.08293v2 [cs.DL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11534",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yinyu Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Sha Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhou Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_W/0/1/0/all/0/1\">Wendy Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>",
          "description": "The Turing Award is recognized as the most influential and prestigious award\nin the field of computer science(CS). With the rise of the science of science\n(SciSci), a large amount of bibliographic data has been analyzed in an attempt\nto understand the hidden mechanism of scientific evolution. These include the\nanalysis of the Nobel Prize, including physics, chemistry, medicine, etc. In\nthis article, we extract and analyze the data of 72 Turing Award laureates from\nthe complete bibliographic data, fill the gap in the lack of Turing Award\nanalysis, and discover the development characteristics of computer science as\nan independent discipline. First, we show most Turing Award laureates have\nlong-term and high-quality educational backgrounds, and more than 61% of them\nhave a degree in mathematics, which indicates that mathematics has played a\nsignificant role in the development of computer science. Secondly, the data\nshows that not all scholars have high productivity and high h-index; that is,\nthe number of publications and h-index is not the leading indicator for\nevaluating the Turing Award. Third, the average age of awardees has increased\nfrom 40 to around 70 in recent years. This may be because new breakthroughs\ntake longer, and some new technologies need time to prove their influence.\nBesides, we have also found that in the past ten years, international\ncollaboration has experienced explosive growth, showing a new paradigm in the\nform of collaboration. It is also worth noting that in recent years, the\nemergence of female winners has also been eye-catching. Finally, by analyzing\nthe personal publication records, we find that many people are more likely to\npublish high-impact articles during their high-yield periods.",
          "link": "http://arxiv.org/abs/2106.11534",
          "publishedOn": "2021-06-23T01:48:37.677Z",
          "wordCount": 724,
          "title": "Turing Award elites revisited: patterns of productivity, collaboration, authorship and impact. (arXiv:2106.11534v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mokrii_I/0/1/0/all/0/1\">Iurii Mokrii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boytsov_L/0/1/0/all/0/1\">Leonid Boytsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braslavski_P/0/1/0/all/0/1\">Pavel Braslavski</a>",
          "description": "Due to high annotation costs making the best use of existing human-created\ntraining data is an important research direction. We, therefore, carry out a\nsystematic evaluation of transferability of BERT-based neural ranking models\nacross five English datasets. Previous studies focused primarily on zero-shot\nand few-shot transfer from a large dataset to a dataset with a small number of\nqueries. In contrast, each of our collections has a substantial number of\nqueries, which enables a full-shot evaluation mode and improves reliability of\nour results. Furthermore, since source datasets licences often prohibit\ncommercial use, we compare transfer learning to training on pseudo-labels\ngenerated by a BM25 scorer. We find that training on pseudo-labels -- possibly\nwith subsequent fine-tuning using a modest number of annotated queries -- can\nproduce a competitive or better model compared to transfer learning. Yet, it is\nnecessary to improve the stability and/or effectiveness of the few-shot\ntraining, which, sometimes, can degrade performance of a pretrained model.",
          "link": "http://arxiv.org/abs/2103.03335",
          "publishedOn": "2021-06-23T01:48:37.666Z",
          "wordCount": 653,
          "title": "A Systematic Evaluation of Transfer Learning and Pseudo-labeling with BERT-based Ranking Models. (arXiv:2103.03335v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>",
          "description": "Place Recognition is a crucial capability for mobile robot localization and\nnavigation. Image-based or Visual Place Recognition (VPR) is a challenging\nproblem as scene appearance and camera viewpoint can change significantly when\nplaces are revisited. Recent VPR methods based on ``sequential\nrepresentations'' have shown promising results as compared to traditional\nsequence score aggregation or single image based techniques. In parallel to\nthese endeavors, 3D point clouds based place recognition is also being explored\nfollowing the advances in deep learning based point cloud processing. However,\na key question remains: is an explicit 3D structure based place representation\nalways superior to an implicit ``spatial'' representation based on sequence of\nRGB images which can inherently learn scene structure. In this extended\nabstract, we attempt to compare these two types of methods by considering a\nsimilar ``metric span'' to represent places. We compare a 3D point cloud based\nmethod (PointNetVLAD) with image sequence based methods (SeqNet and others) and\nshowcase that image sequence based techniques approach, and can even surpass,\nthe performance achieved by point cloud based methods for a given metric span.\nThese performance variations can be attributed to differences in data richness\nof input sensors as well as data accumulation strategies for a mobile robot.\nWhile a perfect apple-to-apple comparison may not be feasible for these two\ndifferent modalities, the presented comparison takes a step in the direction of\nanswering deeper questions regarding spatial representations, relevant to\nseveral applications like Autonomous Driving and Augmented/Virtual Reality.\nSource code available publicly https://github.com/oravus/seqNet.",
          "link": "http://arxiv.org/abs/2106.11481",
          "publishedOn": "2021-06-23T01:48:37.598Z",
          "wordCount": 722,
          "title": "SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for Day-Night Place Recognition. (arXiv:2106.11481v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stankevicius_L/0/1/0/all/0/1\">Lukas Stankevi&#x10d;ius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukosevicius_M/0/1/0/all/0/1\">Mantas Luko&#x161;evi&#x10d;ius</a>",
          "description": "In this work, we train the first monolingual Lithuanian transformer model on\na relatively large corpus of Lithuanian news articles and compare various\noutput decoding algorithms for abstractive news summarization. We achieve an\naverage ROUGE-2 score 0.163, generated summaries are coherent and look\nimpressive at first glance. However, some of them contain misleading\ninformation that is not so easy to spot. We describe all the technical details\nand share our trained model and accompanying code in an online open-source\nrepository, as well as some characteristic samples of the generated summaries.",
          "link": "http://arxiv.org/abs/2105.03279",
          "publishedOn": "2021-06-23T01:48:37.576Z",
          "wordCount": 569,
          "title": "Generating abstractive summaries of Lithuanian news articles using a transformer model. (arXiv:2105.03279v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11403",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yefeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yunpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>",
          "description": "Objective: The objective of this study is to develop a deep learning pipeline\nto detect signals on dietary supplement-related adverse events (DS AEs) from\nTwitter. Material and Methods: We obtained 247,807 tweets ranging from 2012 to\n2018 that mentioned both DS and AE. We annotated biomedical entities and\nrelations on 2,000 randomly selected tweets. For the concept extraction task,\nwe compared the performance of traditional word embeddings with SVM, CRF and\nLSTM-CRF classifiers to BERT models. For the relation extraction task, we\ncompared GloVe vectors with CNN classifiers to BERT models. We chose the best\nperforming models in each task to assemble an end-to-end deep learning pipeline\nto detect DS AE signals and compared the results to the known DS AEs from a DS\nknowledge base (i.e., iDISK). Results: In both tasks, the BERT-based models\noutperformed traditional word embeddings. The best performing concept\nextraction model is the BioBERT model that can identify supplement, symptom,\nand body organ entities with F1-scores of 0.8646, 0.8497, and 0.7104,\nrespectively. The best performing relation extraction model is the BERT model\nthat can identify purpose and AE relations with F1-scores of 0.8335 and 0.7538,\nrespectively. The end-to-end pipeline was able to extract DS indication and DS\nAEs with an F1-score of 0.7459 and 0,7414, respectively. Comparing to the\niDISK, we could find both known and novel DS-AEs. Conclusion: We have\ndemonstrated the feasibility of detecting DS AE signals from Twitter with a\nBioBERT-based deep learning pipeline.",
          "link": "http://arxiv.org/abs/2106.11403",
          "publishedOn": "2021-06-23T01:48:37.549Z",
          "wordCount": 691,
          "title": "Deep Learning Models in Detection of Dietary Supplement Adverse Event Signals from Twitter. (arXiv:2106.11403v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>",
          "description": "Topic modeling is an unsupervised method for revealing the hidden semantic\nstructure of a corpus. It has been increasingly widely adopted as a tool in the\nsocial sciences, including political science, digital humanities and\nsociological research in general. One desirable property of topic models is to\nallow users to find topics describing a specific aspect of the corpus. A\npossible solution is to incorporate domain-specific knowledge into topic\nmodeling, but this requires a specification from domain experts. We propose a\nnovel query-driven topic model that allows users to specify a simple query in\nwords or phrases and return query-related topics, thus avoiding tedious work\nfrom domain experts. Our proposed approach is particularly attractive when the\nuser-specified query has a low occurrence in a text corpus, making it difficult\nfor traditional topic models built on word cooccurrence patterns to identify\nrelevant topics. Experimental results demonstrate the effectiveness of our\nmodel in comparison with both classical topic models and neural topic models.",
          "link": "http://arxiv.org/abs/2106.07346",
          "publishedOn": "2021-06-23T01:48:37.521Z",
          "wordCount": 611,
          "title": "A Query-Driven Topic Model. (arXiv:2106.07346v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1\">Baban Gain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1\">Dibyanayan Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1\">Tanik Saikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>",
          "description": "Natural Language Processing (NLP) and Information Retrieval (IR) in the\njudicial domain is an essential task. With the advent of availability\ndomain-specific data in electronic form and aid of different Artificial\nintelligence (AI) technologies, automated language processing becomes more\ncomfortable, and hence it becomes feasible for researchers and developers to\nprovide various automated tools to the legal community to reduce human burden.\nThe Competition on Legal Information Extraction/Entailment (COLIEE-2019) run in\nassociation with the International Conference on Artificial Intelligence and\nLaw (ICAIL)-2019 has come up with few challenging tasks. The shared defined\nfour sub-tasks (i.e. Task1, Task2, Task3 and Task4), which will be able to\nprovide few automated systems to the judicial system. The paper presents our\nworking note on the experiments carried out as a part of our participation in\nall the sub-tasks defined in this shared task. We make use of different\nInformation Retrieval(IR) and deep learning based approaches to tackle these\nproblems. We obtain encouraging results in all these four sub-tasks.",
          "link": "http://arxiv.org/abs/2104.08653",
          "publishedOn": "2021-06-23T01:48:37.499Z",
          "wordCount": 649,
          "title": "IITP@COLIEE 2019: Legal Information Retrieval using BM25 and BERT. (arXiv:2104.08653v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kunwoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhufeng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>",
          "description": "Understanding who blames or supports whom in news text is a critical research\nquestion in computational social science. Traditional methods and datasets for\nsentiment analysis are, however, not suitable for the domain of political text\nas they do not consider the direction of sentiments expressed between entities.\nIn this paper, we propose a novel NLP task of identifying directed sentiment\nrelationship between political entities from a given news document, which we\ncall directed sentiment extraction. From a million-scale news corpus, we\nconstruct a dataset of news sentences where sentiment relations of political\nentities are manually annotated. We present a simple but effective approach for\nutilizing a pretrained transformer, which infers the target class by predicting\nmultiple question-answering tasks and combining the outcomes. We demonstrate\nthe utility of our proposed method for social science research questions by\nanalyzing positive and negative opinions between political entities in two\nmajor events: 2016 U.S. presidential election and COVID-19. The newly proposed\nproblem, data, and method will facilitate future studies on interdisciplinary\nNLP methods and applications.",
          "link": "http://arxiv.org/abs/2106.01033",
          "publishedOn": "2021-06-23T01:48:37.480Z",
          "wordCount": 698,
          "title": "Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment Extraction in News Text. (arXiv:2106.01033v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11846",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Wright_A/0/1/0/all/0/1\">Austin P Wright</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Ziems_C/0/1/0/all/0/1\">Caleb Ziems</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Park_H/0/1/0/all/0/1\">Haekyu Park</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Saad_Falcon_J/0/1/0/all/0/1\">Jon Saad-Falcon</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Tomprou_M/0/1/0/all/0/1\">Maria Tomprou</a>",
          "description": "As job markets worldwide have become more competitive and applicant selection\ncriteria have become more opaque, and different (and sometimes contradictory)\ninformation and advice is available for job seekers wishing to progress in\ntheir careers, it has never been more difficult to determine which factors in a\nr\\'esum\\'e most effectively help career progression. In this work we present a\nnovel, large scale dataset of over half a million r\\'esum\\'es with preliminary\nanalysis to begin to answer empirically which factors help or hurt people\nwishing to transition to more senior roles as they progress in their career. We\nfind that previous experience forms the most important factor, outweighing\nother aspects of human capital, and find which language factors in a r\\'esum\\'e\nhave significant effects. This lays the groundwork for future inquiry in career\ntrajectories using large scale data analysis and natural language processing\ntechniques.",
          "link": "http://arxiv.org/abs/2106.11846",
          "publishedOn": "2021-06-23T01:48:37.435Z",
          "wordCount": 618,
          "title": "Quantifying the Impact of Human Capital, Job History, and Language Factors on Job Seniority with a Large-scale Analysis of Resumes. (arXiv:2106.11846v1 [econ.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2002.02712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Greiner_Petter_A/0/1/0/all/0/1\">Andre Greiner-Petter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubotz_M/0/1/0/all/0/1\">Moritz Schubotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_F/0/1/0/all/0/1\">Fabian Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breitinger_C/0/1/0/all/0/1\">Corinna Breitinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohl_H/0/1/0/all/0/1\">Howard S. Cohl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1\">Akiko Aizawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>",
          "description": "Mathematical notation, i.e., the writing system used to communicate concepts\nin mathematics, encodes valuable information for a variety of information\nsearch and retrieval systems. Yet, mathematical notations remain mostly\nunutilized by today's systems. In this paper, we present the first in-depth\nstudy on the distributions of mathematical notation in two large scientific\ncorpora: the open access arXiv (2.5B mathematical objects) and the mathematical\nreviewing service for pure and applied mathematics zbMATH (61M mathematical\nobjects). Our study lays a foundation for future research projects on\nmathematical information retrieval for large scientific corpora. Further, we\ndemonstrate the relevance of our results to a variety of use-cases. For\nexample, to assist semantic extraction systems, to improve scientific search\nengines, and to facilitate specialized math recommendation systems. The\ncontributions of our presented research are as follows: (1) we present the\nfirst distributional analysis of mathematical formulae on arXiv and zbMATH; (2)\nwe retrieve relevant mathematical objects for given textual search queries\n(e.g., linking $P_{n}^{(\\alpha, \\beta)}\\!\\left(x\\right)$ with `Jacobi\npolynomial'); (3) we extend zbMATH's search engine by providing relevant\nmathematical formulae; and (4) we exemplify the applicability of the results by\npresenting auto-completion for math inputs as the first contribution to math\nrecommendation systems. To expedite future research projects, we have made\navailable our source code and data.",
          "link": "http://arxiv.org/abs/2002.02712",
          "publishedOn": "2021-06-23T01:48:37.249Z",
          "wordCount": 715,
          "title": "Discovering Mathematical Objects of Interest -- A Study of Mathematical Notations. (arXiv:2002.02712v3 [cs.DL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siriwardhana_S/0/1/0/all/0/1\">Shamane Siriwardhana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weerasekera_R/0/1/0/all/0/1\">Rivindu Weerasekera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_E/0/1/0/all/0/1\">Elliott Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanayakkara_S/0/1/0/all/0/1\">Suranga Nanayakkara</a>",
          "description": "In this paper, we illustrate how to fine-tune the entire Retrieval Augment\nGeneration (RAG) architecture in an end-to-end manner. We highlighted the main\nengineering challenges that needed to be addressed to achieve this objective.\nWe also compare how end-to-end RAG architecture outperforms the original RAG\narchitecture for the task of question answering. We have open-sourced our\nimplementation in the HuggingFace Transformers library.",
          "link": "http://arxiv.org/abs/2106.11517",
          "publishedOn": "2021-06-23T01:48:37.203Z",
          "wordCount": 509,
          "title": "Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering. (arXiv:2106.11517v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zihan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Low_C/0/1/0/all/0/1\">Charles Low</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_M/0/1/0/all/0/1\">Mengqiu Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1\">Daniel E. Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krass_M/0/1/0/all/0/1\">Mark S. Krass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grabmair_M/0/1/0/all/0/1\">Matthias Grabmair</a>",
          "description": "Lawyers and judges spend a large amount of time researching the proper legal\nauthority to cite while drafting decisions. In this paper, we develop a\ncitation recommendation tool that can help improve efficiency in the process of\nopinion drafting. We train four types of machine learning models, including a\ncitation-list based method (collaborative filtering) and three context-based\nmethods (text similarity, BiLSTM and RoBERTa classifiers). Our experiments show\nthat leveraging local textual context improves recommendation, and that deep\nneural models achieve decent performance. We show that non-deep text-based\nmethods benefit from access to structured case metadata, but deep models only\nbenefit from such access when predicting from context of insufficient length.\nWe also find that, even after extensive training, RoBERTa does not outperform a\nrecurrent neural model, despite its benefits of pretraining. Our behavior\nanalysis of the RoBERTa model further shows that predictive performance is\nstable across time and citation classes.",
          "link": "http://arxiv.org/abs/2106.10776",
          "publishedOn": "2021-06-22T01:57:09.190Z",
          "wordCount": 615,
          "title": "Context-Aware Legal Citation Recommendation using Deep Learning. (arXiv:2106.10776v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04494",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiayan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anjum_A/0/1/0/all/0/1\">Ashiq Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panneerselvam_J/0/1/0/all/0/1\">John Panneerselvam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>",
          "description": "With the development of Edge Computing and Artificial Intelligence (AI)\ntechnologies, edge devices are witnessed to generate data at unprecedented\nvolume. The Edge Intelligence (EI) has led to the emergence of edge devices in\nvarious application domains. The EI can provide efficient services to\ndelay-sensitive applications, where the edge devices are deployed as edge nodes\nto host the majority of execution, which can effectively manage services and\nimprove service discovery efficiency. The multilevel index model is a\nwell-known model used for indexing service, such a model is being introduced\nand optimized in the edge environments to efficiently services discovery whilst\nmanaging large volumes of data. However, effectively updating the multilevel\nindex model by adding new services timely and precisely in the dynamic Edge\nComputing environments is still a challenge. Addressing this issue, this paper\nproposes a designated key selection method to improve the efficiency of adding\nservices in the multilevel index models. Our experimental results show that in\nthe partial index and the full index of multilevel index model, our method\nreduces the service addition time by around 84% and 76%, respectively when\ncompared with the original key selection method and by around 78% and 66%,\nrespectively when compared with the random selection method. Our proposed\nmethod significantly improves the service addition efficiency in the multilevel\nindex model, when compared with existing state-of-the-art key selection\nmethods, without compromising the service retrieval stability to any notable\nlevel.",
          "link": "http://arxiv.org/abs/2106.04494",
          "publishedOn": "2021-06-22T01:57:09.125Z",
          "wordCount": 693,
          "title": "Optimization of Service Addition in Multilevel Index Model for Edge Computing. (arXiv:2106.04494v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abu_Rasheed_H/0/1/0/all/0/1\">Hasan Abu-Rasheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Christian Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zenkert_J/0/1/0/all/0/1\">Johannes Zenkert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krumm_R/0/1/0/all/0/1\">Roland Krumm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathi_M/0/1/0/all/0/1\">Madjid Fathi</a>",
          "description": "Industrial processes produce a considerable volume of data and thus\ninformation. Whether it is structured sensory data or semi- to unstructured\ntextual data, the knowledge that can be derived from it is critical to the\nsustainable development of the industrial process. A key challenge of this\nsustainability is the intelligent management of the generated data, as well as\nthe knowledge extracted from it, in order to utilize this knowledge for\nimproving future procedures. This challenge is a result of the tailored\ndocumentation methods and domain-specific requirements, which include the need\nfor quick visibility of the documented knowledge. In this paper, we utilize the\nexpert knowledge documented in chip-design failure reports in supporting user\naccess to information that is relevant to a current chip design. Unstructured,\nfree, textual data in previous failure documentations provides a valuable\nsource of lessons-learned, which expert design-engineers have experienced,\nsolved and documented. To achieve a sustainable utilization of knowledge within\nthe company, not only the inherent knowledge has to be mined from unstructured\ntextual data, but also the relations between the lessons-learned, uncovering\npotentially unknown links. In this research, a knowledge graph is constructed,\nin order to represent and use the interconnections between reported design\nfailures. A search engine is developed and applied onto the graph to answer\nqueries. In contrast to mere keyword-based searching, the searchability of the\nknowledge graph offers enhanced search results beyond direct matches and acts\nas a mean for generating explainable results and result recommendations.\nResults are provided to the design engineer through an interactive search\ninterface, in which, the feedback from the user is used to further optimize\nrelations for future iterations of the knowledge graph.",
          "link": "http://arxiv.org/abs/2105.08442",
          "publishedOn": "2021-06-22T01:57:09.070Z",
          "wordCount": 751,
          "title": "Explainable Graph-based Search for Lessons-Learned Documents in the Semiconductor Industry. (arXiv:2105.08442v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wenga_C/0/1/0/all/0/1\">Carmel Wenga</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Fansi_M/0/1/0/all/0/1\">Majirus Fansi</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Chabrier_S/0/1/0/all/0/1\">S&#xe9;bastien Chabrier</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Mari_J/0/1/0/all/0/1\">Jean-Martial Mari</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gabillon_A/0/1/0/all/0/1\">Alban Gabillon</a> (1) ((1) University of French Polynesia, (2) NzhinuSoft)",
          "description": "Over the past two decades, recommender systems have attracted a lot of\ninterest due to the explosion in the amount of data in online applications. A\nparticular attention has been paid to collaborative filtering, which is the\nmost widely used in applications that involve information recommendations.\nCollaborative filtering (CF) uses the known preference of a group of users to\nmake predictions and recommendations about the unknown preferences of other\nusers (recommendations are made based on the past behavior of users). First\nintroduced in the 1990s, a wide variety of increasingly successful models have\nbeen proposed. Due to the success of machine learning techniques in many areas,\nthere has been a growing emphasis on the application of such algorithms in\nrecommendation systems. In this article, we present an overview of the CF\napproaches for recommender systems, their two main categories, and their\nevaluation metrics. We focus on the application of classical Machine Learning\nalgorithms to CF recommender systems by presenting their evolution from their\nfirst use-cases to advanced Machine Learning models. We attempt to provide a\ncomprehensive and comparative overview of CF systems (with python\nimplementations) that can serve as a guideline for research and practice in\nthis area.",
          "link": "http://arxiv.org/abs/2106.10679",
          "publishedOn": "2021-06-22T01:57:08.778Z",
          "wordCount": 663,
          "title": "A Comprehensive Review on Non-Neural Networks Collaborative Filtering Recommendation Systems. (arXiv:2106.10679v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuqiu Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>",
          "description": "Online gaming is growing faster than ever before, with increasing challenges\nof providing better user experience. Recommender systems (RS) for online games\nface unique challenges since they must fulfill players' distinct desires, at\ndifferent user levels, based on their action sequences of various action types.\nAlthough many sequential RS already exist, they are mainly single-sequence,\nsingle-task, and single-user-level. In this paper, we introduce a new\nsequential recommendation model for multiple sequences, multiple tasks, and\nmultiple user levels (abbreviated as M$^3$Rec) in Tencent Games platform, which\ncan fully utilize complex data in online games. We leverage Graph Neural\nNetwork and multi-task learning to design M$^3$Rec in order to model the\ncomplex information in the heterogeneous sequential recommendation scenario of\nTencent Games. We verify the effectiveness of M$^3$Rec on three online games of\nTencent Games platform, in both offline and online evaluations. The results\nshow that M$^3$Rec successfully addresses the challenges of recommendation in\nonline games, and it generates superior recommendations compared with\nstate-of-the-art sequential recommendation approaches.",
          "link": "http://arxiv.org/abs/2102.06950",
          "publishedOn": "2021-06-22T01:57:08.754Z",
          "wordCount": 642,
          "title": "Sequential Recommendation in Online Games with Multiple Sequences, Tasks and User Levels. (arXiv:2102.06950v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiapeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1\">Guozhi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Weihong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1\">Kai Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yichao Huang</a>",
          "description": "Visual information extraction (VIE) has attracted increasing attention in\nrecent years. The existing methods usually first organized optical character\nrecognition (OCR) results into plain texts and then utilized token-level entity\nannotations as supervision to train a sequence tagging model. However, it\nexpends great annotation costs and may be exposed to label confusion, and the\nOCR errors will also significantly affect the final performance. In this paper,\nwe propose a unified weakly-supervised learning framework called TCPN (Tag,\nCopy or Predict Network), which introduces 1) an efficient encoder to\nsimultaneously model the semantic and layout information in 2D OCR results; 2)\na weakly-supervised training strategy that utilizes only key information\nsequences as supervision; and 3) a flexible and switchable decoder which\ncontains two inference modes: one (Copy or Predict Mode) is to output key\ninformation sequences of different categories by copying a token from the input\nor predicting one in each time step, and the other (Tag Mode) is to directly\ntag the input sequence in a single forward pass. Our method shows new\nstate-of-the-art performance on several public benchmarks, which fully proves\nits effectiveness.",
          "link": "http://arxiv.org/abs/2106.10681",
          "publishedOn": "2021-06-22T01:57:08.719Z",
          "wordCount": 656,
          "title": "Tag, Copy or Predict: A Unified Weakly-Supervised Learning Framework for Visual Information Extraction using Sequences. (arXiv:2106.10681v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10547",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahapatra_C/0/1/0/all/0/1\">Chirag Mahapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellare_K/0/1/0/all/0/1\">Kedar Bellare</a>",
          "description": "Income verification is the problem of validating a person's stated income\ngiven basic identity information such as name, location, job title and\nemployer. It is widely used in the context of mortgage lending, rental\napplications and other financial risk models. However, the current processes\nsurrounding verification involve significant human effort and document\ngathering which can be both time-consuming and expensive. In this paper, we\npropose a novel model for verifying an individual's income given very limited\nidentity information typically available in loan applications. Our model is a\ncombination of a deep neural network and hand-engineered features. The hand\nengineered features are based upon matching the input information against\nincome records extracted automatically from various publicly available online\nsources (e.g. payscale.com, H-1B filings, government employee salaries). We\nconduct experiments on two data sets, one simulated from H-1B records and the\nother from a real-world data set of peer-to-peer (P2P) loan applications\nobtained from one of the world's largest P2P lending platform. Our results show\na significant reduction in error of 3-6% relative to several strong baselines.\nWe also perform ablation studies to demonstrate that a combined model is indeed\nnecessary to achieve state-of-the-art performance on this task.",
          "link": "http://arxiv.org/abs/2106.10547",
          "publishedOn": "2021-06-22T01:57:08.706Z",
          "wordCount": 622,
          "title": "Leveraging Multiple Online Sources for Accurate Income Verification. (arXiv:2106.10547v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Ruoming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi Liu</a>",
          "description": "Recently, Rendle has warned that the use of sampling-based top-$k$ metrics\nmight not suffice. This throws a number of recent studies on deep\nlearning-based recommendation algorithms, and classic non-deep-learning\nalgorithms using such a metric, into jeopardy. In this work, we thoroughly\ninvestigate the relationship between the sampling and global top-$K$ Hit-Ratio\n(HR, or Recall), originally proposed by Koren[2] and extensively used by\nothers. By formulating the problem of aligning sampling top-$k$ ($SHR@k$) and\nglobal top-$K$ ($HR@K$) Hit-Ratios through a mapping function $f$, so that\n$SHR@k\\approx HR@f(k)$, we demonstrate both theoretically and experimentally\nthat the sampling top-$k$ Hit-Ratio provides an accurate approximation of its\nglobal (exact) counterpart, and can consistently predict the correct winners\n(the same as indicate by their corresponding global Hit-Ratios).",
          "link": "http://arxiv.org/abs/2106.10621",
          "publishedOn": "2021-06-22T01:57:08.693Z",
          "wordCount": 551,
          "title": "On Sampling Top-K Recommendation Evaluation. (arXiv:2106.10621v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>",
          "description": "Assessing an AI agent that can converse in human language and understand\nvisual content is challenging. Generation metrics, such as BLEU scores favor\ncorrect syntax over semantics. Hence a discriminative approach is often used,\nwhere an agent ranks a set of candidate options. The mean reciprocal rank (MRR)\nmetric evaluates the model performance by taking into account the rank of a\nsingle human-derived answer. This approach, however, raises a new challenge:\nthe ambiguity and synonymy of answers, for instance, semantic equivalence\n(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative\ngain (NDCG) metric has been used to capture the relevance of all the correct\nanswers via dense annotations. However, the NDCG metric favors the usually\napplicable uncertain answers such as `I don't know. Crafting a model that\nexcels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should\nanswer a human-like reply and validate the correctness of any answer. To\naddress this issue, we describe a two-step non-parametric ranking approach that\ncan merge strong MRR and NDCG models. Using our approach, we manage to keep\nmost MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG\nstate-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won\nthe recent Visual Dialog 2020 challenge. Source code is available at\nhttps://github.com/idansc/mrr-ndcg.",
          "link": "http://arxiv.org/abs/2104.07511",
          "publishedOn": "2021-06-22T01:57:08.672Z",
          "wordCount": 688,
          "title": "Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mysore_S/0/1/0/all/0/1\">Sheshera Mysore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OGorman_T/0/1/0/all/0/1\">Tim O&#x27;Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>",
          "description": "Query by Example is a well-known information retrieval task in which a\ndocument is chosen by the user as the search query and the goal is to retrieve\nrelevant documents from a large collection. However, a document often covers\nmultiple aspects of a topic. To address this scenario we introduce the task of\nfaceted Query by Example in which users can also specify a finer grained aspect\nin addition to the input query document. We focus on the application of this\ntask in scientific literature search. We envision models which are able to\nretrieve scientific papers analogous to a query scientific paper along\nspecifically chosen rhetorical structure elements as one solution to this\nproblem. In this work, the rhetorical structure elements, which we refer to as\nfacets, indicate backgrounds, methods, or results of a scientific paper. We\nintroduce and describe an expert annotated test collection to evaluate models\ntrained to perform this task. Our test collection consists of a diverse set of\n50 query documents, drawn from computational linguistics and machine learning\nvenues. We carefully followed the annotation guideline used by TREC for depth-k\npooling (k = 100 or 250) and the resulting data collection consists of graded\nrelevance scores with high annotation agreement. The data is freely available\nfor research purposes.",
          "link": "http://arxiv.org/abs/2103.12906",
          "publishedOn": "2021-06-21T02:07:36.688Z",
          "wordCount": 700,
          "title": "CSFCube -- A Test Collection of Computer Science Research Articles for Faceted Query by Example. (arXiv:2103.12906v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiancan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fuli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1\">Jianxun Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>",
          "description": "Representation learning on user-item graph for recommendation has evolved\nfrom using single ID or interaction history to exploiting higher-order\nneighbors. This leads to the success of graph convolution networks (GCNs) for\nrecommendation such as PinSage and LightGCN. Despite effectiveness, we argue\nthat they suffer from two limitations: (1) high-degree nodes exert larger\nimpact on the representation learning, deteriorating the recommendations of\nlow-degree (long-tail) items; and (2) representations are vulnerable to noisy\ninteractions, as the neighborhood aggregation scheme further enlarges the\nimpact of observed edges.\n\nIn this work, we explore self-supervised learning on user-item graph, so as\nto improve the accuracy and robustness of GCNs for recommendation. The idea is\nto supplement the classical supervised task of recommendation with an auxiliary\nself-supervised task, which reinforces node representation learning via\nself-discrimination. Specifically, we generate multiple views of a node,\nmaximizing the agreement between different views of the same node compared to\nthat of other nodes. We devise three operators to generate the views -- node\ndropout, edge dropout, and random walk -- that change the graph structure in\ndifferent manners. We term this new learning paradigm as\n\\textit{Self-supervised Graph Learning} (SGL), implementing it on the\nstate-of-the-art model LightGCN. Through theoretical analyses, we find that SGL\nhas the ability of automatically mining hard negatives. Empirical studies on\nthree benchmark datasets demonstrate the effectiveness of SGL, which improves\nthe recommendation accuracy, especially on long-tail items, and the robustness\nagainst interaction noises. Our implementations are available at\n\\url{https://github.com/wujcan/SGL}.",
          "link": "http://arxiv.org/abs/2010.10783",
          "publishedOn": "2021-06-21T02:07:36.677Z",
          "wordCount": 740,
          "title": "Self-supervised Graph Learning for Recommendation. (arXiv:2010.10783v4 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yixing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Pre-training and fine-tuning have achieved remarkable success in many\ndownstream natural language processing (NLP) tasks. Recently, pre-training\nmethods tailored for information retrieval (IR) have also been explored, and\nthe latest success is the PROP method which has reached new SOTA on a variety\nof ad-hoc retrieval benchmarks. The basic idea of PROP is to construct the\n\\textit{representative words prediction} (ROP) task for pre-training inspired\nby the query likelihood model. Despite its exciting performance, the\neffectiveness of PROP might be bounded by the classical unigram language model\nadopted in the ROP task construction process. To tackle this problem, we\npropose a bootstrapped pre-training method (namely B-PROP) based on BERT for\nad-hoc retrieval. The key idea is to use the powerful contextual language model\nBERT to replace the classical unigram language model for the ROP task\nconstruction, and re-train BERT itself towards the tailored objective for IR.\nSpecifically, we introduce a novel contrastive method, inspired by the\ndivergence-from-randomness idea, to leverage BERT's self-attention mechanism to\nsample representative words from the document. By further fine-tuning on\ndownstream ad-hoc retrieval tasks, our method achieves significant improvements\nover baselines without pre-training or with other pre-training methods, and\nfurther pushes forward the SOTA on a variety of ad-hoc retrieval tasks.",
          "link": "http://arxiv.org/abs/2104.09791",
          "publishedOn": "2021-06-21T02:07:36.665Z",
          "wordCount": 696,
          "title": "B-PROP: Bootstrapped Pre-training with Representative Words Prediction for Ad-hoc Retrieval. (arXiv:2104.09791v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yi-Ling Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yu-Che Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng-Te Li</a>",
          "description": "Financial technology (FinTech) has drawn much attention among investors and\ncompanies. While conventional stock analysis in FinTech targets at predicting\nstock prices, less effort is made for profitable stock recommendation. Besides,\nin existing approaches on modeling time series of stock prices, the\nrelationships among stocks and sectors (i.e., categories of stocks) are either\nneglected or pre-defined. Ignoring stock relationships will miss the\ninformation shared between stocks while using pre-defined relationships cannot\ndepict the latent interactions or influence of stock prices between stocks. In\nthis work, we aim at recommending the top-K profitable stocks in terms of\nreturn ratio using time series of stock prices and sector information. We\npropose a novel deep learning-based model, Financial Graph Attention Networks\n(FinGAT), to tackle the task under the setting that no pre-defined\nrelationships between stocks are given. The idea of FinGAT is three-fold.\nFirst, we devise a hierarchical learning component to learn short-term and\nlong-term sequential patterns from stock time series. Second, a fully-connected\ngraph between stocks and a fully-connected graph between sectors are\nconstructed, along with graph attention networks, to learn the latent\ninteractions among stocks and sectors. Third, a multi-task objective is devised\nto jointly recommend the profitable stocks and predict the stock movement.\nExperiments conducted on Taiwan Stock, S&P 500, and NASDAQ datasets exhibit\nremarkable recommendation performance of our FinGAT, comparing to\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.10159",
          "publishedOn": "2021-06-21T02:07:36.627Z",
          "wordCount": 695,
          "title": "FinGAT: Financial Graph Attention Networks for Recommending Top-K Profitable Stocks. (arXiv:2106.10159v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eugene Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_D/0/1/0/all/0/1\">David D. Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieder_O/0/1/0/all/0/1\">Ophir Frieder</a>",
          "description": "Technology-assisted review (TAR) refers to human-in-the-loop active learning\nworkflows for finding relevant documents in large collections. These workflows\noften must meet a target for the proportion of relevant documents found (i.e.\nrecall) while also holding down costs. A variety of heuristic stopping rules\nhave been suggested for striking this tradeoff in particular settings, but none\nhave been tested against a range of recall targets and tasks. We propose two\nnew heuristic stopping rules, Quant and QuantCI based on model-based estimation\ntechniques from survey research. We compare them against a range of proposed\nheuristics and find they are accurate at hitting a range of recall targets\nwhile substantially reducing review costs.",
          "link": "http://arxiv.org/abs/2106.09871",
          "publishedOn": "2021-06-21T02:07:36.436Z",
          "wordCount": 549,
          "title": "Heuristic Stopping Rules For Technology-Assisted Review. (arXiv:2106.09871v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eugene Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_D/0/1/0/all/0/1\">David D. Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieder_O/0/1/0/all/0/1\">Ophir Frieder</a>",
          "description": "Technology-assisted review (TAR) refers to human-in-the-loop machine learning\nworkflows for document review in legal discovery and other high recall review\ntasks. Attorneys and legal technologists have debated whether review should be\na single iterative process (one-phase TAR workflows) or whether model training\nand review should be separate (two-phase TAR workflows), with implications for\nthe choice of active learning algorithm. The relative cost of manual labeling\nfor different purposes (training vs. review) and of different documents\n(positive vs. negative examples) is a key and neglected factor in this debate.\nUsing a novel cost dynamics analysis, we show analytically and empirically that\nthese relative costs strongly impact whether a one-phase or two-phase workflow\nminimizes cost. We also show how category prevalence, classification task\ndifficulty, and collection size impact the optimal choice not only of workflow\ntype, but of active learning method and stopping point.",
          "link": "http://arxiv.org/abs/2106.09866",
          "publishedOn": "2021-06-21T02:07:36.386Z",
          "wordCount": 585,
          "title": "On Minimizing Cost in Legal Document Review Workflows. (arXiv:2106.09866v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1\">Pablo S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1\">Alejandro Bellog&#xed;n</a>",
          "description": "Point-of-Interest recommendation is an increasing research and developing\narea within the widely adopted technologies known as Recommender Systems. Among\nthem, those that exploit information coming from Location-Based Social Networks\n(LBSNs) are very popular nowadays and could work with different information\nsources, which pose several challenges and research questions to the community\nas a whole. We present a systematic review focused on the research done in the\nlast 10 years about this topic. We discuss and categorize the algorithms and\nevaluation methodologies used in these works and point out the opportunities\nand challenges that remain open in the field. More specifically, we report the\nleading recommendation techniques and information sources that have been\nexploited more often (such as the geographical signal and deep learning\napproaches) while we also alert about the lack of reproducibility in the field\nthat may hinder real performance improvements.",
          "link": "http://arxiv.org/abs/2106.10069",
          "publishedOn": "2021-06-21T02:07:36.339Z",
          "wordCount": 586,
          "title": "Point-of-Interest Recommender Systems: A Survey from an Experimental Perspective. (arXiv:2106.10069v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Mustafizur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_D/0/1/0/all/0/1\">Dinesh Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murthy_D/0/1/0/all/0/1\">Dhiraj Murthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutlu_M/0/1/0/all/0/1\">Mucahid Kutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1\">Matthew Lease</a>",
          "description": "Building a benchmark dataset for hate speech detection presents several\nchallenges. Firstly, because hate speech is relatively rare -- e.g., less than\n3\\% of Twitter posts are hateful \\citep{founta2018large} -- random sampling of\ntweets to annotate is inefficient in capturing hate speech. A common practice\nis to only annotate tweets containing known ``hate words'', but this risks\nyielding a biased benchmark that only partially captures the real-world\nphenomenon of interest. A second challenge is that definitions of hate speech\ntend to be highly variable and subjective. Annotators having diverse prior\nnotions of hate speech may not only disagree with one another but also struggle\nto conform to specified labeling guidelines. Our key insight is that the rarity\nand subjectivity of hate speech are akin to that of relevance in information\nretrieval (IR). This connection suggests that well-established methodologies\nfor creating IR test collections might also be usefully applied to create\nbetter benchmark datasets for hate speech detection. Firstly, to intelligently\nand efficiently select which tweets to annotate, we apply established IR\ntechniques of {\\em pooling} and {\\em active learning}. Secondly, to improve\nboth consistency and value of annotations, we apply {\\em task decomposition}\n\\cite{Zhang-sigir14} and {\\em annotator rationale} \\cite{mcdonnell16-hcomp}\ntechniques. Using the above techniques, we create and share a new benchmark\ndataset\\footnote{We will release the dataset upon publication.} for hate speech\ndetection with broader coverage than prior datasets. We also show a dramatic\ndrop in accuracy of existing detection models when tested on these broader\nforms of hate. Collected annotator rationales not only provide documented\nsupport for labeling decisions but also create exciting future work\nopportunities for dual-supervision and/or explanation generation in modeling.",
          "link": "http://arxiv.org/abs/2106.09775",
          "publishedOn": "2021-06-21T02:07:36.316Z",
          "wordCount": 726,
          "title": "An Information Retrieval Approach to Building Datasets for Hate Speech Detection. (arXiv:2106.09775v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.12174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+T_B/0/1/0/all/0/1\">Balamurali B T</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hee_H/0/1/0/all/0/1\">Hwan Ing Hee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1\">Saumitra Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teoh_O/0/1/0/all/0/1\">Oon Hoe Teoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1\">Sung Shin Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Khai Pin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1\">Dorien Herremans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jer Ming Chen</a>",
          "description": "Intelligent systems are transforming the world, as well as our healthcare\nsystem. We propose a deep learning-based cough sound classification model that\ncan distinguish between children with healthy versus pathological coughs such\nas asthma, upper respiratory tract infection (URTI), and lower respiratory\ntract infection (LRTI). In order to train a deep neural network model, we\ncollected a new dataset of cough sounds, labelled with clinician's diagnosis.\nThe chosen model is a bidirectional long-short term memory network (BiLSTM)\nbased on Mel Frequency Cepstral Coefficients (MFCCs) features. The resulting\ntrained model when trained for classifying two classes of coughs -- healthy or\npathology (in general or belonging to a specific respiratory pathology),\nreaches accuracy exceeding 84\\% when classifying cough to the label provided by\nthe physicians' diagnosis. In order to classify subject's respiratory pathology\ncondition, results of multiple cough epochs per subject were combined. The\nresulting prediction accuracy exceeds 91\\% for all three respiratory\npathologies. However, when the model is trained to classify and discriminate\namong the four classes of coughs, overall accuracy dropped: one class of\npathological coughs are often misclassified as other. However, if one consider\nthe healthy cough classified as healthy and pathological cough classified to\nhave some kind of pathologies, then the overall accuracy of four class model is\nabove 84\\%. A longitudinal study of MFCC feature space when comparing\npathological and recovered coughs collected from the same subjects revealed the\nfact that pathological cough irrespective of the underlying conditions occupy\nthe same feature space making it harder to differentiate only using MFCC\nfeatures.",
          "link": "http://arxiv.org/abs/2106.12174",
          "publishedOn": "2021-06-24T01:51:41.860Z",
          "wordCount": 722,
          "title": "Deep Neural Network Based Respiratory Pathology Classification Using Cough Sounds. (arXiv:2106.12174v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shusheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuxin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Bin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>",
          "description": "Recently, query based deep networks catch lots of attention owing to their\nend-to-end pipeline and competitive results on several fundamental computer\nvision tasks, such as object detection, semantic segmentation, and instance\nsegmentation. However, how to establish a query based video instance\nsegmentation (VIS) framework with elegant architecture and strong performance\nremains to be settled. In this paper, we present \\textbf{QueryTrack} (i.e.,\ntracking instances as queries), a unified query based VIS framework fully\nleveraging the intrinsic one-to-one correspondence between instances and\nqueries in QueryInst. The proposed method obtains 52.7 / 52.3 AP on\nYouTube-VIS-2019 / 2021 datasets, which wins the 2-nd place in the YouTube-VIS\nChallenge at CVPR 2021 \\textbf{with a single online end-to-end model, single\nscale testing \\& modest amount of training data}. We also provide\nQueryTrack-ResNet-50 baseline results on YouTube-VIS-2021 dataset as references\nfor the VIS community.",
          "link": "http://arxiv.org/abs/2106.11963",
          "publishedOn": "2021-06-23T01:48:37.697Z",
          "wordCount": 599,
          "title": "Tracking Instances as Queries. (arXiv:2106.11963v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guangxing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shiyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiawei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yicheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>",
          "description": "Few-shot object detection (FSOD) aims to detect objects using only few\nexamples. It's critically needed for many practical applications but so far\nremains challenging. We propose a meta-learning based few-shot object detection\nmethod by transferring meta-knowledge learned from data-abundant base classes\nto data-scarce novel classes. Our method incorporates a coarse-to-fine approach\ninto the proposal based object detection framework and integrates prototype\nbased classifiers into both the proposal generation and classification stages.\nTo improve proposal generation for few-shot novel classes, we propose to learn\na lightweight matching network to measure the similarity between each spatial\nposition in the query image feature map and spatially-pooled class features,\ninstead of the traditional object/nonobject classifier, thus generating\ncategory-specific proposals and improving proposal recall for novel classes. To\naddress the spatial misalignment between generated proposals and few-shot class\nexamples, we propose a novel attentive feature alignment method, thus improving\nthe performance of few-shot object detection. Meanwhile we jointly learn a\nFaster R-CNN detection head for base classes. Extensive experiments conducted\non multiple FSOD benchmarks show our proposed approach achieves state of the\nart results under (incremental) few-shot learning settings.",
          "link": "http://arxiv.org/abs/2104.07719",
          "publishedOn": "2021-06-22T01:57:09.175Z",
          "wordCount": 673,
          "title": "Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with Attentive Feature Alignment. (arXiv:2104.07719v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Juan Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boronat_F/0/1/0/all/0/1\">Fernando Boronat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapena_A/0/1/0/all/0/1\">Almanzor Sapena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pastor_J/0/1/0/all/0/1\">Javier Pastor</a>",
          "description": "Thanks to the improvements experienced in technology in the last few years,\nmost especially in virtual reality systems, the number and potential of\nnetworked virtual environments or NVEs and their users are increasing. NVEs aim\nto give distributed users a feeling of immersion in a virtual world and the\npossibility of interacting with other users or with virtual objects inside it,\nlike when they interact in the real world. Being able to provide that feeling\nand natural interactions when the users are geographically separated is one of\nthe goals of these systems. Nevertheless, this goal is especially sensitive to\ndifferent issues, such as different connections with heterogeneous throughput\nor different network latencies, which can lead to consistency and\nsynchronization problems and, thus, to a worsening of the users' quality of\nexperience or QoE. With the purpose of solving these issues, researchers have\nproposed and evaluated numerous technical solutions, in fields like network\narchitectures, data distribution and filtering, resource balancing, computing\nmodels, predictive modeling and synchronization in NVEs. This paper gathers and\nclassifies them, summarizing their advantages and disadvantages, using a new\nway of classification. With the current increase of the number of NVEs and the\nmultiple solutions proposed so far, this work aims to become a useful tool and\na starting point not only for future researchers in this field but also for\nthose who are new in NVEs development, in which guaranteeing a good users' QoE\nis essential.",
          "link": "http://arxiv.org/abs/2102.09847",
          "publishedOn": "2021-06-22T01:57:09.033Z",
          "wordCount": 706,
          "title": "Key Technologies for Networked Virtual Environments. (arXiv:2102.09847v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farseev_A/0/1/0/all/0/1\">Aleksandr Farseev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1\">Andrey Filchenkov</a>",
          "description": "Human personality traits are the key drivers behind our decision-making,\ninfluencing our life path on a daily basis. Inference of personality traits,\nsuch as Myers-Briggs Personality Type, as well as an understanding of\ndependencies between personality traits and users' behavior on various social\nmedia platforms is of crucial importance to modern research and industry\napplications. The emergence of diverse and cross-purpose social media avenues\nmakes it possible to perform user personality profiling automatically and\nefficiently based on data represented across multiple data modalities. However,\nthe research efforts on personality profiling from multi-source multi-modal\nsocial media data are relatively sparse, and the level of impact of different\nsocial network data on machine learning performance has yet to be\ncomprehensively evaluated. Furthermore, there is not such dataset in the\nresearch community to benchmark. This study is one of the first attempts\ntowards bridging such an important research gap. Specifically, in this work, we\ninfer the Myers-Briggs Personality Type indicators, by applying a novel\nmulti-view fusion framework, called \"PERS\" and comparing the performance\nresults not just across data modalities but also with respect to different\nsocial network data sources. Our experimental results demonstrate the PERS's\nability to learn from multi-view data for personality profiling by efficiently\nleveraging on the significantly different data arriving from diverse social\nmultimedia sources. We have also found that the selection of a machine learning\napproach is of crucial importance when choosing social network data sources and\nthat people tend to reveal multiple facets of their personality in different\nsocial media avenues. Our released social multimedia dataset facilitates future\nresearch on this direction.",
          "link": "http://arxiv.org/abs/2106.10673",
          "publishedOn": "2021-06-22T01:57:08.794Z",
          "wordCount": 714,
          "title": "Two-Faced Humans on Twitter and Facebook: Harvesting Social Multimedia for Human Personality Profiling. (arXiv:2106.10673v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiapeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1\">Guozhi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Weihong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1\">Kai Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yichao Huang</a>",
          "description": "Visual information extraction (VIE) has attracted increasing attention in\nrecent years. The existing methods usually first organized optical character\nrecognition (OCR) results into plain texts and then utilized token-level entity\nannotations as supervision to train a sequence tagging model. However, it\nexpends great annotation costs and may be exposed to label confusion, and the\nOCR errors will also significantly affect the final performance. In this paper,\nwe propose a unified weakly-supervised learning framework called TCPN (Tag,\nCopy or Predict Network), which introduces 1) an efficient encoder to\nsimultaneously model the semantic and layout information in 2D OCR results; 2)\na weakly-supervised training strategy that utilizes only key information\nsequences as supervision; and 3) a flexible and switchable decoder which\ncontains two inference modes: one (Copy or Predict Mode) is to output key\ninformation sequences of different categories by copying a token from the input\nor predicting one in each time step, and the other (Tag Mode) is to directly\ntag the input sequence in a single forward pass. Our method shows new\nstate-of-the-art performance on several public benchmarks, which fully proves\nits effectiveness.",
          "link": "http://arxiv.org/abs/2106.10681",
          "publishedOn": "2021-06-22T01:57:08.660Z",
          "wordCount": 656,
          "title": "Tag, Copy or Predict: A Unified Weakly-Supervised Learning Framework for Visual Information Extraction using Sequences. (arXiv:2106.10681v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1\">Mohit Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pailla_D/0/1/0/all/0/1\">Dheeraj Pailla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_H/0/1/0/all/0/1\">Himanshu Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchawala_A/0/1/0/all/0/1\">Aadilmehdi Sanchawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1\">Manish Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1\">Ponnurangam Kumaraguru</a>",
          "description": "The exponential rise of online social media has enabled the creation,\ndistribution, and consumption of information at an unprecedented rate. However,\nit has also led to the burgeoning of various forms of online abuse. Increasing\ncases of online antisemitism have become one of the major concerns because of\nits socio-political consequences. Unlike other major forms of online abuse like\nracism, sexism, etc., online antisemitism has not been studied much from a\nmachine learning perspective. To the best of our knowledge, we present the\nfirst work in the direction of automated multimodal detection of online\nantisemitism. The task poses multiple challenges that include extracting\nsignals across multiple modalities, contextual references, and handling\nmultiple aspects of antisemitism. Unfortunately, there does not exist any\npublicly available benchmark corpus for this critical task. Hence, we collect\nand label two datasets with 3,102 and 3,509 social media posts from Twitter and\nGab respectively. Further, we present a multimodal deep learning system that\ndetects the presence of antisemitic content and its specific antisemitism\ncategory using text and images from posts. We perform an extensive set of\nexperiments on the two datasets to evaluate the efficacy of the proposed\nsystem. Finally, we also present a qualitative analysis of our study.",
          "link": "http://arxiv.org/abs/2104.05947",
          "publishedOn": "2021-06-22T01:57:08.648Z",
          "wordCount": 683,
          "title": "\"Subverting the Jewtocracy\": Online Antisemitism Detection Using Multimodal Deep Learning. (arXiv:2104.05947v3 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parmar_P/0/1/0/all/0/1\">Paritosh Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_J/0/1/0/all/0/1\">Jaiden Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_B/0/1/0/all/0/1\">Brendan Morris</a>",
          "description": "Can a computer determine a piano player's skill level? Is it preferable to\nbase this assessment on visual analysis of the player's performance or should\nwe trust our ears over our eyes? Since current CNNs have difficulty processing\nlong video videos, how can shorter clips be sampled to best reflect the players\nskill level? In this work, we collect and release a first-of-its-kind dataset\nfor multimodal skill assessment focusing on assessing piano player's skill\nlevel, answer the asked questions, initiate work in automated evaluation of\npiano playing skills and provide baselines for future work. Dataset is\navailable from: https://github.com/ParitoshParmar/Piano-Skills-Assessment.",
          "link": "http://arxiv.org/abs/2101.04884",
          "publishedOn": "2021-06-22T01:57:08.220Z",
          "wordCount": 576,
          "title": "Piano Skills Assessment. (arXiv:2101.04884v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_B/0/1/0/all/0/1\">Brijesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sur_A/0/1/0/all/0/1\">Arijit Sur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1\">Pinaki Mitra</a>",
          "description": "In recent times, deep learning-based steganalysis classifiers became popular\ndue to their state-of-the-art performance. Most deep steganalysis classifiers\nusually extract noise residuals using high-pass filters as preprocessing steps\nand feed them to their deep model for classification. It is observed that\nrecent steganographic embedding does not always restrict their embedding in the\nhigh-frequency zone; instead, they distribute it as per embedding policy.\nTherefore, besides noise residual, learning the embedding zone is another\nchallenging task. In this work, unlike the conventional approaches, the\nproposed model first extracts the noise residual using learned denoising\nkernels to boost the signal-to-noise ratio. After preprocessing, the sparse\nnoise residuals are fed to a novel Multi-Contextual Convolutional Neural\nNetwork (M-CNET) that uses heterogeneous context size to learn the sparse and\nlow-amplitude representation of noise residuals. The model performance is\nfurther improved by incorporating the Self-Attention module to focus on the\nareas prone to steganalytic embedding. A set of comprehensive experiments is\nperformed to show the proposed scheme's efficacy over the prior arts. Besides,\nan ablation study is given to justify the contribution of various modules of\nthe proposed architecture.",
          "link": "http://arxiv.org/abs/2106.10430",
          "publishedOn": "2021-06-22T01:57:08.194Z",
          "wordCount": 657,
          "title": "Multi-Contextual Design of Convolutional Neural Network for Steganalysis. (arXiv:2106.10430v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Lin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_E/0/1/0/all/0/1\">Edward Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Lei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Huaishao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharti_T/0/1/0/all/0/1\">Taroon Bharti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacheti_A/0/1/0/all/0/1\">Arun Sacheti</a>",
          "description": "In this paper, we present GEM as a General Evaluation benchmark for\nMultimodal tasks. Different from existing datasets such as GLUE, SuperGLUE,\nXGLUE and XTREME that mainly focus on natural language tasks, GEM is a\nlarge-scale vision-language benchmark, which consists of GEM-I for\nimage-language tasks and GEM-V for video-language tasks. Comparing with\nexisting multimodal datasets such as MSCOCO and Flicker30K for image-language\ntasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the\nlargest vision-language dataset covering image-language tasks and\nvideo-language tasks at the same time, but also labeled in multiple languages.\nWe also provide two baseline models for this benchmark. We will release the\ndataset, code and baseline models, aiming to advance the development of\nmultilingual multimodal research.",
          "link": "http://arxiv.org/abs/2106.09889",
          "publishedOn": "2021-06-21T02:07:36.399Z",
          "wordCount": 581,
          "title": "GEM: A General Evaluation Benchmark for Multimodal Tasks. (arXiv:2106.09889v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Graf_M/0/1/0/all/0/1\">Max Graf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opara_H/0/1/0/all/0/1\">Harold Chijioke Opara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barthet_M/0/1/0/all/0/1\">Mathieu Barthet</a>",
          "description": "Computer-generated visualisations can accompany recorded or live music to\ncreate novel audiovisual experiences for audiences. We present a system to\nstreamline the creation of audio-driven visualisations based on audio feature\nextraction and mapping interfaces. Its architecture is based on three modular\nsoftware components: backend (audio plugin), frontend (3D game-like\nenvironment), and middleware (visual mapping interface). We conducted a user\nevaluation comprising two stages. Results from the first stage (34\nparticipants) indicate that music visualisations generated with the system were\nsignificantly better at complementing the music than a baseline visualisation.\nNine participants took part in the second stage involving interactive tasks.\nOverall, the system yielded a Creativity Support Index above average (68.1) and\na System Usability Scale index (58.6) suggesting that ease of use can be\nimproved. Thematic analysis revealed that participants enjoyed the system's\nsynchronicity and expressive capabilities, but found technical problems and\ndifficulties understanding the audio feature terminology.",
          "link": "http://arxiv.org/abs/2106.10134",
          "publishedOn": "2021-06-21T02:07:36.368Z",
          "wordCount": 587,
          "title": "An Audio-Driven System For Real-Time Music Visualisation. (arXiv:2106.10134v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geleta_M/0/1/0/all/0/1\">Margarita Geleta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punti_C/0/1/0/all/0/1\">Cristina Punti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1\">Kevin McGuinness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_J/0/1/0/all/0/1\">Jordi Pons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canton_C/0/1/0/all/0/1\">Cristian Canton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1\">Xavier Giro-i-Nieto</a>",
          "description": "Steganography comprises the mechanics of hiding data in a host media that may\nbe publicly available. While previous works focused on unimodal setups (e.g.,\nhiding images in images, or hiding audio in audio), PixInWav targets the\nmultimodal case of hiding images in audio. To this end, we propose a novel\nresidual architecture operating on top of short-time discrete cosine transform\n(STDCT) audio spectrograms. Among our results, we find that the residual audio\nsteganography setup we propose allows independent encoding of the hidden image\nfrom the host audio without compromising quality. Accordingly, while previous\nworks require both host and hidden signals to hide a signal, PixInWav can\nencode images offline -- which can be later hidden, in a residual fashion, into\nany audio signal. Finally, we test our scheme in a lab setting to transmit\nimages over airwaves from a loudspeaker to a microphone verifying our\ntheoretical insights and obtaining promising results.",
          "link": "http://arxiv.org/abs/2106.09814",
          "publishedOn": "2021-06-21T02:07:36.277Z",
          "wordCount": 604,
          "title": "PixInWav: Residual Steganography for Hiding Pixels in Audio. (arXiv:2106.09814v1 [cs.MM])"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2103.03206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimeno_F/0/1/0/all/0/1\">Felix Gimeno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andrew Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Joao Carreira</a>",
          "description": "Biological systems perceive the world by simultaneously processing\nhigh-dimensional inputs from modalities as diverse as vision, audition, touch,\nproprioception, etc. The perception models used in deep learning on the other\nhand are designed for individual modalities, often relying on domain-specific\nassumptions such as the local grid structures exploited by virtually all\nexisting vision models. These priors introduce helpful inductive biases, but\nalso lock models to individual modalities. In this paper we introduce the\nPerceiver - a model that builds upon Transformers and hence makes few\narchitectural assumptions about the relationship between its inputs, but that\nalso scales to hundreds of thousands of inputs, like ConvNets. The model\nleverages an asymmetric attention mechanism to iteratively distill inputs into\na tight latent bottleneck, allowing it to scale to handle very large inputs. We\nshow that this architecture is competitive with or outperforms strong,\nspecialized models on classification tasks across various modalities: images,\npoint clouds, audio, video, and video+audio. The Perceiver obtains performance\ncomparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly\nattending to 50,000 pixels. It is also competitive in all modalities in\nAudioSet.",
          "link": "http://arxiv.org/abs/2103.03206",
          "publishedOn": "2021-06-24T01:51:44.801Z",
          "wordCount": 679,
          "title": "Perceiver: General Perception with Iterative Attention. (arXiv:2103.03206v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yunjey Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Sungjoo Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1\">Youngjung Uh</a>",
          "description": "Generative adversarial networks (GANs) synthesize realistic images from\nrandom latent vectors. Although manipulating the latent vectors controls the\nsynthesized outputs, editing real images with GANs suffers from i)\ntime-consuming optimization for projecting real images to the latent vectors,\nii) or inaccurate embedding through an encoder. We propose StyleMapGAN: the\nintermediate latent space has spatial dimensions, and a spatially variant\nmodulation replaces AdaIN. It makes the embedding through an encoder more\naccurate than existing optimization-based methods while maintaining the\nproperties of GANs. Experimental results demonstrate that our method\nsignificantly outperforms state-of-the-art models in various image manipulation\ntasks such as local editing and image interpolation. Last but not least,\nconventional editing methods on GANs are still valid on our StyleMapGAN. Source\ncode is available at https://github.com/naver-ai/StyleMapGAN.",
          "link": "http://arxiv.org/abs/2104.14754",
          "publishedOn": "2021-06-24T01:51:44.790Z",
          "wordCount": 608,
          "title": "Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing. (arXiv:2104.14754v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">R. Kenny Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charatan_D/0/1/0/all/0/1\">David Charatan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1\">Paul Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>",
          "description": "A popular way to create detailed yet easily controllable 3D shapes is via\nprocedural modeling, i.e. generating geometry using programs. Such programs\nconsist of a series of instructions along with their associated parameter\nvalues. To fully realize the benefits of this representation, a shape program\nshould be compact and only expose degrees of freedom that allow for meaningful\nmanipulation of output geometry. One way to achieve this goal is to design\nhigher-level macro operators that, when executed, expand into a series of\ncommands from the base shape modeling language. However, manually authoring\nsuch macros, much like shape programs themselves, is difficult and largely\nrestricted to domain experts. In this paper, we present ShapeMOD, an algorithm\nfor automatically discovering macros that are useful across large datasets of\n3D shape programs. ShapeMOD operates on shape programs expressed in an\nimperative, statement-based language. It is designed to discover macros that\nmake programs more compact by minimizing the number of function calls and free\nparameters required to represent an input shape collection. We run ShapeMOD on\nmultiple collections of programs expressed in a domain-specific language for 3D\nshape structures. We show that it automatically discovers a concise set of\nmacros that abstract out common structural and parametric patterns that\ngeneralize over large shape collections. We also demonstrate that the macros\nfound by ShapeMOD improve performance on downstream tasks including shape\ngenerative modeling and inferring programs from point clouds. Finally, we\nconduct a user study that indicates that ShapeMOD's discovered macros make\ninteractive shape editing more efficient.",
          "link": "http://arxiv.org/abs/2104.06392",
          "publishedOn": "2021-06-24T01:51:44.766Z",
          "wordCount": 739,
          "title": "ShapeMOD: Macro Operation Discovery for 3D Shape Programs. (arXiv:2104.06392v2 [cs.GR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all the previously\nmentioned drawbacks). The entropic out-of-distribution detection solution\ncomprises the IsoMax loss for training and the entropic score for\nout-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in\nreplacement because swapping the SoftMax loss with the IsoMax loss requires no\nchanges in the model's architecture or training procedures/hyperparameters. In\nthis paper, we propose to perform what we call an isometrization of the\ndistances used in the IsoMax loss. Additionally, we propose to replace the\nentropic score with the minimum distance score. Our experiments showed that\nthese simple modifications increase out-of-distribution detection performance\nwhile keeping the solution seamless.",
          "link": "http://arxiv.org/abs/2105.14399",
          "publishedOn": "2021-06-24T01:51:44.760Z",
          "wordCount": 610,
          "title": "Improving Entropic Out-of-Distribution Detection using Isometric Distances and the Minimum Distance Score. (arXiv:2105.14399v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Somanath_G/0/1/0/all/0/1\">Gowri Somanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1\">Daniel Kurz</a>",
          "description": "We present a method to estimate an HDR environment map from a narrow\nfield-of-view LDR camera image in real-time. This enables perceptually\nappealing reflections and shading on virtual objects of any material finish,\nfrom mirror to diffuse, rendered into a real physical environment using\naugmented reality. Our method is based on our efficient convolutional neural\nnetwork architecture, EnvMapNet, trained end-to-end with two novel losses,\nProjectionLoss for the generated image, and ClusterLoss for adversarial\ntraining. Through qualitative and quantitative comparison to state-of-the-art\nmethods, we demonstrate that our algorithm reduces the directional error of\nestimated light sources by more than 50%, and achieves 3.7 times lower Frechet\nInception Distance (FID). We further showcase a mobile application that is able\nto run our neural network model in under 9 ms on an iPhone XS, and render in\nreal-time, visually coherent virtual objects in previously unseen real-world\nenvironments.",
          "link": "http://arxiv.org/abs/2011.10687",
          "publishedOn": "2021-06-24T01:51:44.713Z",
          "wordCount": 649,
          "title": "HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02869",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Demir_U/0/1/0/all/0/1\">Ugur Demir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Irmakci_I/0/1/0/all/0/1\">Ismail Irmakci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keles_E/0/1/0/all/0/1\">Elif Keles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Topcu_A/0/1/0/all/0/1\">Ahmet Topcu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyue Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Spampinato_C/0/1/0/all/0/1\">Concetto Spampinato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jambawalikar_S/0/1/0/all/0/1\">Sachin Jambawalikar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turkbey_E/0/1/0/all/0/1\">Evrim Turkbey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turkbey_B/0/1/0/all/0/1\">Baris Turkbey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1\">Ulas Bagci</a>",
          "description": "Visual explanation methods have an important role in the prognosis of the\npatients where the annotated data is limited or unavailable. There have been\nseveral attempts to use gradient-based attribution methods to localize\npathology from medical scans without using segmentation labels. This research\ndirection has been impeded by the lack of robustness and reliability. These\nmethods are highly sensitive to the network parameters. In this study, we\nintroduce a robust visual explanation method to address this problem for\nmedical applications. We provide an innovative visual explanation algorithm for\ngeneral purpose and as an example application, we demonstrate its effectiveness\nfor quantifying lesions in the lungs caused by the Covid-19 with high accuracy\nand robustness without using dense segmentation labels. This approach overcomes\nthe drawbacks of commonly used Grad-CAM and its extended versions. The premise\nbehind our proposed strategy is that the information flow is minimized while\nensuring the classifier prediction stays similar. Our findings indicate that\nthe bottleneck condition provides a more stable severity estimation than the\nsimilar attribution methods.",
          "link": "http://arxiv.org/abs/2104.02869",
          "publishedOn": "2021-06-24T01:51:43.731Z",
          "wordCount": 697,
          "title": "Information Bottleneck Attribution for Visual Explanations of Diagnosis and Prognosis. (arXiv:2104.02869v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1\">Weituo Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spell_G/0/1/0/all/0/1\">Gregory Spell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>",
          "description": "The outbreak of COVID-19 Disease due to the novel coronavirus has caused a\nshortage of medical resources. To aid and accelerate the diagnosis process,\nautomatic diagnosis of COVID-19 via deep learning models has recently been\nexplored by researchers across the world. While different data-driven deep\nlearning models have been developed to mitigate the diagnosis of COVID-19, the\ndata itself is still scarce due to patient privacy concerns. Federated Learning\n(FL) is a natural solution because it allows different organizations to\ncooperatively learn an effective deep learning model without sharing raw data.\nHowever, recent studies show that FL still lacks privacy protection and may\ncause data leakage. We investigate this challenging problem by proposing a\nsimple yet effective algorithm, named \\textbf{F}ederated \\textbf{L}earning\n\\textbf{o}n Medical Datasets using \\textbf{P}artial Networks (FLOP), that\nshares only a partial model between the server and clients. Extensive\nexperiments on benchmark data and real-world healthcare tasks show that our\napproach achieves comparable or better performance while reducing the privacy\nand security risks. Of particular interest, we conduct experiments on the\nCOVID-19 dataset and find that our FLOP algorithm can allow different hospitals\nto collaboratively and effectively train a partially shared model without\nsharing local patients' data.",
          "link": "http://arxiv.org/abs/2102.05218",
          "publishedOn": "2021-06-24T01:51:43.725Z",
          "wordCount": 722,
          "title": "FLOP: Federated Learning on Medical Datasets using Partial Networks. (arXiv:2102.05218v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yunfeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mingming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>",
          "description": "Recently, visual Transformer (ViT) and its following works abandon the\nconvolution and exploit the self-attention operation, attaining a comparable or\neven higher accuracy than CNNs. More recently, MLP-Mixer abandons both the\nconvolution and the self-attention operation, proposing an architecture\ncontaining only MLP layers. To achieve cross-patch communications, it devises\nan additional token-mixing MLP besides the channel-mixing MLP. It achieves\npromising results when training on an extremely large-scale dataset. But it\ncannot achieve as outstanding performance as its CNN and ViT counterparts when\ntraining on medium-scale datasets such as ImageNet1K and ImageNet21K. The\nperformance drop of MLP-Mixer motivates us to rethink the token-mixing MLP. We\ndiscover that the token-mixing MLP is a variant of the depthwise convolution\nwith a global reception field and spatial-specific configuration. But the\nglobal reception field and the spatial-specific property make token-mixing MLP\nprone to over-fitting. In this paper, we propose a novel pure MLP architecture,\nspatial-shift MLP (S$^2$-MLP). Different from MLP-Mixer, our S$^2$-MLP only\ncontains channel-mixing MLP. We utilize a spatial-shift operation for\ncommunications between patches. It has a local reception field and is\nspatial-agnostic. It is parameter-free and efficient for computation. The\nproposed S$^2$-MLP attains higher recognition accuracy than MLP-Mixer when\ntraining on ImageNet-1K dataset. Meanwhile, S$^2$-MLP accomplishes as excellent\nperformance as ViT on ImageNet-1K dataset with considerably simpler\narchitecture and fewer FLOPs and parameters.",
          "link": "http://arxiv.org/abs/2106.07477",
          "publishedOn": "2021-06-24T01:51:43.676Z",
          "wordCount": 678,
          "title": "S$^2$-MLP: Spatial-Shift MLP Architecture for Vision. (arXiv:2106.07477v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.15093",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chai_S/0/1/0/all/0/1\">Seoin Chai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fetit_A/0/1/0/all/0/1\">Ahmed E. Fetit</a>",
          "description": "Despite advances in deep learning, robustness under domain shift remains a\nmajor bottleneck in medical imaging settings. Findings on natural images\nsuggest that deep neural models can show a strong textural bias when carrying\nout image classification tasks. In this thorough empirical study, we draw\ninspiration from findings on natural images and investigate ways in which\naddressing the textural bias phenomenon could bring up the robustness of deep\nsegmentation models when applied to three-dimensional (3D) medical data. To\nachieve this, publicly available MRI scans from the Developing Human Connectome\nProject are used to study ways in which simulating textural noise can help\ntrain robust models in a complex semantic segmentation task. We contribute an\nextensive empirical investigation consisting of 176 experiments and illustrate\nhow applying specific types of simulated textural noise prior to training can\nlead to texture invariant models, resulting in improved robustness when\nsegmenting scans corrupted by previously unseen noise types and levels.",
          "link": "http://arxiv.org/abs/2011.15093",
          "publishedOn": "2021-06-24T01:51:43.661Z",
          "wordCount": 625,
          "title": "Reducing Textural Bias Improves Robustness of Deep Segmentation Models. (arXiv:2011.15093v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhangcheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiali Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_E/0/1/0/all/0/1\">Eric Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Leyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qijing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yida Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "Current low-precision quantization algorithms often have the hidden cost of\nconversion back and forth from floating point to quantized integer values. This\nhidden cost limits the latency improvement realized by quantizing Neural\nNetworks. To address this, we present HAWQV3, a novel mixed-precision\ninteger-only quantization framework. The contributions of HAWQV3 are the\nfollowing: (i) An integer-only inference where the entire computational graph\nis performed only with integer multiplication, addition, and bit shifting,\nwithout any floating point operations or even integer division; (ii) A novel\nhardware-aware mixed-precision quantization method where the bit-precision is\ncalculated by solving an integer linear programming problem that balances the\ntrade-off between model perturbation and other constraints, e.g., memory\nfootprint and latency; (iii) Direct hardware deployment and open source\ncontribution for 4-bit uniform/mixed-precision quantization in TVM, achieving\nan average speed up of $1.45\\times$ for uniform 4-bit, as compared to uniform\n8-bit for ResNet50 on T4 GPUs; and (iv) extensive evaluation of the proposed\nmethods on ResNet18/50 and InceptionV3, for various model compression levels\nwith/without mixed precision. For ResNet50, our INT8 quantization achieves an\naccuracy of $77.58\\%$, which is $2.68\\%$ higher than prior integer-only work,\nand our mixed-precision INT4/8 quantization can reduce INT8 latency by $23\\%$\nand still achieve $76.73\\%$ accuracy. Our framework and the TVM implementation\nhave been open sourced.",
          "link": "http://arxiv.org/abs/2011.10680",
          "publishedOn": "2021-06-24T01:51:43.582Z",
          "wordCount": 701,
          "title": "HAWQV3: Dyadic Neural Network Quantization. (arXiv:2011.10680v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1\">Seyed Saeed Changiz Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Fred X. Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1\">Mohammad Salameh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1\">Keith Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1\">Shuo Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1\">Shangling Jui</a>",
          "description": "Despite the empirical success of neural architecture search (NAS) in deep\nlearning applications, the optimality, reproducibility and cost of NAS schemes\nremain hard to assess. In this paper, we propose Generative Adversarial NAS\n(GA-NAS) with theoretically provable convergence guarantees, promoting\nstability and reproducibility in neural architecture search. Inspired by\nimportance sampling, GA-NAS iteratively fits a generator to previously\ndiscovered top architectures, thus increasingly focusing on important parts of\na large search space. Furthermore, we propose an efficient adversarial learning\napproach, where the generator is trained by reinforcement learning based on\nrewards provided by a discriminator, thus being able to explore the search\nspace without evaluating a large number of architectures. Extensive experiments\nshow that GA-NAS beats the best published results under several cases on three\npublic NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search\nconstraints and search spaces. We show that GA-NAS can be used to improve\nalready optimized baselines found by other NAS methods, including EfficientNet\nand ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in\ntheir original search space.",
          "link": "http://arxiv.org/abs/2105.09356",
          "publishedOn": "2021-06-24T01:51:43.523Z",
          "wordCount": 670,
          "title": "Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.11277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hipiny_I/0/1/0/all/0/1\">Irwandi Hipiny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ujir_H/0/1/0/all/0/1\">Hamimah Ujir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mujahid_A/0/1/0/all/0/1\">Aazani Mujahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yahya_N/0/1/0/all/0/1\">Nurhartini Kamalia Yahya</a>",
          "description": "Passive biometric identification enables wildlife monitoring with minimal\ndisturbance. Using a motion-activated camera placed at an elevated position and\nfacing downwards, we collected images of sea turtle carapace, each belonging to\none of sixteen Chelonia mydas juveniles. We then learned co-variant and robust\nimage descriptors from these images, enabling indexing and retrieval. In this\nwork, we presented several classification results of sea turtle carapaces using\nthe learned image descriptors. We found that a template-based descriptor, i.e.,\nHistogram of Oriented Gradients (HOG) performed exceedingly better during\nclassification than keypoint-based descriptors. For our dataset, a\nhigh-dimensional descriptor is a must due to the minimal gradient and color\ninformation inside the carapace images. Using HOG, we obtained an average\nclassification accuracy of 65%.",
          "link": "http://arxiv.org/abs/1909.11277",
          "publishedOn": "2021-06-24T01:51:43.517Z",
          "wordCount": 624,
          "title": "Towards Automated Biometric Identification of Sea Turtles (Chelonia mydas). (arXiv:1909.11277v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zujie Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haifeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiaying Zhu</a>",
          "description": "Most existing Visual Question Answering (VQA) systems tend to overly rely on\nlanguage bias and hence fail to reason from the visual clue. To address this\nissue, we propose a novel Language-Prior Feedback (LPF) objective function, to\nre-balance the proportion of each answer's loss value in the total VQA loss.\nThe LPF firstly calculates a modulating factor to determine the language bias\nusing a question-only branch. Then, the LPF assigns a self-adaptive weight to\neach training sample in the training process. With this reweighting mechanism,\nthe LPF ensures that the total VQA loss can be reshaped to a more balanced\nform. By this means, the samples that require certain visual information to\npredict will be efficiently used during training. Our method is simple to\nimplement, model-agnostic, and end-to-end trainable. We conduct extensive\nexperiments and the results show that the LPF (1) brings a significant\nimprovement over various VQA models, (2) achieves competitive performance on\nthe bias-sensitive VQA-CP v2 benchmark.",
          "link": "http://arxiv.org/abs/2105.14300",
          "publishedOn": "2021-06-24T01:51:43.486Z",
          "wordCount": 629,
          "title": "LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering. (arXiv:2105.14300v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.05785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nuriel_O/0/1/0/all/0/1\">Oren Nuriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benaim_S/0/1/0/all/0/1\">Sagie Benaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "Recent work has shown that convolutional neural network classifiers overly\nrely on texture at the expense of shape cues. We make a similar but different\ndistinction between shape and local image cues, on the one hand, and global\nimage statistics, on the other. Our method, called Permuted Adaptive Instance\nNormalization (pAdaIN), reduces the representation of global statistics in the\nhidden layers of image classifiers. pAdaIN samples a random permutation $\\pi$\nthat rearranges the samples in a given batch. Adaptive Instance Normalization\n(AdaIN) is then applied between the activations of each (non-permuted) sample\n$i$ and the corresponding activations of the sample $\\pi(i)$, thus swapping\nstatistics between the samples of the batch. Since the global image statistics\nare distorted, this swapping procedure causes the network to rely on cues, such\nas shape or texture. By choosing the random permutation with probability $p$\nand the identity permutation otherwise, one can control the effect's strength.\n\nWith the correct choice of $p$, fixed apriori for all experiments and\nselected without considering test data, our method consistently outperforms\nbaselines in multiple settings. In image classification, our method improves on\nboth CIFAR100 and ImageNet using multiple architectures. In the setting of\nrobustness, our method improves on both ImageNet-C and Cifar-100-C for multiple\narchitectures. In the setting of domain adaptation and domain generalization,\nour method achieves state of the art results on the transfer learning task from\nGTAV to Cityscapes and on the PACS benchmark.",
          "link": "http://arxiv.org/abs/2010.05785",
          "publishedOn": "2021-06-24T01:51:43.481Z",
          "wordCount": 721,
          "title": "Permuted AdaIN: Reducing the Bias Towards Global Statistics in Image Classification. (arXiv:2010.05785v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.04806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alexander Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mengye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard S. Zemel</a>",
          "description": "Sketch drawings capture the salient information of visual concepts. Previous\nwork has shown that neural networks are capable of producing sketches of\nnatural objects drawn from a small number of classes. While earlier approaches\nfocus on generation quality or retrieval, we explore properties of image\nrepresentations learned by training a model to produce sketches of images. We\nshow that this generative, class-agnostic model produces informative embeddings\nof images from novel examples, classes, and even novel datasets in a few-shot\nsetting. Additionally, we find that these learned representations exhibit\ninteresting structure and compositionality.",
          "link": "http://arxiv.org/abs/2009.04806",
          "publishedOn": "2021-06-24T01:51:43.468Z",
          "wordCount": 589,
          "title": "SketchEmbedNet: Learning Novel Concepts by Imitating Drawings. (arXiv:2009.04806v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dorr_L/0/1/0/all/0/1\">Laura D&#xf6;rr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandt_F/0/1/0/all/0/1\">Felix Brandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_A/0/1/0/all/0/1\">Alexander Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pouls_M/0/1/0/all/0/1\">Martin Pouls</a>",
          "description": "While common image object detection tasks focus on bounding boxes or\nsegmentation masks as object representations, we consider the problem of\nfinding objects based on four arbitrary vertices. We propose a novel model,\nnamed TetraPackNet, to tackle this problem. TetraPackNet is based on CornerNet\nand uses similar algorithms and ideas. It is designated for applications\nrequiring high-accuracy detection of regularly shaped objects, which is the\ncase in the logistics use-case of packaging structure recognition. We evaluate\nour model on our specific real-world dataset for this use-case. Baselined\nagainst a previous solution, consisting of a Mask R-CNN model and suitable\npost-processing steps, TetraPackNet achieves superior results (9% higher in\naccuracy) in the sub-task of four-corner based transport unit side detection.",
          "link": "http://arxiv.org/abs/2104.09123",
          "publishedOn": "2021-06-24T01:51:43.462Z",
          "wordCount": 586,
          "title": "TetraPackNet: Four-Corner-Based Object Detection in Logistics Use-Cases. (arXiv:2104.09123v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rematas_K/0/1/0/all/0/1\">Konstantinos Rematas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Brualla_R/0/1/0/all/0/1\">Ricardo Martin-Brualla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>",
          "description": "We present a method for estimating neural scenes representations of objects\ngiven only a single image. The core of our method is the estimation of a\ngeometric scaffold for the object and its use as a guide for the reconstruction\nof the underlying radiance field. Our formulation is based on a generative\nprocess that first maps a latent code to a voxelized shape, and then renders it\nto an image, with the object appearance being controlled by a second latent\ncode. During inference, we optimize both the latent codes and the networks to\nfit a test image of a new object. The explicit disentanglement of shape and\nappearance allows our model to be fine-tuned given a single image. We can then\nrender new views in a geometrically consistent manner and they represent\nfaithfully the input object. Additionally, our method is able to generalize to\nimages outside of the training domain (more realistic renderings and even real\nphotographs). Finally, the inferred geometric scaffold is itself an accurate\nestimate of the object's 3D shape. We demonstrate in several experiments the\neffectiveness of our approach in both synthetic and real images.",
          "link": "http://arxiv.org/abs/2102.08860",
          "publishedOn": "2021-06-24T01:51:43.447Z",
          "wordCount": 659,
          "title": "ShaRF: Shape-conditioned Radiance Fields from a Single View. (arXiv:2102.08860v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12534",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wada_K/0/1/0/all/0/1\">Kentaro Wada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laidlow_T/0/1/0/all/0/1\">Tristan Laidlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>",
          "description": "Reflecting on the last few years, the biggest breakthroughs in deep\nreinforcement learning (RL) have been in the discrete action domain. Robotic\nmanipulation, however, is inherently a continuous control environment, but\nthese continuous control reinforcement learning algorithms often depend on\nactor-critic methods that are sample-inefficient and inherently difficult to\ntrain, due to the joint optimisation of the actor and critic. To that end, we\nexplore how we can bring the stability of discrete action RL algorithms to the\nrobot manipulation domain. We extend the recently released ARM algorithm, by\nreplacing the continuous next-best pose agent with a discrete next-best pose\nagent. Discretisation of rotation is trivial given its bounded nature, while\ntranslation is inherently unbounded, making discretisation difficult. We\nformulate the translation prediction as the voxel prediction problem by\ndiscretising the 3D space; however, voxelisation of a large workspace is memory\nintensive and would not work with a high density of voxels, crucial to\nobtaining the resolution needed for robotic manipulation. We therefore propose\nto apply this voxel prediction in a coarse-to-fine manner by gradually\nincreasing the resolution. In each step, we extract the highest valued voxel as\nthe predicted location, which is then used as the centre of the\nhigher-resolution voxelisation in the next step. This coarse-to-fine prediction\nis applied over several steps, giving a near-lossless prediction of the\ntranslation. We show that our new coarse-to-fine algorithm is able to\naccomplish RLBench tasks much more efficiently than the continuous control\nequivalent, and even train some real-world tasks, tabular rasa, in less than 7\nminutes, with only 3 demonstrations. Moreover, we show that by moving to a\nvoxel representation, we are able to easily incorporate observations from\nmultiple cameras.",
          "link": "http://arxiv.org/abs/2106.12534",
          "publishedOn": "2021-06-24T01:51:43.435Z",
          "wordCount": 736,
          "title": "Coarse-to-Fine Q-attention: Efficient Learning for Visual Robotic Manipulation via Discretisation. (arXiv:2106.12534v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2012.02526",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Garcia_A/0/1/0/all/0/1\">Alex Hernandez-Garcia</a>",
          "description": "The renaissance of artificial neural networks was catalysed by the success of\nclassification models, tagged by the community with the broader term supervised\nlearning. The extraordinary results gave rise to a hype loaded with ambitious\npromises and overstatements. Soon the community realised that the success owed\nmuch to the availability of thousands of labelled examples and supervised\nlearning went, for many, from glory to shame: Some criticised deep learning as\na whole and others proclaimed that the way forward had to be alternatives to\nsupervised learning: predictive, unsupervised, semi-supervised and, more\nrecently, self-supervised learning. However, all these seem brand names, rather\nthan actual categories of a theoretically grounded taxonomy. Moreover, the call\nto banish supervised learning was motivated by the questionable claim that\nhumans learn with little or no supervision and are capable of robust\nout-of-distribution generalisation. Here, we review insights about learning and\nsupervision in nature, revisit the notion that learning and generalisation are\nnot possible without supervision or inductive biases and argue that we will\nmake better progress if we just call it by its name.",
          "link": "http://arxiv.org/abs/2012.02526",
          "publishedOn": "2021-06-24T01:51:43.429Z",
          "wordCount": 666,
          "title": "Rethinking supervised learning: insights from biological learning and from calling it by its name. (arXiv:2012.02526v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13936",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1\">Baoliang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1\">Lingyu Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1\">Guo Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_H/0/1/0/all/0/1\">Hongfei Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>",
          "description": "In this work, we propose a no-reference video quality assessment method,\naiming to achieve high-generalization capability in cross-content, -resolution\nand -frame rate quality prediction. In particular, we evaluate the quality of a\nvideo by learning effective feature representations in spatial-temporal domain.\nIn the spatial domain, to tackle the resolution and content variations, we\nimpose the Gaussian distribution constraints on the quality features. The\nunified distribution can significantly reduce the domain gap between different\nvideo samples, resulting in a more generalized quality feature representation.\nAlong the temporal dimension, inspired by the mechanism of visual perception,\nwe propose a pyramid temporal aggregation module by involving the short-term\nand long-term memory to aggregate the frame-level quality. Experiments show\nthat our method outperforms the state-of-the-art methods on cross-dataset\nsettings, and achieves comparable performance on intra-dataset configurations,\ndemonstrating the high-generalization capability of the proposed method.",
          "link": "http://arxiv.org/abs/2012.13936",
          "publishedOn": "2021-06-24T01:51:43.422Z",
          "wordCount": 609,
          "title": "Learning Generalized Spatial-Temporal Deep Feature Representation for No-Reference Video Quality Assessment. (arXiv:2012.13936v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05748",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1\">Chao Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dulay_J/0/1/0/all/0/1\">Justin Dulay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rolwes_G/0/1/0/all/0/1\">Gregory Rolwes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauli_D/0/1/0/all/0/1\">Duke Pauli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakoor_N/0/1/0/all/0/1\">Nadia Shakoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1\">Abby Stylianou</a>",
          "description": "Automated high throughput plant phenotyping involves leveraging sensors, such\nas RGB, thermal and hyperspectral cameras (among others), to make large scale\nand rapid measurements of the physical properties of plants for the purpose of\nbetter understanding the difference between crops and facilitating rapid plant\nbreeding programs. One of the most basic phenotyping tasks is to determine the\ncultivar, or species, in a particular sensor product. This simple phenotype can\nbe used to detect errors in planting and to learn the most differentiating\nfeatures between cultivars. It is also a challenging visual recognition task,\nas a large number of highly related crops are grown simultaneously, leading to\na classification problem with low inter-class variance. In this paper, we\nintroduce the Sorghum-100 dataset, a large dataset of RGB imagery of sorghum\ncaptured by a state-of-the-art gantry system, a multi-resolution network\narchitecture that learns both global and fine-grained features on the crops,\nand a new global pooling strategy called Dynamic Outlier Pooling which\noutperforms standard global pooling strategies on this task.",
          "link": "http://arxiv.org/abs/2106.05748",
          "publishedOn": "2021-06-24T01:51:43.406Z",
          "wordCount": 630,
          "title": "Multi-resolution Outlier Pooling for Sorghum Classification. (arXiv:2106.05748v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.03734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grieggs_S/0/1/0/all/0/1\">Samuel Grieggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bingyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauch_G/0/1/0/all/0/1\">Greta Rauch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1\">Brian Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheirer_W/0/1/0/all/0/1\">Walter J. Scheirer</a>",
          "description": "The subtleties of human perception, as measured by vision scientists through\nthe use of psychophysics, are important clues to the internal workings of\nvisual recognition. For instance, measured reaction time can indicate whether a\nvisual stimulus is easy for a subject to recognize, or whether it is hard. In\nthis paper, we consider how to incorporate psychophysical measurements of\nvisual perception into the loss function of a deep neural network being trained\nfor a recognition task, under the assumption that such information can enforce\nconsistency with human behavior. As a case study to assess the viability of\nthis approach, we look at the problem of handwritten document transcription.\nWhile good progress has been made towards automatically transcribing modern\nhandwriting, significant challenges remain in transcribing historical\ndocuments. Here we describe a general enhancement strategy, underpinned by the\nnew loss formulation, which can be applied to the training regime of any deep\nlearning-based document transcription system. Through experimentation, reliable\nperformance improvement is demonstrated for the standard IAM and RIMES datasets\nfor three different network architectures. Further, we go on to show\nfeasibility for our approach on a new dataset of digitized Latin manuscripts,\noriginally produced by scribes in the Cloister of St. Gall in the the 9th\ncentury.",
          "link": "http://arxiv.org/abs/1904.03734",
          "publishedOn": "2021-06-24T01:51:43.401Z",
          "wordCount": 709,
          "title": "Measuring Human Perception to Improve Handwritten Document Transcription. (arXiv:1904.03734v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mattern_D/0/1/0/all/0/1\">Denny Mattern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martyniuk_D/0/1/0/all/0/1\">Darya Martyniuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willems_H/0/1/0/all/0/1\">Henri Willems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergmann_F/0/1/0/all/0/1\">Fabian Bergmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paschke_A/0/1/0/all/0/1\">Adrian Paschke</a>",
          "description": "Image classification is an important task in various machine learning\napplications. In recent years, a number of classification methods based on\nquantum machine learning and different quantum image encoding techniques have\nbeen proposed. In this paper, we study the effect of three different quantum\nimage encoding approaches on the performance of a convolution-inspired hybrid\nquantum-classical image classification algorithm called quanvolutional neural\nnetwork (QNN). We furthermore examine the effect of variational - i.e.\ntrainable - quantum circuits on the classification results. Our experiments\nindicate that some image encodings are better suited for variational circuits.\nHowever, our experiments show as well that there is not one best image\nencoding, but that the choice of the encoding depends on the specific\nconstraints of the application.",
          "link": "http://arxiv.org/abs/2106.07327",
          "publishedOn": "2021-06-24T01:51:43.394Z",
          "wordCount": 576,
          "title": "Variational Quanvolutional Neural Networks with enhanced image encoding. (arXiv:2106.07327v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1\">Rhea Sanjay Sukthanker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiwu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Suryansh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "Flow-based generative models have shown excellent ability to explicitly learn\nthe probability density function of data via a sequence of invertible\ntransformations. Yet, modeling long-range dependencies over normalizing flows\nremains understudied. To fill the gap, in this paper, we introduce two types of\ninvertible attention mechanisms for generative flow models. To be precise, we\npropose map-based and scaled dot-product attention for unconditional and\nconditional generative flow models. The key idea is to exploit split-based\nattention mechanisms to learn the attention weights and input representations\non every two splits of flow feature maps. Our method provides invertible\nattention modules with tractable Jacobian determinants, enabling seamless\nintegration of it at any positions of the flow-based models. The proposed\nattention mechanism can model the global data dependencies, leading to more\ncomprehensive flow models. Evaluation on multiple generation tasks demonstrates\nthat the introduced attention flow idea results in efficient flow models and\ncompares favorably against the state-of-the-art unconditional and conditional\ngenerative flow methods.",
          "link": "http://arxiv.org/abs/2106.03959",
          "publishedOn": "2021-06-24T01:51:43.388Z",
          "wordCount": 611,
          "title": "Generative Flows with Invertible Attentions. (arXiv:2106.03959v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09841",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esser_P/0/1/0/all/0/1\">Patrick Esser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1\">Robin Rombach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>",
          "description": "Designed to learn long-range interactions on sequential data, transformers\ncontinue to show state-of-the-art results on a wide variety of tasks. In\ncontrast to CNNs, they contain no inductive bias that prioritizes local\ninteractions. This makes them expressive, but also computationally infeasible\nfor long sequences, such as high-resolution images. We demonstrate how\ncombining the effectiveness of the inductive bias of CNNs with the expressivity\nof transformers enables them to model and thereby synthesize high-resolution\nimages. We show how to (i) use CNNs to learn a context-rich vocabulary of image\nconstituents, and in turn (ii) utilize transformers to efficiently model their\ncomposition within high-resolution images. Our approach is readily applied to\nconditional synthesis tasks, where both non-spatial information, such as object\nclasses, and spatial information, such as segmentations, can control the\ngenerated image. In particular, we present the first results on\nsemantically-guided synthesis of megapixel images with transformers and obtain\nthe state of the art among autoregressive models on class-conditional ImageNet.\nCode and pretrained models can be found at\nhttps://github.com/CompVis/taming-transformers .",
          "link": "http://arxiv.org/abs/2012.09841",
          "publishedOn": "2021-06-24T01:51:43.383Z",
          "wordCount": 645,
          "title": "Taming Transformers for High-Resolution Image Synthesis. (arXiv:2012.09841v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengfei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ruoxi Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1\">Kai Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuhao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1\">Guoen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bin Yan</a>",
          "description": "Deep neural networks(DNNs) is vulnerable to be attacked by adversarial\nexamples. Black-box attack is the most threatening attack. At present,\nblack-box attack methods mainly adopt gradient-based iterative attack methods,\nwhich usually limit the relationship between the iteration step size, the\nnumber of iterations, and the maximum perturbation. In this paper, we propose a\nnew gradient iteration framework, which redefines the relationship between the\nabove three. Under this framework, we easily improve the attack success rate of\nDI-TI-MIM. In addition, we propose a gradient iterative attack method based on\ninput dropout, which can be well combined with our framework. We further\npropose a multi dropout rate version of this method. Experimental results show\nthat our best method can achieve attack success rate of 96.2\\% for defense\nmodel on average, which is higher than the state-of-the-art gradient-based\nattacks.",
          "link": "http://arxiv.org/abs/2106.01617",
          "publishedOn": "2021-06-24T01:51:43.367Z",
          "wordCount": 606,
          "title": "Improving the Transferability of Adversarial Examples with New Iteration Framework and Input Dropout. (arXiv:2106.01617v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.15076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1\">Xiujun Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xianghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Person re-identification (re-ID) in the scenario with large spatial and\ntemporal spans has not been fully explored. This is partially because that,\nexisting benchmark datasets were mainly collected with limited spatial and\ntemporal ranges, e.g., using videos recorded in a few days by cameras in a\nspecific region of the campus. Such limited spatial and temporal ranges make it\nhard to simulate the difficulties of person re-ID in real scenarios. In this\nwork, we contribute a novel Large-scale Spatio-Temporal LaST person re-ID\ndataset, including 10,862 identities with more than 228k images. Compared with\nexisting datasets, LaST presents more challenging and high-diversity re-ID\nsettings, and significantly larger spatial and temporal ranges. For instance,\neach person can appear in different cities or countries, and in various time\nslots from daytime to night, and in different seasons from spring to winter. To\nour best knowledge, LaST is a novel person re-ID dataset with the largest\nspatio-temporal ranges. Based on LaST, we verified its challenge by conducting\na comprehensive performance evaluation of 14 re-ID algorithms. We further\npropose an easy-to-implement baseline that works well on such challenging re-ID\nsetting. We also verified that models pre-trained on LaST can generalize well\non existing datasets with short-term and cloth-changing scenarios. We expect\nLaST to inspire future works toward more realistic and challenging re-ID tasks.\nMore information about the dataset is available at\nhttps://github.com/shuxjweb/last.git.",
          "link": "http://arxiv.org/abs/2105.15076",
          "publishedOn": "2021-06-24T01:51:43.361Z",
          "wordCount": 704,
          "title": "Large-Scale Spatio-Temporal Person Re-identification: Algorithm and Benchmark. (arXiv:2105.15076v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shin_C/0/1/0/all/0/1\">Chajin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeoh Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangjin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>",
          "description": "Deep learning-based image inpainting algorithms have shown great performance\nvia powerful learned prior from the numerous external natural images. However,\nthey show unpleasant results on the test image whose distribution is far from\nthe that of training images because their models are biased toward the training\nimages. In this paper, we propose a simple image inpainting algorithm with\ntest-time adaptation named AdaFill. Given a single out-of-distributed test\nimage, our goal is to complete hole region more naturally than the pre-trained\ninpainting models. To achieve this goal, we treat remained valid regions of the\ntest image as another training cues because natural images have strong internal\nsimilarities. From this test-time adaptation, our network can exploit\nexternally learned image priors from the pre-trained features as well as the\ninternal prior of the test image explicitly. Experimental results show that\nAdaFill outperforms other models on the various out-of-distribution test\nimages. Furthermore, the model named ZeroFill, that are not pre-trained also\nsometimes outperforms the pre-trained models.",
          "link": "http://arxiv.org/abs/2102.01360",
          "publishedOn": "2021-06-24T01:51:43.356Z",
          "wordCount": 626,
          "title": "Test-Time Adaptation for Out-of-distributed Image Inpainting. (arXiv:2102.01360v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.07280",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mathew_S/0/1/0/all/0/1\">Shawn Mathew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nadeem_S/0/1/0/all/0/1\">Saad Nadeem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaufman_A/0/1/0/all/0/1\">Arie Kaufman</a>",
          "description": "Optical colonoscopy (OC), the most prevalent colon cancer screening tool, has\na high miss rate due to a number of factors, including the geometry of the\ncolon (haustral fold and sharp bends occlusions), endoscopist inexperience or\nfatigue, endoscope field of view, etc. We present a framework to visualize the\nmissed regions per-frame during the colonoscopy, and provides a workable\nclinical solution. Specifically, we make use of 3D reconstructed virtual\ncolonoscopy (VC) data and the insight that VC and OC share the same underlying\ngeometry but differ in color, texture and specular reflections, embedded in the\nOC domain. A lossy unpaired image-to-image translation model is introduced with\nenforced shared latent space for OC and VC. This shared latent space captures\nthe geometric information while deferring the color, texture, and specular\ninformation creation to additional Gaussian noise input. This additional noise\ninput can be utilized to generate one-to-many mappings from VC to OC and OC to\nOC. The code, data and trained models will be released via our Computational\nEndoscopy Platform at https://github.com/nadeemlab/CEP.",
          "link": "http://arxiv.org/abs/2101.07280",
          "publishedOn": "2021-06-24T01:51:43.349Z",
          "wordCount": 657,
          "title": "Visualizing Missing Surfaces In Colonoscopy Videos using Shared Latent Space Representations. (arXiv:2101.07280v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junru Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Ye Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>",
          "description": "Neural Architecture Search (NAS) often trains and evaluates a large number of\narchitectures. Recent predictor-based NAS approaches attempt to address such\nheavy computation costs with two key steps: sampling some\narchitecture-performance pairs and fitting a proxy accuracy predictor. Given\nlimited samples, these predictors, however, are far from accurate to locate top\narchitectures due to the difficulty of fitting the huge search space. This\npaper reflects on a simple yet crucial question: if our final goal is to find\nthe best architecture, do we really need to model the whole space well?. We\npropose a paradigm shift from fitting the whole architecture space using one\nstrong predictor, to progressively fitting a search path towards the\nhigh-performance sub-space through a set of weaker predictors. As a key\nproperty of the proposed weak predictors, their probabilities of sampling\nbetter architectures keep increasing. Hence we only sample a few well-performed\narchitectures guided by the previously learned predictor and estimate a new\nbetter weak predictor. This embarrassingly easy framework produces\ncoarse-to-fine iteration to refine the ranking of sampling space gradually.\nExtensive experiments demonstrate that our method costs fewer samples to find\ntop-performance architectures on NAS-Bench-101 and NAS-Bench-201, as well as\nachieves the state-of-the-art ImageNet performance on the NASNet search space.\nIn particular, compared to state-of-the-art (SOTA) predictor-based NAS methods,\nWeakNAS outperforms all of them with notable margins, e.g., requiring at least\n7.5x less samples to find global optimal on NAS-Bench-101; and WeakNAS can also\nabsorb them for further performance boost. We further strike the new SOTA\nresult of 81.3% in the ImageNet MobileNet Search Space. The code is available\nat https://github.com/VITA-Group/WeakNAS.",
          "link": "http://arxiv.org/abs/2102.10490",
          "publishedOn": "2021-06-24T01:51:43.337Z",
          "wordCount": 746,
          "title": "Stronger NAS with Weaker Predictors. (arXiv:2102.10490v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1\">Marcel C. B&#xfc;hler</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1\">Abhimitra Meka</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gengyan Li</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1\">Thabo Beeler</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a> (1) ((1) ETH Zurich, (2) Google)",
          "description": "Deep generative models have recently demonstrated the ability to synthesize\nphotorealistic images of human faces with novel identities. A key challenge to\nthe wide applicability of such techniques is to provide independent control\nover semantically meaningful parameters: appearance, head pose, face shape, and\nfacial expressions. In this paper, we propose VariTex - to the best of our\nknowledge the first method that learns a variational latent feature space of\nneural face textures, which allows sampling of novel identities. We combine\nthis generative model with a parametric face model and gain explicit control\nover head pose and facial expressions. To generate images of complete human\nheads, we propose an additive decoder that generates plausible additional\ndetails such as hair. A novel training scheme enforces a pose independent\nlatent space and in consequence, allows learning of a one-to-many mapping\nbetween latent codes and pose-conditioned exterior regions. The resulting\nmethod can generate geometrically consistent images of novel identities\nallowing fine-grained control over head pose, face shape, and facial\nexpressions, facilitating a broad range of downstream tasks, like sampling\nnovel identities, re-posing, expression transfer, and more.",
          "link": "http://arxiv.org/abs/2104.05988",
          "publishedOn": "2021-06-24T01:51:43.321Z",
          "wordCount": 665,
          "title": "VariTex: Variational Neural Face Textures. (arXiv:2104.05988v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Daoxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaolong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiawei Li</a>",
          "description": "Alongside the prevalence of mobile videos, the general public leans towards\nconsuming vertical videos on hand-held devices. To revitalize the exposure of\nhorizontal contents, we hereby set forth the exploration of automated\nhorizontal-to-vertical (abbreviated as H2V) video conversion with our proposed\nH2V framework, accompanied by an accurately annotated H2V-142K dataset.\nConcretely, H2V framework integrates video shot boundary detection, subject\nselection and multi-object tracking to facilitate the subject-preserving\nconversion, wherein the key is subject selection. To achieve so, we propose a\nRank-SS module that detects human objects, then selects the subject-to-preserve\nvia exploiting location, appearance, and salient cues. Afterward, the framework\nautomatically crops the video around the subject to produce vertical contents\nfrom horizontal sources. To build and evaluate our H2V framework, H2V-142K\ndataset is densely annotated with subject bounding boxes for 125 videos with\n132K frames and 9,500 video covers, upon which we demonstrate superior subject\nselection performance comparing to traditional salient approaches, and exhibit\npromising horizontal-to-vertical conversion performance overall. By publicizing\nthis dataset as well as our approach, we wish to pave the way for more valuable\nendeavors on the horizontal-to-vertical video conversion task.",
          "link": "http://arxiv.org/abs/2101.04051",
          "publishedOn": "2021-06-24T01:51:43.316Z",
          "wordCount": 653,
          "title": "Horizontal-to-Vertical Video Conversion. (arXiv:2101.04051v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.12380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomariz_A/0/1/0/all/0/1\">Alvaro Gomariz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portenier_T/0/1/0/all/0/1\">Tiziano Portenier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helbling_P/0/1/0/all/0/1\">Patrick M. Helbling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isringhausen_S/0/1/0/all/0/1\">Stephan Isringhausen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suessbier_U/0/1/0/all/0/1\">Ute Suessbier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nombela_Arrieta_C/0/1/0/all/0/1\">C&#xe9;sar Nombela-Arrieta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goksel_O/0/1/0/all/0/1\">Orcun Goksel</a>",
          "description": "Fluorescence microscopy allows for a detailed inspection of cells, cellular\nnetworks, and anatomical landmarks by staining with a variety of\ncarefully-selected markers visualized as color channels. Quantitative\ncharacterization of structures in acquired images often relies on automatic\nimage analysis methods. Despite the success of deep learning methods in other\nvision applications, their potential for fluorescence image analysis remains\nunderexploited. One reason lies in the considerable workload required to train\naccurate models, which are normally specific for a given combination of\nmarkers, and therefore applicable to a very restricted number of experimental\nsettings. We herein propose Marker Sampling and Excite, a neural network\napproach with a modality sampling strategy and a novel attention module that\ntogether enable (i) flexible training with heterogeneous datasets with\ncombinations of markers and (ii) successful utility of learned models on\narbitrary subsets of markers prospectively. We show that our single neural\nnetwork solution performs comparably to an upper bound scenario where an\nensemble of many networks is na\\\"ively trained for each possible marker\ncombination separately. In addition, we demonstrate the feasibility of this\nframework in high-throughput biological analysis by revising a recent\nquantitative characterization of bone marrow vasculature in 3D confocal\nmicroscopy datasets and further confirm the validity of our approach on an\nadditional, significantly different dataset of microvessels in fetal liver\ntissues. Not only can our work substantially ameliorate the use of deep\nlearning in fluorescence microscopy analysis, but it can also be utilized in\nother fields with incomplete data acquisitions and missing modalities.",
          "link": "http://arxiv.org/abs/2008.12380",
          "publishedOn": "2021-06-24T01:51:43.310Z",
          "wordCount": 758,
          "title": "Modality Attention and Sampling Enables Deep Learning with Heterogeneous Marker Combinations in Fluorescence Microscopy. (arXiv:2008.12380v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farias_T/0/1/0/all/0/1\">Tiago de Souza Farias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maziero_J/0/1/0/all/0/1\">Jonas Maziero</a>",
          "description": "We introduce feature alignment, a technique for obtaining approximate\nreversibility in artificial neural networks. By means of feature extraction, we\ncan train a neural network to learn an estimated map for its reverse process\nfrom outputs to inputs. Combined with variational autoencoders, we can generate\nnew samples from the same statistics as the training data. Improvements of the\nresults are obtained by using concepts from generative adversarial networks.\nFinally, we show that the technique can be modified for training neural\nnetworks locally, saving computational memory resources. Applying these\ntechniques, we report results for three vision generative tasks: MNIST,\nCIFAR-10, and celebA.",
          "link": "http://arxiv.org/abs/2106.12562",
          "publishedOn": "2021-06-24T01:51:43.304Z",
          "wordCount": 540,
          "title": "Feature Alignment for Approximated Reversibility in Neural Networks. (arXiv:2106.12562v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1710.00189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joseph_S/0/1/0/all/0/1\">S. Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ujir_H/0/1/0/all/0/1\">H. Ujir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hipiny_I/0/1/0/all/0/1\">I. Hipiny</a>",
          "description": "Classification of rocks is one of the fundamental tasks in a geological\nstudy. The process requires a human expert to examine sampled thin section\nimages under a microscope. In this study, we propose a method that uses\nmicroscope automation, digital image acquisition, edge detection and colour\nanalysis (histogram). We collected 60 digital images from 20 standard thin\nsections using a digital camera mounted on a conventional microscope. Each\nimage is partitioned into a finite number of cells that form a grid structure.\nEdge and colour profile of pixels inside each cell determine its\nclassification. The individual cells then determine the thin section image\nclassification via a majority voting scheme. Our method yielded successful\nresults as high as 90% to 100% precision.",
          "link": "http://arxiv.org/abs/1710.00189",
          "publishedOn": "2021-06-24T01:51:43.297Z",
          "wordCount": 612,
          "title": "Unsupervised Classification of Intrusive Igneous Rock Thin Section Images using Edge Detection and Colour Analysis. (arXiv:1710.00189v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.00707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jinhong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuhua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Lixin Duan</a>",
          "description": "Cross-domain object detection is challenging, because object detection model\nis often vulnerable to data variance, especially to the considerable domain\nshift between two distinctive domains. In this paper, we propose a new Unbiased\nMean Teacher (UMT) model for cross-domain object detection. We reveal that\nthere often exists a considerable model bias for the simple mean teacher (MT)\nmodel in cross-domain scenarios, and eliminate the model bias with several\nsimple yet highly effective strategies. In particular, for the teacher model,\nwe propose a cross-domain distillation method for MT to maximally exploit the\nexpertise of the teacher model. Moreover, for the student model, we alleviate\nits bias by augmenting training samples with pixel-level adaptation. Finally,\nfor the teaching process, we employ an out-of-distribution estimation strategy\nto select samples that most fit the current model to further enhance the\ncross-domain distillation process. By tackling the model bias issue with these\nstrategies, our UMT model achieves mAPs of 44.1%, 58.1%, 41.7%, and 43.1% on\nbenchmark datasets Clipart1k, Watercolor2k, Foggy Cityscapes, and Cityscapes,\nrespectively, which outperforms the existing state-of-the-art results in\nnotable margins. Our implementation is available at\nhttps://github.com/kinredon/umt.",
          "link": "http://arxiv.org/abs/2003.00707",
          "publishedOn": "2021-06-24T01:51:43.262Z",
          "wordCount": 652,
          "title": "Unbiased Mean Teacher for Cross-domain Object Detection. (arXiv:2003.00707v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10823",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Temniranrat_P/0/1/0/all/0/1\">Pitchayagan Temniranrat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiratiratanapruk_K/0/1/0/all/0/1\">Kantip Kiratiratanapruk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kitvimonrat_A/0/1/0/all/0/1\">Apichon Kitvimonrat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sinthupinyo_W/0/1/0/all/0/1\">Wasin Sinthupinyo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patarapuwadol_S/0/1/0/all/0/1\">Sujin Patarapuwadol</a>",
          "description": "A LINE Bot System to diagnose rice diseases from actual paddy field images\nwas developed and presented in this paper. It was easy-to-use and automatic\nsystem designed to help rice farmers improve the rice yield and quality. The\ntargeted images were taken from the actual paddy environment without special\nsample preparation. We used a deep learning neural networks technique to detect\nrice diseases from the images. We developed an object detection model training\nand refinement process to improve the performance of our previous research on\nrice leave diseases detection. The process was based on analyzing the model's\npredictive results and could be repeatedly used to improve the quality of the\ndatabase in the next training of the model. The deployment model for our LINE\nBot system was created from the selected best performance technique in our\nprevious paper, YOLOv3, trained by refined training data set. The performance\nof the deployment model was measured on 5 target classes and found that the\nAverage True Positive Point improved from 91.1% in the previous paper to 95.6%\nin this study. Therefore, we used this deployment model for Rice Disease LINE\nBot system. Our system worked automatically real-time to suggest primary\ndiagnosis results to the users in the LINE group, which included rice farmers\nand rice disease specialists. They could communicate freely via chat. In the\nreal LINE Bot deployment, the model's performance was measured by our own\ndefined measurement Average True Positive Point and was found to be an average\nof 78.86%. The system was fast and took only 2-3 s for detection process in our\nsystem server.",
          "link": "http://arxiv.org/abs/2011.10823",
          "publishedOn": "2021-06-24T01:51:43.256Z",
          "wordCount": 761,
          "title": "A System for Automatic Rice Disease Detection from Rice Paddy Images Serviced via a Chatbot. (arXiv:2011.10823v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.07849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1\">Takuhiro Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>",
          "description": "Generative adversarial networks (GANs) have gained considerable attention\nowing to their ability to reproduce images. However, they can recreate training\nimages faithfully despite image degradation in the form of blur, noise, and\ncompression, generating similarly degraded images. To solve this problem, the\nrecently proposed noise robust GAN (NR-GAN) provides a partial solution by\ndemonstrating the ability to learn a clean image generator directly from noisy\nimages using a two-generator model comprising image and noise generators.\nHowever, its application is limited to noise, which is relatively easy to\ndecompose owing to its additive and reversible characteristics, and its\napplication to irreversible image degradation, in the form of blur,\ncompression, and combination of all, remains a challenge. To address these\nproblems, we propose blur, noise, and compression robust GAN (BNCR-GAN) that\ncan learn a clean image generator directly from degraded images without\nknowledge of degradation parameters (e.g., blur kernel types, noise amounts, or\nquality factor values). Inspired by NR-GAN, BNCR-GAN uses a multiple-generator\nmodel composed of image, blur-kernel, noise, and quality-factor generators.\nHowever, in contrast to NR-GAN, to address irreversible characteristics, we\nintroduce masking architectures adjusting degradation strength values in a\ndata-driven manner using bypasses before and after degradation. Furthermore, to\nsuppress uncertainty caused by the combination of blur, noise, and compression,\nwe introduce adaptive consistency losses imposing consistency between\nirreversible degradation processes according to the degradation strengths. We\ndemonstrate the effectiveness of BNCR-GAN through large-scale comparative\nstudies on CIFAR-10 and a generality analysis on FFHQ. In addition, we\ndemonstrate the applicability of BNCR-GAN in image restoration.",
          "link": "http://arxiv.org/abs/2003.07849",
          "publishedOn": "2021-06-24T01:51:43.250Z",
          "wordCount": 740,
          "title": "Blur, Noise, and Compression Robust Generative Adversarial Networks. (arXiv:2003.07849v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krylov_I/0/1/0/all/0/1\">Ilya Krylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nosov_S/0/1/0/all/0/1\">Sergei Nosov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sovrasov_V/0/1/0/all/0/1\">Vladislav Sovrasov</a>",
          "description": "A large scale human-labeled dataset plays an important role in creating high\nquality deep learning models. In this paper we present text annotation for Open\nImages V5 dataset. To our knowledge it is the largest among publicly available\nmanually created text annotations. Having this annotation we trained a simple\nMask-RCNN-based network, referred as Yet Another Mask Text Spotter (YAMTS),\nwhich achieves competitive performance or even outperforms current\nstate-of-the-art approaches in some cases on ICDAR2013, ICDAR2015 and\nTotal-Text datasets. Code for text spotting model available online at:\nhttps://github.com/openvinotoolkit/training_extensions. The model can be\nexported to OpenVINO-format and run on Intel CPUs.",
          "link": "http://arxiv.org/abs/2106.12326",
          "publishedOn": "2021-06-24T01:51:43.244Z",
          "wordCount": 541,
          "title": "Open Images V5 Text Annotation and Yet Another Mask Text Spotter. (arXiv:2106.12326v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1\">Apratim Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reino_D/0/1/0/all/0/1\">Daniel Olmeda Reino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>",
          "description": "Accurate prediction of pedestrian and bicyclist paths is integral to the\ndevelopment of reliable autonomous vehicles in dense urban environments. The\ninteractions between vehicle and pedestrian or bicyclist have a significant\nimpact on the trajectories of traffic participants e.g. stopping or turning to\navoid collisions. Although recent datasets and trajectory prediction approaches\nhave fostered the development of autonomous vehicles yet the amount of\nvehicle-pedestrian (bicyclist) interactions modeled are sparse. In this work,\nwe propose Euro-PVI, a dataset of pedestrian and bicyclist trajectories. In\nparticular, our dataset caters more diverse and complex interactions in dense\nurban scenarios compared to the existing datasets. To address the challenges in\npredicting future trajectories with dense interactions, we develop a joint\ninference model that learns an expressive multi-modal shared latent space\nacross agents in the urban scene. This enables our Joint-$\\beta$-cVAE approach\nto better model the distribution of future trajectories. We achieve state of\nthe art results on the nuScenes and Euro-PVI datasets demonstrating the\nimportance of capturing interactions between ego-vehicle and pedestrians\n(bicyclists) for accurate predictions.",
          "link": "http://arxiv.org/abs/2106.12442",
          "publishedOn": "2021-06-24T01:51:43.229Z",
          "wordCount": 622,
          "title": "Euro-PVI: Pedestrian Vehicle Interactions in Dense Urban Centers. (arXiv:2106.12442v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12522",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mathew_S/0/1/0/all/0/1\">Shawn Mathew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nadeem_S/0/1/0/all/0/1\">Saad Nadeem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaufman_A/0/1/0/all/0/1\">Arie Kaufman</a>",
          "description": "Haustral folds are colon wall protrusions implicated for high polyp miss rate\nduring optical colonoscopy procedures. If segmented accurately, haustral folds\ncan allow for better estimation of missed surface and can also serve as\nvaluable landmarks for registering pre-treatment virtual (CT) and optical\ncolonoscopies, to guide navigation towards the anomalies found in pre-treatment\nscans. We present a novel generative adversarial network, FoldIt, for\nfeature-consistent image translation of optical colonoscopy videos to virtual\ncolonoscopy renderings with haustral fold overlays. A new transitive loss is\nintroduced in order to leverage ground truth information between haustral fold\nannotations and virtual colonoscopy renderings. We demonstrate the\neffectiveness of our model on real challenging optical colonoscopy videos as\nwell as on textured virtual colonoscopy videos with clinician-verified haustral\nfold annotations. All code and scripts to reproduce the experiments of this\npaper will be made available via our Computational Endoscopy Platform at\nhttps://github.com/nadeemlab/CEP.",
          "link": "http://arxiv.org/abs/2106.12522",
          "publishedOn": "2021-06-24T01:51:43.220Z",
          "wordCount": 606,
          "title": "FoldIt: Haustral Folds Detection and Segmentation in Colonoscopy Videos. (arXiv:2106.12522v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12313",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhongliang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihao Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xuechen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>",
          "description": "The Coronavirus disease 2019 (COVID-19) has rapidly spread all over the world\nsince its first report in December 2019 and thoracic computed tomography (CT)\nhas become one of the main tools for its diagnosis. In recent years, deep\nlearning-based approaches have shown impressive performance in myriad image\nrecognition tasks. However, they usually require a large number of annotated\ndata for training. Inspired by Ground Glass Opacity (GGO), a common finding in\nCOIVD-19 patient's CT scans, we proposed in this paper a novel self-supervised\npretraining method based on pseudo lesions generation and restoration for\nCOVID-19 diagnosis. We used Perlin noise, a gradient noise based mathematical\nmodel, to generate lesion-like patterns, which were then randomly pasted to the\nlung regions of normal CT images to generate pseudo COVID-19 images. The pairs\nof normal and pseudo COVID-19 images were then used to train an encoder-decoder\narchitecture based U-Net for image restoration, which does not require any\nlabelled data. The pretrained encoder was then fine-tuned using labelled data\nfor COVID-19 diagnosis task. Two public COVID-19 diagnosis datasets made up of\nCT images were employed for evaluation. Comprehensive experimental results\ndemonstrated that the proposed self-supervised learning approach could extract\nbetter feature representation for COVID-19 diagnosis and the accuracy of the\nproposed method outperformed the supervised model pretrained on large scale\nimages by 6.57% and 3.03% on SARS-CoV-2 dataset and Jinan COVID-19 dataset,\nrespectively.",
          "link": "http://arxiv.org/abs/2106.12313",
          "publishedOn": "2021-06-24T01:51:43.214Z",
          "wordCount": 721,
          "title": "Learning from Pseudo Lesion: A Self-supervised Framework for COVID-19 Diagnosis. (arXiv:2106.12313v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.07991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deza_A/0/1/0/all/0/1\">Arturo Deza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konkle_T/0/1/0/all/0/1\">Talia Konkle</a>",
          "description": "The goal of this work is to characterize the representational impact that\nfoveation operations have for machine vision systems, inspired by the foveated\nhuman visual system, which has higher acuity at the center of gaze and\ntexture-like encoding in the periphery. To do so, we introduce models\nconsisting of a first-stage \\textit{fixed} image transform followed by a\nsecond-stage \\textit{learnable} convolutional neural network, and we varied the\nfirst stage component. The primary model has a foveated-textural input stage,\nwhich we compare to a model with foveated-blurred input and a model with\nspatially-uniform blurred input (both matched for perceptual compression), and\na final reference model with minimal input-based compression. We find that: 1)\nthe foveated-texture model shows similar scene classification accuracy as the\nreference model despite its compressed input, with greater i.i.d.\ngeneralization than the other models; 2) the foveated-texture model has greater\nsensitivity to high-spatial frequency information and greater robustness to\nocclusion, w.r.t the comparison models; 3) both the foveated systems, show a\nstronger center image-bias relative to the spatially-uniform systems even with\na weight sharing constraint. Critically, these results are preserved over\ndifferent classical CNN architectures throughout their learning dynamics.\nAltogether, this suggests that foveation with peripheral texture-based\ncomputations yields an efficient, distinct, and robust representational format\nof scene information, and provides symbiotic computational insight into the\nrepresentational consequences that texture-based peripheral encoding may have\nfor processing in the human visual system, while also potentially inspiring the\nnext generation of computer vision models via spatially-adaptive computation.\nCode + Data available here: https://github.com/ArturoDeza/EmergentProperties",
          "link": "http://arxiv.org/abs/2006.07991",
          "publishedOn": "2021-06-24T01:51:43.207Z",
          "wordCount": 752,
          "title": "Emergent Properties of Foveated Perceptual Systems. (arXiv:2006.07991v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12545",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Odeh_I/0/1/0/all/0/1\">Israa Odeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alkasassbeh_M/0/1/0/all/0/1\">Mouhammd Alkasassbeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alauthman_M/0/1/0/all/0/1\">Mohammad Alauthman</a>",
          "description": "Diabetic Retinopathy (DR) is among the worlds leading vision loss causes in\ndiabetic patients. DR is a microvascular disease that affects the eye retina,\nwhich causes vessel blockage and therefore cuts the main source of nutrition\nfor the retina tissues. Treatment for this visual disorder is most effective\nwhen it is detected in its earliest stages, as severe DR can result in\nirreversible blindness. Nonetheless, DR identification requires the expertise\nof Ophthalmologists which is often expensive and time-consuming. Therefore,\nautomatic detection systems were introduced aiming to facilitate the\nidentification process, making it available globally in a time and\ncost-efficient manner. However, due to the limited reliable datasets and\nmedical records for this particular eye disease, the obtained predictions\naccuracies were relatively unsatisfying for eye specialists to rely on them as\ndiagnostic systems. Thus, we explored an ensemble-based learning strategy,\nmerging a substantial selection of well-known classification algorithms in one\nsophisticated diagnostic model. The proposed framework achieved the highest\naccuracy rates among all other common classification algorithms in the area. 4\nsubdatasets were generated to contain the top 5 and top 10 features of the\nMessidor dataset, selected by InfoGainEval. and WrapperSubsetEval., accuracies\nof 70.7% and 75.1% were achieved on the InfoGainEval. top 5 and original\ndataset respectively. The results imply the impressive performance of the\nsubdataset, which significantly conduces to a less complex classification\nprocess",
          "link": "http://arxiv.org/abs/2106.12545",
          "publishedOn": "2021-06-24T01:51:43.201Z",
          "wordCount": 669,
          "title": "Diabetic Retinopathy Detection using Ensemble Machine Learning. (arXiv:2106.12545v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhengqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1\">Tianyu Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zihui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonglong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>",
          "description": "Transformers recently are adapted from the community of natural language\nprocessing as a promising substitute of convolution-based neural networks for\nvisual learning tasks. However, its supremacy degenerates given an insufficient\namount of training data (e.g., ImageNet). To make it into practical utility, we\npropose a novel distillation-based method to train vision transformers. Unlike\nprevious works, where merely heavy convolution-based teachers are provided, we\nintroduce lightweight teachers with different architectural inductive biases\n(e.g., convolution and involution) to co-advise the student transformer. The\nkey is that teachers with different inductive biases attain different knowledge\ndespite that they are trained on the same dataset, and such different knowledge\ncompounds and boosts the student's performance during distillation. Equipped\nwith this cross inductive bias distillation method, our vision transformers\n(termed as CivT) outperform all previous transformers of the same architecture\non ImageNet.",
          "link": "http://arxiv.org/abs/2106.12378",
          "publishedOn": "2021-06-24T01:51:43.196Z",
          "wordCount": 577,
          "title": "Co-advise: Cross Inductive Bias Distillation. (arXiv:2106.12378v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shaoqing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dingfu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Junbo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bin_Z/0/1/0/all/0/1\">Zhou Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjun Zhang</a>",
          "description": "Accurate detection of obstacles in 3D is an essential task for autonomous\ndriving and intelligent transportation. In this work, we propose a general\nmultimodal fusion framework FusionPainting to fuse the 2D RGB image and 3D\npoint clouds at a semantic level for boosting the 3D object detection task.\nEspecially, the FusionPainting framework consists of three main modules: a\nmulti-modal semantic segmentation module, an adaptive attention-based semantic\nfusion module, and a 3D object detector. First, semantic information is\nobtained for 2D images and 3D Lidar point clouds based on 2D and 3D\nsegmentation approaches. Then the segmentation results from different sensors\nare adaptively fused based on the proposed attention-based semantic fusion\nmodule. Finally, the point clouds painted with the fused semantic label are\nsent to the 3D detector for obtaining the 3D objection results. The\neffectiveness of the proposed framework has been verified on the large-scale\nnuScenes detection benchmark by comparing it with three different baselines.\nThe experimental results show that the fusion strategy can significantly\nimprove the detection performance compared to the methods using only point\nclouds, and the methods using point clouds only painted with 2D segmentation\ninformation. Furthermore, the proposed approach outperforms other\nstate-of-the-art methods on the nuScenes testing benchmark.",
          "link": "http://arxiv.org/abs/2106.12449",
          "publishedOn": "2021-06-24T01:51:43.177Z",
          "wordCount": 650,
          "title": "FusionPainting: Multimodal Fusion with Adaptive Attention for 3D Object Detection. (arXiv:2106.12449v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1\">Tero Karras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1\">Miika Aittala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1\">Samuli Laine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1\">Erik H&#xe4;rk&#xf6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1\">Janne Hellsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1\">Jaakko Lehtinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1\">Timo Aila</a>",
          "description": "We observe that despite their hierarchical convolutional nature, the\nsynthesis process of typical generative adversarial networks depends on\nabsolute pixel coordinates in an unhealthy manner. This manifests itself as,\ne.g., detail appearing to be glued to image coordinates instead of the surfaces\nof depicted objects. We trace the root cause to careless signal processing that\ncauses aliasing in the generator network. Interpreting all signals in the\nnetwork as continuous, we derive generally applicable, small architectural\nchanges that guarantee that unwanted information cannot leak into the\nhierarchical synthesis process. The resulting networks match the FID of\nStyleGAN2 but differ dramatically in their internal representations, and they\nare fully equivariant to translation and rotation even at subpixel scales. Our\nresults pave the way for generative models better suited for video and\nanimation.",
          "link": "http://arxiv.org/abs/2106.12423",
          "publishedOn": "2021-06-24T01:51:43.171Z",
          "wordCount": 583,
          "title": "Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madadi_M/0/1/0/all/0/1\">Meysam Madadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertiche_H/0/1/0/all/0/1\">Hugo Bertiche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>",
          "description": "In this paper we propose the first deep unsupervised approach in human body\nreconstruction to estimate body surface from a sparse set of landmarks, so\ncalled DeepMurf. We apply a denoising autoencoder to estimate missing\nlandmarks. Then we apply an attention model to estimate body joints from\nlandmarks. Finally, a cascading network is applied to regress parameters of a\nstatistical generative model that reconstructs body. Our set of proposed loss\nfunctions allows us to train the network in an unsupervised way. Results on\nfour public datasets show that our approach accurately reconstructs the human\nbody from real world mocap data.",
          "link": "http://arxiv.org/abs/2106.12282",
          "publishedOn": "2021-06-24T01:51:43.163Z",
          "wordCount": 548,
          "title": "Deep unsupervised 3D human body reconstruction from a sparse set of landmarks. (arXiv:2106.12282v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.07177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghiasi_G/0/1/0/all/0/1\">Golnaz Ghiasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1\">Aravind Srinivas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cubuk_E/0/1/0/all/0/1\">Ekin D. Cubuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>",
          "description": "Building instance segmentation models that are data-efficient and can handle\nrare object categories is an important challenge in computer vision. Leveraging\ndata augmentations is a promising direction towards addressing this challenge.\nHere, we perform a systematic study of the Copy-Paste augmentation ([13, 12])\nfor instance segmentation where we randomly paste objects onto an image. Prior\nstudies on Copy-Paste relied on modeling the surrounding visual context for\npasting the objects. However, we find that the simple mechanism of pasting\nobjects randomly is good enough and can provide solid gains on top of strong\nbaselines. Furthermore, we show Copy-Paste is additive with semi-supervised\nmethods that leverage extra data through pseudo labeling (e.g. self-training).\nOn COCO instance segmentation, we achieve 49.1 mask AP and 57.3 box AP, an\nimprovement of +0.6 mask AP and +1.5 box AP over the previous state-of-the-art.\nWe further demonstrate that Copy-Paste can lead to significant improvements on\nthe LVIS benchmark. Our baseline model outperforms the LVIS 2020 Challenge\nwinning entry by +3.6 mask AP on rare categories.",
          "link": "http://arxiv.org/abs/2012.07177",
          "publishedOn": "2021-06-24T01:51:43.157Z",
          "wordCount": 657,
          "title": "Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation. (arXiv:2012.07177v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1710.00187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hipiny_I/0/1/0/all/0/1\">I. Hipiny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ujir_H/0/1/0/all/0/1\">H. Ujir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minoi_J/0/1/0/all/0/1\">J.L. Minoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_S/0/1/0/all/0/1\">S.F. Samson Juan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khairuddin_M/0/1/0/all/0/1\">M.A. Khairuddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunar_M/0/1/0/all/0/1\">M.S. Sunar</a>",
          "description": "Unsupervised segmentation of action segments in egocentric videos is a\ndesirable feature in tasks such as activity recognition and content-based video\nretrieval. Reducing the search space into a finite set of action segments\nfacilitates a faster and less noisy matching. However, there exist a\nsubstantial gap in machine understanding of natural temporal cuts during a\ncontinuous human activity. This work reports on a novel gaze-based approach for\nsegmenting action segments in videos captured using an egocentric camera. Gaze\nis used to locate the region-of-interest inside a frame. By tracking two simple\nmotion-based parameters inside successive regions-of-interest, we discover a\nfinite set of temporal cuts. We present several results using combinations (of\nthe two parameters) on a dataset, i.e., BRISGAZE-ACTIONS. The dataset contains\negocentric videos depicting several daily-living activities. The quality of the\ntemporal cuts is further improved by implementing two entropy measures.",
          "link": "http://arxiv.org/abs/1710.00187",
          "publishedOn": "2021-06-24T01:51:43.141Z",
          "wordCount": 636,
          "title": "Unsupervised Segmentation of Action Segments in Egocentric Videos using Gaze. (arXiv:1710.00187v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Widdicombe_A/0/1/0/all/0/1\">Amy Widdicombe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Julier_S/0/1/0/all/0/1\">Simon J. Julier</a>",
          "description": "Binarized Neural Networks (BNNs) have the potential to revolutionize the way\nthat deep learning is carried out in edge computing platforms. However, the\neffectiveness of interpretability methods on these networks has not been\nassessed.\n\nIn this paper, we compare the performance of several widely used saliency\nmap-based interpretabilty techniques (Gradient, SmoothGrad and GradCAM), when\napplied to Binarized or Full Precision Neural Networks (FPNNs). We found that\nthe basic Gradient method produces very similar-looking maps for both types of\nnetwork. However, SmoothGrad produces significantly noisier maps for BNNs.\nGradCAM also produces saliency maps which differ between network types, with\nsome of the BNNs having seemingly nonsensical explanations. We comment on\npossible reasons for these differences in explanations and present it as an\nexample of why interpretability techniques should be tested on a wider range of\nnetwork types.",
          "link": "http://arxiv.org/abs/2106.12569",
          "publishedOn": "2021-06-24T01:51:43.126Z",
          "wordCount": 592,
          "title": "Gradient-Based Interpretability Methods and Binarized Neural Networks. (arXiv:2106.12569v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puyol_Anton_E/0/1/0/all/0/1\">Esther Puyol-Anton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruijsink_B/0/1/0/all/0/1\">Bram Ruijsink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piechnik_S/0/1/0/all/0/1\">Stefan K. Piechnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubauer_S/0/1/0/all/0/1\">Stefan Neubauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_S/0/1/0/all/0/1\">Steffen E. Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_R/0/1/0/all/0/1\">Reza Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_A/0/1/0/all/0/1\">Andrew P. King</a>",
          "description": "The subject of \"fairness\" in artificial intelligence (AI) refers to assessing\nAI algorithms for potential bias based on demographic characteristics such as\nrace and gender, and the development of algorithms to address this bias. Most\napplications to date have been in computer vision, although some work in\nhealthcare has started to emerge. The use of deep learning (DL) in cardiac MR\nsegmentation has led to impressive results in recent years, and such techniques\nare starting to be translated into clinical practice. However, no work has yet\ninvestigated the fairness of such models. In this work, we perform such an\nanalysis for racial/gender groups, focusing on the problem of training data\nimbalance, using a nnU-Net model trained and evaluated on cine short axis\ncardiac MR data from the UK Biobank dataset, consisting of 5,903 subjects from\n6 different racial groups. We find statistically significant differences in\nDice performance between different racial groups. To reduce the racial bias, we\ninvestigated three strategies: (1) stratified batch sampling, in which batch\nsampling is stratified to ensure balance between racial groups; (2) fair\nmeta-learning for segmentation, in which a DL classifier is trained to classify\nrace and jointly optimized with the segmentation model; and (3) protected group\nmodels, in which a different segmentation model is trained for each racial\ngroup. We also compared the results to the scenario where we have a perfectly\nbalanced database. To assess fairness we used the standard deviation (SD) and\nskewed error ratio (SER) of the average Dice values. Our results demonstrate\nthat the racial bias results from the use of imbalanced training data, and that\nall proposed bias mitigation strategies improved fairness, with the best SD and\nSER resulting from the use of protected group models.",
          "link": "http://arxiv.org/abs/2106.12387",
          "publishedOn": "2021-06-24T01:51:43.120Z",
          "wordCount": 762,
          "title": "Fairness in Cardiac MR Image Analysis: An Investigation of Bias Due to Data Imbalance in Deep Learning Based Segmentation. (arXiv:2106.12387v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dilber_T/0/1/0/all/0/1\">Talha Dilber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzel_M/0/1/0/all/0/1\">Mehmet Serdar Guzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bostanci_E/0/1/0/all/0/1\">Erkan Bostanci</a>",
          "description": "In today's world, the amount of data produced in every field has increased at\nan unexpected level. In the face of increasing data, the importance of data\nprocessing has increased remarkably. Our resource topic is on the processing of\nvideo data, which has an important place in increasing data, and the production\nof summary videos. Within the scope of this resource, a new method for anomaly\ndetection with object-based unsupervised learning has been developed while\ncreating a video summary. By using this method, the video data is processed as\npixels and the result is produced as a video segment. The process flow can be\nbriefly summarized as follows. Objects on the video are detected according to\ntheir type, and then they are tracked. Then, the tracking history data of the\nobjects are processed, and the classifier is trained with the object type.\nThanks to this classifier, anomaly behavior of objects is detected. Video\nsegments are determined by processing video moments containing anomaly\nbehaviors. The video summary is created by extracting the detected video\nsegments from the original video and combining them. The model we developed has\nbeen tested and verified separately for single camera and dual camera systems.",
          "link": "http://arxiv.org/abs/2106.12362",
          "publishedOn": "2021-06-24T01:51:43.114Z",
          "wordCount": 641,
          "title": "A new Video Synopsis Based Approach Using Stereo Camera. (arXiv:2106.12362v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12303",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ho_K/0/1/0/all/0/1\">Kalun Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfreundt_F/0/1/0/all/0/1\">Franz-Josef Pfreundt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1\">Janis Keuper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1\">Margret Keuper</a>",
          "description": "Over the last decade, the development of deep image classification networks\nhas mostly been driven by the search for the best performance in terms of\nclassification accuracy on standardized benchmarks like ImageNet. More\nrecently, this focus has been expanded by the notion of model robustness, i.e.\nthe generalization abilities of models towards previously unseen changes in the\ndata distribution. While new benchmarks, like ImageNet-C, have been introduced\nto measure robustness properties, we argue that fixed testsets are only able to\ncapture a small portion of possible data variations and are thus limited and\nprone to generate new overfitted solutions. To overcome these drawbacks, we\nsuggest to estimate the robustness of a model directly from the structure of\nits learned feature-space. We introduce robustness indicators which are\nobtained via unsupervised clustering of latent representations inside a trained\nclassifier and show very high correlations to the model performance on\ncorrupted test data.",
          "link": "http://arxiv.org/abs/2106.12303",
          "publishedOn": "2021-06-24T01:51:43.109Z",
          "wordCount": 595,
          "title": "Estimating the Robustness of Classification Models by the Structure of the Learned Feature-Space. (arXiv:2106.12303v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mengdi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Ximeng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1\">Mufeng Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhe Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiangxi Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chuanqing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1\">Qiushi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yanye Lu</a>",
          "description": "Diabetic retinopathy (DR) remains the most prevalent cause of vision\nimpairment and irreversible blindness in the working-age adults. Due to the\nrenaissance of deep learning (DL), DL-based DR diagnosis has become a promising\ntool for the early screening and severity grading of DR. However, training deep\nneural networks (DNNs) requires an enormous amount of carefully labeled data.\nNoisy label data may be introduced when labeling plenty of data, degrading the\nperformance of models. In this work, we propose a novel label management\nmechanism (LMM) for the DNN to overcome overfitting on the noisy data. LMM\nutilizes maximum posteriori probability (MAP) in the Bayesian statistic and\ntime-weighted technique to selectively correct the labels of unclean data,\nwhich gradually purify the training data and improve classification\nperformance. Comprehensive experiments on both synthetic noise data (Messidor\n\\& our collected DR dataset) and real-world noise data (ANIMAL-10N)\ndemonstrated that LMM could boost performance of models and is superior to\nthree state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.12284",
          "publishedOn": "2021-06-24T01:51:43.104Z",
          "wordCount": 621,
          "title": "A Label Management Mechanism for Retinal Fundus Image Classification of Diabetic Retinopathy. (arXiv:2106.12284v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qibin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zihang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "In this paper, we present Vision Permutator, a conceptually simple and data\nefficient MLP-like architecture for visual recognition. By realizing the\nimportance of the positional information carried by 2D feature representations,\nunlike recent MLP-like models that encode the spatial information along the\nflattened spatial dimensions, Vision Permutator separately encodes the feature\nrepresentations along the height and width dimensions with linear projections.\nThis allows Vision Permutator to capture long-range dependencies along one\nspatial direction and meanwhile preserve precise positional information along\nthe other direction. The resulting position-sensitive outputs are then\naggregated in a mutually complementing manner to form expressive\nrepresentations of the objects of interest. We show that our Vision Permutators\nare formidable competitors to convolutional neural networks (CNNs) and vision\ntransformers. Without the dependence on spatial convolutions or attention\nmechanisms, Vision Permutator achieves 81.5% top-1 accuracy on ImageNet without\nextra large-scale training data (e.g., ImageNet-22k) using only 25M learnable\nparameters, which is much better than most CNNs and vision transformers under\nthe same model size constraint. When scaling up to 88M, it attains 83.2% top-1\naccuracy. We hope this work could encourage research on rethinking the way of\nencoding spatial information and facilitate the development of MLP-like models.\nCode is available at https://github.com/Andrew-Qibin/VisionPermutator.",
          "link": "http://arxiv.org/abs/2106.12368",
          "publishedOn": "2021-06-24T01:51:43.087Z",
          "wordCount": 649,
          "title": "Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition. (arXiv:2106.12368v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowakowski_A/0/1/0/all/0/1\">Artur Nowakowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puglisi_E/0/1/0/all/0/1\">Erika Puglisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mifdal_J/0/1/0/all/0/1\">Jamila Mifdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirri_F/0/1/0/all/0/1\">Fiora Pirri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "The abundance of clouds, located both spatially and temporally, often makes\nremote sensing applications with optical images difficult or even impossible.\nIn this manuscript, a novel method for clouds-corrupted optical image\nrestoration has been presented and developed, based on a joint data fusion\nparadigm, where three deep neural networks have been combined in order to fuse\nspatio-temporal features extracted from Sentinel-1 and Sentinel-2 time-series\nof data. It is worth highlighting that both the code and the dataset have been\nimplemented from scratch and made available to interested research for further\nanalysis and investigation.",
          "link": "http://arxiv.org/abs/2106.12226",
          "publishedOn": "2021-06-24T01:51:43.082Z",
          "wordCount": 554,
          "title": "Sentinel-1 and Sentinel-2 Spatio-Temporal Data Fusion for Clouds Removal. (arXiv:2106.12226v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12023",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1\">Raivo Koot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haiping Lu</a>",
          "description": "This report describes the technical details of our submission to the\nEPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for Action\nRecognition. The EPIC-Kitchens dataset is more difficult than other video\ndomain adaptation datasets due to multi-tasks with more modalities. Firstly, to\nparticipate in the challenge, we employ a transformer to capture the spatial\ninformation from each modality. Secondly, we employ a temporal attention module\nto model temporal-wise inter-dependency. Thirdly, we employ the adversarial\ndomain adaptation network to learn the general features between labeled source\nand unlabeled target domain. Finally, we incorporate multiple modalities to\nimprove the performance by a three-stream network with late fusion. Our network\nachieves the comparable performance with the state-of-the-art baseline T$A^3$N\nand outperforms the baseline on top-1 accuracy for verb class and top-5\naccuracies for all three tasks which are verb, noun and action. Under the team\nname xy9, our submission achieved 5th place in terms of top-1 accuracy for verb\nclass and all top-5 accuracies.",
          "link": "http://arxiv.org/abs/2106.12023",
          "publishedOn": "2021-06-24T01:51:43.076Z",
          "wordCount": 611,
          "title": "Team PyKale (xy9) Submission to the EPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for Action Recognition. (arXiv:2106.12023v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roland S. Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borowski_J/0/1/0/all/0/1\">Judy Borowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1\">Robert Geirhos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallis_T/0/1/0/all/0/1\">Thomas S. A. Wallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>",
          "description": "One widely used approach towards understanding the inner workings of deep\nconvolutional neural networks is to visualize unit responses via activation\nmaximization. Feature visualizations via activation maximization are thought to\nprovide humans with precise information about the image features that cause a\nunit to be activated. If this is indeed true, these synthetic images should\nenable humans to predict the effect of an intervention, such as whether\noccluding a certain patch of the image (say, a dog's head) changes a unit's\nactivation. Here, we test this hypothesis by asking humans to predict which of\ntwo square occlusions causes a larger change to a unit's activation. Both a\nlarge-scale crowdsourced experiment and measurements with experts show that on\naverage, the extremely activating feature visualizations by Olah et al. (2017)\nindeed help humans on this task ($67 \\pm 4\\%$ accuracy; baseline performance\nwithout any visualizations is $60 \\pm 3\\%$). However, they do not provide any\nsignificant advantage over other visualizations (such as e.g. dataset samples),\nwhich yield similar performance ($66 \\pm 3\\%$ to $67 \\pm 3\\%$ accuracy). Taken\ntogether, we propose an objective psychophysical task to quantify the benefit\nof unit-level interpretability methods for humans, and find no evidence that\nfeature visualizations provide humans with better \"causal understanding\" than\nsimple alternative visualizations.",
          "link": "http://arxiv.org/abs/2106.12447",
          "publishedOn": "2021-06-24T01:51:43.062Z",
          "wordCount": 688,
          "title": "How Well do Feature Visualizations Support Causal Understanding of CNN Activations?. (arXiv:2106.12447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1\">Maureen Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1\">Jiachen Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timothy_R/0/1/0/all/0/1\">Reese Timothy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1\">Jerry L. Prince</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "Self-training based unsupervised domain adaptation (UDA) has shown great\npotential to address the problem of domain shift, when applying a trained deep\nlearning model in a source domain to unlabeled target domains. However, while\nthe self-training UDA has demonstrated its effectiveness on discriminative\ntasks, such as classification and segmentation, via the reliable pseudo-label\nselection based on the softmax discrete histogram, the self-training UDA for\ngenerative tasks, such as image synthesis, is not fully investigated. In this\nwork, we propose a novel generative self-training (GST) UDA framework with\ncontinuous value prediction and regression objective for cross-domain image\nsynthesis. Specifically, we propose to filter the pseudo-label with an\nuncertainty mask, and quantify the predictive confidence of generated images\nwith practical variational Bayes learning. The fast test-time adaptation is\nachieved by a round-based alternative optimization scheme. We validated our\nframework on the tagged-to-cine magnetic resonance imaging (MRI) synthesis\nproblem, where datasets in the source and target domains were acquired from\ndifferent scanners or centers. Extensive validations were carried out to verify\nour framework against popular adversarial training UDA methods. Results show\nthat our GST, with tagged MRI of test subjects in new target domains, improved\nthe synthesis quality by a large margin, compared with the adversarial training\nUDA methods.",
          "link": "http://arxiv.org/abs/2106.12499",
          "publishedOn": "2021-06-24T01:51:43.057Z",
          "wordCount": 671,
          "title": "Generative Self-training for Cross-domain Unsupervised Tagged-to-Cine MRI Synthesis. (arXiv:2106.12499v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Back_J/0/1/0/all/0/1\">Jihye Back</a>",
          "description": "Recent studies have shown remarkable success in the unsupervised image to\nimage (I2I) translation. However, due to the imbalance in the data, learning\njoint distribution for various domains is still very challenging. Although\nexisting models can generate realistic target images, it's difficult to\nmaintain the structure of the source image. In addition, training a generative\nmodel on large data in multiple domains requires a lot of time and computer\nresources. To address these limitations, we propose a novel image-to-image\ntranslation method that generates images of the target domain by finetuning a\nstylegan2 pretrained model. The stylegan2 model is suitable for unsupervised\nI2I translation on unbalanced datasets; it is highly stable, produces realistic\nimages, and even learns properly from limited data when applied with simple\nfine-tuning techniques. Thus, in this paper, we propose new methods to preserve\nthe structure of the source images and generate realistic images in the target\ndomain. The code and results are available at\nhttps://github.com/happy-jihye/Cartoon-StyleGan2",
          "link": "http://arxiv.org/abs/2106.12445",
          "publishedOn": "2021-06-24T01:51:43.043Z",
          "wordCount": 598,
          "title": "Fine-Tuning StyleGAN2 For Cartoon Face Generation. (arXiv:2106.12445v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bezugam_S/0/1/0/all/0/1\">Sai Sukruth Bezugam</a>",
          "description": "The diagnosis of blood-based diseases often involves identifying and\ncharacterizing patient blood samples. Automated methods to detect and classify\nblood cell subtypes have important medical applications. Automated medical\nimage processing and analysis offers a powerful tool for medical diagnosis. In\nthis work we tackle the problem of white blood cell classification based on the\nmorphological characteristics of their outer contour, color. The work we would\nexplore a set of preprocessing and segmentation (Color-based segmentation,\nMorphological processing, contouring) algorithms along with a set of features\nextraction methods (Corner detection algorithms and Histogram of\nGradients(HOG)), dimensionality reduction algorithms (Principal Component\nAnalysis(PCA)) that are able to recognize and classify through various\nUnsupervised(k-nearest neighbors) and Supervised (Support Vector Machine,\nDecision Trees, Linear Discriminant Analysis, Quadratic Discriminant Analysis,\nNaive Bayes) algorithms different categories of white blood cells to\nEosinophil, Lymphocyte, Monocyte, and Neutrophil. We even take a step forwards\nto explore various Deep Convolutional Neural network architecture (Sqeezent,\nMobilenetV1,MobilenetV2, InceptionNet etc.) without preprocessing/segmentation\nand with preprocessing. We would like to explore many algorithms to identify\nthe robust algorithm with least time complexity and low resource requirement.\nThe outcome of this work can be a cue to selection of algorithms as per\nrequirement for automated blood cell classification.",
          "link": "http://arxiv.org/abs/2106.12548",
          "publishedOn": "2021-06-24T01:51:43.037Z",
          "wordCount": 667,
          "title": "Multi-Class Classification of Blood Cells - End to End Computer Vision based diagnosis case study. (arXiv:2106.12548v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12511",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Duffy_G/0/1/0/all/0/1\">Grant Duffy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1\">Paul P Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_N/0/1/0/all/0/1\">Neal Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_B/0/1/0/all/0/1\">Bryan He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kwan_A/0/1/0/all/0/1\">Alan C. Kwan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shun_Shin_M/0/1/0/all/0/1\">Matthew J. Shun-Shin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alexander_K/0/1/0/all/0/1\">Kevin M. Alexander</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebinger_J/0/1/0/all/0/1\">Joseph Ebinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rader_F/0/1/0/all/0/1\">Florian Rader</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">David H. Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schnittger_I/0/1/0/all/0/1\">Ingela Schnittger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ashley_E/0/1/0/all/0/1\">Euan A. Ashley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zou_J/0/1/0/all/0/1\">James Y. Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_J/0/1/0/all/0/1\">Jignesh Patel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Witteles_R/0/1/0/all/0/1\">Ronald Witteles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_S/0/1/0/all/0/1\">Susan Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ouyang_D/0/1/0/all/0/1\">David Ouyang</a>",
          "description": "Left ventricular hypertrophy (LVH) results from chronic remodeling caused by\na broad range of systemic and cardiovascular disease including hypertension,\naortic stenosis, hypertrophic cardiomyopathy, and cardiac amyloidosis. Early\ndetection and characterization of LVH can significantly impact patient care but\nis limited by under-recognition of hypertrophy, measurement error and\nvariability, and difficulty differentiating etiologies of LVH. To overcome this\nchallenge, we present EchoNet-LVH - a deep learning workflow that automatically\nquantifies ventricular hypertrophy with precision equal to human experts and\npredicts etiology of LVH. Trained on 28,201 echocardiogram videos, our model\naccurately measures intraventricular wall thickness (mean absolute error [MAE]\n1.4mm, 95% CI 1.2-1.5mm), left ventricular diameter (MAE 2.4mm, 95% CI\n2.2-2.6mm), and posterior wall thickness (MAE 1.2mm, 95% CI 1.1-1.3mm) and\nclassifies cardiac amyloidosis (area under the curve of 0.83) and hypertrophic\ncardiomyopathy (AUC 0.98) from other etiologies of LVH. In external datasets\nfrom independent domestic and international healthcare systems, EchoNet-LVH\naccurately quantified ventricular parameters (R2 of 0.96 and 0.90 respectively)\nand detected cardiac amyloidosis (AUC 0.79) and hypertrophic cardiomyopathy\n(AUC 0.89) on the domestic external validation site. Leveraging measurements\nacross multiple heart beats, our model can more accurately identify subtle\nchanges in LV geometry and its causal etiologies. Compared to human experts,\nEchoNet-LVH is fully automated, allowing for reproducible, precise\nmeasurements, and lays the foundation for precision diagnosis of cardiac\nhypertrophy. As a resource to promote further innovation, we also make publicly\navailable a large dataset of 23,212 annotated echocardiogram videos.",
          "link": "http://arxiv.org/abs/2106.12511",
          "publishedOn": "2021-06-24T01:51:43.031Z",
          "wordCount": 731,
          "title": "High-Throughput Precision Phenotyping of Left Ventricular Hypertrophy with Cardiovascular Deep Learning. (arXiv:2106.12511v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Libo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenxi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Teng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiaoliang Meng</a>",
          "description": "Semantic segmentation from very fine resolution (VFR) urban scene images\nplays a significant role in several application scenarios including autonomous\ndriving, land cover classification, and urban planning, etc. However, the\ntremendous details contained in the VFR image severely limit the potential of\nthe existing deep learning approaches. More seriously, the considerable\nvariations in scale and appearance of objects further deteriorate the\nrepresentational capacity of those se-mantic segmentation methods, leading to\nthe confusion of adjacent objects. Addressing such is-sues represents a\npromising research field in the remote sensing community, which paves the way\nfor scene-level landscape pattern analysis and decision making. In this\nmanuscript, we pro-pose a bilateral awareness network (BANet) which contains a\ndependency path and a texture path to fully capture the long-range\nrelationships and fine-grained details in VFR images. Specif-ically, the\ndependency path is conducted based on the ResT, a novel Transformer backbone\nwith memory-efficient multi-head self-attention, while the texture path is\nbuilt on the stacked convo-lution operation. Besides, using the linear\nattention mechanism, a feature aggregation module (FAM) is designed to\neffectively fuse the dependency features and texture features. Extensive\nexperiments conducted on the three large-scale urban scene image segmentation\ndatasets, i.e., ISPRS Vaihingen dataset, ISPRS Potsdam dataset, and UAVid\ndataset, demonstrate the effective-ness of our BANet. Specifically, a 64.6%\nmIoU is achieved on the UAVid dataset.",
          "link": "http://arxiv.org/abs/2106.12413",
          "publishedOn": "2021-06-24T01:51:43.026Z",
          "wordCount": 677,
          "title": "Transformer Meets Convolution: A Bilateral Awareness Net-work for Semantic Segmentation of Very Fine Resolution Ur-ban Scene Images. (arXiv:2106.12413v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12489",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_X/0/1/0/all/0/1\">Xiaozhen Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kong_W/0/1/0/all/0/1\">Wenfeng Kong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ning_J/0/1/0/all/0/1\">Jifeng Ning</a>",
          "description": "Low-rankness is important in the hyperspectral image (HSI) denoising tasks.\nThe tensor nuclear norm (TNN), defined based on the tensor singular value\ndecomposition, is a state-of-the-art method to describe the low-rankness of\nHSI. However, TNN ignores some of the physical meanings of HSI in tackling the\ndenoising tasks, leading to suboptimal denoising performance. In this paper, we\npropose the multi-modal and frequency-weighted tensor nuclear norm (MFWTNN) and\nthe non-convex MFWTNN for HSI denoising tasks. Firstly, we investigate the\nphysical meaning of frequency components and reconsider their weights to\nimprove the low-rank representation ability of TNN. Meanwhile, we also consider\nthe correlation among two spatial dimensions and the spectral dimension of HSI\nand combine the above improvements to TNN to propose MFWTNN. Secondly, we use\nnon-convex functions to approximate the rank function of the frequency tensor\nand propose the NonMFWTNN to relax the MFWTNN better. Besides, we adaptively\nchoose bigger weights for slices mainly containing noise information and\nsmaller weights for slices containing profile information. Finally, we develop\nthe efficient alternating direction method of multiplier (ADMM) based algorithm\nto solve the proposed models, and the effectiveness of our models are\nsubstantiated in simulated and real HSI datasets.",
          "link": "http://arxiv.org/abs/2106.12489",
          "publishedOn": "2021-06-24T01:51:43.020Z",
          "wordCount": 644,
          "title": "Multi-modal and frequency-weighted tensor nuclear norm for hyperspectral image denoising. (arXiv:2106.12489v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jialing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kaiqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianhua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Dongyan Guo</a>",
          "description": "The efficiency and accuracy of mapping are crucial in a large scene and\nlong-term AR applications. Multi-agent cooperative SLAM is the precondition of\nmulti-user AR interaction. The cooperation of multiple smart phones has the\npotential to improve efficiency and robustness of task completion and can\ncomplete tasks that a single agent cannot do. However, it depends on robust\ncommunication, efficient location detection, robust mapping, and efficient\ninformation sharing among agents. We propose a multi-intelligence collaborative\nmonocular visual-inertial SLAM deployed on multiple ios mobile devices with a\ncentralized architecture. Each agent can independently explore the environment,\nrun a visual-inertial odometry module online, and then send all the measurement\ninformation to a central server with higher computing resources. The server\nmanages all the information received, detects overlapping areas, merges and\noptimizes the map, and shares information with the agents when needed. We have\nverified the performance of the system in public datasets and real\nenvironments. The accuracy of mapping and fusion of the proposed system is\ncomparable to VINS-Mono which requires higher computing resources.",
          "link": "http://arxiv.org/abs/2106.12186",
          "publishedOn": "2021-06-24T01:51:43.001Z",
          "wordCount": 616,
          "title": "Collaborative Visual Inertial SLAM for Multiple Smart Phones. (arXiv:2106.12186v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from\na labeled source domain to an unlabeled and unseen target domain, which is\nusually trained on data from both domains. Access to the source domain data at\nthe adaptation stage, however, is often limited, due to data storage or privacy\nissues. To alleviate this, in this work, we target source free UDA for\nsegmentation, and propose to adapt an ``off-the-shelf\" segmentation model\npre-trained in the source domain to the target domain, with an adaptive\nbatch-wise normalization statistics adaptation framework. Specifically, the\ndomain-specific low-order batch statistics, i.e., mean and variance, are\ngradually adapted with an exponential momentum decay scheme, while the\nconsistency of domain shareable high-order batch statistics, i.e., scaling and\nshifting parameters, is explicitly enforced by our optimization objective. The\ntransferability of each channel is adaptively measured first from which to\nbalance the contribution of each channel. Moreover, the proposed source free\nUDA framework is orthogonal to unsupervised learning methods, e.g.,\nself-entropy minimization, which can thus be simply added on top of our\nframework. Extensive experiments on the BraTS 2018 database show that our\nsource free UDA framework outperformed existing source-relaxed UDA methods for\nthe cross-subtype UDA segmentation task and yielded comparable results for the\ncross-modality UDA segmentation task, compared with a supervised UDA methods\nwith the source data.",
          "link": "http://arxiv.org/abs/2106.12497",
          "publishedOn": "2021-06-24T01:51:42.987Z",
          "wordCount": 673,
          "title": "Adapting Off-the-Shelf Source Segmenter for Target Medical Image Segmentation. (arXiv:2106.12497v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ploumpis_S/0/1/0/all/0/1\">Stylianos Ploumpis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschoglou_S/0/1/0/all/0/1\">Stylianos Moschoglou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triantafyllou_V/0/1/0/all/0/1\">Vasileios Triantafyllou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1\">Stefanos Zafeiriou</a>",
          "description": "3D face reconstruction from a single image is a task that has garnered\nincreased interest in the Computer Vision community, especially due to its\nbroad use in a number of applications such as realistic 3D avatar creation,\npose invariant face recognition and face hallucination. Since the introduction\nof the 3D Morphable Model in the late 90's, we witnessed an explosion of\nresearch aiming at particularly tackling this task. Nevertheless, despite the\nincreasing level of detail in the 3D face reconstructions from single images\nmainly attributed to deep learning advances, finer and highly deformable\ncomponents of the face such as the tongue are still absent from all 3D face\nmodels in the literature, although being very important for the realness of the\n3D avatar representations. In this work we present the first, to the best of\nour knowledge, end-to-end trainable pipeline that accurately reconstructs the\n3D face together with the tongue. Moreover, we make this pipeline robust in\n\"in-the-wild\" images by introducing a novel GAN method tailored for 3D tongue\nsurface generation. Finally, we make publicly available to the community the\nfirst diverse tongue dataset, consisting of 1,800 raw scans of 700 individuals\nvarying in gender, age, and ethnicity backgrounds. As we demonstrate in an\nextensive series of quantitative as well as qualitative experiments, our model\nproves to be robust and realistically captures the 3D tongue structure, even in\nadverse \"in-the-wild\" conditions.",
          "link": "http://arxiv.org/abs/2106.12302",
          "publishedOn": "2021-06-24T01:51:42.953Z",
          "wordCount": 678,
          "title": "3D human tongue reconstruction from single \"in-the-wild\" images. (arXiv:2106.12302v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lanzini_E/0/1/0/all/0/1\">Edoardo Lanzini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1\">Sara Beery</a>",
          "description": "The natural world is long-tailed: rare classes are observed orders of\nmagnitudes less frequently than common ones, leading to highly-imbalanced data\nwhere rare classes can have only handfuls of examples. Learning from few\nexamples is a known challenge for deep learning based classification\nalgorithms, and is the focus of the field of low-shot learning. One potential\napproach to increase the training data for these rare classes is to augment the\nlimited real data with synthetic samples. This has been shown to help, but the\ndomain shift between real and synthetic hinders the approaches' efficacy when\ntested on real data.\n\nWe explore the use of image-to-image translation methods to close the domain\ngap between synthetic and real imagery for animal species classification in\ndata collected from camera traps: motion-activated static cameras used to\nmonitor wildlife. We use low-level feature alignment between source and target\ndomains to make synthetic data for a rare species generated using a graphics\nengine more \"realistic\". Compared against a system augmented with unaligned\nsynthetic data, our experiments show a considerable decrease in classification\nerror rates on a rare species.",
          "link": "http://arxiv.org/abs/2106.12212",
          "publishedOn": "2021-06-24T01:51:42.937Z",
          "wordCount": 613,
          "title": "Image-to-Image Translation of Synthetic Samples for Rare Classes. (arXiv:2106.12212v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lie_J/0/1/0/all/0/1\">Ji Lie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Leida Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiumei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>",
          "description": "Visual Emotion Analysis (VEA) has attracted increasing attention recently\nwith the prevalence of sharing images on social networks. Since human emotions\nare ambiguous and subjective, it is more reasonable to address VEA in a label\ndistribution learning (LDL) paradigm rather than a single-label classification\ntask. Different from other LDL tasks, there exist intrinsic relationships\nbetween emotions and unique characteristics within them, as demonstrated in\npsychological theories. Inspired by this, we propose a well-grounded\ncircular-structured representation to utilize the prior knowledge for visual\nemotion distribution learning. To be specific, we first construct an Emotion\nCircle to unify any emotional state within it. On the proposed Emotion Circle,\neach emotion distribution is represented with an emotion vector, which is\ndefined with three attributes (i.e., emotion polarity, emotion type, emotion\nintensity) as well as two properties (i.e., similarity, additivity). Besides,\nwe design a novel Progressive Circular (PC) loss to penalize the\ndissimilarities between predicted emotion vector and labeled one in a\ncoarse-to-fine manner, which further boosts the learning process in an\nemotion-specific way. Extensive experiments and comparisons are conducted on\npublic visual emotion distribution datasets, and the results demonstrate that\nthe proposed method outperforms the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.12450",
          "publishedOn": "2021-06-24T01:51:42.925Z",
          "wordCount": 636,
          "title": "A Circular-Structured Representation for Visual Emotion Distribution Learning. (arXiv:2106.12450v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12407",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Junshen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turk_E/0/1/0/all/0/1\">Esra Abaci Turk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grant_P/0/1/0/all/0/1\">P. Ellen Grant</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1\">Polina Golland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1\">Elfar Adalsteinsson</a>",
          "description": "Fetal motion is unpredictable and rapid on the scale of conventional MR scan\ntimes. Therefore, dynamic fetal MRI, which aims at capturing fetal motion and\ndynamics of fetal function, is limited to fast imaging techniques with\ncompromises in image quality and resolution. Super-resolution for dynamic fetal\nMRI is still a challenge, especially when multi-oriented stacks of image slices\nfor oversampling are not available and high temporal resolution for recording\nthe dynamics of the fetus or placenta is desired. Further, fetal motion makes\nit difficult to acquire high-resolution images for supervised learning methods.\nTo address this problem, in this work, we propose STRESS (Spatio-Temporal\nResolution Enhancement with Simulated Scans), a self-supervised\nsuper-resolution framework for dynamic fetal MRI with interleaved slice\nacquisitions. Our proposed method simulates an interleaved slice acquisition\nalong the high-resolution axis on the originally acquired data to generate\npairs of low- and high-resolution images. Then, it trains a super-resolution\nnetwork by exploiting both spatial and temporal correlations in the MR time\nseries, which is used to enhance the resolution of the original data.\nEvaluations on both simulated and in utero data show that our proposed method\noutperforms other self-supervised super-resolution methods and improves image\nquality, which is beneficial to other downstream tasks and evaluations.",
          "link": "http://arxiv.org/abs/2106.12407",
          "publishedOn": "2021-06-24T01:51:42.915Z",
          "wordCount": 656,
          "title": "STRESS: Super-Resolution for Dynamic Fetal MRI using Self-Supervised Learning. (arXiv:2106.12407v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wentao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Zhiyu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_C/0/1/0/all/0/1\">Chengyu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiman Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Tingming Bai</a>",
          "description": "Although instance segmentation has made considerable advancement over recent\nyears, it's still a challenge to design high accuracy algorithms with real-time\nperformance. In this paper, we propose a real-time instance segmentation\nframework termed OrienMask. Upon the one-stage object detector YOLOv3, a mask\nhead is added to predict some discriminative orientation maps, which are\nexplicitly defined as spatial offset vectors for both foreground and background\npixels. Thanks to the discrimination ability of orientation maps, masks can be\nrecovered without the need for extra foreground segmentation. All instances\nthat match with the same anchor size share a common orientation map. This\nspecial sharing strategy reduces the amortized memory utilization for mask\npredictions but without loss of mask granularity. Given the surviving box\npredictions after NMS, instance masks can be concurrently constructed from the\ncorresponding orientation maps with low complexity. Owing to the concise design\nfor mask representation and its effective integration with the anchor-based\nobject detector, our method is qualified under real-time conditions while\nmaintaining competitive accuracy. Experiments on COCO benchmark show that\nOrienMask achieves 34.8 mask AP at the speed of 42.7 fps evaluated with a\nsingle RTX 2080 Ti. The code is available at https://github.com/duwt/OrienMask.",
          "link": "http://arxiv.org/abs/2106.12204",
          "publishedOn": "2021-06-24T01:51:42.879Z",
          "wordCount": 635,
          "title": "Real-time Instance Segmentation with Discriminative Orientation Maps. (arXiv:2106.12204v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">Fanhua Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongying Liu</a>",
          "description": "Federated Learning (FL) has become an active and promising distributed\nmachine learning paradigm. As a result of statistical heterogeneity, recent\nstudies clearly show that the performance of popular FL methods (e.g., FedAvg)\ndeteriorates dramatically due to the client drift caused by local updates. This\npaper proposes a novel Federated Learning algorithm (called IGFL), which\nleverages both Individual and Group behaviors to mimic distribution, thereby\nimproving the ability to deal with heterogeneity. Unlike existing FL methods,\nour IGFL can be applied to both client and server optimization. As a\nby-product, we propose a new attention-based federated learning in the server\noptimization of IGFL. To the best of our knowledge, this is the first time to\nincorporate attention mechanisms into federated optimization. We conduct\nextensive experiments and show that IGFL can significantly improve the\nperformance of existing federated learning methods. Especially when the\ndistributions of data among individuals are diverse, IGFL can improve the\nclassification accuracy by about 13% compared with prior baselines.",
          "link": "http://arxiv.org/abs/2106.12300",
          "publishedOn": "2021-06-24T01:51:42.818Z",
          "wordCount": 632,
          "title": "Behavior Mimics Distribution: Combining Individual and Group Behaviors for Federated Learning. (arXiv:2106.12300v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zeyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_B/0/1/0/all/0/1\">Bangyang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xianli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chang Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jialun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunbao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>",
          "description": "Histological subtype of papillary (p) renal cell carcinoma (RCC), type 1 vs.\ntype 2, is an essential prognostic factor. The two subtypes of pRCC have a\nsimilar pattern, i.e., the papillary architecture, yet some subtle differences,\nincluding cellular and cell-layer level patterns. However, the cellular and\ncell-layer level patterns almost cannot be captured by existing CNN-based\nmodels in large-size histopathological images, which brings obstacles to\ndirectly applying these models to such a fine-grained classification task. This\npaper proposes a novel instance-based Vision Transformer (i-ViT) to learn\nrobust representations of histopathological images for the pRCC subtyping task\nby extracting finer features from instance patches (by cropping around\nsegmented nuclei and assigning predicted grades). The proposed i-ViT takes\ntop-K instances as input and aggregates them for capturing both the cellular\nand cell-layer level patterns by a position-embedding layer, a grade-embedding\nlayer, and a multi-head multi-layer self-attention module. To evaluate the\nperformance of the proposed framework, experienced pathologists are invited to\nselected 1162 regions of interest from 171 whole slide images of type 1 and\ntype 2 pRCC. Experimental results show that the proposed method achieves better\nperformance than existing CNN-based models with a significant margin.",
          "link": "http://arxiv.org/abs/2106.12265",
          "publishedOn": "2021-06-24T01:51:42.810Z",
          "wordCount": 653,
          "title": "Instance-based Vision Transformer for Subtyping of Papillary Renal Cell Carcinoma in Histopathological Image. (arXiv:2106.12265v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1\">Malik Boudiaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masud_Z/0/1/0/all/0/1\">Ziko Imtiaz Masud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rony_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Rony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>",
          "description": "We introduce Transductive Infomation Maximization (TIM) for few-shot\nlearning. Our method maximizes the mutual information between the query\nfeatures and their label predictions for a given few-shot task, in conjunction\nwith a supervision loss based on the support set. We motivate our transductive\nloss by deriving a formal relation between the classification accuracy and\nmutual-information maximization. Furthermore, we propose a new\nalternating-direction solver, which substantially speeds up transductive\ninference over gradient-based optimization, while yielding competitive\naccuracy. We also provide a convergence analysis of our solver based on\nZangwill's theory and bound-optimization arguments. TIM inference is modular:\nit can be used on top of any base-training feature extractor. Following\nstandard transductive few-shot settings, our comprehensive experiments\ndemonstrate that TIM outperforms state-of-the-art methods significantly across\nvarious datasets and networks, while used on top of a fixed feature extractor\ntrained with simple cross-entropy on the base classes, without resorting to\ncomplex meta-learning schemes. It consistently brings between 2 % and 5 %\nimprovement in accuracy over the best performing method, not only on all the\nwell-established few-shot benchmarks but also on more challenging scenarios,\nwith random tasks, domain shift and larger numbers of classes, as in the\nrecently introduced META-DATASET. Our code is publicly available at\nhttps://github.com/mboudiaf/TIM. We also publicly release a standalone PyTorch\nimplementation of META-DATASET, along with additional benchmarking results, at\nhttps://github.com/mboudiaf/pytorch-meta-dataset.",
          "link": "http://arxiv.org/abs/2106.12252",
          "publishedOn": "2021-06-24T01:51:42.804Z",
          "wordCount": 673,
          "title": "Mutual-Information Based Few-Shot Classification. (arXiv:2106.12252v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12175",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Junshen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1\">Elfar Adalsteinsson</a>",
          "description": "Image denoising is of great importance for medical imaging system, since it\ncan improve image quality for disease diagnosis and downstream image analyses.\nIn a variety of applications, dynamic imaging techniques are utilized to\ncapture the time-varying features of the subject, where multiple images are\nacquired for the same subject at different time points. Although\nsignal-to-noise ratio of each time frame is usually limited by the short\nacquisition time, the correlation among different time frames can be exploited\nto improve denoising results with shared information across time frames. With\nthe success of neural networks in computer vision, supervised deep learning\nmethods show prominent performance in single-image denoising, which rely on\nlarge datasets with clean-vs-noisy image pairs. Recently, several\nself-supervised deep denoising models have been proposed, achieving promising\nresults without needing the pairwise ground truth of clean images. In the field\nof multi-image denoising, however, very few works have been done on extracting\ncorrelated information from multiple slices for denoising using self-supervised\ndeep learning methods. In this work, we propose Deformed2Self, an end-to-end\nself-supervised deep learning framework for dynamic imaging denoising. It\ncombines single-image and multi-image denoising to improve image quality and\nuse a spatial transformer network to model motion between different slices.\nFurther, it only requires a single noisy image with a few auxiliary\nobservations at different time frames for training and inference. Evaluations\non phantom and in vivo data with different noise statistics show that our\nmethod has comparable performance to other state-of-the-art unsupervised or\nself-supervised denoising methods and outperforms under high noise levels.",
          "link": "http://arxiv.org/abs/2106.12175",
          "publishedOn": "2021-06-24T01:51:42.786Z",
          "wordCount": 694,
          "title": "Deformed2Self: Self-Supervised Denoising for Dynamic Medical Imaging. (arXiv:2106.12175v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12169",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Boyuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_T/0/1/0/all/0/1\">Tong Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yufei Ding</a>",
          "description": "Over the years, accelerating neural networks with quantization has been\nwidely studied. Unfortunately, prior efforts with diverse precisions (e.g.,\n1-bit weights and 2-bit activations) are usually restricted by limited\nprecision support on GPUs (e.g., int1 and int4). To break such restrictions, we\nintroduce the first Arbitrary Precision Neural Network framework (APNN-TC) to\nfully exploit quantization benefits on Ampere GPU Tensor Cores. Specifically,\nAPNN-TC first incorporates a novel emulation algorithm to support arbitrary\nshort bit-width computation with int1 compute primitives and XOR/AND Boolean\noperations. Second, APNN-TC integrates arbitrary precision layer designs to\nefficiently map our emulation algorithm to Tensor Cores with novel batching\nstrategies and specialized memory organization. Third, APNN-TC embodies a novel\narbitrary precision NN design to minimize memory access across layers and\nfurther improve performance. Extensive evaluations show that APNN-TC can\nachieve significant speedup over CUTLASS kernels and various NN models, such as\nResNet and VGG.",
          "link": "http://arxiv.org/abs/2106.12169",
          "publishedOn": "2021-06-24T01:51:42.780Z",
          "wordCount": 610,
          "title": "APNN-TC: Accelerating Arbitrary Precision Neural Networks on Ampere GPU Tensor Cores. (arXiv:2106.12169v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "One of the distinct features of this century has been the population of older\nadults which has been on a constant rise. Elderly people have several needs and\nrequirements due to physical disabilities, cognitive issues, weakened memory\nand disorganized behavior, that they face with increasing age. The extent of\nthese limitations also differs according to the varying diversities in elderly,\nwhich include age, gender, background, experience, skills, knowledge and so on.\nThese varying needs and challenges with increasing age, limits abilities of\nolder adults to perform Activities of Daily Living (ADLs) in an independent\nmanner. To add to it, the shortage of caregivers creates a looming need for\ntechnology-based services for elderly people, to assist them in performing\ntheir daily routine tasks to sustain their independent living and active aging.\nTo address these needs, this work consists of making three major contributions\nin this field. First, it provides a rather comprehensive review of assisted\nliving technologies aimed at helping elderly people to perform ADLs. Second,\nthe work discusses the challenges identified through this review, that\ncurrently exist in the context of implementation of assisted living services\nfor elderly care in Smart Homes and Smart Cities. Finally, the work also\noutlines an approach for implementation, extension and integration of the\nexisting works in this field for development of a much-needed framework that\ncan provide personalized assistance and user-centered behavior interventions to\nelderly as per their varying and ever-changing needs.",
          "link": "http://arxiv.org/abs/2106.12183",
          "publishedOn": "2021-06-24T01:51:42.774Z",
          "wordCount": 707,
          "title": "A Review of Assistive Technologies for Activities of Daily Living of Elderly. (arXiv:2106.12183v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuehai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Badong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Shaoyi Du</a>",
          "description": "Background noise and scale variation are common problems that have been long\nrecognized in crowd counting. Humans glance at a crowd image and instantly know\nthe approximate number of human and where they are through attention the crowd\nregions and the congestion degree of crowd regions with a global receptive\nfiled. Hence, in this paper, we propose a novel feedback network with\nRegion-Aware block called RANet by modeling human's Top-Down visual perception\nmechanism. Firstly, we introduce a feedback architecture to generate priority\nmaps that provide prior about candidate crowd regions in input images. The\nprior enables the RANet pay more attention to crowd regions. Then we design\nRegion-Aware block that could adaptively encode the contextual information into\ninput images through global receptive field. More specifically, we scan the\nwhole input images and its priority maps in the form of column vector to obtain\na relevance matrix estimating their similarity. The relevance matrix obtained\nwould be utilized to build global relationships between pixels. Our method\noutperforms state-of-the-art crowd counting methods on several public datasets.",
          "link": "http://arxiv.org/abs/2106.12163",
          "publishedOn": "2021-06-24T01:51:42.768Z",
          "wordCount": 618,
          "title": "Region-Aware Network: Model Human's Top-Down Visual Perception Mechanism for Crowd Counting. (arXiv:2106.12163v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_F/0/1/0/all/0/1\">Fangyuan Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Da Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jianjian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruijun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Senhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiangzhong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yusen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qingyun Dai</a>",
          "description": "In deep learning area, large-scale image datasets bring a breakthrough in the\nsuccess of object recognition and retrieval. Nowadays, as the embodiment of\ninnovation, the diversity of the industrial goods is significantly larger, in\nwhich the incomplete multiview, multimodal and multilabel are different from\nthe traditional dataset. In this paper, we introduce an industrial goods\ndataset, namely PatentNet, with numerous highly diverse, accurate and detailed\nannotations of industrial goods images, and corresponding texts. In PatentNet,\nthe images and texts are sourced from design patent. Within over 6M images and\ncorresponding texts of industrial goods labeled manually checked by\nprofessionals, PatentNet is the first ongoing industrial goods image database\nwhose varieties are wider than industrial goods datasets used previously for\nbenchmarking. PatentNet organizes millions of images into 32 classes and 219\nsubclasses based on the Locarno Classification Agreement. Through extensive\nexperiments on image classification, image retrieval and incomplete multiview\nclustering, we demonstrate that our PatentNet is much more diverse, complex,\nand challenging, enjoying higher potentials than existing industrial image\ndatasets. Furthermore, the characteristics of incomplete multiview, multimodal\nand multilabel in PatentNet are able to offer unparalleled opportunities in the\nartificial intelligence community and beyond.",
          "link": "http://arxiv.org/abs/2106.12139",
          "publishedOn": "2021-06-24T01:51:42.762Z",
          "wordCount": 649,
          "title": "PatentNet: A Large-Scale Incomplete Multiview, Multimodal, Multilabel Industrial Goods Image Database. (arXiv:2106.12139v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shirke_A/0/1/0/all/0/1\">Aniket Shirke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golden_R/0/1/0/all/0/1\">Rebecca Golden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautam_M/0/1/0/all/0/1\">Mrinal Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_Miller_A/0/1/0/all/0/1\">Angela Green-Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caesar_M/0/1/0/all/0/1\">Matthew Caesar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dilger_R/0/1/0/all/0/1\">Ryan N. Dilger</a>",
          "description": "Behavioral scoring of research data is crucial for extracting domain-specific\nmetrics but is bottlenecked on the ability to analyze enormous volumes of\ninformation using human labor. Deep learning is widely viewed as a key\nadvancement to relieve this bottleneck. We identify one such domain, where deep\nlearning can be leveraged to alleviate the process of manual scoring. Novelty\npreference paradigms have been widely used to study recognition memory in pigs,\nbut analysis of these videos requires human intervention. We introduce a subset\nof such videos in the form of the 'Pig Novelty Preference Behavior' (PNPB)\ndataset that is fully annotated with pig actions and keypoints. In order to\ndemonstrate the application of state-of-the-art action recognition models on\nthis dataset, we compare LRCN, C3D, and TSM on the basis of various analytical\nmetrics and discuss common pitfalls of the models. Our methods achieve an\naccuracy of 93% and a mean Average Precision of 96% in estimating piglet\nbehavior.\n\nWe open-source our code and annotated dataset at\nhttps://github.com/AIFARMS/NOR-behavior-recognition",
          "link": "http://arxiv.org/abs/2106.12181",
          "publishedOn": "2021-06-24T01:51:42.746Z",
          "wordCount": 621,
          "title": "Vision-based Behavioral Recognition of Novelty Preference in Pigs. (arXiv:2106.12181v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12157",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Akilan_T/0/1/0/all/0/1\">Thangarajah Akilan</a>",
          "description": "The coronavirus continues to disrupt our everyday lives as it spreads at an\nexponential rate. It needs to be detected quickly in order to quarantine\npositive patients so as to avoid further spread. This work proposes a new\nconvolutional neural network (CNN) architecture called 'slow Encoding CNN. The\nproposed model's best performance wrt Sensitivity, Positive Predictive Value\n(PPV) found to be SP=0.67, PP=0.98, SN=0.96, and PN=0.52 on AI AGAINST COVID19\n- Screening X-ray images for COVID-19 Infections competition's test data\nsamples. SP and PP stand for the Sensitivity and PPV of the COVID-19 positive\nclass, while PN and SN stand for the Sensitivity and PPV of the COVID-19\nnegative class.",
          "link": "http://arxiv.org/abs/2106.12157",
          "publishedOn": "2021-06-24T01:51:42.741Z",
          "wordCount": 594,
          "title": "CxSE: Chest X-ray Slow Encoding CNN forCOVID-19 Diagnosis. (arXiv:2106.12157v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yusong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaogang Jia</a>",
          "description": "It is desirable to transfer the knowledge stored in a well-trained source\nmodel onto non-annotated target domain in the absence of source data. However,\nstate-of-the-art methods for source free domain adaptation (SFDA) are subject\nto strict limits: 1) access to internal specifications of source models is a\nmust; and 2) pseudo labels should be clean during self-training, making\ncritical tasks relying on semantic segmentation unreliable. Aiming at these\npitfalls, this study develops a domain adaptive solution to semantic\nsegmentation with pseudo label rectification (namely \\textit{PR-SFDA}), which\noperates in two phases: 1) \\textit{Confidence-regularized unsupervised\nlearning}: Maximum squares loss applies to regularize the target model to\nensure the confidence in prediction; and 2) \\textit{Noise-aware pseudo label\nlearning}: Negative learning enables tolerance to noisy pseudo labels in\ntraining, meanwhile positive learning achieves fast convergence. Extensive\nexperiments have been performed on domain adaptive semantic segmentation\nbenchmark, \\textit{GTA5 $\\to$ Cityscapes}. Overall, \\textit{PR-SFDA} achieves a\nperformance of 49.0 mIoU, which is very close to that of the state-of-the-art\ncounterparts. Note that the latter demand accesses to the source model's\ninternal specifications, whereas the \\textit{PR-SFDA} solution needs none as a\nsharp contrast.",
          "link": "http://arxiv.org/abs/2106.12123",
          "publishedOn": "2021-06-24T01:51:42.728Z",
          "wordCount": 645,
          "title": "Exploiting Negative Learning for Implicit Pseudo Label Rectification in Source-Free Domain Adaptive Semantic Segmentation. (arXiv:2106.12123v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaodong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_T/0/1/0/all/0/1\">Tomoya Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hoang-Dung Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoxha_B/0/1/0/all/0/1\">Bardh Hoxha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_T/0/1/0/all/0/1\">Taylor T Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prokhorov_D/0/1/0/all/0/1\">Danil Prokhorov</a>",
          "description": "Deep convolutional neural networks have been widely employed as an effective\ntechnique to handle complex and practical problems. However, one of the\nfundamental problems is the lack of formal methods to analyze their behavior.\nTo address this challenge, we propose an approach to compute the exact\nreachable sets of a network given an input domain, where the reachable set is\nrepresented by the face lattice structure. Besides the computation of reachable\nsets, our approach is also capable of backtracking to the input domain given an\noutput reachable set. Therefore, a full analysis of a network's behavior can be\nrealized. In addition, an approach for fast analysis is also introduced, which\nconducts fast computation of reachable sets by considering selected sensitive\nneurons in each layer. The exact pixel-level reachability analysis method is\nevaluated on a CNN for the CIFAR10 dataset and compared to related works. The\nfast analysis method is evaluated over a CNN CIFAR10 dataset and VGG16\narchitecture for the ImageNet dataset.",
          "link": "http://arxiv.org/abs/2106.12074",
          "publishedOn": "2021-06-24T01:51:42.722Z",
          "wordCount": 599,
          "title": "Reachability Analysis of Convolutional Neural Networks. (arXiv:2106.12074v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yariv_L/0/1/0/all/0/1\">Lior Yariv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiatao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasten_Y/0/1/0/all/0/1\">Yoni Kasten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1\">Yaron Lipman</a>",
          "description": "Neural volume rendering became increasingly popular recently due to its\nsuccess in synthesizing novel views of a scene from a sparse set of input\nimages. So far, the geometry learned by neural volume rendering techniques was\nmodeled using a generic density function. Furthermore, the geometry itself was\nextracted using an arbitrary level set of the density function leading to a\nnoisy, often low fidelity reconstruction. The goal of this paper is to improve\ngeometry representation and reconstruction in neural volume rendering. We\nachieve that by modeling the volume density as a function of the geometry. This\nis in contrast to previous work modeling the geometry as a function of the\nvolume density. In more detail, we define the volume density function as\nLaplace's cumulative distribution function (CDF) applied to a signed distance\nfunction (SDF) representation. This simple density representation has three\nbenefits: (i) it provides a useful inductive bias to the geometry learned in\nthe neural volume rendering process; (ii) it facilitates a bound on the opacity\napproximation error, leading to an accurate sampling of the viewing ray.\nAccurate sampling is important to provide a precise coupling of geometry and\nradiance; and (iii) it allows efficient unsupervised disentanglement of shape\nand appearance in volume rendering. Applying this new density representation to\nchallenging scene multiview datasets produced high quality geometry\nreconstructions, outperforming relevant baselines. Furthermore, switching shape\nand appearance between scenes is possible due to the disentanglement of the\ntwo.",
          "link": "http://arxiv.org/abs/2106.12052",
          "publishedOn": "2021-06-24T01:51:42.700Z",
          "wordCount": 671,
          "title": "Volume Rendering of Neural Implicit Surfaces. (arXiv:2106.12052v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hacheme_G/0/1/0/all/0/1\">Gilles Hacheme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayouti_N/0/1/0/all/0/1\">Noureini Sayouti</a>",
          "description": "Image captioning has increasingly large domains of application, and fashion\nis not an exception. Having automatic item descriptions is of great interest\nfor fashion web platforms hosting sometimes hundreds of thousands of images.\nThis paper is one of the first tackling image captioning for fashion images. To\ncontribute addressing dataset diversity issues, we introduced the InFashAIv1\ndataset containing almost 16.000 African fashion item images with their titles,\nprices and general descriptions. We also used the well known DeepFashion\ndataset in addition to InFashAIv1. Captions are generated using the\n\\textit{Show and Tell} model made of CNN encoder and RNN Decoder. We showed\nthat jointly training the model on both datasets improves captions quality for\nAfrican style fashion images, suggesting a transfer learning from Western style\ndata. The InFashAIv1 dataset is released on\n\\href{https://github.com/hgilles06/infashai}{Github} to encourage works with\nmore diversity inclusion.",
          "link": "http://arxiv.org/abs/2106.12154",
          "publishedOn": "2021-06-24T01:51:42.695Z",
          "wordCount": 583,
          "title": "Neural Fashion Image Captioning : Accounting for Data Diversity. (arXiv:2106.12154v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youshan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_B/0/1/0/all/0/1\">Brian D. Davison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talghader_V/0/1/0/all/0/1\">Vivien W. Talghader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhiyong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunkel_G/0/1/0/all/0/1\">Gary J. Kunkel</a>",
          "description": "Transmission electron microscopy (TEM) is one of the primary tools to show\nmicrostructural characterization of materials as well as film thickness.\nHowever, manual determination of film thickness from TEM images is\ntime-consuming as well as subjective, especially when the films in question are\nvery thin and the need for measurement precision is very high. Such is the case\nfor head overcoat (HOC) thickness measurements in the magnetic hard disk drive\nindustry. It is therefore necessary to develop software to automatically\nmeasure HOC thickness. In this paper, for the first time, we propose a HOC\nlayer segmentation method using NASNet-Large as an encoder and then followed by\na decoder architecture, which is one of the most commonly used architectures in\ndeep learning for image segmentation. To further improve segmentation results,\nwe are the first to propose a post-processing layer to remove irrelevant\nportions in the segmentation result. To measure the thickness of the segmented\nHOC layer, we propose a regressive convolutional neural network (RCNN) model as\nwell as orthogonal thickness calculation methods. Experimental results\ndemonstrate a higher dice score for our model which has lower mean squared\nerror and outperforms current state-of-the-art manual measurement.",
          "link": "http://arxiv.org/abs/2106.12054",
          "publishedOn": "2021-06-24T01:51:42.690Z",
          "wordCount": 635,
          "title": "Automatic Head Overcoat Thickness Measure with NASNet-Large-Decoder Net. (arXiv:2106.12054v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12016",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arian_R/0/1/0/all/0/1\">Reeshad Arian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamm_K/0/1/0/all/0/1\">Keaton Hamm</a>",
          "description": "This article explores subspace clustering algorithms using CUR\ndecompositions, and examines the effect of various hyperparameters in these\nalgorithms on clustering performance on two real-world benchmark datasets, the\nHopkins155 motion segmentation dataset and the Yale face dataset. Extensive\nexperiments are done for a variety of sampling methods and oversampling\nparameters for these datasets, and some guidelines for parameter choices are\ngiven for practical applications.",
          "link": "http://arxiv.org/abs/2106.12016",
          "publishedOn": "2021-06-24T01:51:42.684Z",
          "wordCount": 505,
          "title": "On Matrix Factorizations in Subspace Clustering. (arXiv:2106.12016v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1\">Navid Kardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Ankit Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1\">Kenneth O. Stanley</a>",
          "description": "Deep neural networks are behind many of the recent successes in machine\nlearning applications. However, these models can produce overconfident\ndecisions while encountering out-of-distribution (OOD) examples or making a\nwrong prediction. This inconsistent predictive confidence limits the\nintegration of independently-trained learning models into a larger system. This\npaper introduces separable concept learning framework to realistically measure\nthe performance of classifiers in presence of OOD examples. In this setup,\nseveral instances of a classifier are trained on different parts of a partition\nof the set of classes. Later, the performance of the combination of these\nmodels is evaluated on a separate test set. Unlike current OOD detection\ntechniques, this framework does not require auxiliary OOD datasets and does not\nseparate classification from detection performance. Furthermore, we present a\nnew strong baseline for more consistent predictive confidence in deep models,\ncalled fitted ensembles, where overconfident predictions are rectified by\ntransformed versions of the original classification task. Fitted ensembles can\nnaturally detect OOD examples without requiring auxiliary data by observing\ncontradicting predictions among its components. Experiments on MNIST, SVHN,\nCIFAR-10/100, and ImageNet show fitted ensemble significantly outperform\nconventional ensembles on OOD examples and are possible to scale.",
          "link": "http://arxiv.org/abs/2106.12070",
          "publishedOn": "2021-06-24T01:51:42.668Z",
          "wordCount": 635,
          "title": "Towards Consistent Predictive Confidence through Fitted Ensembles. (arXiv:2106.12070v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">R. Kenny Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanocka_R/0/1/0/all/0/1\">Rana Hanocka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>",
          "description": "Many learning-based 3D shape semantic segmentation methods assign labels to\nshape atoms (e.g. points in a point cloud or faces in a mesh) with a\nsingle-pass approach trained in an end-to-end fashion. Such methods achieve\nimpressive performance but require large amounts of labeled training data. This\nparadigm entangles two separable subproblems: (1) decomposing a shape into\nregions and (2) assigning semantic labels to these regions. We claim that\ndisentangling these subproblems reduces the labeled data burden: (1) region\ndecomposition requires no semantic labels and could be performed in an\nunsupervised fashion, and (2) labeling shape regions instead of atoms results\nin a smaller search space and should be learnable with less labeled training\ndata. In this paper, we investigate this second claim by presenting the\nNeurally-Guided Shape Parser (NGSP), a method that learns how to assign\nsemantic labels to regions of an over-segmented 3D shape. We solve this problem\nvia MAP inference, modeling the posterior probability of a labeling assignment\nconditioned on an input shape. We employ a Monte Carlo importance sampling\napproach guided by a neural proposal network, a search-based approach made\nfeasible by assuming the input shape is decomposed into discrete regions. We\nevaluate NGSP on the task of hierarchical semantic segmentation on manufactured\n3D shapes from PartNet. We find that NGSP delivers significant performance\nimprovements over baselines that learn to label shape atoms and then aggregate\npredictions for each shape region, especially in low-data regimes. Finally, we\ndemonstrate that NGSP is robust to region granularity, as it maintains strong\nsegmentation performance even as the regions undergo significant corruption.",
          "link": "http://arxiv.org/abs/2106.12026",
          "publishedOn": "2021-06-24T01:51:42.663Z",
          "wordCount": 719,
          "title": "The Neurally-Guided Shape Parser: A Monte Carlo Method for Hierarchical Labeling of Over-segmented 3D Shapes. (arXiv:2106.12026v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yagubbayli_F/0/1/0/all/0/1\">Farid Yagubbayli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonioni_A/0/1/0/all/0/1\">Alessio Tonioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "Most modern deep learning-based multi-view 3D reconstruction techniques use\nRNNs or fusion modules to combine information from multiple images after\nencoding them. These two separate steps have loose connections and do not\nconsider all available information while encoding each view. We propose\nLegoFormer, a transformer-based model that unifies object reconstruction under\na single framework and parametrizes the reconstructed occupancy grid by its\ndecomposition factors. This reformulation allows the prediction of an object as\na set of independent structures then aggregated to obtain the final\nreconstruction. Experiments conducted on ShapeNet display the competitive\nperformance of our network with respect to the state-of-the-art methods. We\nalso demonstrate how the use of self-attention leads to increased\ninterpretability of the model output.",
          "link": "http://arxiv.org/abs/2106.12102",
          "publishedOn": "2021-06-24T01:51:42.657Z",
          "wordCount": 550,
          "title": "LegoFormer: Transformers for Block-by-Block Multi-view 3D Reconstruction. (arXiv:2106.12102v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zejian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1\">Wei Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianfu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wufeng Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>",
          "description": "In this work, we propose a novel straightforward method for medical volume\nand sequence segmentation with limited annotations. To avert laborious\nannotating, the recent success of self-supervised learning(SSL) motivates the\npre-training on unlabeled data. Despite its success, it is still challenging to\nadapt typical SSL methods to volume/sequence segmentation, due to their lack of\nmining on local semantic discrimination and rare exploitation on volume and\nsequence structures. Based on the continuity between slices/frames and the\ncommon spatial layout of organs across volumes/sequences, we introduced a novel\nbootstrap self-supervised representation learning method by leveraging the\npredictable possibility of neighboring slices. At the core of our method is a\nsimple and straightforward dense self-supervision on the predictions of local\nrepresentations and a strategy of predicting locals based on global context,\nwhich enables stable and reliable supervision for both global and local\nrepresentation mining among volumes. Specifically, we first proposed an\nasymmetric network with an attention-guided predictor to enforce\ndistance-specific prediction and supervision on slices within and across\nvolumes/sequences. Secondly, we introduced a novel prototype-based\nforeground-background calibration module to enhance representation consistency.\nThe two parts are trained jointly on labeled and unlabeled data. When evaluated\non three benchmark datasets of medical volumes and sequences, our model\noutperforms existing methods with a large margin of 4.5\\% DSC on ACDC, 1.7\\% on\nProstate, and 2.3\\% on CAMUS. Intensive evaluations reveals the effectiveness\nand superiority of our method.",
          "link": "http://arxiv.org/abs/2106.12153",
          "publishedOn": "2021-06-24T01:51:42.649Z",
          "wordCount": 676,
          "title": "Bootstrap Representation Learning for Segmentation on Medical Volumes and Sequences. (arXiv:2106.12153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Garcia_F/0/1/0/all/0/1\">Fernando P&#xe9;rez-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scott_C/0/1/0/all/0/1\">Catherine Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sparks_R/0/1/0/all/0/1\">Rachel Sparks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diehl_B/0/1/0/all/0/1\">Beate Diehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1\">S&#xe9;bastien Ourselin</a>",
          "description": "Detailed analysis of seizure semiology, the symptoms and signs which occur\nduring a seizure, is critical for management of epilepsy patients. Inter-rater\nreliability using qualitative visual analysis is often poor for semiological\nfeatures. Therefore, automatic and quantitative analysis of video-recorded\nseizures is needed for objective assessment.\n\nWe present GESTURES, a novel architecture combining convolutional neural\nnetworks (CNNs) and recurrent neural networks (RNNs) to learn deep\nrepresentations of arbitrarily long videos of epileptic seizures.\n\nWe use a spatiotemporal CNN (STCNN) pre-trained on large human action\nrecognition (HAR) datasets to extract features from short snippets (approx. 0.5\ns) sampled from seizure videos. We then train an RNN to learn seizure-level\nrepresentations from the sequence of features.\n\nWe curated a dataset of seizure videos from 68 patients and evaluated\nGESTURES on its ability to classify seizures into focal onset seizures (FOSs)\n(N = 106) vs. focal to bilateral tonic-clonic seizures (TCSs) (N = 77),\nobtaining an accuracy of 98.9% using bidirectional long short-term memory\n(BLSTM) units.\n\nWe demonstrate that an STCNN trained on a HAR dataset can be used in\ncombination with an RNN to accurately represent arbitrarily long videos of\nseizures. GESTURES can provide accurate seizure classification by modeling\nsequences of semiologies.",
          "link": "http://arxiv.org/abs/2106.12014",
          "publishedOn": "2021-06-24T01:51:42.611Z",
          "wordCount": 664,
          "title": "Transfer Learning of Deep Spatiotemporal Networks to Model Arbitrarily Long Videos of Seizures. (arXiv:2106.12014v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shishido_T/0/1/0/all/0/1\">Tomoyuki Shishido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fati_F/0/1/0/all/0/1\">Fehmiju Fati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tokushige_D/0/1/0/all/0/1\">Daisuke Tokushige</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ono_Y/0/1/0/all/0/1\">Yasuhiro Ono</a>",
          "description": "Deep learning has recently been applied to optical music recognition (OMR).\nHowever, currently OMR processing from various sheet music images still lacks\nprecision to be widely applicable. Here, we present an MMdA (Measure-based\nMultimodal deep learning (DL)-driven Assembly) method allowing for end-to-end\nOMR processing from various images including inclined photo images. Using this\nmethod, measures are extracted by a deep learning model, aligned, and resized\nto be used for inference of given musical symbol components by using multiple\ndeep learning models in sequence or in parallel. Use of each standardized\nmeasure enables efficient training of the models and accurate adjustment of\nfive staff lines in each measure. Multiple musical symbol component category\nmodels with a small number of feature types can represent a diverse set of\nnotes and other musical symbols including chords. This MMdA method provides a\nsolution to end-to-end OMR processing with precision.",
          "link": "http://arxiv.org/abs/2106.12037",
          "publishedOn": "2021-06-24T01:51:42.604Z",
          "wordCount": 606,
          "title": "Listen to Your Favorite Melodies with img2Mxml, Producing MusicXML from Sheet Music Image by Measure-based Multimodal Deep Learning-driven Assembly. (arXiv:2106.12037v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu-Huan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>",
          "description": "This paper jointly resolves two problems in vision transformer: i) the\ncomputation of Multi-Head Self-Attention (MHSA) has high computational/space\ncomplexity; ii) recent vision transformer networks are overly tuned for image\nclassification, ignoring the difference between image classification (simple\nscenarios, more similar to NLP) and downstream scene understanding tasks\n(complicated scenarios, rich structural and contextual information). To this\nend, we note that pyramid pooling has been demonstrated to be effective in\nvarious vision tasks owing to its powerful context abstraction, and its natural\nproperty of spatial invariance is suitable to address the loss of structural\ninformation (problem ii)). Hence, we propose to adapt pyramid pooling to MHSA\nfor alleviating its high requirement on computational resources (problem i)).\nIn this way, this pooling-based MHSA can well address the above two problems\nand is thus flexible and powerful for downstream scene understanding tasks.\nPlugged with our pooling-based MHSA, we build a downstream-task-oriented\ntransformer network, dubbed Pyramid Pooling Transformer (P2T). Extensive\nexperiments demonstrate that, when applied P2T as the backbone network, it\nshows substantial superiority in various downstream scene understanding tasks\nsuch as semantic segmentation, object detection, instance segmentation, and\nvisual saliency detection, compared to previous CNN- and transformer-based\nnetworks. The code will be released at https://github.com/yuhuan-wu/P2T. Note\nthat this technical report will keep updating.",
          "link": "http://arxiv.org/abs/2106.12011",
          "publishedOn": "2021-06-24T01:51:42.577Z",
          "wordCount": 646,
          "title": "P2T: Pyramid Pooling Transformer for Scene Understanding. (arXiv:2106.12011v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14602",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chettri_B/0/1/0/all/0/1\">Bhusan Chettri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hautamaki_R/0/1/0/all/0/1\">Rosa Gonz&#xe1;lez Hautam&#xe4;ki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sahidullah_M/0/1/0/all/0/1\">Md Sahidullah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kinnunen_T/0/1/0/all/0/1\">Tomi Kinnunen</a>",
          "description": "Voice anti-spoofing aims at classifying a given utterance either as a\nbonafide human sample, or a spoofing attack (e.g. synthetic or replayed\nsample). Many anti-spoofing methods have been proposed but most of them fail to\ngeneralize across domains (corpora) -- and we do not know \\emph{why}. We\noutline a novel interpretative framework for gauging the impact of data quality\nupon anti-spoofing performance. Our within- and between-domain experiments pool\ndata from seven public corpora and three anti-spoofing methods based on\nGaussian mixture and convolutive neural network models. We assess the impacts\nof long-term spectral information, speaker population (through x-vector speaker\nembeddings), signal-to-noise ratio, and selected voice quality features.",
          "link": "http://arxiv.org/abs/2103.14602",
          "publishedOn": "2021-06-23T01:48:40.033Z",
          "wordCount": 579,
          "title": "Data Quality as Predictor of Voice Anti-Spoofing Generalization. (arXiv:2103.14602v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kopf_J/0/1/0/all/0/1\">Johannes Kopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_X/0/1/0/all/0/1\">Xuejian Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Bin Huang</a>",
          "description": "We present an algorithm for estimating consistent dense depth maps and camera\nposes from a monocular video. We integrate a learning-based depth prior, in the\nform of a convolutional neural network trained for single-image depth\nestimation, with geometric optimization, to estimate a smooth camera trajectory\nas well as detailed and stable depth reconstruction. Our algorithm combines two\ncomplementary techniques: (1) flexible deformation-splines for low-frequency\nlarge-scale alignment and (2) geometry-aware depth filtering for high-frequency\nalignment of fine depth details. In contrast to prior approaches, our method\ndoes not require camera poses as input and achieves robust reconstruction for\nchallenging hand-held cell phone captures containing a significant amount of\nnoise, shake, motion blur, and rolling shutter deformations. Our method\nquantitatively outperforms state-of-the-arts on the Sintel benchmark for both\ndepth and pose estimations and attains favorable qualitative results across\ndiverse wild datasets.",
          "link": "http://arxiv.org/abs/2012.05901",
          "publishedOn": "2021-06-23T01:48:40.017Z",
          "wordCount": 600,
          "title": "Robust Consistent Video Depth Estimation. (arXiv:2012.05901v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Ying Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaohan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tiange Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigall_E/0/1/0/all/0/1\">Eric Rigall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>",
          "description": "Textures contain a wealth of image information and are widely used in various\nfields such as computer graphics and computer vision. With the development of\nmachine learning, the texture synthesis and generation have been greatly\nimproved. As a very common element in everyday life, wallpapers contain a\nwealth of texture information, making it difficult to annotate with a simple\nsingle label. Moreover, wallpaper designers spend significant time to create\ndifferent styles of wallpaper. For this purpose, this paper proposes to\ndescribe wallpaper texture images by using multi-label semantics. Based on\nthese labels and generative adversarial networks, we present a framework for\nperception driven wallpaper texture generation and style transfer. In this\nframework, a perceptual model is trained to recognize whether the wallpapers\nproduced by the generator network are sufficiently realistic and have the\nattribute designated by given perceptual description; these multi-label\nsemantic attributes are treated as condition variables to generate wallpaper\nimages. The generated wallpaper images can be converted to those with\nwell-known artist styles using CycleGAN. Finally, using the aesthetic\nevaluation method, the generated wallpaper images are quantitatively measured.\nThe experimental results demonstrate that the proposed method can generate\nwallpaper textures conforming to human aesthetics and have artistic\ncharacteristics.",
          "link": "http://arxiv.org/abs/2106.11482",
          "publishedOn": "2021-06-23T01:48:39.981Z",
          "wordCount": 658,
          "title": "Wallpaper Texture Generation and Style Transfer Based on Multi-label Semantics. (arXiv:2106.11482v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11841",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhipeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jiexi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Aming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Cheng Deng</a>",
          "description": "Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) is a novel cross-modal\nretrieval task, where abstract sketches are used as queries to retrieve natural\nimages under zero-shot scenario. Most existing methods regard ZS-SBIR as a\ntraditional classification problem and employ a cross-entropy or triplet-based\nloss to achieve retrieval, which neglect the problems of the domain gap between\nsketches and natural images and the large intra-class diversity in sketches.\nToward this end, we propose a novel Domain-Smoothing Network (DSN) for ZS-SBIR.\nSpecifically, a cross-modal contrastive method is proposed to learn generalized\nrepresentations to smooth the domain gap by mining relations with additional\naugmented samples. Furthermore, a category-specific memory bank with sketch\nfeatures is explored to reduce intra-class diversity in the sketch domain.\nExtensive experiments demonstrate that our approach notably outperforms the\nstate-of-the-art methods in both Sketchy and TU-Berlin datasets. Our source\ncode is publicly available at https://github.com/haowang1992/DSN.",
          "link": "http://arxiv.org/abs/2106.11841",
          "publishedOn": "2021-06-23T01:48:39.974Z",
          "wordCount": 586,
          "title": "Domain-Smoothing Network for Zero-Shot Sketch-Based Image Retrieval. (arXiv:2106.11841v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McLaughlin_N/0/1/0/all/0/1\">Niall McLaughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rincon_J/0/1/0/all/0/1\">Jesus Martinez del Rincon</a>",
          "description": "Data augmentation has been successfully used in many areas of deep-learning\nto significantly improve model performance. Typically data augmentation\nsimulates realistic variations in data in order to increase the apparent\ndiversity of the training-set. However, for opcode-based malware analysis,\nwhere deep learning methods are already achieving state of the art performance,\nit is not immediately clear how to apply data augmentation. In this paper we\nstudy different methods of data augmentation starting with basic methods using\nfixed transformations and moving to methods that adapt to the data. We propose\na novel data augmentation method based on using an opcode embedding layer\nwithin the network and its corresponding opcode embedding matrix to perform\nadaptive data augmentation during training. To the best of our knowledge this\nis the first paper to carry out a systematic study of different augmentation\nmethods applied to opcode sequence based malware classification.",
          "link": "http://arxiv.org/abs/2106.11821",
          "publishedOn": "2021-06-23T01:48:39.962Z",
          "wordCount": 590,
          "title": "Data Augmentation for Opcode Sequence Based Malware Detection. (arXiv:2106.11821v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xin Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangrui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Feng Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>",
          "description": "LiDAR-based SLAM system is admittedly more accurate and stable than others,\nwhile its loop closure detection is still an open issue. With the development\nof 3D semantic segmentation for point cloud, semantic information can be\nobtained conveniently and steadily, essential for high-level intelligence and\nconductive to SLAM. In this paper, we present a novel semantic-aided LiDAR SLAM\nwith loop closure based on LOAM, named SA-LOAM, which leverages semantics in\nodometry as well as loop closure detection. Specifically, we propose a\nsemantic-assisted ICP, including semantically matching, downsampling and plane\nconstraint, and integrates a semantic graph-based place recognition method in\nour loop closure detection module. Benefitting from semantics, we can improve\nthe localization accuracy, detect loop closures effectively, and construct a\nglobal consistent semantic map even in large-scale scenes. Extensive\nexperiments on KITTI and Ford Campus dataset show that our system significantly\nimproves baseline performance, has generalization ability to unseen data and\nachieves competitive results compared with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.11516",
          "publishedOn": "2021-06-23T01:48:39.955Z",
          "wordCount": 599,
          "title": "SA-LOAM: Semantic-aided LiDAR SLAM with Loop Closure. (arXiv:2106.11516v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1\">Zhiwu Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jianwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Changxin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1\">Nong Sang</a>",
          "description": "This technical report presents our solution for temporal action detection\ntask in AcitivityNet Challenge 2021. The purpose of this task is to locate and\nidentify actions of interest in long untrimmed videos. The crucial challenge of\nthe task comes from that the temporal duration of action varies dramatically,\nand the target actions are typically embedded in a background of irrelevant\nactivities. Our solution builds on BMN, and mainly contains three steps: 1)\naction classification and feature encoding by Slowfast, CSN and ViViT; 2)\nproposal generation. We improve BMN by embedding the proposed Proposal Relation\nNetwork (PRN), by which we can generate proposals of high quality; 3) action\ndetection. We calculate the detection results by assigning the proposals with\ncorresponding classification results. Finally, we ensemble the results under\ndifferent settings and achieve 44.7% on the test set, which improves the\nchampion result in ActivityNet 2020 by 1.9% in terms of average mAP.",
          "link": "http://arxiv.org/abs/2106.11812",
          "publishedOn": "2021-06-23T01:48:39.948Z",
          "wordCount": 609,
          "title": "Proposal Relation Network for Temporal Action Detection. (arXiv:2106.11812v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seiskari_O/0/1/0/all/0/1\">Otto Seiskari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rantalankila_P/0/1/0/all/0/1\">Pekka Rantalankila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1\">Juho Kannala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ylilammi_J/0/1/0/all/0/1\">Jerry Ylilammi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1\">Arno Solin</a>",
          "description": "We present HybVIO, a novel hybrid approach for combining filtering-based\nvisual-inertial odometry (VIO) with optimization-based SLAM. The core of our\nmethod is highly robust, independent VIO with improved IMU bias modeling,\noutlier rejection, stationarity detection, and feature track selection, which\nis adjustable to run on embedded hardware. Long-term consistency is achieved\nwith a loosely-coupled SLAM module. In academic benchmarks, our solution yields\nexcellent performance in all categories, especially in the real-time use case,\nwhere we outperform the current state-of-the-art. We also demonstrate the\nfeasibility of VIO for vehicular tracking on consumer-grade hardware using a\ncustom dataset, and show good performance in comparison to current commercial\nVISLAM alternatives.",
          "link": "http://arxiv.org/abs/2106.11857",
          "publishedOn": "2021-06-23T01:48:39.928Z",
          "wordCount": 547,
          "title": "HybVIO: Pushing the Limits of Real-time Visual-inertial Odometry. (arXiv:2106.11857v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.07404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plizzari_C/0/1/0/all/0/1\">Chiara Plizzari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cannici_M/0/1/0/all/0/1\">Marco Cannici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1\">Matteo Matteucci</a>",
          "description": "Skeleton-based Human Activity Recognition has achieved great interest in\nrecent years as skeleton data has demonstrated being robust to illumination\nchanges, body scales, dynamic camera views, and complex background. In\nparticular, Spatial-Temporal Graph Convolutional Networks (ST-GCN) demonstrated\nto be effective in learning both spatial and temporal dependencies on\nnon-Euclidean data such as skeleton graphs. Nevertheless, an effective encoding\nof the latent information underlying the 3D skeleton is still an open problem,\nespecially when it comes to extracting effective information from joint motion\npatterns and their correlations. In this work, we propose a novel\nSpatial-Temporal Transformer network (ST-TR) which models dependencies between\njoints using the Transformer self-attention operator. In our ST-TR model, a\nSpatial Self-Attention module (SSA) is used to understand intra-frame\ninteractions between different body parts, and a Temporal Self-Attention module\n(TSA) to model inter-frame correlations. The two are combined in a two-stream\nnetwork, whose performance is evaluated on three large-scale datasets,\nNTU-RGB+D 60, NTU-RGB+D 120, and Kinetics Skeleton 400, consistently improving\nbackbone results. Compared with methods that use the same input data, the\nproposed ST-TR achieves state-of-the-art performance on all datasets when using\njoints' coordinates as input, and results on-par with state-of-the-art when\nadding bones information.",
          "link": "http://arxiv.org/abs/2008.07404",
          "publishedOn": "2021-06-23T01:48:39.922Z",
          "wordCount": 705,
          "title": "Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks. (arXiv:2008.07404v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1\">Abraham George Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_J/0/1/0/all/0/1\">Jens Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terrones_Campos_C/0/1/0/all/0/1\">Cynthia Terrones-Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berthelsen_A/0/1/0/all/0/1\">Anne Kiil Berthelsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forbes_N/0/1/0/all/0/1\">Nora Jarrett Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darkner_S/0/1/0/all/0/1\">Sune Darkner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specht_L/0/1/0/all/0/1\">Lena Specht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelius_I/0/1/0/all/0/1\">Ivan Richter Vogelius</a>",
          "description": "Organ-at-risk contouring is still a bottleneck in radiotherapy, with many\ndeep learning methods falling short of promised results when evaluated on\nclinical data. We investigate the accuracy and time-savings resulting from the\nuse of an interactive-machine-learning method for an organ-at-risk contouring\ntask. We compare the method to the Eclipse contouring software and find strong\nagreement with manual delineations, with a dice score of 0.95. The annotations\ncreated using corrective-annotation also take less time to create as more\nimages are annotated, resulting in substantial time savings compared to manual\nmethods, with hearts that take 2 minutes and 2 seconds to delineate on average,\nafter 923 images have been delineated, compared to 7 minutes and 1 seconds when\ndelineating manually. Our experiment demonstrates that\ninteractive-machine-learning with corrective-annotation provides a fast and\naccessible way for non computer-scientists to train deep-learning models to\nsegment their own structures of interest as part of routine clinical workflows.\n\nSource code is available at\n\\href{https://github.com/Abe404/RootPainter3D}{this HTTPS URL}.",
          "link": "http://arxiv.org/abs/2106.11942",
          "publishedOn": "2021-06-23T01:48:39.914Z",
          "wordCount": 618,
          "title": "RootPainter3D: Interactive-machine-learning enables rapid and accurate contouring for radiotherapy. (arXiv:2106.11942v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thames_Q/0/1/0/all/0/1\">Quin Thames</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpur_A/0/1/0/all/0/1\">Arjun Karpur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norris_W/0/1/0/all/0/1\">Wade Norris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fangting Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panait_L/0/1/0/all/0/1\">Liviu Panait</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyand_T/0/1/0/all/0/1\">Tobias Weyand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_J/0/1/0/all/0/1\">Jack Sim</a>",
          "description": "Understanding the nutritional content of food from visual data is a\nchallenging computer vision problem, with the potential to have a positive and\nwidespread impact on public health. Studies in this area are limited to\nexisting datasets in the field that lack sufficient diversity or labels\nrequired for training models with nutritional understanding capability. We\nintroduce Nutrition5k, a novel dataset of 5k diverse, real world food dishes\nwith corresponding video streams, depth images, component weights, and high\naccuracy nutritional content annotation. We demonstrate the potential of this\ndataset by training a computer vision algorithm capable of predicting the\ncaloric and macronutrient values of a complex, real world dish at an accuracy\nthat outperforms professional nutritionists. Further we present a baseline for\nincorporating depth sensor data to improve nutrition predictions. We will\npublicly release Nutrition5k in the hope that it will accelerate innovation in\nthe space of nutritional understanding.",
          "link": "http://arxiv.org/abs/2103.03375",
          "publishedOn": "2021-06-23T01:48:39.888Z",
          "wordCount": 630,
          "title": "Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food. (arXiv:2103.03375v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxiang Zhang</a>",
          "description": "Benefited from considerable pixel-level annotations collected from a specific\nsituation (source), the trained semantic segmentation model performs quite\nwell, but fails in a new situation (target) due to the large domain shift. To\nmitigate the domain gap, previous cross-domain semantic segmentation methods\nalways assume the co-existence of source data and target data during\ndistribution alignment. However, the access to source data in the real scenario\nmay raise privacy concerns and violate intellectual property. To tackle this\nproblem, we focus on an interesting and challenging cross-domain semantic\nsegmentation task where only the trained source model is provided to the target\ndomain, and further propose a unified framework called Domain Adaptive Semantic\nSegmentation without Source data (DAS$^3$ for short). Specifically, DAS$^3$\nconsists of three schemes, i.e., feature alignment, self-training, and\ninformation propagation. First, we mainly develop a focal entropic loss on the\nnetwork outputs to implicitly align the target features with unseen source\nfeatures via the provided source model. Second, besides positive pseudo labels\nin vanilla self-training, we first introduce negative pseudo labels to the\nfield and develop a bi-directional self-training strategy to enhance the\nrepresentation learning in the target domain. Finally, the information\npropagation scheme further reduces the intra-domain discrepancy within the\ntarget domain via pseudo semi-supervised learning. Extensive results on\nsynthesis-to-real and cross-city driving datasets validate DAS$^3$ yields\nstate-of-the-art performance, even on par with methods that need access to\nsource data.",
          "link": "http://arxiv.org/abs/2106.11653",
          "publishedOn": "2021-06-23T01:48:39.881Z",
          "wordCount": 672,
          "title": "Give Me Your Trained Model: Domain Adaptive Semantic Segmentation without Source Data. (arXiv:2106.11653v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_N/0/1/0/all/0/1\">Naeun Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We propose a novel and effective input transformation based adversarial\ndefense method against gray- and black-box attack, which is computationally\nefficient and does not require any adversarial training or retraining of a\nclassification model. We first show that a very simple iterative Gaussian\nsmoothing can effectively wash out adversarial noise and achieve substantially\nhigh robust accuracy. Based on the observation, we propose Self-Supervised\nIterative Contextual Smoothing (SSICS), which aims to reconstruct the original\ndiscriminative features from the Gaussian-smoothed image in context-adaptive\nmanner, while still smoothing out the adversarial noise. From the experiments\non ImageNet, we show that our SSICS achieves both high standard accuracy and\nvery competitive robust accuracy for the gray- and black-box attacks; e.g.,\ntransfer-based PGD-attack and score-based attack. A note-worthy point to stress\nis that our defense is free of computationally expensive adversarial training,\nyet, can approach its robust accuracy via input transformation.",
          "link": "http://arxiv.org/abs/2106.11644",
          "publishedOn": "2021-06-23T01:48:39.873Z",
          "wordCount": 599,
          "title": "Self-Supervised Iterative Contextual Smoothing for Efficient Adversarial Defense against Gray- and Black-Box Attack. (arXiv:2106.11644v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Aadarsh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1\">Ruining Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Tianyuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_Jansen_A/0/1/0/all/0/1\">Anita Mahadevan-Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyska_M/0/1/0/all/0/1\">Matthew J.Tyska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millis_B/0/1/0/all/0/1\">Bryan A. Millis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>",
          "description": "Recent advances in bioimaging have provided scientists a superior high\nspatial-temporal resolution to observe dynamics of living cells as 3D\nvolumetric videos. Unfortunately, the 3D biomedical video analysis is lagging,\nimpeded by resource insensitive human curation using off-the-shelf 3D analytic\ntools. Herein, biologists often need to discard a considerable amount of rich\n3D spatial information by compromising on 2D analysis via maximum intensity\nprojection. Recently, pixel embedding-based cell instance segmentation and\ntracking provided a neat and generalizable computing paradigm for understanding\ncellular dynamics. In this work, we propose a novel spatial-temporal\nvoxel-embedding (VoxelEmbed) based learning method to perform simultaneous cell\ninstance segmenting and tracking on 3D volumetric video sequences. Our\ncontribution is in four-fold: (1) The proposed voxel embedding generalizes the\npixel embedding with 3D context information; (2) Present a simple multi-stream\nlearning approach that allows effective spatial-temporal embedding; (3)\nAccomplished an end-to-end framework for one-stage 3D cell instance\nsegmentation and tracking without heavy parameter tuning; (4) The proposed 3D\nquantification is memory efficient via a single GPU with 12 GB memory. We\nevaluate our VoxelEmbed method on four 3D datasets (with different cell types)\nfrom the ISBI Cell Tracking Challenge. The proposed VoxelEmbed method achieved\nconsistent superior overall performance (OP) on two densely annotated datasets.\nThe performance is also competitive on two sparsely annotated cohorts with\n20.6% and 2% of data-set having segmentation annotations. The results\ndemonstrate that the VoxelEmbed method is a generalizable and memory-efficient\nsolution.",
          "link": "http://arxiv.org/abs/2106.11480",
          "publishedOn": "2021-06-23T01:48:39.865Z",
          "wordCount": 700,
          "title": "VoxelEmbed: 3D Instance Segmentation and Tracking with Voxel Embedding based Deep Learning. (arXiv:2106.11480v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1903.12561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Shaokai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaidi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambrechts_J/0/1/0/all/0/1\">Jan-Henrik Lambrechts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aojun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaisheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>",
          "description": "It is well known that deep neural networks (DNNs) are vulnerable to\nadversarial attacks, which are implemented by adding crafted perturbations onto\nbenign examples. Min-max robust optimization based adversarial training can\nprovide a notion of security against adversarial attacks. However, adversarial\nrobustness requires a significantly larger capacity of the network than that\nfor the natural training with only benign examples. This paper proposes a\nframework of concurrent adversarial training and weight pruning that enables\nmodel compression while still preserving the adversarial robustness and\nessentially tackles the dilemma of adversarial training. Furthermore, this work\nstudies two hypotheses about weight pruning in the conventional setting and\nfinds that weight pruning is essential for reducing the network model size in\nthe adversarial setting, training a small model from scratch even with\ninherited initialization from the large model cannot achieve both adversarial\nrobustness and high standard accuracy. Code is available at\nhttps://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM.",
          "link": "http://arxiv.org/abs/1903.12561",
          "publishedOn": "2021-06-23T01:48:39.858Z",
          "wordCount": 666,
          "title": "Adversarial Robustness vs Model Compression, or Both?. (arXiv:1903.12561v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moller_R/0/1/0/all/0/1\">Ronja M&#xf6;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1\">Antonino Furnari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battiato_S/0/1/0/all/0/1\">Sebastiano Battiato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harma_A/0/1/0/all/0/1\">Aki H&#xe4;rm&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1\">Giovanni Maria Farinella</a>",
          "description": "Intelligent systems are increasingly part of our everyday lives and have been\nintegrated seamlessly to the point where it is difficult to imagine a world\nwithout them. Physical manifestations of those systems on the other hand, in\nthe form of embodied agents or robots, have so far been used only for specific\napplications and are often limited to functional roles (e.g. in the industry,\nentertainment and military fields). Given the current growth and innovation in\nthe research communities concerned with the topics of robot navigation,\nhuman-robot-interaction and human activity recognition, it seems like this\nmight soon change. Robots are increasingly easy to obtain and use and the\nacceptance of them in general is growing. However, the design of a socially\ncompliant robot that can function as a companion needs to take various areas of\nresearch into account. This paper is concerned with the navigation aspect of a\nsocially-compliant robot and provides a survey of existing solutions for the\nrelevant areas of research as well as an outlook on possible future directions.",
          "link": "http://arxiv.org/abs/2106.11650",
          "publishedOn": "2021-06-23T01:48:39.850Z",
          "wordCount": 611,
          "title": "A Survey on Human-aware Robot Navigation. (arXiv:2106.11650v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Girbau_A/0/1/0/all/0/1\">Andreu Girbau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1\">Xavier Gir&#xf3;-i-Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rius_I/0/1/0/all/0/1\">Ignasi Rius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marques_F/0/1/0/all/0/1\">Ferran Marqu&#xe9;s</a>",
          "description": "Multiple object tracking faces several challenges that may be alleviated with\ntrajectory information. Knowing the posterior locations of an object helps\ndisambiguating and solving situations such as occlusions, re-identification,\nand identity switching. In this work, we show that trajectory estimation can\nbecome a key factor for tracking, and present TrajE, a trajectory estimator\nbased on recurrent mixture density networks, as a generic module that can be\nadded to existing object trackers. To provide several trajectory hypotheses,\nour method uses beam search. Also, relying on the same estimated trajectory, we\npropose to reconstruct a track after an occlusion occurs. We integrate TrajE\ninto two state of the art tracking algorithms, CenterTrack [63] and Tracktor\n[3]. Their respective performances in the MOTChallenge 2017 test set are\nboosted 6.3 and 0.3 points in MOTA score, and 1.8 and 3.1 in IDF1, setting a\nnew state of the art for the CenterTrack+TrajE configuration",
          "link": "http://arxiv.org/abs/2106.10950",
          "publishedOn": "2021-06-23T01:48:39.843Z",
          "wordCount": 616,
          "title": "Multiple Object Tracking with Mixture Density Networks for Trajectory Estimation. (arXiv:2106.10950v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>",
          "description": "Image reconstruction is likely the most predominant auxiliary task for image\nclassification. In this paper, we investigate ``estimating the Fourier\nTransform of the input image\" as a potential alternative auxiliary task, in the\nhope that it may further boost the performances on the primary task or\nintroduce novel constraints not well covered by image reconstruction. We\nexperimented with five popular classification architectures on the CIFAR-10\ndataset, and the empirical results indicated that our proposed auxiliary task\ngenerally improves the classification accuracy. More notably, the results\nshowed that in certain cases our proposed auxiliary task may enhance the\nclassifiers' resistance to adversarial attacks generated using the fast\ngradient sign method.",
          "link": "http://arxiv.org/abs/2106.11478",
          "publishedOn": "2021-06-23T01:48:39.835Z",
          "wordCount": 563,
          "title": "An Alternative Auxiliary Task for Enhancing Image Classification. (arXiv:2106.11478v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1\">Pietro Barbiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1\">Gabriele Ciravegna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1\">Francesco Giannini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1\">Marco Gori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1\">Stefano Melacci</a>",
          "description": "Explainable artificial intelligence has rapidly emerged since lawmakers have\nstarted requiring interpretable models for safety-critical domains.\nConcept-based neural networks have arisen as explainable-by-design methods as\nthey leverage human-understandable symbols (i.e. concepts) to predict class\nmemberships. However, most of these approaches focus on the identification of\nthe most relevant concepts but do not provide concise, formal explanations of\nhow such concepts are leveraged by the classifier to make predictions. In this\npaper, we propose a novel end-to-end differentiable approach enabling the\nextraction of logic explanations from neural networks using the formalism of\nFirst-Order Logic. The method relies on an entropy-based criterion which\nautomatically identifies the most relevant concepts. We consider four different\ncase studies to demonstrate that: (i) this entropy-based criterion enables the\ndistillation of concise logic explanations in safety-critical domains from\nclinical data to computer vision; (ii) the proposed approach outperforms\nstate-of-the-art white-box models in terms of classification accuracy.",
          "link": "http://arxiv.org/abs/2106.06804",
          "publishedOn": "2021-06-23T01:48:39.808Z",
          "wordCount": 622,
          "title": "Entropy-based Logic Explanations of Neural Networks. (arXiv:2106.06804v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yousong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Lu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Yi Shan</a>",
          "description": "Recent work attempts to improve semantic segmentation performance by\nexploring well-designed architectures on a target dataset. However, it remains\nchallenging to build a unified system that simultaneously learns from various\ndatasets due to the inherent distribution shift across different datasets. In\nthis paper, we present a simple, flexible, and general method for semantic\nsegmentation, termed Cross-Dataset Collaborative Learning (CDCL). Given\nmultiple labeled datasets, we aim to improve the generalization and\ndiscrimination of feature representations on each dataset. Specifically, we\nfirst introduce a family of Dataset-Aware Blocks (DAB) as the fundamental\ncomputing units of the network, which help capture homogeneous representations\nand heterogeneous statistics across different datasets. Second, we propose a\nDataset Alternation Training (DAT) mechanism to efficiently facilitate the\noptimization procedure. We conduct extensive evaluations on four diverse\ndatasets, i.e., Cityscapes, BDD100K, CamVid, and COCO Stuff, with\nsingle-dataset and cross-dataset settings. Experimental results demonstrate our\nmethod consistently achieves notable improvements over prior single-dataset and\ncross-dataset training methods without introducing extra FLOPs. Particularly,\nwith the same architecture of PSPNet (ResNet-18), our method outperforms the\nsingle-dataset baseline by 5.65\\%, 6.57\\%, and 5.79\\% of mIoU on the validation\nsets of Cityscapes, BDD100K, CamVid, respectively. Code and models will be\nreleased.",
          "link": "http://arxiv.org/abs/2103.11351",
          "publishedOn": "2021-06-23T01:48:39.799Z",
          "wordCount": 661,
          "title": "Cross-Dataset Collaborative Learning for Semantic Segmentation. (arXiv:2103.11351v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihajlovic_M/0/1/0/all/0/1\">Marko Mihajlovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qianli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>",
          "description": "In this paper, we aim to create generalizable and controllable neural signed\ndistance fields (SDFs) that represent clothed humans from monocular depth\nobservations. Recent advances in deep learning, especially neural implicit\nrepresentations, have enabled human shape reconstruction and controllable\navatar generation from different sensor inputs. However, to generate realistic\ncloth deformations from novel input poses, watertight meshes or dense full-body\nscans are usually needed as inputs. Furthermore, due to the difficulty of\neffectively modeling pose-dependent cloth deformations for diverse body shapes\nand cloth types, existing approaches resort to per-subject/cloth-type\noptimization from scratch, which is computationally expensive. In contrast, we\npropose an approach that can quickly generate realistic clothed human avatars,\nrepresented as controllable neural SDFs, given only monocular depth images. We\nachieve this by using meta-learning to learn an initialization of a\nhypernetwork that predicts the parameters of neural SDFs. The hypernetwork is\nconditioned on human poses and represents a clothed neural avatar that deforms\nnon-rigidly according to the input poses. Meanwhile, it is meta-learned to\neffectively incorporate priors of diverse body shapes and cloth types and thus\ncan be much faster to fine-tune, compared to models trained from scratch. We\nqualitatively and quantitatively show that our approach outperforms\nstate-of-the-art approaches that require complete meshes as inputs while our\napproach requires only depth frames as inputs and runs orders of magnitudes\nfaster. Furthermore, we demonstrate that our meta-learned hypernetwork is very\nrobust, being the first to generate avatars with realistic dynamic cloth\ndeformations given as few as 8 monocular depth frames.",
          "link": "http://arxiv.org/abs/2106.11944",
          "publishedOn": "2021-06-23T01:48:39.769Z",
          "wordCount": 702,
          "title": "MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images. (arXiv:2106.11944v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esteban_J/0/1/0/all/0/1\">Jan Moros Esteban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loosdrecht_J/0/1/0/all/0/1\">Jaap van de Loosdrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghaei_M/0/1/0/all/0/1\">Maya Aghaei</a>",
          "description": "With the introduction of new regulations in the European Union, the future of\nBeyond Visual Line Of Sight (BVLOS) drones is set to bloom. This led to the\ncreation of the theBEAST project, which aims to create an autonomous security\ndrone, with focus on those regulations and on safety. This technical paper\ndescribes the first steps of a module within this project, which revolves\naround detecting obstacles so they can be avoided in a fail-safe landing. A\ndeep learning powered object detection method is the subject of our research,\nand various experiments are held to maximize its performance, such as comparing\nvarious data augmentation techniques or YOLOv3 and YOLOv5. According to the\nresults of the experiments, we conclude that although object detection is a\npromising approach to resolve this problem, more volume of data is required for\npotential usage in a real-life application.",
          "link": "http://arxiv.org/abs/2106.11098",
          "publishedOn": "2021-06-23T01:48:39.756Z",
          "wordCount": 606,
          "title": "Obstacle Detection for BVLOS Drones. (arXiv:2106.11098v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiahao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xiaohang Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1\">Yew Soon Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>",
          "description": "Contrastive self-supervised learning has largely narrowed the gap to\nsupervised pre-training on ImageNet. However, its success highly relies on the\nobject-centric priors of ImageNet, i.e., different augmented views of the same\nimage correspond to the same object. Such a heavily curated constraint becomes\nimmediately infeasible when pre-trained on more complex scene images with many\nobjects. To overcome this limitation, we introduce Object-level Representation\nLearning (ORL), a new self-supervised learning framework towards scene images.\nOur key insight is to leverage image-level self-supervised pre-training as the\nprior to discover object-level semantic correspondence, thus realizing\nobject-level representation learning from scene images. Extensive experiments\non COCO show that ORL significantly improves the performance of self-supervised\nlearning on scene images, even surpassing supervised ImageNet pre-training on\nseveral downstream tasks. Furthermore, ORL improves the downstream performance\nwhen more unlabeled scene images are available, demonstrating its great\npotential of harnessing unlabeled data in the wild. We hope our approach can\nmotivate future research on more general-purpose unsupervised representation\nlearning from scene data. Project page: https://www.mmlab-ntu.com/project/orl/.",
          "link": "http://arxiv.org/abs/2106.11952",
          "publishedOn": "2021-06-23T01:48:39.734Z",
          "wordCount": 608,
          "title": "Unsupervised Object-Level Representation Learning from Scene Images. (arXiv:2106.11952v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.13630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "As soon as abstract mathematical computations were adapted to computation on\ndigital computers, the problem of efficient representation, manipulation, and\ncommunication of the numerical values in those computations arose. Strongly\nrelated to the problem of numerical representation is the problem of\nquantization: in what manner should a set of continuous real-valued numbers be\ndistributed over a fixed discrete set of numbers to minimize the number of bits\nrequired and also to maximize the accuracy of the attendant computations? This\nperennial problem of quantization is particularly relevant whenever memory\nand/or computational resources are severely restricted, and it has come to the\nforefront in recent years due to the remarkable performance of Neural Network\nmodels in computer vision, natural language processing, and related areas.\nMoving from floating-point representations to low-precision fixed integer\nvalues represented in four bits or less holds the potential to reduce the\nmemory footprint and latency by a factor of 16x; and, in fact, reductions of 4x\nto 8x are often realized in practice in these applications. Thus, it is not\nsurprising that quantization has emerged recently as an important and very\nactive sub-area of research in the efficient implementation of computations\nassociated with Neural Networks. In this article, we survey approaches to the\nproblem of quantizing the numerical values in deep Neural Network computations,\ncovering the advantages/disadvantages of current methods. With this survey and\nits organization, we hope to have presented a useful snapshot of the current\nresearch in quantization for Neural Networks and to have given an intelligent\norganization to ease the evaluation of future research in this area.",
          "link": "http://arxiv.org/abs/2103.13630",
          "publishedOn": "2021-06-23T01:48:39.712Z",
          "wordCount": 759,
          "title": "A Survey of Quantization Methods for Efficient Neural Network Inference. (arXiv:2103.13630v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haiping Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yingying Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qiuyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guohui Zheng</a>",
          "description": "Predefined evenly-distributed class centroids (PEDCC) can be widely used in\nmodels and algorithms of pattern classification, such as CNN classifiers,\nclassification autoencoders, clustering, and semi-supervised learning, etc. Its\nbasic idea is to predefine the class centers, which are evenly-distributed on\nthe unit hypersphere in feature space, to maximize the inter-class distance.\nThe previous method of generating PEDCC uses an iterative algorithm based on a\ncharge model, that is, the initial values of various centers (charge positions)\nare randomly set from the normal distribution, and the charge positions are\nupdated iteratively with the help of the repulsive force between charges of the\nsame polarity. The class centers generated by the algorithm will produce some\nerrors with the theoretically evenly-distributed points, and the generation\ntime will be longer. This paper takes advantage of regular polyhedron in\nhigh-dimensional space and the evenly distribution of points on the n\ndimensional hypersphere to generate PEDCC mathematically. Then, we discussed\nthe basic and extensive characteristics of the frames formed by PEDCC. Finally,\nexperiments show that new algorithm is not only faster than the iterative\nmethod, but also more accurate in position. The mathematical analysis and\nexperimental results of this paper can provide a theoretical tool for using\nPEDCC to solve the key problems in the field of pattern recognition, such as\ninterpretable supervised/unsupervised learning, incremental learning,\nuncertainty analysis and so on.",
          "link": "http://arxiv.org/abs/2105.00401",
          "publishedOn": "2021-06-23T01:48:39.704Z",
          "wordCount": 695,
          "title": "Generation and frame characteristics of predefined evenly-distributed class centroids for pattern classification. (arXiv:2105.00401v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soutif__Cormerais_A/0/1/0/all/0/1\">Albin Soutif--Cormerais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masana_M/0/1/0/all/0/1\">Marc Masana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost Van de Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1\">Bart&#x142;omiej Twardowski</a>",
          "description": "In class-incremental learning, an agent with limited resources needs to learn\na sequence of classification tasks, forming an ever growing classification\nproblem, with the constraint of not being able to access data from previous\ntasks. The main difference with task-incremental learning, where a task-ID is\navailable at inference time, is that the learner also needs to perform\ncross-task discrimination, i.e. distinguish between classes that have not been\nseen together. Approaches to tackle this problem are numerous and mostly make\nuse of an external memory (buffer) of non-negligible size. In this paper, we\nablate the learning of cross-task features and study its influence on the\nperformance of basic replay strategies used for class-IL. We also define a new\nforgetting measure for class-incremental learning, and see that forgetting is\nnot the principal cause of low performance. Our experimental results show that\nfuture algorithms for class-incremental learning should not only prevent\nforgetting, but also aim to improve the quality of the cross-task features.\nThis is especially important when the number of classes per task is small.",
          "link": "http://arxiv.org/abs/2106.11930",
          "publishedOn": "2021-06-23T01:48:39.685Z",
          "wordCount": 618,
          "title": "On the importance of cross-task features for class-incremental learning. (arXiv:2106.11930v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1\">Lei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yu-Wing Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chi-Keung Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>",
          "description": "Multiple object tracking and segmentation requires detecting, tracking, and\nsegmenting objects belonging to a set of given classes. Most approaches only\nexploit the temporal dimension to address the association problem, while\nrelying on single frame predictions for the segmentation mask itself. We\npropose Prototypical Cross-Attention Network (PCAN), capable of leveraging rich\nspatio-temporal information for online multiple object tracking and\nsegmentation. PCAN first distills a space-time memory into a set of prototypes\nand then employs cross-attention to retrieve rich information from the past\nframes. To segment each object, PCAN adopts a prototypical appearance module to\nlearn a set of contrastive foreground and background prototypes, which are then\npropagated over time. Extensive experiments demonstrate that PCAN outperforms\ncurrent video instance tracking and segmentation competition winners on both\nYoutube-VIS and BDD100K datasets, and shows efficacy to both one-stage and\ntwo-stage segmentation frameworks. Code will be available at\nthis http URL",
          "link": "http://arxiv.org/abs/2106.11958",
          "publishedOn": "2021-06-23T01:48:39.676Z",
          "wordCount": 596,
          "title": "Prototypical Cross-Attention Networks for Multiple Object Tracking and Segmentation. (arXiv:2106.11958v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11725",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiayi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_F/0/1/0/all/0/1\">Franziska Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1\">Florian Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorli_S/0/1/0/all/0/1\">Suzanne Sorli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sotnychenko_O/0/1/0/all/0/1\">Oleksandr Sotnychenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_N/0/1/0/all/0/1\">Neng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otaduy_M/0/1/0/all/0/1\">Miguel A. Otaduy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casas_D/0/1/0/all/0/1\">Dan Casas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>",
          "description": "Tracking and reconstructing the 3D pose and geometry of two hands in\ninteraction is a challenging problem that has a high relevance for several\nhuman-computer interaction applications, including AR/VR, robotics, or sign\nlanguage recognition. Existing works are either limited to simpler tracking\nsettings (e.g., considering only a single hand or two spatially separated\nhands), or rely on less ubiquitous sensors, such as depth cameras. In contrast,\nin this work we present the first real-time method for motion capture of\nskeletal pose and 3D surface geometry of hands from a single RGB camera that\nexplicitly considers close interactions. In order to address the inherent depth\nambiguities in RGB data, we propose a novel multi-task CNN that regresses\nmultiple complementary pieces of information, including segmentation, dense\nmatchings to a 3D hand model, and 2D keypoint positions, together with newly\nproposed intra-hand relative depth and inter-hand distance maps. These\npredictions are subsequently used in a generative model fitting framework in\norder to estimate pose and shape parameters of a 3D hand model for both hands.\nWe experimentally verify the individual components of our RGB two-hand tracking\nand 3D reconstruction pipeline through an extensive ablation study. Moreover,\nwe demonstrate that our approach offers previously unseen two-hand tracking\nperformance from RGB, and quantitatively and qualitatively outperforms existing\nRGB-based methods that were not explicitly designed for two-hand interactions.\nMoreover, our method even performs on-par with depth-based real-time methods.",
          "link": "http://arxiv.org/abs/2106.11725",
          "publishedOn": "2021-06-23T01:48:39.669Z",
          "wordCount": 699,
          "title": "RGB2Hands: Real-Time Tracking of 3D Hand Interactions from Monocular RGB Video. (arXiv:2106.11725v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07467",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yeung_M/0/1/0/all/0/1\">Michael Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sala_E/0/1/0/all/0/1\">Evis Sala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1\">Leonardo Rundo</a>",
          "description": "Background: Colonoscopy remains the gold-standard screening for colorectal\ncancer. However, significant miss rates for polyps have been reported,\nparticularly when there are multiple small adenomas. This presents an\nopportunity to leverage computer-aided systems to support clinicians and reduce\nthe number of polyps missed.\n\nMethod: In this work we introduce the Focus U-Net, a novel dual\nattention-gated deep neural network, which combines efficient spatial and\nchannel-based attention into a single Focus Gate module to encourage selective\nlearning of polyp features. The Focus U-Net further incorporates short-range\nskip connections and deep supervision. Furthermore, we introduce the Hybrid\nFocal loss, a new compound loss function based on the Focal loss and Focal\nTversky loss, to handle class-imbalanced image segmentation. For our\nexperiments, we selected five public datasets containing images of polyps\nobtained during optical colonoscopy: CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB,\nETIS-Larib PolypDB and EndoScene test set. To evaluate model performance, we\nuse the Dice similarity coefficient (DSC) and Intersection over Union (IoU)\nmetrics.\n\nResults: Our model achieves state-of-the-art results for both CVC-ClinicDB\nand Kvasir-SEG, with a mean DSC of 0.941 and 0.910, respectively. When\nevaluated on a combination of five public polyp datasets, our model similarly\nachieves state-of-the-art results with a mean DSC of 0.878 and mean IoU of\n0.809, a 14% and 15% improvement over the previous state-of-the-art results of\n0.768 and 0.702, respectively.\n\nConclusions: This study shows the potential for deep learning to provide fast\nand accurate polyp segmentation results for use during colonoscopy. The Focus\nU-Net may be adapted for future use in newer non-invasive screening and more\nbroadly to other biomedical image segmentation tasks involving class imbalance\nand requiring efficiency.",
          "link": "http://arxiv.org/abs/2105.07467",
          "publishedOn": "2021-06-23T01:48:39.662Z",
          "wordCount": 745,
          "title": "Focus U-Net: A novel dual attention-gated CNN for polyp segmentation during colonoscopy. (arXiv:2105.07467v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luan_F/0/1/0/all/0/1\">Fujun Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bala_K/0/1/0/all/0/1\">Kavita Bala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhao Dong</a>",
          "description": "Reconstructing the shape and appearance of real-world objects using measured\n2D images has been a long-standing problem in computer vision. In this paper,\nwe introduce a new analysis-by-synthesis technique capable of producing\nhigh-quality reconstructions through robust coarse-to-fine optimization and\nphysics-based differentiable rendering.\n\nUnlike most previous methods that handle geometry and reflectance largely\nseparately, our method unifies the optimization of both by leveraging image\ngradients with respect to both object reflectance and geometry. To obtain\nphysically accurate gradient estimates, we develop a new GPU-based Monte Carlo\ndifferentiable renderer leveraging recent advances in differentiable rendering\ntheory to offer unbiased gradients while enjoying better performance than\nexisting tools like PyTorch3D and redner. To further improve robustness, we\nutilize several shape and material priors as well as a coarse-to-fine\noptimization strategy to reconstruct geometry. We demonstrate that our\ntechnique can produce reconstructions with higher quality than previous methods\nsuch as COLMAP and Kinect Fusion.",
          "link": "http://arxiv.org/abs/2103.15208",
          "publishedOn": "2021-06-23T01:48:39.655Z",
          "wordCount": 618,
          "title": "Unified Shape and SVBRDF Recovery using Differentiable Monte Carlo Rendering. (arXiv:2103.15208v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08533",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Simoes_M/0/1/0/all/0/1\">Miguel Sim&#xf5;es</a>, <a href=\"http://arxiv.org/find/math/1/au:+Themelis_A/0/1/0/all/0/1\">Andreas Themelis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Patrinos_P/0/1/0/all/0/1\">Panagiotis Patrinos</a>",
          "description": "In large-scale optimization, the presence of nonsmooth and nonconvex terms in\na given problem typically makes it hard to solve. A popular approach to address\nnonsmooth terms in convex optimization is to approximate them with their\nrespective Moreau envelopes. In this work, we study the use of Lasry-Lions\ndouble envelopes to approximate nonsmooth terms that are also not convex. These\nenvelopes are an extension of the Moreau ones but exhibit an additional\nsmoothness property that makes them amenable to fast optimization algorithms.\nLasry-Lions envelopes can also be seen as an \"intermediate\" between a given\nfunction and its convex envelope, and we make use of this property to develop a\nmethod that builds a sequence of approximate subproblems that are easier to\nsolve than the original problem. We discuss convergence properties of this\nmethod when used to address composite minimization problems; additionally,\nbased on a number of experiments, we discuss settings where it may be more\nuseful than classical alternatives in two domains: signal decoding and spectral\nunmixing.",
          "link": "http://arxiv.org/abs/2103.08533",
          "publishedOn": "2021-06-23T01:48:39.633Z",
          "wordCount": 646,
          "title": "Lasry-Lions Envelopes and Nonconvex Optimization: A Homotopy Approach. (arXiv:2103.08533v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangrun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangcong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>",
          "description": "Self-supervised learning (especially contrastive learning) has attracted\ngreat interest due to its tremendous potentials in learning discriminative\nrepresentations in an unsupervised manner. Despite the acknowledged successes,\nexisting contrastive learning methods suffer from very low learning efficiency,\ne.g., taking about ten times more training epochs than supervised learning for\ncomparable recognition accuracy. In this paper, we discover two contradictory\nphenomena in contrastive learning that we call under-clustering and\nover-clustering problems, which are major obstacles to learning efficiency.\nUnder-clustering means that the model cannot efficiently learn to discover the\ndissimilarity between inter-class samples when the negative sample pairs for\ncontrastive learning are insufficient to differentiate all the actual object\ncategories. Over-clustering implies that the model cannot efficiently learn the\nfeature representation from excessive negative sample pairs, which enforces the\nmodel to over-cluster samples of the same actual categories into different\nclusters. To simultaneously overcome these two problems, we propose a novel\nself-supervised learning framework using a median triplet loss. Precisely, we\nemploy a triplet loss tending to maximize the relative distance between the\npositive pair and negative pairs to address the under-clustering problem; and\nwe construct the negative pair by selecting the negative sample of a median\nsimilarity score from all negative samples to avoid the over-clustering\nproblem, guaranteed by the Bernoulli Distribution model. We extensively\nevaluate our proposed framework in several large-scale benchmarks (e.g.,\nImageNet, SYSU-30k, and COCO). The results demonstrate the superior performance\n(e.g., the learning efficiency) of our model over the latest state-of-the-art\nmethods by a clear margin. Codes available at:\nhttps://github.com/wanggrun/triplet.",
          "link": "http://arxiv.org/abs/2104.08760",
          "publishedOn": "2021-06-23T01:48:39.626Z",
          "wordCount": 739,
          "title": "Towards Solving Inefficiency of Self-supervised Representation Learning. (arXiv:2104.08760v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amor_B/0/1/0/all/0/1\">Boulbaba Ben Amor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xichan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>",
          "description": "Non-linear (large) time warping is a challenging source of nuisance in\ntime-series analysis. In this paper, we propose a novel diffeomorphic temporal\ntransformer network for both pairwise and joint time-series alignment. Our\nResNet-TW (Deep Residual Network for Time Warping) tackles the alignment\nproblem by compositing a flow of incremental diffeomorphic mappings. Governed\nby the flow equation, our Residual Network (ResNet) builds smooth, fluid and\nregular flows of velocity fields and consequently generates smooth and\ninvertible transformations (i.e. diffeomorphic warping functions). Inspired by\nthe elegant Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework,\nthe final transformation is built by the flow of time-dependent vector fields\nwhich are none other than the building blocks of our Residual Network. The\nlatter is naturally viewed as an Eulerian discretization schema of the flow\nequation (an ODE). Once trained, our ResNet-TW aligns unseen data by a single\ninexpensive forward pass. As we show in experiments on both univariate (84\ndatasets from UCR archive) and multivariate time-series (MSR Action-3D,\nFlorence-3D and MSR Daily Activity), ResNet-TW achieves competitive performance\nin joint alignment and classification.",
          "link": "http://arxiv.org/abs/2106.11911",
          "publishedOn": "2021-06-23T01:48:39.619Z",
          "wordCount": 625,
          "title": "Residual Networks as Flows of Velocity Fields for Diffeomorphic Time Series Alignment. (arXiv:2106.11911v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shusheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuxin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Bin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>",
          "description": "Recently, query based deep networks catch lots of attention owing to their\nend-to-end pipeline and competitive results on several fundamental computer\nvision tasks, such as object detection, semantic segmentation, and instance\nsegmentation. However, how to establish a query based video instance\nsegmentation (VIS) framework with elegant architecture and strong performance\nremains to be settled. In this paper, we present \\textbf{QueryTrack} (i.e.,\ntracking instances as queries), a unified query based VIS framework fully\nleveraging the intrinsic one-to-one correspondence between instances and\nqueries in QueryInst. The proposed method obtains 52.7 / 52.3 AP on\nYouTube-VIS-2019 / 2021 datasets, which wins the 2-nd place in the YouTube-VIS\nChallenge at CVPR 2021 \\textbf{with a single online end-to-end model, single\nscale testing \\& modest amount of training data}. We also provide\nQueryTrack-ResNet-50 baseline results on YouTube-VIS-2021 dataset as references\nfor the VIS community.",
          "link": "http://arxiv.org/abs/2106.11963",
          "publishedOn": "2021-06-23T01:48:39.612Z",
          "wordCount": 599,
          "title": "Tracking Instances as Queries. (arXiv:2106.11963v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kelu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>",
          "description": "Adversarial training is one of the most effective approaches to improve model\nrobustness against adversarial examples. However, previous works mainly focus\non the overall robustness of the model, and the in-depth analysis on the role\nof each class involved in adversarial training is still missing. In this paper,\nwe propose to analyze the class-wise robustness in adversarial training. First,\nwe provide a detailed diagnosis of adversarial training on six benchmark\ndatasets, i.e., MNIST, CIFAR-10, CIFAR-100, SVHN, STL-10 and ImageNet.\nSurprisingly, we find that there are remarkable robustness discrepancies among\nclasses, leading to unbalance/unfair class-wise robustness in the robust\nmodels. Furthermore, we keep investigating the relations between classes and\nfind that the unbalanced class-wise robustness is pretty consistent among\ndifferent attack and defense methods. Moreover, we observe that the stronger\nattack methods in adversarial learning achieve performance improvement mainly\nfrom a more successful attack on the vulnerable classes (i.e., classes with\nless robustness). Inspired by these interesting findings, we design a simple\nbut effective attack method based on the traditional PGD attack, named\nTemperature-PGD attack, which proposes to enlarge the robustness disparity\namong classes with a temperature factor on the confidence distribution of each\nimage. Experiments demonstrate our method can achieve a higher attack rate than\nthe PGD attack. Furthermore, from the defense perspective, we also make some\nmodifications in the training and inference phases to improve the robustness of\nthe most vulnerable class, so as to mitigate the large difference in class-wise\nrobustness. We believe our work can contribute to a more comprehensive\nunderstanding of adversarial training as well as rethinking the class-wise\nproperties in robust models.",
          "link": "http://arxiv.org/abs/2105.14240",
          "publishedOn": "2021-06-23T01:48:39.593Z",
          "wordCount": 736,
          "title": "Analysis and Applications of Class-wise Robustness in Adversarial Training. (arXiv:2105.14240v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazhar_O/0/1/0/all/0/1\">Osama Mazhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babuska_R/0/1/0/all/0/1\">Robert Babuska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kober_J/0/1/0/all/0/1\">Jens Kober</a>",
          "description": "Deep neural networks designed for vision tasks are often prone to failure\nwhen they encounter environmental conditions not covered by the training data.\nSingle-modal strategies are insufficient when the sensor fails to acquire\ninformation due to malfunction or its design limitations. Multi-sensor\nconfigurations are known to provide redundancy, increase reliability, and are\ncrucial in achieving robustness against asymmetric sensor failures. To address\nthe issue of changing lighting conditions and asymmetric sensor degradation in\nobject detection, we develop a multi-modal 2D object detector, and propose\ndeterministic and stochastic sensor-aware feature fusion strategies. The\nproposed fusion mechanisms are driven by the estimated sensor measurement\nreliability values/weights. Reliable object detection in harsh lighting\nconditions is essential for applications such as self-driving vehicles and\nhuman-robot interaction. We also propose a new \"r-blended\" hybrid depth\nmodality for RGB-D sensors. Through extensive experimentation, we show that the\nproposed strategies outperform the existing state-of-the-art methods on the\nFLIR-Thermal dataset, and obtain promising results on the SUNRGB-D dataset. We\nadditionally record a new RGB-Infra indoor dataset, namely L515-Indoors, and\ndemonstrate that the proposed object detection methodologies are highly\neffective for a variety of lighting conditions.",
          "link": "http://arxiv.org/abs/2102.12319",
          "publishedOn": "2021-06-23T01:48:39.584Z",
          "wordCount": 678,
          "title": "GEM: Glare or Gloom, I Can Still See You -- End-to-End Multimodal Object Detection. (arXiv:2102.12319v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1\">Hau Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jia-Hong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yao-Chih Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Ching-Hsien Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia-Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chu-Song Chen</a>",
          "description": "This paper introduces an approach for multi-human 3D pose estimation and\ntracking based on calibrated multi-view. The main challenge lies in finding the\ncross-view and temporal correspondences correctly even when several human pose\nestimations are noisy. Compare to previous solutions that construct 3D poses\nfrom multiple views, our approach takes advantage of temporal consistency to\nmatch the 2D poses estimated with previously constructed 3D skeletons in every\nview. Therefore cross-view and temporal associations are accomplished\nsimultaneously. Since the performance suffers from mistaken association and\nnoisy predictions, we design two strategies for aiming better correspondences\nand 3D reconstruction. Specifically, we propose a part-aware measurement for\n2D-3D association and a filter that can cope with 2D outliers during\nreconstruction. Our approach is efficient and effective comparing to\nstate-of-the-art methods; it achieves competitive results on two benchmarks:\n96.8% on Campus and 97.4% on Shelf. Moreover, we extends the length of Campus\nevaluation frames to be more challenging and our proposal also reach\nwell-performed result.",
          "link": "http://arxiv.org/abs/2106.11589",
          "publishedOn": "2021-06-23T01:48:39.577Z",
          "wordCount": 619,
          "title": "Part-Aware Measurement for Robust Multi-View Multi-Human 3D Pose Estimation and Tracking. (arXiv:2106.11589v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11769",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Haiyang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jihan Zhang</a>",
          "description": "Speech production is a dynamic procedure, which involved multi human organs\nincluding the tongue, jaw and lips. Modeling the dynamics of the vocal tract\ndeformation is a fundamental problem to understand the speech, which is the\nmost common way for human daily communication. Researchers employ several\nsensory streams to describe the process simultaneously, which are\nincontrovertibly statistically related to other streams. In this paper, we\naddress the following question: given an observable image sequences of lips,\ncan we picture the corresponding tongue motion. We formulated this problem as\nthe self-supervised learning problem, and employ the two-stream convolutional\nnetwork and long-short memory network for the learning task, with the attention\nmechanism. We evaluate the performance of the proposed method by leveraging the\nunlabeled lip videos to predict an upcoming ultrasound tongue image sequence.\nThe results show that our model is able to generate images that close to the\nreal ultrasound tongue images, and results in the matching between two imaging\nmodalities.",
          "link": "http://arxiv.org/abs/2106.11769",
          "publishedOn": "2021-06-23T01:48:39.570Z",
          "wordCount": 633,
          "title": "Improving Ultrasound Tongue Image Reconstruction from Lip Images Using Self-supervised Learning and Attention Mechanism. (arXiv:2106.11769v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11731",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Langner_T/0/1/0/all/0/1\">Taro Langner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mora_A/0/1/0/all/0/1\">Andr&#xe9;s Mart&#xed;nez Mora</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strand_R/0/1/0/all/0/1\">Robin Strand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahlstrom_H/0/1/0/all/0/1\">H&#xe5;kan Ahlstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kullberg_J/0/1/0/all/0/1\">Joel Kullberg</a>",
          "description": "UK Biobank (UKB) is conducting a large-scale study of more than half a\nmillion volunteers, collecting health-related information on genetics,\nlifestyle, blood biochemistry, and more. Medical imaging furthermore targets\n100,000 subjects, with 70,000 follow-up sessions, enabling measurements of\norgans, muscle, and body composition. With up to 170,000 mounting MR images,\nvarious methodologies are accordingly engaged in large-scale image analysis.\nThis work presents an experimental inference engine that can automatically\npredict a comprehensive profile of subject metadata from UKB neck-to-knee body\nMRI. In cross-validation, it accurately inferred baseline characteristics such\nas age, height, weight, and sex, but also emulated measurements of body\ncomposition by DXA, organ volumes, and abstract properties like grip strength,\npulse rate, and type 2 diabetic status (AUC: 0.866). The proposed system can\nautomatically analyze thousands of subjects within hours and provide individual\nconfidence intervals. The underlying methodology is based on convolutional\nneural networks for image-based mean-variance regression on two-dimensional\nrepresentations of the MRI data. This work aims to make the proposed system\navailable for free to researchers, who can use it to obtain fast and\nfully-automated estimates of 72 different measurements immediately upon release\nof new UK Biobank image data.",
          "link": "http://arxiv.org/abs/2106.11731",
          "publishedOn": "2021-06-23T01:48:39.561Z",
          "wordCount": 650,
          "title": "MIMIR: Deep Regression for Automated Analysis of UK Biobank Body MRI. (arXiv:2106.11731v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Wei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Spiking Neural Networks (SNNs) have been attached great importance due to\ntheir biological plausibility and high energy-efficiency on neuromorphic chips.\nAs these chips are usually resource-constrained, the compression of SNNs is\nthus crucial along the road of practical use of SNNs. Most existing methods\ndirectly apply pruning approaches in artificial neural networks (ANNs) to SNNs,\nwhich ignore the difference between ANNs and SNNs, thus limiting the\nperformance of the pruned SNNs. Besides, these methods are only suitable for\nshallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination\nin the neural system, we propose gradient rewiring (Grad R), a joint learning\nalgorithm of connectivity and weight for SNNs, that enables us to seamlessly\noptimize network structure without retraining. Our key innovation is to\nredefine the gradient to a new synaptic parameter, allowing better exploration\nof network structures by taking full advantage of the competition between\npruning and regrowth of connections. The experimental results show that the\nproposed method achieves minimal loss of SNNs' performance on MNIST and\nCIFAR-10 dataset so far. Moreover, it reaches a $\\sim$3.5% accuracy loss under\nunprecedented 0.73% connectivity, which reveals remarkable structure refining\ncapability in SNNs. Our work suggests that there exists extremely high\nredundancy in deep SNNs. Our codes are available at\nhttps://github.com/Yanqi-Chen/Gradient-Rewiring.",
          "link": "http://arxiv.org/abs/2105.04916",
          "publishedOn": "2021-06-23T01:48:39.554Z",
          "wordCount": 722,
          "title": "Pruning of Deep Spiking Neural Networks through Gradient Rewiring. (arXiv:2105.04916v3 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Mingfu Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yinghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yushu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiqiang Liu</a>",
          "description": "Recent researches show that deep learning model is susceptible to backdoor\nattacks. Many defenses against backdoor attacks have been proposed. However,\nexisting defense works require high computational overhead or backdoor attack\ninformation such as the trigger size, which is difficult to satisfy in\nrealistic scenarios. In this paper, a novel backdoor detection method based on\nadversarial examples is proposed. The proposed method leverages intentional\nadversarial perturbations to detect whether an image contains a trigger, which\ncan be applied in both the training stage and the inference stage (sanitize the\ntraining set in training stage and detect the backdoor instances in inference\nstage). Specifically, given an untrusted image, the adversarial perturbation is\nadded to the image intentionally. If the prediction of the model on the\nperturbed image is consistent with that on the unperturbed image, the input\nimage will be considered as a backdoor instance. Compared with most existing\ndefense works, the proposed adversarial perturbation based method requires low\ncomputational resources and maintains the visual quality of the images.\nExperimental results show that, the backdoor detection rate of the proposed\ndefense method is 99.63%, 99.76% and 99.91% on Fashion-MNIST, CIFAR-10 and\nGTSRB datasets, respectively. Besides, the proposed method maintains the visual\nquality of the image as the l2 norm of the added perturbation are as low as\n2.8715, 3.0513 and 2.4362 on Fashion-MNIST, CIFAR-10 and GTSRB datasets,\nrespectively. In addition, it is also demonstrated that the proposed method can\nachieve high defense performance against backdoor attacks under different\nattack settings (trigger transparency, trigger size and trigger pattern).\nCompared with the existing defense work (STRIP), the proposed method has better\ndetection performance on all the three datasets, and is more efficient than\nSTRIP.",
          "link": "http://arxiv.org/abs/2105.14259",
          "publishedOn": "2021-06-23T01:48:39.531Z",
          "wordCount": 746,
          "title": "Detecting Backdoor in Deep Neural Networks via Intentional Adversarial Perturbations. (arXiv:2105.14259v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tomas_H/0/1/0/all/0/1\">Henri Tomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_M/0/1/0/all/0/1\">Marcus Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dionido_R/0/1/0/all/0/1\">Raimarc Dionido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ty_M/0/1/0/all/0/1\">Mark Ty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirando_J/0/1/0/all/0/1\">Jonric Mirando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casimiro_J/0/1/0/all/0/1\">Joel Casimiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atienza_R/0/1/0/all/0/1\">Rowel Atienza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guinto_R/0/1/0/all/0/1\">Richard Guinto</a>",
          "description": "One of the most fundamental and information-laden actions humans do is to\nlook at objects. However, a survey of current works reveals that existing\ngaze-related datasets annotate only the pixel being looked at, and not the\nboundaries of a specific object of interest. This lack of object annotation\npresents an opportunity for further advancing gaze estimation research. To this\nend, we present a challenging new task called gaze object prediction, where the\ngoal is to predict a bounding box for a person's gazed-at object. To train and\nevaluate gaze networks on this task, we present the Gaze On Objects (GOO)\ndataset. GOO is composed of a large set of synthetic images (GOO Synth)\nsupplemented by a smaller subset of real images (GOO-Real) of people looking at\nobjects in a retail environment. Our work establishes extensive baselines on\nGOO by re-implementing and evaluating selected state-of-the art models on the\ntask of gaze following and domain adaptation. Code is available on github.",
          "link": "http://arxiv.org/abs/2105.10793",
          "publishedOn": "2021-06-23T01:48:39.522Z",
          "wordCount": 650,
          "title": "GOO: A Dataset for Gaze Object Prediction in Retail Environments. (arXiv:2105.10793v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Metzen_J/0/1/0/all/0/1\">Jan Hendrik Metzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finnie_N/0/1/0/all/0/1\">Nicole Finnie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutmacher_R/0/1/0/all/0/1\">Robin Hutmacher</a>",
          "description": "Recently demonstrated physical-world adversarial attacks have exposed\nvulnerabilities in perception systems that pose severe risks for\nsafety-critical applications such as autonomous driving. These attacks place\nadversarial artifacts in the physical world that indirectly cause the addition\nof a universal patch to inputs of a model that can fool it in a variety of\ncontexts. Adversarial training is the most effective defense against\nimage-dependent adversarial attacks. However, tailoring adversarial training to\nuniversal patches is computationally expensive since the optimal universal\npatch depends on the model weights which change during training. We propose\nmeta adversarial training (MAT), a novel combination of adversarial training\nwith meta-learning, which overcomes this challenge by meta-learning universal\npatches along with model training. MAT requires little extra computation while\ncontinuously adapting a large set of patches to the current model. MAT\nconsiderably increases robustness against universal patch attacks on image\nclassification and traffic-light detection.",
          "link": "http://arxiv.org/abs/2101.11453",
          "publishedOn": "2021-06-23T01:48:39.515Z",
          "wordCount": 634,
          "title": "Meta Adversarial Training against Universal Patches. (arXiv:2101.11453v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1\">Qi Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_T/0/1/0/all/0/1\">Taihe Yi</a>",
          "description": "Probabilistic Face Embeddings (PFE) can improve face recognition performance\nin unconstrained scenarios by integrating data uncertainty into the feature\nrepresentation. However, existing PFE methods tend to be over-confident in\nestimating uncertainty and is too slow to apply to large-scale face matching.\nThis paper proposes a regularized probabilistic face embedding method to\nimprove the robustness and speed of PFE. Specifically, the mutual likelihood\nscore (MLS) metric used in PFE is simplified to speedup the matching of face\nfeature pairs. Then, an output-constraint loss is proposed to penalize the\nvariance of the uncertainty output, which can regularize the output of the\nneural network. In addition, an identification preserving loss is proposed to\nimprove the discriminative of the MLS metric, and a multi-layer feature fusion\nmodule is proposed to improve the neural network's uncertainty estimation\nability. Comprehensive experiments show that the proposed method can achieve\ncomparable or better results in 9 benchmarks than the state-of-the-art methods,\nand can improve the performance of risk-controlled face recognition. The code\nof our work is publicly available in GitHub\n(https://github.com/KaenChan/ProbFace).",
          "link": "http://arxiv.org/abs/2102.04075",
          "publishedOn": "2021-06-23T01:48:39.508Z",
          "wordCount": 650,
          "title": "Fast and Reliable Probabilistic Face Embeddings in the Wild. (arXiv:2102.04075v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Engelmann_F/0/1/0/all/0/1\">Francis Engelmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rematas_K/0/1/0/all/0/1\">Konstantinos Rematas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1\">Bastian Leibe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>",
          "description": "We propose a method to detect and reconstruct multiple 3D objects from a\nsingle RGB image. The key idea is to optimize for detection, alignment and\nshape jointly over all objects in the RGB image, while focusing on realistic\nand physically plausible reconstructions. To this end, we propose a keypoint\ndetector that localizes objects as center points and directly predicts all\nobject properties, including 9-DoF bounding boxes and 3D shapes -- all in a\nsingle forward pass. The proposed method formulates 3D shape reconstruction as\na shape selection problem, i.e. it selects among exemplar shapes from a given\ndatabase. This makes it agnostic to shape representations, which enables a\nlightweight reconstruction of realistic and visually-pleasing shapes based on\nCAD-models, while the training objective is formulated around point clouds and\nvoxel representations. A collision-loss promotes non-intersecting objects,\nfurther increasing the reconstruction realism. Given the RGB image, the\npresented approach performs lightweight reconstruction in a single-stage, it is\nreal-time capable, fully differentiable and end-to-end trainable. Our\nexperiments compare multiple approaches for 9-DoF bounding box estimation,\nevaluate the novel shape-selection mechanism and compare to recent methods in\nterms of 3D bounding box estimation and 3D shape reconstruction quality.",
          "link": "http://arxiv.org/abs/2012.11575",
          "publishedOn": "2021-06-23T01:48:39.499Z",
          "wordCount": 672,
          "title": "From Points to Multi-Object 3D Reconstruction. (arXiv:2012.11575v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yixin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zihao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jiang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhongchao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jianping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiqiang He</a>",
          "description": "Medical imaging plays a pivotal role in diagnosis and treatment in clinical\npractice. Inspired by the significant progress in automatic image captioning,\nvarious deep learning (DL)-based architectures have been proposed for\ngenerating radiology reports for medical images. However, model uncertainty\n(i.e., model reliability/confidence on report generation) is still an\nunder-explored problem. In this paper, we propose a novel method to explicitly\nquantify both the visual uncertainty and the textual uncertainty for the task\nof radiology report generation. Such multi-modal uncertainties can sufficiently\ncapture the model confidence scores at both the report-level and the\nsentence-level, and thus they are further leveraged to weight the losses for\nachieving more comprehensive model optimization. Our experimental results have\ndemonstrated that our proposed method for model uncertainty characterization\nand estimation can provide more reliable confidence scores for radiology report\ngeneration, and our proposed uncertainty-weighted losses can achieve more\ncomprehensive model optimization and result in state-of-the-art performance on\na public radiology report dataset.",
          "link": "http://arxiv.org/abs/2106.10887",
          "publishedOn": "2021-06-23T01:48:39.477Z",
          "wordCount": 611,
          "title": "Confidence-Guided Radiology Report Generation. (arXiv:2106.10887v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02638",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "This paper investigates how to realize better and more efficient embedding\nlearning to tackle the semi-supervised video object segmentation under\nchallenging multi-object scenarios. The state-of-the-art methods learn to\ndecode features with a single positive object and thus have to match and\nsegment each target separately under multi-object scenarios, consuming multiple\ntimes computing resources. To solve the problem, we propose an Associating\nObjects with Transformers (AOT) approach to match and decode multiple objects\nuniformly. In detail, AOT employs an identification mechanism to associate\nmultiple targets into the same high-dimensional embedding space. Thus, we can\nsimultaneously process the matching and segmentation decoding of multiple\nobjects as efficiently as processing a single object. For sufficiently modeling\nmulti-object association, a Long Short-Term Transformer is designed for\nconstructing hierarchical matching and propagation. We conduct extensive\nexperiments on both multi-object and single-object benchmarks to examine AOT\nvariant networks with different complexities. Particularly, our AOT-L\noutperforms all the state-of-the-art competitors on three popular benchmarks,\ni.e., YouTube-VOS (83.7% J&F), DAVIS 2017 (83.0%), and DAVIS 2016 (91.0%),\nwhile keeping more than 3X faster multi-object run-time. Meanwhile, our AOT-T\ncan maintain real-time multi-object speed on the above benchmarks. We ranked\n1st in the 3rd Large-scale Video Object Segmentation Challenge. The code will\nbe publicly available at https://github.com/z-x-yang/AOT.",
          "link": "http://arxiv.org/abs/2106.02638",
          "publishedOn": "2021-06-23T01:48:39.469Z",
          "wordCount": 667,
          "title": "Associating Objects with Transformers for Video Object Segmentation. (arXiv:2106.02638v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1\">Ismail Elezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taixe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>",
          "description": "Deep neural networks have reached very high accuracy on object detection but\ntheir success hinges on large amounts of labeled data. To reduce the dependency\non labels, various active-learning strategies have been proposed, typically\nbased on the confidence of the detector. However, these methods are biased\ntowards best-performing classes and can lead to acquired datasets that are not\ngood representatives of the data in the testing set. In this work, we propose a\nunified framework for active learning, that considers both the uncertainty and\nthe robustness of the detector, ensuring that the network performs accurately\nin all classes. Furthermore, our method is able to pseudo-label the very\nconfident predictions, suppressing a potential distribution drift while further\nboosting the performance of the model. Experiments show that our method\ncomprehensively outperforms a wide range of active-learning methods on PASCAL\nVOC07+12 and MS-COCO, having up to a 7.7% relative improvement, or up to 82%\nreduction in labeling cost.",
          "link": "http://arxiv.org/abs/2106.11921",
          "publishedOn": "2021-06-23T01:48:39.461Z",
          "wordCount": 601,
          "title": "Towards Reducing Labeling Cost in Deep Object Detection. (arXiv:2106.11921v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.13028",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengxue Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_G/0/1/0/all/0/1\">Guangwei Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_H/0/1/0/all/0/1\">Huimin Lu</a>",
          "description": "Recently, the single image super-resolution (SISR) approaches with deep and\ncomplex convolutional neural network structures have achieved promising\nperformance. However, those methods improve the performance at the cost of\nhigher memory consumption, which is difficult to be applied for some mobile\ndevices with limited storage and computing resources. To solve this problem, we\npresent a lightweight multi-scale feature interaction network (MSFIN). For\nlightweight SISR, MSFIN expands the receptive field and adequately exploits the\ninformative features of the low-resolution observed images from various scales\nand interactive connections. In addition, we design a lightweight recurrent\nresidual channel attention block (RRCAB) so that the network can benefit from\nthe channel attention mechanism while being sufficiently lightweight. Extensive\nexperiments on some benchmarks have confirmed that our proposed MSFIN can\nachieve comparable performance against the state-of-the-arts with a more\nlightweight model.",
          "link": "http://arxiv.org/abs/2103.13028",
          "publishedOn": "2021-06-23T01:48:39.454Z",
          "wordCount": 607,
          "title": "Lightweight Image Super-Resolution with Multi-scale Feature Interaction Network. (arXiv:2103.13028v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1\">Zhiwu Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jianwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yuanjie Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1\">Nong Sang</a>",
          "description": "Weakly-Supervised Temporal Action Localization (WS-TAL) task aims to\nrecognize and localize temporal starts and ends of action instances in an\nuntrimmed video with only video-level label supervision. Due to lack of\nnegative samples of background category, it is difficult for the network to\nseparate foreground and background, resulting in poor detection performance. In\nthis report, we present our 2021 HACS Challenge - Weakly-supervised Learning\nTrack solution that based on BaSNet to address above problem. Specifically, we\nfirst adopt pre-trained CSN, Slowfast, TDN, and ViViT as feature extractors to\nget feature sequences. Then our proposed Local-Global Background Modeling\nNetwork (LGBM-Net) is trained to localize instances by using only video-level\nlabels based on Multi-Instance Learning (MIL). Finally, we ensemble multiple\nmodels to get the final detection results and reach 22.45% mAP on the test set",
          "link": "http://arxiv.org/abs/2106.11811",
          "publishedOn": "2021-06-23T01:48:39.447Z",
          "wordCount": 594,
          "title": "Weakly-Supervised Temporal Action Localization Through Local-Global Background Modeling. (arXiv:2106.11811v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03515",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bukchin_G/0/1/0/all/0/1\">Guy Bukchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_E/0/1/0/all/0/1\">Eli Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahar_O/0/1/0/all/0/1\">Ori Shahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1\">Leonid Karlinsky</a>",
          "description": "Few-shot learning methods offer pre-training techniques optimized for easier\nlater adaptation of the model to new classes (unseen during training) using one\nor a few examples. This adaptivity to unseen classes is especially important\nfor many practical applications where the pre-trained label space cannot remain\nfixed for effective use and the model needs to be \"specialized\" to support new\ncategories on the fly. One particularly interesting scenario, essentially\noverlooked by the few-shot literature, is Coarse-to-Fine Few-Shot (C2FS), where\nthe training classes (e.g. animals) are of much `coarser granularity' than the\ntarget (test) classes (e.g. breeds). A very practical example of C2FS is when\nthe target classes are sub-classes of the training classes. Intuitively, it is\nespecially challenging as (both regular and few-shot) supervised pre-training\ntends to learn to ignore intra-class variability which is essential for\nseparating sub-classes. In this paper, we introduce a novel 'Angular\nnormalization' module that allows to effectively combine supervised and\nself-supervised contrastive pre-training to approach the proposed C2FS task,\ndemonstrating significant gains in a broad study over multiple baselines and\ndatasets. We hope that this work will help to pave the way for future research\non this new, challenging, and very practical topic of C2FS classification.",
          "link": "http://arxiv.org/abs/2012.03515",
          "publishedOn": "2021-06-23T01:48:39.424Z",
          "wordCount": 678,
          "title": "Fine-grained Angular Contrastive Learning with Coarse Labels. (arXiv:2012.03515v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mohammad Arif Ul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Mahmudur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Widberg_J/0/1/0/all/0/1\">Jared Q Widberg</a>",
          "description": "With the advancement of deep neural networks and computer vision-based Human\nActivity Recognition, employment of Point-Cloud Data technologies (LiDAR,\nmmWave) has seen a lot interests due to its privacy preserving nature. Given\nthe high promise of accurate PCD technologies, we develop, PALMAR, a\nmultiple-inhabitant activity recognition system by employing efficient signal\nprocessing and novel machine learning techniques to track individual person\ntowards developing an adaptive multi-inhabitant tracking and HAR system. More\nspecifically, we propose (i) a voxelized feature representation-based real-time\nPCD fine-tuning method, (ii) efficient clustering (DBSCAN and BIRCH), Adaptive\nOrder Hidden Markov Model based multi-person tracking and crossover ambiguity\nreduction techniques and (iii) novel adaptive deep learning-based domain\nadaptation technique to improve the accuracy of HAR in presence of data\nscarcity and diversity (device, location and population diversity). We\nexperimentally evaluate our framework and systems using (i) a real-time PCD\ncollected by three devices (3D LiDAR and 79 GHz mmWave) from 6 participants,\n(ii) one publicly available 3D LiDAR activity data (28 participants) and (iii)\nan embedded hardware prototype system which provided promising HAR performances\nin multi-inhabitants (96%) scenario with a 63% improvement of multi-person\ntracking than state-of-art framework without losing significant system\nperformances in the edge computing device.",
          "link": "http://arxiv.org/abs/2106.11902",
          "publishedOn": "2021-06-23T01:48:39.416Z",
          "wordCount": 655,
          "title": "PALMAR: Towards Adaptive Multi-inhabitant Activity Recognition in Point-Cloud Technology. (arXiv:2106.11902v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1904.08475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arar_M/0/1/0/all/0/1\">Moab Arar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danon_D/0/1/0/all/0/1\">Dov Danon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1\">Ariel Shamir</a>",
          "description": "Traditional image resizing methods usually work in pixel space and use\nvarious saliency measures. The challenge is to adjust the image shape while\ntrying to preserve important content. In this paper we perform image resizing\nin feature space where the deep layers of a neural network contain rich\nimportant semantic information. We directly adjust the image feature maps,\nextracted from a pre-trained classification network, and reconstruct the\nresized image using a neural-network based optimization. This novel approach\nleverages the hierarchical encoding of the network, and in particular, the\nhigh-level discriminative power of its deeper layers, that recognizes semantic\nobjects and regions and allows maintaining their aspect ratio. Our use of\nreconstruction from deep features diminishes the artifacts introduced by\nimage-space resizing operators. We evaluate our method on benchmarks, compare\nto alternative approaches, and demonstrate its strength on challenging images.",
          "link": "http://arxiv.org/abs/1904.08475",
          "publishedOn": "2021-06-23T01:48:39.408Z",
          "wordCount": 607,
          "title": "Image Resizing by Reconstruction from Deep Features. (arXiv:1904.08475v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.01250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this work, we address the problem of 3D object detection from point cloud\ndata in real time. For autonomous vehicles to work, it is very important for\nthe perception component to detect the real world objects with both high\naccuracy and fast inference. We propose a novel neural network architecture\nalong with the training and optimization details for detecting 3D objects in\npoint cloud data. We compare the results with different backbone architectures\nincluding the standard ones like VGG, ResNet, Inception with our backbone. Also\nwe present the optimization and ablation studies including designing an\nefficient anchor. We use the Kitti 3D Birds Eye View dataset for benchmarking\nand validating our results. Our work surpasses the state of the art in this\ndomain both in terms of average precision and speed running at > 30 FPS. This\nmakes it a feasible option to be deployed in real time applications including\nself driving cars.",
          "link": "http://arxiv.org/abs/2006.01250",
          "publishedOn": "2021-06-23T01:48:39.400Z",
          "wordCount": 684,
          "title": "RUHSNet: 3D Object Detection Using Lidar Data in Real Time. (arXiv:2006.01250v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jianhua Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lanqing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chaoqiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>",
          "description": "Aiming at facilitating a real-world, ever-evolving and scalable autonomous\ndriving system, we present a large-scale benchmark for standardizing the\nevaluation of different self-supervised and semi-supervised approaches by\nlearning from raw data, which is the first and largest benchmark to date.\nExisting autonomous driving systems heavily rely on `perfect' visual perception\nmodels (e.g., detection) trained using extensive annotated data to ensure the\nsafety. However, it is unrealistic to elaborately label instances of all\nscenarios and circumstances (e.g., night, extreme weather, cities) when\ndeploying a robust autonomous driving system. Motivated by recent powerful\nadvances of self-supervised and semi-supervised learning, a promising direction\nis to learn a robust detection model by collaboratively exploiting large-scale\nunlabeled data and few labeled data. Existing dataset (e.g., KITTI, Waymo)\neither provides only a small amount of data or covers limited domains with full\nannotation, hindering the exploration of large-scale pre-trained models. Here,\nwe release a Large-Scale Object Detection benchmark for Autonomous driving,\nnamed as SODA10M, containing 10 million unlabeled images and 20K images labeled\nwith 6 representative object categories. To improve diversity, the images are\ncollected every ten seconds per frame within 32 different cities under\ndifferent weather conditions, periods and location scenes. We provide extensive\nexperiments and deep analyses of existing supervised state-of-the-art detection\nmodels, popular self-supervised and semi-supervised approaches, and some\ninsights about how to develop future models. The data and more up-to-date\ninformation have been released at https://soda-2d.github.io.",
          "link": "http://arxiv.org/abs/2106.11118",
          "publishedOn": "2021-06-23T01:48:39.391Z",
          "wordCount": 702,
          "title": "SODA10M: Towards Large-Scale Object Detection Benchmark for Autonomous Driving. (arXiv:2106.11118v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>",
          "description": "Confidence-aware learning is proven as an effective solution to prevent\nnetworks becoming overconfident. We present a confidence-aware camouflaged\nobject detection framework using dynamic supervision to produce both accurate\ncamouflage map and meaningful \"confidence\" representing model awareness about\nthe current prediction. A camouflaged object detection network is designed to\nproduce our camouflage prediction. Then, we concatenate it with the input image\nand feed it to the confidence estimation network to produce an one channel\nconfidence map.We generate dynamic supervision for the confidence estimation\nnetwork, representing the agreement of camouflage prediction with the ground\ntruth camouflage map. With the produced confidence map, we introduce\nconfidence-aware learning with the confidence map as guidance to pay more\nattention to the hard/low-confidence pixels in the loss function. We claim\nthat, once trained, our confidence estimation network can evaluate pixel-wise\naccuracy of the prediction without relying on the ground truth camouflage map.\nExtensive results on four camouflaged object detection testing datasets\nillustrate the superior performance of the proposed model in explaining the\ncamouflage prediction.",
          "link": "http://arxiv.org/abs/2106.11641",
          "publishedOn": "2021-06-23T01:48:39.371Z",
          "wordCount": 598,
          "title": "Confidence-Aware Learning for Camouflaged Object Detection. (arXiv:2106.11641v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newson_A/0/1/0/all/0/1\">Alasdair Newson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gousseau_Y/0/1/0/all/0/1\">Yann Gousseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellier_P/0/1/0/all/0/1\">Pierre Hellier</a>",
          "description": "High quality facial image editing is a challenging problem in the movie\npost-production industry, requiring a high degree of control and identity\npreservation. Previous works that attempt to tackle this problem may suffer\nfrom the entanglement of facial attributes and the loss of the person's\nidentity. Furthermore, many algorithms are limited to a certain task. To tackle\nthese limitations, we propose to edit facial attributes via the latent space of\na StyleGAN generator, by training a dedicated latent transformation network and\nincorporating explicit disentanglement and identity preservation terms in the\nloss function. We further introduce a pipeline to generalize our face editing\nto videos. Our model achieves a disentangled, controllable, and\nidentity-preserving facial attribute editing, even in the challenging case of\nreal (i.e., non-synthetic) images and videos. We conduct extensive experiments\non image and video datasets and show that our model outperforms other\nstate-of-the-art methods in visual quality and quantitative evaluation.",
          "link": "http://arxiv.org/abs/2106.11895",
          "publishedOn": "2021-06-23T01:48:39.364Z",
          "wordCount": 589,
          "title": "A Latent Transformer for Disentangled and Identity-Preserving Face Editing. (arXiv:2106.11895v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hechen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Ao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>",
          "description": "Nowadays, analysis of transparent images in the field of computer vision has\ngradually become a hot spot. In this paper, we compare the classification\nperformance of different deep learning for the problem that transparent images\nare difficult to analyze. We crop the transparent images into 8 * 8 and 224 *\n224 pixels patches in the same proportion, and then divide the two different\npixels patches into foreground and background according to groundtruch. We also\nuse 4 types of convolutional neural networks and a novel ViT network model to\ncompare the foreground and background classification experiments. We conclude\nthat ViT performs the worst in classifying 8 * 8 pixels patches, but it\noutperforms most convolutional neural networks in classifying 224 * 224.",
          "link": "http://arxiv.org/abs/2106.11582",
          "publishedOn": "2021-06-23T01:48:39.357Z",
          "wordCount": 584,
          "title": "A Comparison for Patch-level Classification of Deep Learning Methods on Transparent Images: from Convolutional Neural Networks to Visual Transformers. (arXiv:2106.11582v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1\">Deepthi Sreenivasaiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1\">Thomas Wollmann</a>",
          "description": "Image segmentation is a common and challenging task in autonomous driving.\nAvailability of sufficient pixel-level annotations for the training data is a\nhurdle. Active learning helps learning from small amounts of data by suggesting\nthe most promising samples for labeling. In this work, we propose a new\npool-based method for active learning, which proposes promising image regions,\nin each acquisition step. The problem is framed in an exploration-exploitation\nframework by combining an embedding based on Uniform Manifold Approximation to\nmodel representativeness with entropy as uncertainty measure to model\ninformativeness. We applied our proposed method to the challenging autonomous\ndriving data sets CamVid and Cityscapes and performed a quantitative comparison\nwith state-of-the-art methods. We find that our active learning method achieves\nbetter performance on CamVid compared to other methods, while on Cityscapes,\nthe performance lift was negligible.",
          "link": "http://arxiv.org/abs/2106.11858",
          "publishedOn": "2021-06-23T01:48:39.349Z",
          "wordCount": 565,
          "title": "MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_O/0/1/0/all/0/1\">Orchid Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1\">Alessandro Achille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1\">Marzia Polito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "Few-shot learning aims to transfer information from one task to enable\ngeneralization on novel tasks given a few examples. This information is present\nboth in the domain and the class labels. In this work we investigate the\ncomplementary roles of these two sources of information by combining\ninstance-discriminative contrastive learning and supervised learning in a\nsingle framework called Supervised Momentum Contrastive learning (SUPMOCO). Our\napproach avoids a problem observed in supervised learning where information in\nimages not relevant to the task is discarded, which hampers their\ngeneralization to novel tasks. We show that (self-supervised) contrastive\nlearning and supervised learning are mutually beneficial, leading to a new\nstate-of-the-art on the META-DATASET - a recently introduced benchmark for\nfew-shot learning. Our method is based on a simple modification of MOCO and\nscales better than prior work on combining supervised and self-supervised\nlearning. This allows us to easily combine data from multiple domains leading\nto further improvements.",
          "link": "http://arxiv.org/abs/2101.11058",
          "publishedOn": "2021-06-23T01:48:39.341Z",
          "wordCount": 634,
          "title": "Supervised Momentum Contrastive Learning for Few-Shot Classification. (arXiv:2101.11058v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.10399",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this work, we present a novel neural network to generate high resolution\nimages. We replace the decoder of VAE with a discriminator while using the\nencoder as it is. The encoder is fed data from a normal distribution while the\ngenerator is fed from a gaussian distribution. The combination from both is\ngiven to a discriminator which tells whether the generated image is correct or\nnot. We evaluate our network on 3 different datasets: MNIST, LSUN and CelebA\ndataset. Our network beats the previous state of the art using MMD, SSIM, log\nlikelihood, reconstruction error, ELBO and KL divergence as the evaluation\nmetrics while generating much sharper images. This work is potentially very\nexciting as we are able to combine the advantages of generative models and\ninference models in a principled bayesian manner.",
          "link": "http://arxiv.org/abs/2008.10399",
          "publishedOn": "2021-06-23T01:48:39.320Z",
          "wordCount": 622,
          "title": "Generate High Resolution Images With Generative Variational Autoencoder. (arXiv:2008.10399v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guillard_B/0/1/0/all/0/1\">Benoit Guillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remelli_E/0/1/0/all/0/1\">Edoardo Remelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukoianov_A/0/1/0/all/0/1\">Artem Lukoianov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richter_S/0/1/0/all/0/1\">Stephan Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baque_P/0/1/0/all/0/1\">Pierre Baque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>",
          "description": "Geometric Deep Learning has recently made striking progress with the advent\nof continuous Deep Implicit Fields. They allow for detailed modeling of\nwatertight surfaces of arbitrary topology while not relying on a 3D Euclidean\ngrid, resulting in a learnable parameterization that is unlimited in\nresolution. Unfortunately, these methods are often unsuitable for applications\nthat require an explicit mesh-based surface representation because converting\nan implicit field to such a representation relies on the Marching Cubes\nalgorithm, which cannot be differentiated with respect to the underlying\nimplicit field. In this work, we remove this limitation and introduce a\ndifferentiable way to produce explicit surface mesh representations from Deep\nImplicit Fields. Our key insight is that by reasoning on how implicit field\nperturbations impact local surface geometry, one can ultimately differentiate\nthe 3D location of surface samples with respect to the underlying deep implicit\nfield. We exploit this to define DeepMesh -- end-to-end differentiable mesh\nrepresentation that can vary its topology. We use two different applications to\nvalidate our theoretical insight: Single view 3D Reconstruction via\nDifferentiable Rendering and Physically-Driven Shape Optimization. In both\ncases our end-to-end differentiable parameterization gives us an edge over\nstate-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2106.11795",
          "publishedOn": "2021-06-23T01:48:39.313Z",
          "wordCount": 636,
          "title": "DeepMesh: Differentiable Iso-Surface Extraction. (arXiv:2106.11795v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.08940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shixing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W Mahoney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "Pruning is an effective method to reduce the memory footprint and FLOPs\nassociated with neural network models. However, existing structured-pruning\nmethods often result in significant accuracy degradation for moderate pruning\nlevels. To address this problem, we introduce a new Hessian Aware Pruning (HAP)\nmethod coupled with a Neural Implant approach that uses second-order\nsensitivity as a metric for structured pruning. The basic idea is to prune\ninsensitive components and to use a Neural Implant for moderately sensitive\ncomponents, instead of completely pruning them. For the latter approach, the\nmoderately sensitive components are replaced with with a low rank implant that\nis smaller and less computationally expensive than the original component. We\nuse the relative Hessian trace to measure sensitivity, as opposed to the\nmagnitude based sensitivity metric commonly used in the literature. We test HAP\nfor both computer vision tasks and natural language tasks, and we achieve new\nstate-of-the-art results. Specifically, HAP achieves less than $0.1\\%$/$0.5\\%$\ndegradation on PreResNet29/ResNet50 (CIFAR-10/ImageNet) with more than\n70\\%/50\\% of parameters pruned. Meanwhile, HAP also achieves significantly\nbetter performance (up to 0.8\\% with 60\\% of parameters pruned) as compared to\ngradient based method for head pruning on transformer-based models. The\nframework has been open sourced and available online.",
          "link": "http://arxiv.org/abs/2101.08940",
          "publishedOn": "2021-06-23T01:48:39.305Z",
          "wordCount": 682,
          "title": "Hessian-Aware Pruning and Optimal Neural Implant. (arXiv:2101.08940v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iyer_C/0/1/0/all/0/1\">C.V.Krishnakumar Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1\">Feili Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Henry Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yonghong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_K/0/1/0/all/0/1\">Kay Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Swetava Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_V/0/1/0/all/0/1\">Vipul Pandey</a>",
          "description": "We present a no-code Artificial Intelligence (AI) platform called Trinity\nwith the main design goal of enabling both machine learning researchers and\nnon-technical geospatial domain experts to experiment with domain-specific\nsignals and datasets for solving a variety of complex problems on their own.\nThis versatility to solve diverse problems is achieved by transforming complex\nSpatio-temporal datasets to make them consumable by standard deep learning\nmodels, in this case, Convolutional Neural Networks (CNNs), and giving the\nability to formulate disparate problems in a standard way, eg. semantic\nsegmentation. With an intuitive user interface, a feature store that hosts\nderivatives of complex feature engineering, a deep learning kernel, and a\nscalable data processing mechanism, Trinity provides a powerful platform for\ndomain experts to share the stage with scientists and engineers in solving\nbusiness-critical problems. It enables quick prototyping, rapid experimentation\nand reduces the time to production by standardizing model building and\ndeployment. In this paper, we present our motivation behind Trinity and its\ndesign along with showcasing sample applications to motivate the idea of\nlowering the bar to using AI.",
          "link": "http://arxiv.org/abs/2106.11756",
          "publishedOn": "2021-06-23T01:48:39.295Z",
          "wordCount": 635,
          "title": "Trinity: A No-Code AI platform for complex spatial datasets. (arXiv:2106.11756v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Quiros_L/0/1/0/all/0/1\">Lorenzo Quir&#xf3;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_E/0/1/0/all/0/1\">Enrique Vidal</a>",
          "description": "Automatically recognizing the layout of handwritten documents is an important\nstep towards useful extraction of information from those documents. The most\ncommon application is to feed downstream applications such as automatic text\nrecognition and keyword spotting; however, the recognition of the layout also\nhelps to establish relationships between elements in the document which allows\nto enrich the information that can be extracted. Most of the modern document\nlayout analysis systems are designed to address only one part of the document\nlayout problem, namely: baseline detection or region segmentation. In contrast,\nwe evaluate the effectiveness of the Mask-RCNN architecture to address the\nproblem of baseline detection and region segmentation in an integrated manner.\nWe present experimental results on two handwritten text datasets and one\nhandwritten music dataset. The analyzed architecture yields promising results,\noutperforming state-of-the-art techniques in all three datasets.",
          "link": "http://arxiv.org/abs/2106.11797",
          "publishedOn": "2021-06-23T01:48:39.287Z",
          "wordCount": 577,
          "title": "Evaluation of a Region Proposal Architecture for Multi-task Document Layout Analysis. (arXiv:2106.11797v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.07609",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Monakhova_K/0/1/0/all/0/1\">Kristina Monakhova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tran_V/0/1/0/all/0/1\">Vi Tran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_G/0/1/0/all/0/1\">Grace Kuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Waller_L/0/1/0/all/0/1\">Laura Waller</a>",
          "description": "Compressive lensless imagers enable novel applications in an extremely\ncompact device, requiring only a phase or amplitude mask placed close to the\nsensor. They have been demonstrated for 2D and 3D microscopy, single-shot\nvideo, and single-shot hyperspectral imaging; in each of these cases, a\ncompressive-sensing-based inverse problem is solved in order to recover a 3D\ndata-cube from a 2D measurement. Typically, this is accomplished using convex\noptimization and hand-picked priors. Alternatively, deep learning-based\nreconstruction methods offer the promise of better priors, but require many\nthousands of ground truth training pairs, which can be difficult or impossible\nto acquire. In this work, we propose the use of untrained networks for\ncompressive image recovery. Our approach does not require any labeled training\ndata, but instead uses the measurement itself to update the network weights. We\ndemonstrate our untrained approach on lensless compressive 2D imaging as well\nas single-shot high-speed video recovery using the camera's rolling shutter,\nand single-shot hyperspectral imaging. We provide simulation and experimental\nverification, showing that our method results in improved image quality over\nexisting methods.",
          "link": "http://arxiv.org/abs/2103.07609",
          "publishedOn": "2021-06-23T01:48:39.264Z",
          "wordCount": 655,
          "title": "Untrained networks for compressive lensless photography. (arXiv:2103.07609v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youshan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_B/0/1/0/all/0/1\">Brian D. Davison</a>",
          "description": "Domain adaptation aims to mitigate the domain gap when transferring knowledge\nfrom an existing labeled domain to a new domain. However, existing\ndisentanglement-based methods do not fully consider separation between\ndomain-invariant and domain-specific features, which means the domain-invariant\nfeatures are not discriminative. The reconstructed features are also not\nsufficiently used during training. In this paper, we propose a novel enhanced\nseparable disentanglement (ESD) model. We first employ a disentangler to\ndistill domain-invariant and domain-specific features. Then, we apply feature\nseparation enhancement processes to minimize contamination between\ndomain-invariant and domain-specific features. Finally, our model reconstructs\ncomplete feature vectors, which are used for further disentanglement during the\ntraining phase. Extensive experiments from three benchmark datasets outperform\nstate-of-the-art methods, especially on challenging cross-domain tasks.",
          "link": "http://arxiv.org/abs/2106.11915",
          "publishedOn": "2021-06-23T01:48:39.256Z",
          "wordCount": 554,
          "title": "Enhanced Separable Disentanglement for Unsupervised Domain Adaptation. (arXiv:2106.11915v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amor_B/0/1/0/all/0/1\">Boulbaba Ben Amor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xichan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>",
          "description": "Analyzing the structure of proteins is a key part of understanding their\nfunctions and thus their role in biology at the molecular level. In addition,\ndesign new proteins in a methodical way is a major engineering challenge. In\nthis work, we introduce a joint geometric-neural networks approach for\ncomparing, deforming and generating 3D protein structures. Viewing protein\nstructures as 3D open curves, we adopt the Square Root Velocity Function (SRVF)\nrepresentation and leverage its suitable geometric properties along with Deep\nResidual Networks (ResNets) for a joint registration and comparison. Our\nResNets handle better large protein deformations while being more\ncomputationally efficient. On top of the mathematical framework, we further\ndesign a Geometric Variational Auto-Encoder (G-VAE), that once trained, maps\noriginal, previously unseen structures, into a low-dimensional (latent)\nhyper-sphere. Motivated by the spherical structure of the pre-shape space, we\nnaturally adopt the von Mises-Fisher (vMF) distribution to model our hidden\nvariables. We test the effectiveness of our models by generating novel protein\nstructures and predicting completions of corrupted protein structures.\nExperimental results show that our method is able to generate plausible\nstructures, different from the structures in the training data.",
          "link": "http://arxiv.org/abs/2106.11920",
          "publishedOn": "2021-06-23T01:48:39.242Z",
          "wordCount": 630,
          "title": "G-VAE, a Geometric Convolutional VAE for ProteinStructure Generation. (arXiv:2106.11920v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11759",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mitra_V/0/1/0/all/0/1\">Vikramjit Mitra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zifang Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lea_C/0/1/0/all/0/1\">Colin Lea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tooley_L/0/1/0/all/0/1\">Lauren Tooley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1\">Sarah Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Botten_D/0/1/0/all/0/1\">Darren Botten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Palekar_A/0/1/0/all/0/1\">Ashwini Palekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thelapurath_S/0/1/0/all/0/1\">Shrinath Thelapurath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Georgiou_P/0/1/0/all/0/1\">Panayiotis Georgiou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kajarekar_S/0/1/0/all/0/1\">Sachin Kajarekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bigham_J/0/1/0/all/0/1\">Jefferey Bigham</a>",
          "description": "Dysfluencies and variations in speech pronunciation can severely degrade\nspeech recognition performance, and for many individuals with\nmoderate-to-severe speech disorders, voice operated systems do not work.\nCurrent speech recognition systems are trained primarily with data from fluent\nspeakers and as a consequence do not generalize well to speech with\ndysfluencies such as sound or word repetitions, sound prolongations, or audible\nblocks. The focus of this work is on quantitative analysis of a consumer speech\nrecognition system on individuals who stutter and production-oriented\napproaches for improving performance for common voice assistant tasks (i.e.,\n\"what is the weather?\"). At baseline, this system introduces a significant\nnumber of insertion and substitution errors resulting in intended speech Word\nError Rates (isWER) that are 13.64\\% worse (absolute) for individuals with\nfluency disorders. We show that by simply tuning the decoding parameters in an\nexisting hybrid speech recognition system one can improve isWER by 24\\%\n(relative) for individuals with fluency disorders. Tuning these parameters\ntranslates to 3.6\\% better domain recognition and 1.7\\% better intent\nrecognition relative to the default setup for the 18 study participants across\nall stuttering severities.",
          "link": "http://arxiv.org/abs/2106.11759",
          "publishedOn": "2021-06-23T01:48:39.232Z",
          "wordCount": 671,
          "title": "Analysis and Tuning of a Voice Assistant System for Dysfluent Speech. (arXiv:2106.11759v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2010.07092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ni_R/0/1/0/all/0/1\">Renkun Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharaf_A/0/1/0/all/0/1\">Amr Sharaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1\">Kezhi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "Conventional image classifiers are trained by randomly sampling mini-batches\nof images. To achieve state-of-the-art performance, practitioners use\nsophisticated data augmentation schemes to expand the amount of training data\navailable for sampling. In contrast, meta-learning algorithms sample support\ndata, query data, and tasks on each training step. In this complex sampling\nscenario, data augmentation can be used not only to expand the number of images\navailable per class, but also to generate entirely new classes/tasks. We\nsystematically dissect the meta-learning pipeline and investigate the distinct\nways in which data augmentation can be integrated at both the image and class\nlevels. Our proposed meta-specific data augmentation significantly improves the\nperformance of meta-learners on few-shot classification benchmarks.",
          "link": "http://arxiv.org/abs/2010.07092",
          "publishedOn": "2021-06-23T01:48:39.223Z",
          "wordCount": 583,
          "title": "Data Augmentation for Meta-Learning. (arXiv:2010.07092v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1\">Ayaan Haque</a>",
          "description": "Semi-supervised learning has been gaining attention as it allows for\nperforming image analysis tasks such as classification with limited labeled\ndata. Some popular algorithms using Generative Adversarial Networks (GANs) for\nsemi-supervised classification share a single architecture for classification\nand discrimination. However, this may require a model to converge to a separate\ndata distribution for each task, which may reduce overall performance. While\nprogress in semi-supervised learning has been made, less addressed are\nsmall-scale, fully-supervised tasks where even unlabeled data is unavailable\nand unattainable. We therefore, propose a novel GAN model namely External\nClassifier GAN (EC-GAN), that utilizes GANs and semi-supervised algorithms to\nimprove classification in fully-supervised regimes. Our method leverages a GAN\nto generate artificial data used to supplement supervised classification. More\nspecifically, we attach an external classifier, hence the name EC-GAN, to the\nGAN's generator, as opposed to sharing an architecture with the discriminator.\nOur experiments demonstrate that EC-GAN's performance is comparable to the\nshared architecture method, far superior to the standard data augmentation and\nregularization-based approach, and effective on a small, realistic dataset.",
          "link": "http://arxiv.org/abs/2012.15864",
          "publishedOn": "2021-06-23T01:48:39.202Z",
          "wordCount": 647,
          "title": "EC-GAN: Low-Sample Classification using Semi-Supervised Algorithms and GANs. (arXiv:2012.15864v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingye Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>",
          "description": "Chinese character recognition has attracted much research interest due to its\nwide applications. Although it has been studied for many years, some issues in\nthis field have not been completely resolved yet, e.g. the zero-shot problem.\nPrevious character-based and radical-based methods have not fundamentally\naddressed the zero-shot problem since some characters or radicals in test sets\nmay not appear in training sets under a data-hungry condition. Inspired by the\nfact that humans can generalize to know how to write characters unseen before\nif they have learned stroke orders of some characters, we propose a\nstroke-based method by decomposing each character into a sequence of strokes,\nwhich are the most basic units of Chinese characters. However, we observe that\nthere is a one-to-many relationship between stroke sequences and Chinese\ncharacters. To tackle this challenge, we employ a matching-based strategy to\ntransform the predicted stroke sequence to a specific character. We evaluate\nthe proposed method on handwritten characters, printed artistic characters, and\nscene characters. The experimental results validate that the proposed method\noutperforms existing methods on both character zero-shot and radical zero-shot\ntasks. Moreover, the proposed method can be easily generalized to other\nlanguages whose characters can be decomposed into strokes.",
          "link": "http://arxiv.org/abs/2106.11613",
          "publishedOn": "2021-06-23T01:48:39.190Z",
          "wordCount": 639,
          "title": "Zero-Shot Chinese Character Recognition with Stroke-Level Decomposition. (arXiv:2106.11613v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1901.11259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Yang_L/0/1/0/all/0/1\">Le Ou-Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hong Yan</a>",
          "description": "Deep hashing models have been proposed as an efficient method for large-scale\nsimilarity search. However, most existing deep hashing methods only utilize\nfine-level labels for training while ignoring the natural semantic hierarchy\nstructure. This paper presents an effective method that preserves the classwise\nsimilarity of full-level semantic hierarchy for large-scale image retrieval.\nExperiments on two benchmark datasets show that our method helps improve the\nfine-level retrieval performance. Moreover, with the help of the semantic\nhierarchy, it can produce significantly better binary codes for hierarchical\nretrieval, which indicates its potential of providing more user-desired\nretrieval results.",
          "link": "http://arxiv.org/abs/1901.11259",
          "publishedOn": "2021-06-23T01:48:39.176Z",
          "wordCount": 573,
          "title": "Semantic Hierarchy Preserving Deep Hashing for Large-scale Image Retrieval. (arXiv:1901.11259v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sipka_T/0/1/0/all/0/1\">Tomas Sipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulc_M/0/1/0/all/0/1\">Milan Sulc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>",
          "description": "In many computer vision classification tasks, class priors at test time often\ndiffer from priors on the training set. In the case of such prior shift,\nclassifiers must be adapted correspondingly to maintain close to optimal\nperformance. This paper analyzes methods for adaptation of probabilistic\nclassifiers to new priors and for estimating new priors on an unlabeled test\nset. We propose a novel method to address a known issue of prior estimation\nmethods based on confusion matrices, where inconsistent estimates of decision\nprobabilities and confusion matrices lead to negative values in the estimated\npriors. Experiments on fine-grained image classification datasets provide\ninsight into the best practice of prior shift estimation and classifier\nadaptation and show that the proposed method achieves state-of-the-art results\nin prior adaptation. Applying the best practice to two tasks with naturally\nimbalanced priors, learning from web-crawled images and plant species\nclassification, increased the recognition accuracy by 1.1% and 3.4%\nrespectively.",
          "link": "http://arxiv.org/abs/2106.11695",
          "publishedOn": "2021-06-23T01:48:39.170Z",
          "wordCount": 592,
          "title": "The Hitchhiker's Guide to Prior-Shift Adaptation. (arXiv:2106.11695v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guanlin_L/0/1/0/all/0/1\">Li Guanlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangwei_G/0/1/0/all/0/1\">Guo Shangwei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Run_W/0/1/0/all/0/1\">Wang Run</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guowen_X/0/1/0/all/0/1\">Xu Guowen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tianwei_Z/0/1/0/all/0/1\">Zhang Tianwei</a>",
          "description": "This paper presents a novel fingerprinting methodology for the Intellectual\nProperty protection of generative models. Prior solutions for discriminative\nmodels usually adopt adversarial examples as the fingerprints, which give\nanomalous inference behaviors and prediction results. Hence, these methods are\nnot stealthy and can be easily recognized by the adversary. Our approach\nleverages the invisible backdoor technique to overcome the above limitation.\nSpecifically, we design verification samples, whose model outputs look normal\nbut can trigger a backdoor classifier to make abnormal predictions. We propose\na new backdoor embedding approach with Unique-Triplet Loss and fine-grained\ncategorization to enhance the effectiveness of our fingerprints. Extensive\nevaluations show that this solution can outperform other strategies with higher\nrobustness, uniqueness and stealthiness for various GAN models.",
          "link": "http://arxiv.org/abs/2106.11760",
          "publishedOn": "2021-06-23T01:48:39.163Z",
          "wordCount": 571,
          "title": "A Stealthy and Robust Fingerprinting Scheme for Generative Models. (arXiv:2106.11760v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Huiwen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagano_K/0/1/0/all/0/1\">Koki Nagano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kung_H/0/1/0/all/0/1\">Han-Wei Kung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwhite_M/0/1/0/all/0/1\">Mclean Goldwhite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qingguo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zejian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Lingyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Liwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>",
          "description": "We introduce a highly robust GAN-based framework for digitizing a normalized\n3D avatar of a person from a single unconstrained photo. While the input image\ncan be of a smiling person or taken in extreme lighting conditions, our method\ncan reliably produce a high-quality textured model of a person's face in\nneutral expression and skin textures under diffuse lighting condition.\nCutting-edge 3D face reconstruction methods use non-linear morphable face\nmodels combined with GAN-based decoders to capture the likeness and details of\na person but fail to produce neutral head models with unshaded albedo textures\nwhich is critical for creating relightable and animation-friendly avatars for\nintegration in virtual environments. The key challenges for existing methods to\nwork is the lack of training and ground truth data containing normalized 3D\nfaces. We propose a two-stage approach to address this problem. First, we adopt\na highly robust normalized 3D face generator by embedding a non-linear\nmorphable face model into a StyleGAN2 network. This allows us to generate\ndetailed but normalized facial assets. This inference is then followed by a\nperceptual refinement step that uses the generated assets as regularization to\ncope with the limited available training samples of normalized faces. We\nfurther introduce a Normalized Face Dataset, which consists of a combination\nphotogrammetry scans, carefully selected photographs, and generated fake people\nwith neutral expressions in diffuse lighting conditions. While our prepared\ndataset contains two orders of magnitude less subjects than cutting edge\nGAN-based 3D facial reconstruction methods, we show that it is possible to\nproduce high-quality normalized face models for very challenging unconstrained\ninput images, and demonstrate superior performance to the current\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2106.11423",
          "publishedOn": "2021-06-23T01:48:39.155Z",
          "wordCount": 723,
          "title": "Normalized Avatar Synthesis Using StyleGAN and Perceptual Refinement. (arXiv:2106.11423v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1\">Holger Caesar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabzan_J/0/1/0/all/0/1\">Juraj Kabzan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kok Seang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fong_W/0/1/0/all/0/1\">Whye Kit Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolff_E/0/1/0/all/0/1\">Eric Wolff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_A/0/1/0/all/0/1\">Alex Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fletcher_L/0/1/0/all/0/1\">Luke Fletcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omari_S/0/1/0/all/0/1\">Sammy Omari</a>",
          "description": "In this work, we propose the world's first closed-loop ML-based planning\nbenchmark for autonomous driving. While there is a growing body of ML-based\nmotion planners, the lack of established datasets and metrics has limited the\nprogress in this area. Existing benchmarks for autonomous vehicle motion\nprediction have focused on short-term motion forecasting, rather than long-term\nplanning. This has led previous works to use open-loop evaluation with L2-based\nmetrics, which are not suitable for fairly evaluating long-term planning. Our\nbenchmark overcomes these limitations by introducing a large-scale driving\ndataset, lightweight closed-loop simulator, and motion-planning-specific\nmetrics. We provide a high-quality dataset with 1500h of human driving data\nfrom 4 cities across the US and Asia with widely varying traffic patterns\n(Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop\nsimulation framework with reactive agents and provide a large set of both\ngeneral and scenario-specific planning metrics. We plan to release the dataset\nat NeurIPS 2021 and organize benchmark challenges starting in early 2022.",
          "link": "http://arxiv.org/abs/2106.11810",
          "publishedOn": "2021-06-23T01:48:39.135Z",
          "wordCount": 619,
          "title": "nuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles. (arXiv:2106.11810v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liguo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Miaopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Congyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Juntao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinguo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Jinxiang Chai</a>",
          "description": "We introduce an approach that accurately reconstructs 3D human poses and\ndetailed 3D full-body geometric models from single images in realtime. The key\nidea of our approach is a novel end-to-end multi-task deep learning framework\nthat uses single images to predict five outputs simultaneously: foreground\nsegmentation mask, 2D joints positions, semantic body partitions, 3D part\norientations and uv coordinates (uv map). The multi-task network architecture\nnot only generates more visual cues for reconstruction, but also makes each\nindividual prediction more accurate. The CNN regressor is further combined with\nan optimization based algorithm for accurate kinematic pose reconstruction and\nfull-body shape modeling. We show that the realtime reconstruction reaches\naccurate fitting that has not been seen before, especially for wild images. We\ndemonstrate the results of our realtime 3D pose and human body reconstruction\nsystem on various challenging in-the-wild videos. We show the system advances\nthe frontier of 3D human body and pose reconstruction from single images by\nquantitative evaluations and comparisons with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.11536",
          "publishedOn": "2021-06-23T01:48:39.127Z",
          "wordCount": 614,
          "title": "Deep3DPose: Realtime Reconstruction of Arbitrarily Posed Human Bodies from Single RGB Images. (arXiv:2106.11536v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1\">Ghalib Tahir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1\">Chu Kiong Loo</a>",
          "description": "Dietary-related problems such as obesity are a growing concern in todays\nmodern world. If the current trend continues, it is most likely that the\nquality of life, in general, is significantly affected since obesity is\nassociated with other chronic diseases such as hypertension, irregular blood\nsugar levels, and increased risk of heart attacks. The primary cause of these\nproblems is poor lifestyle choices and unhealthy dietary habits, with emphasis\non a select few food groups such as sugars, fats, and carbohydrates. In this\nregard, computer-based food recognition offers automatic visual-based methods\nto assess dietary intake and help people make healthier choices. Thus, the\nfollowing paper presents a brief review of visual-based methods for food\nrecognition, including their accuracy, performance, and the use of popular food\ndatabases to evaluate existing models. The work further aims to highlight\nfuture challenges in this area. New high-quality studies for developing\nstandard benchmarks and using continual learning methods for food recognition\nare recommended.",
          "link": "http://arxiv.org/abs/2106.11776",
          "publishedOn": "2021-06-23T01:48:39.118Z",
          "wordCount": 599,
          "title": "A Review of the Vision-based Approaches for Dietary Assessment. (arXiv:2106.11776v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11558",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Singh_M/0/1/0/all/0/1\">Mohana Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rameshan_R/0/1/0/all/0/1\">Renu M. Rameshan</a>",
          "description": "Light field technology has increasingly attracted the attention of the\nresearch community with its many possible applications. The lenslet array in\ncommercial plenoptic cameras helps capture both the spatial and angular\ninformation of light rays in a single exposure. While the resulting high\ndimensionality of light field data enables its superior capabilities, it also\nimpedes its extensive adoption. Hence, there is a compelling need for efficient\ncompression of light field images. Existing solutions are commonly composed of\nseveral separate modules, some of which may not have been designed for the\nspecific structure and quality of light field data. This increases the\ncomplexity of the codec and results in impractical decoding runtimes. We\npropose a new learning-based, disparity-aided model for compression of 4D light\nfield images capable of parallel decoding. The model is end-to-end trainable,\neliminating the need for hand-tuning separate modules and allowing joint\nlearning of rate and distortion. The disparity-aided approach ensures the\nstructural integrity of the reconstructed light fields. Comparisons with the\nstate of the art show encouraging performance in terms of PSNR and MS-SSIM\nmetrics. Also, there is a notable gain in the encoding and decoding runtimes.\nSource code is available at https://moha23.github.io/LFDAAE.",
          "link": "http://arxiv.org/abs/2106.11558",
          "publishedOn": "2021-06-23T01:48:39.105Z",
          "wordCount": 651,
          "title": "Learning-Based Practical Light Field Image Compression Using A Disparity-Aware Model. (arXiv:2106.11558v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boris_C/0/1/0/all/0/1\">Chidlovskii Boris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadek_A/0/1/0/all/0/1\">Assem Sadek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1\">Christian Wolf</a>",
          "description": "We address the problem of universal domain adaptation (UDA) in ordinal\nregression (OR), which attempts to solve classification problems in which\nlabels are not independent, but follow a natural order. We show that the UDA\ntechniques developed for classification and based on the clustering assumption,\nunder-perform in OR settings. We propose a method that complements the OR\nclassifier with an auxiliary task of order learning, which plays the double\nrole of discriminating between common and private instances, and expanding\nclass labels to the private target images via ranking. Combined with\nadversarial domain discrimination, our model is able to address the closed set,\npartial and open set configurations. We evaluate our method on three face age\nestimation datasets, and show that it outperforms the baseline methods.",
          "link": "http://arxiv.org/abs/2106.11576",
          "publishedOn": "2021-06-23T01:48:39.087Z",
          "wordCount": 558,
          "title": "Universal Domain Adaptation in Ordinal Regression. (arXiv:2106.11576v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11559",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Rachala Rohith Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panicker_M/0/1/0/all/0/1\">Mahesh Raveendranatha Panicker</a>",
          "description": "With the recent developments in neural networks, there has been a resurgence\nin algorithms for the automatic generation of simulation ready electronic\ncircuits from hand-drawn circuits. However, most of the approaches in\nliterature were confined to classify different types of electrical components\nand only a few of those methods have shown a way to rebuild the circuit\nschematic from the scanned image, which is extremely important for further\nautomation of netlist generation. This paper proposes a real-time algorithm for\nthe automatic recognition of hand-drawn electrical circuits based on object\ndetection and circuit node recognition. The proposed approach employs You Only\nLook Once version 5 (YOLOv5) for detection of circuit components and a novel\nHough transform based approach for node recognition. Using YOLOv5 object\ndetection algorithm, a mean average precision (mAP0.5) of 98.2% is achieved in\ndetecting the components. The proposed method is also able to rebuild the\ncircuit schematic with 80% accuracy.",
          "link": "http://arxiv.org/abs/2106.11559",
          "publishedOn": "2021-06-23T01:48:39.079Z",
          "wordCount": 605,
          "title": "Hand-Drawn Electrical Circuit Recognition using Object Detection and Node Recognition. (arXiv:2106.11559v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiwen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1\">Hao Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Linchuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiao Zheng</a>",
          "description": "Multi-label image classification (MLIC) is a fundamental and practical task,\nwhich aims to assign multiple possible labels to an image. In recent years,\nmany deep convolutional neural network (CNN) based approaches have been\nproposed which model label correlations to discover semantics of labels and\nlearn semantic representations of images. This paper advances this research\ndirection by improving both the modeling of label correlations and the learning\nof semantic representations. On the one hand, besides the local semantics of\neach label, we propose to further explore global semantics shared by multiple\nlabels. On the other hand, existing approaches mainly learn the semantic\nrepresentations at the last convolutional layer of a CNN. But it has been noted\nthat the image representations of different layers of CNN capture different\nlevels or scales of features and have different discriminative abilities. We\nthus propose to learn semantic representations at multiple convolutional\nlayers. To this end, this paper designs a Multi-layered Semantic Representation\nNetwork (MSRN) which discovers both local and global semantics of labels\nthrough modeling label correlations and utilizes the label semantics to guide\nthe semantic representations learning at multiple layers through an attention\nmechanism. Extensive experiments on four benchmark datasets including VOC 2007,\nCOCO, NUS-WIDE, and Apparel show a competitive performance of the proposed MSRN\nagainst state-of-the-art models.",
          "link": "http://arxiv.org/abs/2106.11596",
          "publishedOn": "2021-06-23T01:48:39.072Z",
          "wordCount": 656,
          "title": "Multi-layered Semantic Representation Network for Multi-label Image Classification. (arXiv:2106.11596v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yutong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_N/0/1/0/all/0/1\">Nicholas Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">William Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Chenlin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burke_M/0/1/0/all/0/1\">Marshall Burke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobell_D/0/1/0/all/0/1\">David B. Lobell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>",
          "description": "High-resolution satellite imagery has proven useful for a broad range of\ntasks, including measurement of global human population, local economic\nlivelihoods, and biodiversity, among many others. Unfortunately,\nhigh-resolution imagery is both infrequently collected and expensive to\npurchase, making it hard to efficiently and effectively scale these downstream\ntasks over both time and space. We propose a new conditional pixel synthesis\nmodel that uses abundant, low-cost, low-resolution imagery to generate accurate\nhigh-resolution imagery at locations and times in which it is unavailable. We\nshow that our model attains photo-realistic sample quality and outperforms\ncompeting baselines on a key downstream task -- object counting -- particularly\nin geographic locations where conditions on the ground are changing rapidly.",
          "link": "http://arxiv.org/abs/2106.11485",
          "publishedOn": "2021-06-23T01:48:39.060Z",
          "wordCount": 565,
          "title": "Spatial-Temporal Super-Resolution of Satellite Imagery via Conditional Pixel Synthesis. (arXiv:2106.11485v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeongho Kim</a>",
          "description": "The author of this work proposes an overview of the recent semi-supervised\nlearning approaches and related works. Despite the remarkable success of neural\nnetworks in various applications, there exist few formidable constraints\nincluding the need for a large amount of labeled data. Therefore,\nsemi-supervised learning, which is a learning scheme in which the scarce labels\nand a larger amount of unlabeled data are utilized to train models (e.g., deep\nneural networks) is getting more important. Based on the key assumptions of\nsemi-supervised learning, which are the manifold assumption, cluster\nassumption, and continuity assumption, the work reviews the recent\nsemi-supervised learning approaches. In particular, the methods in regard to\nusing deep neural networks in a semi-supervised learning setting are primarily\ndiscussed. In addition, the existing works are first classified based on the\nunderlying idea and explained, and then the holistic approaches that unify the\naforementioned ideas are detailed.",
          "link": "http://arxiv.org/abs/2106.11528",
          "publishedOn": "2021-06-23T01:48:39.053Z",
          "wordCount": 582,
          "title": "Recent Deep Semi-supervised Learning Approaches and Related Works. (arXiv:2106.11528v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leo_J/0/1/0/all/0/1\">Justin Leo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1\">Jugal Kalita</a>",
          "description": "Most modern neural networks for classification fail to take into account the\nconcept of the unknown. Trained neural networks are usually tested in an\nunrealistic scenario with only examples from a closed set of known classes. In\nan attempt to develop a more realistic model, the concept of working in an open\nset environment has been introduced. This in turn leads to the concept of\nincremental learning where a model with its own architecture and initial\ntrained set of data can identify unknown classes during the testing phase and\nautonomously update itself if evidence of a new class is detected. Some\nproblems that arise in incremental learning are inefficient use of resources to\nretrain the classifier repeatedly and the decrease of classification accuracy\nas multiple classes are added over time. This process of instantiating new\nclasses is repeated as many times as necessary, accruing errors. To address\nthese problems, this paper proposes the Classification Confidence Threshold\napproach to prime neural networks for incremental learning to keep accuracies\nhigh by limiting forgetting. A lean method is also used to reduce resources\nused in the retraining of the neural network. The proposed method is based on\nthe idea that a network is able to incrementally learn a new class even when\nexposed to a limited number samples associated with the new class. This method\ncan be applied to most existing neural networks with minimal changes to network\narchitecture.",
          "link": "http://arxiv.org/abs/2106.11437",
          "publishedOn": "2021-06-23T01:48:39.046Z",
          "wordCount": 683,
          "title": "Incremental Deep Neural Network Learning using Classification Confidence Thresholding. (arXiv:2106.11437v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Doan_T/0/1/0/all/0/1\">Tung Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takasu_A/0/1/0/all/0/1\">Atsuhiro Takasu</a>",
          "description": "Kernel segmentation aims at partitioning a data sequence into several\nnon-overlapping segments that may have nonlinear and complex structures. In\ngeneral, it is formulated as a discrete optimization problem with combinatorial\nconstraints. A popular algorithm for optimally solving this problem is dynamic\nprogramming (DP), which has quadratic computation and memory requirements.\nGiven that sequences in practice are too long, this algorithm is not a\npractical approach. Although many heuristic algorithms have been proposed to\napproximate the optimal segmentation, they have no guarantee on the quality of\ntheir solutions. In this paper, we take a differentiable approach to alleviate\nthe aforementioned issues. First, we introduce a novel sigmoid-based\nregularization to smoothly approximate the combinatorial constraints. Combining\nit with objective of the balanced kernel clustering, we formulate a\ndifferentiable model termed Kernel clustering with sigmoid-based regularization\n(KCSR), where the gradient-based algorithm can be exploited to obtain the\noptimal segmentation. Second, we develop a stochastic variant of the proposed\nmodel. By using the stochastic gradient descent algorithm, which has much lower\ntime and space complexities, for optimization, the second model can perform\nsegmentation on overlong data sequences. Finally, for simultaneously segmenting\nmultiple data sequences, we slightly modify the sigmoid-based regularization to\nfurther introduce an extended variant of the proposed model. Through extensive\nexperiments on various types of data sequences performances of our models are\nevaluated and compared with those of the existing methods. The experimental\nresults validate advantages of the proposed models. Our Matlab source code is\navailable on github.",
          "link": "http://arxiv.org/abs/2106.11541",
          "publishedOn": "2021-06-23T01:48:39.039Z",
          "wordCount": 686,
          "title": "Kernel Clustering with Sigmoid-based Regularization for Efficient Segmentation of Sequential Data. (arXiv:2106.11541v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Steven Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>",
          "description": "With leveraging the weight-sharing and continuous relaxation to enable\ngradient-descent to alternately optimize the supernet weights and the\narchitecture parameters through a bi-level optimization paradigm,\n\\textit{Differentiable ARchiTecture Search} (DARTS) has become the mainstream\nmethod in Neural Architecture Search (NAS) due to its simplicity and\nefficiency. However, more recent works found that the performance of the\nsearched architecture barely increases with the optimization proceeding in\nDARTS. In addition, several concurrent works show that the NAS could find more\ncompetitive architectures without labels. The above observations reveal that\nthe supervision signal in DARTS may be a poor indicator for architecture\noptimization, inspiring a foundational question: instead of using the\nsupervision signal to perform bi-level optimization, \\textit{can we find\nhigh-quality architectures \\textbf{without any training nor labels}}? We\nprovide an affirmative answer by customizing the NAS as a network pruning at\ninitialization problem. By leveraging recent techniques on the network pruning\nat initialization, we designed a FreeFlow proxy to score the importance of\ncandidate operations in NAS without any training nor labels, and proposed a\nnovel framework called \\textit{training and label free neural architecture\nsearch} (\\textbf{FreeNAS}) accordingly. We show that, without any training nor\nlabels, FreeNAS with the proposed FreeFlow proxy can outperform most NAS\nbaselines. More importantly, our framework is extremely efficient, which\ncompletes the architecture search within only \\textbf{3.6s} and \\textbf{79s} on\na single GPU for the NAS-Bench-201 and DARTS search space, respectively. We\nhope our work inspires more attempts in solving NAS from the perspective of\npruning at initialization.",
          "link": "http://arxiv.org/abs/2106.11542",
          "publishedOn": "2021-06-23T01:48:39.016Z",
          "wordCount": 693,
          "title": "Differentiable Architecture Search Without Training Nor Labels: A Pruning Perspective. (arXiv:2106.11542v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungmin Cha. Beomyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We consider a class-incremental semantic segmentation (CISS) problem. While\nsome recently proposed algorithms utilized variants of knowledge distillation\n(KD) technique to tackle the problem, they only partially addressed the key\nadditional challenges in CISS that causes the catastrophic forgetting; i.e.,\nthe semantic drift of the background class and multi-label prediction issue. To\nbetter address these challenges, we propose a new method, dubbed as SSUL-M\n(Semantic Segmentation with Unknown Label with Memory), by carefully combining\nseveral techniques tailored for semantic segmentation. More specifically, we\nmake three main contributions; (1) modeling unknown class within the background\nclass to help learning future classes (help plasticity), (2) freezing backbone\nnetwork and past classifiers with binary cross-entropy loss and pseudo-labeling\nto overcome catastrophic forgetting (help stability), and (3) utilizing tiny\nexemplar memory for the first time in CISS to improve both plasticity and\nstability. As a result, we show our method achieves significantly better\nperformance than the recent state-of-the-art baselines on the standard\nbenchmark datasets. Furthermore, we justify our contributions with thorough and\nextensive ablation analyses and discuss different natures of the CISS problem\ncompared to the standard class-incremental learning for classification.",
          "link": "http://arxiv.org/abs/2106.11562",
          "publishedOn": "2021-06-23T01:48:38.963Z",
          "wordCount": 631,
          "title": "SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning. (arXiv:2106.11562v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nazaria_K/0/1/0/all/0/1\">Kobra Nazaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazaheri_S/0/1/0/all/0/1\">Samaneh Mazaheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigham_B/0/1/0/all/0/1\">Bahram Sadeghi Bigham</a>",
          "description": "Skin color detection is an essential required step in various applications\nrelated to computer vision. These applications will include face detection,\nfinding pornographic images in movies and photos, finding ethnicity, age,\ndiagnosis, and so on. Therefore, proposing a proper skin detection method can\nprovide solution to several problems. In this study, first a new color space is\ncreated using FCM and PSO algorithms. Then, skin classification has been\nperformed in the new color space utilizing linear and nonlinear modes.\nAdditionally, it has been done in RGB and LAB color spaces by using ANFIS and\nneural network. Skin detection in RBG color space has been performed using\nMahalanobis distance and Euclidean distance algorithms. In comparison, this\nmethod has 18.38% higher accuracy than the most accurate method on the same\ndatabase. Additionally, this method has achieved 90.05% in equal error rate\n(1-EER) in testing COMPAQ dataset and 92.93% accuracy in testing Pratheepan\ndataset, which compared to the previous method on COMPAQ database, 1-EER has\nincreased by %0.87.",
          "link": "http://arxiv.org/abs/2106.11563",
          "publishedOn": "2021-06-23T01:48:38.946Z",
          "wordCount": 621,
          "title": "Creating A New Color Space utilizing PSO and FCM to Perform Skin Detection by using Neural Network and ANFIS. (arXiv:2106.11563v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Appalaraju_S/0/1/0/all/0/1\">Srikar Appalaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jasani_B/0/1/0/all/0/1\">Bhavan Jasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kota_B/0/1/0/all/0/1\">Bhargava Urala Kota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yusheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1\">R. Manmatha</a>",
          "description": "We present DocFormer -- a multi-modal transformer based architecture for the\ntask of Visual Document Understanding (VDU). VDU is a challenging problem which\naims to understand documents in their varied formats (forms, receipts etc.) and\nlayouts. In addition, DocFormer is pre-trained in an unsupervised fashion using\ncarefully designed tasks which encourage multi-modal interaction. DocFormer\nuses text, vision and spatial features and combines them using a novel\nmulti-modal self-attention layer. DocFormer also shares learned spatial\nembeddings across modalities which makes it easy for the model to correlate\ntext to visual tokens and vice versa. DocFormer is evaluated on 4 different\ndatasets each with strong baselines. DocFormer achieves state-of-the-art\nresults on all of them, sometimes beating models 4x its size (in no. of\nparameters).",
          "link": "http://arxiv.org/abs/2106.11539",
          "publishedOn": "2021-06-23T01:48:38.940Z",
          "wordCount": 556,
          "title": "DocFormer: End-to-End Transformer for Document Understanding. (arXiv:2106.11539v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khac Chinh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniel_M/0/1/0/all/0/1\">Marc Daniel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meunier_J/0/1/0/all/0/1\">Jean Meunier</a>",
          "description": "Gait analysis is an important aspect of clinical investigation for detecting\nneurological and musculoskeletal disorders and assessing the global health of a\npatient. In this paper we propose to focus our attention on extracting relevant\ncurvature information from the body surface provided by a depth camera. We\nassumed that the 3D mesh was made available in a previous step and demonstrated\nhow curvature maps could be useful to assess asymmetric anomalies with two\nsimple simulated abnormal gaits compared with a normal one. This research set\nthe grounds for the future development of a curvature-based gait analysis\nsystem for healthcare professionals.",
          "link": "http://arxiv.org/abs/2106.11466",
          "publishedOn": "2021-06-23T01:48:38.933Z",
          "wordCount": 541,
          "title": "Gait analysis with curvature maps: A simulation study. (arXiv:2106.11466v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jingni Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianyun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yushi Zhu</a>",
          "description": "In Argoverse motion forecasting competition, the task is to predict the\nprobabilistic future trajectory distribution for the interested targets in the\ntraffic scene. We use vectorized lane map and 2 s targets' history trajectories\nas input. Then the model outputs 6 forecasted trajectories with probability for\neach target.",
          "link": "http://arxiv.org/abs/2106.11467",
          "publishedOn": "2021-06-23T01:48:38.925Z",
          "wordCount": 481,
          "title": "Multimodal trajectory forecasting based on discrete heat map. (arXiv:2106.11467v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Eslam Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1\">Ahmed El-Sallab</a>",
          "description": "Moving objects have special importance for Autonomous Driving tasks.\nDetecting moving objects can be posed as Moving Object Segmentation, by\nsegmenting the object pixels, or Moving Object Detection, by generating a\nbounding box for the moving targets. In this paper, we present a Multi-Task\nLearning architecture, based on Transformers, to jointly perform both tasks\nthrough one network. Due to the importance of the motion features to the task,\nthe whole setup is based on a Spatio-Temporal aggregation. We evaluate the\nperformance of the individual tasks architecture versus the MTL setup, both\nwith early shared encoders, and late shared encoder-decoder transformers. For\nthe latter, we present a novel joint tasks query decoder transformer, that\nenables us to have tasks dedicated heads out of the shared model. To evaluate\nour approach, we use the KITTI MOD [29] data set. Results show1.5% mAP\nimprovement for Moving Object Detection, and 2%IoU improvement for Moving\nObject Segmentation, over the individual tasks networks.",
          "link": "http://arxiv.org/abs/2106.11401",
          "publishedOn": "2021-06-23T01:48:38.895Z",
          "wordCount": 599,
          "title": "Spatio-Temporal Multi-Task Learning Transformer for Joint Moving Object Detection and Segmentation. (arXiv:2106.11401v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Amol S. Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1\">Ali Dabouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "While working with fingerprint images acquired from crime scenes, mobile\ncameras, or low-quality sensors, it becomes difficult for automated\nidentification systems to verify the identity due to image blur and distortion.\nWe propose a fingerprint deblurring model FDeblur-GAN, based on the conditional\nGenerative Adversarial Networks (cGANs) and multi-stage framework of the stack\nGAN. Additionally, we integrate two auxiliary sub-networks into the model for\nthe deblurring task. The first sub-network is a ridge extractor model. It is\nadded to generate ridge maps to ensure that fingerprint information and\nminutiae are preserved in the deblurring process and prevent the model from\ngenerating erroneous minutiae. The second sub-network is a verifier that helps\nthe generator to preserve the ID information during the generation process.\nUsing a database of blurred fingerprints and corresponding ridge maps, the deep\nnetwork learns to deblur from the input blurry samples. We evaluate the\nproposed method in combination with two different fingerprint matching\nalgorithms. We achieved an accuracy of 95.18% on our fingerprint database for\nthe task of matching deblurred and ground truth fingerprints.",
          "link": "http://arxiv.org/abs/2106.11354",
          "publishedOn": "2021-06-23T01:48:38.887Z",
          "wordCount": 618,
          "title": "FDeblur-GAN: Fingerprint Deblurring using Generative Adversarial Network. (arXiv:2106.11354v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mattos_A/0/1/0/all/0/1\">Agatha C. H. de Mattos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McArdle_G/0/1/0/all/0/1\">Gavin McArdle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertolotto_M/0/1/0/all/0/1\">Michela Bertolotto</a>",
          "description": "The UN-Habitat estimates that over one billion people live in slums around\nthe world. However, state-of-the-art techniques to detect the location of slum\nareas employ high-resolution satellite imagery, which is costly to obtain and\nprocess. As a result, researchers have started to look at utilising free and\nopen-access medium resolution satellite imagery. Yet, there is no clear\nconsensus on which data preparation and machine learning approaches are the\nmost appropriate to use with such imagery data. In this paper, we evaluate two\ntechniques (multi-spectral data and grey-level co-occurrence matrix feature\nextraction) on an open-access dataset consisting of labelled Sentinel-2 images\nwith a spatial resolution of 10 meters. Both techniques were paired with a\ncanonical correlation forests classifier. The results show that the grey-level\nco-occurrence matrix performed better than multi-spectral data for all four\ncities. It had an average accuracy for the slum class of 97% and a mean\nintersection over union of 94%, while multi-spectral data had 75% and 64% for\nthe respective metrics. These results indicate that open-access satellite\nimagery with a resolution of at least 10 meters may be suitable for keeping\ntrack of development goals such as the detection of slums in cities.",
          "link": "http://arxiv.org/abs/2106.11395",
          "publishedOn": "2021-06-23T01:48:38.880Z",
          "wordCount": 673,
          "title": "Mapping Slums with Medium Resolution Satellite Imagery: a Comparative Analysis of Multi-Spectral Data and Grey-level Co-occurrence Matrix Techniques. (arXiv:2106.11395v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11486",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong Hoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1\">Sae-Young Chung</a>",
          "description": "We propose unsupervised embedding adaptation for the downstream few-shot\nclassification task. Based on findings that deep neural networks learn to\ngeneralize before memorizing, we develop Early-Stage Feature Reconstruction\n(ESFR) -- a novel adaptation scheme with feature reconstruction and\ndimensionality-driven early stopping that finds generalizable features.\nIncorporating ESFR consistently improves the performance of baseline methods on\nall standard settings, including the recently proposed transductive method.\nESFR used in conjunction with the transductive method further achieves\nstate-of-the-art performance on mini-ImageNet, tiered-ImageNet, and CUB;\nespecially with 1.2%~2.0% improvements in accuracy over the previous best\nperforming method on 1-shot setting.",
          "link": "http://arxiv.org/abs/2106.11486",
          "publishedOn": "2021-06-23T01:48:38.872Z",
          "wordCount": 539,
          "title": "Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction for Few-Shot Classification. (arXiv:2106.11486v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Acuna_D/0/1/0/all/0/1\">David Acuna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guojun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1\">Marc T. Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>",
          "description": "Unsupervised domain adaptation is used in many machine learning applications\nwhere, during training, a model has access to unlabeled data in the target\ndomain, and a related labeled dataset. In this paper, we introduce a novel and\ngeneral domain-adversarial framework. Specifically, we derive a novel\ngeneralization bound for domain adaptation that exploits a new measure of\ndiscrepancy between distributions based on a variational characterization of\nf-divergences. It recovers the theoretical results from Ben-David et al.\n(2010a) as a special case and supports divergences used in practice. Based on\nthis bound, we derive a new algorithmic framework that introduces a key\ncorrection in the original adversarial training method of Ganin et al. (2016).\nWe show that many regularizers and ad-hoc objectives introduced over the last\nyears in this framework are then not required to achieve performance comparable\nto (if not better than) state-of-the-art domain-adversarial methods.\nExperimental analysis conducted on real-world natural language and computer\nvision datasets show that our framework outperforms existing baselines, and\nobtains the best results for f-divergences that were not considered previously\nin domain-adversarial learning.",
          "link": "http://arxiv.org/abs/2106.11344",
          "publishedOn": "2021-06-23T01:48:38.801Z",
          "wordCount": 616,
          "title": "f-Domain-Adversarial Learning: Theory and Algorithms. (arXiv:2106.11344v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>",
          "description": "This open-source book represents our attempt to make deep learning\napproachable, teaching readers the concepts, the context, and the code. The\nentire book is drafted in Jupyter notebooks, seamlessly integrating exposition\nfigures, math, and interactive examples with self-contained code. Our goal is\nto offer a resource that could (i) be freely available for everyone; (ii) offer\nsufficient technical depth to provide a starting point on the path to actually\nbecoming an applied machine learning scientist; (iii) include runnable code,\nshowing readers how to solve problems in practice; (iv) allow for rapid\nupdates, both by us and also by the community at large; (v) be complemented by\na forum for interactive discussion of technical details and to answer\nquestions.",
          "link": "http://arxiv.org/abs/2106.11342",
          "publishedOn": "2021-06-23T01:48:38.772Z",
          "wordCount": 565,
          "title": "Dive into Deep Learning. (arXiv:2106.11342v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_F/0/1/0/all/0/1\">Freddie Bickford Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roads_B/0/1/0/all/0/1\">Brett D Roads</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiaoliang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Love_B/0/1/0/all/0/1\">Bradley C Love</a>",
          "description": "Top-down attention allows neural networks, both artificial and biological, to\nfocus on the information most relevant for a given task. This is known to\nenhance performance in visual perception. But it remains unclear how attention\nbrings about its perceptual boost, especially when it comes to naturalistic\nsettings like recognising an object in an everyday scene. What aspects of a\nvisual task does attention help to deal with? We aim to answer this with a\ncomputational experiment based on a general framework called task-oriented\nablation design. First we define a broad range of visual tasks and identify six\nfactors that underlie task variability. Then on each task we compare the\nperformance of two neural networks, one with top-down attention and one\nwithout. These comparisons reveal the task-dependence of attention's perceptual\nboost, giving a clearer idea of the role attention plays. Whereas many existing\ncognitive accounts link attention to stimulus-level variables, such as visual\nclutter and object scale, we find greater explanatory power in system-level\nvariables that capture the interaction between the model, the distribution of\ntraining data and the task format. This finding suggests a shift in how\nattention is studied could be fruitful. We make publicly available our code and\nresults, along with statistics relevant to ImageNet-based experiments beyond\nthis one. Our contribution serves to support the development of more human-like\nvision models and the design of more informative machine-learning experiments.",
          "link": "http://arxiv.org/abs/2106.11339",
          "publishedOn": "2021-06-23T01:48:38.755Z",
          "wordCount": 672,
          "title": "Understanding top-down attention using task-oriented ablation design. (arXiv:2106.11339v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>",
          "description": "Place Recognition is a crucial capability for mobile robot localization and\nnavigation. Image-based or Visual Place Recognition (VPR) is a challenging\nproblem as scene appearance and camera viewpoint can change significantly when\nplaces are revisited. Recent VPR methods based on ``sequential\nrepresentations'' have shown promising results as compared to traditional\nsequence score aggregation or single image based techniques. In parallel to\nthese endeavors, 3D point clouds based place recognition is also being explored\nfollowing the advances in deep learning based point cloud processing. However,\na key question remains: is an explicit 3D structure based place representation\nalways superior to an implicit ``spatial'' representation based on sequence of\nRGB images which can inherently learn scene structure. In this extended\nabstract, we attempt to compare these two types of methods by considering a\nsimilar ``metric span'' to represent places. We compare a 3D point cloud based\nmethod (PointNetVLAD) with image sequence based methods (SeqNet and others) and\nshowcase that image sequence based techniques approach, and can even surpass,\nthe performance achieved by point cloud based methods for a given metric span.\nThese performance variations can be attributed to differences in data richness\nof input sensors as well as data accumulation strategies for a mobile robot.\nWhile a perfect apple-to-apple comparison may not be feasible for these two\ndifferent modalities, the presented comparison takes a step in the direction of\nanswering deeper questions regarding spatial representations, relevant to\nseveral applications like Autonomous Driving and Augmented/Virtual Reality.\nSource code available publicly https://github.com/oravus/seqNet.",
          "link": "http://arxiv.org/abs/2106.11481",
          "publishedOn": "2021-06-23T01:48:38.724Z",
          "wordCount": 722,
          "title": "SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for Day-Night Place Recognition. (arXiv:2106.11481v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11447",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Silva_J/0/1/0/all/0/1\">Jo&#xe3;o Louren&#xe7;o Silva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menezes_M/0/1/0/all/0/1\">Miguel Nobre Menezes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodrigues_T/0/1/0/all/0/1\">Tiago Rodrigues</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Silva_B/0/1/0/all/0/1\">Beatriz Silva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pinto_F/0/1/0/all/0/1\">Fausto J. Pinto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_A/0/1/0/all/0/1\">Arlindo L. Oliveira</a>",
          "description": "Coronary X-ray angiography is a crucial clinical procedure for the diagnosis\nand treatment of coronary artery disease, which accounts for roughly 16% of\nglobal deaths every year. However, the images acquired in these procedures have\nlow resolution and poor contrast, making lesion detection and assessment\nchallenging. Accurate coronary artery segmentation not only helps mitigate\nthese problems, but also allows the extraction of relevant anatomical features\nfor further analysis by quantitative methods. Although automated segmentation\nof coronary arteries has been proposed before, previous approaches have used\nnon-optimal segmentation criteria, leading to less useful results. Most methods\neither segment only the major vessel, discarding important information from the\nremaining ones, or segment the whole coronary tree based mostly on contrast\ninformation, producing a noisy output that includes vessels that are not\nrelevant for diagnosis. We adopt a better-suited clinical criterion and segment\nvessels according to their clinical relevance. Additionally, we simultaneously\nperform catheter segmentation, which may be useful for diagnosis due to the\nscale factor provided by the catheter's known diameter, and is a task that has\nnot yet been performed with good results. To derive the optimal approach, we\nconducted an extensive comparative study of encoder-decoder architectures\ntrained on a combination of focal loss and a variant of generalized dice loss.\nBased on the EfficientNet and the UNet++ architectures, we propose a line of\nefficient and high-performance segmentation models using a new decoder\narchitecture, the EfficientUNet++, whose best-performing version achieved\naverage dice scores of 0.8904 and 0.7526 for the artery and catheter classes,\nrespectively, and an average generalized dice score of 0.9234.",
          "link": "http://arxiv.org/abs/2106.11447",
          "publishedOn": "2021-06-23T01:48:38.709Z",
          "wordCount": 719,
          "title": "Encoder-Decoder Architectures for Clinically Relevant Coronary Artery Segmentation. (arXiv:2106.11447v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Eslam Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1\">Ahmad El-Sallab</a>",
          "description": "Moving Object Detection (MOD) is a crucial task for the Autonomous Driving\npipeline. MOD is usually handled via 2-stream convolutional architectures that\nincorporates both appearance and motion cues, without considering the\ninter-relations between the spatial or motion features. In this paper, we\ntackle this problem through multi-head attention mechanisms, both across the\nspatial and motion streams. We propose MODETR; a Moving Object DEtection\nTRansformer network, comprised of multi-stream transformer encoders for both\nspatial and motion modalities, and an object transformer decoder that produces\nthe moving objects bounding boxes using set predictions. The whole architecture\nis trained end-to-end using bi-partite loss. Several methods of incorporating\nmotion cues with the Transformer model are explored, including two-stream RGB\nand Optical Flow (OF) methods, and multi-stream architectures that take\nadvantage of sequence information. To incorporate the temporal information, we\npropose a new Temporal Positional Encoding (TPE) approach to extend the Spatial\nPositional Encoding(SPE) in DETR. We explore two architectural choices for\nthat, balancing between speed and time. To evaluate the our network, we perform\nthe MOD task on the KITTI MOD [6] data set. Results show significant 5% mAP of\nthe Transformer network for MOD over the state-of-the art methods. Moreover,\nthe proposed TPE encoding provides 10% mAP improvement over the SPE baseline.",
          "link": "http://arxiv.org/abs/2106.11422",
          "publishedOn": "2021-06-23T01:48:38.699Z",
          "wordCount": 662,
          "title": "MODETR: Moving Object Detection with Transformers. (arXiv:2106.11422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hyolim Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyungmin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seon Joo Kim</a>",
          "description": "Generic Event Boundary Detection (GEBD) is a newly introduced task that aims\nto detect \"general\" event boundaries that correspond to natural human\nperception. In this paper, we introduce a novel contrastive learning based\napproach to deal with the GEBD. Our intuition is that the feature similarity of\nthe video snippet would significantly vary near the event boundaries, while\nremaining relatively the same in the remaining part of the video. In our model,\nTemporal Self-similarity Matrix (TSM) is utilized as an intermediate\nrepresentation which takes on a role as an information bottleneck. With our\nmodel, we achieved significant performance boost compared to the given\nbaselines. Our code is available at\nhttps://github.com/hello-jinwoo/LOVEU-CVPR2021.",
          "link": "http://arxiv.org/abs/2106.11549",
          "publishedOn": "2021-06-23T01:48:38.671Z",
          "wordCount": 551,
          "title": "Winning the CVPR'2021 Kinetics-GEBD Challenge: Contrastive Learning Approach. (arXiv:2106.11549v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_D/0/1/0/all/0/1\">Daniel V. Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todt_E/0/1/0/all/0/1\">Eduardo Todt</a>",
          "description": "With the rise of automation, unmanned vehicles became a hot topic both as\ncommercial products and as a scientific research topic. It composes a\nmulti-disciplinary field of robotics that encompasses embedded systems, control\ntheory, path planning, Simultaneous Localization and Mapping (SLAM), scene\nreconstruction, and pattern recognition. In this work, we present our\nexploratory research of how sensor data fusion and state-of-the-art machine\nlearning algorithms can perform the Embodied Artificial Intelligence (E-AI)\ntask called Visual Semantic Navigation. This task, a.k.a Object-Goal Navigation\n(ObjectNav) consists of autonomous navigation using egocentric visual\nobservations to reach an object belonging to the target semantic class without\nprior knowledge of the environment. Our method reached fourth place on the\nHabitat Challenge 2021 ObjectNav on the Minival phase and the Test-Standard\nPhase.",
          "link": "http://arxiv.org/abs/2106.11379",
          "publishedOn": "2021-06-23T01:48:38.662Z",
          "wordCount": 568,
          "title": "BEyond observation: an approach for ObjectNav. (arXiv:2106.11379v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singhal_T/0/1/0/all/0/1\">Trisha Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blessing_L/0/1/0/all/0/1\">Lucienne T. M. Blessing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">Kwan Hui Lim</a>",
          "description": "The advent of social media platforms has been a catalyst for the development\nof digital photography that engendered a boom in vision applications. With this\nmotivation, we introduce a large-scale dataset termed 'Photozilla', which\nincludes over 990k images belonging to 10 different photographic styles. The\ndataset is then used to train 3 classification models to automatically classify\nthe images into the relevant style which resulted in an accuracy of ~96%. With\nthe rapid evolution of digital photography, we have seen new types of\nphotography styles emerging at an exponential rate. On that account, we present\na novel Siamese-based network that uses the trained classification models as\nthe base architecture to adapt and classify unseen styles with only 25 training\nsamples. We report an accuracy of over 68% for identifying 10 other distinct\ntypes of photography styles. This dataset can be found at\nhttps://trisha025.github.io/Photozilla/",
          "link": "http://arxiv.org/abs/2106.11359",
          "publishedOn": "2021-06-23T01:48:38.651Z",
          "wordCount": 617,
          "title": "Photozilla: A Large-Scale Photography Dataset and Visual Embedding for 20 Photography Styles. (arXiv:2106.11359v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11396",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Bilevel optimization recently has attracted increased interest in machine\nlearning due to its many applications such as hyper-parameter optimization and\npolicy optimization. Although some methods recently have been proposed to solve\nthe bilevel problems, these methods do not consider using adaptive learning\nrates. To fill this gap, in the paper, we propose a class of fast and effective\nadaptive methods for solving bilevel optimization problems that the outer\nproblem is possibly nonconvex and the inner problem is strongly-convex.\nSpecifically, we propose a fast single-loop BiAdam algorithm based on the basic\nmomentum technique, which achieves a sample complexity of\n$\\tilde{O}(\\epsilon^{-4})$ for finding an $\\epsilon$-stationary point. At the\nsame time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by\nusing variance reduced technique, which reaches the best known sample\ncomplexity of $\\tilde{O}(\\epsilon^{-3})$. To further reduce computation in\nestimating derivatives, we propose a fast single-loop stochastic approximated\nBiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still\nachieves a sample complexity of $\\tilde{O}(\\epsilon^{-4})$ without large\nbatches. We further present an accelerated version of saBiAdam algorithm\n(VR-saBiAdam), which also reaches the best known sample complexity of\n$\\tilde{O}(\\epsilon^{-3})$. We apply the unified adaptive matrices to our\nmethods as the SUPER-ADAM \\citep{huang2021super}, which including many types of\nadaptive learning rates. Moreover, our framework can flexibly use the momentum\nand variance reduced techniques. In particular, we provide a useful convergence\nanalysis framework for both the constrained and unconstrained bilevel\noptimization. To the best of our knowledge, we first study the adaptive bilevel\noptimization methods with adaptive learning rates.",
          "link": "http://arxiv.org/abs/2106.11396",
          "publishedOn": "2021-06-23T01:48:38.628Z",
          "wordCount": 692,
          "title": "BiAdam: Fast Adaptive Bilevel Optimization Methods. (arXiv:2106.11396v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bu_X/0/1/0/all/0/1\">Xingyuan Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Junran Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxiang Zhang</a>",
          "description": "Transfer learning with pre-training on large-scale datasets has played an\nincreasingly significant role in computer vision and natural language\nprocessing recently. However, as there exist numerous application scenarios\nthat have distinctive demands such as certain latency constraints and\nspecialized data distributions, it is prohibitively expensive to take advantage\nof large-scale pre-training for per-task requirements. In this paper, we focus\non the area of object detection and present a transfer learning system named\nGAIA, which could automatically and efficiently give birth to customized\nsolutions according to heterogeneous downstream needs. GAIA is capable of\nproviding powerful pre-trained weights, selecting models that conform to\ndownstream demands such as latency constraints and specified data domains, and\ncollecting relevant data for practitioners who have very few datapoints for\ntheir tasks. With GAIA, we achieve promising results on COCO, Objects365, Open\nImages, Caltech, CityPersons, and UODB which is a collection of datasets\nincluding KITTI, VOC, WiderFace, DOTA, Clipart, Comic, and more. Taking COCO as\nan example, GAIA is able to efficiently produce models covering a wide range of\nlatency from 16ms to 53ms, and yields AP from 38.2 to 46.5 without whistles and\nbells. To benefit every practitioner in the community of object detection, GAIA\nis released at https://github.com/GAIA-vision.",
          "link": "http://arxiv.org/abs/2106.11346",
          "publishedOn": "2021-06-23T01:48:38.606Z",
          "wordCount": 666,
          "title": "GAIA: A Transfer Learning System of Object Detection that Fits Your Needs. (arXiv:2106.11346v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11322",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Lebreton_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;my Lebreton</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Brochard_R/0/1/0/all/0/1\">Roland Brochard</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Baudry_M/0/1/0/all/0/1\">Matthieu Baudry</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Jonniaux_G/0/1/0/all/0/1\">Gr&#xe9;gory Jonniaux</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Salah_A/0/1/0/all/0/1\">Adrien Hadj Salah</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kanani_K/0/1/0/all/0/1\">Keyvan Kanani</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Goff_M/0/1/0/all/0/1\">Matthieu Le Goff</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Masson_A/0/1/0/all/0/1\">Aurore Masson</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Ollagnier_N/0/1/0/all/0/1\">Nicolas Ollagnier</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Panicucci_P/0/1/0/all/0/1\">Paolo Panicucci</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Proag_A/0/1/0/all/0/1\">Amsha Proag</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Robin_C/0/1/0/all/0/1\">Cyril Robin</a>",
          "description": "Image Processing algorithms for vision-based navigation require reliable\nimage simulation capacities. In this paper we explain why traditional rendering\nengines may present limitations that are potentially critical for space\napplications. We introduce Airbus SurRender software v7 and provide details on\nfeatures that make it a very powerful space image simulator. We show how\nSurRender is at the heart of the development processes of our computer vision\nsolutions and we provide a series of illustrations of rendered images for\nvarious use cases ranging from Moon and Solar System exploration, to in orbit\nrendezvous and planetary robotics.",
          "link": "http://arxiv.org/abs/2106.11322",
          "publishedOn": "2021-06-23T01:48:38.550Z",
          "wordCount": 586,
          "title": "Image simulation for space applications with the SurRender software. (arXiv:2106.11322v1 [astro-ph.EP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11330",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Liping Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_S/0/1/0/all/0/1\">Simon Chun-Ho Yu</a>",
          "description": "Accurate liver and lesion segmentation from computed tomography (CT) images\nare highly demanded in clinical practice for assisting the diagnosis and\nassessment of hepatic tumor disease. However, automatic liver and lesion\nsegmentation from contrast-enhanced CT volumes is extremely challenging due to\nthe diversity in contrast, resolution, and quality of images. Previous methods\nbased on UNet for 2D slice-by-slice or 3D volume-by-volume segmentation either\nlack sufficient spatial contexts or suffer from high GPU computational cost,\nwhich limits the performance. To tackle these issues, we propose a novel\ncontext-aware PolyUNet for accurate liver and lesion segmentation. It jointly\nexplores structural diversity and consecutive t-adjacent slices to enrich\nfeature expressive power and spatial contextual information while avoiding the\noverload of GPU memory consumption. In addition, we utilize zoom out/in and\ntwo-stage refinement strategy to exclude the irrelevant contexts and focus on\nthe specific region for the fine-grained segmentation. Our method achieved very\ncompetitive performance at the MICCAI 2017 Liver Tumor Segmentation (LiTS)\nChallenge among all tasks with a single model and ranked the $3^{rd}$,\n$12^{th}$, $2^{nd}$, and $5^{th}$ places in the liver segmentation, lesion\nsegmentation, lesion detection, and tumor burden estimation, respectively.",
          "link": "http://arxiv.org/abs/2106.11330",
          "publishedOn": "2021-06-23T01:48:38.542Z",
          "wordCount": 643,
          "title": "Context-aware PolyUNet for Liver and Lesion Segmentation from Abdominal CT Images. (arXiv:2106.11330v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zejiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lucy Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>",
          "description": "Classifying the core textual components of a scientific paper-title, author,\nbody text, etc.-is a critical first step in automated scientific document\nunderstanding. Previous work has shown how using elementary layout information,\ni.e., each token's 2D position on the page, leads to more accurate\nclassification. We introduce new methods for incorporating VIsual LAyout (VILA)\nstructures, e.g., the grouping of page texts into text lines or text blocks,\ninto language models to further improve performance. We show that the I-VILA\napproach, which simply adds special tokens denoting the boundaries of layout\nstructures into model inputs, can lead to 1.9% Macro F1 improvements for token\nclassification. Moreover, we design a hierarchical model, H-VILA, that encodes\nthe text based on layout structures and record an up-to 47% inference time\nreduction with less than 1.5% Macro F1 loss for the text classification models.\nExperiments are conducted on a newly curated evaluation suite, S2-VLUE, with a\nnovel metric measuring classification uniformity within visual groups and a new\ndataset of gold annotations covering papers from 19 scientific disciplines.\nPre-trained weights, benchmark datasets, and source code will be available at\nhttps://github.com/allenai/VILA.",
          "link": "http://arxiv.org/abs/2106.00676",
          "publishedOn": "2021-06-22T01:57:13.921Z",
          "wordCount": 653,
          "title": "Incorporating Visual Layout Structures for Scientific Text Classification. (arXiv:2106.00676v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>",
          "description": "Assessing an AI agent that can converse in human language and understand\nvisual content is challenging. Generation metrics, such as BLEU scores favor\ncorrect syntax over semantics. Hence a discriminative approach is often used,\nwhere an agent ranks a set of candidate options. The mean reciprocal rank (MRR)\nmetric evaluates the model performance by taking into account the rank of a\nsingle human-derived answer. This approach, however, raises a new challenge:\nthe ambiguity and synonymy of answers, for instance, semantic equivalence\n(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative\ngain (NDCG) metric has been used to capture the relevance of all the correct\nanswers via dense annotations. However, the NDCG metric favors the usually\napplicable uncertain answers such as `I don't know. Crafting a model that\nexcels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should\nanswer a human-like reply and validate the correctness of any answer. To\naddress this issue, we describe a two-step non-parametric ranking approach that\ncan merge strong MRR and NDCG models. Using our approach, we manage to keep\nmost MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG\nstate-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won\nthe recent Visual Dialog 2020 challenge. Source code is available at\nhttps://github.com/idansc/mrr-ndcg.",
          "link": "http://arxiv.org/abs/2104.07511",
          "publishedOn": "2021-06-22T01:57:13.906Z",
          "wordCount": 688,
          "title": "Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07524",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kollias_D/0/1/0/all/0/1\">Dimitrios Kollias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arsenos_A/0/1/0/all/0/1\">Anastasios Arsenos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soukissian_L/0/1/0/all/0/1\">Levon Soukissian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kollias_S/0/1/0/all/0/1\">Stefanos Kollias</a>",
          "description": "Early and reliable COVID-19 diagnosis based on chest 3-D CT scans can assist\nmedical specialists in vital circumstances. Deep learning methodologies\nconstitute a main approach for chest CT scan analysis and disease prediction.\nHowever, large annotated databases are necessary for developing deep learning\nmodels that are able to provide COVID-19 diagnosis across various medical\nenvironments in different countries. Due to privacy issues, publicly available\nCOVID-19 CT datasets are highly difficult to obtain, which hinders the research\nand development of AI-enabled diagnosis methods of COVID-19 based on CT scans.\nIn this paper we present the COV19-CT-DB database which is annotated for\nCOVID-19, consisting of about 5,000 3-D CT scans, We have split the database in\ntraining, validation and test datasets. The former two datasets can be used for\ntraining and validation of machine learning models, while the latter will be\nused for evaluation of the developed models. We also present a deep learning\napproach, based on a CNN-RNN network and report its performance on the\nCOVID19-CT-DB database.",
          "link": "http://arxiv.org/abs/2106.07524",
          "publishedOn": "2021-06-22T01:57:13.425Z",
          "wordCount": 678,
          "title": "MIA-COV19D: COVID-19 Detection through 3-D Chest CT Image Analysis. (arXiv:2106.07524v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1\">Fang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingjia Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhiliang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhenjun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>",
          "description": "Weakly supervised object localization (WSOL) is a challenging problem when\ngiven image category labels but requires to learn object localization models.\nOptimizing a convolutional neural network (CNN) for classification tends to\nactivate local discriminative regions while ignoring complete object extent,\ncausing the partial activation issue. In this paper, we argue that partial\nactivation is caused by the intrinsic characteristics of CNN, where the\nconvolution operations produce local receptive fields and experience difficulty\nto capture long-range feature dependency among pixels. We introduce the token\nsemantic coupled attention map (TS-CAM) to take full advantage of the\nself-attention mechanism in visual transformer for long-range dependency\nextraction. TS-CAM first splits an image into a sequence of patch tokens for\nspatial embedding, which produce attention maps of long-range visual dependency\nto avoid partial activation. TS-CAM then re-allocates category-related\nsemantics for patch tokens, enabling each of them to be aware of object\ncategories. TS-CAM finally couples the patch tokens with the semantic-agnostic\nattention map to achieve semantic-aware localization. Experiments on the\nILSVRC/CUB-200-2011 datasets show that TS-CAM outperforms its CNN-CAM\ncounterparts by 7.1%/27.1% for WSOL, achieving state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2103.14862",
          "publishedOn": "2021-06-22T01:57:13.231Z",
          "wordCount": 689,
          "title": "TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised Object Localization. (arXiv:2103.14862v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1\">Masaya Ueda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimura_A/0/1/0/all/0/1\">Akisato Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>",
          "description": "Various fonts give different impressions, such as legible, rough, and\ncomic-text.This paper aims to analyze the correlation between the local shapes,\nor parts, and the impression of fonts. By focusing on local shapes instead of\nthe whole letter shape, we can realize letter-shape independent and more\ngeneral analysis. The analysis is performed by newly combining SIFT and\nDeepSets, to extract an arbitrary number of essential parts from a particular\nfont and aggregate them to infer the font impressions by nonlinear regression.\nOur qualitative and quantitative analyses prove that (1)fonts with similar\nparts have similar impressions, (2)many impressions, such as legible and rough,\nlargely depend on specific parts, (3)several impressions are very irrelevant to\nparts.",
          "link": "http://arxiv.org/abs/2103.14216",
          "publishedOn": "2021-06-22T01:57:11.731Z",
          "wordCount": 586,
          "title": "Which Parts Determine the Impression of the Font?. (arXiv:2103.14216v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1903.10143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhiwen Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cham_T/0/1/0/all/0/1\">Tat-Jen Cham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>",
          "description": "Facial action unit (AU) detection in the wild is a challenging problem, due\nto the unconstrained variability in facial appearances and the lack of accurate\nannotations. Most existing methods depend on either impractical labor-intensive\nlabeling or inaccurate pseudo labels. In this paper, we propose an end-to-end\nunconstrained facial AU detection framework based on domain adaptation, which\ntransfers accurate AU labels from a constrained source domain to an\nunconstrained target domain by exploiting labels of AU-related facial\nlandmarks. Specifically, we map a source image with label and a target image\nwithout label into a latent feature domain by combining source landmark-related\nfeature with target landmark-free feature. Due to the combination of source\nAU-related information and target AU-free information, the latent feature\ndomain with transferred source label can be learned by maximizing the\ntarget-domain AU detection performance. Moreover, we introduce a novel landmark\nadversarial loss to disentangle the landmark-free feature from the\nlandmark-related feature by treating the adversarial learning as a multi-player\nminimax game. Our framework can also be naturally extended for use with\ntarget-domain pseudo AU labels. Extensive experiments show that our method\nsoundly outperforms lower-bounds and upper-bounds of the basic model, as well\nas state-of-the-art approaches on the challenging in-the-wild benchmarks. The\ncode is available at https://github.com/ZhiwenShao/ADLD.",
          "link": "http://arxiv.org/abs/1903.10143",
          "publishedOn": "2021-06-22T01:57:11.214Z",
          "wordCount": 711,
          "title": "Unconstrained Facial Action Unit Detection via Latent Feature Domain. (arXiv:1903.10143v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guizhong Liu</a>",
          "description": "Metric learning is a widely used method for few shot learning in which the\nquality of prototypes plays a key role in the algorithm. In this paper we\npropose the trainable prototypes for distance measure instead of the artificial\nones within the meta-training and task-training framework. Also to avoid the\ndisadvantages that the episodic meta-training brought, we adopt non-episodic\nmeta-training based on self-supervised learning. Overall we solve the few-shot\ntasks in two phases: meta-training a transferable feature extractor via\nself-supervised learning and training the prototypes for metric classification.\nIn addition, the simple attention mechanism is used in both meta-training and\ntask-training. Our method achieves state-of-the-art performance in a variety of\nestablished few-shot tasks on the standard few-shot visual classification\ndataset, with about 20% increase compared to the available unsupervised\nfew-shot learning methods.",
          "link": "http://arxiv.org/abs/2106.10846",
          "publishedOn": "2021-06-22T01:57:11.209Z",
          "wordCount": 582,
          "title": "Trainable Class Prototypes for Few-Shot Learning. (arXiv:2106.10846v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.04884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parmar_P/0/1/0/all/0/1\">Paritosh Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_J/0/1/0/all/0/1\">Jaiden Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_B/0/1/0/all/0/1\">Brendan Morris</a>",
          "description": "Can a computer determine a piano player's skill level? Is it preferable to\nbase this assessment on visual analysis of the player's performance or should\nwe trust our ears over our eyes? Since current CNNs have difficulty processing\nlong video videos, how can shorter clips be sampled to best reflect the players\nskill level? In this work, we collect and release a first-of-its-kind dataset\nfor multimodal skill assessment focusing on assessing piano player's skill\nlevel, answer the asked questions, initiate work in automated evaluation of\npiano playing skills and provide baselines for future work. Dataset is\navailable from: https://github.com/ParitoshParmar/Piano-Skills-Assessment.",
          "link": "http://arxiv.org/abs/2101.04884",
          "publishedOn": "2021-06-22T01:57:11.204Z",
          "wordCount": 576,
          "title": "Piano Skills Assessment. (arXiv:2101.04884v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1\">Aditya Golatkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1\">Alessandro Achille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1\">Marzia Polito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We show that the influence of a subset of the training samples can be removed\n-- or \"forgotten\" -- from the weights of a network trained on large-scale image\nclassification tasks, and we provide strong computable bounds on the amount of\nremaining information after forgetting. Inspired by real-world applications of\nforgetting techniques, we introduce a novel notion of forgetting in\nmixed-privacy setting, where we know that a \"core\" subset of the training\nsamples does not need to be forgotten. While this variation of the problem is\nconceptually simple, we show that working in this setting significantly\nimproves the accuracy and guarantees of forgetting methods applied to vision\nclassification tasks. Moreover, our method allows efficient removal of all\ninformation contained in non-core data by simply setting to zero a subset of\nthe weights with minimal loss in performance. We achieve these results by\nreplacing a standard deep network with a suitable linear approximation. With\nopportune changes to the network architecture and training procedure, we show\nthat such linear approximation achieves comparable performance to the original\nnetwork and that the forgetting problem becomes quadratic and can be solved\nefficiently even for large models. Unlike previous forgetting methods on deep\nnetworks, ours can achieve close to the state-of-the-art accuracy on large\nscale vision tasks. In particular, we show that our method allows forgetting\nwithout having to trade off the model accuracy.",
          "link": "http://arxiv.org/abs/2012.13431",
          "publishedOn": "2021-06-22T01:57:11.195Z",
          "wordCount": 696,
          "title": "Mixed-Privacy Forgetting in Deep Networks. (arXiv:2012.13431v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.07954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kassel_L/0/1/0/all/0/1\">Levi Kassel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werman_M/0/1/0/all/0/1\">Michael Werman</a>",
          "description": "Neural networks are a powerful framework for foreground segmentation in video\nacquired by static cameras, segmenting moving objects from the background in a\nrobust way in various challenging scenarios. The premier methods are those\nbased on supervision requiring a final training stage on a database of tens to\nhundreds of manually segmented images from the specific static camera. In this\nwork, we propose a method to automatically create an \"artificial\" database that\nis sufficient for training the supervised methods so that it performs better\nthan current unsupervised methods. It is based on combining a weak foreground\nsegmenter, compared to the supervised method, to extract suitable objects from\nthe training images and randomly inserting these objects back into a background\nimage. Test results are shown on the test sequences in CDnet.",
          "link": "http://arxiv.org/abs/2011.07954",
          "publishedOn": "2021-06-22T01:57:11.190Z",
          "wordCount": 608,
          "title": "Using a Supervised Method without supervision for foreground segmentation. (arXiv:2011.07954v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10641",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1\">Zeyu Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1\">Jiangbo Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xianli Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Haichuan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jialun Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chunbao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>",
          "description": "The grade of clear cell renal cell carcinoma (ccRCC) is a critical prognostic\nfactor, making ccRCC nuclei grading a crucial task in RCC pathology analysis.\nComputer-aided nuclei grading aims to improve pathologists' work efficiency\nwhile reducing their misdiagnosis rate by automatically identifying the grades\nof tumor nuclei within histopathological images. Such a task requires precisely\nsegment and accurately classify the nuclei. However, most of the existing\nnuclei segmentation and classification methods can not handle the inter-class\nsimilarity property of nuclei grading, thus can not be directly applied to the\nccRCC grading task. In this paper, we propose a Composite High-Resolution\nNetwork for ccRCC nuclei grading. Specifically, we propose a segmentation\nnetwork called W-Net that can separate the clustered nuclei. Then, we recast\nthe fine-grained classification of nuclei to two cross-category classification\ntasks, based on two high-resolution feature extractors (HRFEs) which are\nproposed for learning these two tasks. The two HRFEs share the same backbone\nencoder with W-Net by a composite connection so that meaningful features for\nthe segmentation task can be inherited for the classification task. Last, a\nhead-fusion block is applied to generate the predicted label of each nucleus.\nFurthermore, we introduce a dataset for ccRCC nuclei grading, containing 1000\nimage patches with 70945 annotated nuclei. We demonstrate that our proposed\nmethod achieves state-of-the-art performance compared to existing methods on\nthis large ccRCC grading dataset.",
          "link": "http://arxiv.org/abs/2106.10641",
          "publishedOn": "2021-06-22T01:57:11.185Z",
          "wordCount": 699,
          "title": "Nuclei Grading of Clear Cell Renal Cell Carcinoma in Histopathological Image by Composite High-Resolution Network. (arXiv:2106.10641v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalwana_M/0/1/0/all/0/1\">Muhammad A. A. K. Jalwana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>",
          "description": "Deep visual models are susceptible to adversarial perturbations to inputs.\nAlthough these signals are carefully crafted, they still appear noise-like\npatterns to humans. This observation has led to the argument that deep visual\nrepresentation is misaligned with human perception. We counter-argue by\nproviding evidence of human-meaningful patterns in adversarial perturbations.\nWe first propose an attack that fools a network to confuse a whole category of\nobjects (source class) with a target label. Our attack also limits the\nunintended fooling by samples from non-sources classes, thereby circumscribing\nhuman-defined semantic notions for network fooling. We show that the proposed\nattack not only leads to the emergence of regular geometric patterns in the\nperturbations, but also reveals insightful information about the decision\nboundaries of deep models. Exploring this phenomenon further, we alter the\n`adversarial' objective of our attack to use it as a tool to `explain' deep\nvisual representation. We show that by careful channeling and projection of the\nperturbations computed by our method, we can visualize a model's understanding\nof human-defined semantic notions. Finally, we exploit the explanability\nproperties of our perturbations to perform image generation, inpainting and\ninteractive image manipulation by attacking adversarialy robust\n`classifiers'.In all, our major contribution is a novel pragmatic adversarial\nattack that is subsequently transformed into a tool to interpret the visual\nmodels. The article also makes secondary contributions in terms of establishing\nthe utility of our attack beyond the adversarial objective with multiple\ninteresting applications.",
          "link": "http://arxiv.org/abs/2106.10606",
          "publishedOn": "2021-06-22T01:57:11.171Z",
          "wordCount": 703,
          "title": "Attack to Fool and Explain Deep Networks. (arXiv:2106.10606v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guangxing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shiyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiawei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yicheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>",
          "description": "Few-shot object detection (FSOD) aims to detect objects using only few\nexamples. It's critically needed for many practical applications but so far\nremains challenging. We propose a meta-learning based few-shot object detection\nmethod by transferring meta-knowledge learned from data-abundant base classes\nto data-scarce novel classes. Our method incorporates a coarse-to-fine approach\ninto the proposal based object detection framework and integrates prototype\nbased classifiers into both the proposal generation and classification stages.\nTo improve proposal generation for few-shot novel classes, we propose to learn\na lightweight matching network to measure the similarity between each spatial\nposition in the query image feature map and spatially-pooled class features,\ninstead of the traditional object/nonobject classifier, thus generating\ncategory-specific proposals and improving proposal recall for novel classes. To\naddress the spatial misalignment between generated proposals and few-shot class\nexamples, we propose a novel attentive feature alignment method, thus improving\nthe performance of few-shot object detection. Meanwhile we jointly learn a\nFaster R-CNN detection head for base classes. Extensive experiments conducted\non multiple FSOD benchmarks show our proposed approach achieves state of the\nart results under (incremental) few-shot learning settings.",
          "link": "http://arxiv.org/abs/2104.07719",
          "publishedOn": "2021-06-22T01:57:11.162Z",
          "wordCount": 673,
          "title": "Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with Attentive Feature Alignment. (arXiv:2104.07719v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dognin_P/0/1/0/all/0/1\">Pierre Dognin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_I/0/1/0/all/0/1\">Igor Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mroueh_Y/0/1/0/all/0/1\">Youssef Mroueh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padhi_I/0/1/0/all/0/1\">Inkit Padhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigotti_M/0/1/0/all/0/1\">Mattia Rigotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_J/0/1/0/all/0/1\">Jarret Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiff_Y/0/1/0/all/0/1\">Yair Schiff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_R/0/1/0/all/0/1\">Richard A. Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belgodere_B/0/1/0/all/0/1\">Brian Belgodere</a>",
          "description": "Image captioning has recently demonstrated impressive progress largely owing\nto the introduction of neural network algorithms trained on curated dataset\nlike MS-COCO. Often work in this field is motivated by the promise of\ndeployment of captioning systems in practical applications. However, the\nscarcity of data and contexts in many competition datasets renders the utility\nof systems trained on these datasets limited as an assistive technology in\nreal-world settings, such as helping visually impaired people navigate and\naccomplish everyday tasks. This gap motivated the introduction of the novel\nVizWiz dataset, which consists of images taken by the visually impaired and\ncaptions that have useful, task-oriented information. In an attempt to help the\nmachine learning computer vision field realize its promise of producing\ntechnologies that have positive social impact, the curators of the VizWiz\ndataset host several competitions, including one for image captioning. This\nwork details the theory and engineering from our winning submission to the 2020\ncaptioning competition. Our work provides a step towards improved assistive\nimage captioning systems.",
          "link": "http://arxiv.org/abs/2012.11696",
          "publishedOn": "2021-06-22T01:57:11.142Z",
          "wordCount": 679,
          "title": "Image Captioning as an Assistive Technology: Lessons Learned from VizWiz 2020 Challenge. (arXiv:2012.11696v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1\">Harshay Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Prateek Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1\">Praneeth Netrapalli</a>",
          "description": "Post-hoc gradient-based interpretability methods [Simonyan et al., 2013,\nSmilkov et al., 2017] that provide instance-specific explanations of model\npredictions are often based on assumption (A): magnitude of input gradients --\ngradients of logits with respect to input -- noisily highlight discriminative\ntask-relevant features. In this work, we test the validity of assumption (A)\nusing a three-pronged approach. First, we develop an evaluation framework,\nDiffROAR, to test assumption (A) on four image classification benchmarks. Our\nresults suggest that (i) input gradients of standard models (i.e., trained on\noriginal data) may grossly violate (A), whereas (ii) input gradients of\nadversarially robust models satisfy (A). Second, we then introduce BlockMNIST,\nan MNIST-based semi-real dataset, that by design encodes a priori knowledge of\ndiscriminative features. Our analysis on BlockMNIST leverages this information\nto validate as well as characterize differences between input gradient\nattributions of standard and robust models. Finally, we theoretically prove\nthat our empirical findings hold on a simplified version of the BlockMNIST\ndataset. Specifically, we prove that input gradients of standard\none-hidden-layer MLPs trained on this dataset do not highlight\ninstance-specific signal coordinates, thus grossly violating assumption (A).\nOur findings motivate the need to formalize and test common assumptions in\ninterpretability in a falsifiable manner [Leavitt and Morcos, 2020].\nAdditionally, we believe that the DiffROAR evaluation framework and\nBlockMNIST-based datasets can serve as sanity checks to audit instance-specific\ninterpretability methods.",
          "link": "http://arxiv.org/abs/2102.12781",
          "publishedOn": "2021-06-22T01:57:11.118Z",
          "wordCount": 700,
          "title": "Do Input Gradients Highlight Discriminative Features?. (arXiv:2102.12781v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ouali_Y/0/1/0/all/0/1\">Yassine Ouali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1\">C&#xe9;line Hudelot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tami_M/0/1/0/all/0/1\">Myriam Tami</a>",
          "description": "In this paper, we explore contrastive learning for few-shot classification,\nin which we propose to use it as an additional auxiliary training objective\nacting as a data-dependent regularizer to promote more general and transferable\nfeatures. In particular, we present a novel attention-based spatial contrastive\nobjective to learn locally discriminative and class-agnostic features. As a\nresult, our approach overcomes some of the limitations of the cross-entropy\nloss, such as its excessive discrimination towards seen classes, which reduces\nthe transferability of features to unseen classes. With extensive experiments,\nwe show that the proposed method outperforms state-of-the-art approaches,\nconfirming the importance of learning good and transferable embeddings for\nfew-shot learning.",
          "link": "http://arxiv.org/abs/2012.13831",
          "publishedOn": "2021-06-22T01:57:11.113Z",
          "wordCount": 580,
          "title": "Spatial Contrastive Learning for Few-Shot Classification. (arXiv:2012.13831v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10635",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yijie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fan Xue</a>",
          "description": "This paper presents a deep learning-based point cloud processing method named\nFloorPP-Net for the task of Scan-to-BIM (building information model).\nFloorPP-Net first converts the input point cloud of a building story into point\npillars (PP), then predicts the corners and edges to output the floor plan.\nAltogether, FloorPP-Net establishes an end-to-end supervised learning framework\nfor the Scan-to-Floor-Plan (Scan2FP) task. In the 1st International Scan-to-BIM\nChallenge held in conjunction with CVPR 2021, FloorPP-Net was ranked the second\nrunner-up in the floor plan reconstruction track. Future work includes general\nedge proposals, 2D plan regularization, and 3D BIM reconstruction.",
          "link": "http://arxiv.org/abs/2106.10635",
          "publishedOn": "2021-06-22T01:57:11.108Z",
          "wordCount": 530,
          "title": "FloorPP-Net: Reconstructing Floor Plans using Point Pillars for Scan-to-BIM. (arXiv:2106.10635v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.06772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_A/0/1/0/all/0/1\">Anthony Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1\">Max Paul Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resch_M/0/1/0/all/0/1\">Michael Resch</a>",
          "description": "The usage of environment sensor models for virtual testing is a promising\napproach to reduce the testing effort of autonomous driving. However, in order\nto deduce any statements regarding the performance of an autonomous driving\nfunction based on simulation, the sensor model has to be validated to determine\nthe discrepancy between the synthetic and real sensor data. Since a certain\ndegree of divergence can be assumed to exist, the sufficient level of fidelity\nmust be determined, which poses a major challenge. In particular, a method for\nquantifying the fidelity of a sensor model does not exist and the problem of\ndefining an appropriate metric remains. In this work, we train a neural network\nto distinguish real and simulated radar sensor data with the purpose of\nlearning the latent features of real radar point clouds. Furthermore, we\npropose the classifier's confidence score for the `real radar point cloud'\nclass as a metric to determine the degree of fidelity of synthetically\ngenerated radar data. The presented approach is evaluated and it can be\ndemonstrated that the proposed deep evaluation metric outperforms conventional\nmetrics in terms of its capability to identify characteristic differences\nbetween real and simulated radar data.",
          "link": "http://arxiv.org/abs/2104.06772",
          "publishedOn": "2021-06-22T01:57:11.103Z",
          "wordCount": 690,
          "title": "Deep Evaluation Metric: Learning to Evaluate Simulated Radar Point Clouds for Virtual Testing of Autonomous Driving. (arXiv:2104.06772v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Subhankar Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhiming Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>",
          "description": "In this paper, we address Novel Class Discovery (NCD), the task of unveiling\nnew classes in a set of unlabeled samples given a labeled dataset with known\nclasses. We exploit the peculiarities of NCD to build a new framework, named\nNeighborhood Contrastive Learning (NCL), to learn discriminative\nrepresentations that are important to clustering performance. Our contribution\nis twofold. First, we find that a feature extractor trained on the labeled set\ngenerates representations in which a generic query sample and its neighbors are\nlikely to share the same class. We exploit this observation to retrieve and\naggregate pseudo-positive pairs with contrastive learning, thus encouraging the\nmodel to learn more discriminative representations. Second, we notice that most\nof the instances are easily discriminated by the network, contributing less to\nthe contrastive loss. To overcome this issue, we propose to generate hard\nnegatives by mixing labeled and unlabeled samples in the feature space. We\nexperimentally demonstrate that these two ingredients significantly contribute\nto clustering performance and lead our model to outperform state-of-the-art\nmethods by a large margin (e.g., clustering accuracy +13% on CIFAR-100 and +8%\non ImageNet).",
          "link": "http://arxiv.org/abs/2106.10731",
          "publishedOn": "2021-06-22T01:57:11.086Z",
          "wordCount": 632,
          "title": "Neighborhood Contrastive Learning for Novel Class Discovery. (arXiv:2106.10731v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.07584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yihao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Caihong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianjiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qi Feng</a>",
          "description": "Recently spiking neural networks (SNNs), the third-generation of neural\nnetworks has shown remarkable capabilities of energy-efficient computing, which\nis a promising alternative for deep neural networks (DNNs) with high energy\nconsumption. SNNs have reached competitive results compared to DNNs in\nrelatively simple tasks and small datasets such as image classification and\nMNIST/CIFAR, while few studies on more challenging vision tasks on complex\ndatasets. In this paper, we focus on extending deep SNNs to object tracking, a\nmore advanced vision task with embedded applications and energy-saving\nrequirements, and present a spike-based Siamese network called SiamSNN.\nSpecifically, we propose an optimized hybrid similarity estimation method to\nexploit temporal information in the SNNs, and introduce a novel two-status\ncoding scheme to optimize the temporal distribution of output spike trains for\nfurther improvements. SiamSNN is the first deep SNN tracker that achieves short\nlatency and low precision loss on the visual object tracking benchmarks\nOTB2013/2015, VOT2016/2018, and GOT-10k. Moreover, SiamSNN achieves notably low\nenergy consumption and real-time on Neuromorphic chip TrueNorth.",
          "link": "http://arxiv.org/abs/2003.07584",
          "publishedOn": "2021-06-22T01:57:11.081Z",
          "wordCount": 664,
          "title": "SiamSNN: Siamese Spiking Neural Networks for Energy-Efficient Object Tracking. (arXiv:2003.07584v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06725",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narihira_T/0/1/0/all/0/1\">Takuya Narihira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonsogarcia_J/0/1/0/all/0/1\">Javier Alonsogarcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardinaux_F/0/1/0/all/0/1\">Fabien Cardinaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayakawa_A/0/1/0/all/0/1\">Akio Hayakawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_M/0/1/0/all/0/1\">Masato Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwaki_K/0/1/0/all/0/1\">Kazunori Iwaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemp_T/0/1/0/all/0/1\">Thomas Kemp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_Y/0/1/0/all/0/1\">Yoshiyuki Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mauch_L/0/1/0/all/0/1\">Lukas Mauch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_A/0/1/0/all/0/1\">Akira Nakamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obuchi_Y/0/1/0/all/0/1\">Yukio Obuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_A/0/1/0/all/0/1\">Andrew Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_K/0/1/0/all/0/1\">Kenji Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiedmann_S/0/1/0/all/0/1\">Stephen Tiedmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uhlich_S/0/1/0/all/0/1\">Stefan Uhlich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yashima_T/0/1/0/all/0/1\">Takuya Yashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshiyama_K/0/1/0/all/0/1\">Kazuki Yoshiyama</a>",
          "description": "While there exist a plethora of deep learning tools and frameworks, the\nfast-growing complexity of the field brings new demands and challenges, such as\nmore flexible network design, speedy computation on distributed setting, and\ncompatibility between different tools. In this paper, we introduce Neural\nNetwork Libraries (https://nnabla.org), a deep learning framework designed from\nengineer's perspective, with emphasis on usability and compatibility as its\ncore design principles. We elaborate on each of our design principles and its\nmerits, and validate our attempts via experiments.",
          "link": "http://arxiv.org/abs/2102.06725",
          "publishedOn": "2021-06-22T01:57:11.076Z",
          "wordCount": 587,
          "title": "Neural Network Libraries: A Deep Learning Framework Designed from Engineers' Perspectives. (arXiv:2102.06725v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1\">Pranesh Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karwande_A/0/1/0/all/0/1\">Atharva Karwande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolhe_T/0/1/0/all/0/1\">Tejas Kolhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamble_S/0/1/0/all/0/1\">Soham Kamble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Akshay Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wyawahare_M/0/1/0/all/0/1\">Medha Wyawahare</a>",
          "description": "One of the important and tedious task in agricultural practices is the\ndetection of the disease on crops. It requires huge time as well as skilled\nlabor. This paper proposes a smart and efficient technique for detection of\ncrop disease which uses computer vision and machine learning techniques. The\nproposed system is able to detect 20 different diseases of 5 common plants with\n93% accuracy.",
          "link": "http://arxiv.org/abs/2106.10698",
          "publishedOn": "2021-06-22T01:57:11.071Z",
          "wordCount": 513,
          "title": "Plant Disease Detection Using Image Processing and Machine Learning. (arXiv:2106.10698v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1904.12144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shimada_S/0/1/0/all/0/1\">Soshi Shimada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>",
          "description": "The majority of the existing methods for non-rigid 3D surface regression from\nmonocular 2D images require an object template or point tracks over multiple\nframes as an input, and are still far from real-time processing rates. In this\nwork, we present the Isometry-Aware Monocular Generative Adversarial Network\n(IsMo-GAN) - an approach for direct 3D reconstruction from a single image,\ntrained for the deformation model in an adversarial manner on a light-weight\nsynthetic dataset. IsMo-GAN reconstructs surfaces from real images under\nvarying illumination, camera poses, textures and shading at over 250 Hz. In\nmultiple experiments, it consistently outperforms several approaches in the\nreconstruction accuracy, runtime, generalisation to unknown surfaces and\nrobustness to occlusions. In comparison to the state-of-the-art, we reduce the\nreconstruction error by 10-30% including the textureless case and our surfaces\nevince fewer artefacts qualitatively.",
          "link": "http://arxiv.org/abs/1904.12144",
          "publishedOn": "2021-06-22T01:57:11.064Z",
          "wordCount": 611,
          "title": "IsMo-GAN: Adversarial Learning for Monocular Non-Rigid 3D Reconstruction. (arXiv:1904.12144v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13138",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hemati_S/0/1/0/all/0/1\">Sobhan Hemati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdizavareh_M/0/1/0/all/0/1\">Mohammad Hadi Mehdizavareh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenouri_S/0/1/0/all/0/1\">Shojaeddin Chenouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1\">Hamid R Tizhoosh</a>",
          "description": "In the era of big data, methods for improving memory and computational\nefficiency have become crucial for successful deployment of technologies.\nHashing is one of the most effective approaches to deal with computational\nlimitations that come with big data. One natural way for formulating this\nproblem is spectral hashing that directly incorporates affinity to learn binary\ncodes. However, due to binary constraints, the optimization becomes\nintractable. To mitigate this challenge, different relaxation approaches have\nbeen proposed to reduce the computational load of obtaining binary codes and\nstill attain a good solution. The problem with all existing relaxation methods\nis resorting to one or more additional auxiliary variables to attain high\nquality binary codes while relaxing the problem. The existence of auxiliary\nvariables leads to coordinate descent approach which increases the\ncomputational complexity. We argue that introducing these variables is\nunnecessary. To this end, we propose a novel relaxed formulation for spectral\nhashing that adds no additional variables to the problem. Furthermore, instead\nof solving the problem in original space where number of variables is equal to\nthe data points, we solve the problem in a much smaller space and retrieve the\nbinary codes from this solution. This trick reduces both the memory and\ncomputational complexity at the same time. We apply two optimization\ntechniques, namely projected gradient and optimization on manifold, to obtain\nthe solution. Using comprehensive experiments on four public datasets, we show\nthat the proposed efficient spectral hashing (ESH) algorithm achieves highly\ncompetitive retrieval performance compared with state of the art at low\ncomplexity.",
          "link": "http://arxiv.org/abs/2012.13138",
          "publishedOn": "2021-06-22T01:57:11.049Z",
          "wordCount": 745,
          "title": "A non-alternating graph hashing algorithm for large scale image search. (arXiv:2012.13138v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1901.09723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reisenhofer_R/0/1/0/all/0/1\">Rafael Reisenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_E/0/1/0/all/0/1\">Emily J. King</a>",
          "description": "We present a novel approach to the detection and characterization of edges,\nridges, and blobs in two-dimensional images which exploits the symmetry\nproperties of directionally sensitive analyzing functions in multiscale systems\nthat are constructed in the framework of alpha-molecules. The proposed feature\ndetectors are inspired by the notion of phase congruency, stable in the\npresence of noise, and by definition invariant to changes in contrast. We also\nshow how the behavior of coefficients corresponding to differently scaled and\noriented analyzing functions can be used to obtain a comprehensive\ncharacterization of the geometry of features in terms of local tangent\ndirections, widths, and heights. The accuracy and robustness of the proposed\nmeasures are validated and compared to various state-of-the-art algorithms in\nextensive numerical experiments in which we consider sets of clean and\ndistorted synthetic images that are associated with reliable ground truths. To\nfurther demonstrate the applicability, we show how the proposed ridge measure\ncan be used to detect and characterize blood vessels in digital retinal images\nand how the proposed blob measure can be applied to automatically count the\nnumber of cell colonies in a Petri dish.",
          "link": "http://arxiv.org/abs/1901.09723",
          "publishedOn": "2021-06-22T01:57:11.033Z",
          "wordCount": 677,
          "title": "Edge, Ridge, and Blob Detection with Symmetric Molecules. (arXiv:1901.09723v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lamba_J/0/1/0/all/0/1\">Jatin Lamba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abhishek/0/1/0/all/0/1\">Abhishek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akula_J/0/1/0/all/0/1\">Jayaprakash Akula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabral_R/0/1/0/all/0/1\">Rishabh Dabral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>",
          "description": "In this paper, we present a novel approach to the audio-visual video parsing\n(AVVP) task that demarcates events from a video separately for audio and visual\nmodalities. The proposed parsing approach simultaneously detects the temporal\nboundaries in terms of start and end times of such events. We show how AVVP can\nbenefit from the following techniques geared towards effective cross-modal\nlearning: (i) adversarial training and skip connections (ii) global context\naware attention and, (iii) self-supervised pretraining using an audio-video\ngrounding objective to obtain cross-modal audio-video representations. We\npresent extensive experimental evaluations on the Look, Listen, and Parse (LLP)\ndataset and show that we outperform the state-of-the-art Hybrid Attention\nNetwork (HAN) on all five metrics proposed for AVVP. We also present several\nablations to validate the effect of pretraining, global attention and\nadversarial training.",
          "link": "http://arxiv.org/abs/2104.04598",
          "publishedOn": "2021-06-22T01:57:11.022Z",
          "wordCount": 618,
          "title": "Cross-Modal learning for Audio-Visual Video Parsing. (arXiv:2104.04598v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1\">Yuanhao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1\">David Doermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>",
          "description": "This technical report presents our solution to the HACS Temporal Action\nLocalization Challenge 2021, Weakly-Supervised Learning Track. The goal of\nweakly-supervised temporal action localization is to temporally locate and\nclassify action of interest in untrimmed videos given only video-level labels.\nWe adopt the two-stream consensus network (TSCN) as the main framework in this\nchallenge. The TSCN consists of a two-stream base model training procedure and\na pseudo ground truth learning procedure. The base model training encourages\nthe model to predict reliable predictions based on single modality (i.e., RGB\nor optical flow), based on the fusion of which a pseudo ground truth is\ngenerated and in turn used as supervision to train the base models. On the HACS\nv1.1.1 dataset, without fine-tuning the feature-extraction I3D models, our\nmethod achieves 22.20% on the validation set and 21.68% on the testing set in\nterms of average mAP. Our solution ranked the 2rd in this challenge, and we\nhope our method can serve as a baseline for future academic research.",
          "link": "http://arxiv.org/abs/2106.10829",
          "publishedOn": "2021-06-22T01:57:11.016Z",
          "wordCount": 627,
          "title": "Two-Stream Consensus Network: Submission to HACS Challenge 2021 Weakly-Supervised Learning Track. (arXiv:2106.10829v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaya_E/0/1/0/all/0/1\">Emre Can Kaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabus_I/0/1/0/all/0/1\">Ioan Tabus</a>",
          "description": "This paper describes a novel lossless point cloud compression algorithm that\nuses a neural network for estimating the coding probabilities for the occupancy\nstatus of voxels, depending on wide three dimensional contexts around the voxel\nto be encoded. The point cloud is represented as an octree, with each\nresolution layer being sequentially encoded and decoded using arithmetic\ncoding, starting from the lowest resolution, until the final resolution is\nreached. The occupancy probability of each voxel of the splitting pattern at\neach node of the octree is modeled by a neural network, having at its input the\nalready encoded occupancy status of several octree nodes (belonging to the past\nand current resolutions), corresponding to a 3D context surrounding the node to\nbe encoded. The algorithm has a fast and a slow version, the fast version\nselecting differently several voxels of the context, which allows an increased\nparallelization by sending larger batches of templates to be estimated by the\nneural network, at both encoder and decoder. The proposed algorithms yield\nstate-of-the-art results on benchmark datasets. The implementation will be made\navailable at https://github.com/marmus12/nnctx",
          "link": "http://arxiv.org/abs/2106.06482",
          "publishedOn": "2021-06-22T01:57:11.010Z",
          "wordCount": 671,
          "title": "Neural Network Modeling of Probabilities for Coding the Octree Representation of Point Clouds. (arXiv:2106.06482v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1\">Divakar Singh</a>",
          "description": "The unprecedented growth in the easy availability of photo-editing tools has\nendangered the power of digital images.An image was supposed to be worth more\nthan a thousand words,but now this can be said only if it can be authenticated\northe integrity of the image can be proved to be intact. In thispaper, we\npropose a digital image forensic technique for JPEG images. It can detect any\nforgery in the image if the forged portion called a ghost image is having a\ncompression quality different from that of the cover image. It is based on\nresaving the JPEG image at different JPEG qualities, and the detection of the\nforged portion is maximum when it is saved at the same JPEG quality as the\ncover image. Also, we can precisely predictthe JPEG quality of the cover image\nby analyzing the similarity using Structural Similarity Index Measure (SSIM) or\nthe energyof the images. The first maxima in SSIM or the first minima inenergy\ncorrespond to the cover image JPEG quality. We created adataset for varying\nJPEG compression qualities of the ghost and the cover images and validated the\nscalability of the experimental results.We also, experimented with varied\nattack scenarios, e.g. high-quality ghost image embedded in low quality of\ncover image,low-quality ghost image embedded in high-quality of cover image,and\nghost image and cover image both at the same quality.The proposed method is\nable to localize the tampered portions accurately even for forgeries as small\nas 10x10 sized pixel blocks.Our technique is also robust against other attack\nscenarios like copy-move forgery, inserting text into image, rescaling\n(zoom-out/zoom-in) ghost image and then pasting on cover image.",
          "link": "http://arxiv.org/abs/2106.06439",
          "publishedOn": "2021-06-22T01:57:10.994Z",
          "wordCount": 732,
          "title": "An Image Forensic Technique Based on JPEG Ghosts. (arXiv:2106.06439v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13084",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengwen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>",
          "description": "Most consumer-grade digital cameras can only capture a limited range of\nluminance in real-world scenes due to sensor constraints. Besides, noise and\nquantization errors are often introduced in the imaging process. In order to\nobtain high dynamic range (HDR) images with excellent visual quality, the most\ncommon solution is to combine multiple images with different exposures.\nHowever, it is not always feasible to obtain multiple images of the same scene\nand most HDR reconstruction methods ignore the noise and quantization loss. In\nthis work, we propose a novel learning-based approach using a spatially dynamic\nencoder-decoder network, HDRUNet, to learn an end-to-end mapping for single\nimage HDR reconstruction with denoising and dequantization. The network\nconsists of a UNet-style base network to make full use of the hierarchical\nmulti-scale information, a condition network to perform pattern-specific\nmodulation and a weighting network for selectively retaining information.\nMoreover, we propose a Tanh_L1 loss function to balance the impact of\nover-exposed values and well-exposed values on the network learning. Our method\nachieves the state-of-the-art performance in quantitative comparisons and\nvisual quality. The proposed HDRUNet model won the second place in the single\nframe track of NITRE2021 High Dynamic Range Challenge.",
          "link": "http://arxiv.org/abs/2105.13084",
          "publishedOn": "2021-06-22T01:57:10.746Z",
          "wordCount": 661,
          "title": "HDRUNet: Single Image HDR Reconstruction with Denoising and Dequantization. (arXiv:2105.13084v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.09391",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Fechter_T/0/1/0/all/0/1\">Tobias Fechter</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Adebahr_S/0/1/0/all/0/1\">Sonja Adebahr</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Grosu_A/0/1/0/all/0/1\">Anca-Ligia Grosu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Baltas_D/0/1/0/all/0/1\">Dimos Baltas</a>",
          "description": "Stereotactic body radiation therapy allows for a precise and accurate dose\ndelivery. Organ motion during treatment bears the risk of undetected high dose\nhealthy tissue exposure. An organ very susceptible to high dose is the\noesophagus. Its low contrast on CT and the oblong shape renders motion\nestimation difficult. We tackle this issue by modern algorithms to measure the\noesophageal motion voxel-wise and to estimate motion related dosimetric impact.\nOesophageal motion was measured using deformable image registration and 4DCT of\n11 internal and 5 public datasets. Current clinical practice of contouring the\norgan on 3DCT was compared to timely resolved 4DCT contours. The dosimetric\nimpact of the motion was estimated by analysing the trajectory of each voxel in\nthe 4D dose distribution. Finally an organ motion model was built, allowing for\neasier patient-wise comparisons. Motion analysis showed mean absolute maximal\nmotion amplitudes of 4.55 +/- 1.81 mm left-right, 5.29 +/- 2.67 mm\nanterior-posterior and 10.78 +/- 5.30 mm superior-inferior. Motion between the\ncohorts differed significantly. In around 50 % of the cases the dosimetric\npassing criteria was violated. Contours created on 3DCT did not cover 14 % of\nthe organ for 50 % of the respiratory cycle and the 3D contour is around 38 %\nsmaller than the union of all 4D contours. The motion model revealed that the\nmaximal motion is not limited to the lower part of the organ. Our results\nshowed motion amplitudes higher than most reported values in the literature and\nthat motion is very heterogeneous across patients. Therefore, individual motion\ninformation should be considered in contouring and planning.",
          "link": "http://arxiv.org/abs/2010.09391",
          "publishedOn": "2021-06-22T01:57:10.692Z",
          "wordCount": 741,
          "title": "Measuring breathing induced oesophageal motion and its dosimetric impact. (arXiv:2010.09391v3 [physics.med-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mekhalfa_F/0/1/0/all/0/1\">Faiza Mekhalfa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yacef_F/0/1/0/all/0/1\">Fouad Yacef</a>",
          "description": "Computer vision techniques have attracted a great interest in precision\nagriculture, recently. The common goal of all computer vision-based precision\nagriculture tasks is to detect the objects of interest (e.g., crop, weed) and\ndiscriminating them from the background. The Weeds are unwanted plants growing\namong crops competing for nutrients, water, and sunlight, causing losses to\ncrop yields. Weed detection and mapping is critical for site-specific weed\nmanagement to reduce the cost of labor and impact of herbicides. This paper\ninvestigates the use of color and texture features for discrimination of\nSoybean crops and weeds. Feature extraction methods including two color spaces\n(RGB, HSV), gray level Co-occurrence matrix (GLCM), and Local Binary Pattern\n(LBP) are used to train the Support Vector Machine (SVM) classifier. The\nexperiment was carried out on image dataset of soybean crop, obtained from an\nunmanned aerial vehicle (UAV), which is publicly available. The results from\nthe experiment showed that the highest accuracy (above 96%) was obtained from\nthe combination of color and LBP features.",
          "link": "http://arxiv.org/abs/2106.10581",
          "publishedOn": "2021-06-22T01:57:10.659Z",
          "wordCount": 604,
          "title": "Supervised learning for crop/weed classification based on color and texture features. (arXiv:2106.10581v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Shabat_Y/0/1/0/all/0/1\">Yizhak Ben-Shabat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koneputugodage_C/0/1/0/all/0/1\">Chamin Hewa Koneputugodage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1\">Stephen Gould</a>",
          "description": "Neural shape representations have recently shown to be effective in shape\nanalysis and reconstruction tasks. Existing neural network methods require\npoint coordinates and corresponding normal vectors to learn the implicit level\nsets of the shape. Normal vectors are often not provided as raw data,\ntherefore, approximation and reorientation are required as pre-processing\nstages, both of which can introduce noise. In this paper, we propose a\ndivergence guided shape representation learning approach that does not require\nnormal vectors as input. We show that incorporating a soft constraint on the\ndivergence of the distance function favours smooth solutions that reliably\norients gradients to match the unknown normal at each point, in some cases even\nbetter than approaches that use ground truth normal vectors directly.\nAdditionally, we introduce a novel geometric initialization method for\nsinusoidal shape representation networks that further improves convergence to\nthe desired solution. We evaluate the effectiveness of our approach on the task\nof surface reconstruction and show state-of-the-art performance compared to\nother unoriented methods and on-par performance compared to oriented methods.",
          "link": "http://arxiv.org/abs/2106.10811",
          "publishedOn": "2021-06-22T01:57:10.642Z",
          "wordCount": 623,
          "title": "DiGS : Divergence guided shape implicit neural representation for unoriented point clouds. (arXiv:2106.10811v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10836",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Senzaki_Y/0/1/0/all/0/1\">Yuya Senzaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamelain_C/0/1/0/all/0/1\">Christian Hamelain</a>",
          "description": "When dealing with deep neural network (DNN) applications on edge devices,\ncontinuously updating the model is important. Although updating a model with\nreal incoming data is ideal, using all of them is not always feasible due to\nlimits, such as labeling and communication costs. Thus, it is necessary to\nfilter and select the data to use for training (i.e., active learning) on the\ndevice. In this paper, we formalize a practical active learning problem for\nDNNs on edge devices and propose a general task-agnostic framework to tackle\nthis problem, which reduces it to a stream submodular maximization. This\nframework is light enough to be run with low computational resources, yet\nprovides solutions whose quality is theoretically guaranteed thanks to the\nsubmodular property. Through this framework, we can configure data selection\ncriteria flexibly, including using methods proposed in previous active learning\nstudies. We evaluate our approach on both classification and object detection\ntasks in a practical setting to simulate a real-life scenario. The results of\nour study show that the proposed framework outperforms all other methods in\nboth tasks, while running at a practical speed on real devices.",
          "link": "http://arxiv.org/abs/2106.10836",
          "publishedOn": "2021-06-22T01:57:10.608Z",
          "wordCount": 627,
          "title": "Active Learning for Deep Neural Networks on Edge Devices. (arXiv:2106.10836v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.10042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zelin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>",
          "description": "As a 3D counterpart of object classification in images, object point cloud\nclassification is fundamental to 3D scene understanding, and has drawn great\nresearch attention since the release of benchmarking datasets, such as the\nModelNet and the ShapeNet. These benchmarks assume point clouds covering\ncomplete surfaces of object instances, for which plenty of high-performing\nmethods have been developed. However, their settings deviate from those often\nmet in practice, where, due to (self-)occlusion, a point cloud covering partial\nsurface of an object is captured from an arbitrary view. We show in this paper\nthat performance of existing point cloud classification methods drops\ndrastically under the considered practical single-view, partial setting; the\nphenomenon is consistent with the observation that semantic category of a\npartial object surface is less ambiguous only when its distribution on the\nwhole surface is clearly specified. To this end, we argue for a single-view,\npartial setting where supervised learning of object pose estimation should be\naccompanied with classification. Technically, we propose a baseline method of\nPose-Accompanied Point cloud classification Network (PAPNet); built upon\nSE(3)-equivariant convolutions, the PAPNet learns intermediate pose\ntransformations for equivariant features defined on vector fields, which makes\nthe subsequent classification easier (ideally) in the category-level, canonical\npose. We adapt existing ModelNet40 and ScanNet datasets on point set\nclassification to the introduced single-view, partial setting to verify our\nhypothesis. Thorough experiments confirm the necessity of object pose\nestimation; our PAPNet also outperforms existing methods greatly on the new\nbenchmarks.",
          "link": "http://arxiv.org/abs/2012.10042",
          "publishedOn": "2021-06-22T01:57:10.603Z",
          "wordCount": 725,
          "title": "3D Object Classification on Partial Point Clouds: A Practical Perspective. (arXiv:2012.10042v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10634",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chaolei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zihang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jian-Fang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>",
          "description": "We propose an effective two-stage approach to tackle the problem of\nlanguage-based Human-centric Spatio-Temporal Video Grounding (HC-STVG) task. In\nthe first stage, we propose an Augmented 2D Temporal Adjacent Network\n(Augmented 2D-TAN) to temporally ground the target moment corresponding to the\ngiven description. Primarily, we improve the original 2D-TAN from two aspects:\nFirst, a temporal context-aware Bi-LSTM Aggregation Module is developed to\naggregate clip-level representations, replacing the original max-pooling.\nSecond, we propose to employ Random Concatenation Augmentation (RCA) mechanism\nduring the training phase. In the second stage, we use pretrained MDETR model\nto generate per-frame bounding boxes via language query, and design a set of\nhand-crafted rules to select the best matching bounding box outputted by MDETR\nfor each frame within the grounded moment.",
          "link": "http://arxiv.org/abs/2106.10634",
          "publishedOn": "2021-06-22T01:57:10.580Z",
          "wordCount": 584,
          "title": "Augmented 2D-TAN: A Two-stage Approach for Human-centric Spatio-Temporal Video Grounding. (arXiv:2106.10634v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruoyun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haozhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chao Tao</a>",
          "description": "A new learning paradigm, self-supervised learning (SSL), can be used to solve\nsuch problems by pre-training a general model with large unlabeled images and\nthen fine-tuning on a downstream task with very few labeled samples.\nContrastive learning is a typical method of SSL, which can learn general\ninvariant features. However, most of the existing contrastive learning is\ndesigned for classification tasks to obtain an image-level representation,\nwhich may be sub-optimal for semantic segmentation tasks requiring pixel-level\ndiscrimination. Therefore, we propose Global style and Local matching\nContrastive Learning Network (GLCNet) for remote sensing semantic segmentation.\nSpecifically, the global style contrastive module is used to learn an\nimage-level representation better, as we consider the style features can better\nrepresent the overall image features; The local features matching contrastive\nmodule is designed to learn representations of local regions which is\nbeneficial for semantic segmentation. We evaluate four remote sensing semantic\nsegmentation datasets, and the experimental results show that our method mostly\noutperforms state-of-the-art self-supervised methods and ImageNet pre-training.\nSpecifically, with 1\\% annotation from the original dataset, our approach\nimproves Kappa by 6\\% on the ISPRS Potsdam dataset and 3\\% on Deep Globe Land\nCover Classification dataset relative to the existing baseline. Moreover, our\nmethod outperforms supervised learning when there are some differences between\nthe datasets of upstream tasks and downstream tasks. Our study promotes the\ndevelopment of self-supervised learning in the field of remote sensing semantic\nsegmentation. The source code is available at\nhttps://github.com/GeoX-Lab/G-RSIM.",
          "link": "http://arxiv.org/abs/2106.10605",
          "publishedOn": "2021-06-22T01:57:10.573Z",
          "wordCount": 709,
          "title": "Remote Sensing Images Semantic Segmentation with General Remote Sensing Vision Model via a Self-Supervised Contrastive Learning Method. (arXiv:2106.10605v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10718",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Han_J/0/1/0/all/0/1\">Junlin Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shoeiby_M/0/1/0/all/0/1\">Mehrdad Shoeiby</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malthus_T/0/1/0/all/0/1\">Tim Malthus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Botha_E/0/1/0/all/0/1\">Elizabeth Botha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anstee_J/0/1/0/all/0/1\">Janet Anstee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_R/0/1/0/all/0/1\">Ran Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Armin_M/0/1/0/all/0/1\">Mohammad Ali Armin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>",
          "description": "Underwater image restoration is of significant importance in unveiling the\nunderwater world. Numerous techniques and algorithms have been developed in the\npast decades. However, due to fundamental difficulties associated with\nimaging/sensing, lighting, and refractive geometric distortions, in capturing\nclear underwater images, no comprehensive evaluations have been conducted of\nunderwater image restoration. To address this gap, we have constructed a\nlarge-scale real underwater image dataset, dubbed `HICRD' (Heron Island Coral\nReef Dataset), for the purpose of benchmarking existing methods and supporting\nthe development of new deep-learning based methods. We employ accurate water\nparameter (diffuse attenuation coefficient) in generating reference images.\nThere are 2000 reference restored images and 6003 original underwater images in\nthe unpaired training set. Further, we present a novel method for underwater\nimage restoration based on unsupervised image-to-image translation framework.\nOur proposed method leveraged contrastive learning and generative adversarial\nnetworks to maximize the mutual information between raw and restored images.\nExtensive experiments with comparisons to recent approaches further demonstrate\nthe superiority of our proposed method. Our code and dataset are publicly\navailable at GitHub.",
          "link": "http://arxiv.org/abs/2106.10718",
          "publishedOn": "2021-06-22T01:57:10.565Z",
          "wordCount": 653,
          "title": "Underwater Image Restoration via Contrastive Learning and a Real-world Dataset. (arXiv:2106.10718v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1\">Abhinav Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_C/0/1/0/all/0/1\">Caleb Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haobo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">James C. Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiruvathukal_G/0/1/0/all/0/1\">George K. Thiruvathukal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yung-Hsiang Lu</a>",
          "description": "Low-power computer vision on embedded devices has many applications. This\npaper describes a low-power technique for the object re-identification (reID)\nproblem: matching a query image against a gallery of previously seen images.\nState-of-the-art techniques rely on large, computationally-intensive Deep\nNeural Networks (DNNs). We propose a novel hierarchical DNN architecture that\nuses attribute labels in the training dataset to perform efficient object reID.\nAt each node in the hierarchy, a small DNN identifies a different attribute of\nthe query image. The small DNN at each leaf node is specialized to re-identify\na subset of the gallery: only the images with the attributes identified along\nthe path from the root to a leaf. Thus, a query image is re-identified\naccurately after processing with a few small DNNs. We compare our method with\nstate-of-the-art object reID techniques. With a 4% loss in accuracy, our\napproach realizes significant resource savings: 74% less memory, 72% fewer\noperations, and 67% lower query latency, yielding 65% less energy consumption.",
          "link": "http://arxiv.org/abs/2106.10588",
          "publishedOn": "2021-06-22T01:57:10.545Z",
          "wordCount": 617,
          "title": "Low-Power Multi-Camera Object Re-Identification using Hierarchical Neural Networks. (arXiv:2106.10588v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.05278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Weihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jing-Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>",
          "description": "GAN inversion aims to invert a given image back into the latent space of a\npretrained GAN model, for the image to be faithfully reconstructed from the\ninverted code by the generator. As an emerging technique to bridge the real and\nfake image domains, GAN inversion plays an essential role in enabling the\npretrained GAN models such as StyleGAN and BigGAN to be used for real image\nediting applications. Meanwhile, GAN inversion also provides insights on the\ninterpretation of GAN's latent space and how the realistic images can be\ngenerated. In this paper, we provide an overview of GAN inversion with a focus\non its recent algorithms and applications. We cover important techniques of GAN\ninversion and their applications to image restoration and image manipulation.\nWe further elaborate on some trends and challenges for future directions.",
          "link": "http://arxiv.org/abs/2101.05278",
          "publishedOn": "2021-06-22T01:57:10.539Z",
          "wordCount": 621,
          "title": "GAN Inversion: A Survey. (arXiv:2101.05278v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Satyaki Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebert_M/0/1/0/all/0/1\">Martial Hebert</a>",
          "description": "Occlusion is one of the most significant challenges encountered by object\ndetectors and trackers. While both object detection and tracking has received a\nlot of attention in the past, most existing methods in this domain do not\ntarget detecting or tracking objects when they are occluded. However, being\nable to detect or track an object of interest through occlusion has been a long\nstanding challenge for different autonomous tasks. Traditional methods that\nemploy visual object trackers with explicit occlusion modeling experience drift\nand make several fundamental assumptions about the data. We propose to address\nthis with a `tracking-by-detection` approach that builds upon the success of\nregion based video object detectors. Our video level object detector uses a\nnovel recurrent computational unit at its core that enables long term\npropagation of object features even under occlusion. Finally, we compare our\napproach with existing state-of-the-art video object detectors and show that\nour approach achieves superior results on a dataset of furniture assembly\nvideos collected from the internet, where small objects like screws, nuts, and\nbolts often get occluded from the camera viewpoint.",
          "link": "http://arxiv.org/abs/2106.10766",
          "publishedOn": "2021-06-22T01:57:10.527Z",
          "wordCount": 613,
          "title": "Learning to Track Object Position through Occlusion. (arXiv:2106.10766v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.12391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bull_D/0/1/0/all/0/1\">David Bull</a>",
          "description": "This paper reviews the current state of the art in Artificial Intelligence\n(AI) technologies and applications in the context of the creative industries. A\nbrief background of AI, and specifically Machine Learning (ML) algorithms, is\nprovided including Convolutional Neural Network (CNNs), Generative Adversarial\nNetworks (GANs), Recurrent Neural Networks (RNNs) and Deep Reinforcement\nLearning (DRL). We categorise creative applications into five groups related to\nhow AI technologies are used: i) content creation, ii) information analysis,\niii) content enhancement and post production workflows, iv) information\nextraction and enhancement, and v) data compression. We critically examine the\nsuccesses and limitations of this rapidly advancing technology in each of these\nareas. We further differentiate between the use of AI as a creative tool and\nits potential as a creator in its own right. We foresee that, in the near\nfuture, machine learning-based AI will be adopted widely as a tool or\ncollaborative assistant for creativity. In contrast, we observe that the\nsuccesses of machine learning in domains with fewer constraints, where AI is\nthe `creator', remain modest. The potential of AI (or its developers) to win\nawards for its original creations in competition with human creatives is also\nlimited, based on contemporary technologies. We therefore conclude that, in the\ncontext of creative industries, maximum benefit from AI will be derived where\nits focus is human centric -- where it is designed to augment, rather than\nreplace, human creativity.",
          "link": "http://arxiv.org/abs/2007.12391",
          "publishedOn": "2021-06-22T01:57:10.522Z",
          "wordCount": 737,
          "title": "Artificial Intelligence in the Creative Industries: A Review. (arXiv:2007.12391v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aldahdooh_A/0/1/0/all/0/1\">Ahmed Aldahdooh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1\">Olivier D&#xe9;forges</a>",
          "description": "Security-sensitive applications that rely on Deep Neural Networks (DNNs) are\nvulnerable to small perturbations that are crafted to generate Adversarial\nExamples(AEs). The AEs are imperceptible to humans and cause DNN to misclassify\nthem. Many defense and detection techniques have been proposed. Model's\nconfidences and Dropout, as a popular way to estimate the model's uncertainty,\nhave been used for AE detection but they showed limited success against black-\nand gray-box attacks. Moreover, the state-of-the-art detection techniques have\nbeen designed for specific attacks or broken by others, need knowledge about\nthe attacks, are not consistent, increase model parameters overhead, are\ntime-consuming, or have latency in inference time. To trade off these factors,\nwe revisit the model's uncertainty and confidences and propose a novel\nunsupervised ensemble AE detection mechanism that 1) uses the uncertainty\nmethod called SelectiveNet, 2) processes model layers outputs, i.e.feature\nmaps, to generate new confidence probabilities. The detection method is called\nSelective and Feature based Adversarial Detection (SFAD). Experimental results\nshow that the proposed approach achieves better performance against black- and\ngray-box attacks than the state-of-the-art methods and achieves comparable\nperformance against white-box attacks. Moreover, results show that SFAD is\nfully robust against High Confidence Attacks (HCAs) for MNIST and partially\nrobust for CIFAR10 datasets.",
          "link": "http://arxiv.org/abs/2103.05354",
          "publishedOn": "2021-06-22T01:57:10.505Z",
          "wordCount": 677,
          "title": "Revisiting Model's Uncertainty and Confidences for Adversarial Example Detection. (arXiv:2103.05354v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gefei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yuling Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Can Yang</a>",
          "description": "We propose to learn a generative model via entropy interpolation with a\nSchr\\\"{o}dinger Bridge. The generative learning task can be formulated as\ninterpolating between a reference distribution and a target distribution based\non the Kullback-Leibler divergence. At the population level, this entropy\ninterpolation is characterized via an SDE on $[0,1]$ with a time-varying drift\nterm. At the sample level, we derive our Schr\\\"{o}dinger Bridge algorithm by\nplugging the drift term estimated by a deep score estimator and a deep density\nratio estimator into the Euler-Maruyama method. Under some mild smoothness\nassumptions of the target distribution, we prove the consistency of both the\nscore estimator and the density ratio estimator, and then establish the\nconsistency of the proposed Schr\\\"{o}dinger Bridge approach. Our theoretical\nresults guarantee that the distribution learned by our approach converges to\nthe target distribution. Experimental results on multimodal synthetic data and\nbenchmark data support our theoretical findings and indicate that the\ngenerative model via Schr\\\"{o}dinger Bridge is comparable with state-of-the-art\nGANs, suggesting a new formulation of generative learning. We demonstrate its\nusefulness in image interpolation and image inpainting.",
          "link": "http://arxiv.org/abs/2106.10410",
          "publishedOn": "2021-06-22T01:57:10.499Z",
          "wordCount": 619,
          "title": "Deep Generative Learning via Schr\\\"{o}dinger Bridge. (arXiv:2106.10410v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>",
          "description": "In this paper, we propose to utilize Automated Machine Learning to\nautomatically search architecture for deepfake detection. Unlike previous\nworks, our method benefits from the superior capability of deep learning while\nrelieving us from the high labor cost in the manual network design process. It\nis experimentally proved that our proposed method not only outperforms previous\nnon-deep learning methods but achieves comparable or even better prediction\naccuracy compared to previous deep learning methods. To improve the generality\nof our method, especially when training data and testing data are manipulated\nby different methods, we propose a multi-task strategy in our network learning\nprocess, making it estimate potential manipulation regions in given samples as\nwell as predict whether the samples are real. Comparing to previous works using\nsimilar strategies, our method depends much less on prior knowledge, such as no\nneed to know which manipulation method is utilized and whether it is utilized\nalready. Extensive experimental results on two benchmark datasets demonstrate\nthe effectiveness of our proposed method on deepfake detection.",
          "link": "http://arxiv.org/abs/2106.10705",
          "publishedOn": "2021-06-22T01:57:10.493Z",
          "wordCount": 589,
          "title": "Automated Deepfake Detection. (arXiv:2106.10705v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aimen_A/0/1/0/all/0/1\">Aroof Aimen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidheekh_S/0/1/0/all/0/1\">Sahil Sidheekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">Narayanan C. Krishnan</a>",
          "description": "Meta-learning (ML) has emerged as a promising direction in learning models\nunder constrained resource settings like few-shot learning. The popular\napproaches for ML either learn a generalizable initial model or a generic\nparametric optimizer through episodic training. The former approaches leverage\nthe knowledge from a batch of tasks to learn an optimal prior. In this work, we\nstudy the importance of a batch for ML. Specifically, we first incorporate a\nbatch episodic training regimen to improve the learning of the generic\nparametric optimizer. We also hypothesize that the common assumption in batch\nepisodic training that each task in a batch has an equal contribution to\nlearning an optimal meta-model need not be true. We propose to weight the tasks\nin a batch according to their \"importance\" in improving the meta-model's\nlearning. To this end, we introduce a training curriculum motivated by\nselective focus in humans, called task attended meta-training, to weight the\ntasks in a batch. Task attention is a standalone module that can be integrated\nwith any batch episodic training regimen. The comparisons of the models with\ntheir non-task-attended counterparts on complex datasets like miniImageNet and\ntieredImageNet validate its effectiveness.",
          "link": "http://arxiv.org/abs/2106.10642",
          "publishedOn": "2021-06-22T01:57:10.488Z",
          "wordCount": 623,
          "title": "Task Attended Meta-Learning for Few-Shot Learning. (arXiv:2106.10642v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turgutlu_K/0/1/0/all/0/1\">Kerem Turgutlu</a>",
          "description": "Existing computer vision research in categorization struggles with\nfine-grained attributes recognition due to the inherently high intra-class\nvariances and low inter-class variances. SOTA methods tackle this challenge by\nlocating the most informative image regions and rely on them to classify the\ncomplete image. The most recent work, Vision Transformer (ViT), shows its\nstrong performance in both traditional and fine-grained classification tasks.\nIn this work, we propose a multi-stage ViT framework for fine-grained image\nclassification tasks, which localizes the informative image regions without\nrequiring architectural changes using the inherent multi-head self-attention\nmechanism. We also introduce attention-guided augmentations for improving the\nmodel's capabilities. We demonstrate the value of our approach by experimenting\nwith four popular fine-grained benchmarks: CUB-200-2011, Stanford Cars,\nStanford Dogs, and FGVC7 Plant Pathology. We also prove our model's\ninterpretability via qualitative results.",
          "link": "http://arxiv.org/abs/2106.10587",
          "publishedOn": "2021-06-22T01:57:10.474Z",
          "wordCount": 594,
          "title": "Exploring Vision Transformers for Fine-grained Classification. (arXiv:2106.10587v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.08398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1\">Mustafa Hajij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batayneh_F/0/1/0/all/0/1\">Fawwaz Batayneh</a>",
          "description": "Topological Data Analysis (TDA) has emerged recently as a robust tool to\nextract and compare the structure of datasets. TDA identifies features in data\nsuch as connected components and holes and assigns a quantitative measure to\nthese features. Several studies reported that topological features extracted by\nTDA tools provide unique information about the data, discover new insights, and\ndetermine which feature is more related to the outcome. On the other hand, the\noverwhelming success of deep neural networks in learning patterns and\nrelationships has been proven on a vast array of data applications, images in\nparticular. To capture the characteristics of both powerful tools, we propose\n\\textit{TDA-Net}, a novel ensemble network that fuses topological and deep\nfeatures for the purpose of enhancing model generalizability and accuracy. We\napply the proposed \\textit{TDA-Net} to a critical application, which is the\nautomated detection of COVID-19 from CXR images. The experimental results\nshowed that the proposed network achieved excellent performance and suggests\nthe applicability of our method in practice.",
          "link": "http://arxiv.org/abs/2101.08398",
          "publishedOn": "2021-06-22T01:57:10.469Z",
          "wordCount": 697,
          "title": "TDA-Net: Fusion of Persistent Homology and Deep Learning Features for COVID-19 Detection in Chest X-Ray Images. (arXiv:2101.08398v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaxiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xueming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Li Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "Recently, some approaches are proposed to harness deep convolutional networks\nto facilitate superpixel segmentation. The common practice is to first evenly\ndivide the image into a pre-defined number of grids and then learn to associate\neach pixel with its surrounding grids. However, simply applying a series of\nconvolution operations with limited receptive fields can only implicitly\nperceive the relations between the pixel and its surrounding grids.\nConsequently, existing methods often fail to provide an effective context when\ninferring the association map. To remedy this issue, we propose a novel\n\\textbf{A}ssociation \\textbf{I}mplantation (AI) module to enable the network to\nexplicitly capture the relations between the pixel and its surrounding grids.\nThe proposed AI module directly implants the features of grid cells to the\nsurrounding of its corresponding central pixel, and conducts convolution on the\npadded window to adaptively transfer knowledge between them. With such an\nimplantation operation, the network could explicitly harvest the pixel-grid\nlevel context, which is more in line with the target of superpixel segmentation\ncomparing to the pixel-wise relation. Furthermore, to pursue better boundary\nprecision, we design a boundary-perceiving loss to help the network\ndiscriminate the pixels around boundaries in hidden feature level, which could\nbenefit the subsequent inferring modules to accurately identify more boundary\npixels. Extensive experiments on BSDS500 and NYUv2 datasets show that our\nmethod could not only achieve state-of-the-art performance but maintain\nsatisfactory inference efficiency.",
          "link": "http://arxiv.org/abs/2101.10696",
          "publishedOn": "2021-06-22T01:57:10.463Z",
          "wordCount": 700,
          "title": "AINet: Association Implantation for Superpixel Segmentation. (arXiv:2101.10696v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Meng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Recently, image captioning has aroused great interest in both academic and\nindustrial worlds. Most existing systems are built upon large-scale datasets\nconsisting of image-sentence pairs, which, however, are time-consuming to\nconstruct. In addition, even for the most advanced image captioning systems, it\nis still difficult to realize deep image understanding. In this work, we\nachieve unpaired image captioning by bridging the vision and the language\ndomains with high-level semantic information. The motivation stems from the\nfact that the semantic concepts with the same modality can be extracted from\nboth images and descriptions. To further improve the quality of captions\ngenerated by the model, we propose the Semantic Relationship Explorer, which\nexplores the relationships between semantic concepts for better understanding\nof the image. Extensive experiments on MSCOCO dataset show that we can generate\ndesirable captions without paired datasets. Furthermore, the proposed approach\nboosts five strong baselines under the paired setting, where the most\nsignificant improvement in CIDEr score reaches 8%, demonstrating that it is\neffective and generalizes well to a wide range of models.",
          "link": "http://arxiv.org/abs/2106.10658",
          "publishedOn": "2021-06-22T01:57:10.458Z",
          "wordCount": 606,
          "title": "Exploring Semantic Relationships for Unpaired Image Captioning. (arXiv:2106.10658v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.07296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stojanov_S/0/1/0/all/0/1\">Stefan Stojanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thai_A/0/1/0/all/0/1\">Anh Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1\">James M. Rehg</a>",
          "description": "It is widely accepted that reasoning about object shape is important for\nobject recognition. However, the most powerful object recognition methods today\ndo not explicitly make use of object shape during learning. In this work,\nmotivated by recent developments in low-shot learning, findings in\ndevelopmental psychology, and the increased use of synthetic data in computer\nvision research, we investigate how reasoning about 3D shape can be used to\nimprove low-shot learning methods' generalization performance. We propose a new\nway to improve existing low-shot learning approaches by learning a\ndiscriminative embedding space using 3D object shape, and using this embedding\nby learning how to map images into it. Our new approach improves the\nperformance of image-only low-shot learning approaches on multiple datasets. We\nalso introduce Toys4K, a 3D object dataset with the largest number of object\ncategories currently available, which supports low-shot learning.",
          "link": "http://arxiv.org/abs/2101.07296",
          "publishedOn": "2021-06-22T01:57:10.453Z",
          "wordCount": 629,
          "title": "Using Shape to Categorize: Low-Shot Learning with an Explicit Shape Bias. (arXiv:2101.07296v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10486",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yujun Shen</a>",
          "description": "Convolutional Neural Networks (CNNs) have achieved remarkable success in\nvarious computer vision tasks but rely on tremendous computational cost. To\nsolve this problem, existing approaches either compress well-trained\nlarge-scale models or learn lightweight models with carefully designed network\nstructures. In this work, we make a close study of the convolution operator,\nwhich is the basic unit used in CNNs, to reduce its computing load. In\nparticular, we propose a compact convolution module, called CompConv, to\nfacilitate efficient feature learning. With the divide-and-conquer strategy,\nCompConv is able to save a great many computations as well as parameters to\nproduce a certain dimensional feature map. Furthermore, CompConv discreetly\nintegrates the input features into the outputs to efficiently inherit the input\ninformation. More importantly, the novel CompConv is a plug-and-play module\nthat can be directly applied to modern CNN structures to replace the vanilla\nconvolution layers without further effort. Extensive experimental results\nsuggest that CompConv can adequately compress the benchmark CNN structures yet\nbarely sacrifice the performance, surpassing other competitors.",
          "link": "http://arxiv.org/abs/2106.10486",
          "publishedOn": "2021-06-22T01:57:10.435Z",
          "wordCount": 603,
          "title": "CompConv: A Compact Convolution Module for Efficient Feature Learning. (arXiv:2106.10486v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yihe Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yijun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuting Zhang</a>",
          "description": "We propose a semi-supervised approach for contemporary object detectors\nfollowing the teacher-student dual model framework. Our method is featured with\n1) the exponential moving averaging strategy to update the teacher from the\nstudent online, 2) using plenty of region proposals and soft pseudo-labels as\nthe student's training targets, and 3) a light-weighted detection-specific data\nensemble for the teacher to generate more reliable pseudo-labels. Compared to\nthe recent state-of-the-art -- STAC, which uses hard labels on sparsely\nselected hard pseudo samples, the teacher in our model exposes richer\ninformation to the student with soft-labels on many proposals. Our model\nachieves COCO-style AP of 53.04% on VOC07 val set, 8.4% better than STAC, when\nusing VOC12 as unlabeled data. On MS-COCO, it outperforms prior work when only\na small percentage of data is taken as labeled. It also reaches 53.8% AP on\nMS-COCO test-dev with 3.1% gain over the fully supervised ResNet-152 Cascaded\nR-CNN, by tapping into unlabeled data of a similar size to the labeled data.",
          "link": "http://arxiv.org/abs/2106.10456",
          "publishedOn": "2021-06-22T01:57:10.429Z",
          "wordCount": 611,
          "title": "Humble Teachers Teach Better Students for Semi-Supervised Object Detection. (arXiv:2106.10456v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10458",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barros_T/0/1/0/all/0/1\">Tiago Barros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_R/0/1/0/all/0/1\">Ricardo Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrote_L/0/1/0/all/0/1\">Lu&#xed;s Garrote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Premebida_C/0/1/0/all/0/1\">Cristiano Premebida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nunes_U/0/1/0/all/0/1\">Urbano J. Nunes</a>",
          "description": "Autonomous Vehicles (AV) are becoming more capable of navigating in complex\nenvironments with dynamic and changing conditions. A key component that enables\nthese intelligent vehicles to overcome such conditions and become more\nautonomous is the sophistication of the perception and localization systems. As\npart of the localization system, place recognition has benefited from recent\ndevelopments in other perception tasks such as place categorization or object\nrecognition, namely with the emergence of deep learning (DL) frameworks. This\npaper surveys recent approaches and methods used in place recognition,\nparticularly those based on deep learning. The contributions of this work are\ntwofold: surveying recent sensors such as 3D LiDARs and RADARs, applied in\nplace recognition; and categorizing the various DL-based place recognition\nworks into supervised, unsupervised, semi-supervised, parallel, and\nhierarchical categories. First, this survey introduces key place recognition\nconcepts to contextualize the reader. Then, sensor characteristics are\naddressed. This survey proceeds by elaborating on the various DL-based works,\npresenting summaries for each framework. Some lessons learned from this survey\ninclude: the importance of NetVLAD for supervised end-to-end learning; the\nadvantages of unsupervised approaches in place recognition, namely for\ncross-domain applications; or the increasing tendency of recent works to seek,\nnot only for higher performance but also for higher efficiency.",
          "link": "http://arxiv.org/abs/2106.10458",
          "publishedOn": "2021-06-22T01:57:10.423Z",
          "wordCount": 701,
          "title": "Place recognition survey: An update on deep learning approaches. (arXiv:2106.10458v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shao-Lun Huang</a>",
          "description": "Transferability estimation is an essential problem in transfer learning to\npredict how good the performance is when transfer a source model (source task)\nto a target task. Recent analytical transferability metrics have been widely\nused for source model selection and multi-task learning. Earlier metrics does\nnot work sufficiently well under the challenging cross-domain cross-task\ntransfer settings, but recent OTCE score achieves a noteworthy performance\nusing auxiliary tasks. A simplified version named OT-based NCE score sacrifices\naccuracy to be more efficient, but it can be further improved. Consequently, we\npropose a practical transferability metric called JC-NCE score to further\nimprove the cross-domain cross-task transferability estimation performance,\nwhich is more efficient than the OTCE score and more accurate than the OT-based\nNCE score. Specifically, we build the joint correspondences between source and\ntarget data via solving an optimal transport problem with considering both the\nsample distance and label distance, and then compute the transferability score\nas the negative conditional entropy. Extensive validations under the\nintra-dataset and inter-dataset transfer settings demonstrate that our JC-NCE\nscore outperforms the OT-based NCE score with about 7% and 12% gains,\nrespectively.",
          "link": "http://arxiv.org/abs/2106.10479",
          "publishedOn": "2021-06-22T01:57:10.411Z",
          "wordCount": 627,
          "title": "Practical Transferability Estimation for Image Classification Tasks. (arXiv:2106.10479v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.03891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Weiwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaodong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_N/0/1/0/all/0/1\">Naixue Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yi Ding</a>",
          "description": "Deep Neural Networks (DNNs) have achieved remarkable success in many computer\nvision tasks recently, but the huge number of parameters and the high\ncomputation overhead hinder their deployments on resource-constrained edge\ndevices. It is worth noting that channel pruning is an effective approach for\ncompressing DNN models. A critical challenge is to determine which channels are\nto be removed, so that the model accuracy will not be negatively affected. In\nthis paper, we first propose Spatial and Channel Attention (SCA), a new\nattention module combining both spatial and channel attention that respectively\nfocuses on \"where\" and \"what\" are the most informative parts. Guided by the\nscale values generated by SCA for measuring channel importance, we further\npropose a new channel pruning approach called Channel Pruning guided by Spatial\nand Channel Attention (CPSCA). Experimental results indicate that SCA achieves\nthe best inference accuracy, while incurring negligibly extra resource\nconsumption, compared to other state-of-the-art attention modules. Our\nevaluation on two benchmark datasets shows that, with the guidance of SCA, our\nCPSCA approach achieves higher inference accuracy than other state-of-the-art\npruning methods under the same pruning ratios.",
          "link": "http://arxiv.org/abs/2011.03891",
          "publishedOn": "2021-06-22T01:57:10.397Z",
          "wordCount": 669,
          "title": "Channel Pruning Guided by Spatial and Channel Attention for DNNs in Intelligent Edge Computing. (arXiv:2011.03891v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rozenberg_R/0/1/0/all/0/1\">Rapha&#xeb;l Rozenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gesnouin_J/0/1/0/all/0/1\">Joseph Gesnouin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>",
          "description": "Pedestrian motion behavior involves a combination of individual goals and\nsocial interactions with other agents. In this article, we present an\nasymmetrical bidirectional recurrent neural network architecture called U-RNN\nto encode pedestrian trajectories and evaluate its relevance to replace LSTMs\nfor various forecasting models. Experimental results on the Trajnet++ benchmark\nshow that the U-LSTM variant yields better results regarding every available\nmetrics (ADE, FDE, Collision rate) than common trajectory encoders for a\nvariety of approaches and interaction modules, suggesting that the proposed\napproach is a viable alternative to the de facto sequence encoding RNNs.\n\nOur implementation of the asymmetrical Bi-RNNs for the Trajnet++ benchmark is\navailable at:\ngithub.com/JosephGesnouin/Asymmetrical-Bi-RNNs-to-encode-pedestrian-trajectories",
          "link": "http://arxiv.org/abs/2106.04419",
          "publishedOn": "2021-06-22T01:57:10.324Z",
          "wordCount": 571,
          "title": "Asymmetrical Bi-RNN for pedestrian trajectory encoding. (arXiv:2106.04419v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farnoosh_A/0/1/0/all/0/1\">Amirreza Farnoosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1\">Sarah Ostadabbas</a>",
          "description": "In this paper, we propose a Bayesian switching dynamical model for\nsegmentation of 3D pose data over time that uncovers interpretable patterns in\nthe data and is generative. Our model decomposes highly correlated skeleton\ndata into a set of few spatial basis of switching temporal processes in a\nlow-dimensional latent framework. We parameterize these temporal processes with\nregard to a switching deep vector autoregressive prior in order to accommodate\nboth multimodal and higher-order nonlinear inter-dependencies. This results in\na dynamical deep generative latent model that parses the meaningful intrinsic\nstates in the dynamics of 3D pose data using approximate variational inference,\nand enables a realistic low-level dynamical generation and segmentation of\ncomplex skeleton movements. Our experiments on four biological motion data\ncontaining bat flight, salsa dance, walking, and golf datasets substantiate\nsuperior performance of our model in comparison with the state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2106.10393",
          "publishedOn": "2021-06-22T01:57:10.319Z",
          "wordCount": 575,
          "title": "Dynamical Deep Generative Latent Modeling of 3D Skeletal Motion. (arXiv:2106.10393v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.02968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_A/0/1/0/all/0/1\">Adrian Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fanconi_C/0/1/0/all/0/1\">Claudio Fanconi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rade_R/0/1/0/all/0/1\">Rahul Rade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1\">Jonas Kohler</a>",
          "description": "Deep neural networks that yield human interpretable decisions by\narchitectural design have lately become an increasingly popular alternative to\npost hoc interpretation of traditional black-box models. Among these networks,\nthe arguably most widespread approach is so-called prototype learning, where\nsimilarities to learned latent prototypes serve as the basis of classifying an\nunseen data point. In this work, we point to an important shortcoming of such\napproaches. Namely, there is a semantic gap between similarity in latent space\nand similarity in input space, which can corrupt interpretability. We design\ntwo experiments that exemplify this issue on the so-called ProtoPNet.\nSpecifically, we find that this network's interpretability mechanism can be led\nastray by intentionally crafted or even JPEG compression artefacts, which can\nproduce incomprehensible decisions. We argue that practitioners ought to have\nthis shortcoming in mind when deploying prototype-based models in practice.",
          "link": "http://arxiv.org/abs/2105.02968",
          "publishedOn": "2021-06-22T01:57:10.285Z",
          "wordCount": 648,
          "title": "This Looks Like That... Does it? Shortcomings of Latent Space Prototype Interpretability in Deep Networks. (arXiv:2105.02968v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Triantafillou_E/0/1/0/all/0/1\">Eleni Triantafillou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1\">Hugo Larochelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1\">Vincent Dumoulin</a>",
          "description": "Few-shot dataset generalization is a challenging variant of the well-studied\nfew-shot classification problem where a diverse training set of several\ndatasets is given, for the purpose of training an adaptable model that can then\nlearn classes from new datasets using only a few examples. To this end, we\npropose to utilize the diverse training set to construct a universal template:\na partial model that can define a wide array of dataset-specialized models, by\nplugging in appropriate components. For each new few-shot classification\nproblem, our approach therefore only requires inferring a small number of\nparameters to insert into the universal template. We design a separate network\nthat produces an initialization of those parameters for each given task, and we\nthen fine-tune its proposed initialization via a few steps of gradient descent.\nOur approach is more parameter-efficient, scalable and adaptable compared to\nprevious methods, and achieves the state-of-the-art on the challenging\nMeta-Dataset benchmark.",
          "link": "http://arxiv.org/abs/2105.07029",
          "publishedOn": "2021-06-22T01:57:10.279Z",
          "wordCount": 618,
          "title": "Learning a Universal Template for Few-shot Dataset Generalization. (arXiv:2105.07029v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chun-Tse Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wei-Chih Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chih-Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Shao-Yi Chien</a>",
          "description": "In the interactive segmentation, users initially click on the target object\nto segment the main body and then provide corrections on mislabeled regions to\niteratively refine the segmentation masks. Most existing methods transform\nthese user-provided clicks into interaction maps and concatenate them with\nimage as the input tensor. Typically, the interaction maps are determined by\nmeasuring the distance of each pixel to the clicked points, ignoring the\nrelation between clicks and mislabeled regions. We propose a Dynamic Click\nTransform Network~(DCT-Net), consisting of Spatial-DCT and Feature-DCT, to\nbetter represent user interactions. Spatial-DCT transforms each user-provided\nclick with individual diffusion distance according to the target scale, and\nFeature-DCT normalizes the extracted feature map to a specific distribution\npredicted from the clicked points. We demonstrate the effectiveness of our\nproposed method and achieve favorable performance compared to the\nstate-of-the-art on three standard benchmark datasets.",
          "link": "http://arxiv.org/abs/2106.10465",
          "publishedOn": "2021-06-22T01:57:10.264Z",
          "wordCount": 589,
          "title": "Interactive Object Segmentation with Dynamic Click Transform. (arXiv:2106.10465v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00666",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuxin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1\">Bencheng Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiemin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiyang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1\">Jianwei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>",
          "description": "Can Transformer perform $2\\mathrm{D}$ object-level recognition from a pure\nsequence-to-sequence perspective with minimal knowledge about the $2\\mathrm{D}$\nspatial structure? To answer this question, we present You Only Look at One\nSequence (YOLOS), a series of object detection models based on the na\\\"ive\nVision Transformer with the fewest possible modifications as well as inductive\nbiases. We find that YOLOS pre-trained on the mid-sized ImageNet-$1k$ dataset\nonly can already achieve competitive object detection performance on COCO,\n\\textit{e.g.}, YOLOS-Base directly adopted from BERT-Base can achieve $42.0$\nbox AP. We also discuss the impacts as well as limitations of current pre-train\nschemes and model scaling strategies for Transformer in vision through object\ndetection. Code and model weights are available at\n\\url{https://github.com/hustvl/YOLOS}.",
          "link": "http://arxiv.org/abs/2106.00666",
          "publishedOn": "2021-06-22T01:57:10.259Z",
          "wordCount": 611,
          "title": "You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection. (arXiv:2106.00666v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>",
          "description": "Although hierarchical structures are popular in recent vision transformers,\nthey require sophisticated designs and massive datasets to work well. In this\nwork, we explore the idea of nesting basic local transformers on\nnon-overlapping image blocks and aggregating them in a hierarchical manner. We\nfind that the block aggregation function plays a critical role in enabling\ncross-block non-local information communication. This observation leads us to\ndesign a simplified architecture with minor code changes upon the original\nvision transformer and obtains improved performance compared to existing\nmethods. Our empirical results show that the proposed method NesT converges\nfaster and requires much less training data to achieve good generalization. For\nexample, a NesT with 68M parameters trained on ImageNet for 100/300 epochs\nachieves $82.3\\%/83.8\\%$ accuracy evaluated on $224\\times 224$ image size,\noutperforming previous methods with up to $57\\%$ parameter reduction. Training\na NesT with 6M parameters from scratch on CIFAR10 achieves $96\\%$ accuracy\nusing a single GPU, setting a new state of the art for vision transformers.\nBeyond image classification, we extend the key idea to image generation and\nshow NesT leads to a strong decoder that is 8$\\times$ faster than previous\ntransformer based generators. Furthermore, we also propose a novel method for\nvisually interpreting the learned model. Source code is available\nhttps://github.com/google-research/nested-transformer.",
          "link": "http://arxiv.org/abs/2105.12723",
          "publishedOn": "2021-06-22T01:57:10.254Z",
          "wordCount": 672,
          "title": "Aggregating Nested Transformers. (arXiv:2105.12723v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingfeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>",
          "description": "Graphically-rich applications such as games are ubiquitous with attractive\nvisual effects of Graphical User Interface (GUI) that offers a bridge between\nsoftware applications and end-users. However, various types of graphical\nglitches may arise from such GUI complexity and have become one of the main\ncomponent of software compatibility issues. Our study on bug reports from game\ndevelopment teams in NetEase Inc. indicates that graphical glitches frequently\noccur during the GUI rendering and severely degrade the quality of\ngraphically-rich applications such as video games. Existing automated testing\ntechniques for such applications focus mainly on generating various GUI test\nsequences and check whether the test sequences can cause crashes. These\ntechniques require constant human attention to captures non-crashing bugs such\nas bugs causing graphical glitches. In this paper, we present the first step in\nautomating the test oracle for detecting non-crashing bugs in graphically-rich\napplications. Specifically, we propose \\texttt{GLIB} based on a code-based data\naugmentation technique to detect game GUI glitches. We perform an evaluation of\n\\texttt{GLIB} on 20 real-world game apps (with bug reports available) and the\nresult shows that \\texttt{GLIB} can achieve 100\\% precision and 99.5\\% recall\nin detecting non-crashing bugs such as game GUI glitches. Practical application\nof \\texttt{GLIB} on another 14 real-world games (without bug reports) further\ndemonstrates that \\texttt{GLIB} can effectively uncover GUI glitches, with 48\nof 53 bugs reported by \\texttt{GLIB} having been confirmed and fixed so far.",
          "link": "http://arxiv.org/abs/2106.10507",
          "publishedOn": "2021-06-22T01:57:10.249Z",
          "wordCount": 689,
          "title": "GLIB: Towards Automated Test Oracle for Graphically-Rich Applications. (arXiv:2106.10507v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seo_A/0/1/0/all/0/1\">Ahjeong Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1\">Gi-Cheon Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Joonhan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Byoung-Tak Zhang</a>",
          "description": "Video Question Answering is a task which requires an AI agent to answer\nquestions grounded in video. This task entails three key challenges: (1)\nunderstand the intention of various questions, (2) capturing various elements\nof the input video (e.g., object, action, causality), and (3) cross-modal\ngrounding between language and vision information. We propose Motion-Appearance\nSynergistic Networks (MASN), which embed two cross-modal features grounded on\nmotion and appearance information and selectively utilize them depending on the\nquestion's intentions. MASN consists of a motion module, an appearance module,\nand a motion-appearance fusion module. The motion module computes the\naction-oriented cross-modal joint representations, while the appearance module\nfocuses on the appearance aspect of the input video. Finally, the\nmotion-appearance fusion module takes each output of the motion module and the\nappearance module as input, and performs question-guided fusion. As a result,\nMASN achieves new state-of-the-art performance on the TGIF-QA and MSVD-QA\ndatasets. We also conduct qualitative analysis by visualizing the inference\nresults of MASN. The code is available at\nhttps://github.com/ahjeongseo/MASN-pytorch.",
          "link": "http://arxiv.org/abs/2106.10446",
          "publishedOn": "2021-06-22T01:57:10.236Z",
          "wordCount": 618,
          "title": "Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering. (arXiv:2106.10446v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10359",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gong_K/0/1/0/all/0/1\">Kuang Gong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Catana_C/0/1/0/all/0/1\">Ciprian Catana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_J/0/1/0/all/0/1\">Jinyi Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Quanzheng Li</a>",
          "description": "Direct reconstruction methods have been developed to estimate parametric\nimages directly from the measured PET sinograms by combining the PET imaging\nmodel and tracer kinetics in an integrated framework. Due to limited counts\nreceived, signal-to-noise-ratio (SNR) and resolution of parametric images\nproduced by direct reconstruction frameworks are still limited. Recently\nsupervised deep learning methods have been successfully applied to medical\nimaging denoising/reconstruction when large number of high-quality training\nlabels are available. For static PET imaging, high-quality training labels can\nbe acquired by extending the scanning time. However, this is not feasible for\ndynamic PET imaging, where the scanning time is already long enough. In this\nwork, we proposed an unsupervised deep learning framework for direct parametric\nreconstruction from dynamic PET, which was tested on the Patlak model and the\nrelative equilibrium Logan model. The patient's anatomical prior image, which\nis readily available from PET/CT or PET/MR scans, was supplied as the network\ninput to provide a manifold constraint, and also utilized to construct a kernel\nlayer to perform non-local feature denoising. The linear kinetic model was\nembedded in the network structure as a 1x1 convolution layer. The training\nobjective function was based on the PET statistical model. Evaluations based on\ndynamic datasets of 18F-FDG and 11C-PiB tracers show that the proposed\nframework can outperform the traditional and the kernel method-based direct\nreconstruction methods.",
          "link": "http://arxiv.org/abs/2106.10359",
          "publishedOn": "2021-06-22T01:57:10.231Z",
          "wordCount": 690,
          "title": "Direct Reconstruction of Linear Parametric Images from Dynamic PET Using Nonlocal Deep Image Prior. (arXiv:2106.10359v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02221",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jimenez_Martin_L/0/1/0/all/0/1\">Lauren Jimenez-Martin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Perez_D/0/1/0/all/0/1\">Daniel A. Vald&#xe9;s P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asteasuainzarra_A/0/1/0/all/0/1\">Ana M. Solares Asteasuainzarra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leonard_L/0/1/0/all/0/1\">Ludwig Leonard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diaz_Romanach_M/0/1/0/all/0/1\">Marta L. Baguer D&#xed;az-Roma&#xf1;ach</a>",
          "description": "Cervical cancer is a malignant tumor that seriously threatens women's health,\nand is one of the most common that affects women worldwide. For its early\ndetection, colposcopic images of the cervix are used for searching for possible\ninjuries or abnormalities. An inherent characteristic of these images is the\npresence of specular reflections (brightness) that make it difficult to observe\nsome regions, which might imply misdiagnosis. In this paper, a new strategy\nbased on neural networks is introduced for eliminating specular reflections and\nestimating the unobserved anatomical cervix portion under the bright zones. For\novercoming the fact that the ground truth corresponding to the specular\nreflection regions is always unknown, the new strategy proposes the supervised\ntraining of a neural network to learn how to restore any hidden regions of\ncolposcopic images. Once the specular reflections are identified, they are\nremoved from the image, and the previously trained network is used to fulfill\nthese deleted areas. The quality of the processed images was evaluated\nquantitatively and qualitatively. In 21 of the 22 evaluated images, the\ndetected specular reflections were eliminated, whereas, in the remaining one,\nthese reflections were almost completely eliminated. The distribution of the\ncolors and the content of the restored images are similar to those of the\noriginals. The evaluation carried out by a specialist in Cervix Pathology\nconcluded that, after eliminating the specular reflections, the anatomical and\nphysiological elements of the cervix are observable in the restored images,\nwhich facilitates the medical diagnosis of cervical pathologies. Our method has\nthe potential to improve the early detection of cervical cancer.",
          "link": "http://arxiv.org/abs/2106.02221",
          "publishedOn": "2021-06-22T01:57:10.226Z",
          "wordCount": 760,
          "title": "Specular reflections removal in colposcopic images based on neural networks: Supervised training with no ground truth previous knowledge. (arXiv:2106.02221v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_L/0/1/0/all/0/1\">Lu Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yubin Yang</a>",
          "description": "In this paper, we propose an efficient saliency map generation method, called\nGroup score-weighted Class Activation Mapping (Group-CAM), which adopts the\n\"split-transform-merge\" strategy to generate saliency maps. Specifically, for\nan input image, the class activations are firstly split into groups. In each\ngroup, the sub-activations are summed and de-noised as an initial mask. After\nthat, the initial masks are transformed with meaningful perturbations and then\napplied to preserve sub-pixels of the input (i.e., masked inputs), which are\nthen fed into the network to calculate the confidence scores. Finally, the\ninitial masks are weighted summed to form the final saliency map, where the\nweights are confidence scores produced by the masked inputs. Group-CAM is\nefficient yet effective, which only requires dozens of queries to the network\nwhile producing target-related saliency maps. As a result, Group-CAM can be\nserved as an effective data augment trick for fine-tuning the networks. We\ncomprehensively evaluate the performance of Group-CAM on common-used\nbenchmarks, including deletion and insertion tests on ImageNet-1k, and pointing\ngame tests on COCO2017. Extensive experimental results demonstrate that\nGroup-CAM achieves better visual performance than the current state-of-the-art\nexplanation approaches. The code is available at\nhttps://github.com/wofmanaf/Group-CAM.",
          "link": "http://arxiv.org/abs/2103.13859",
          "publishedOn": "2021-06-22T01:57:10.215Z",
          "wordCount": 695,
          "title": "Group-CAM: Group Score-Weighted Visual Explanations for Deep Convolutional Networks. (arXiv:2103.13859v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_S/0/1/0/all/0/1\">Shinnosuke Matsuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaomeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atarsaikhan_G/0/1/0/all/0/1\">Gantugs Atarsaikhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimura_A/0/1/0/all/0/1\">Akisato Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashino_K/0/1/0/all/0/1\">Kunio Kashino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwana_B/0/1/0/all/0/1\">Brian Kenji Iwana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>",
          "description": "Deep time series metric learning is challenging due to the difficult\ntrade-off between temporal invariance to nonlinear distortion and\ndiscriminative power in identifying non-matching sequences. This paper proposes\na novel neural network-based approach for robust yet discriminative time series\nclassification and verification. This approach adapts a parameterized attention\nmodel to time warping for greater and more adaptive temporal invariance. It is\nrobust against not only local but also large global distortions, so that even\nmatching pairs that do not satisfy the monotonicity, continuity, and boundary\nconditions can still be successfully identified. Learning of this model is\nfurther guided by dynamic time warping to impose temporal constraints for\nstabilized training and higher discriminative power. It can learn to augment\nthe inter-class variation through warping, so that similar but different\nclasses can be effectively distinguished. We experimentally demonstrate the\nsuperiority of the proposed approach over previous non-parametric and deep\nmodels by combining it with a deep online signature verification framework,\nafter confirming its promising behavior in single-letter handwriting\nclassification on the Unipen dataset.",
          "link": "http://arxiv.org/abs/2103.15074",
          "publishedOn": "2021-06-22T01:57:10.208Z",
          "wordCount": 652,
          "title": "Attention to Warp: Deep Metric Learning for Multivariate Time Series. (arXiv:2103.15074v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yap_M/0/1/0/all/0/1\">Moi Hoon Yap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassidy_B/0/1/0/all/0/1\">Bill Cassidy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappachan_J/0/1/0/all/0/1\">Joseph M. Pappachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OShea_C/0/1/0/all/0/1\">Claire O&#x27;Shea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillespie_D/0/1/0/all/0/1\">David Gillespie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reeves_N/0/1/0/all/0/1\">Neil Reeves</a>",
          "description": "This paper introduces the Diabetic Foot Ulcers dataset (DFUC2021) for\nanalysis of pathology, focusing on infection and ischaemia. We describe the\ndata preparation of DFUC2021 for ground truth annotation, data curation and\ndata analysis. The final release of DFUC2021 consists of 15,683 DFU patches,\nwith 5,955 training, 5,734 for testing and 3,994 unlabeled DFU patches. The\nground truth labels are four classes, i.e. control, infection, ischaemia and\nboth conditions. We curate the dataset using image hashing techniques and\nanalyse the separability using UMAP projection. We benchmark the performance of\nfive key backbones of deep learning, i.e. VGG16, ResNet101, InceptionV3,\nDenseNet121 and EfficientNet on DFUC2021. We report the optimised results of\nthese key backbones with different strategies. Based on our observations, we\nconclude that EfficientNetB0 with data augmentation and transfer learning\nprovided the best results for multi-class (4-class) classification with\nmacro-average Precision, Recall and F1-score of 0.57, 0.62 and 0.55,\nrespectively. In ischaemia and infection recognition, when trained on\none-versus-all, EfficientNetB0 achieved comparable results with the state of\nthe art. Finally, we interpret the results with statistical analysis and\nGrad-CAM visualisation.",
          "link": "http://arxiv.org/abs/2104.03068",
          "publishedOn": "2021-06-22T01:57:10.202Z",
          "wordCount": 666,
          "title": "Analysis Towards Classification of Infection and Ischaemia of Diabetic Foot Ulcers. (arXiv:2104.03068v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roland S. Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1\">Yash Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1\">Steffen Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>",
          "description": "Contrastive learning has recently seen tremendous success in self-supervised\nlearning. So far, however, it is largely unclear why the learned\nrepresentations generalize so effectively to a large variety of downstream\ntasks. We here prove that feedforward models trained with objectives belonging\nto the commonly used InfoNCE family learn to implicitly invert the underlying\ngenerative model of the observed data. While the proofs make certain\nstatistical assumptions about the generative model, we observe empirically that\nour findings hold even if these assumptions are severely violated. Our theory\nhighlights a fundamental connection between contrastive learning, generative\nmodeling, and nonlinear independent component analysis, thereby furthering our\nunderstanding of the learned representations as well as providing a theoretical\nfoundation to derive more effective contrastive losses.",
          "link": "http://arxiv.org/abs/2102.08850",
          "publishedOn": "2021-06-22T01:57:10.187Z",
          "wordCount": 624,
          "title": "Contrastive Learning Inverts the Data Generating Process. (arXiv:2102.08850v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15348",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zejiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruochen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dell_M/0/1/0/all/0/1\">Melissa Dell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Benjamin Charles Germain Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlson_J/0/1/0/all/0/1\">Jacob Carlson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weining Li</a>",
          "description": "Recent advances in document image analysis (DIA) have been primarily driven\nby the application of neural networks. Ideally, research outcomes could be\neasily deployed in production and extended for further investigation. However,\nvarious factors like loosely organized codebases and sophisticated model\nconfigurations complicate the easy reuse of important innovations by a wide\naudience. Though there have been on-going efforts to improve reusability and\nsimplify deep learning (DL) model development in disciplines like natural\nlanguage processing and computer vision, none of them are optimized for\nchallenges in the domain of DIA. This represents a major gap in the existing\ntoolkit, as DIA is central to academic research across a wide range of\ndisciplines in the social sciences and humanities. This paper introduces\nlayoutparser, an open-source library for streamlining the usage of DL in DIA\nresearch and applications. The core layoutparser library comes with a set of\nsimple and intuitive interfaces for applying and customizing DL models for\nlayout detection, character recognition, and many other document processing\ntasks. To promote extensibility, layoutparser also incorporates a community\nplatform for sharing both pre-trained models and full document digitization\npipelines. We demonstrate that layoutparser is helpful for both lightweight and\nlarge-scale digitization pipelines in real-word use cases. The library is\npublicly available at https://layout-parser.github.io/.",
          "link": "http://arxiv.org/abs/2103.15348",
          "publishedOn": "2021-06-22T01:57:10.177Z",
          "wordCount": 697,
          "title": "LayoutParser: A Unified Toolkit for Deep Learning Based Document Image Analysis. (arXiv:2103.15348v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruiwen Li</a> (co-first author), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhibo Zhang</a> (co-first author), <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiani Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1\">Scott Sanner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Jongseong Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yeonjeong Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_D/0/1/0/all/0/1\">Dongsub Shim</a>",
          "description": "Recent years have seen the introduction of a range of methods for post-hoc\nexplainability of image classifier predictions. However, these post-hoc\nexplanations may not always align perfectly with classifier predictions, which\nposes a significant challenge when attempting to debug models based on such\nexplanations. To this end, we seek a methodology that can improve alignment\nbetween model predictions and explanation method that is both agnostic to the\nmodel and explanation classes and which does not require ground truth\nexplanations. We achieve this through a novel explanation-driven data\naugmentation (EDDA) method that augments the training data with occlusions of\nexisting data stemming from model-explanations; this is based on the simple\nmotivating principle that occluding salient regions for the model prediction\nshould decrease the model confidence in the prediction, while occluding\nnon-salient regions should not change the prediction -- if the model and\nexplainer are aligned. To verify that this augmentation method improves model\nand explainer alignment, we evaluate the methodology on a variety of datasets,\nimage classification models, and explanation methods. We verify in all cases\nthat our explanation-driven data augmentation method improves alignment of the\nmodel and explanation in comparison to no data augmentation and non-explanation\ndriven data augmentation methods. In conclusion, this approach provides a novel\nmodel- and explainer-agnostic methodology for improving alignment between model\npredictions and explanations, which we see as a critical step forward for\npractical deployment and debugging of image classification models.",
          "link": "http://arxiv.org/abs/2105.14162",
          "publishedOn": "2021-06-22T01:57:10.167Z",
          "wordCount": 707,
          "title": "EDDA: Explanation-driven Data Augmentation to Improve Model and Explanation Alignment. (arXiv:2105.14162v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Togo_R/0/1/0/all/0/1\">Ren Togo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogawa_T/0/1/0/all/0/1\">Takahiro Ogawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haseyama_M/0/1/0/all/0/1\">Miki Haseyama</a>",
          "description": "Background and Objective: Manually annotating gastric X-ray images for\ngastritis detection is time-consuming and expensive because it typically\nrequires expert knowledge. This paper proposes a self-supervised learning\nmethod to solve this problem. This study aims to verify the effectiveness of\nthe proposed self-supervised learning method in gastritis detection using a few\nannotated gastric X-ray images. Methods: In this paper, we propose a novel\nself-supervised learning method that can perform explicit self-supervised\nlearning and learn discriminative representations from gastric X-ray images.\nModels trained with the proposed method were fine-tuned on datasets with a few\nannotated gastric X-ray images. For comparison, several state-of-the-art\nself-supervised learning methods, i.e., containing SimSiam, BYOL, PIRL-jigsaw,\nPIRL-rotation, and SimCLR, were compared with the proposed method. Furthermore,\ntwo baseline methods, one pretrained on ImageNet and the other trained from\nscratch, were compared with the proposed method. Results: The proposed method's\nharmonic mean score of sensitivity and specificity after fine-tuning with the\nannotated data of 10, 20, 30, and 40 patients were 0.875, 0.911, 0.915, and\n0.931, respectively. The proposed method outperformed all comparative methods,\nincluding the five state-of-the-art self-supervised learning and two baseline\nmethods. Experimental results showed the effectiveness of the proposed method\nin gastritis detection with a few annotated gastric X-ray images. Conclusions:\nThe proposed self-supervised learning method shows potential for clinical use\nin gastritis detection using a few annotated gastric X-ray images.",
          "link": "http://arxiv.org/abs/2104.02864",
          "publishedOn": "2021-06-22T01:57:10.159Z",
          "wordCount": 690,
          "title": "Self-Supervised Learning for Gastritis Detection with Gastric X-ray Images. (arXiv:2104.02864v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stewart_C/0/1/0/all/0/1\">Charles V. Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parham_J/0/1/0/all/0/1\">Jason R. Parham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holmberg_J/0/1/0/all/0/1\">Jason Holmberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_Wolf_T/0/1/0/all/0/1\">Tanya Y. Berger-Wolf</a>",
          "description": "Hoping to stimulate new research in individual animal identification from\nimages, we propose to formulate the problem as the human-machine Continual\nCuration of images and animal identities. This is an open world recognition\nproblem, where most new animals enter the system after its algorithms are\ninitially trained and deployed. Continual Curation, as defined here, requires\n(1) an improvement in the effectiveness of current recognition methods, (2) a\npairwise verification algorithm that allows the possibility of no decision, and\n(3) an algorithmic decision mechanism that seeks human input to guide the\ncuration process. Error metrics must evaluate the ability of recognition\nalgorithms to identify not only animals that have been seen just once or twice\nbut also recognize new animals not in the database. An important measure of\noverall system performance is accuracy as a function of the amount of human\ninput required.",
          "link": "http://arxiv.org/abs/2106.10377",
          "publishedOn": "2021-06-22T01:57:10.127Z",
          "wordCount": 590,
          "title": "The Animal ID Problem: Continual Curation. (arXiv:2106.10377v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bohan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1\">Suraj Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Martin-Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>",
          "description": "A video prediction model that generalizes to diverse scenes would enable\nintelligent agents such as robots to perform a variety of tasks via planning\nwith the model. However, while existing video prediction models have produced\npromising results on small datasets, they suffer from severe underfitting when\ntrained on large and diverse datasets. To address this underfitting challenge,\nwe first observe that the ability to train larger video prediction models is\noften bottlenecked by the memory constraints of GPUs or TPUs. In parallel, deep\nhierarchical latent variable models can produce higher quality predictions by\ncapturing the multi-level stochasticity of future observations, but end-to-end\noptimization of such models is notably difficult. Our key insight is that\ngreedy and modular optimization of hierarchical autoencoders can simultaneously\naddress both the memory constraints and the optimization challenges of\nlarge-scale video prediction. We introduce Greedy Hierarchical Variational\nAutoencoders (GHVAEs), a method that learns high-fidelity video predictions by\ngreedily training each level of a hierarchical autoencoder. In comparison to\nstate-of-the-art models, GHVAEs provide 17-55% gains in prediction performance\non four video datasets, a 35-40% higher success rate on real robot tasks, and\ncan improve performance monotonically by simply adding more modules.",
          "link": "http://arxiv.org/abs/2103.04174",
          "publishedOn": "2021-06-22T01:57:10.114Z",
          "wordCount": 691,
          "title": "Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction. (arXiv:2103.04174v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>",
          "description": "This paper serves as a survey of recent advances in large margin training and\nits theoretical foundations, mostly for (nonlinear) deep neural networks (DNNs)\nthat are probably the most prominent machine learning models for large-scale\ndata in the community over the past decade. We generalize the formulation of\nclassification margins from classical research to latest DNNs, summarize\ntheoretical connections between the margin, network generalization, and\nrobustness, and introduce recent efforts in enlarging the margins for DNNs\ncomprehensively. Since the viewpoint of different methods is discrepant, we\ncategorize them into groups for ease of comparison and discussion in the paper.\nHopefully, our discussions and overview inspire new research work in the\ncommunity that aim to improve the performance of DNNs, and we also point to\ndirections where the large margin principle can be verified to provide\ntheoretical evidence why certain regularizations for DNNs function well in\npractice. We managed to shorten the paper such that the crucial spirit of large\nmargin learning and related methods are better emphasized.",
          "link": "http://arxiv.org/abs/2103.13598",
          "publishedOn": "2021-06-22T01:57:10.109Z",
          "wordCount": 642,
          "title": "Recent Advances in Large Margin Learning. (arXiv:2103.13598v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.07488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1\">Shounak Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mariottoni_E/0/1/0/all/0/1\">Eduardo B. Mariottoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dov_D/0/1/0/all/0/1\">David Dov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jammal_A/0/1/0/all/0/1\">Alessandro A. Jammal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medeiros_F/0/1/0/all/0/1\">Felipe A. Medeiros</a>",
          "description": "Glaucoma is the leading cause of irreversible blindness in the world,\naffecting over 70 million people. The cumbersome Standard Automated Perimetry\n(SAP) test is most frequently used to detect visual loss due to glaucoma. Due\nto the SAP test's innate difficulty and its high test-retest variability, we\npropose the RetiNerveNet, a deep convolutional recursive neural network for\nobtaining estimates of the SAP visual field. RetiNerveNet uses information from\nthe more objective Spectral-Domain Optical Coherence Tomography (SDOCT).\nRetiNerveNet attempts to trace-back the arcuate convergence of the retinal\nnerve fibers, starting from the Retinal Nerve Fiber Layer (RNFL) thickness\naround the optic disc, to estimate individual age-corrected 24-2 SAP values.\nRecursive passes through the proposed network sequentially yield estimates of\nthe visual locations progressively farther from the optic disc. While all the\nmethods used for our experiments exhibit lower performance for the advanced\ndisease group, the proposed network is observed to be more accurate than all\nthe baselines for estimating the individual visual field values. We further\naugment RetiNerveNet to additionally predict the SAP Mean Deviation values and\nalso create an ensemble of RetiNerveNets that further improves the performance,\nby increasingly weighting-up underrepresented parts of the training data.",
          "link": "http://arxiv.org/abs/2010.07488",
          "publishedOn": "2021-06-22T01:57:10.103Z",
          "wordCount": 695,
          "title": "RetiNerveNet: Using Recursive Deep Learning to Estimate Pointwise 24-2 Visual Field Data based on Retinal Structure. (arXiv:2010.07488v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1\">Marios Savvides</a>",
          "description": "Previous studies dominantly target at self-supervised learning on real-valued\nnetworks and have achieved many promising results. However, on the more\nchallenging binary neural networks (BNNs), this task has not yet been fully\nexplored in the community. In this paper, we focus on this more difficult\nscenario: learning networks where both weights and activations are binary,\nmeanwhile, without any human annotated labels. We observe that the commonly\nused contrastive objective is not satisfying on BNNs for competitive accuracy,\nsince the backbone network contains relatively limited capacity and\nrepresentation ability. Hence instead of directly applying existing\nself-supervised methods, which cause a severe decline in performance, we\npresent a novel guided learning paradigm from real-valued to distill binary\nnetworks on the final prediction distribution, to minimize the loss and obtain\ndesirable accuracy. Our proposed method can boost the simple contrastive\nlearning baseline by an absolute gain of 5.5~15% on BNNs. We further reveal\nthat it is difficult for BNNs to recover the similar predictive distributions\nas real-valued models when training without labels. Thus, how to calibrate them\nis key to address the degradation in performance. Extensive experiments are\nconducted on the large-scale ImageNet and downstream datasets. Our method\nachieves substantial improvement over the simple contrastive learning baseline,\nand is even comparable to many mainstream supervised BNN methods. Code is\navailable at https://github.com/szq0214/S2-BNN.",
          "link": "http://arxiv.org/abs/2102.08946",
          "publishedOn": "2021-06-22T01:57:10.097Z",
          "wordCount": 738,
          "title": "S2-BNN: Bridging the Gap Between Self-Supervised Real and 1-bit Neural Networks via Guided Distribution Calibration. (arXiv:2102.08946v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Calvin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lala Li</a>",
          "description": "Contrastive loss and its variants have become very popular recently for\nlearning visual representations without supervision. In this work, we study\nthree intriguing properties of contrastive learning. We first generalize the\nstandard contrastive loss to a broader family of losses, and we find that\nvarious instantiations of the generalized loss perform similarly under the\npresence of a multi-layer non-linear projection head. We then study if\ninstance-based contrastive learning (such as in SimCLR, MoCo, BYOL, and so on,\nwhich are based on global image representation) can learn well on images with\nmultiple objects present. We find that meaningful hierarchical local features\ncan be learned despite the fact that these objectives operate on global\ninstance-level features.\n\nFinally, we study an intriguing phenomenon of feature suppression among\ncompeting features shared across augmented views, such as \"color distribution\"\nvs \"object class\". We construct datasets with explicit and controllable\ncompeting features, and show that, for contrastive learning, a few bits of\neasy-to-learn shared features can suppress, and even fully prevent, the\nlearning of other sets of competing features. In scenarios where there are\nmultiple objects in an image, the dominant object would suppress the learning\nof smaller objects. Existing contrastive learning methods critically rely on\ndata augmentation to favor certain sets of features over others, and face\npotential limitation for scenarios where existing augmentations cannot fully\naddress the feature suppression. This poses open challenges to existing\ncontrastive learning techniques.",
          "link": "http://arxiv.org/abs/2011.02803",
          "publishedOn": "2021-06-22T01:57:10.081Z",
          "wordCount": 707,
          "title": "Intriguing Properties of Contrastive Losses. (arXiv:2011.02803v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schlagenhauf_T/0/1/0/all/0/1\">Tobias Schlagenhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brander_T/0/1/0/all/0/1\">Tim Brander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleischer_J/0/1/0/all/0/1\">Juergen Fleischer</a>",
          "description": "This paper provides a novel approach to stitching surface images of\nrotationally symmetric parts. It presents a process pipeline that uses a\nfeature-based stitching approach to create a distortion-free and true-to-life\nimage from a video file. The developed process thus enables, for example,\ncondition monitoring without having to view many individual images. For\nvalidation purposes, this will be demonstrated in the paper using the concrete\nexample of a worn ball screw drive spindle. The developed algorithm aims at\nreproducing the functional principle of a line scan camera system, whereby the\nphysical measuring systems are replaced by a feature-based approach. For\nevaluation of the stitching algorithms, metrics are used, some of which have\nonly been developed in this work or have been supplemented by test procedures\nalready in use. The applicability of the developed algorithm is not only\nlimited to machine tool spindles. Instead, the developed method allows a\ngeneral approach to the surface inspection of various rotationally symmetric\ncomponents and can therefore be used in a variety of industrial applications.\nDeep-learning-based detection Algorithms can easily be implemented to generate\na complete pipeline for failure detection and condition monitoring on\nrotationally symmetric parts.",
          "link": "http://arxiv.org/abs/2012.00308",
          "publishedOn": "2021-06-22T01:57:10.075Z",
          "wordCount": 685,
          "title": "A Stitching Algorithm for Automated Surface Inspection of Rotationally Symmetric Components. (arXiv:2012.00308v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Sandipan Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Ajjen Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_P/0/1/0/all/0/1\">Prashant Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Sneha Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyal_S/0/1/0/all/0/1\">Survi Kyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_T/0/1/0/all/0/1\">Taniya Mishra</a>",
          "description": "Building facial analysis systems that generalize to extreme variations in\nlighting and facial expressions is a challenging problem that can potentially\nbe alleviated using natural-looking synthetic data. Towards that, we propose\nLEGAN, a novel synthesis framework that leverages perceptual quality judgments\nfor jointly manipulating lighting and expressions in face images, without\nrequiring paired training data. LEGAN disentangles the lighting and expression\nsubspaces and performs transformations in the feature space before upscaling to\nthe desired output image. The fidelity of the synthetic image is further\nrefined by integrating a perceptual quality estimation model, trained with face\nimages rendered using multiple synthesis methods and their crowd-sourced\nnaturalness ratings, into the LEGAN framework as an auxiliary discriminator.\nUsing objective metrics like FID and LPIPS, LEGAN is shown to generate higher\nquality face images when compared with popular GAN models like StarGAN and\nStarGAN-v2 for lighting and expression synthesis. We also conduct a perceptual\nstudy using images synthesized by LEGAN and other GAN models and show the\ncorrelation between our quality estimation and visual fidelity. Finally, we\ndemonstrate the effectiveness of LEGAN as training data augmenter for\nexpression recognition and face verification tasks.",
          "link": "http://arxiv.org/abs/2010.01464",
          "publishedOn": "2021-06-22T01:57:10.070Z",
          "wordCount": 698,
          "title": "LEGAN: Disentangled Manipulation of Directional Lighting and Facial Expressions by Leveraging Human Perceptual Judgements. (arXiv:2010.01464v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xuran Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1\">Zhuofan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Erran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>",
          "description": "Feature learning for 3D object detection from point clouds is very\nchallenging due to the irregularity of 3D point cloud data. In this paper, we\npropose Pointformer, a Transformer backbone designed for 3D point clouds to\nlearn features effectively. Specifically, a Local Transformer module is\nemployed to model interactions among points in a local region, which learns\ncontext-dependent region features at an object level. A Global Transformer is\ndesigned to learn context-aware representations at the scene level. To further\ncapture the dependencies among multi-scale representations, we propose\nLocal-Global Transformer to integrate local features with global features from\nhigher resolution. In addition, we introduce an efficient coordinate refinement\nmodule to shift down-sampled points closer to object centroids, which improves\nobject proposal generation. We use Pointformer as the backbone for\nstate-of-the-art object detection models and demonstrate significant\nimprovements over original models on both indoor and outdoor datasets.",
          "link": "http://arxiv.org/abs/2012.11409",
          "publishedOn": "2021-06-22T01:57:10.064Z",
          "wordCount": 627,
          "title": "3D Object Detection with Pointformer. (arXiv:2012.11409v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1\">Krzysztof Choromanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1\">Deepali Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1\">Jack Parker-Holder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyou Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likhosherstov_V/0/1/0/all/0/1\">Valerii Likhosherstov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santara_A/0/1/0/all/0/1\">Anirban Santara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1\">Aldo Pacchiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yunhao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>",
          "description": "There has recently been significant interest in training reinforcement\nlearning (RL) agents in vision-based environments. This poses many challenges,\nsuch as high dimensionality and potential for observational overfitting through\nspurious correlations. A promising approach to solve both of these problems is\na self-attention bottleneck, which provides a simple and effective framework\nfor learning high performing policies, even in the presence of distractions.\nHowever, due to poor scalability of attention architectures, these methods do\nnot scale beyond low resolution visual inputs, using large patches (thus small\nattention matrices). In this paper we make use of new efficient attention\nalgorithms, recently shown to be highly effective for Transformers, and\ndemonstrate that these new techniques can be applied in the RL setting. This\nallows our attention-based controllers to scale to larger visual inputs, and\nfacilitate the use of smaller patches, even individual pixels, improving\ngeneralization. In addition, we propose a new efficient algorithm approximating\nsoftmax attention with what we call hybrid random features, leveraging the\ntheory of angular kernels. We show theoretically and empirically that hybrid\nrandom features is a promising approach when using attention for vision-based\nRL.",
          "link": "http://arxiv.org/abs/2102.04353",
          "publishedOn": "2021-06-22T01:57:10.058Z",
          "wordCount": 678,
          "title": "Unlocking Pixels for Reinforcement Learning via Implicit Attention. (arXiv:2102.04353v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12463",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chenghao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>",
          "description": "A deraining network can be interpreted as a conditional generator that aims\nat removing rain streaks from image. Most existing image deraining methods\nignore model errors caused by uncertainty that reduces embedding quality.\nUnlike existing image deraining methods that embed low-quality features into\nthe model directly, we replace low-quality features by latent high-quality\nfeatures. The spirit of closed-loop feedback in the automatic control field is\nborrowed to obtain latent high-quality features. A new method for error\ndetection and feature compensation is proposed to address model errors.\nExtensive experiments on benchmark datasets as well as specific real datasets\ndemonstrate that the proposed method outperforms recent state-of-the-art\nmethods. Code is available at: \\\\ https://github.com/LI-Hao-SJTU/DerainRLNet",
          "link": "http://arxiv.org/abs/2101.12463",
          "publishedOn": "2021-06-22T01:57:10.043Z",
          "wordCount": 596,
          "title": "Robust Representation Learning with Feedback for Single Image Deraining. (arXiv:2101.12463v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xian_W/0/1/0/all/0/1\">Wenqi Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Bin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopf_J/0/1/0/all/0/1\">Johannes Kopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changil Kim</a>",
          "description": "We present a method that learns a spatiotemporal neural irradiance field for\ndynamic scenes from a single video. Our learned representation enables\nfree-viewpoint rendering of the input video. Our method builds upon recent\nadvances in implicit representations. Learning a spatiotemporal irradiance\nfield from a single video poses significant challenges because the video\ncontains only one observation of the scene at any point in time. The 3D\ngeometry of a scene can be legitimately represented in numerous ways since\nvarying geometry (motion) can be explained with varying appearance and vice\nversa. We address this ambiguity by constraining the time-varying geometry of\nour dynamic scene representation using the scene depth estimated from video\ndepth estimation methods, aggregating contents from individual frames into a\nsingle global representation. We provide an extensive quantitative evaluation\nand demonstrate compelling free-viewpoint rendering results.",
          "link": "http://arxiv.org/abs/2011.12950",
          "publishedOn": "2021-06-22T01:57:10.038Z",
          "wordCount": 604,
          "title": "Space-time Neural Irradiance Fields for Free-Viewpoint Video. (arXiv:2011.12950v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.09508",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Veerabadran_V/0/1/0/all/0/1\">Vijay Veerabadran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pourreza_R/0/1/0/all/0/1\">Reza Pourreza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Habibian_A/0/1/0/all/0/1\">Amirhossein Habibian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_T/0/1/0/all/0/1\">Taco Cohen</a>",
          "description": "In this paper, we present a novel adversarial lossy video compression model.\nAt extremely low bit-rates, standard video coding schemes suffer from\nunpleasant reconstruction artifacts such as blocking, ringing etc. Existing\nlearned neural approaches to video compression have achieved reasonable success\non reducing the bit-rate for efficient transmission and reduce the impact of\nartifacts to an extent. However, they still tend to produce blurred results\nunder extreme compression. In this paper, we present a deep adversarial learned\nvideo compression model that minimizes an auxiliary adversarial distortion\nobjective. We find this adversarial objective to correlate better with human\nperceptual quality judgement relative to traditional quality metrics such as\nMS-SSIM and PSNR. Our experiments using a state-of-the-art learned video\ncompression system demonstrate a reduction of perceptual artifacts and\nreconstruction of detail lost especially under extremely high compression.",
          "link": "http://arxiv.org/abs/2004.09508",
          "publishedOn": "2021-06-22T01:57:10.032Z",
          "wordCount": 614,
          "title": "Adversarial Distortion for Learned Video Compression. (arXiv:2004.09508v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.03365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pino_O/0/1/0/all/0/1\">Omar Vidal Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nascimento_E/0/1/0/all/0/1\">Erickson Rangel Nascimento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_M/0/1/0/all/0/1\">Mario Fernando Montenegro Campos</a>",
          "description": "In this paper, we introduce a novel semantic description approach inspired on\nPrototype Theory foundations. We propose a Computational Prototype Model (CPM)\nthat encodes and stores the central semantic meaning of objects category: the\nsemantic prototype. Also, we introduce a Prototype-based Description Model that\nencodes the semantic meaning of an object while describing its features using\nour CPM model. Our description method uses semantic prototypes computed by\nCNN-classifications models to create discriminative signatures that describe an\nobject highlighting its most distinctive features within the category. Our\nexperiments show that: i) our CPM model (semantic prototype + distance metric)\nis able to describe the internal semantic structure of objects categories; ii)\nour semantic distance metric can be understood as the object visual typicality\nscore within a category; iii) our descriptor encoding is semantically\ninterpretable and significantly outperforms other image global encodings in\nclustering and classification tasks.",
          "link": "http://arxiv.org/abs/1906.03365",
          "publishedOn": "2021-06-22T01:57:10.026Z",
          "wordCount": 679,
          "title": "Global Semantic Description of Objects based on Prototype Theory. (arXiv:1906.03365v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Levi_M/0/1/0/all/0/1\">Matan Levi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attias_I/0/1/0/all/0/1\">Idan Attias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kontorovich_A/0/1/0/all/0/1\">Aryeh Kontorovich</a>",
          "description": "The phenomenon of adversarial examples illustrates one of the most basic\nvulnerabilities of deep neural networks. Among the variety of techniques\nintroduced to surmount this inherent weakness, adversarial training has emerged\nas the most common and efficient strategy to achieve robustness. Typically,\nthis is achieved by balancing robust and natural objectives. In this work, we\naim to achieve better trade-off between robust and natural performances by\nenforcing a domain-invariant feature representation. We present a new\nadversarial training method, Domain Invariant Adversarial Learning (DIAL),\nwhich learns a feature representation which is both robust and domain\ninvariant. DIAL uses a variant of Domain Adversarial Neural Network (DANN) on\nthe natural domain and its corresponding adversarial domain. In a case where\nthe source domain consists of natural examples and the target domain is the\nadversarially perturbed examples, our method learns a feature representation\nconstrained not to discriminate between the natural and adversarial examples,\nand can therefore achieve a more robust representation. Our experiments\nindicate that our method improves both robustness and natural accuracy, when\ncompared to current state-of-the-art adversarial training methods.",
          "link": "http://arxiv.org/abs/2104.00322",
          "publishedOn": "2021-06-22T01:57:10.021Z",
          "wordCount": 633,
          "title": "Domain Invariant Adversarial Learning. (arXiv:2104.00322v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.04539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Winkelbauer_D/0/1/0/all/0/1\">Dominik Winkelbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denninger_M/0/1/0/all/0/1\">Maximilian Denninger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1\">Rudolph Triebel</a>",
          "description": "Most existing approaches for visual localization either need a detailed 3D\nmodel of the environment or, in the case of learning-based methods, must be\nretrained for each new scene. This can either be very expensive or simply\nimpossible for large, unknown environments, for example in search-and-rescue\nscenarios. Although there are learning-based approaches that operate\nscene-agnostically, the generalization capability of these methods is still\noutperformed by classical approaches. In this paper, we present an approach\nthat can generalize to new scenes by applying specific changes to the model\narchitecture, including an extended regression part, the use of hierarchical\ncorrelation layers, and the exploitation of scale and uncertainty information.\nOur approach outperforms the 5-point algorithm using SIFT features on equally\nbig images and additionally surpasses all previous learning-based approaches\nthat were trained on different data. It is also superior to most of the\napproaches that were specifically trained on the respective scenes. We also\nevaluate our approach in a scenario where only very few reference images are\navailable, showing that under such more realistic conditions our learning-based\napproach considerably exceeds both existing learning-based and classical\nmethods.",
          "link": "http://arxiv.org/abs/2011.04539",
          "publishedOn": "2021-06-22T01:57:10.005Z",
          "wordCount": 665,
          "title": "Learning to Localize in New Environments from Synthetic Training Data. (arXiv:2011.04539v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1\">Xin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>",
          "description": "Autonomous driving is regarded as one of the most promising remedies to\nshield human beings from severe crashes. To this end, 3D object detection\nserves as the core basis of such perception system especially for the sake of\npath planning, motion prediction, collision avoidance, etc. Generally, stereo\nor monocular images with corresponding 3D point clouds are already standard\nlayout for 3D object detection, out of which point clouds are increasingly\nprevalent with accurate depth information being provided. Despite existing\nefforts, 3D object detection on point clouds is still in its infancy due to\nhigh sparseness and irregularity of point clouds by nature, misalignment view\nbetween camera view and LiDAR bird's eye of view for modality synergies,\nocclusions and scale variations at long distances, etc. Recently, profound\nprogress has been made in 3D object detection, with a large body of literature\nbeing investigated to address this vision task. As such, we present a\ncomprehensive review of the latest progress in this field covering all the main\ntopics including sensors, fundamentals, and the recent state-of-the-art\ndetection methods with their pros and cons. Furthermore, we introduce metrics\nand provide quantitative comparisons on popular public datasets. The avenues\nfor future work are going to be judiciously identified after an in-deep\nanalysis of the surveyed works. Finally, we conclude this paper.",
          "link": "http://arxiv.org/abs/2106.10823",
          "publishedOn": "2021-06-22T01:57:09.991Z",
          "wordCount": 657,
          "title": "3D Object Detection for Autonomous Driving: A Survey. (arXiv:2106.10823v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xintian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1\">Muhammad Usman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Rentuo Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1\">Pengfei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huanhuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>",
          "description": "Generative Adversarial Networks (GANs) have been widely applied in different\nscenarios thanks to the development of deep neural networks. The original GAN\nwas proposed based on the non-parametric assumption of the infinite capacity of\nnetworks. However, it is still unknown whether GANs can generate realistic\nsamples without any prior information. Due to the overconfident assumption,\nmany issues remain unaddressed in GANs' training, such as non-convergence, mode\ncollapses, gradient vanishing. Regularization and normalization are common\nmethods of introducing prior information to stabilize training and improve\ndiscrimination. Although a handful number of regularization and normalization\nmethods have been proposed for GANs, to the best of our knowledge, there exists\nno comprehensive survey which primarily focuses on objectives and development\nof these methods, apart from some in-comprehensive and limited scope studies.\nIn this work, we conduct a comprehensive survey on the regularization and\nnormalization techniques from different perspectives of GANs training. First,\nwe systematically describe different perspectives of GANs training and thus\nobtain the different objectives of regularization and normalization. Based on\nthese objectives, we propose a new taxonomy. Furthermore, we compare the\nperformance of the mainstream methods on different datasets and investigate the\nregularization and normalization techniques that have been frequently employed\nin SOTA GANs. Finally, we highlight potential future directions of research in\nthis domain.",
          "link": "http://arxiv.org/abs/2008.08930",
          "publishedOn": "2021-06-22T01:57:09.981Z",
          "wordCount": 721,
          "title": "A Systematic Survey of Regularization and Normalization in GANs. (arXiv:2008.08930v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_D/0/1/0/all/0/1\">Divyam Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>",
          "description": "Adversarial learning has emerged as one of the successful techniques to\ncircumvent the susceptibility of existing methods against adversarial\nperturbations. However, the majority of existing defense methods are tailored\nto defend against a single category of adversarial perturbation (e.g.\n$\\ell_\\infty$-attack). In safety-critical applications, this makes these\nmethods extraneous as the attacker can adopt diverse adversaries to deceive the\nsystem. Moreover, training on multiple perturbations simultaneously\nsignificantly increases the computational overhead during training. To address\nthese challenges, we propose a novel meta-learning framework that explicitly\nlearns to generate noise to improve the model's robustness against multiple\ntypes of attacks. Its key component is Meta Noise Generator (MNG) that outputs\noptimal noise to stochastically perturb a given sample, such that it helps\nlower the error on diverse adversarial perturbations. By utilizing samples\ngenerated by MNG, we train a model by enforcing the label consistency across\nmultiple perturbations. We validate the robustness of models trained by our\nscheme on various datasets and against a wide variety of perturbations,\ndemonstrating that it significantly outperforms the baselines across multiple\nperturbations with a marginal computational cost.",
          "link": "http://arxiv.org/abs/2006.12135",
          "publishedOn": "2021-06-22T01:57:09.965Z",
          "wordCount": 672,
          "title": "Learning to Generate Noise for Multi-Attack Robustness. (arXiv:2006.12135v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yingying Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Daichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengju Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shiming Ge</a>",
          "description": "Why should we trust the detections of deep neural networks for manipulated\nfaces? Understanding the reasons is important for users in improving the\nfairness, reliability, privacy and trust of the detection models. In this work,\nwe propose an interpretable face manipulation detection approach to achieve the\ntrustworthy and accurate inference. The approach could make the face\nmanipulation detection process transparent by embedding the feature whitening\nmodule. This module aims to whiten the internal working mechanism of deep\nnetworks through feature decorrelation and feature constraint. The experimental\nresults demonstrate that our proposed approach can strike a balance between the\ndetection accuracy and the model interpretability.",
          "link": "http://arxiv.org/abs/2106.10834",
          "publishedOn": "2021-06-22T01:57:09.942Z",
          "wordCount": 538,
          "title": "Interpretable Face Manipulation Detection via Feature Whitening. (arXiv:2106.10834v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.00113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Aamir Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1\">Kazuki Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruohua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1\">Katherine Driggs-Campbell</a>",
          "description": "Trajectory prediction is one of the key capabilities for robots to safely\nnavigate and interact with pedestrians. Critical insights from human intention\nand behavioral patterns need to be integrated to effectively forecast long-term\npedestrian behavior. Thus, we propose a framework incorporating a Mutable\nIntention Filter and a Warp LSTM (MIF-WLSTM) to simultaneously estimate human\nintention and perform trajectory prediction. The Mutable Intention Filter is\ninspired by particle filtering and genetic algorithms, where particles\nrepresent intention hypotheses that can be mutated throughout the pedestrian\nmotion. Instead of predicting sequential displacement over time, our Warp LSTM\nlearns to generate offsets on a full trajectory predicted by a nominal\nintention-aware linear model, which considers the intention hypotheses during\nfiltering process. Through experiments on a publicly available dataset, we show\nthat our method outperforms baseline approaches and demonstrate the robust\nperformance of our method under abnormal intention-changing scenarios. Code is\navailable at https://github.com/tedhuang96/mifwlstm.",
          "link": "http://arxiv.org/abs/2007.00113",
          "publishedOn": "2021-06-22T01:57:09.919Z",
          "wordCount": 644,
          "title": "Long-term Pedestrian Trajectory Prediction using Mutable Intention Filter and Warp LSTM. (arXiv:2007.00113v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10683",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1\">Yuqiao Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jia-Xin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fufu Yu</a>",
          "description": "This is a technical report for CVPR 2021 AliProducts Challenge. AliProducts\nChallenge is a competition proposed for studying the large-scale and\nfine-grained commodity image recognition problem encountered by worldleading\necommerce companies. The large-scale product recognition simultaneously meets\nthe challenge of noisy annotations, imbalanced (long-tailed) data distribution\nand fine-grained classification. In our solution, we adopt stateof-the-art\nmodel architectures of both CNNs and Transformer, including ResNeSt,\nEfficientNetV2, and DeiT. We found that iterative data cleaning, classifier\nweight normalization, high-resolution finetuning, and test time augmentation\nare key components to improve the performance of training with the noisy and\nimbalanced dataset. Finally, we obtain 6.4365% mean class error rate in the\nleaderboard with our ensemble model.",
          "link": "http://arxiv.org/abs/2106.10683",
          "publishedOn": "2021-06-22T01:57:09.913Z",
          "wordCount": 567,
          "title": "Solution for Large-scale Long-tailed Recognition with Noisy Labels. (arXiv:2106.10683v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xuanyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>",
          "description": "Training images with data transformations have been suggested as contrastive\nexamples to complement the testing set for generalization performance\nevaluation of deep neural networks (DNNs). In this work, we propose a practical\nframework ContRE (The word \"contre\" means \"against\" or \"versus\" in French.)\nthat uses Contrastive examples for DNN geneRalization performance Estimation.\nSpecifically, ContRE follows the assumption in contrastive learning that robust\nDNN models with good generalization performance are capable of extracting a\nconsistent set of features and making consistent predictions from the same\nimage under varying data transformations. Incorporating with a set of\nrandomized strategies for well-designed data transformations over the training\nset, ContRE adopts classification errors and Fisher ratios on the generated\ncontrastive examples to assess and analyze the generalization performance of\ndeep models in complement with a testing set. To show the effectiveness and the\nefficiency of ContRE, extensive experiments have been done using various DNN\nmodels on three open source benchmark datasets with thorough ablation studies\nand applicability analyses. Our experiment results confirm that (1) behaviors\nof deep models on contrastive examples are strongly correlated to what on the\ntesting set, and (2) ContRE is a robust measure of generalization performance\ncomplementing to the testing set in various settings.",
          "link": "http://arxiv.org/abs/2106.10653",
          "publishedOn": "2021-06-22T01:57:09.898Z",
          "wordCount": 652,
          "title": "Practical Assessment of Generalization Performance Robustness for Deep Networks via Contrastive Examples. (arXiv:2106.10653v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Guoqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>",
          "description": "Unsupervised domain adaptive classification intends to improve\ntheclassification performance on unlabeled target domain. To alleviate the\nadverse effect of domain shift, many approaches align the source and target\ndomains in the feature space. However, a feature is usually taken as a whole\nfor alignment without explicitly making domain alignment proactively serve the\nclassification task, leading to sub-optimal solution. What sub-feature should\nbe aligned for better adaptation is under-explored. In this paper, we propose\nan effective Task-oriented Alignment (ToAlign) for unsupervised domain\nadaptation (UDA). We study what features should be aligned across domains and\npropose to make the domain alignment proactively serve classification by\nperforming feature decomposition and alignment under the guidance of the prior\nknowledge induced from the classification taskitself. Particularly, we\nexplicitly decompose a feature in the source domain intoa\ntask-related/discriminative feature that should be aligned, and a\ntask-irrelevant feature that should be avoided/ignored, based on the\nclassification meta-knowledge. Extensive experimental results on various\nbenchmarks (e.g., Office-Home, Visda-2017, and DomainNet) under different\ndomain adaptation settings demonstrate theeffectiveness of ToAlign which helps\nachieve the state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2106.10812",
          "publishedOn": "2021-06-22T01:57:09.892Z",
          "wordCount": 609,
          "title": "ToAlign: Task-oriented Alignment for Unsupervised Domain Adaptation. (arXiv:2106.10812v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liulei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bredell_G/0/1/0/all/0/1\">Gustav Bredell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1\">Ender Konukoglu</a>",
          "description": "Despite recent progress of automatic medical image segmentation techniques,\nfully automatic results usually fail to meet the clinical use and typically\nrequire further refinement. In this work, we propose a quality-aware memory\nnetwork for interactive segmentation of 3D medical images. Provided by user\nguidance on an arbitrary slice, an interaction network is firstly employed to\nobtain an initial 2D segmentation. The quality-aware memory network\nsubsequently propagates the initial segmentation estimation bidirectionally\nover the entire volume. Subsequent refinement based on additional user guidance\non other slices can be incorporated in the same manner. To further facilitate\ninteractive segmentation, a quality assessment module is introduced to suggest\nthe next slice to segment based on the current segmentation quality of each\nslice. The proposed network has two appealing characteristics: 1) The\nmemory-augmented network offers the ability to quickly encode past segmentation\ninformation, which will be retrieved for the segmentation of other slices; 2)\nThe quality assessment module enables the model to directly estimate the\nqualities of segmentation predictions, which allows an active learning paradigm\nwhere users preferentially label the lowest-quality slice for multi-round\nrefinement. The proposed network leads to a robust interactive segmentation\nengine, which can generalize well to various types of user annotations (e.g.,\nscribbles, boxes). Experimental results on various medical datasets demonstrate\nthe superiority of our approach in comparison with existing techniques.",
          "link": "http://arxiv.org/abs/2106.10686",
          "publishedOn": "2021-06-22T01:57:09.886Z",
          "wordCount": 665,
          "title": "Quality-Aware Memory Network for Interactive Volumetric Image Segmentation. (arXiv:2106.10686v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10733",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aboah_A/0/1/0/all/0/1\">Armstrong Aboah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boeding_M/0/1/0/all/0/1\">Michael Boeding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adu_Gyamfi_Y/0/1/0/all/0/1\">Yaw Adu-Gyamfi</a>",
          "description": "Routine and consistent data collection is required to address contemporary\ntransportation issues.The cost of data collection increases significantly when\nsophisticated machines are used to collect data. Due to this constraint, State\nDepartments of Transportation struggles to collect consistent data for\nanalyzing and resolving transportation problems in a timely manner. Recent\nadvancements in the sensors integrated into smartphones have resulted in a more\naffordable method of data collection.The primary objective of this study is to\ndevelop and implement a smartphone application for data collection.The\ncurrently designed app consists of three major modules: a frontend graphical\nuser interface (GUI), a sensor module, and a backend module. While the frontend\nuser interface enables interaction with the app, the sensor modules collect\nrelevant data such as video and accelerometer readings while the app is in use.\nThe backend, on the other hand, is made up of firebase storage, which is used\nto store the gathered data.In comparison to other developed apps for collecting\npavement information, this current app is not overly reliant on the internet\nenabling the app to be used in areas of restricted internet access.The\ndeveloped application was evaluated by collecting data on the i70W highway\nconnecting Columbia, Missouri, and Kansas City, Missouri.The data was analyzed\nfor a variety of purposes, including calculating the International Roughness\nIndex (IRI), identifying pavement distresses, and understanding driver's\nbehaviour and environment .The results of the application indicate that the\ndata collected by the app is of high quality.",
          "link": "http://arxiv.org/abs/2106.10733",
          "publishedOn": "2021-06-22T01:57:09.871Z",
          "wordCount": 674,
          "title": "Mobile Sensing for Multipurpose Applications in Transportation. (arXiv:2106.10733v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1\">Yao Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>",
          "description": "Scene graph generation (SGG) is to detect entity pairs with their relations\nin an image. Existing SGG approaches often use multi-stage pipelines to\ndecompose this task into object detection, relation graph construction, and\ndense or dense-to-sparse relation prediction. Instead, from a perspective on\nSGG as a direct set prediction, this paper presents a simple, sparse, and\nunified framework for relation detection, termed as Structured Sparse R-CNN.\nThe key to our method is a set of learnable triplet queries and structured\ntriplet detectors which could be jointly optimized from the training set in an\nend-to-end manner. Specifically, the triplet queries encode the general prior\nfor entity pair locations, categories, and their relations, and provide an\ninitial guess of relation detection for subsequent refinement. The triplet\ndetector presents a cascaded dynamic head design to progressively refine the\nresults of relation detection. In addition, to relieve the training difficulty\nof Structured Sparse R-CNN, we propose a relaxed and enhanced training strategy\nbased on knowledge distillation from a Siamese Sparse R-CNN. We also propose\nadaptive focusing parameter and average logit approach for imbalance data\ndistribution. We perform experiments on two benchmarks: Visual Genome and Open\nImages, and the results demonstrate that our method achieves the\nstate-of-the-art performance. Meanwhile, we perform in-depth ablation studies\nto provide insights on our structured modeling in triplet detector design and\ntraining strategies.",
          "link": "http://arxiv.org/abs/2106.10815",
          "publishedOn": "2021-06-22T01:57:09.853Z",
          "wordCount": 657,
          "title": "Structured Sparse R-CNN for Direct Scene Graph Generation. (arXiv:2106.10815v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1\">Taku Komura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>",
          "description": "We present a novel neural surface reconstruction method, called NeuS, for\nreconstructing objects and scenes with high fidelity from 2D image inputs.\nExisting neural surface reconstruction approaches, such as DVR and IDR, require\nforeground mask as supervision, easily get trapped in local minima, and\ntherefore struggle with the reconstruction of objects with severe\nself-occlusion or thin structures. Meanwhile, recent neural methods for novel\nview synthesis, such as NeRF and its variants, use volume rendering to produce\na neural scene representation with robustness of optimization, even for highly\ncomplex objects. However, extracting high-quality surfaces from this learned\nimplicit representation is difficult because there are not sufficient surface\nconstraints in the representation. In NeuS, we propose to represent a surface\nas the zero-level set of a signed distance function (SDF) and develop a new\nvolume rendering method to train a neural SDF representation. We observe that\nthe conventional volume rendering method causes inherent geometric errors (i.e.\nbias) for surface reconstruction, and therefore propose a new formulation that\nis free of bias in the first order of approximation, thus leading to more\naccurate surface reconstruction even without the mask supervision. Experiments\non the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the\nstate-of-the-arts in high-quality surface reconstruction, especially for\nobjects and scenes with complex structures and self-occlusion.",
          "link": "http://arxiv.org/abs/2106.10689",
          "publishedOn": "2021-06-22T01:57:09.848Z",
          "wordCount": 669,
          "title": "NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction. (arXiv:2106.10689v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiapeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1\">Guozhi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Weihong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1\">Kai Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yichao Huang</a>",
          "description": "Visual information extraction (VIE) has attracted increasing attention in\nrecent years. The existing methods usually first organized optical character\nrecognition (OCR) results into plain texts and then utilized token-level entity\nannotations as supervision to train a sequence tagging model. However, it\nexpends great annotation costs and may be exposed to label confusion, and the\nOCR errors will also significantly affect the final performance. In this paper,\nwe propose a unified weakly-supervised learning framework called TCPN (Tag,\nCopy or Predict Network), which introduces 1) an efficient encoder to\nsimultaneously model the semantic and layout information in 2D OCR results; 2)\na weakly-supervised training strategy that utilizes only key information\nsequences as supervision; and 3) a flexible and switchable decoder which\ncontains two inference modes: one (Copy or Predict Mode) is to output key\ninformation sequences of different categories by copying a token from the input\nor predicting one in each time step, and the other (Tag Mode) is to directly\ntag the input sequence in a single forward pass. Our method shows new\nstate-of-the-art performance on several public benchmarks, which fully proves\nits effectiveness.",
          "link": "http://arxiv.org/abs/2106.10681",
          "publishedOn": "2021-06-22T01:57:09.842Z",
          "wordCount": 656,
          "title": "Tag, Copy or Predict: A Unified Weakly-Supervised Learning Framework for Visual Information Extraction using Sequences. (arXiv:2106.10681v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10651",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Rojas_Azabache_C/0/1/0/all/0/1\">Carlos Rojas-Azabache</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vilca_Janampa_K/0/1/0/all/0/1\">Karen Vilca-Janampa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guerrero_Huayta_R/0/1/0/all/0/1\">Renzo Guerrero-Huayta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nunez_Fernandez_D/0/1/0/all/0/1\">Dennis N&#xfa;&#xf1;ez-Fern&#xe1;ndez</a>",
          "description": "The COVID-19 pandemic started in China in December 2019 and quickly spread to\nseveral countries. The consequences of this pandemic are incalculable, causing\nthe death of millions of people and damaging the global economy. To achieve\nlarge-scale control of this pandemic, fast tools for detection and treatment of\npatients are needed. Thus, the demand for alternative tools for the diagnosis\nof COVID-19 has increased dramatically since accurated and automated tools are\nnot available. In this paper we present the ongoing work on a system for\nCOVID-19 detection using ultrasound imaging and using Deep Learning techniques.\nFurthermore, such a system is implemented on a Raspberry Pi to make it portable\nand easy to use in remote regions without an Internet connection.",
          "link": "http://arxiv.org/abs/2106.10651",
          "publishedOn": "2021-06-22T01:57:09.836Z",
          "wordCount": 629,
          "title": "Implementing a Detection System for COVID-19 based on Lung Ultrasound Imaging and Deep Learning. (arXiv:2106.10651v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Ran Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zlateski_A/0/1/0/all/0/1\">Aleksandar Zlateski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seung_H/0/1/0/all/0/1\">H. Sebastian Seung</a>",
          "description": "Many approaches to 3D image segmentation are based on hierarchical clustering\nof supervoxels into image regions. Here we describe a distributed algorithm\ncapable of handling a tremendous number of supervoxels. The algorithm works\nrecursively, the regions are divided into chunks that are processed\nindependently in parallel by multiple workers. At each round of the recursive\nprocedure, the chunk size in all dimensions are doubled until a single chunk\nencompasses the entire image. The final result is provably independent of the\nchunking scheme, and the same as if the entire image were processed without\ndivision into chunks. This is nontrivial because a pair of adjacent regions is\nscored by some statistical property (e.g. mean or median) of the affinities at\nthe interface, and the interface may extend over arbitrarily many chunks. The\ntrick is to delay merge decisions for regions that touch chunk boundaries, and\nonly complete them in a later round after the regions are fully contained\nwithin a chunk. We demonstrate the algorithm by clustering an affinity graph\nwith over 1.5 trillion edges between 135 billion supervoxels derived from a 3D\nelectron microscopic brain image.",
          "link": "http://arxiv.org/abs/2106.10795",
          "publishedOn": "2021-06-22T01:57:09.829Z",
          "wordCount": 625,
          "title": "Large-scale image segmentation based on distributed clustering algorithms. (arXiv:2106.10795v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1\">Mengyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_H/0/1/0/all/0/1\">Haibin Hang</a>",
          "description": "We propose a manifold matching approach to generative models which includes a\ndistribution generator (or data generator) and a metric generator. In our\nframework, we view the real data set as some manifold embedded in a\nhigh-dimensional Euclidean space. The distribution generator aims at generating\nsamples that follow some distribution condensed around the real data manifold.\nIt is achieved by matching two sets of points using their geometric shape\ndescriptors, such as centroid and $p$-diameter, with learned distance metric;\nthe metric generator utilizes both real data and generated samples to learn a\ndistance metric which is close to some intrinsic geodesic distance on the real\ndata manifold. The produced distance metric is further used for manifold\nmatching. The two networks are learned simultaneously during the training\nprocess. We apply the approach on both unsupervised and supervised learning\ntasks: in unconditional image generation task, the proposed method obtains\ncompetitive results compared with existing generative models; in\nsuper-resolution task, we incorporate the framework in perception-based models\nand improve visual qualities by producing samples with more natural textures.\nBoth theoretical analysis and real data experiments guarantee the feasibility\nand effectiveness of the proposed framework.",
          "link": "http://arxiv.org/abs/2106.10777",
          "publishedOn": "2021-06-22T01:57:09.811Z",
          "wordCount": 626,
          "title": "Adversarial Manifold Matching via Deep Metric Learning for Generative Modeling. (arXiv:2106.10777v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jalwana_M/0/1/0/all/0/1\">Mohammad A. A. K. Jalwana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>",
          "description": "Backpropagation image saliency aims at explaining model predictions by\nestimating model-centric importance of individual pixels in the input. However,\nclass-insensitivity of the earlier layers in a network only allows saliency\ncomputation with low resolution activation maps of the deeper layers, resulting\nin compromised image saliency. Remedifying this can lead to sanity failures. We\npropose CAMERAS, a technique to compute high-fidelity backpropagation saliency\nmaps without requiring any external priors and preserving the map sanity. Our\nmethod systematically performs multi-scale accumulation and fusion of the\nactivation maps and backpropagated gradients to compute precise saliency maps.\nFrom accurate image saliency to articulation of relative importance of input\nfeatures for different models, and precise discrimination between model\nperception of visually similar objects, our high-resolution mapping offers\nmultiple novel insights into the black-box deep visual models, which are\npresented in the paper. We also demonstrate the utility of our saliency maps in\nadversarial setup by drastically reducing the norm of attack signals by\nfocusing them on the precise regions identified by our maps. Our method also\ninspires new evaluation metrics and a sanity check for this developing research\ndirection. Code is available here https://github.com/VisMIL/CAMERAS",
          "link": "http://arxiv.org/abs/2106.10649",
          "publishedOn": "2021-06-22T01:57:09.805Z",
          "wordCount": 652,
          "title": "CAMERAS: Enhanced Resolution And Sanity preserving Class Activation Mapping for image saliency. (arXiv:2106.10649v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1\">Kaiwen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gongjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jianxiong Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changgong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Feiying Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "Despite the great success of GANs in images translation with different\nconditioned inputs such as semantic segmentation and edge maps, generating\nhigh-fidelity realistic images with reference styles remains a grand challenge\nin conditional image-to-image translation. This paper presents a general image\ntranslation framework that incorporates optimal transport for feature alignment\nbetween conditional inputs and style exemplars in image translation. The\nintroduction of optimal transport mitigates the constraint of many-to-one\nfeature matching significantly while building up accurate semantic\ncorrespondences between conditional inputs and exemplars. We design a novel\nunbalanced optimal transport to address the transport between features with\ndeviational distributions which exists widely between conditional inputs and\nexemplars. In addition, we design a semantic-activation normalization scheme\nthat injects style features of exemplars into the image translation process\nsuccessfully. Extensive experiments over multiple image translation tasks show\nthat our method achieves superior image translation qualitatively and\nquantitatively as compared with the state-of-the-art.",
          "link": "http://arxiv.org/abs/2106.10482",
          "publishedOn": "2021-06-22T01:57:09.799Z",
          "wordCount": 600,
          "title": "Unbalanced Feature Transport for Exemplar-based Image Translation. (arXiv:2106.10482v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10437",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1\">Sieun Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_E/0/1/0/all/0/1\">Eunho Lee</a>",
          "description": "Super-resolution (SR) is a one-to-many task with multiple possible solutions.\nHowever, previous works were not concerned about this characteristic. For a\none-to-many pipeline, the generator should be able to generate multiple\nestimates of the reconstruction, and not be penalized for generating similar\nand equally realistic images. To achieve this, we propose adding weighted\npixel-wise noise after every Residual-in-Residual Dense Block (RRDB) to enable\nthe generator to generate various images. We modify the strict content loss to\nnot penalize the stochastic variation in reconstructed images as long as it has\nconsistent content. Additionally, we observe that there are out-of-focus\nregions in the DIV2K, DIV8K datasets that provide unhelpful guidelines. We\nfilter blurry regions in the training data using the method of [10]. Finally,\nwe modify the discriminator to receive the low-resolution image as a reference\nimage along with the target image to provide better feedback to the generator.\nUsing our proposed methods, we were able to improve the performance of ESRGAN\nin x4 perceptual SR and achieve the state-of-the-art LPIPS score in x16\nperceptual extreme SR.",
          "link": "http://arxiv.org/abs/2106.10437",
          "publishedOn": "2021-06-22T01:57:09.787Z",
          "wordCount": 613,
          "title": "One-to-many Approach for Improving Super-Resolution. (arXiv:2106.10437v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kazmierczak_S/0/1/0/all/0/1\">Stanis&#x142;aw Ka&#x17a;mierczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juszka_Z/0/1/0/all/0/1\">Zofia Juszka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fudalej_P/0/1/0/all/0/1\">Piotr Fudalej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandziuk_J/0/1/0/all/0/1\">Jacek Ma&#x144;dziuk</a>",
          "description": "First attempts of prediction of the facial growth (FG) direction were made\nover half of a century ago. Despite numerous attempts and elapsed time, a\nsatisfactory method has not been established yet and the problem still poses a\nchallenge for medical experts. To our knowledge, this paper is the first\nMachine Learning approach to the prediction of FG direction. Conducted data\nanalysis reveals the inherent complexity of the problem and explains the\nreasons of difficulty in FG direction prediction based on 2D X-ray images. To\nperform growth forecasting, we employ a wide range of algorithms, from logistic\nregression, through tree ensembles to neural networks and consider three,\nslightly different, problem formulations. The resulting classification accuracy\nvaries between 71% and 75%.",
          "link": "http://arxiv.org/abs/2106.10464",
          "publishedOn": "2021-06-22T01:57:09.780Z",
          "wordCount": 560,
          "title": "Prediction of the facial growth direction with Machine Learning methods. (arXiv:2106.10464v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Argho Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahnemoonfar_M/0/1/0/all/0/1\">Maryam Rahnemoonfar</a>",
          "description": "Visual Question Answering system integrated with Unmanned Aerial Vehicle\n(UAV) has a lot of potentials to advance the post-disaster damage assessment\npurpose. Providing assistance to affected areas is highly dependent on\nreal-time data assessment and analysis. Scope of the Visual Question Answering\nis to understand the scene and provide query related answer which certainly\nfaster the recovery process after any disaster. In this work, we address the\nimportance of \\textit{visual question answering (VQA)} task for post-disaster\ndamage assessment by presenting our recently developed VQA dataset called\n\\textit{HurMic-VQA} collected during hurricane Michael, and comparing the\nperformances of baseline VQA models.",
          "link": "http://arxiv.org/abs/2106.10548",
          "publishedOn": "2021-06-22T01:57:09.764Z",
          "wordCount": 540,
          "title": "VQA-Aid: Visual Question Answering for Post-Disaster Damage Assessment and Analysis. (arXiv:2106.10548v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "We study how to evaluate the quantitative information content of a region\nwithin an image for a particular label. To this end, we bridge class activation\nmaps with information theory. We develop an informative class activation map\n(infoCAM). Given a classification task, infoCAM depict how to accumulate\ninformation of partial regions to that of the entire image toward a label.\nThus, we can utilise infoCAM to locate the most informative features for a\nlabel. When applied to an image classification task, infoCAM performs better\nthan the traditional classification map in the weakly supervised object\nlocalisation task. We achieve state-of-the-art results on Tiny-ImageNet.",
          "link": "http://arxiv.org/abs/2106.10472",
          "publishedOn": "2021-06-22T01:57:09.759Z",
          "wordCount": 541,
          "title": "Informative Class Activation Maps. (arXiv:2106.10472v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wenyuan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyong Li</a>",
          "description": "A table arranging data in rows and columns is a very effective data\nstructure, which has been widely used in business and scientific research.\nConsidering large-scale tabular data in online and offline documents, automatic\ntable recognition has attracted increasing attention from the document analysis\ncommunity. Though human can easily understand the structure of tables, it\nremains a challenge for machines to understand that, especially due to a\nvariety of different table layouts and styles. Existing methods usually model a\ntable as either the markup sequence or the adjacency matrix between different\ntable cells, failing to address the importance of the logical location of table\ncells, e.g., a cell is located in the first row and the second column of the\ntable. In this paper, we reformulate the problem of table structure recognition\nas the table graph reconstruction, and propose an end-to-end trainable table\ngraph reconstruction network (TGRNet) for table structure recognition.\nSpecifically, the proposed method has two main branches, a cell detection\nbranch and a cell logical location branch, to jointly predict the spatial\nlocation and the logical location of different cells. Experimental results on\nthree popular table recognition datasets and a new dataset with table graph\nannotations (TableGraph-350K) demonstrate the effectiveness of the proposed\nTGRNet for table structure recognition. Code and annotations will be made\npublicly available.",
          "link": "http://arxiv.org/abs/2106.10598",
          "publishedOn": "2021-06-22T01:57:09.748Z",
          "wordCount": 664,
          "title": "TGRNet: A Table Graph Reconstruction Network for Table Structure Recognition. (arXiv:2106.10598v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianrui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qingjie Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun-Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlontzos_A/0/1/0/all/0/1\">Athanasios Vlontzos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>",
          "description": "Intelligent video summarization algorithms allow to quickly convey the most\nrelevant information in videos through the identification of the most essential\nand explanatory content while removing redundant video frames. In this paper,\nwe introduce the 3DST-UNet-RL framework for video summarization. A 3D\nspatio-temporal U-Net is used to efficiently encode spatio-temporal information\nof the input videos for downstream reinforcement learning (RL). An RL agent\nlearns from spatio-temporal latent scores and predicts actions for keeping or\nrejecting a video frame in a video summary. We investigate if real/inflated 3D\nspatio-temporal CNN features are better suited to learn representations from\nvideos than commonly used 2D image features. Our framework can operate in both,\na fully unsupervised mode and a supervised training mode. We analyse the impact\nof prescribed summary lengths and show experimental evidence for the\neffectiveness of 3DST-UNet-RL on two commonly used general video summarization\nbenchmarks. We also applied our method on a medical video summarization task.\nThe proposed video summarization method has the potential to save storage costs\nof ultrasound screening videos as well as to increase efficiency when browsing\npatient video data during retrospective analysis or audit without loosing\nessential information",
          "link": "http://arxiv.org/abs/2106.10528",
          "publishedOn": "2021-06-22T01:57:09.743Z",
          "wordCount": 635,
          "title": "Video Summarization through Reinforcement Learning with a 3D Spatio-Temporal U-Net. (arXiv:2106.10528v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaxiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xueming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Li Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "We aim to tackle the challenging yet practical scenery image outpainting task\nin this work. Recently, generative adversarial learning has significantly\nadvanced the image outpainting by producing semantic consistent content for the\ngiven image. However, the existing methods always suffer from the blurry\ntexture and the artifacts of the generative part, making the overall\noutpainting results lack authenticity. To overcome the weakness, this work\ninvestigates a principle way to synthesize texture-rich results by borrowing\npixels from its neighbors (\\ie, reference images), named\n\\textbf{Re}ference-\\textbf{G}uided \\textbf{O}utpainting (ReGO). Particularly,\nthe ReGO designs an Adaptive Content Selection (ACS) module to transfer the\npixel of reference images for texture compensating of the target one. To\nprevent the style of the generated part from being affected by the reference\nimages, a style ranking loss is further proposed to augment the ReGO to\nsynthesize style-consistent results. Extensive experiments on two popular\nbenchmarks, NS6K~\\cite{yangzx} and NS8K~\\cite{wang}, well demonstrate the\neffectiveness of our ReGO.",
          "link": "http://arxiv.org/abs/2106.10601",
          "publishedOn": "2021-06-22T01:57:09.728Z",
          "wordCount": 593,
          "title": "ReGO: Reference-Guided Outpainting for Scenery Image. (arXiv:2106.10601v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10506",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yichao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Person search has recently emerged as a challenging task that jointly\naddresses pedestrian detection and person re-identification. Existing\napproaches follow a fully supervised setting where both bounding box and\nidentity annotations are available. However, annotating identities is\nlabor-intensive, limiting the practicability and scalability of current\nframeworks. This paper inventively considers weakly supervised person search\nwith only bounding box annotations. We proposed the first framework to address\nthis novel task, namely Context-Guided Person Search (CGPS), by investigating\nthree levels of context clues (i.e., detection, memory and scene) in\nunconstrained natural images. The first two are employed to promote local and\nglobal discriminative capabilities, while the latter enhances clustering\naccuracy. Despite its simple design, our CGPS boosts the baseline model by 8.3%\nin mAP on CUHK-SYSU. Surprisingly, it even achieves comparable performance to\ntwo-step person search models, while displaying higher efficiency. Our code is\navailable at https://github.com/ljpadam/CGPS.",
          "link": "http://arxiv.org/abs/2106.10506",
          "publishedOn": "2021-06-22T01:57:09.722Z",
          "wordCount": 589,
          "title": "Exploring Visual Context for Weakly Supervised Person Search. (arXiv:2106.10506v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yijiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Wentian Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Ying Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiping Hu</a>",
          "description": "General segmentation models downsample images and then upsample to restore\nresolution for pixel level prediction. In such schema, upsample technique is\nvital in maintaining information for better performance. In this paper, we\npresent a new upsample approach, Attention Upsample (AU), that could serve as\ngeneral upsample method and be incorporated into any segmentation model that\npossesses lateral connections. AU leverages pixel-level attention to model long\nrange dependency and global information for better reconstruction. It consists\nof Attention Decoder (AD) and bilinear upsample as residual connection to\ncomplement the upsampled features. AD adopts the idea of decoder from\ntransformer which upsamples features conditioned on local and detailed\ninformation from contracting path. Moreover, considering the extensive memory\nand computation cost of pixel-level attention, we further propose to use window\nattention scheme to restrict attention computation in local windows instead of\nglobal range. Incorporating window attention, we denote our decoder as Window\nAttention Decoder (WAD) and our upsample method as Window Attention Upsample\n(WAU). We test our method on classic U-Net structure with lateral connection to\ndeliver information from contracting path and achieve state-of-the-arts\nperformance on Synapse (80.30 DSC and 23.12 HD) and MSD Brain (74.75 DSC)\ndatasets.",
          "link": "http://arxiv.org/abs/2106.10637",
          "publishedOn": "2021-06-22T01:57:09.717Z",
          "wordCount": 643,
          "title": "More than Encoder: Introducing Transformer Decoder to Upsample. (arXiv:2106.10637v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianyun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_J/0/1/0/all/0/1\">Jian Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1\">Xu Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yushi Zhu</a>",
          "description": "In this technical report, we introduce the methods of HIKVISION_LiDAR_Det in\nthe challenge of waymo open dataset real-time 3D detection. Our solution for\nthe competition are built upon Centerpoint 3D detection framework. Several\nvariants of CenterPoint are explored, including center attention head and\nfeature pyramid network neck. In order to achieve real time detection, methods\nlike batchnorm merge, half-precision floating point network and GPU-accelerated\nvoxelization process are adopted. By using these methods, our team ranks 6th\namong all the methods on real-time 3D detection challenge in the waymo open\ndataset.",
          "link": "http://arxiv.org/abs/2106.10493",
          "publishedOn": "2021-06-22T01:57:09.706Z",
          "wordCount": 524,
          "title": "CenterAtt: Fast 2-stage Center Attention Network. (arXiv:2106.10493v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10542",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jose_A/0/1/0/all/0/1\">Arun Jose</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Francis_A/0/1/0/all/0/1\">Abraham Francis</a>",
          "description": "Image compression using colour densities is historically impractical to\ndecompress losslessly. We examine the use of conditional generative adversarial\nnetworks in making this transformation more feasible, through learning a\nmapping between the images and a loss function to train on. We show that this\nmethod is effective at producing visually lossless generations, indicating that\nefficient colour compression is viable.",
          "link": "http://arxiv.org/abs/2106.10542",
          "publishedOn": "2021-06-22T01:57:09.689Z",
          "wordCount": 504,
          "title": "Reversible Colour Density Compression of Images using cGANs. (arXiv:2106.10542v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akiva_P/0/1/0/all/0/1\">Peri Akiva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dana_K/0/1/0/all/0/1\">Kristin Dana</a>",
          "description": "The costly process of obtaining semantic segmentation labels has driven\nresearch towards weakly supervised semantic segmentation (WSSS) methods, using\nonly image-level, point, or box labels. The lack of dense scene representation\nrequires methods to increase complexity to obtain additional semantic\ninformation about the scene, often done through multiple stages of training and\nrefinement. Current state-of-the-art (SOTA) models leverage image-level labels\nto produce class activation maps (CAMs) which go through multiple stages of\nrefinement before they are thresholded to make pseudo-masks for supervision.\nThe multi-stage approach is computationally expensive, and dependency on\nimage-level labels for CAMs generation lacks generalizability to more complex\nscenes. In contrary, our method offers a single-stage approach generalizable to\narbitrary dataset, that is trainable from scratch, without any dependency on\npre-trained backbones, classification, or separate refinement tasks. We utilize\npoint annotations to generate reliable, on-the-fly pseudo-masks through refined\nand filtered features. While our method requires point annotations that are\nonly slightly more expensive than image-level annotations, we are to\ndemonstrate SOTA performance on benchmark datasets (PascalVOC 2012), as well as\nsignificantly outperform other SOTA WSSS methods on recent real-world datasets\n(CRAID, CityPersons, IAD).",
          "link": "http://arxiv.org/abs/2106.10309",
          "publishedOn": "2021-06-22T01:57:09.684Z",
          "wordCount": 620,
          "title": "Towards Single Stage Weakly Supervised Semantic Segmentation. (arXiv:2106.10309v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fei_X/0/1/0/all/0/1\">Xiaohan Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Henry Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangyu Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheong_L/0/1/0/all/0/1\">Lin Lee Cheong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>",
          "description": "We propose a fully automated system that simultaneously estimates the camera\nintrinsics, the ground plane, and physical distances between people from a\nsingle RGB image or video captured by a camera viewing a 3-D scene from a fixed\nvantage point. To automate camera calibration and distance estimation, we\nleverage priors about human pose and develop a novel direct formulation for\npose-based auto-calibration and distance estimation, which shows\nstate-of-the-art performance on publicly available datasets. The proposed\napproach enables existing camera systems to measure physical distances without\nneeding a dedicated calibration process or range sensors, and is applicable to\na broad range of use cases such as social distancing and workplace safety.\nFurthermore, to enable evaluation and drive research in this area, we\ncontribute to the publicly available MEVA dataset with additional distance\nannotations, resulting in MEVADA -- the first evaluation benchmark in the world\nfor the pose-based auto-calibration and distance estimation problem.",
          "link": "http://arxiv.org/abs/2106.10335",
          "publishedOn": "2021-06-22T01:57:09.679Z",
          "wordCount": 591,
          "title": "Single View Physical Distance Estimation using Human Pose. (arXiv:2106.10335v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_B/0/1/0/all/0/1\">Brijesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sur_A/0/1/0/all/0/1\">Arijit Sur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1\">Pinaki Mitra</a>",
          "description": "In recent times, deep learning-based steganalysis classifiers became popular\ndue to their state-of-the-art performance. Most deep steganalysis classifiers\nusually extract noise residuals using high-pass filters as preprocessing steps\nand feed them to their deep model for classification. It is observed that\nrecent steganographic embedding does not always restrict their embedding in the\nhigh-frequency zone; instead, they distribute it as per embedding policy.\nTherefore, besides noise residual, learning the embedding zone is another\nchallenging task. In this work, unlike the conventional approaches, the\nproposed model first extracts the noise residual using learned denoising\nkernels to boost the signal-to-noise ratio. After preprocessing, the sparse\nnoise residuals are fed to a novel Multi-Contextual Convolutional Neural\nNetwork (M-CNET) that uses heterogeneous context size to learn the sparse and\nlow-amplitude representation of noise residuals. The model performance is\nfurther improved by incorporating the Self-Attention module to focus on the\nareas prone to steganalytic embedding. A set of comprehensive experiments is\nperformed to show the proposed scheme's efficacy over the prior arts. Besides,\nan ablation study is given to justify the contribution of various modules of\nthe proposed architecture.",
          "link": "http://arxiv.org/abs/2106.10430",
          "publishedOn": "2021-06-22T01:57:09.668Z",
          "wordCount": 657,
          "title": "Multi-Contextual Design of Convolutional Neural Network for Steganalysis. (arXiv:2106.10430v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goel_V/0/1/0/all/0/1\">Vidit Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Shubhika Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_H/0/1/0/all/0/1\">Harsh Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>",
          "description": "In this work we present a novel solution for Video Instance\nSegmentation(VIS), that is automatically generating instance level segmentation\nmasks along with object class and tracking them in a video. Our method improves\nthe masks from segmentation and propagation branches in an online manner using\nthe Mask Selection Network (MSN) hence limiting the noise accumulation during\nmask tracking. We propose an effective design of MSN by using patch-based\nconvolutional neural network. The network is able to distinguish between very\nsubtle differences between the masks and choose the better masks out of the\nassociated masks accurately. Further, we make use of temporal consistency and\nprocess the video sequences in both forward and reverse manner as a post\nprocessing step to recover lost objects. The proposed method can be used to\nadapt any video object segmentation method for the task of VIS. Our method\nachieves a score of 49.1 mAP on 2021 YouTube-VIS Challenge and was ranked third\nplace among more than 30 global teams. Our code will be available at\nhttps://github.com/SHI-Labs/Mask-Selection-Networks.",
          "link": "http://arxiv.org/abs/2106.10452",
          "publishedOn": "2021-06-22T01:57:09.648Z",
          "wordCount": 629,
          "title": "MSN: Efficient Online Mask Selection Network for Video Instance Segmentation. (arXiv:2106.10452v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingtao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yali Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengjin Wang</a>",
          "description": "Detection in large-scale scenes is a challenging problem due to small objects\nand extreme scale variation. It is essential to focus on the image regions of\nsmall objects. In this paper, we propose a novel Adaptive Zoom (AdaZoom)\nnetwork as a selective magnifier with flexible shape and focal length to\nadaptively zoom the focus regions for object detection. Based on policy\ngradient, we construct a reinforcement learning framework for focus region\ngeneration, with the reward formulated by object distributions. The scales and\naspect ratios of the generated regions are adaptive to the scales and\ndistribution of objects inside. We apply variable magnification according to\nthe scale of the region for adaptive multi-scale detection. We further propose\ncollaborative training to complementarily promote the performance of AdaZoom\nand the detection network. To validate the effectiveness, we conduct extensive\nexperiments on VisDrone2019, UAVDT, and DOTA datasets. The experiments show\nAdaZoom brings a consistent and significant improvement over different\ndetection networks, achieving state-of-the-art performance on these datasets,\nespecially outperforming the existing methods by AP of 4.64% on Vis-Drone2019.",
          "link": "http://arxiv.org/abs/2106.10409",
          "publishedOn": "2021-06-22T01:57:09.637Z",
          "wordCount": 614,
          "title": "AdaZoom: Adaptive Zoom Network for Multi-Scale Object Detection in Large Scenes. (arXiv:2106.10409v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atashgahi_Z/0/1/0/all/0/1\">Zahra Atashgahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Lu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kou_H/0/1/0/all/0/1\">Huanyu Kou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1\">Decebal Constantin Mocanu</a>",
          "description": "Works on lottery ticket hypothesis (LTH) and single-shot network pruning\n(SNIP) have raised a lot of attention currently on post-training pruning\n(iterative magnitude pruning), and before-training pruning (pruning at\ninitialization). The former method suffers from an extremely large computation\ncost and the latter category of methods usually struggles with insufficient\nperformance. In comparison, during-training pruning, a class of pruning methods\nthat simultaneously enjoys the training/inference efficiency and the comparable\nperformance, temporarily, has been less explored. To better understand\nduring-training pruning, we quantitatively study the effect of pruning\nthroughout training from the perspective of pruning plasticity (the ability of\nthe pruned networks to recover the original performance). Pruning plasticity\ncan help explain several other empirical observations about neural network\npruning in literature. We further find that pruning plasticity can be\nsubstantially improved by injecting a brain-inspired mechanism called\nneuroregeneration, i.e., to regenerate the same number of connections as\npruned. Based on the insights from pruning plasticity, we design a novel\ngradual magnitude pruning (GMP) method, named gradual pruning with zero-cost\nneuroregeneration (GraNet), and its dynamic sparse training (DST) variant\n(GraNet-ST). Both of them advance state of the art. Perhaps most impressively,\nthe latter for the first time boosts the sparse-to-sparse training performance\nover various dense-to-sparse methods by a large margin with ResNet-50 on\nImageNet. We will release all codes.",
          "link": "http://arxiv.org/abs/2106.10404",
          "publishedOn": "2021-06-22T01:57:09.632Z",
          "wordCount": 668,
          "title": "Sparse Training via Boosting Pruning Plasticity with Neuroregeneration. (arXiv:2106.10404v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Muhammad Monjurul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ruwen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhaozheng Yin</a>",
          "description": "To assist human drivers and autonomous vehicles in assessing crash risks,\ndriving scene analysis using dash cameras on vehicles and deep learning\nalgorithms is of paramount importance. Although these technologies are\nincreasingly available, driving scene analysis for this purpose still remains a\nchallenge. This is mainly due to the lack of annotated large image datasets for\nanalyzing crash risk indicators and crash likelihood, and the lack of an\neffective method to extract lots of required information from complex driving\nscenes. To fill the gap, this paper develops a scene analysis system. The\nMulti-Net of the system includes two multi-task neural networks that perform\nscene classification to provide four labels for each scene. The DeepLab v3 and\nYOLO v3 are combined by the system to detect and locate risky pedestrians and\nthe nearest vehicles. All identified information can provide the situational\nawareness to autonomous vehicles or human drivers for identifying crash risks\nfrom the surrounding traffic. To address the scarcity of annotated image\ndatasets for studying traffic crashes, two completely new datasets have been\ndeveloped by this paper and made available to the public, which were proved to\nbe effective in training the proposed deep neural networks. The paper further\nevaluates the performance of the Multi-Net and the efficiency of the developed\nsystem. Comprehensive scene analysis is further illustrated with representative\nexamples. Results demonstrate the effectiveness of the developed system and\ndatasets for driving scene analysis, and their supportiveness for crash risk\nassessment and crash prevention.",
          "link": "http://arxiv.org/abs/2106.10319",
          "publishedOn": "2021-06-22T01:57:09.592Z",
          "wordCount": 723,
          "title": "A system of vision sensor based deep neural networks for complex driving scene analysis in support of crash risk assessment and prevention. (arXiv:2106.10319v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haris_M/0/1/0/all/0/1\">Muhamad Amin Husni Abdul Haris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Sin Liang Lim</a>",
          "description": "This study is to investigate and compare the facial recognition accuracy\nperformance of Dlib ResNet against a K-Nearest Neighbour (KNN) classifier.\nParticularly when used against a dataset from an Asian ethnicity as Dlib ResNet\nwas reported to have an accuracy deficiency when it comes to Asian faces. The\ncomparisons are both implemented on the facial vectors extracted using the\nHistogram of Oriented Gradients (HOG) method and use the same dataset for a\nfair comparison. Authentication of a user by facial recognition in an electric\nvehicle (EV) charging station demonstrates a practical use case for such an\nauthentication system.",
          "link": "http://arxiv.org/abs/2106.10432",
          "publishedOn": "2021-06-22T01:57:09.584Z",
          "wordCount": 555,
          "title": "Neural Network Facial Authentication for Public Electric Vehicle Charging Station. (arXiv:2106.10432v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Postels_J/0/1/0/all/0/1\">Janis Postels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengya Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spezialetti_R/0/1/0/all/0/1\">Riccardo Spezialetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "Recently normalizing flows (NFs) have demonstrated state-of-the-art\nperformance on modeling 3D point clouds while allowing sampling with arbitrary\nresolution at inference time. However, these flow-based models still require\nlong training times and large models for representing complicated geometries.\nThis work enhances their representational power by applying mixtures of NFs to\npoint clouds. We show that in this more general framework each component learns\nto specialize in a particular subregion of an object in a completely\nunsupervised fashion. By instantiating each mixture component with a\ncomparatively small NF we generate point clouds with improved details compared\nto single-flow-based models while using fewer parameters and considerably\nreducing the inference runtime. We further demonstrate that by adding data\naugmentation, individual mixture components can learn to specialize in a\nsemantically meaningful manner. We evaluate mixtures of NFs on generation,\nautoencoding and single-view reconstruction based on the ShapeNet dataset.",
          "link": "http://arxiv.org/abs/2106.03135",
          "publishedOn": "2021-06-21T02:07:40.727Z",
          "wordCount": 612,
          "title": "Go with the Flows: Mixtures of Normalizing Flows for Point Cloud Generation and Reconstruction. (arXiv:2106.03135v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jinka_S/0/1/0/all/0/1\">Sai Sagar Jinka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chacko_R/0/1/0/all/0/1\">Rohan Chacko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Astitva Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Avinash Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_P/0/1/0/all/0/1\">P.J. Narayanan</a>",
          "description": "3D human body reconstruction from monocular images is an interesting and\nill-posed problem in computer vision with wider applications in multiple\ndomains. In this paper, we propose SHARP, a novel end-to-end trainable network\nthat accurately recovers the detailed geometry and appearance of 3D people in\nloose clothing from a monocular image. We propose a sparse and efficient fusion\nof a parametric body prior with a non-parametric peeled depth map\nrepresentation of clothed models. The parametric body prior constraints our\nmodel in two ways: first, the network retains geometrically consistent body\nparts that are not occluded by clothing, and second, it provides a body shape\ncontext that improves prediction of the peeled depth maps. This enables SHARP\nto recover fine-grained 3D geometrical details with just L1 losses on the 2D\nmaps, given an input image. We evaluate SHARP on publicly available Cloth3D and\nTHuman datasets and report superior performance to state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2106.04778",
          "publishedOn": "2021-06-21T02:07:39.749Z",
          "wordCount": 606,
          "title": "SHARP: Shape-Aware Reconstruction of People In Loose Clothing. (arXiv:2106.04778v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunlong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhehan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wenao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xinghao Ding</a>",
          "description": "Retinal artery/vein (A/V) classification is a critical technique for\ndiagnosing diabetes and cardiovascular diseases. Although deep learning based\nmethods achieve impressive results in A/V classification, their performances\nusually degrade severely when being directly applied to another database, due\nto the domain shift, e.g., caused by the variations in imaging protocols. In\nthis paper, we propose a novel vessel-mixing based consistency regularization\nframework, for cross-domain learning in retinal A/V classification. Specially,\nto alleviate the severe bias to source domain, based on the label smooth prior,\nthe model is regularized to give consistent predictions for unlabeled\ntarget-domain inputs that are under perturbation. This consistency\nregularization implicitly introduces a mechanism where the model and the\nperturbation is opponent to each other, where the model is pushed to be robust\nenough to cope with the perturbation. Thus, we investigate a more difficult\nopponent to further inspire the robustness of model, in the scenario of retinal\nA/V, called vessel-mixing perturbation. Specially, it effectively disturbs the\nfundus images especially the vessel structures by mixing two images regionally.\nWe conduct extensive experiments on cross-domain A/V classification using four\npublic datasets, which are collected by diverse institutions and imaging\ndevices. The results demonstrate that our method achieves the state-of-the-art\ncross-domain performance, which is also close to the upper bound obtained by\nfully supervised learning on target domain.",
          "link": "http://arxiv.org/abs/2103.09097",
          "publishedOn": "2021-06-21T02:07:39.678Z",
          "wordCount": 698,
          "title": "Consistent Posterior Distributions under Vessel-Mixing: A Regularization for Cross-Domain Retinal Artery/Vein Classification. (arXiv:2103.09097v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1\">Nikola Popovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Probst_T/0/1/0/all/0/1\">Thomas Probst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guolei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "We define the concept of CompositeTasking as the fusion of multiple,\nspatially distributed tasks, for various aspects of image understanding.\nLearning to perform spatially distributed tasks is motivated by the frequent\navailability of only sparse labels across tasks, and the desire for a compact\nmulti-tasking network. To facilitate CompositeTasking, we introduce a novel\ntask conditioning model -- a single encoder-decoder network that performs\nmultiple, spatially varying tasks at once. The proposed network takes an image\nand a set of pixel-wise dense task requests as inputs, and performs the\nrequested prediction task for each pixel. Moreover, we also learn the\ncomposition of tasks that needs to be performed according to some\nCompositeTasking rules, which includes the decision of where to apply which\ntask. It not only offers us a compact network for multi-tasking, but also\nallows for task-editing. Another strength of the proposed method is\ndemonstrated by only having to supply sparse supervision per task. The obtained\nresults are on par with our baselines that use dense supervision and a\nmulti-headed multi-tasking design. The source code will be made publicly\navailable at www.github.com/nikola3794/composite-tasking.",
          "link": "http://arxiv.org/abs/2012.09030",
          "publishedOn": "2021-06-21T02:07:39.653Z",
          "wordCount": 655,
          "title": "CompositeTasking: Understanding Images by Spatial Composition of Tasks. (arXiv:2012.09030v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.01496",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shuo Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fanglan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chang-Tien Lu</a>",
          "description": "Despite the great progress made by deep neural networks in the semantic\nsegmentation task, traditional neural-networkbased methods typically suffer\nfrom a shortage of large amounts of pixel-level annotations. Recent progress in\nfewshot semantic segmentation tackles the issue by only a few pixel-level\nannotated examples. However, these few-shot approaches cannot easily be applied\nto multi-way or weak annotation settings. In this paper, we advance the\nfew-shot segmentation paradigm towards a scenario where image-level annotations\nare available to help the training process of a few pixel-level annotations.\nOur key idea is to learn a better prototype representation of the class by\nfusing the knowledge from the image-level labeled data. Specifically, we\npropose a new framework, called PAIA, to learn the class prototype\nrepresentation in a metric space by integrating image-level annotations.\nFurthermore, by considering the uncertainty of pseudo-masks, a distilled soft\nmasked average pooling strategy is designed to handle distractions in\nimage-level annotations. Extensive empirical results on two datasets show\nsuperior performance of PAIA.",
          "link": "http://arxiv.org/abs/2007.01496",
          "publishedOn": "2021-06-21T02:07:39.582Z",
          "wordCount": 644,
          "title": "Few-Shot Semantic Segmentation Augmented with Image-Level Weak Annotations. (arXiv:2007.01496v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.01607",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1\">Tian Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1\">Agisilaos Chartsias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>",
          "description": "Pseudo-healthy synthesis is the task of creating a subject-specific `healthy'\nimage from a pathological one. Such images can be helpful in tasks such as\nanomaly detection and understanding changes induced by pathology and disease.\nIn this paper, we present a model that is encouraged to disentangle the\ninformation of pathology from what seems to be healthy. We disentangle what\nappears to be healthy and where disease is as a segmentation map, which are\nthen recombined by a network to reconstruct the input disease image. We train\nour models adversarially using either paired or unpaired settings, where we\npair disease images and maps when available. We quantitatively and\nsubjectively, with a human study, evaluate the quality of pseudo-healthy images\nusing several criteria. We show in a series of experiments, performed on ISLES,\nBraTS and Cam-CAN datasets, that our method is better than several baselines\nand methods from the literature. We also show that due to better training\nprocesses we could recover deformations, on surrounding tissue, caused by\ndisease. Our implementation is publicly available at\nhttps://github.com/xiat0616/pseudo-healthy-synthesis. This paper has been\naccepted by Medical Image Analysis:\nhttps://doi.org/10.1016/j.media.2020.101719.",
          "link": "http://arxiv.org/abs/2005.01607",
          "publishedOn": "2021-06-21T02:07:39.155Z",
          "wordCount": 674,
          "title": "Pseudo-healthy synthesis with pathology disentanglement and adversarial learning. (arXiv:2005.01607v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scribano_C/0/1/0/all/0/1\">Carmelo Scribano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapienza_D/0/1/0/all/0/1\">Davide Sapienza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franchini_G/0/1/0/all/0/1\">Giorgia Franchini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verucchi_M/0/1/0/all/0/1\">Micaela Verucchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertogna_M/0/1/0/all/0/1\">Marko Bertogna</a>",
          "description": "Combining Natural Language with Vision represents a unique and interesting\nchallenge in the domain of Artificial Intelligence. The AI City Challenge Track\n5 for Natural Language-Based Vehicle Retrieval focuses on the problem of\ncombining visual and textual information, applied to a smart-city use case. In\nthis paper, we present All You Can Embed (AYCE), a modular solution to\ncorrelate single-vehicle tracking sequences with natural language. The main\nbuilding blocks of the proposed architecture are (i) BERT to provide an\nembedding of the textual descriptions, (ii) a convolutional backbone along with\na Transformer model to embed the visual information. For the training of the\nretrieval model, a variation of the Triplet Margin Loss is proposed to learn a\ndistance measure between the visual and language embeddings. The code is\npublicly available at https://github.com/cscribano/AYCE_2021.",
          "link": "http://arxiv.org/abs/2106.10153",
          "publishedOn": "2021-06-21T02:07:38.907Z",
          "wordCount": 590,
          "title": "All You Can Embed: Natural Language based Vehicle Retrieval with Spatio-Temporal Transformers. (arXiv:2106.10153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1\">Andreas Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1\">Alexander Kolesnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wightman_R/0/1/0/all/0/1\">Ross Wightman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1\">Jakob Uszkoreit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>",
          "description": "Vision Transformers (ViT) have been shown to attain highly competitive\nperformance for a wide range of vision applications, such as image\nclassification, object detection and semantic image segmentation. In comparison\nto convolutional neural networks, the Vision Transformer's weaker inductive\nbias is generally found to cause an increased reliance on model regularization\nor data augmentation (``AugReg'' for short) when training on smaller training\ndatasets. We conduct a systematic empirical study in order to better understand\nthe interplay between the amount of training data, AugReg, model size and\ncompute budget. As one result of this study we find that the combination of\nincreased compute and AugReg can yield models with the same performance as\nmodels trained on an order of magnitude more training data: we train ViT models\nof various sizes on the public ImageNet-21k dataset which either match or\noutperform their counterparts trained on the larger, but not publicly available\nJFT-300M dataset.",
          "link": "http://arxiv.org/abs/2106.10270",
          "publishedOn": "2021-06-21T02:07:38.882Z",
          "wordCount": 649,
          "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers. (arXiv:2106.10270v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chongzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanghang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Daisheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>",
          "description": "Recently, Vision Transformers (ViTs) have achieved impressive results on\nvarious vision tasks. Yet, their generalization ability under different\ndistribution shifts is rarely understood. In this work, we provide a\ncomprehensive study on the out-of-distribution generalization of ViTs. To\nsupport a systematic investigation, we first present a taxonomy of distribution\nshifts by categorizing them into five conceptual groups: corruption shift,\nbackground shift, texture shift, destruction shift, and style shift. Then we\nperform extensive evaluations of ViT variants under different groups of\ndistribution shifts and compare their generalization ability with CNNs. Several\nimportant observations are obtained: 1) ViTs generalize better than CNNs under\nmultiple distribution shifts. With the same or fewer parameters, ViTs are ahead\nof corresponding CNNs by more than 5% in top-1 accuracy under most distribution\nshifts. 2) Larger ViTs gradually narrow the in-distribution and\nout-of-distribution performance gap. To further improve the generalization of\nViTs, we design the Generalization-Enhanced ViTs by integrating adversarial\nlearning, information theory, and self-supervised learning. By investigating\nthree types of generalization-enhanced ViTs, we observe their\ngradient-sensitivity and design a smoother learning strategy to achieve a\nstable training process. With modified training schemes, we achieve\nimprovements on performance towards out-of-distribution data by 4% from vanilla\nViTs. We comprehensively compare three generalization-enhanced ViTs with their\ncorresponding CNNs, and observe that: 1) For the enhanced model, larger ViTs\nstill benefit more for the out-of-distribution generalization. 2)\ngeneralization-enhanced ViTs are more sensitive to the hyper-parameters than\ncorresponding CNNs. We hope our comprehensive study could shed light on the\ndesign of more generalizable learning architectures.",
          "link": "http://arxiv.org/abs/2106.07617",
          "publishedOn": "2021-06-21T02:07:38.810Z",
          "wordCount": 734,
          "title": "Delving Deep into the Generalization of Vision Transformers under Distribution Shifts. (arXiv:2106.07617v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.03369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Daqing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1\">Minghua Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>",
          "description": "Nearest neighbor search is to find the data points in the database such that\nthe distances from them to the query are the smallest, which is a fundamental\nproblem in various domains, such as computer vision, recommendation systems and\nmachine learning. Hashing is one of the most widely used methods for its\ncomputational and storage efficiency. With the development of deep learning,\ndeep hashing methods show more advantages than traditional methods. In this\npaper, we present a comprehensive survey of the deep hashing algorithms.\nSpecifically, we categorize deep supervised hashing methods into pairwise\nsimilarity preserving, multiwise similarity preserving, implicit similarity\npreserving, classification-oriented preserving as well as quantization\naccording to the manners of preserving the similarities. In addition, we also\nintroduce some other topics such as deep unsupervised hashing and multi-modal\ndeep hashing methods. Meanwhile, we also present some commonly used public\ndatasets and the scheme to measure the performance of deep hashing algorithms.\nFinally, we discussed some potential research directions in conclusion.",
          "link": "http://arxiv.org/abs/2003.03369",
          "publishedOn": "2021-06-21T02:07:38.792Z",
          "wordCount": 634,
          "title": "A Survey on Deep Hashing Methods. (arXiv:2003.03369v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1\">Marcos Vendramini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1\">Hugo Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1\">Alexei Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>",
          "description": "Image classification methods are usually trained to perform predictions\ntaking into account a predefined group of known classes. Real-world problems,\nhowever, may not allow for a full knowledge of the input and label spaces,\nmaking failures in recognition a hazard to deep visual learning. Open set\nrecognition methods are characterized by the ability to correctly identifying\ninputs of known and unknown classes. In this context, we propose GeMOS: simple\nand plug-and-play open set recognition modules that can be attached to\npretrained Deep Neural Networks for visual recognition. The GeMOS framework\npairs pre-trained Convolutional Neural Networks with generative models for open\nset recognition to extract open set scores for each sample, allowing for\nfailure recognition in object recognition tasks. We conduct a thorough\nevaluation of the proposed method in comparison with state-of-the-art open set\nalgorithms, finding that GeMOS either outperforms or is statistically\nindistinguishable from more complex and costly models.",
          "link": "http://arxiv.org/abs/2105.10013",
          "publishedOn": "2021-06-21T02:07:38.785Z",
          "wordCount": 617,
          "title": "Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05496",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aksoy_A/0/1/0/all/0/1\">Ahmet Kerem Aksoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravanbakhsh_M/0/1/0/all/0/1\">Mahdyar Ravanbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreuziger_T/0/1/0/all/0/1\">Tristan Kreuziger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1\">Begum Demir</a>",
          "description": "Collecting a large number of reliable training images annotated by multiple\nland-cover class labels in the framework of multi-label classification is\ntime-consuming and costly in remote sensing (RS). To address this problem,\npublicly available thematic products are often used for annotating RS images\nwith zero-labeling-cost. However, such an approach may result in constructing a\ntraining set with noisy multi-labels, distorting the learning process. To\naddress this problem, we propose a Consensual Collaborative Multi-Label\nLearning (CCML) method. The proposed CCML identifies, ranks and corrects\ntraining images with noisy multi-labels through four main modules: 1)\ndiscrepancy module; 2) group lasso module; 3) flipping module; and 4) swap\nmodule. The discrepancy module ensures that the two networks learn diverse\nfeatures, while obtaining the same predictions. The group lasso module detects\nthe potentially noisy labels by estimating the label uncertainty based on the\naggregation of two collaborative networks. The flipping module corrects the\nidentified noisy labels, whereas the swap module exchanges the ranking\ninformation between the two networks. The experimental results confirm the\nsuccess of the proposed CCML under high (synthetically added) multi-label noise\nrates. The code of the proposed method is publicly available at\nhttps://noisy-labels-in-rs.org",
          "link": "http://arxiv.org/abs/2105.05496",
          "publishedOn": "2021-06-21T02:07:38.778Z",
          "wordCount": 687,
          "title": "A Consensual Collaborative Learning Method for Remote Sensing Image Classification Under Noisy Multi-Labels. (arXiv:2105.05496v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yifan_W/0/1/0/all/0/1\">Wang Yifan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmann_L/0/1/0/all/0/1\">Lukas Rahmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorkine_Hornung_O/0/1/0/all/0/1\">Olga Sorkine-Hornung</a>",
          "description": "We present implicit displacement fields, a novel representation for detailed\n3D geometry. Inspired by a classic surface deformation technique, displacement\nmapping, our method represents a complex surface as a smooth base surface plus\na displacement along the base's normal directions, resulting in a\nfrequency-based shape decomposition, where the high frequency signal is\nconstrained geometrically by the low frequency signal. Importantly, this\ndisentanglement is unsupervised thanks to a tailored architectural design that\nhas an innate frequency hierarchy by construction. We explore implicit\ndisplacement field surface reconstruction and detail transfer and demonstrate\nsuperior representational power, training stability and generalizability.",
          "link": "http://arxiv.org/abs/2106.05187",
          "publishedOn": "2021-06-21T02:07:38.756Z",
          "wordCount": 567,
          "title": "Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields. (arXiv:2106.05187v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaolong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qimeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>",
          "description": "Temporal action detection (TAD) aims to determine the semantic label and the\nboundaries of every action instance in an untrimmed video. It is a fundamental\ntask in video understanding and significant progress has been made in TAD.\nPrevious methods involve multiple stages or networks and hand-designed rules or\noperations, which fall short in efficiency and flexibility. Here, we construct\nan end-to-end framework for TAD upon Transformer, termed \\textit{TadTR}, which\nsimultaneously predicts all action instances as a set of labels and temporal\nlocations in parallel. TadTR is able to adaptively extract temporal context\ninformation needed for making action predictions, by selectively attending to a\nnumber of snippets in a video. It greatly simplifies the pipeline of TAD and\nruns much faster than previous detectors. Our method achieves state-of-the-art\nperformance on HACS Segments and THUMOS14 and competitive performance on\nActivityNet-1.3. Our code will be made available at\n\\url{https://github.com/xlliu7/TadTR}.",
          "link": "http://arxiv.org/abs/2106.10271",
          "publishedOn": "2021-06-21T02:07:38.741Z",
          "wordCount": 583,
          "title": "End-to-end Temporal Action Detection with Transformer. (arXiv:2106.10271v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_C/0/1/0/all/0/1\">Chintan Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liapis_A/0/1/0/all/0/1\">Antonios Liapis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1\">Georgios N. Yannakakis</a>",
          "description": "Representing games through their pixels offers a promising approach for\nbuilding general-purpose and versatile game models. While games are not merely\nimages, neural network models trained on game pixels often capture differences\nof the visual style of the image rather than the content of the game. As a\nresult, such models cannot generalize well even within similar games of the\nsame genre. In this paper we build on recent advances in contrastive learning\nand showcase its benefits for representation learning in games. Learning to\ncontrast images of games not only classifies games in a more efficient manner;\nit also yields models that separate games in a more meaningful fashion by\nignoring the visual style and focusing, instead, on their content. Our results\nin a large dataset of sports video games containing 100k images across 175\ngames and 10 game genres suggest that contrastive learning is better suited for\nlearning generalized game representations compared to conventional supervised\nlearning. The findings of this study bring us closer to universal visual\nencoders for games that can be reused across previously unseen games without\nrequiring retraining or fine-tuning.",
          "link": "http://arxiv.org/abs/2106.10060",
          "publishedOn": "2021-06-21T02:07:38.710Z",
          "wordCount": 625,
          "title": "Contrastive Learning of Generalized Game Representations. (arXiv:2106.10060v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neverova_N/0/1/0/all/0/1\">Natalia Neverova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanakoyeu_A/0/1/0/all/0/1\">Artsiom Sanakoyeu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labatut_P/0/1/0/all/0/1\">Patrick Labatut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novotny_D/0/1/0/all/0/1\">David Novotny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>",
          "description": "We tackle the problem of learning the geometry of multiple categories of\ndeformable objects jointly. Recent work has shown that it is possible to learn\na unified dense pose predictor for several categories of related objects.\nHowever, training such models requires to initialize inter-category\ncorrespondences by hand. This is suboptimal and the resulting models fail to\nmaintain correct correspondences as individual categories are learned. In this\npaper, we show that improved correspondences can be learned automatically as a\nnatural byproduct of learning category-specific dense pose predictors. To do\nthis, we express correspondences between different categories and between\nimages and categories using a unified embedding. Then, we use the latter to\nenforce two constraints: symmetric inter-category cycle consistency and a new\nasymmetric image-to-category cycle consistency. Without any manual annotations\nfor the inter-category correspondences, we obtain state-of-the-art alignment\nresults, outperforming dedicated methods for matching 3D shapes. Moreover, the\nnew model is also better at the task of dense pose prediction than prior work.",
          "link": "http://arxiv.org/abs/2106.09758",
          "publishedOn": "2021-06-21T02:07:38.680Z",
          "wordCount": 610,
          "title": "Discovering Relationships between Object Categories via Universal Canonical Maps. (arXiv:2106.09758v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jiabao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yi Ma</a>",
          "description": "Reconstruction of object or scene surfaces has tremendous applications in\ncomputer vision, computer graphics, and robotics. In this paper, we study a\nfundamental problem in this context about recovering a surface mesh from an\nimplicit field function whose zero-level set captures the underlying surface.\nTo achieve the goal, existing methods rely on traditional meshing algorithms;\nwhile promising, they suffer from loss of precision learned in the implicit\nsurface networks, due to the use of discrete space sampling in marching cubes.\nGiven that an MLP with activations of Rectified Linear Unit (ReLU) partitions\nits input space into a number of linear regions, we are motivated to connect\nthis local linearity with a same property owned by the desired result of\npolygon mesh. More specifically, we identify from the linear regions,\npartitioned by an MLP based implicit function, the analytic cells and analytic\nfaces that are associated with the function's zero-level isosurface. We prove\nthat under mild conditions, the identified analytic faces are guaranteed to\nconnect and form a closed, piecewise planar surface. Based on the theorem, we\npropose an algorithm of analytic marching, which marches among analytic cells\nto exactly recover the mesh captured by an implicit surface network. We also\nshow that our theory and algorithm are equally applicable to advanced MLPs with\nshortcut connections and max pooling. Given the parallel nature of analytic\nmarching, we contribute AnalyticMesh, a software package that supports\nefficient meshing of implicit surface networks via CUDA parallel computing, and\nmesh simplification for efficient downstream processing. We apply our method to\ndifferent settings of generative shape modeling using implicit surface\nnetworks. Extensive experiments demonstrate our advantages over existing\nmethods in terms of both meshing accuracy and efficiency.",
          "link": "http://arxiv.org/abs/2106.10031",
          "publishedOn": "2021-06-21T02:07:38.672Z",
          "wordCount": 740,
          "title": "Learning and Meshing from Deep Implicit Surface Networks Using an Efficient Implementation of Analytic Marching. (arXiv:2106.10031v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_A/0/1/0/all/0/1\">Alireza Ahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halstead_M/0/1/0/all/0/1\">Michael Halstead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCool_C/0/1/0/all/0/1\">Chris McCool</a>",
          "description": "This paper explores the potential for performing temporal semantic\nsegmentation in the context of agricultural robotics without temporally\nlabelled data. We achieve this by proposing to generate virtual temporal\nsamples from labelled still images. This allows us, with no extra annotation\neffort, to generate virtually labelled temporal sequences. Normally, to train a\nrecurrent neural network (RNN), labelled samples from a video (temporal)\nsequence are required which is laborious and has stymied work in this\ndirection. By generating virtual temporal samples, we demonstrate that it is\npossible to train a lightweight RNN to perform semantic segmentation on two\nchallenging agricultural datasets. Our results show that by training a temporal\nsemantic segmenter using virtual samples we can increase the performance by an\nabsolute amount of 4.6 and 4.9 on sweet pepper and sugar beet datasets,\nrespectively. This indicates that our virtual data augmentation technique is\nable to accurately classify agricultural images temporally without the use of\ncomplicated synthetic data generation techniques nor with the overhead of\nlabelling large amounts of temporal sequences.",
          "link": "http://arxiv.org/abs/2106.10118",
          "publishedOn": "2021-06-21T02:07:38.655Z",
          "wordCount": 616,
          "title": "Virtual Temporal Samples for Recurrent Neural Networks: applied to semantic segmentation in agriculture. (arXiv:2106.10118v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chengwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaohui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_R/0/1/0/all/0/1\">Ruizhi Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>",
          "description": "Novelty detection is the process of determining whether a query example\ndiffers from the learned training distribution. Previous methods attempt to\nlearn the representation of the normal samples via generative adversarial\nnetworks (GANs). However, they will suffer from instability training, mode\ndropping, and low discriminative ability. Recently, various pretext tasks (e.g.\nrotation prediction and clustering) have been proposed for self-supervised\nlearning in novelty detection. However, the learned latent features are still\nlow discriminative. We overcome such problems by introducing a novel\ndecoder-encoder framework. Firstly, a generative network (a.k.a. decoder)\nlearns the representation by mapping the initialized latent vector to an image.\nIn particular, this vector is initialized by considering the entire\ndistribution of training data to avoid the problem of mode-dropping. Secondly,\na contrastive network (a.k.a. encoder) aims to ``learn to compare'' through\nmutual information estimation, which directly helps the generative network to\nobtain a more discriminative representation by using a negative data\naugmentation strategy. Extensive experiments show that our model has\nsignificant superiority over cutting-edge novelty detectors and achieves new\nstate-of-the-art results on some novelty detection benchmarks, e.g. CIFAR10 and\nDCASE. Moreover, our model is more stable for training in a non-adversarial\nmanner, compared to other adversarial based novelty detection methods.",
          "link": "http://arxiv.org/abs/2106.09958",
          "publishedOn": "2021-06-21T02:07:38.648Z",
          "wordCount": 655,
          "title": "Novelty Detection via Contrastive Learning with Negative Data Augmentation. (arXiv:2106.09958v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Nanqing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maggioni_M/0/1/0/all/0/1\">Matteo Maggioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Pellitero_E/0/1/0/all/0/1\">Eduardo P&#xe9;rez-Pellitero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonagh_S/0/1/0/all/0/1\">Steven McDonagh</a>",
          "description": "The breakthrough of contrastive learning (CL) has fueled the recent success\nof self-supervised learning (SSL) in high-level vision tasks on RGB images.\nHowever, CL is still ill-defined for low-level vision tasks, such as joint\ndemosaicking and denoising (JDD), in the RAW domain. To bridge this\nmethodological gap, we present a novel CL approach on RAW images, residual\ncontrastive learning (RCL), which aims to learn meaningful representations for\nJDD. Our work is built on the assumption that noise contained in each RAW image\nis signal-dependent, thus two crops from the same RAW image should have more\nsimilar noise distribution than two crops from different RAW images. We use\nresiduals as a discriminative feature and the earth mover's distance to measure\nthe distribution divergence for the contrastive loss. To evaluate the proposed\nCL strategy, we simulate a series of unsupervised JDD experiments with\nlarge-scale data corrupted by synthetic signal-dependent noise, where we set a\nnew benchmark for unsupervised JDD tasks with unknown (random) noise variance.\nOur empirical study not only validates that CL can be applied on distributions\n(c.f. features), but also exposes the lack of robustness of previous non-ML and\nSSL JDD methods when the statistics of the noise are unknown, thus providing\nsome further insight into signal-dependent noise problems.",
          "link": "http://arxiv.org/abs/2106.10070",
          "publishedOn": "2021-06-21T02:07:38.640Z",
          "wordCount": 653,
          "title": "Residual Contrastive Learning for Joint Demosaicking and Denoising. (arXiv:2106.10070v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1\">Jiahong Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_E/0/1/0/all/0/1\">Edith V Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfefferbaum_A/0/1/0/all/0/1\">Adolf Pfefferbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharchuk_G/0/1/0/all/0/1\">Greg Zaharchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M Pohl</a>",
          "description": "Longitudinal MRIs are often used to capture the gradual deterioration of\nbrain structure and function caused by aging or neurological diseases.\nAnalyzing this data via machine learning generally requires a large number of\nground-truth labels, which are often missing or expensive to obtain. Reducing\nthe need for labels, we propose a self-supervised strategy for representation\nlearning named Longitudinal Neighborhood Embedding (LNE). Motivated by concepts\nin contrastive learning, LNE explicitly models the similarity between\ntrajectory vectors across different subjects. We do so by building a graph in\neach training iteration defining neighborhoods in the latent space so that the\nprogression direction of a subject follows the direction of its neighbors. This\nresults in a smooth trajectory field that captures the global morphological\nchange of the brain while maintaining the local continuity. We apply LNE to\nlongitudinal T1w MRIs of two neuroimaging studies: a dataset composed of 274\nhealthy subjects, and Alzheimer's Disease Neuroimaging Initiative (ADNI,\nN=632). The visualization of the smooth trajectory vector field and superior\nperformance on downstream tasks demonstrate the strength of the proposed method\nover existing self-supervised methods in extracting information associated with\nnormal aging and in revealing the impact of neurodegenerative disorders. The\ncode is available at\n\\url{https://github.com/ouyangjiahong/longitudinal-neighbourhood-embedding.git}.",
          "link": "http://arxiv.org/abs/2103.03840",
          "publishedOn": "2021-06-21T02:07:38.633Z",
          "wordCount": 692,
          "title": "Self-Supervised Longitudinal Neighbourhood Embedding. (arXiv:2103.03840v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_V/0/1/0/all/0/1\">Veronika A. Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia A. Schnabel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>",
          "description": "Late gadolinium enhancement magnetic resonance imaging (LGE MRI) is commonly\nused to visualize and quantify left atrial (LA) scars. The position and extent\nof scars provide important information of the pathophysiology and progression\nof atrial fibrillation (AF). Hence, LA scar segmentation and quantification\nfrom LGE MRI can be useful in computer-assisted diagnosis and treatment\nstratification of AF patients. Since manual delineation can be time-consuming\nand subject to intra- and inter-expert variability, automating this computing\nis highly desired, which nevertheless is still challenging and\nunder-researched.\n\nThis paper aims to provide a systematic review on computing methods for LA\ncavity, wall, scar and ablation gap segmentation and quantification from LGE\nMRI, and the related literature for AF studies. Specifically, we first\nsummarize AF-related imaging techniques, particularly LGE MRI. Then, we review\nthe methodologies of the four computing tasks in detail, and summarize the\nvalidation strategies applied in each task. Finally, the possible future\ndevelopments are outlined, with a brief survey on the potential clinical\napplications of the aforementioned methods. The review shows that the research\ninto this topic is still in early stages. Although several methods have been\nproposed, especially for LA segmentation, there is still large scope for\nfurther algorithmic developments due to performance issues related to the high\nvariability of enhancement appearance and differences in image acquisition.",
          "link": "http://arxiv.org/abs/2106.09862",
          "publishedOn": "2021-06-21T02:07:38.626Z",
          "wordCount": 669,
          "title": "Medical Image Analysis on Left Atrial LGE MRI for Atrial Fibrillation Studies: A Review. (arXiv:2106.09862v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yoojin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Khamy_M/0/1/0/all/0/1\">Mostafa El-Khamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungwon Lee</a>",
          "description": "This paper proposes two novel knowledge transfer techniques for\nclass-incremental learning (CIL). First, we propose data-free generative replay\n(DF-GR) to mitigate catastrophic forgetting in CIL by using synthetic samples\nfrom a generative model. In the conventional generative replay, the generative\nmodel is pre-trained for old data and shared in extra memory for later\nincremental learning. In our proposed DF-GR, we train a generative model from\nscratch without using any training data, based on the pre-trained\nclassification model from the past, so we curtail the cost of sharing\npre-trained generative models. Second, we introduce dual-teacher information\ndistillation (DT-ID) for knowledge distillation from two teachers to one\nstudent. In CIL, we use DT-ID to learn new classes incrementally based on the\npre-trained model for old classes and another model (pre-)trained on the new\ndata for new classes. We implemented the proposed schemes on top of one of the\nstate-of-the-art CIL methods and showed the performance improvement on\nCIFAR-100 and ImageNet datasets.",
          "link": "http://arxiv.org/abs/2106.09835",
          "publishedOn": "2021-06-21T02:07:38.617Z",
          "wordCount": 612,
          "title": "Dual-Teacher Class-Incremental Learning With Data-Free Generative Replay. (arXiv:2106.09835v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Toering_M/0/1/0/all/0/1\">Martine Toering</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatopoulos_I/0/1/0/all/0/1\">Ioannis Gatopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stol_M/0/1/0/all/0/1\">Maarten Stol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_V/0/1/0/all/0/1\">Vincent Tao Hu</a>",
          "description": "Instance-level contrastive learning techniques, which rely on data\naugmentation and a contrastive loss function, have found great success in the\ndomain of visual representation learning. They are not suitable for exploiting\nthe rich dynamical structure of video however, as operations are done on many\naugmented instances. In this paper we propose \"Video Cross-Stream Prototypical\nContrasting\", a novel method which predicts consistent prototype assignments\nfrom both RGB and optical flow views, operating on sets of samples.\nSpecifically, we alternate the optimization process; while optimizing one of\nthe streams, all views are mapped to one set of stream prototype vectors. Each\nof the assignments is predicted with all views except the one matching the\nprediction, pushing representations closer to their assigned prototypes. As a\nresult, more efficient video embeddings with ingrained motion information are\nlearned, without the explicit need for optical flow computation during\ninference. We obtain state-of-the-art results on nearest neighbour video\nretrieval and action recognition, outperforming previous best by +3.2% on\nUCF101 using the S3D backbone (90.5% Top-1 acc), and by +7.2% on UCF101 and\n+15.1% on HMDB51 using the R(2+1)D backbone.",
          "link": "http://arxiv.org/abs/2106.10137",
          "publishedOn": "2021-06-21T02:07:38.609Z",
          "wordCount": 619,
          "title": "Self-supervised Video Representation Learning with Cross-Stream Prototypical Contrasting. (arXiv:2106.10137v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2001.08026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stucker_C/0/1/0/all/0/1\">Corinne Stucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>",
          "description": "We propose an embarrassingly simple but very effective scheme for\nhigh-quality dense stereo reconstruction: (i) generate an approximate\nreconstruction with your favourite stereo matcher; (ii) rewarp the input images\nwith that approximate model; (iii) with the initial reconstruction and the\nwarped images as input, train a deep network to enhance the reconstruction by\nregressing a residual correction; and (iv) if desired, iterate the refinement\nwith the new, improved reconstruction. The strategy to only learn the residual\ngreatly simplifies the learning problem. A standard Unet without bells and\nwhistles is enough to reconstruct even small surface details, like dormers and\nroof substructures in satellite images. We also investigate residual\nreconstruction with less information and find that even a single image is\nenough to greatly improve an approximate reconstruction. Our full model reduces\nthe mean absolute error of state-of-the-art stereo reconstruction systems by\n>50%, both in our target domain of satellite stereo and on stereo pairs from\nthe ETH3D benchmark.",
          "link": "http://arxiv.org/abs/2001.08026",
          "publishedOn": "2021-06-21T02:07:38.601Z",
          "wordCount": 640,
          "title": "ResDepth: Learned Residual Stereo Reconstruction. (arXiv:2001.08026v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aboutalebi_H/0/1/0/all/0/1\">Hossein Aboutalebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1\">Mohammad Javad Shafiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karg_M/0/1/0/all/0/1\">Michelle Karg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharfenberger_C/0/1/0/all/0/1\">Christian Scharfenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>",
          "description": "Despite the significant advances in deep learning over the past decade, a\nmajor challenge that limits the wide-spread adoption of deep learning has been\ntheir fragility to adversarial attacks. This sensitivity to making erroneous\npredictions in the presence of adversarially perturbed data makes deep neural\nnetworks difficult to adopt for certain real-world, mission-critical\napplications. While much of the research focus has revolved around adversarial\nexample creation and adversarial hardening, the area of performance measures\nfor assessing adversarial robustness is not well explored. Motivated by this,\nthis study presents the concept of residual error, a new performance measure\nfor not only assessing the adversarial robustness of a deep neural network at\nthe individual sample level, but also can be used to differentiate between\nadversarial and non-adversarial examples to facilitate for adversarial example\ndetection. Furthermore, we introduce a hybrid model for approximating the\nresidual error in a tractable manner. Experimental results using the case of\nimage classification demonstrates the effectiveness and efficacy of the\nproposed residual error metric for assessing several well-known deep neural\nnetwork architectures. These results thus illustrate that the proposed measure\ncould be a useful tool for not only assessing the robustness of deep neural\nnetworks used in mission-critical scenarios, but also in the design of\nadversarially robust models.",
          "link": "http://arxiv.org/abs/2106.10212",
          "publishedOn": "2021-06-21T02:07:38.577Z",
          "wordCount": 657,
          "title": "Residual Error: a New Performance Measure for Adversarial Robustness. (arXiv:2106.10212v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lijin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugano_Y/0/1/0/all/0/1\">Yusuke Sugano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>",
          "description": "In this report, we describe the technical details of our submission to the\n2021 EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action\nRecognition. Leveraging multiple modalities has been proved to benefit the\nUnsupervised Domain Adaptation (UDA) task. In this work, we present Multi-Modal\nMutual Enhancement Module (M3EM), a deep module for jointly considering\ninformation from multiple modalities to find the most transferable\nrepresentations across domains. We achieve this by implementing two sub-modules\nfor enhancing each modality using the context of other modalities. The first\nsub-module exchanges information across modalities through the semantic space,\nwhile the second sub-module finds the most transferable spatial region based on\nthe consensus of all modalities.",
          "link": "http://arxiv.org/abs/2106.10026",
          "publishedOn": "2021-06-21T02:07:38.549Z",
          "wordCount": 555,
          "title": "EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2021: Team M3EM Technical Report. (arXiv:2106.10026v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Armandpour_M/0/1/0/all/0/1\">Mohammadreza Armandpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghian_A/0/1/0/all/0/1\">Ali Sadeghian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>",
          "description": "Despite the success of Generative Adversarial Networks (GANs), their training\nsuffers from several well-known problems, including mode collapse and\ndifficulties learning a disconnected set of manifolds. In this paper, we break\ndown the challenging task of learning complex high dimensional distributions,\nsupporting diverse data samples, to simpler sub-tasks. Our solution relies on\ndesigning a partitioner that breaks the space into smaller regions, each having\na simpler distribution, and training a different generator for each partition.\nThis is done in an unsupervised manner without requiring any labels.\n\nWe formulate two desired criteria for the space partitioner that aid the\ntraining of our mixture of generators: 1) to produce connected partitions and\n2) provide a proxy of distance between partitions and data samples, along with\na direction for reducing that distance. These criteria are developed to avoid\nproducing samples from places with non-existent data density, and also\nfacilitate training by providing additional direction to the generators. We\ndevelop theoretical constraints for a space partitioner to satisfy the above\ncriteria. Guided by our theoretical analysis, we design an effective neural\narchitecture for the space partitioner that empirically assures these\nconditions. Experimental results on various standard benchmarks show that the\nproposed unsupervised model outperforms several recent methods.",
          "link": "http://arxiv.org/abs/2104.00816",
          "publishedOn": "2021-06-21T02:07:38.540Z",
          "wordCount": 662,
          "title": "Partition-Guided GANs. (arXiv:2104.00816v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Baoming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Ke Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1\">Bo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ban_C/0/1/0/all/0/1\">Chao Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobo Li</a>",
          "description": "Video affective understanding, which aims to predict the evoked expressions\nby the video content, is desired for video creation and recommendation. In the\nrecent EEV challenge, a dense affective understanding task is proposed and\nrequires frame-level affective prediction. In this paper, we propose a\nmulti-granularity network with modal attention (MGN-MA), which employs\nmulti-granularity features for better description of the target frame.\nSpecifically, the multi-granularity features could be divided into frame-level,\nclips-level and video-level features, which corresponds to visual-salient\ncontent, semantic-context and video theme information. Then the modal attention\nfusion module is designed to fuse the multi-granularity features and emphasize\nmore affection-relevant modals. Finally, the fused feature is fed into a\nMixtures Of Experts (MOE) classifier to predict the expressions. Further\nemploying model-ensemble post-processing, the proposed method achieves the\ncorrelation score of 0.02292 in the EEV challenge.",
          "link": "http://arxiv.org/abs/2106.09964",
          "publishedOn": "2021-06-21T02:07:38.523Z",
          "wordCount": 591,
          "title": "Multi-Granularity Network with Modal Attention for Dense Affective Understanding. (arXiv:2106.09964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiquan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changhao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>",
          "description": "GasHisSDB is a New Gastric Histopathology Subsize Image Database with a total\nof 245196 images. GasHisSDB is divided into 160*160 pixels sub-database,\n120*120 pixels sub-database and 80*80 pixels sub-database. GasHisSDB is made to\nrealize the function of valuating image classification. In order to prove that\nthe methods of different periods in the field of image classification have\ndiscrepancies on GasHisSDB, we select a variety of classifiers for evaluation.\nSeven classical machine learning classifiers, three CNN classifiers and a novel\ntransformer-based classifier are selected for testing on image classification\ntasks. GasHisSDB is available at the\nURL:https://github.com/NEUhwm/GasHisSDB.git.",
          "link": "http://arxiv.org/abs/2106.02473",
          "publishedOn": "2021-06-21T02:07:38.505Z",
          "wordCount": 592,
          "title": "A New Gastric Histopathology Subsize Image Database (GasHisSDB) for Classification Algorithm Test: from Linear Regression to Visual Transformer. (arXiv:2106.02473v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lina Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingshu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yawei Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xuemei Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "The vulnerability of deep neural networks to adversarial examples, which are\ncrafted maliciously by modifying the inputs with imperceptible perturbations to\nmisled the network produce incorrect outputs, reveals the lack of robustness\nand poses security concerns. Previous works study the adversarial robustness of\nimage classifiers on image level and use all the pixel information in an image\nindiscriminately, lacking of exploration of regions with different semantic\nmeanings in the pixel space of an image. In this work, we fill this gap and\nexplore the pixel space of the adversarial image by proposing an algorithm to\nlooking for possible perturbations pixel by pixel in different regions of the\nsegmented image. The extensive experimental results on CIFAR-10 and ImageNet\nverify that searching for the modified pixel in only some pixels of an image\ncan successfully launch the one-pixel adversarial attacks without requiring all\nthe pixels of the entire image, and there exist multiple vulnerable points\nscattered in different regions of an image. We also demonstrate that the\nadversarial robustness of different regions on the image varies with the amount\nof semantic information contained.",
          "link": "http://arxiv.org/abs/2106.09872",
          "publishedOn": "2021-06-21T02:07:38.492Z",
          "wordCount": 646,
          "title": "Analyzing Adversarial Robustness of Deep Neural Networks in Pixel Space: a Semantic Perspective. (arXiv:2106.09872v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.14611",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jinlong Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Deep learning has demonstrated its power in image rectification by leveraging\nthe representation capacity of deep neural networks via supervised training\nbased on a large-scale synthetic dataset. However, the model may overfit the\nsynthetic images and generalize not well on real-world fisheye images due to\nthe limited universality of a specific distortion model and the lack of\nexplicitly modeling the distortion and rectification process. In this paper, we\npropose a novel self-supervised image rectification (SIR) method based on an\nimportant insight that the rectified results of distorted images of a same\nscene from different lens should be the same. Specifically, we devise a new\nnetwork architecture with a shared encoder and several prediction heads, each\nof which predicts the distortion parameter of a specific distortion model. We\nfurther leverage a differentiable warping module to generate the rectified\nimages and re-distorted images from the distortion parameters and exploit the\nintra- and inter-model consistency between them during training, thereby\nleading to a self-supervised learning scheme without the need for ground-truth\ndistortion parameters or normal images. Experiments on synthetic dataset and\nreal-world fisheye images demonstrate that our method achieves comparable or\neven better performance than the supervised baseline method and representative\nstate-of-the-art methods. Self-supervised learning also improves the\nuniversality of distortion models while keeping their self-consistency.",
          "link": "http://arxiv.org/abs/2011.14611",
          "publishedOn": "2021-06-21T02:07:38.474Z",
          "wordCount": 691,
          "title": "SIR: Self-supervised Image Rectification via Seeing the Same Scene from Multiple Different Lenses. (arXiv:2011.14611v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Lin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_E/0/1/0/all/0/1\">Edward Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Lei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Huaishao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharti_T/0/1/0/all/0/1\">Taroon Bharti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacheti_A/0/1/0/all/0/1\">Arun Sacheti</a>",
          "description": "In this paper, we present GEM as a General Evaluation benchmark for\nMultimodal tasks. Different from existing datasets such as GLUE, SuperGLUE,\nXGLUE and XTREME that mainly focus on natural language tasks, GEM is a\nlarge-scale vision-language benchmark, which consists of GEM-I for\nimage-language tasks and GEM-V for video-language tasks. Comparing with\nexisting multimodal datasets such as MSCOCO and Flicker30K for image-language\ntasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the\nlargest vision-language dataset covering image-language tasks and\nvideo-language tasks at the same time, but also labeled in multiple languages.\nWe also provide two baseline models for this benchmark. We will release the\ndataset, code and baseline models, aiming to advance the development of\nmultilingual multimodal research.",
          "link": "http://arxiv.org/abs/2106.09889",
          "publishedOn": "2021-06-21T02:07:38.455Z",
          "wordCount": 581,
          "title": "GEM: A General Evaluation Benchmark for Multimodal Tasks. (arXiv:2106.09889v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2011.08809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pena_A/0/1/0/all/0/1\">Alejandro Pe&#xf1;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serna_I/0/1/0/all/0/1\">Ignacio Serna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapedriza_A/0/1/0/all/0/1\">Agata Lapedriza</a>",
          "description": "This work explores facial expression bias as a security vulnerability of face\nrecognition systems. Despite the great performance achieved by state-of-the-art\nface recognition systems, the algorithms are still sensitive to a large range\nof covariates. We present a comprehensive analysis of how facial expression\nbias impacts the performance of face recognition technologies. Our study\nanalyzes: i) facial expression biases in the most popular face recognition\ndatabases; and ii) the impact of facial expression in face recognition\nperformances. Our experimental framework includes two face detectors, three\nface recognition models, and three different databases. Our results demonstrate\na huge facial expression bias in the most widely used databases, as well as a\nrelated impact of face expression in the performance of state-of-the-art\nalgorithms. This work opens the door to new research lines focused on\nmitigating the observed vulnerability.",
          "link": "http://arxiv.org/abs/2011.08809",
          "publishedOn": "2021-06-21T02:07:38.440Z",
          "wordCount": 611,
          "title": "Facial Expressions as a Vulnerability in Face Recognition. (arXiv:2011.08809v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.00549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ke_R/0/1/0/all/0/1\">Ruimin Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhiyong Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Meixin Zhu</a>, Hao (Frank) <a href=\"http://arxiv.org/find/cs/1/au:+Yang/0/1/0/all/0/1\">Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yinhai Wang</a>",
          "description": "Traffic near-crash events serve as critical data sources for various smart\ntransportation applications, such as being surrogate safety measures for\ntraffic safety research and corner case data for automated vehicle testing.\nHowever, there are several key challenges for near-crash detection. First,\nextracting near-crashes from original data sources requires significant\ncomputing, communication, and storage resources. Also, existing methods lack\nefficiency and transferability, which bottlenecks prospective large-scale\napplications. To this end, this paper leverages the power of edge computing to\naddress these challenges by processing the video streams from existing dashcams\nonboard in a real-time manner. We design a multi-thread system architecture\nthat operates on edge devices and model the bounding boxes generated by object\ndetection and tracking in linear complexity. The method is insensitive to\ncamera parameters and backward compatible with different vehicles. The edge\ncomputing system has been evaluated with recorded videos and real-world tests\non two cars and four buses for over ten thousand hours. It filters out\nirrelevant videos in real-time thereby saving labor cost, processing time,\nnetwork bandwidth, and data storage. It collects not only event videos but also\nother valuable data such as road user type, event location, time to collision,\nvehicle trajectory, vehicle speed, brake switch, and throttle. The experiments\ndemonstrate the promising performance of the system regarding efficiency,\naccuracy, reliability, and transferability. It is among the first efforts in\napplying edge computing for real-time traffic video analytics and is expected\nto benefit multiple sub-fields in smart transportation research and\napplications.",
          "link": "http://arxiv.org/abs/2008.00549",
          "publishedOn": "2021-06-21T02:07:38.432Z",
          "wordCount": 724,
          "title": "Edge Computing for Real-Time Near-Crash Detection for Smart Transportation Applications. (arXiv:2008.00549v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.01210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prabhudesai_M/0/1/0/all/0/1\">Mihir Prabhudesai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_H/0/1/0/all/0/1\">Hsiao-Yu Fish Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1\">Syed Ashar Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sieb_M/0/1/0/all/0/1\">Maximilian Sieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1\">Adam W. Harley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>",
          "description": "We propose associating language utterances to 3D visual abstractions of the\nscene they describe. The 3D visual abstractions are encoded as 3-dimensional\nvisual feature maps. We infer these 3D visual scene feature maps from RGB\nimages of the scene via view prediction: when the generated 3D scene feature\nmap is neurally projected from a camera viewpoint, it should match the\ncorresponding RGB image. We present generative models that condition on the\ndependency tree of an utterance and generate a corresponding visual 3D feature\nmap as well as reason about its plausibility, and detector models that\ncondition on both the dependency tree of an utterance and a related image and\nlocalize the object referents in the 3D feature map inferred from the image.\nOur model outperforms models of language and vision that associate language\nwith 2D CNN activations or 2D images by a large margin in a variety of tasks,\nsuch as, classifying plausibility of utterances, detecting referential\nexpressions, and supplying rewards for trajectory optimization of object\nplacement policies from language instructions. We perform numerous ablations\nand show the improved performance of our detectors is due to its better\ngeneralization across camera viewpoints and lack of object interferences in the\ninferred 3D feature space, and the improved performance of our generators is\ndue to their ability to spatially reason about objects and their configurations\nin 3D when mapping from language to scenes.",
          "link": "http://arxiv.org/abs/1910.01210",
          "publishedOn": "2021-06-21T02:07:38.413Z",
          "wordCount": 735,
          "title": "Embodied Language Grounding with 3D Visual Feature Representations. (arXiv:1910.01210v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Binnan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yunxiang Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Langechuan Liu</a>",
          "description": "Radars and cameras are mature, cost-effective, and robust sensors and have\nbeen widely used in the perception stack of mass-produced autonomous driving\nsystems. Due to their complementary properties, outputs from radar detection\n(radar pins) and camera perception (2D bounding boxes) are usually fused to\ngenerate the best perception results. The key to successful radar-camera fusion\nis the accurate data association. The challenges in the radar-camera\nassociation can be attributed to the complexity of driving scenes, the noisy\nand sparse nature of radar measurements, and the depth ambiguity from 2D\nbounding boxes. Traditional rule-based association methods are susceptible to\nperformance degradation in challenging scenarios and failure in corner cases.\nIn this study, we propose to address radar-camera association via deep\nrepresentation learning, to explore feature-level interaction and global\nreasoning. Additionally, we design a loss sampling mechanism and an innovative\nordinal loss to overcome the difficulty of imperfect labeling and to enforce\ncritical human-like reasoning. Despite being trained with noisy labels\ngenerated by a rule-based algorithm, our proposed method achieves a performance\nof 92.2% F1 score, which is 11.6% higher than the rule-based teacher. Moreover,\nthis data-driven method also lends itself to continuous improvement via corner\ncase mining.",
          "link": "http://arxiv.org/abs/2103.07825",
          "publishedOn": "2021-06-21T02:07:38.394Z",
          "wordCount": 690,
          "title": "Radar Camera Fusion via Representation Learning in Autonomous Driving. (arXiv:2103.07825v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Zhijian Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Huanshu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suo_C/0/1/0/all/0/1\">Chuanzhe Suo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>",
          "description": "3D Point cloud registration is still a very challenging topic due to the\ndifficulty in finding the rigid transformation between two point clouds with\npartial correspondences, and it's even harder in the absence of any initial\nestimation information. In this paper, we present an end-to-end deep-learning\nbased approach to resolve the point cloud registration problem. Firstly, the\nrevised LPD-Net is introduced to extract features and aggregate them with the\ngraph network. Secondly, the self-attention mechanism is utilized to enhance\nthe structure information in the point cloud and the cross-attention mechanism\nis designed to enhance the corresponding information between the two input\npoint clouds. Based on which, the virtual corresponding points can be generated\nby a soft pointer based method, and finally, the point cloud registration\nproblem can be solved by implementing the SVD method. Comparison results in\nModelNet40 dataset validate that the proposed approach reaches the\nstate-of-the-art in point cloud registration tasks and experiment resutls in\nKITTI dataset validate the effectiveness of the proposed approach in real\napplications.Our source code is available at\n\\url{https://github.com/qiaozhijian/VCR-Net.git}",
          "link": "http://arxiv.org/abs/2011.14579",
          "publishedOn": "2021-06-21T02:07:38.383Z",
          "wordCount": 656,
          "title": "End-to-End 3D Point Cloud Learning for Registration Task Using Virtual Correspondences. (arXiv:2011.14579v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jenner_E/0/1/0/all/0/1\">Erik Jenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiler_M/0/1/0/all/0/1\">Maurice Weiler</a>",
          "description": "Recent work in equivariant deep learning bears strong similarities to\nphysics. Fields over a base space are fundamental entities in both subjects, as\nare equivariant maps between these fields. In deep learning, however, these\nmaps are usually defined by convolutions with a kernel, whereas they are\npartial differential operators (PDOs) in physics. Developing the theory of\nequivariant PDOs in the context of deep learning could bring these subjects\neven closer together and lead to a stronger flow of ideas. In this work, we\nderive a $G$-steerability constraint that completely characterizes when a PDO\nbetween feature vector fields is equivariant, for arbitrary symmetry groups\n$G$. We then fully solve this constraint for several important groups. We use\nour solutions as equivariant drop-in replacements for convolutional layers and\nbenchmark them in that role. Finally, we develop a framework for equivariant\nmaps based on Schwartz distributions that unifies classical convolutions and\ndifferential operators and gives insight about the relation between the two.",
          "link": "http://arxiv.org/abs/2106.10163",
          "publishedOn": "2021-06-21T02:07:38.376Z",
          "wordCount": 603,
          "title": "Steerable Partial Differential Operators for Equivariant Neural Networks. (arXiv:2106.10163v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengrui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Guangchun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Ling Tian</a>",
          "description": "Finding a suitable data representation for a specific task has been shown to\nbe crucial in many applications. The success of subspace clustering depends on\nthe assumption that the data can be separated into different subspaces.\nHowever, this simple assumption does not always hold since the raw data might\nnot be separable into subspaces. To recover the ``clustering-friendly''\nrepresentation and facilitate the subsequent clustering, we propose a graph\nfiltering approach by which a smooth representation is achieved. Specifically,\nit injects graph similarity into data features by applying a low-pass filter to\nextract useful data representations for clustering. Extensive experiments on\nimage and document clustering datasets demonstrate that our method improves\nupon state-of-the-art subspace clustering techniques. Especially, its\ncomparable performance with deep learning methods emphasizes the effectiveness\nof the simple graph filtering scheme for many real-world applications. An\nablation study shows that graph filtering can remove noise, preserve structure\nin the image, and increase the separability of classes.",
          "link": "http://arxiv.org/abs/2106.09874",
          "publishedOn": "2021-06-21T02:07:38.368Z",
          "wordCount": 606,
          "title": "Towards Clustering-friendly Representations: Subspace Clustering via Graph Filtering. (arXiv:2106.09874v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2002.00391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Biao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1\">Guocheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chingyao Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>",
          "description": "Pedestrian trajectory prediction in dynamic scenes remains a challenging and\ncritical problem in numerous applications, such as self-driving cars and\nsocially aware robots. Challenges concentrate on capturing pedestrians' motion\npatterns and social interactions, as well as handling the future uncertainties.\nRecent studies focus on modeling pedestrians' motion patterns with recurrent\nneural networks, capturing social interactions with pooling-based or\ngraph-based methods, and handling future uncertainties by using random Gaussian\nnoise as the latent variable. However, they do not integrate specific obstacle\navoidance experience (OAE) that may improve prediction performance. For\nexample, pedestrians' future trajectories are always influenced by others in\nfront. Here we propose GTPPO (Graph-based Trajectory Predictor with Pseudo\nOracle), an encoder-decoder-based method conditioned on pedestrians' future\nbehaviors. Pedestrians' motion patterns are encoded with a long short-term\nmemory unit, which introduces the temporal attention to highlight specific time\nsteps. Their interactions are captured by a graph-based attention mechanism,\nwhich draws OAE into the data-driven learning process of graph attention.\nFuture uncertainties are handled by generating multi-modal outputs with an\ninformative latent variable. Such a variable is generated by a novel pseudo\noracle predictor, which minimizes the knowledge gap between historical and\nground-truth trajectories. Finally, the GTPPO is evaluated on ETH, UCY and\nStanford Drone datasets, and the results demonstrate state-of-the-art\nperformance. Besides, the qualitative evaluations show successful cases of\nhandling sudden motion changes in the future. Such findings indicate that GTPPO\ncan peek into the future.",
          "link": "http://arxiv.org/abs/2002.00391",
          "publishedOn": "2021-06-21T02:07:38.351Z",
          "wordCount": 721,
          "title": "A Novel Graph based Trajectory Predictor with Pseudo Oracle. (arXiv:2002.00391v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianzhong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuaijun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>",
          "description": "Multi-source unsupervised domain adaptation~(MSDA) aims at adapting models\ntrained on multiple labeled source domains to an unlabeled target domain. In\nthis paper, we propose a novel multi-source domain adaptation framework based\non collaborative learning for semantic segmentation. Firstly, a simple image\ntranslation method is introduced to align the pixel value distribution to\nreduce the gap between source domains and target domain to some extent. Then,\nto fully exploit the essential semantic information across source domains, we\npropose a collaborative learning method for domain adaptation without seeing\nany data from target domain. In addition, similar to the setting of\nunsupervised domain adaptation, unlabeled target domain data is leveraged to\nfurther improve the performance of domain adaptation. This is achieved by\nadditionally constraining the outputs of multiple adaptation models with pseudo\nlabels online generated by an ensembled model. Extensive experiments and\nablation studies are conducted on the widely-used domain adaptation benchmark\ndatasets in semantic segmentation. Our proposed method achieves 59.0\\% mIoU on\nthe validation set of Cityscapes by training on the labeled Synscapes and GTA5\ndatasets and unlabeled training set of Cityscapes. It significantly outperforms\nall previous state-of-the-arts single-source and multi-source unsupervised\ndomain adaptation methods.",
          "link": "http://arxiv.org/abs/2103.04717",
          "publishedOn": "2021-06-21T02:07:38.343Z",
          "wordCount": 670,
          "title": "Multi-Source Domain Adaptation with Collaborative Learning for Semantic Segmentation. (arXiv:2103.04717v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xuefeng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "In adversarial training (AT), the main focus has been the objective and\noptimizer while the model has been less studied, so that the models being used\nare still those classic ones in standard training (ST). Classic network\narchitectures (NAs) are generally worse than searched NAs in ST, which should\nbe the same in AT. In this paper, we argue that NA and AT cannot be handled\nindependently, since given a dataset, the optimal NA in ST would be no longer\noptimal in AT. That being said, AT is time-consuming itself; if we directly\nsearch NAs in AT over large search spaces, the computation will be practically\ninfeasible. Thus, we propose a diverse-structured network (DS-Net), to\nsignificantly reduce the size of the search space: instead of low-level\noperations, we only consider predefined atomic blocks, where an atomic block is\na time-tested building block like the residual block. There are only a few\natomic blocks and thus we can weight all atomic blocks rather than find the\nbest one in a searched block of DS-Net, which is an essential trade-off between\nexploring diverse structures and exploiting the best structures. Empirical\nresults demonstrate the advantages of DS-Net, i.e., weighting the atomic\nblocks.",
          "link": "http://arxiv.org/abs/2102.01886",
          "publishedOn": "2021-06-21T02:07:38.336Z",
          "wordCount": 695,
          "title": "Learning Diverse-Structured Networks for Adversarial Robustness. (arXiv:2102.01886v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10230",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mahapatra_D/0/1/0/all/0/1\">Dwarikanath Mahapatra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_A/0/1/0/all/0/1\">Ankur Singh</a>",
          "description": "While medical image segmentation is an important task for computer aided\ndiagnosis, the high expertise requirement for pixelwise manual annotations\nmakes it a challenging and time consuming task. Since conventional data\naugmentations do not fully represent the underlying distribution of the\ntraining set, the trained models have varying performance when tested on images\ncaptured from different sources. Most prior work on image synthesis for data\naugmentation ignore the interleaved geometric relationship between different\nanatomical labels. We propose improvements over previous GAN-based medical\nimage synthesis methods by learning the relationship between different\nanatomical labels. We use a weakly supervised segmentation method to obtain\npixel level semantic label map of images which is used learn the intrinsic\nrelationship of geometry and shape across semantic labels. Latent space\nvariable sampling results in diverse generated images from a base image and\nimproves robustness. We use the synthetic images from our method to train\nnetworks for segmenting COVID-19 infected areas from lung CT images. The\nproposed method outperforms state-of-the-art segmentation methods on a public\ndataset. Ablation studies also demonstrate benefits of integrating geometry and\ndiversity.",
          "link": "http://arxiv.org/abs/2106.10230",
          "publishedOn": "2021-06-21T02:07:38.328Z",
          "wordCount": 697,
          "title": "CT Image Synthesis Using Weakly Supervised Segmentation and Geometric Inter-Label Relations For COVID Image Analysis. (arXiv:2106.10230v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sixiang_T/0/1/0/all/0/1\">Tan Sixiang</a>",
          "description": "For real-time semantic segmentation, how to increase the speed while\nmaintaining high resolution is a problem that has been discussed and solved.\nBackbone design and fusion design have always been two essential parts of\nreal-time semantic segmentation. We hope to design a light-weight network based\non previous design experience and reach the level of state-of-the-art real-time\nsemantic segmentation without any pre-training. To achieve this goal, a\nencoder-decoder architectures are proposed to solve this problem by applying a\ndecoder network onto a backbone model designed for real-time segmentation tasks\nand designed three different ways to fuse semantics and detailed information in\nthe aggregation phase. We have conducted extensive experiments on two semantic\nsegmentation benchmarks. Experiments on the Cityscapes and CamVid datasets show\nthat the proposed FRFNet strikes a balance between speed calculation and\naccuracy. It achieves 72% Mean Intersection over Union (mIoU%) on the\nCityscapes test dataset with the speed of 144 on a single RTX 1080Ti card. The\nCode is available at https://github.com/favoMJ/FRFNet.",
          "link": "http://arxiv.org/abs/2105.12964",
          "publishedOn": "2021-06-21T02:07:38.308Z",
          "wordCount": 632,
          "title": "Feature Reuse and Fusion for Real-time Semantic segmentation. (arXiv:2105.12964v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09624",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Grohl_J/0/1/0/all/0/1\">Janek Gr&#xf6;hl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schellenberg_M/0/1/0/all/0/1\">Melanie Schellenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dreher_K/0/1/0/all/0/1\">Kris Dreher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Holzwarth_N/0/1/0/all/0/1\">Niklas Holzwarth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu D. Tizabi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seitel_A/0/1/0/all/0/1\">Alexander Seitel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a>",
          "description": "Photoacoustic imaging has the potential to revolutionise healthcare due to\nthe valuable information on tissue physiology that is contained in\nmultispectral photoacoustic measurements. Clinical translation of the\ntechnology requires conversion of the high-dimensional acquired data into\nclinically relevant and interpretable information. In this work, we present a\ndeep learning-based approach to semantic segmentation of multispectral\nphotoacoustic images to facilitate the interpretability of recorded images.\nManually annotated multispectral photoacoustic imaging data are used as gold\nstandard reference annotations and enable the training of a deep learning-based\nsegmentation algorithm in a supervised manner. Based on a validation study with\nexperimentally acquired data of healthy human volunteers, we show that\nautomatic tissue segmentation can be used to create powerful analyses and\nvisualisations of multispectral photoacoustic images. Due to the intuitive\nrepresentation of high-dimensional information, such a processing algorithm\ncould be a valuable means to facilitate the clinical translation of\nphotoacoustic imaging.",
          "link": "http://arxiv.org/abs/2105.09624",
          "publishedOn": "2021-06-21T02:07:38.300Z",
          "wordCount": 631,
          "title": "Semantic segmentation of multispectral photoacoustic images using deep learning. (arXiv:2105.09624v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Muhammad Monjurul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ruwen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhaozheng Yin</a>",
          "description": "Recently, autonomous vehicles and those equipped with an Advanced Driver\nAssistance System (ADAS) are emerging. They share the road with regular ones\noperated by human drivers entirely. To ensure guaranteed safety for passengers\nand other road users, it becomes essential for autonomous vehicles and ADAS to\nanticipate traffic accidents from natural driving scenes. The dynamic\nspatial-temporal interaction of the traffic agents is complex, and visual cues\nfor predicting a future accident are embedded deeply in dashcam video data.\nTherefore, early anticipation of traffic accidents remains a challenge. To this\nend, the paper presents a dynamic spatial-temporal attention (DSTA) network for\nearly anticipation of traffic accidents from dashcam videos. The proposed\nDSTA-network learns to select discriminative temporal segments of a video\nsequence with a module named Dynamic Temporal Attention (DTA). It also learns\nto focus on the informative spatial regions of frames with another module named\nDynamic Spatial Attention (DSA). The spatial-temporal relational features of\naccidents, along with scene appearance features, are learned jointly with a\nGated Recurrent Unit (GRU) network. The experimental evaluation of the\nDSTA-network on two benchmark datasets confirms that it has exceeded the\nstate-of-the-art performance. A thorough ablation study evaluates the\ncontributions of individual components of the DSTA-network, revealing how the\nnetwork achieves such performance. Furthermore, this paper proposes a new\nstrategy that fuses the prediction scores from two complementary models and\nverifies its effectiveness in further boosting the performance of early\naccident anticipation.",
          "link": "http://arxiv.org/abs/2106.10197",
          "publishedOn": "2021-06-21T02:07:38.292Z",
          "wordCount": 693,
          "title": "A Dynamic Spatial-temporal Attention Network for Early Anticipation of Traffic Accidents. (arXiv:2106.10197v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.08482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhaowei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1\">Charless Fowlkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We present a plug-in replacement for batch normalization (BN) called\nexponential moving average normalization (EMAN), which improves the performance\nof existing student-teacher based self- and semi-supervised learning\ntechniques. Unlike the standard BN, where the statistics are computed within\neach batch, EMAN, used in the teacher, updates its statistics by exponential\nmoving average from the BN statistics of the student. This design reduces the\nintrinsic cross-sample dependency of BN and enhances the generalization of the\nteacher. EMAN improves strong baselines for self-supervised learning by 4-6/1-2\npoints and semi-supervised learning by about 7/2 points, when 1%/10% supervised\nlabels are available on ImageNet. These improvements are consistent across\nmethods, network architectures, training duration, and datasets, demonstrating\nthe general effectiveness of this technique. The code is available at\nhttps://github.com/amazon-research/exponential-moving-average-normalization.",
          "link": "http://arxiv.org/abs/2101.08482",
          "publishedOn": "2021-06-21T02:07:38.285Z",
          "wordCount": 614,
          "title": "Exponential Moving Average Normalization for Self-supervised and Semi-supervised Learning. (arXiv:2101.08482v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jialiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zickler_T/0/1/0/all/0/1\">Todd Zickler</a>",
          "description": "Localizing stereo boundaries is difficult because matching cues are absent in\nthe occluded regions that are adjacent to them. We introduce an energy and\nlevel-set optimizer that improves boundaries by encoding the essential geometry\nof occlusions: The spatial extent of an occlusion must equal the amplitude of\nthe disparity jump that causes it. In a collection of figure-ground scenes from\nMiddlebury and Falling Things stereo datasets, the model provides more accurate\nboundaries than previous occlusion-handling techniques.",
          "link": "http://arxiv.org/abs/2006.16094",
          "publishedOn": "2021-06-21T02:07:38.277Z",
          "wordCount": 554,
          "title": "Level Set Stereo for Cooperative Grouping with Occlusion. (arXiv:2006.16094v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rai_A/0/1/0/all/0/1\">Ayush K Rai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_T/0/1/0/all/0/1\">Tarun Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietlmeier_J/0/1/0/all/0/1\">Julia Dietlmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1\">Kevin McGuinness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smeaton_A/0/1/0/all/0/1\">Alan F Smeaton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1\">Noel E O&#x27;Connor</a>",
          "description": "Detecting generic, taxonomy-free event boundaries invideos represents a major\nstride forward towards holisticvideo understanding. In this paper we present a\ntechnique forgeneric event boundary detection based on a two stream in-flated\n3D convolutions architecture, which can learn spatio-temporal features from\nvideos. Our work is inspired from theGeneric Event Boundary Detection Challenge\n(part of CVPR2021 Long Form Video Understanding- LOVEU Workshop).Throughout the\npaper we provide an in-depth analysis ofthe experiments performed along with an\ninterpretation ofthe results obtained.",
          "link": "http://arxiv.org/abs/2106.10090",
          "publishedOn": "2021-06-21T02:07:38.258Z",
          "wordCount": 537,
          "title": "Discerning Generic Event Boundaries in Long-Form Wild Videos. (arXiv:2106.10090v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10195",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Uelwer_T/0/1/0/all/0/1\">Tobias Uelwer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_T/0/1/0/all/0/1\">Tobias Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harmeling_S/0/1/0/all/0/1\">Stefan Harmeling</a>",
          "description": "Fourier phase retrieval is the problem of reconstructing a signal given only\nthe magnitude of its Fourier transformation. Optimization-based approaches,\nlike the well-established Gerchberg-Saxton or the hybrid input output\nalgorithm, struggle at reconstructing images from magnitudes that are not\noversampled. This motivates the application of learned methods, which allow\nreconstruction from non-oversampled magnitude measurements after a learning\nphase. In this paper, we want to push the limits of these learned methods by\nmeans of a deep neural network cascade that reconstructs the image successively\non different resolutions from its non-oversampled Fourier magnitude. We\nevaluate our method on four different datasets (MNIST, EMNIST, Fashion-MNIST,\nand KMNIST) and demonstrate that it yields improved performance over other\nnon-iterative methods and optimization-based methods.",
          "link": "http://arxiv.org/abs/2106.10195",
          "publishedOn": "2021-06-21T02:07:38.251Z",
          "wordCount": 573,
          "title": "Non-Iterative Phase Retrieval With Cascaded Neural Networks. (arXiv:2106.10195v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/1907.00856",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sarker_M/0/1/0/all/0/1\">Md. Mostafa Kamal Sarker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rashwan_H/0/1/0/all/0/1\">Hatem A. Rashwan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akram_F/0/1/0/all/0/1\">Farhan Akram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_V/0/1/0/all/0/1\">Vivek Kumar Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Banu_S/0/1/0/all/0/1\">Syeda Furruka Banu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_F/0/1/0/all/0/1\">Forhad U H Chowdhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choudhury_K/0/1/0/all/0/1\">Kabir Ahmed Choudhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chambon_S/0/1/0/all/0/1\">Sylvie Chambon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Radeva_P/0/1/0/all/0/1\">Petia Radeva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puig_D/0/1/0/all/0/1\">Domenec Puig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abdel_Nasser_M/0/1/0/all/0/1\">Mohamed Abdel-Nasser</a>",
          "description": "The determination of precise skin lesion boundaries in dermoscopic images\nusing automated methods faces many challenges, most importantly, the presence\nof hair, inconspicuous lesion edges and low contrast in dermoscopic images, and\nvariability in the color, texture and shapes of skin lesions. Existing deep\nlearning-based skin lesion segmentation algorithms are expensive in terms of\ncomputational time and memory. Consequently, running such segmentation\nalgorithms requires a powerful GPU and high bandwidth memory, which are not\navailable in dermoscopy devices. Thus, this article aims to achieve precise\nskin lesion segmentation with minimum resources: a lightweight, efficient\ngenerative adversarial network (GAN) model called SLSNet, which combines 1-D\nkernel factorized networks, position and channel attention, and multiscale\naggregation mechanisms with a GAN model. The 1-D kernel factorized network\nreduces the computational cost of 2D filtering. The position and channel\nattention modules enhance the discriminative ability between the lesion and\nnon-lesion feature representations in spatial and channel dimensions,\nrespectively. A multiscale block is also used to aggregate the coarse-to-fine\nfeatures of input skin images and reduce the effect of the artifacts. SLSNet is\nevaluated on two publicly available datasets: ISBI 2017 and the ISIC 2018.\nAlthough SLSNet has only 2.35 million parameters, the experimental results\ndemonstrate that it achieves segmentation results on a par with the\nstate-of-the-art skin lesion segmentation methods with an accuracy of 97.61%,\nand Dice and Jaccard similarity coefficients of 90.63% and 81.98%,\nrespectively. SLSNet can run at more than 110 frames per second (FPS) in a\nsingle GTX1080Ti GPU, which is faster than well-known deep learning-based image\nsegmentation models, such as FCN. Therefore, SLSNet can be used for practical\ndermoscopic applications.",
          "link": "http://arxiv.org/abs/1907.00856",
          "publishedOn": "2021-06-21T02:07:38.240Z",
          "wordCount": 776,
          "title": "SLSNet: Skin lesion segmentation using a lightweight generative adversarial network. (arXiv:1907.00856v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.10351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lucas Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1\">Caleb Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1\">Bistra Dilkina</a>",
          "description": "Recent work has shown that deep learning models can be used to classify\nland-use data from geospatial satellite imagery. We show that when these deep\nlearning models are trained on data from specific continents/seasons, there is\na high degree of variability in model performance on out-of-sample\ncontinents/seasons. This suggests that just because a model accurately predicts\nland-use classes in one continent or season does not mean that the model will\naccurately predict land-use classes in a different continent or season. We then\nuse clustering techniques on satellite imagery from different continents to\nvisualize the differences in landscapes that make geospatial generalization\nparticularly difficult, and summarize our takeaways for future satellite\nimagery-related applications.",
          "link": "http://arxiv.org/abs/2008.10351",
          "publishedOn": "2021-06-21T02:07:38.212Z",
          "wordCount": 606,
          "title": "Model Generalization in Deep Learning Applications for Land Cover Mapping. (arXiv:2008.10351v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaolin Wu</a>",
          "description": "Nighttime photographers are often troubled by light pollution of unwanted\nartificial lights. Artificial lights, after scattered by aerosols in the\natmosphere, can inundate the starlight and degrade the quality of nighttime\nimages, by reducing contrast and dynamic range and causing hazes. In this paper\nwe develop a physically-based light pollution reduction (LPR) algorithm that\ncan substantially alleviate the aforementioned degradations of perceptual\nquality and restore the pristine state of night sky. The key to the success of\nthe proposed LPR algorithm is an inverse method to estimate the spatial\nradiance distribution and spectral signature of ground artificial lights.\nExtensive experiments are carried out to evaluate the efficacy and limitations\nof the LPR algorithm.",
          "link": "http://arxiv.org/abs/2106.10046",
          "publishedOn": "2021-06-21T02:07:38.197Z",
          "wordCount": 550,
          "title": "Light Pollution Reduction in Nighttime Photography. (arXiv:2106.10046v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.04960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Huan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuecheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Rong Xiong</a>",
          "description": "Place recognition is critical for both offline mapping and online\nlocalization. However, current single-sensor based place recognition still\nremains challenging in adverse conditions. In this paper, a heterogeneous\nmeasurements based framework is proposed for long-term place recognition, which\nretrieves the query radar scans from the existing lidar maps. To achieve this,\na deep neural network is built with joint training in the learning stage, and\nthen in the testing stage, shared embeddings of radar and lidar are extracted\nfor heterogeneous place recognition. To validate the effectiveness of the\nproposed method, we conduct tests and generalization experiments on the\nmulti-session public datasets compared to other competitive methods. The\nexperimental results indicate that our model is able to perform multiple place\nrecognitions: lidar-to-lidar, radar-to-radar and radar-to-lidar, while the\nlearned model is trained only once. We also release the source code publicly:\nhttps://github.com/ZJUYH/radar-to-lidar-place-recognition.",
          "link": "http://arxiv.org/abs/2102.04960",
          "publishedOn": "2021-06-21T02:07:38.175Z",
          "wordCount": 636,
          "title": "Radar-to-Lidar: Heterogeneous Place Recognition via Joint Learning. (arXiv:2102.04960v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.03384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1\">Fenglei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>",
          "description": "Despite its best performance in image denoising, the supervised deep\ndenoising methods require paired noise-clean data, which are often unavailable.\nTo address this challenge, Noise2Noise was designed based on the fact that\npaired noise-clean images can be replaced by paired noise-noise images that are\neasier to collect. However, in many scenarios the collection of paired\nnoise-noise images is still impractical. To bypass labeled images, Noise2Void\nmethods predict masked pixels from their surroundings with single noisy images\nonly and give improved denoising results that still need improvements. An\nobservation on classic denoising methods is that non-local mean (NLM) outcomes\nare typically superior to locally denoised results. In contrast, Noise2Void and\nits variants do not utilize self-similarities in an image as the NLM-based\nmethods do. Here we propose Noise2Sim, an NLM-inspired self-learning method for\nimage denoising. Specifically, Noise2Sim leverages the self-similarity of image\npixels to train the denoising network, requiring single noisy images only. Our\ntheoretical analysis shows that Noise2Sim tends to be equivalent to Noise2Noise\nunder mild conditions. To efficiently manage the computational burden for\nglobally searching similar pixels, we design a two-step procedure to provide\ndata for Noise2Sim training. Extensive experiments demonstrate the superiority\nof Noise2Sim on common benchmark datasets.",
          "link": "http://arxiv.org/abs/2011.03384",
          "publishedOn": "2021-06-21T02:07:38.167Z",
          "wordCount": 687,
          "title": "Noise2Sim -- Similarity-based Self-Learning for Image Denoising. (arXiv:2011.03384v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.09136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Byerly_A/0/1/0/all/0/1\">Adam Byerly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalganova_T/0/1/0/all/0/1\">Tatiana Kalganova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dear_I/0/1/0/all/0/1\">Ian Dear</a>",
          "description": "Most capsule network designs rely on traditional matrix multiplication\nbetween capsule layers and computationally expensive routing mechanisms to deal\nwith the capsule dimensional entanglement that the matrix multiplication\nintroduces. By using Homogeneous Vector Capsules (HVCs), which use element-wise\nmultiplication rather than matrix multiplication, the dimensions of the\ncapsules remain unentangled. In this work, we study HVCs as applied to the\nhighly structured MNIST dataset in order to produce a direct comparison to the\ncapsule research direction of Geoffrey Hinton, et al. In our study, we show\nthat a simple convolutional neural network using HVCs performs as well as the\nprior best performing capsule network on MNIST using 5.5x fewer parameters, 4x\nfewer training epochs, no reconstruction sub-network, and requiring no routing\nmechanism. The addition of multiple classification branches to the network\nestablishes a new state of the art for the MNIST dataset with an accuracy of\n99.87% for an ensemble of these models, as well as establishing a new state of\nthe art for a single model (99.83% accurate).",
          "link": "http://arxiv.org/abs/2001.09136",
          "publishedOn": "2021-06-21T02:07:38.160Z",
          "wordCount": 671,
          "title": "No Routing Needed Between Capsules. (arXiv:2001.09136v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10044",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gandikota_K/0/1/0/all/0/1\">Kanchana Vaishnavi Gandikota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahner_Z/0/1/0/all/0/1\">Zorah L&#xe4;hner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czaplinski_A/0/1/0/all/0/1\">Adam Czapli&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1\">Michael Moeller</a>",
          "description": "Many applications require the robustness, or ideally the invariance, of a\nneural network to certain transformations of input data. Most commonly, this\nrequirement is addressed by either augmenting the training data, using\nadversarial training, or defining network architectures that include the\ndesired invariance automatically. Unfortunately, the latter often relies on the\nability to enlist all possible transformations, which make such approaches\nlargely infeasible for infinite sets of transformations, such as arbitrary\nrotations or scaling. In this work, we propose a method for provably invariant\nnetwork architectures with respect to group actions by choosing one element\nfrom a (possibly continuous) orbit based on a fixed criterion. In a nutshell,\nwe intend to 'undo' any possible transformation before feeding the data into\nthe actual network. We analyze properties of such approaches, extend them to\nequivariant networks, and demonstrate their advantages in terms of robustness\nas well as computational efficiency in several numerical examples. In\nparticular, we investigate the robustness with respect to rotations of images\n(which can possibly hold up to discretization artifacts only) as well as the\nprovable rotational and scaling invariance of 3D point cloud classification.",
          "link": "http://arxiv.org/abs/2106.10044",
          "publishedOn": "2021-06-21T02:07:38.153Z",
          "wordCount": 630,
          "title": "Training or Architecture? How to Incorporate Invariance in Neural Networks. (arXiv:2106.10044v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10258",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fornoni_M/0/1/0/all/0/1\">Marco Fornoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chaochao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Liangchen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilber_K/0/1/0/all/0/1\">Kimberly Wilber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stark_A/0/1/0/all/0/1\">Alex Stark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howard_A/0/1/0/all/0/1\">Andrew Howard</a>",
          "description": "When interacting with objects through cameras, or pictures, users often have\na specific intent. For example, they may want to perform a visual search.\nHowever, most object detection models ignore the user intent, relying on image\npixels as their only input. This often leads to incorrect results, such as lack\nof a high-confidence detection on the object of interest, or detection with a\nwrong class label. In this paper we investigate techniques to modulate standard\nobject detectors to explicitly account for the user intent, expressed as an\nembedding of a simple query. Compared to standard object detectors,\nquery-modulated detectors show superior performance at detecting objects for a\ngiven label of interest. Thanks to large-scale training data synthesized from\nstandard object detection annotations, query-modulated detectors can also\noutperform specialized referring expression recognition systems. Furthermore,\nthey can be simultaneously trained to solve for both query-modulated detection\nand standard object detection.",
          "link": "http://arxiv.org/abs/2106.10258",
          "publishedOn": "2021-06-21T02:07:38.146Z",
          "wordCount": 606,
          "title": "Bridging the Gap Between Object Detection and User Intent via Query-Modulation. (arXiv:2106.10258v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivashechkin_M/0/1/0/all/0/1\">Maksym Ivashechkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1\">Daniel Barath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>",
          "description": "We present VSAC, a RANSAC-type robust estimator with a number of novelties.\nIt benefits from the introduction of the concept of independent inliers that\nimproves significantly the efficacy of the dominant plane handling and, also,\nallows near error-free rejection of incorrect models, without false positives.\nThe local optimization process and its application is improved so that it is\nrun on average only once. Further technical improvements include adaptive\nsequential hypothesis verification and efficient model estimation via Gaussian\nelimination. Experiments on four standard datasets show that VSAC is\nsignificantly faster than all its predecessors and runs on average in 1-2 ms,\non a CPU. It is two orders of magnitude faster and yet as precise as MAGSAC++,\nthe currently most accurate estimator of two-view geometry. In the repeated\nruns on EVD, HPatches, PhotoTourism, and Kusvod2 datasets, it never failed.",
          "link": "http://arxiv.org/abs/2106.10240",
          "publishedOn": "2021-06-21T02:07:38.126Z",
          "wordCount": 573,
          "title": "VSAC: Efficient and Accurate Estimator for H and F. (arXiv:2106.10240v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Ci Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_N/0/1/0/all/0/1\">Nima Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broome_S/0/1/0/all/0/1\">Sofia Broom&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1\">Maheen Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernlund_E/0/1/0/all/0/1\">Elin Hernlund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuffi_S/0/1/0/all/0/1\">Silvia Zuffi</a>",
          "description": "In this paper we present our preliminary work on model-based behavioral\nanalysis of horse motion. Our approach is based on the SMAL model, a 3D\narticulated statistical model of animal shape. We define a novel SMAL model for\nhorses based on a new template, skeleton and shape space learned from $37$\nhorse toys. We test the accuracy of our hSMAL model in reconstructing a horse\nfrom 3D mocap data and images. We apply the hSMAL model to the problem of\nlameness detection from video, where we fit the model to images to recover 3D\npose and train an ST-GCN network on pose data. A comparison with the same\nnetwork trained on mocap points illustrates the benefit of our approach.",
          "link": "http://arxiv.org/abs/2106.10102",
          "publishedOn": "2021-06-21T02:07:38.111Z",
          "wordCount": 576,
          "title": "hSMAL: Detailed Horse Shape and Pose Reconstruction for Motion Pattern Recognition. (arXiv:2106.10102v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Feng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1\">Bin-Bin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jiangpeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiu Li</a>",
          "description": "Boundary-based instance segmentation has drawn much attention since of its\nattractive efficiency. However, existing methods suffer from the difficulty in\nlong-distance regression. In this paper, we propose a coarse-to-fine module to\naddress the problem. Approximate boundary points are generated at the coarse\nstage and then features of these points are sampled and fed to a refined\nregressor for fine prediction. It is end-to-end trainable since differential\nsampling operation is well supported in the module. Furthermore, we design a\nholistic boundary-aware branch and introduce instance-agnostic supervision to\nassist regression. Equipped with ResNet-101, our approach achieves 31.7\\% mask\nAP on COCO dataset with single-scale training and testing, outperforming the\nbaseline 1.3\\% mask AP with less than 1\\% additional parameters and GFLOPs.\nExperiments also show that our proposed method achieves competitive performance\ncompared to existing boundary-based methods with a lightweight design and a\nsimple pipeline.",
          "link": "http://arxiv.org/abs/2106.10213",
          "publishedOn": "2021-06-21T02:07:38.104Z",
          "wordCount": 591,
          "title": "A Coarse-to-Fine Instance Segmentation Network with Learning Boundary Representation. (arXiv:2106.10213v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10080",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhangyang_C/0/1/0/all/0/1\">Cao Peibei. Wang Zhangyang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kede_M/0/1/0/all/0/1\">Ma Kede</a>",
          "description": "In real-world image enhancement, it is often challenging (if not impossible)\nto acquire ground-truth data, preventing the adoption of distance metrics for\nobjective quality assessment. As a result, one often resorts to subjective\nquality assessment, the most straightforward and reliable means of evaluating\nimage enhancement. Conventional subjective testing requires manually\npre-selecting a small set of visual examples, which may suffer from three\nsources of biases: 1) sampling bias due to the extremely sparse distribution of\nthe selected samples in the image space; 2) algorithmic bias due to potential\noverfitting the selected samples; 3) subjective bias due to further potential\ncherry-picking test results. This eventually makes the field of real-world\nimage enhancement more of an art than a science. Here we take steps towards\ndebiasing conventional subjective assessment by automatically sampling a set of\nadaptive and diverse images for subsequent testing. This is achieved by casting\nsample selection into a joint maximization of the discrepancy between the\nenhancers and the diversity among the selected input images. Careful visual\ninspection on the resulting enhanced images provides a debiased ranking of the\nenhancement algorithms. We demonstrate our subjective assessment method using\nthree popular and practically demanding image enhancement tasks: dehazing,\nsuper-resolution, and low-light enhancement.",
          "link": "http://arxiv.org/abs/2106.10080",
          "publishedOn": "2021-06-21T02:07:38.096Z",
          "wordCount": 641,
          "title": "Debiased Subjective Assessment of Real-World Image Enhancement. (arXiv:2106.10080v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09965",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Junwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wenqing Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "In this work, we propose a high fidelity face swapping method, called\nHifiFace, which can well preserve the face shape of the source face and\ngenerate photo-realistic results. Unlike other existing face swapping works\nthat only use face recognition model to keep the identity similarity, we\npropose 3D shape-aware identity to control the face shape with the geometric\nsupervision from 3DMM and 3D face reconstruction method. Meanwhile, we\nintroduce the Semantic Facial Fusion module to optimize the combination of\nencoder and decoder features and make adaptive blending, which makes the\nresults more photo-realistic. Extensive experiments on faces in the wild\ndemonstrate that our method can preserve better identity, especially on the\nface shape, and can generate more photo-realistic results than previous\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.09965",
          "publishedOn": "2021-06-21T02:07:38.088Z",
          "wordCount": 585,
          "title": "HifiFace: 3D Shape and Semantic Prior Guided High Fidelity Face Swapping. (arXiv:2106.09965v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdelhalim_A/0/1/0/all/0/1\">Awad Abdelhalim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbas_M/0/1/0/all/0/1\">Montasir Abbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotha_B/0/1/0/all/0/1\">Bhavi Bharat Kotha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wicks_A/0/1/0/all/0/1\">Alfred Wicks</a>",
          "description": "In a previous study, we presented VT-Lane, a three-step framework for\nreal-time vehicle detection, tracking, and turn movement classification at\nurban intersections. In this study, we present a case study incorporating the\nhighly accurate trajectories and movement classification obtained via VT-Lane\nfor the purpose of speed estimation and driver behavior calibration for traffic\nat urban intersections. First, we use a highly instrumented vehicle to verify\nthe estimated speeds obtained from video inference. The results of the speed\nvalidation show that our method can estimate the average travel speed of\ndetected vehicles in real-time with an error of 0.19 m/sec, which is equivalent\nto 2% of the average observed travel speeds in the intersection of the study.\nInstantaneous speeds (at the resolution of 30 Hz) were found to be estimated\nwith an average error of 0.21 m/sec and 0.86 m/sec respectively for\nfree-flowing and congested traffic conditions. We then use the estimated speeds\nto calibrate the parameters of a driver behavior model for the vehicles in the\narea of study. The results show that the calibrated model replicates the\ndriving behavior with an average error of 0.45 m/sec, indicating the high\npotential for using this framework for automated, large-scale calibration of\ncar-following models from roadside traffic video data, which can lead to\nsubstantial improvements in traffic modeling via microscopic simulation.",
          "link": "http://arxiv.org/abs/2106.09932",
          "publishedOn": "2021-06-21T02:07:38.068Z",
          "wordCount": 680,
          "title": "A Framework for Real-time Traffic Trajectory Tracking, Speed Estimation, and Driver Behavior Calibration at Urban Intersections Using Virtual Traffic Lanes. (arXiv:2106.09932v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antony_J/0/1/0/all/0/1\">Jibinraj Antony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlather_D/0/1/0/all/0/1\">Dr. Florian Schlather</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safronov_G/0/1/0/all/0/1\">Georgij Safronov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitz_M/0/1/0/all/0/1\">Markus Schmitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laerhoven_P/0/1/0/all/0/1\">Prof. Dr. Kristof Van Laerhoven</a>",
          "description": "With the rise of deep learning models in the field of computer vision, new\npossibilities for their application in industrial processes proves to return\ngreat benefits. Nevertheless, the actual fit of machine learning for highly\nstandardised industrial processes is still under debate. This paper addresses\nthe challenges on the industrial realization of the AI tools, considering the\nuse case of Laser Beam Welding quality control as an example. We use object\ndetection algorithms from the TensorFlow object detection API and adapt them to\nour use case using transfer learning. The baseline models we develop are used\nas benchmarks and evaluated and compared to models that undergo dataset scaling\nand hyperparameter tuning. We find that moderate scaling of the dataset via\nimage augmentation leads to improvements in intersection over union (IoU) and\nrecall, whereas high levels of augmentation and scaling may lead to\ndeterioration of results. Finally, we put our results into perspective of the\nunderlying use case and evaluate their fit.",
          "link": "http://arxiv.org/abs/2106.10160",
          "publishedOn": "2021-06-21T02:07:38.061Z",
          "wordCount": 616,
          "title": "Toward Fault Detection in Industrial Welding Processes with Deep Learning and Data Augmentation. (arXiv:2106.10160v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhar_S/0/1/0/all/0/1\">Sauptik Dhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heydari_J/0/1/0/all/0/1\">Javad Heydari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Samarth Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurup_U/0/1/0/all/0/1\">Unmesh Kurup</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mohak Shah</a>",
          "description": "Limited availability of labeled-data makes any supervised learning problem\nchallenging. Alternative learning settings like semi-supervised and universum\nlearning alleviate the dependency on labeled data, but still require a large\namount of unlabeled data, which may be unavailable or expensive to acquire.\nGAN-based synthetic data generation methods have recently shown promise by\ngenerating synthetic samples to improve task at hand. However, these samples\ncannot be used for other purposes. In this paper, we propose a GAN game which\nprovides improved discriminator accuracy under limited data settings, while\ngenerating realistic synthetic data. This provides the added advantage that now\nthe generated data can be used for other similar tasks. We provide the\ntheoretical guarantees and empirical results in support of our approach.",
          "link": "http://arxiv.org/abs/2106.09946",
          "publishedOn": "2021-06-21T02:07:38.054Z",
          "wordCount": 574,
          "title": "Evolving GANs: When Contradictions Turn into Compliance. (arXiv:2106.09946v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinzhe Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tonghuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yaqian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Dongdong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">RenGang Li</a>",
          "description": "Interpreting how does deep neural networks (DNNs) make predictions is a vital\nfield in artificial intelligence, which hinders wide applications of DNNs.\nVisualization of learned representations helps we humans understand the vision\nof DNNs. In this work, visualized images that can activate the neural network\nto the target classes are generated by back-propagation method. Here, rotation\nand scaling operations are applied to introduce the transformation invariance\nin the image generating process, which we find a significant improvement on\nvisualization effect. Finally, we show some cases that such method can help us\nto gain insight into neural networks.",
          "link": "http://arxiv.org/abs/2106.09982",
          "publishedOn": "2021-06-21T02:07:38.047Z",
          "wordCount": 551,
          "title": "Towards interpreting computer vision based on transformation invariant optimization. (arXiv:2106.09982v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Awiszus_M/0/1/0/all/0/1\">Maren Awiszus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_F/0/1/0/all/0/1\">Frederik Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>",
          "description": "This work introduces World-GAN, the first method to perform data-driven\nProcedural Content Generation via Machine Learning in Minecraft from a single\nexample. Based on a 3D Generative Adversarial Network (GAN) architecture, we\nare able to create arbitrarily sized world snippets from a given sample. We\nevaluate our approach on creations from the community as well as structures\ngenerated with the Minecraft World Generator. Our method is motivated by the\ndense representations used in Natural Language Processing (NLP) introduced with\nword2vec [1]. The proposed block2vec representations make World-GAN independent\nfrom the number of different blocks, which can vary a lot in Minecraft, and\nenable the generation of larger levels. Finally, we demonstrate that changing\nthis new representation space allows us to change the generated style of an\nalready trained generator. World-GAN enables its users to generate Minecraft\nworlds based on parts of their creations.",
          "link": "http://arxiv.org/abs/2106.10155",
          "publishedOn": "2021-06-21T02:07:38.024Z",
          "wordCount": 593,
          "title": "World-GAN: a Generative Model for Minecraft Worlds. (arXiv:2106.10155v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1\">Fangwei Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenhan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1\">Tingyun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>",
          "description": "In active visual tracking, it is notoriously difficult when distracting\nobjects appear, as distractors often mislead the tracker by occluding the\ntarget or bringing a confusing appearance. To address this issue, we propose a\nmixed cooperative-competitive multi-agent game, where a target and multiple\ndistractors form a collaborative team to play against a tracker and make it\nfail to follow. Through learning in our game, diverse distracting behaviors of\nthe distractors naturally emerge, thereby exposing the tracker's weakness,\nwhich helps enhance the distraction-robustness of the tracker. For effective\nlearning, we then present a bunch of practical methods, including a reward\nfunction for distractors, a cross-modal teacher-student learning strategy, and\na recurrent attention mechanism for the tracker. The experimental results show\nthat our tracker performs desired distraction-robust active visual tracking and\ncan be well generalized to unseen environments. We also show that the\nmulti-agent game can be used to adversarially test the robustness of trackers.",
          "link": "http://arxiv.org/abs/2106.10110",
          "publishedOn": "2021-06-21T02:07:38.000Z",
          "wordCount": 601,
          "title": "Towards Distraction-Robust Active Visual Tracking. (arXiv:2106.10110v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_A/0/1/0/all/0/1\">A. Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">J. Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Y. Pang</a>",
          "description": "Pseudo-LiDAR based 3D object detectors have gained popularity due to their\nhigh accuracy. However, these methods need dense depth supervision and suffer\nfrom inferior speed. To solve these two issues, a recently introduced RTS3D\nbuilds an efficient 4D Feature-Consistency Embedding (FCE) space for the\nintermediate representation of object without depth supervision. FCE space\nsplits the entire object region into 3D uniform grid latent space for feature\nsampling point generation, which ignores the importance of different object\nregions. However, we argue that, compared with the inner region, the outer\nregion plays a more important role for accurate 3D detection. To encode more\ninformation from the outer region, we propose a shape prior non-uniform\nsampling strategy that performs dense sampling in outer region and sparse\nsampling in inner region. As a result, more points are sampled from the outer\nregion and more useful features are extracted for 3D detection. Further, to\nenhance the feature discrimination of each sampling point, we propose a\nhigh-level semantic enhanced FCE module to exploit more contextual information\nand suppress noise better. Experiments on the KITTI dataset are performed to\nshow the effectiveness of the proposed method. Compared with the baseline\nRTS3D, our proposed method has 2.57% improvement on AP3d almost without extra\nnetwork parameters. Moreover, our proposed method outperforms the\nstate-of-the-art methods without extra supervision at a real-time speed.",
          "link": "http://arxiv.org/abs/2106.10013",
          "publishedOn": "2021-06-21T02:07:37.993Z",
          "wordCount": 665,
          "title": "Shape Prior Non-Uniform Sampling Guided Real-time Stereo 3D Object Detection. (arXiv:2106.10013v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tropin_D/0/1/0/all/0/1\">D.V. Tropin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ershov_A/0/1/0/all/0/1\">A.M. Ershov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaev_D/0/1/0/all/0/1\">D.P. Nikolaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arlazarov_V/0/1/0/all/0/1\">V.V. Arlazarov</a>",
          "description": "The demand for on-device document recognition systems increases in\nconjunction with the emergence of more strict privacy and security\nrequirements. In such systems, there is no data transfer from the end device to\na third-party information processing servers. The response time is vital to the\nuser experience of on-device document recognition. Combined with the\nunavailability of discrete GPUs, powerful CPUs, or a large RAM capacity on\nconsumer-grade end devices such as smartphones, the time limitations put\nsignificant constraints on the computational complexity of the applied\nalgorithms for on-device execution.\n\nIn this work, we consider document location in an image without prior\nknowledge of the document content or its internal structure. In accordance with\nthe published works, at least 5 systems offer solutions for on-device document\nlocation. All these systems use a location method which can be considered\nHough-based. The precision of such systems seems to be lower than that of the\nstate-of-the-art solutions which were not designed to account for the limited\ncomputational resources.\n\nWe propose an advanced Hough-based method. In contrast with other approaches,\nit accounts for the geometric invariants of the central projection model and\ncombines both edge and color features for document boundary detection. The\nproposed method allowed for the second best result for SmartDoc dataset in\nterms of precision, surpassed by U-net like neural network. When evaluated on a\nmore challenging MIDV-500 dataset, the proposed algorithm guaranteed the best\nprecision compared to published methods. Our method retained the applicability\nto on-device computations.",
          "link": "http://arxiv.org/abs/2106.09987",
          "publishedOn": "2021-06-21T02:07:37.973Z",
          "wordCount": 695,
          "title": "Advanced Hough-based method for on-device document localization. (arXiv:2106.09987v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sungwon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Hyungtae Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1\">Hyun Myung</a>",
          "description": "Training a Convolutional Neural Network (CNN) to be robust against rotation\nhas mostly been done with data augmentation. In this paper, another progressive\nvision of research direction is highlighted to encourage less dependence on\ndata augmentation by achieving structural rotational invariance of a network.\nThe deep equivariance-bridged SO(2) invariant network is proposed to echo such\nvision. First, Self-Weighted Nearest Neighbors Graph Convolutional Network\n(SWN-GCN) is proposed to implement Graph Convolutional Network (GCN) on the\ngraph representation of an image to acquire rotationally equivariant\nrepresentation, as GCN is more suitable for constructing deeper network than\nspectral graph convolution-based approaches. Then, invariant representation is\neventually obtained with Global Average Pooling (GAP), a permutation-invariant\noperation suitable for aggregating high-dimensional representations, over the\nequivariant set of vertices retrieved from SWN-GCN. Our method achieves the\nstate-of-the-art image classification performance on rotated MNIST and CIFAR-10\nimages, where the models are trained with a non-augmented dataset only.\nQuantitative validations over invariance of the representations also\ndemonstrate strong invariance of deep representations of SWN-GCN over\nrotations.",
          "link": "http://arxiv.org/abs/2106.09996",
          "publishedOn": "2021-06-21T02:07:37.966Z",
          "wordCount": 601,
          "title": "Equivariance-bridged SO(2)-Invariant Representation Learning using Graph Convolutional Network. (arXiv:2106.09996v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1\">Tomoki Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1\">Paolo Favaro</a>",
          "description": "We propose a novel GAN training scheme that can handle any level of labeling\nin a unified manner. Our scheme introduces a form of artificial labeling that\ncan incorporate manually defined labels, when available, and induce an\nalignment between them. To define the artificial labels, we exploit the\nassumption that neural network generators can be trained more easily to map\nnearby latent vectors to data with semantic similarities, than across separate\ncategories. We use generated data samples and their corresponding artificial\nconditioning labels to train a classifier. The classifier is then used to\nself-label real data. To boost the accuracy of the self-labeling, we also use\nthe exponential moving average of the classifier. However, because the\nclassifier might still make mistakes, especially at the beginning of the\ntraining, we also refine the labels through self-attention, by using the\nlabeling of real data samples only when the classifier outputs a high\nclassification probability score. We evaluate our approach on CIFAR-10, STL-10\nand SVHN, and show that both self-labeling and self-attention consistently\nimprove the quality of generated data. More surprisingly, we find that the\nproposed scheme can even outperform class-conditional GANs.",
          "link": "http://arxiv.org/abs/2106.09914",
          "publishedOn": "2021-06-21T02:07:37.958Z",
          "wordCount": 626,
          "title": "A Unified Generative Adversarial Network Training via Self-Labeling and Self-Attention. (arXiv:2106.09914v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Huan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Rong Xiong</a>",
          "description": "We present a heterogeneous localization framework for solving radar global\nlocalization and pose tracking on pre-built lidar maps. To bridge the gap of\nsensing modalities, deep neural networks are constructed to create shared\nembedding space for radar scans and lidar maps. Herein learned feature\nembeddings are supportive for similarity measurement, thus improving map\nretrieval and data matching respectively. In RobotCar and MulRan datasets, we\ndemonstrate the effectiveness of the proposed framework with the comparison to\nScan Context and RaLL. In addition, the proposed pose tracking pipeline is with\nless neural networks compared to the original RaLL.",
          "link": "http://arxiv.org/abs/2106.10000",
          "publishedOn": "2021-06-21T02:07:37.939Z",
          "wordCount": 546,
          "title": "Improved Radar Localization on Lidar Maps Using Shared Embedding. (arXiv:2106.10000v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_I/0/1/0/all/0/1\">Indrajit Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schedl_D/0/1/0/all/0/1\">David C. Schedl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimber_O/0/1/0/all/0/1\">Oliver Bimber</a>",
          "description": "Fully autonomous drones have been demonstrated to find lost or injured\npersons under strongly occluding forest canopy. Airborne Optical Sectioning\n(AOS), a novel synthetic aperture imaging technique, together with\ndeep-learning-based classification enables high detection rates under realistic\nsearch-and-rescue conditions. We demonstrate that false detections can be\nsignificantly suppressed and true detections boosted by combining\nclassifications from multiple AOS rather than single integral images. This\nimproves classification rates especially in the presence of occlusion. To make\nthis possible, we modified the AOS imaging process to support large overlaps\nbetween subsequent integrals, enabling real-time and on-board scanning and\nprocessing of groundspeeds up to 10 m/s.",
          "link": "http://arxiv.org/abs/2106.10077",
          "publishedOn": "2021-06-21T02:07:37.932Z",
          "wordCount": 567,
          "title": "Combined Person Classification with Airborne Optical Sectioning. (arXiv:2106.10077v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "Collecting training data from untrusted sources exposes machine learning\nservices to poisoning adversaries, who maliciously manipulate training data to\ndegrade the model accuracy. When trained on offline datasets, poisoning\nadversaries have to inject the poisoned data in advance before training, and\nthe order of feeding these poisoned batches into the model is stochastic. In\ncontrast, practical systems are more usually trained/fine-tuned on sequentially\ncaptured real-time data, in which case poisoning adversaries could dynamically\npoison each data batch according to the current model state. In this paper, we\nfocus on the real-time settings and propose a new attacking strategy, which\naffiliates an accumulative phase with poisoning attacks to secretly (i.e.,\nwithout affecting accuracy) magnify the destructive effect of a (poisoned)\ntrigger batch. By mimicking online learning and federated learning on CIFAR-10,\nwe show that the model accuracy will significantly drop by a single update step\non the trigger batch after the accumulative phase. Our work validates that a\nwell-designed but straightforward attacking strategy can dramatically amplify\nthe poisoning effects, with no need to explore complex techniques.",
          "link": "http://arxiv.org/abs/2106.09993",
          "publishedOn": "2021-06-21T02:07:37.919Z",
          "wordCount": 615,
          "title": "Accumulative Poisoning Attacks on Real-time Data. (arXiv:2106.09993v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pintor_M/0/1/0/all/0/1\">Maura Pintor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demetrio_L/0/1/0/all/0/1\">Luca Demetrio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sotgiu_A/0/1/0/all/0/1\">Angelo Sotgiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manca_G/0/1/0/all/0/1\">Giovanni Manca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1\">Ambra Demontis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1\">Battista Biggio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1\">Fabio Roli</a>",
          "description": "Evaluating robustness of machine-learning models to adversarial examples is a\nchallenging problem. Many defenses have been shown to provide a false sense of\nsecurity by causing gradient-based attacks to fail, and they have been broken\nunder more rigorous evaluations. Although guidelines and best practices have\nbeen suggested to improve current adversarial robustness evaluations, the lack\nof automatic testing and debugging tools makes it difficult to apply these\nrecommendations in a systematic manner. In this work, we overcome these\nlimitations by (i) defining a set of quantitative indicators which unveil\ncommon failures in the optimization of gradient-based attacks, and (ii)\nproposing specific mitigation strategies within a systematic evaluation\nprotocol. Our extensive experimental analysis shows that the proposed\nindicators of failure can be used to visualize, debug and improve current\nadversarial robustness evaluations, providing a first concrete step towards\nautomatizing and systematizing current adversarial robustness evaluations. Our\nopen-source code is available at:\nhttps://github.com/pralab/IndicatorsOfAttackFailure.",
          "link": "http://arxiv.org/abs/2106.09947",
          "publishedOn": "2021-06-21T02:07:37.888Z",
          "wordCount": 609,
          "title": "Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples. (arXiv:2106.09947v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyu-Lim Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeong-Soo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Seung-Ri Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jun-Ho Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_C/0/1/0/all/0/1\">Chul-Min Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jong-Seok Lee</a>",
          "description": "A significant amount of work has been done on adversarial attacks that inject\nimperceptible noise to images to deteriorate the image classification\nperformance of deep models. However, most of the existing studies consider\nattacks in the digital (pixel) domain where an image acquired by an image\nsensor with sampling and quantization has been recorded. This paper, for the\nfirst time, introduces an optical adversarial attack, which physically alters\nthe light field information arriving at the image sensor so that the\nclassification model yields misclassification. More specifically, we modulate\nthe phase of the light in the Fourier domain using a spatial light modulator\nplaced in the photographic system. The operative parameters of the modulator\nare obtained by gradient-based optimization to maximize cross-entropy and\nminimize distortions. We present experiments based on both simulation and a\nreal hardware optical system, from which the feasibility of the proposed\noptical attack is demonstrated. It is also verified that the proposed attack is\ncompletely different from common optical-domain distortions such as spherical\naberration, defocus, and astigmatism in terms of both perturbation patterns and\nclassification results.",
          "link": "http://arxiv.org/abs/2106.09908",
          "publishedOn": "2021-06-21T02:07:37.880Z",
          "wordCount": 624,
          "title": "Light Lies: Optical Adversarial Attack. (arXiv:2106.09908v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1\">Lie Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Donghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wanji He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yelin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiwen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiufen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>",
          "description": "In medical image segmentation, it is difficult to mark ambiguous areas\naccurately with binary masks, especially when dealing with small lesions.\nTherefore, it is a challenge for radiologists to reach a consensus by using\nbinary masks under the condition of multiple annotations. However, these areas\nmay contain anatomical structures that are conducive to diagnosis. Uncertainty\nis introduced to study these situations. Nevertheless, the uncertainty is\nusually measured by the variances between predictions in a multiple trial way.\nIt is not intuitive, and there is no exact correspondence in the image.\nInspired by image matting, we introduce matting as a soft segmentation method\nand a new perspective to deal with and represent uncertain regions into medical\nscenes, namely medical matting. More specifically, because there is no\navailable medical matting dataset, we first labeled two medical datasets with\nalpha matte. Secondly, the matting method applied to the natural image is not\nsuitable for the medical scene, so we propose a new architecture to generate\nbinary masks and alpha matte in a row. Thirdly, the uncertainty map is\nintroduced to highlight the ambiguous regions from the binary results and\nimprove the matting performance. Evaluated on these datasets, the proposed\nmodel outperformed state-of-the-art matting algorithms by a large margin, and\nalpha matte is proved to be a more efficient labeling form than a binary mask.",
          "link": "http://arxiv.org/abs/2106.09887",
          "publishedOn": "2021-06-21T02:07:37.814Z",
          "wordCount": 675,
          "title": "Medical Matting: A New Perspective on Medical Segmentation with Uncertainty. (arXiv:2106.09887v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haiping Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1\">Robert Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_P/0/1/0/all/0/1\">Peizhen Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1\">Raivo E Koot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chasmai_M/0/1/0/all/0/1\">Mustafa Chasmai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schobs_L/0/1/0/all/0/1\">Lawrence Schobs</a>",
          "description": "Machine learning is a general-purpose technology holding promises for many\ninterdisciplinary research problems. However, significant barriers exist in\ncrossing disciplinary boundaries when most machine learning tools are developed\nin different areas separately. We present Pykale - a Python library for\nknowledge-aware machine learning on graphs, images, texts, and videos to enable\nand accelerate interdisciplinary research. We formulate new green machine\nlearning guidelines based on standard software engineering practices and\npropose a novel pipeline-based application programming interface (API). PyKale\nfocuses on leveraging knowledge from multiple sources for accurate and\ninterpretable prediction, thus supporting multimodal learning and transfer\nlearning (particularly domain adaptation) with latest deep learning and\ndimensionality reduction models. We build PyKale on PyTorch and leverage the\nrich PyTorch ecosystem. Our pipeline-based API design enforces standardization\nand minimalism, embracing green machine learning concepts via reducing\nrepetitions and redundancy, reusing existing resources, and recycling learning\nmodels across areas. We demonstrate its interdisciplinary nature via examples\nin bioinformatics, knowledge graph, image/video recognition, and medical\nimaging.",
          "link": "http://arxiv.org/abs/2106.09756",
          "publishedOn": "2021-06-21T02:07:37.799Z",
          "wordCount": 628,
          "title": "PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python. (arXiv:2106.09756v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stember_J/0/1/0/all/0/1\">Joseph Stember</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1\">Hrithwik Shalu</a>",
          "description": "Purpose: Image classification is perhaps the most fundamental task in imaging\nAI. However, labeling images is time-consuming and tedious. We have recently\ndemonstrated that reinforcement learning (RL) can classify 2D slices of MRI\nbrain images with high accuracy. Here we make two important steps toward\nspeeding image classification: Firstly, we automatically extract class labels\nfrom the clinical reports. Secondly, we extend our prior 2D classification work\nto fully 3D image volumes from our institution. Hence, we proceed as follows:\nin Part 1, we extract labels from reports automatically using the SBERT natural\nlanguage processing approach. Then, in Part 2, we use these labels with RL to\ntrain a classification Deep-Q Network (DQN) for 3D image volumes.\n\nMethods: For Part 1, we trained SBERT with 90 radiology report impressions.\nWe then used the trained SBERT to predict class labels for use in Part 2. In\nPart 2, we applied multi-step image classification to allow for combined Deep-Q\nlearning using 3D convolutions and TD(0) Q learning. We trained on a set of 90\nimages. We tested on a separate set of 61 images, again using the classes\npredicted from patient reports by the trained SBERT in Part 1. For comparison,\nwe also trained and tested a supervised deep learning classification network on\nthe same set of training and testing images using the same labels.\n\nResults: Part 1: Upon training with the corpus of radiology reports, the\nSBERT model had 100% accuracy for both normal and metastasis-containing scans.\nPart 2: Then, using these labels, whereas the supervised approach quickly\noverfit the training data and as expected performed poorly on the testing set\n(66% accuracy, just over random guessing), the reinforcement learning approach\nachieved an accuracy of 92%. The results were found to be statistically\nsignificant, with a p-value of 3.1 x 10^-5.",
          "link": "http://arxiv.org/abs/2106.09812",
          "publishedOn": "2021-06-21T02:07:37.792Z",
          "wordCount": 751,
          "title": "Deep reinforcement learning with automated label extraction from clinical reports accurately classifies 3D MRI brain volumes. (arXiv:2106.09812v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qigong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiufang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">Fanhua Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Licheng Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>",
          "description": "The training of deep neural networks (DNNs) always requires intensive\nresources for both computation and data storage. Thus, DNNs cannot be\nefficiently applied to mobile phones and embedded devices, which severely\nlimits their applicability in industrial applications. To address this issue,\nwe propose a novel encoding scheme using {-1, +1} to decompose quantized neural\nnetworks (QNNs) into multi-branch binary networks, which can be efficiently\nimplemented by bitwise operations (i.e., xnor and bitcount) to achieve model\ncompression, computational acceleration, and resource saving. By using our\nmethod, users can achieve different encoding precisions arbitrarily according\nto their requirements and hardware resources. The proposed mechanism is highly\nsuitable for the use of FPGA and ASIC in terms of data storage and computation,\nwhich provides a feasible idea for smart chips. We validate the effectiveness\nof our method on large-scale image classification (e.g., ImageNet), object\ndetection, and semantic segmentation tasks. In particular, our method with\nlow-bit encoding can still achieve almost the same performance as its high-bit\ncounterparts.",
          "link": "http://arxiv.org/abs/2106.09886",
          "publishedOn": "2021-06-21T02:07:37.784Z",
          "wordCount": 619,
          "title": "Quantized Neural Networks via {-1, +1} Encoding Decomposition and Acceleration. (arXiv:2106.09886v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Minghai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zejiang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Kuang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>",
          "description": "Deep neural networks (DNNs) are effective in solving many real-world\nproblems. Larger DNN models usually exhibit better quality (e.g., accuracy) but\ntheir excessive computation results in long training and inference time. Model\nsparsification can reduce the computation and memory cost while maintaining\nmodel quality. Most existing sparsification algorithms unidirectionally remove\nweights, while others randomly or greedily explore a small subset of weights in\neach layer. The inefficiency of the algorithms reduces the achievable sparsity\nlevel. In addition, many algorithms still require pre-trained dense models and\nthus suffer from large memory footprint and long training time. In this paper,\nwe propose a novel scheduled grow-and-prune (GaP) methodology without\npre-training the dense models. It addresses the shortcomings of the previous\nworks by repeatedly growing a subset of layers to dense and then pruning back\nto sparse after some training. Experiments have shown that such models can\nmatch or beat the quality of highly optimized dense models at 80% sparsity on a\nvariety of tasks, such as image classification, objective detection, 3D object\npart segmentation, and translation. They also outperform other state-of-the-art\n(SOTA) pruning methods, including pruning from pre-trained dense models. As an\nexample, a 90% sparse ResNet-50 obtained via GaP achieves 77.9% top-1 accuracy\non ImageNet, improving the SOTA results by 1.5%.",
          "link": "http://arxiv.org/abs/2106.09857",
          "publishedOn": "2021-06-21T02:07:37.777Z",
          "wordCount": 669,
          "title": "Effective Model Sparsification by Scheduled Grow-and-Prune Methods. (arXiv:2106.09857v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1\">Shuyue Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loew_M/0/1/0/all/0/1\">Murray Loew</a>",
          "description": "To evaluate clustering results is a significant part of cluster analysis.\nSince there are no true class labels for clustering in typical unsupervised\nlearning, many internal cluster validity indices (CVIs), which use predicted\nlabels and data, have been created. Without true labels, to design an effective\nCVI is as difficult as to create a clustering method. And it is crucial to have\nmore CVIs because there are no universal CVIs that can be used to measure all\ndatasets and no specific methods of selecting a proper CVI for clusters without\ntrue labels. Therefore, to apply a variety of CVIs to evaluate clustering\nresults is necessary. In this paper, we propose a novel internal CVI -- the\nDistance-based Separability Index (DSI), based on a data separability measure.\nWe compared the DSI with eight internal CVIs including studies from early Dunn\n(1974) to most recent CVDD (2019) and an external CVI as ground truth, by using\nclustering results of five clustering algorithms on 12 real and 97 synthetic\ndatasets. Results show DSI is an effective, unique, and competitive CVI to\nother compared CVIs. We also summarized the general process to evaluate CVIs\nand created the rank-difference metric for comparison of CVIs' results.",
          "link": "http://arxiv.org/abs/2106.09794",
          "publishedOn": "2021-06-21T02:07:37.759Z",
          "wordCount": 647,
          "title": "A Distance-based Separability Measure for Internal Cluster Validation. (arXiv:2106.09794v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09748",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weber_M/0/1/0/all/0/1\">Mark Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Siyuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1\">Maxwell D. Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yukun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Liangzhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qihang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taixe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan L. Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroff_F/0/1/0/all/0/1\">Florian Schroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1\">Hartwig Adam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang-Chieh Chen</a>",
          "description": "DeepLab2 is a TensorFlow library for deep labeling, aiming to provide a\nstate-of-the-art and easy-to-use TensorFlow codebase for general dense pixel\nprediction problems in computer vision. DeepLab2 includes all our recently\ndeveloped DeepLab model variants with pretrained checkpoints as well as model\ntraining and evaluation code, allowing the community to reproduce and further\nimprove upon the state-of-art systems. To showcase the effectiveness of\nDeepLab2, our Panoptic-DeepLab employing Axial-SWideRNet as network backbone\nachieves 68.0% PQ or 83.5% mIoU on Cityscaspes validation set, with only\nsingle-scale inference and ImageNet-1K pretrained checkpoints. We hope that\npublicly sharing our library could facilitate future research on dense pixel\nlabeling tasks and envision new applications of this technology. Code is made\npublicly available at \\url{https://github.com/google-research/deeplab2}.",
          "link": "http://arxiv.org/abs/2106.09748",
          "publishedOn": "2021-06-21T02:07:37.711Z",
          "wordCount": 592,
          "title": "DeepLab2: A TensorFlow Library for Deep Labeling. (arXiv:2106.09748v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaolin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenghua Xu</a>",
          "description": "Imbalanced datasets widely exist in practice and area great challenge for\ntraining deep neural models with agood generalization on infrequent classes. In\nthis work, wepropose a new rare-class sample generator (RSG) to solvethis\nproblem. RSG aims to generate some new samplesfor rare classes during training,\nand it has in particularthe following advantages: (1) it is convenient to use\nandhighly versatile, because it can be easily integrated intoany kind of\nconvolutional neural network, and it works wellwhen combined with different\nloss functions, and (2) it isonly used during the training phase, and\ntherefore, no ad-ditional burden is imposed on deep neural networks duringthe\ntesting phase. In extensive experimental evaluations, weverify the\neffectiveness of RSG. Furthermore, by leveragingRSG, we obtain competitive\nresults on Imbalanced CIFARand new state-of-the-art results on Places-LT,\nImageNet-LT, and iNaturalist 2018. The source code is available at\nhttps://github.com/Jianf-Wang/RSG.",
          "link": "http://arxiv.org/abs/2106.09859",
          "publishedOn": "2021-06-21T02:07:37.694Z",
          "wordCount": 606,
          "title": "RSG: A Simple but Effective Module for Learning Imbalanced Datasets. (arXiv:2106.09859v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09832",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gaggion_N/0/1/0/all/0/1\">Nicol&#xe1;s Gaggion</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mansilla_L/0/1/0/all/0/1\">Lucas Mansilla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milone_D/0/1/0/all/0/1\">Diego Milone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferrante_E/0/1/0/all/0/1\">Enzo Ferrante</a>",
          "description": "In this work we address the problem of landmark-based segmentation for\nanatomical structures. We propose HybridGNet, an encoder-decoder neural\narchitecture which combines standard convolutions for image feature encoding,\nwith graph convolutional neural networks to decode plausible representations of\nanatomical structures. We benchmark the proposed architecture considering other\nstandard landmark and pixel-based models for anatomical segmentation in chest\nx-ray images, and found that HybridGNet is more robust to image occlusions. We\nalso show that it can be used to construct landmark-based segmentations from\npixel level annotations. Our experimental results suggest that HybridGNet\nproduces accurate and anatomically plausible landmark-based segmentations, by\nnaturally incorporating shape constraints within the decoding process via\nspectral convolutions.",
          "link": "http://arxiv.org/abs/2106.09832",
          "publishedOn": "2021-06-21T02:07:37.671Z",
          "wordCount": 567,
          "title": "Hybrid graph convolutional neural networks for landmark-based anatomical segmentation. (arXiv:2106.09832v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09834",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Weiwen Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebrahimian_S/0/1/0/all/0/1\">Shadi Ebrahimian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1\">Hengyong Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalra_M/0/1/0/all/0/1\">Mannu Kalra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>",
          "description": "By the ALARA (As Low As Reasonably Achievable) principle, ultra-low-dose CT\nreconstruction is a holy grail to minimize cancer risks and genetic damages,\nespecially for children. With the development of medical CT technologies, the\niterative algorithms are widely used to reconstruct decent CT images from a\nlow-dose scan. Recently, artificial intelligence (AI) techniques have shown a\ngreat promise in further reducing CT radiation dose to the next level. In this\npaper, we demonstrate that AI-powered CT reconstruction offers diagnostic image\nquality at an ultra-low-dose level comparable to that of radiography.\nSpecifically, here we develop a Split Unrolled Grid-like Alternative\nReconstruction (SUGAR) network, in which deep learning, physical modeling and\nimage prior are integrated. The reconstruction results from clinical datasets\nshow that excellent images can be reconstructed using SUGAR from 36\nprojections. This approach has a potential to change future healthcare.",
          "link": "http://arxiv.org/abs/2106.09834",
          "publishedOn": "2021-06-21T02:07:37.644Z",
          "wordCount": 597,
          "title": "AI-Enabled Ultra-Low-Dose CT Reconstruction. (arXiv:2106.09834v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>",
          "description": "This paper investigates two techniques for developing efficient\nself-supervised vision transformers (EsViT) for visual representation learning.\nFirst, we show through a comprehensive empirical study that multi-stage\narchitectures with sparse self-attentions can significantly reduce modeling\ncomplexity but with a cost of losing the ability to capture fine-grained\ncorrespondences between image regions. Second, we propose a new pre-training\ntask of region matching which allows the model to capture fine-grained region\ndependencies and as a result significantly improves the quality of the learned\nvision representations. Our results show that combining the two techniques,\nEsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation,\noutperforming prior arts with around an order magnitude of higher throughput.\nWhen transferring to downstream linear classification tasks, EsViT outperforms\nits supervised counterpart on 17 out of 18 datasets. The code and models will\nbe publicly available.",
          "link": "http://arxiv.org/abs/2106.09785",
          "publishedOn": "2021-06-21T02:07:37.637Z",
          "wordCount": 595,
          "title": "Efficient Self-supervised Vision Transformers for Representation Learning. (arXiv:2106.09785v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kapishnikov_A/0/1/0/all/0/1\">Andrei Kapishnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1\">Subhashini Venugopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avci_B/0/1/0/all/0/1\">Besim Avci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wedin_B/0/1/0/all/0/1\">Ben Wedin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terry_M/0/1/0/all/0/1\">Michael Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolukbasi_T/0/1/0/all/0/1\">Tolga Bolukbasi</a>",
          "description": "Integrated Gradients (IG) is a commonly used feature attribution method for\ndeep neural networks. While IG has many desirable properties, the method often\nproduces spurious/noisy pixel attributions in regions that are not related to\nthe predicted class when applied to visual models. While this has been\npreviously noted, most existing solutions are aimed at addressing the symptoms\nby explicitly reducing the noise in the resulting attributions. In this work,\nwe show that one of the causes of the problem is the accumulation of noise\nalong the IG path. To minimize the effect of this source of noise, we propose\nadapting the attribution path itself -- conditioning the path not just on the\nimage but also on the model being explained. We introduce Adaptive Path Methods\n(APMs) as a generalization of path methods, and Guided IG as a specific\ninstance of an APM. Empirically, Guided IG creates saliency maps better aligned\nwith the model's prediction and the input image that is being explained. We\nshow through qualitative and quantitative experiments that Guided IG\noutperforms other, related methods in nearly every experiment.",
          "link": "http://arxiv.org/abs/2106.09788",
          "publishedOn": "2021-06-21T02:07:37.630Z",
          "wordCount": 657,
          "title": "Guided Integrated Gradients: An Adaptive Path Method for Removing Noise. (arXiv:2106.09788v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengrui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>",
          "description": "In recent years, multi-view subspace clustering has achieved impressive\nperformance due to the exploitation of complementary imformation across\nmultiple views. However, multi-view data can be very complicated and are not\neasy to cluster in real-world applications. Most existing methods operate on\nraw data and may not obtain the optimal solution. In this work, we propose a\nnovel multi-view clustering method named smoothed multi-view subspace\nclustering (SMVSC) by employing a novel technique, i.e., graph filtering, to\nobtain a smooth representation for each view, in which similar data points have\nsimilar feature values. Specifically, it retains the graph geometric features\nthrough applying a low-pass filter. Consequently, it produces a\n``clustering-friendly\" representation and greatly facilitates the downstream\nclustering task. Extensive experiments on benchmark datasets validate the\nsuperiority of our approach. Analysis shows that graph filtering increases the\nseparability of classes.",
          "link": "http://arxiv.org/abs/2106.09875",
          "publishedOn": "2021-06-21T02:07:37.609Z",
          "wordCount": 584,
          "title": "Smoothed Multi-View Subspace Clustering. (arXiv:2106.09875v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09759",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zunair_H/0/1/0/all/0/1\">Hasib Zunair</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamza_A/0/1/0/all/0/1\">A. Ben Hamza</a>",
          "description": "We introduce a new dataset called Synthetic COVID-19 Chest X-ray Dataset for\ntraining machine learning models. The dataset consists of 21,295 synthetic\nCOVID-19 chest X-ray images to be used for computer-aided diagnosis. These\nimages, generated via an unsupervised domain adaptation approach, are of high\nquality. We find that the synthetic images not only improve performance of\nvarious deep learning architectures when used as additional training data under\nheavy imbalance conditions, but also detect the target class with high\nconfidence. We also find that comparable performance can also be achieved when\ntrained only on synthetic images. Further, salient features of the synthetic\nCOVID-19 images indicate that the distribution is significantly different from\nNon-COVID-19 classes, enabling a proper decision boundary. We hope the\navailability of such high fidelity chest X-ray images of COVID-19 will\nencourage advances in the development of diagnostic and/or management tools.",
          "link": "http://arxiv.org/abs/2106.09759",
          "publishedOn": "2021-06-21T02:07:37.584Z",
          "wordCount": 633,
          "title": "Synthetic COVID-19 Chest X-ray Dataset for Computer-Aided Diagnosis. (arXiv:2106.09759v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_R/0/1/0/all/0/1\">Ryan Kim</a>",
          "description": "Throughout the COVID-19 pandemic, the most common symptom displayed by\npatients has been a fever, leading to the use of temperature scanning as a\npreemptive measure to detect potential carriers of the virus. Human employees\nwith handheld thermometers have been used to fulfill this task, however this\nputs them at risk as they cannot be physically distanced and the sequential\nnature of this method leads to great inconveniences and inefficiency. The\nproposed solution is an autonomously navigating robot capable of conversing and\nscanning people's temperature to detect fevers and help screen for COVID-19. To\nsatisfy this objective, the robot must be able to (1) navigate autonomously,\n(2) detect and track people, and (3) get individuals' temperature reading and\nconverse with them if it exceeds 38{\\deg}C. An autonomously navigating mobile\nrobot is used with a manipulator controlled using a face tracking algorithm,\nand an end effector consisting of a thermal camera, smartphone, and chatbot.\nThe goal is to develop a functioning solution that performs the above tasks. In\naddition, technical challenges encountered and their engineering solutions will\nbe presented, and recommendations will be made for enhancements that could be\nincorporated when approaching commercialization.",
          "link": "http://arxiv.org/abs/2106.09894",
          "publishedOn": "2021-06-21T02:07:37.576Z",
          "wordCount": 684,
          "title": "Development of a conversing and body temperature scanning autonomously navigating robot to help screen for COVID-19. (arXiv:2106.09894v1 [cs.RO])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.12027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yanjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting-hao/0/1/0/all/0/1\">Ting-hao</a> (Kenneth) <a href=\"http://arxiv.org/find/cs/1/au:+Huang/0/1/0/all/0/1\">Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1\">Rebecca J. Passonneau</a>",
          "description": "Atomic clauses are fundamental text units for understanding complex\nsentences. Identifying the atomic sentences within complex sentences is\nimportant for applications such as summarization, argument mining, discourse\nanalysis, discourse parsing, and question answering. Previous work mainly\nrelies on rule-based methods dependent on parsing. We propose a new task to\ndecompose each complex sentence into simple sentences derived from the tensed\nclauses in the source, and a novel problem formulation as a graph edit task.\nOur neural model learns to Accept, Break, Copy or Drop elements of a graph that\ncombines word adjacency and grammatical dependencies. The full processing\npipeline includes modules for graph construction, graph editing, and sentence\ngeneration from the output graph. We introduce DeSSE, a new dataset designed to\ntrain and evaluate complex sentence decomposition, and MinWiki, a subset of\nMinWikiSplit. ABCD achieves comparable performance as two parsing baselines on\nMinWiki. On DeSSE, which has a more even balance of complex sentence types, our\nmodel achieves higher accuracy on the number of atomic sentences than an\nencoder-decoder baseline. Results include a detailed error analysis.",
          "link": "http://arxiv.org/abs/2106.12027",
          "publishedOn": "2021-06-24T01:51:45.769Z",
          "wordCount": 649,
          "title": "ABCD: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences. (arXiv:2106.12027v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roland S. Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borowski_J/0/1/0/all/0/1\">Judy Borowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1\">Robert Geirhos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallis_T/0/1/0/all/0/1\">Thomas S. A. Wallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>",
          "description": "One widely used approach towards understanding the inner workings of deep\nconvolutional neural networks is to visualize unit responses via activation\nmaximization. Feature visualizations via activation maximization are thought to\nprovide humans with precise information about the image features that cause a\nunit to be activated. If this is indeed true, these synthetic images should\nenable humans to predict the effect of an intervention, such as whether\noccluding a certain patch of the image (say, a dog's head) changes a unit's\nactivation. Here, we test this hypothesis by asking humans to predict which of\ntwo square occlusions causes a larger change to a unit's activation. Both a\nlarge-scale crowdsourced experiment and measurements with experts show that on\naverage, the extremely activating feature visualizations by Olah et al. (2017)\nindeed help humans on this task ($67 \\pm 4\\%$ accuracy; baseline performance\nwithout any visualizations is $60 \\pm 3\\%$). However, they do not provide any\nsignificant advantage over other visualizations (such as e.g. dataset samples),\nwhich yield similar performance ($66 \\pm 3\\%$ to $67 \\pm 3\\%$ accuracy). Taken\ntogether, we propose an objective psychophysical task to quantify the benefit\nof unit-level interpretability methods for humans, and find no evidence that\nfeature visualizations provide humans with better \"causal understanding\" than\nsimple alternative visualizations.",
          "link": "http://arxiv.org/abs/2106.12447",
          "publishedOn": "2021-06-24T01:51:45.764Z",
          "wordCount": 688,
          "title": "How Well do Feature Visualizations Support Causal Understanding of CNN Activations?. (arXiv:2106.12447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">R. Kenny Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanocka_R/0/1/0/all/0/1\">Rana Hanocka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>",
          "description": "Many learning-based 3D shape semantic segmentation methods assign labels to\nshape atoms (e.g. points in a point cloud or faces in a mesh) with a\nsingle-pass approach trained in an end-to-end fashion. Such methods achieve\nimpressive performance but require large amounts of labeled training data. This\nparadigm entangles two separable subproblems: (1) decomposing a shape into\nregions and (2) assigning semantic labels to these regions. We claim that\ndisentangling these subproblems reduces the labeled data burden: (1) region\ndecomposition requires no semantic labels and could be performed in an\nunsupervised fashion, and (2) labeling shape regions instead of atoms results\nin a smaller search space and should be learnable with less labeled training\ndata. In this paper, we investigate this second claim by presenting the\nNeurally-Guided Shape Parser (NGSP), a method that learns how to assign\nsemantic labels to regions of an over-segmented 3D shape. We solve this problem\nvia MAP inference, modeling the posterior probability of a labeling assignment\nconditioned on an input shape. We employ a Monte Carlo importance sampling\napproach guided by a neural proposal network, a search-based approach made\nfeasible by assuming the input shape is decomposed into discrete regions. We\nevaluate NGSP on the task of hierarchical semantic segmentation on manufactured\n3D shapes from PartNet. We find that NGSP delivers significant performance\nimprovements over baselines that learn to label shape atoms and then aggregate\npredictions for each shape region, especially in low-data regimes. Finally, we\ndemonstrate that NGSP is robust to region granularity, as it maintains strong\nsegmentation performance even as the regions undergo significant corruption.",
          "link": "http://arxiv.org/abs/2106.12026",
          "publishedOn": "2021-06-24T01:51:45.758Z",
          "wordCount": 719,
          "title": "The Neurally-Guided Shape Parser: A Monte Carlo Method for Hierarchical Labeling of Over-segmented 3D Shapes. (arXiv:2106.12026v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junru Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Ye Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>",
          "description": "Neural Architecture Search (NAS) often trains and evaluates a large number of\narchitectures. Recent predictor-based NAS approaches attempt to address such\nheavy computation costs with two key steps: sampling some\narchitecture-performance pairs and fitting a proxy accuracy predictor. Given\nlimited samples, these predictors, however, are far from accurate to locate top\narchitectures due to the difficulty of fitting the huge search space. This\npaper reflects on a simple yet crucial question: if our final goal is to find\nthe best architecture, do we really need to model the whole space well?. We\npropose a paradigm shift from fitting the whole architecture space using one\nstrong predictor, to progressively fitting a search path towards the\nhigh-performance sub-space through a set of weaker predictors. As a key\nproperty of the proposed weak predictors, their probabilities of sampling\nbetter architectures keep increasing. Hence we only sample a few well-performed\narchitectures guided by the previously learned predictor and estimate a new\nbetter weak predictor. This embarrassingly easy framework produces\ncoarse-to-fine iteration to refine the ranking of sampling space gradually.\nExtensive experiments demonstrate that our method costs fewer samples to find\ntop-performance architectures on NAS-Bench-101 and NAS-Bench-201, as well as\nachieves the state-of-the-art ImageNet performance on the NASNet search space.\nIn particular, compared to state-of-the-art (SOTA) predictor-based NAS methods,\nWeakNAS outperforms all of them with notable margins, e.g., requiring at least\n7.5x less samples to find global optimal on NAS-Bench-101; and WeakNAS can also\nabsorb them for further performance boost. We further strike the new SOTA\nresult of 81.3% in the ImageNet MobileNet Search Space. The code is available\nat https://github.com/VITA-Group/WeakNAS.",
          "link": "http://arxiv.org/abs/2102.10490",
          "publishedOn": "2021-06-24T01:51:45.753Z",
          "wordCount": 746,
          "title": "Stronger NAS with Weaker Predictors. (arXiv:2102.10490v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12200",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1\">Branislav Kveton</a>",
          "description": "This paper studies regret minimization in multi-armed bandits, a classical\nonline learning problem. To develop more statistically-efficient algorithms, we\npropose to use the assumption of a random-effect model. In this model, the mean\nrewards of arms are drawn independently from an unknown distribution, whose\nparameters we estimate. We provide an estimator of the arm means in this model\nand also analyze its uncertainty. Based on these results, we design a UCB\nalgorithm, which we call ReUCB. We analyze ReUCB and prove a Bayes regret bound\non its $n$-round regret, which matches an existing lower bound. Our experiments\nshow that ReUCB can outperform Thompson sampling in various scenarios, without\nassuming that the prior distribution of arm means is known.",
          "link": "http://arxiv.org/abs/2106.12200",
          "publishedOn": "2021-06-24T01:51:45.748Z",
          "wordCount": 539,
          "title": "Random Effect Bandits. (arXiv:2106.12200v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.01593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vial_D/0/1/0/all/0/1\">Daniel Vial</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parulekar_A/0/1/0/all/0/1\">Advait Parulekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1\">R. Srikant</a>",
          "description": "We propose two algorithms that use linear function approximation (LFA) for\nstochastic shortest path (SSP) and bound their regret over $K$ episodes. When\nall stationary policies are proper, our first algorithm obtains sublinear\nregret ($K^{3/4}$), is computationally efficient, and uses stationary policies.\nThis is the first LFA algorithm with these three properties, to the best of our\nknowledge. Our second algorithm improves the regret to $\\sqrt{K}$ when the\nfeature vectors satisfy certain assumptions. Both algorithms are special cases\nof a more general one, which has $\\sqrt{K}$ regret for general features given\naccess to a certain computation oracle. These algorithms and regret bounds are\nthe first for SSP with function approximation.",
          "link": "http://arxiv.org/abs/2105.01593",
          "publishedOn": "2021-06-24T01:51:45.741Z",
          "wordCount": 594,
          "title": "Regret Bounds for Stochastic Shortest Path Problems with Linear Function Approximation. (arXiv:2105.01593v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12382",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tong_L/0/1/0/all/0/1\">Lang Tong</a>",
          "description": "An innovations sequence of a time series is a sequence of independent and\nidentically distributed random variables with which the original time series\nhas a causal representation. The innovation at a time is statistically\nindependent of the prior history of the time series. As such, it represents the\nnew information contained at present but not in the past. Because of its simple\nprobability structure, an innovations sequence is the most efficient signature\nof the original. Unlike the principle or independent analysis (PCA/ICA)\nrepresentations, an innovations sequence preserves not only the complete\nstatistical properties but also the temporal order of the original time series.\nAn long-standing open problem is to find a computationally tractable way to\nextract an innovations sequence of non-Gaussian processes. This paper presents\na deep learning approach, referred to as Innovations Autoencoder (IAE), that\nextracts innovations sequences using a causal convolutional neural network. An\napplication of IAE to nonparametric anomaly detection with unknown anomaly and\nanomaly-free models is also presented.",
          "link": "http://arxiv.org/abs/2106.12382",
          "publishedOn": "2021-06-24T01:51:45.726Z",
          "wordCount": 595,
          "title": "Innovations Autoencoder and its Application in Real-Time Anomaly Detection. (arXiv:2106.12382v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Albaseer_A/0/1/0/all/0/1\">Abdullatif Albaseer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_M/0/1/0/all/0/1\">Mohamed Abdallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Fuqaha_A/0/1/0/all/0/1\">Ala Al-Fuqaha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1\">Aiman Erbad</a>",
          "description": "In Federated edge learning (FEEL), energy-constrained devices at the network\nedge consume significant energy when training and uploading their local machine\nlearning models, leading to a decrease in their lifetime. This work proposes\nnovel solutions for energy-efficient FEEL by jointly considering local training\ndata, available computation, and communications resources, and deadline\nconstraints of FEEL rounds to reduce energy consumption. This paper considers a\nsystem model where the edge server is equipped with multiple antennas employing\nbeamforming techniques to communicate with the local users through orthogonal\nchannels. Specifically, we consider a problem that aims to find the optimal\nuser's resources, including the fine-grained selection of relevant training\nsamples, bandwidth, transmission power, beamforming weights, and processing\nspeed with the goal of minimizing the total energy consumption given a deadline\nconstraint on the communication rounds of FEEL. Then, we devise tractable\nsolutions by first proposing a novel fine-grained training algorithm that\nexcludes less relevant training samples and effectively chooses only the\nsamples that improve the model's performance. After that, we derive closed-form\nsolutions, followed by a Golden-Section-based iterative algorithm to find the\noptimal computation and communication resources that minimize energy\nconsumption. Experiments using MNIST and CIFAR-10 datasets demonstrate that our\nproposed algorithms considerably outperform the state-of-the-art solutions as\nenergy consumption decreases by 79% for MNIST and 73% for CIFAR-10 datasets.",
          "link": "http://arxiv.org/abs/2106.12561",
          "publishedOn": "2021-06-24T01:51:45.721Z",
          "wordCount": 653,
          "title": "Fine-Grained Data Selection for Improved Energy Efficiency of Federated Edge Learning. (arXiv:2106.12561v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhengqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1\">Tianyu Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zihui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonglong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>",
          "description": "Transformers recently are adapted from the community of natural language\nprocessing as a promising substitute of convolution-based neural networks for\nvisual learning tasks. However, its supremacy degenerates given an insufficient\namount of training data (e.g., ImageNet). To make it into practical utility, we\npropose a novel distillation-based method to train vision transformers. Unlike\nprevious works, where merely heavy convolution-based teachers are provided, we\nintroduce lightweight teachers with different architectural inductive biases\n(e.g., convolution and involution) to co-advise the student transformer. The\nkey is that teachers with different inductive biases attain different knowledge\ndespite that they are trained on the same dataset, and such different knowledge\ncompounds and boosts the student's performance during distillation. Equipped\nwith this cross inductive bias distillation method, our vision transformers\n(termed as CivT) outperform all previous transformers of the same architecture\non ImageNet.",
          "link": "http://arxiv.org/abs/2106.12378",
          "publishedOn": "2021-06-24T01:51:45.707Z",
          "wordCount": 577,
          "title": "Co-advise: Cross Inductive Bias Distillation. (arXiv:2106.12378v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12190",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rahmani_M/0/1/0/all/0/1\">Mostafa Rahmani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>",
          "description": "The idea of Innovation Search, which was initially proposed for data\nclustering, was recently used for outlier detection. In the application of\nInnovation Search for outlier detection, the directions of innovation were\nutilized to measure the innovation of the data points. We study the Innovation\nValues computed by the Innovation Search algorithm under a quadratic cost\nfunction and it is proved that Innovation Values with the new cost function are\nequivalent to Leverage Scores. This interesting connection is utilized to\nestablish several theoretical guarantees for a Leverage Score based robust PCA\nmethod and to design a new robust PCA method. The theoretical results include\nperformance guarantees with different models for the distribution of outliers\nand the distribution of inliers. In addition, we demonstrate the robustness of\nthe algorithms against the presence of noise. The numerical and theoretical\nstudies indicate that while the presented approach is fast and closed-form, it\ncan outperform most of the existing algorithms.",
          "link": "http://arxiv.org/abs/2106.12190",
          "publishedOn": "2021-06-24T01:51:45.703Z",
          "wordCount": 610,
          "title": "Closed-Form, Provable, and Robust PCA via Leverage Statistics and Innovation Search. (arXiv:2106.12190v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryabinin_M/0/1/0/all/0/1\">Max Ryabinin</a>",
          "description": "Commonsense reasoning is one of the key problems in natural language\nprocessing, but the relative scarcity of labeled data holds back the progress\nfor languages other than English. Pretrained cross-lingual models are a source\nof powerful language-agnostic representations, yet their inherent reasoning\ncapabilities are still actively studied. In this work, we design a simple\napproach to commonsense reasoning which trains a linear classifier with weights\nof multi-head attention as features. To evaluate this approach, we create a\nmultilingual Winograd Schema corpus by processing several datasets from prior\nwork within a standardized pipeline and measure cross-lingual generalization\nability in terms of out-of-sample performance. The method performs\ncompetitively with recent supervised and unsupervised approaches for\ncommonsense reasoning, even when applied to other languages in a zero-shot\nmanner. Also, we demonstrate that most of the performance is given by the same\nsmall subset of attention heads for all studied languages, which provides\nevidence of universal reasoning capabilities in multilingual encoders.",
          "link": "http://arxiv.org/abs/2106.12066",
          "publishedOn": "2021-06-24T01:51:45.698Z",
          "wordCount": 622,
          "title": "It's All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning. (arXiv:2106.12066v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2002.02508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostina_V/0/1/0/all/0/1\">Victoria Kostina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassibi_B/0/1/0/all/0/1\">Babak Hassibi</a>",
          "description": "This paper considers quantized distributed optimization algorithms in the\nparameter server framework of distributed training. We introduce the principle\nwe call Differential Quantization (DQ) that prescribes that the past\nquantization errors should be compensated in such a way as to direct the\ndescent trajectory of a quantized algorithm towards that of its unquantized\ncounterpart. Assuming that the objective function is smooth and strongly\nconvex, we prove that in the limit of large problem dimension, Differentially\nQuantized Gradient Descent (DQ-GD) attains a linear contraction factor of\n$\\max\\{\\sigma_{\\mathrm{GD}}, 2^{-R}\\}$, where $\\sigma_{\\mathrm{GD}}$ is the\ncontraction factor of unquantized gradient descent (GD). Thus at any\n$R\\geq\\log_2 1 /\\sigma_{\\mathrm{GD}}$ bits, the contraction factor of DQ-GD is\nthe same as that of unquantized GD, i.e., there is no loss due to quantization.\nWe show a converse demonstrating that no quantized gradient descent algorithm\ncan converge faster than $\\max\\{\\sigma_{\\mathrm{GD}}, 2^{-R}\\}$. In contrast,\nnaively quantized GD where the worker directly quantizes the gradient barely\nattains $\\sigma_{\\mathrm{GD}} + 2^{-R}$. The principle of differential\nquantization continues to apply to gradient methods with momentum such as\nNesterov's accelerated gradient descent, and Polyak's heavy ball method. For\nthese algorithms as well, if the rate is above a certain threshold, there is no\nloss in contraction factor obtained by the differentially quantized algorithm\ncompared to its unquantized counterpart, and furthermore, the differentially\nquantized heavy ball method attains the optimal contraction achievable among\nall (even unquantized) gradient methods. Experimental results on both simulated\nand real-world least-squares problems validate our theoretical analysis.",
          "link": "http://arxiv.org/abs/2002.02508",
          "publishedOn": "2021-06-24T01:51:45.693Z",
          "wordCount": 725,
          "title": "Differentially Quantized Gradient Methods. (arXiv:2002.02508v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.08271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yige Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_N/0/1/0/all/0/1\">Ning Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>",
          "description": "Recently, the emergence of pre-trained models (PTMs) has brought natural\nlanguage processing (NLP) to a new era. In this survey, we provide a\ncomprehensive review of PTMs for NLP. We first briefly introduce language\nrepresentation learning and its research progress. Then we systematically\ncategorize existing PTMs based on a taxonomy with four perspectives. Next, we\ndescribe how to adapt the knowledge of PTMs to the downstream tasks. Finally,\nwe outline some potential directions of PTMs for future research. This survey\nis purposed to be a hands-on guide for understanding, using, and developing\nPTMs for various NLP tasks.",
          "link": "http://arxiv.org/abs/2003.08271",
          "publishedOn": "2021-06-24T01:51:45.687Z",
          "wordCount": 603,
          "title": "Pre-trained Models for Natural Language Processing: A Survey. (arXiv:2003.08271v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarma_A/0/1/0/all/0/1\">Anup Sarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sonali Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huaipan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandemir_M/0/1/0/all/0/1\">Mahmut T Kandemir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_C/0/1/0/all/0/1\">Chita R Das</a>",
          "description": "Recurrent Neural Networks (RNNs), more specifically their Long Short-Term\nMemory (LSTM) variants, have been widely used as a deep learning tool for\ntackling sequence-based learning tasks in text and speech. Training of such\nLSTM applications is computationally intensive due to the recurrent nature of\nhidden state computation that repeats for each time step. While sparsity in\nDeep Neural Nets has been widely seen as an opportunity for reducing\ncomputation time in both training and inference phases, the usage of non-ReLU\nactivation in LSTM RNNs renders the opportunities for such dynamic sparsity\nassociated with neuron activation and gradient values to be limited or\nnon-existent. In this work, we identify dropout induced sparsity for LSTMs as a\nsuitable mode of computation reduction. Dropout is a widely used regularization\nmechanism, which randomly drops computed neuron values during each iteration of\ntraining. We propose to structure dropout patterns, by dropping out the same\nset of physical neurons within a batch, resulting in column (row) level hidden\nstate sparsity, which are well amenable to computation reduction at run-time in\ngeneral-purpose SIMD hardware as well as systolic arrays. We conduct our\nexperiments for three representative NLP tasks: language modelling on the PTB\ndataset, OpenNMT based machine translation using the IWSLT De-En and En-Vi\ndatasets, and named entity recognition sequence labelling using the CoNLL-2003\nshared task. We demonstrate that our proposed approach can be used to translate\ndropout-based computation reduction into reduced training time, with\nimprovement ranging from 1.23x to 1.64x, without sacrificing the target metric.",
          "link": "http://arxiv.org/abs/2106.12089",
          "publishedOn": "2021-06-24T01:51:45.682Z",
          "wordCount": 697,
          "title": "Structured in Space, Randomized in Time: Leveraging Dropout in RNNs for Efficient Training. (arXiv:2106.12089v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1\">Divyansh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Shuvam Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cundy_C/0/1/0/all/0/1\">Chris Cundy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiaming Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>",
          "description": "In many sequential decision-making problems (e.g., robotics control, game\nplaying, sequential prediction), human or expert data is available containing\nuseful information about the task. However, imitation learning (IL) from a\nsmall amount of expert data can be challenging in high-dimensional environments\nwith complex dynamics. Behavioral cloning is a simple method that is widely\nused due to its simplicity of implementation and stable convergence but doesn't\nutilize any information involving the environment's dynamics. Many existing\nmethods that exploit dynamics information are difficult to train in practice\ndue to an adversarial optimization process over reward and policy approximators\nor biased, high variance gradient estimators. We introduce a method for\ndynamics-aware IL which avoids adversarial training by learning a single\nQ-function, implicitly representing both reward and policy. On standard\nbenchmarks, the implicitly learned rewards show a high positive correlation\nwith the ground-truth rewards, illustrating our method can also be used for\ninverse reinforcement learning (IRL). Our method, Inverse soft-Q learning\n(IQ-Learn) obtains state-of-the-art results in offline and online imitation\nlearning settings, surpassing existing methods both in the number of required\nenvironment interactions and scalability in high-dimensional spaces.",
          "link": "http://arxiv.org/abs/2106.12142",
          "publishedOn": "2021-06-24T01:51:45.677Z",
          "wordCount": 616,
          "title": "IQ-Learn: Inverse soft-Q Learning for Imitation. (arXiv:2106.12142v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zantedeschi_V/0/1/0/all/0/1\">Valentina Zantedeschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viallard_P/0/1/0/all/0/1\">Paul Viallard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morvant_E/0/1/0/all/0/1\">Emilie Morvant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emonet_R/0/1/0/all/0/1\">R&#xe9;mi Emonet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habrard_A/0/1/0/all/0/1\">Amaury Habrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Germain_P/0/1/0/all/0/1\">Pascal Germain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guedj_B/0/1/0/all/0/1\">Benjamin Guedj</a>",
          "description": "We investigate a stochastic counterpart of majority votes over finite\nensembles of classifiers, and study its generalization properties. While our\napproach holds for arbitrary distributions, we instantiate it with Dirichlet\ndistributions: this allows for a closed-form and differentiable expression for\nthe expected risk, which then turns the generalization bound into a tractable\ntraining objective. The resulting stochastic majority vote learning algorithm\nachieves state-of-the-art accuracy and benefits from (non-vacuous) tight\ngeneralization bounds, in a series of numerical experiments when compared to\ncompeting algorithms which also minimize PAC-Bayes objectives -- both with\nuninformed (data-independent) and informed (data-dependent) priors.",
          "link": "http://arxiv.org/abs/2106.12535",
          "publishedOn": "2021-06-24T01:51:45.672Z",
          "wordCount": 544,
          "title": "Learning Stochastic Majority Votes by Minimizing a PAC-Bayes Generalization Bound. (arXiv:2106.12535v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iana_A/0/1/0/all/0/1\">Andreea Iana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>",
          "description": "In today's academic publishing model, especially in Computer Science,\nconferences commonly constitute the main platforms for releasing the latest\npeer-reviewed advancements in their respective fields. However, choosing a\nsuitable academic venue for publishing one's research can represent a\nchallenging task considering the plethora of available conferences,\nparticularly for those at the start of their academic careers, or for those\nseeking to publish outside of their usual domain. In this paper, we propose\nGraphConfRec, a conference recommender system which combines SciGraph and graph\nneural networks, to infer suggestions based not only on title and abstract, but\nalso on co-authorship and citation relationships. GraphConfRec achieves a\nrecall@10 of up to 0.580 and a MAP of up to 0.336 with a graph attention\nnetwork-based recommendation model. A user study with 25 subjects supports the\npositive results.",
          "link": "http://arxiv.org/abs/2106.12340",
          "publishedOn": "2021-06-24T01:51:45.656Z",
          "wordCount": 577,
          "title": "GraphConfRec: A Graph Neural Network-Based Conference Recommender System. (arXiv:2106.12340v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zrnic_T/0/1/0/all/0/1\">Tijana Zrnic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazumdar_E/0/1/0/all/0/1\">Eric Mazumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_S/0/1/0/all/0/1\">S. Shankar Sastry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "As predictive models are deployed into the real world, they must increasingly\ncontend with strategic behavior. A growing body of work on strategic\nclassification treats this problem as a Stackelberg game: the decision-maker\n\"leads\" in the game by deploying a model, and the strategic agents \"follow\" by\nplaying their best response to the deployed model. Importantly, in this\nframing, the burden of learning is placed solely on the decision-maker, while\nthe agents' best responses are implicitly treated as instantaneous. In this\nwork, we argue that the order of play in strategic classification is\nfundamentally determined by the relative frequencies at which the\ndecision-maker and the agents adapt to each other's actions. In particular, by\ngeneralizing the standard model to allow both players to learn over time, we\nshow that a decision-maker that makes updates faster than the agents can\nreverse the order of play, meaning that the agents lead and the decision-maker\nfollows. We observe in standard learning settings that such a role reversal can\nbe desirable for both the decision-maker and the strategic agents. Finally, we\nshow that a decision-maker with the freedom to choose their update frequency\ncan induce learning dynamics that converge to Stackelberg equilibria with\neither order of play.",
          "link": "http://arxiv.org/abs/2106.12529",
          "publishedOn": "2021-06-24T01:51:45.642Z",
          "wordCount": 642,
          "title": "Who Leads and Who Follows in Strategic Classification?. (arXiv:2106.12529v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hanshu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "We investigate the adversarial robustness of CNNs from the perspective of\nchannel-wise activations. By comparing \\textit{non-robust} (normally trained)\nand \\textit{robustified} (adversarially trained) models, we observe that\nadversarial training (AT) robustifies CNNs by aligning the channel-wise\nactivations of adversarial data with those of their natural counterparts.\nHowever, the channels that are \\textit{negatively-relevant} (NR) to predictions\nare still over-activated when processing adversarial data. Besides, we also\nobserve that AT does not result in similar robustness for all classes. For the\nrobust classes, channels with larger activation magnitudes are usually more\n\\textit{positively-relevant} (PR) to predictions, but this alignment does not\nhold for the non-robust classes. Given these observations, we hypothesize that\nsuppressing NR channels and aligning PR ones with their relevances further\nenhances the robustness of CNNs under AT. To examine this hypothesis, we\nintroduce a novel mechanism, i.e., \\underline{C}hannel-wise\n\\underline{I}mportance-based \\underline{F}eature \\underline{S}election (CIFS).\nThe CIFS manipulates channels' activations of certain layers by generating\nnon-negative multipliers to these channels based on their relevances to\npredictions. Extensive experiments on benchmark datasets including CIFAR10 and\nSVHN clearly verify the hypothesis and CIFS's effectiveness of robustifying\nCNNs.",
          "link": "http://arxiv.org/abs/2102.05311",
          "publishedOn": "2021-06-24T01:51:45.620Z",
          "wordCount": 660,
          "title": "CIFS: Improving Adversarial Robustness of CNNs via Channel-wise Importance-based Feature Selection. (arXiv:2102.05311v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shanqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Junjie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenzhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Licheng Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>",
          "description": "It is challenging learning from demonstrated observation-only trajectories in\na non-time-aligned environment because most imitation learning methods aim to\nimitate experts by following the demonstration step-by-step. However, aligned\ndemonstrations are seldom obtainable in real-world scenarios. In this work, we\npropose a new imitation learning approach called Hierarchical Imitation\nLearning from Observation(HILONet), which adopts a hierarchical structure to\nchoose feasible sub-goals from demonstrated observations dynamically. Our\nmethod can solve all kinds of tasks by achieving these sub-goals, whether it\nhas a single goal position or not. We also present three different ways to\nincrease sample efficiency in the hierarchical structure. We conduct extensive\nexperiments using several environments. The results show the improvement in\nboth performance and learning efficiency.",
          "link": "http://arxiv.org/abs/2011.02671",
          "publishedOn": "2021-06-24T01:51:45.607Z",
          "wordCount": 602,
          "title": "HILONet: Hierarchical Imitation Learning from Non-Aligned Observations. (arXiv:2011.02671v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1\">Apratim Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reino_D/0/1/0/all/0/1\">Daniel Olmeda Reino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>",
          "description": "Accurate prediction of pedestrian and bicyclist paths is integral to the\ndevelopment of reliable autonomous vehicles in dense urban environments. The\ninteractions between vehicle and pedestrian or bicyclist have a significant\nimpact on the trajectories of traffic participants e.g. stopping or turning to\navoid collisions. Although recent datasets and trajectory prediction approaches\nhave fostered the development of autonomous vehicles yet the amount of\nvehicle-pedestrian (bicyclist) interactions modeled are sparse. In this work,\nwe propose Euro-PVI, a dataset of pedestrian and bicyclist trajectories. In\nparticular, our dataset caters more diverse and complex interactions in dense\nurban scenarios compared to the existing datasets. To address the challenges in\npredicting future trajectories with dense interactions, we develop a joint\ninference model that learns an expressive multi-modal shared latent space\nacross agents in the urban scene. This enables our Joint-$\\beta$-cVAE approach\nto better model the distribution of future trajectories. We achieve state of\nthe art results on the nuScenes and Euro-PVI datasets demonstrating the\nimportance of capturing interactions between ego-vehicle and pedestrians\n(bicyclists) for accurate predictions.",
          "link": "http://arxiv.org/abs/2106.12442",
          "publishedOn": "2021-06-24T01:51:45.580Z",
          "wordCount": 622,
          "title": "Euro-PVI: Pedestrian Vehicle Interactions in Dense Urban Centers. (arXiv:2106.12442v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jie Shen</a>",
          "description": "We study efficient PAC learning of homogeneous halfspaces in $\\mathbb{R}^d$\nin the presence of malicious noise of Valiant~(1985). This is a challenging\nnoise model and only until recently has near-optimal noise tolerance bound been\nestablished under the mild condition that the unlabeled data distribution is\nisotropic log-concave. However, it remains unsettled how to obtain the optimal\nsample complexity simultaneously. In this work, we present a new analysis for\nthe algorithm of Awasthi~et~al.~(2017) and show that it essentially achieves\nthe near-optimal sample complexity bound of $\\tilde{O}(d)$, improving the best\nknown result of $\\tilde{O}(d^2)$. Our main ingredient is a novel incorporation\nof a matrix Chernoff-type inequality to bound the spectrum of an empirical\ncovariance matrix for well-behaved distributions, in conjunction with a careful\nexploration of the localization schemes of Awasthi~et~al.~(2017). We further\nextend the algorithm and analysis to the more general and stronger nasty noise\nmodel of Bshouty~et~al.~(2002), showing that it is still possible to achieve\nnear-optimal noise tolerance and sample complexity in polynomial time.",
          "link": "http://arxiv.org/abs/2102.06247",
          "publishedOn": "2021-06-24T01:51:45.014Z",
          "wordCount": 632,
          "title": "Sample-Optimal PAC Learning of Halfspaces with Malicious Noise. (arXiv:2102.06247v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhouxing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jinfeng Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>",
          "description": "Recently, bound propagation based certified robust training methods have been\nproposed for training neural networks with certifiable robustness guarantees.\nDespite that state-of-the-art (SOTA) methods including interval bound\npropagation (IBP) and CROWN-IBP have per-batch training complexity similar to\nstandard neural network training, they usually use a long warmup schedule with\nhundreds or thousands epochs to reach SOTA performance and are thus still\ncostly. In this paper, we identify two important issues in existing methods,\nnamely exploded bounds at initialization, and the imbalance in ReLU activation\nstates. These two issues make certified training difficult and unstable, and\nthereby long warmup schedules were needed in prior works. To mitigate these\nissues and conduct certified training with shorter warmup, we propose three\nimprovements: 1) We derive a new weight initialization method for IBP training;\n2) We propose to fully add Batch Normalization (BN) to each layer in the model,\nsince we find BN can reduce the imbalance in ReLU activation states; 3) We also\ndesign regularization to explicitly tighten certified bounds and balance ReLU\nactivation states. In our experiments, we are able to obtain 65.03% verified\nerror on CIFAR-10 ($\\epsilon=\\frac{8}{255}$) and 82.36% verified error on\nTinyImageNet ($\\epsilon=\\frac{1}{255}$) using very short training schedules\n(160 and 80 total epochs, respectively), outperforming literature SOTA trained\nwith hundreds or thousands epochs under the same network architecture.",
          "link": "http://arxiv.org/abs/2103.17268",
          "publishedOn": "2021-06-24T01:51:45.008Z",
          "wordCount": 695,
          "title": "Fast Certified Robust Training with Short Warmup. (arXiv:2103.17268v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengfei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ruoxi Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1\">Kai Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuhao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1\">Guoen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bin Yan</a>",
          "description": "Deep neural networks(DNNs) is vulnerable to be attacked by adversarial\nexamples. Black-box attack is the most threatening attack. At present,\nblack-box attack methods mainly adopt gradient-based iterative attack methods,\nwhich usually limit the relationship between the iteration step size, the\nnumber of iterations, and the maximum perturbation. In this paper, we propose a\nnew gradient iteration framework, which redefines the relationship between the\nabove three. Under this framework, we easily improve the attack success rate of\nDI-TI-MIM. In addition, we propose a gradient iterative attack method based on\ninput dropout, which can be well combined with our framework. We further\npropose a multi dropout rate version of this method. Experimental results show\nthat our best method can achieve attack success rate of 96.2\\% for defense\nmodel on average, which is higher than the state-of-the-art gradient-based\nattacks.",
          "link": "http://arxiv.org/abs/2106.01617",
          "publishedOn": "2021-06-24T01:51:44.984Z",
          "wordCount": 606,
          "title": "Improving the Transferability of Adversarial Examples with New Iteration Framework and Input Dropout. (arXiv:2106.01617v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sivasubramanian_D/0/1/0/all/0/1\">Durga Sivasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1\">Abir De</a>",
          "description": "Data subset selection from a large number of training instances has been a\nsuccessful approach toward efficient and cost-effective machine learning.\nHowever, models trained on a smaller subset may show poor generalization\nability. In this paper, our goal is to design an algorithm for selecting a\nsubset of the training data, so that the model can be trained quickly, without\nsignificantly sacrificing on accuracy. More specifically, we focus on data\nsubset selection for L2 regularized regression problems and provide a novel\nproblem formulation which seeks to minimize the training loss with respect to\nboth the trainable parameters and the subset of training data, subject to error\nbounds on the validation set. We tackle this problem using several technical\ninnovations. First, we represent this problem with simplified constraints using\nthe dual of the original training problem and show that the objective of this\nnew representation is a monotone and alpha-submodular function, for a wide\nvariety of modeling choices. Such properties lead us to develop SELCON, an\nefficient majorization-minimization algorithm for data subset selection, that\nadmits an approximation guarantee even when the training provides an imperfect\nestimate of the trained model. Finally, our experiments on several datasets\nshow that SELCON trades off accuracy and efficiency more effectively than the\ncurrent state-of-the-art.",
          "link": "http://arxiv.org/abs/2106.12491",
          "publishedOn": "2021-06-24T01:51:44.978Z",
          "wordCount": 651,
          "title": "Training Data Subset Selection for Regression with Controlled Generalization Error. (arXiv:2106.12491v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14543",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Haug_T/0/1/0/all/0/1\">Tobias Haug</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kim_M/0/1/0/all/0/1\">M.S. Kim</a>",
          "description": "Variational quantum algorithms (VQAs) promise efficient use of near-term\nquantum computers. However, training VQAs often requires an extensive amount of\ntime and suffers from the barren plateau problem where the magnitude of the\ngradients vanishes with increasing number of qubits. Here, we show how to\noptimally train VQAs for learning quantum states. Parameterized quantum\ncircuits can form Gaussian kernels, which we use to derive adaptive learning\nrates for gradient ascent. We introduce the generalized quantum natural\ngradient that features stability and optimized movement in parameter space.\nBoth methods together outperform other optimization routines in training VQAs.\nOur methods also excel at numerically optimizing driving protocols for quantum\ncontrol problems. The gradients of the VQA do not vanish when the fidelity\nbetween the initial state and the state to be learned is bounded from below. We\nidentify a VQA for quantum simulation with such a constraint that thus can be\ntrained free of barren plateaus. Finally, we propose the application of\nGaussian kernels for quantum machine learning.",
          "link": "http://arxiv.org/abs/2104.14543",
          "publishedOn": "2021-06-24T01:51:44.972Z",
          "wordCount": 633,
          "title": "Optimal training of variational quantum algorithms without barren plateaus. (arXiv:2104.14543v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bodnar_C/0/1/0/all/0/1\">Cristian Bodnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frasca_F/0/1/0/all/0/1\">Fabrizio Frasca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otter_N/0/1/0/all/0/1\">Nina Otter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Guang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf2;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montufar_G/0/1/0/all/0/1\">Guido Mont&#xfa;far</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1\">Michael Bronstein</a>",
          "description": "Graph Neural Networks (GNNs) are limited in their expressive power, struggle\nwith long-range interactions and lack a principled way to model higher-order\nstructures. These problems can be attributed to the strong coupling between the\ncomputational graph and the input graph structure. The recently proposed\nMessage Passing Simplicial Networks naturally decouple these elements by\nperforming message passing on the clique complex of the graph. Nevertheless,\nthese models are severely constrained by the rigid combinatorial structure of\nSimplicial Complexes (SCs). In this work, we extend recent theoretical results\non SCs to regular Cell Complexes, topological objects that flexibly subsume SCs\nand graphs. We show that this generalisation provides a powerful set of graph\n``lifting'' transformations, each leading to a unique hierarchical message\npassing procedure. The resulting methods, which we collectively call CW\nNetworks (CWNs), are strictly more powerful than the WL test and, in certain\ncases, not less powerful than the 3-WL test. In particular, we demonstrate the\neffectiveness of one such scheme, based on rings, when applied to molecular\ngraph problems. The proposed architecture benefits from provably larger\nexpressivity than commonly used GNNs, principled modelling of higher-order\nsignals and from compressing the distances between nodes. We demonstrate that\nour model achieves state-of-the-art results on a variety of molecular datasets.",
          "link": "http://arxiv.org/abs/2106.12575",
          "publishedOn": "2021-06-24T01:51:44.967Z",
          "wordCount": 648,
          "title": "Weisfeiler and Lehman Go Cellular: CW Networks. (arXiv:2106.12575v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shengjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shanda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Tianle Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Dinglan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuxin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1\">Guolin Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "The attention module, which is a crucial component in Transformer, cannot\nscale efficiently to long sequences due to its quadratic complexity. Many works\nfocus on approximating the dot-then-exponentiate softmax function in the\noriginal attention, leading to sub-quadratic or even linear-complexity\nTransformer architectures. However, we show that these methods cannot be\napplied to more powerful attention modules that go beyond the\ndot-then-exponentiate style, e.g., Transformers with relative positional\nencoding (RPE). Since in many state-of-the-art models, relative positional\nencoding is used as default, designing efficient Transformers that can\nincorporate RPE is appealing. In this paper, we propose a novel way to\naccelerate attention calculation for Transformers with RPE on top of the\nkernelized attention. Based upon the observation that relative positional\nencoding forms a Toeplitz matrix, we mathematically show that kernelized\nattention with RPE can be calculated efficiently using Fast Fourier Transform\n(FFT). With FFT, our method achieves $\\mathcal{O}(n\\log n)$ time complexity.\nInterestingly, we further demonstrate that properly using relative positional\nencoding can mitigate the training instability problem of vanilla kernelized\nattention. On a wide range of tasks, we empirically show that our models can be\ntrained from scratch without any optimization issues. The learned model\nperforms better than many efficient Transformer variants and is faster than\nstandard Transformer in the long-sequence regime.",
          "link": "http://arxiv.org/abs/2106.12566",
          "publishedOn": "2021-06-24T01:51:44.952Z",
          "wordCount": 670,
          "title": "Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding. (arXiv:2106.12566v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14958",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Davila_Pena_L/0/1/0/all/0/1\">L. Davila-Pena</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Garcia_Jurado_I/0/1/0/all/0/1\">Ignacio Garc&#xed;a-Jurado</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Casas_Mendez_B/0/1/0/all/0/1\">B. Casas-M&#xe9;ndez</a>",
          "description": "This paper deals with an important subject in classification problems\naddressed by machine learning techniques: the evaluation of the influence of\neach of the features on the classification of individuals. Specifically, a\nmeasure of that influence is introduced using the Shapley value of cooperative\ngames. In addition, an axiomatic characterisation of the proposed measure is\nprovided based on properties of efficiency and balanced contributions.\nFurthermore, some experiments have been designed in order to validate the\nappropriate performance of such measure. Finally, the methodology introduced is\napplied to a sample of COVID-19 patients to study the influence of certain\ndemographic or risk factors on various events of interest related to the\nevolution of the disease.",
          "link": "http://arxiv.org/abs/2104.14958",
          "publishedOn": "2021-06-24T01:51:44.947Z",
          "wordCount": 629,
          "title": "Assessment of the influence of features on a classification problem: an application to COVID-19 patients. (arXiv:2104.14958v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yaghoubi_V/0/1/0/all/0/1\">Vahid Yaghoubi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paepegem_W/0/1/0/all/0/1\">Wim Van Paepegem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersemans_M/0/1/0/all/0/1\">Mathias Kersemans</a>",
          "description": "Achieving a high prediction rate is a crucial task in fault detection.\nAlthough various classification procedures are available, none of them can give\nhigh accuracy in all applications. Therefore, in this paper, a novel\nmulti-classifier fusion approach is developed to boost the performance of the\nindividual classifiers. This is acquired by using Dempster-Shafer theory (DST).\nHowever, in cases with conflicting evidences, the DST may give\ncounter-intuitive results. In this regard, a preprocessing technique based on a\nnew metric is devised in order to measure and mitigate the conflict between the\nevidences. To evaluate and validate the effectiveness of the proposed approach,\nthe method is applied to 15 benchmarks datasets from UCI and KEEL. Further, it\nis applied for classifying polycrystalline Nickel alloy first-stage turbine\nblades based on their broadband vibrational response. Through statistical\nanalysis with different noise levels, and by comparing with four\nstate-of-the-art fusion techniques, it is shown that that the proposed method\nimproves the classification accuracy and outperforms the individual\nclassifiers.",
          "link": "http://arxiv.org/abs/2012.02481",
          "publishedOn": "2021-06-24T01:51:44.942Z",
          "wordCount": 639,
          "title": "A novel multi-classifier information fusion based on Dempster-Shafer theory: application to vibration-based fault detection. (arXiv:2012.02481v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">R. Kenny Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charatan_D/0/1/0/all/0/1\">David Charatan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1\">Paul Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>",
          "description": "A popular way to create detailed yet easily controllable 3D shapes is via\nprocedural modeling, i.e. generating geometry using programs. Such programs\nconsist of a series of instructions along with their associated parameter\nvalues. To fully realize the benefits of this representation, a shape program\nshould be compact and only expose degrees of freedom that allow for meaningful\nmanipulation of output geometry. One way to achieve this goal is to design\nhigher-level macro operators that, when executed, expand into a series of\ncommands from the base shape modeling language. However, manually authoring\nsuch macros, much like shape programs themselves, is difficult and largely\nrestricted to domain experts. In this paper, we present ShapeMOD, an algorithm\nfor automatically discovering macros that are useful across large datasets of\n3D shape programs. ShapeMOD operates on shape programs expressed in an\nimperative, statement-based language. It is designed to discover macros that\nmake programs more compact by minimizing the number of function calls and free\nparameters required to represent an input shape collection. We run ShapeMOD on\nmultiple collections of programs expressed in a domain-specific language for 3D\nshape structures. We show that it automatically discovers a concise set of\nmacros that abstract out common structural and parametric patterns that\ngeneralize over large shape collections. We also demonstrate that the macros\nfound by ShapeMOD improve performance on downstream tasks including shape\ngenerative modeling and inferring programs from point clouds. Finally, we\nconduct a user study that indicates that ShapeMOD's discovered macros make\ninteractive shape editing more efficient.",
          "link": "http://arxiv.org/abs/2104.06392",
          "publishedOn": "2021-06-24T01:51:44.936Z",
          "wordCount": 739,
          "title": "ShapeMOD: Macro Operation Discovery for 3D Shape Programs. (arXiv:2104.06392v2 [cs.GR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1\">MohammadJavad Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1\">Branislav Kveton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1\">Mohammad Ghavamzadeh</a>",
          "description": "We study the problem of best-arm identification (BAI) in contextual bandits\nin the fixed-budget setting. We propose a general successive elimination\nalgorithm that proceeds in stages and eliminates a fixed fraction of suboptimal\narms in each stage. This design takes advantage of the strengths of static and\nadaptive allocations. We analyze the algorithm in linear models and obtain a\nbetter error bound than prior work. We also apply it to generalized linear\nmodels (GLMs) and bound its error. This is the first BAI algorithm for GLMs in\nthe fixed-budget setting. Our extensive numerical experiments show that our\nalgorithm outperforms the state of art.",
          "link": "http://arxiv.org/abs/2106.04763",
          "publishedOn": "2021-06-24T01:51:44.930Z",
          "wordCount": 559,
          "title": "Fixed-Budget Best-Arm Identification in Contextual Bandits: A Static-Adaptive Algorithm. (arXiv:2106.04763v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.10793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jie Shen</a>",
          "description": "We study {\\em online} active learning of homogeneous halfspaces in\n$\\mathbb{R}^d$ with adversarial noise where the overall probability of a noisy\nlabel is constrained to be at most $\\nu$. Our main contribution is a\nPerceptron-like online active learning algorithm that runs in polynomial time,\nand under the conditions that the marginal distribution is isotropic\nlog-concave and $\\nu = \\Omega(\\epsilon)$, where $\\epsilon \\in (0, 1)$ is the\ntarget error rate, our algorithm PAC learns the underlying halfspace with\nnear-optimal label complexity of $\\tilde{O}\\big(d \\cdot\npolylog(\\frac{1}{\\epsilon})\\big)$ and sample complexity of\n$\\tilde{O}\\big(\\frac{d}{\\epsilon} \\big)$. Prior to this work, existing online\nalgorithms designed for tolerating the adversarial noise are subject to either\nlabel complexity polynomial in $\\frac{1}{\\epsilon}$, or suboptimal noise\ntolerance, or restrictive marginal distributions. With the additional prior\nknowledge that the underlying halfspace is $s$-sparse, we obtain\nattribute-efficient label complexity of $\\tilde{O}\\big( s \\cdot polylog(d,\n\\frac{1}{\\epsilon}) \\big)$ and sample complexity of\n$\\tilde{O}\\big(\\frac{s}{\\epsilon} \\cdot polylog(d) \\big)$. As an immediate\ncorollary, we show that under the agnostic model where no assumption is made on\nthe noise rate $\\nu$, our active learner achieves an error rate of $O(OPT) +\n\\epsilon$ with the same running time and label and sample complexity, where\n$OPT$ is the best possible error rate achievable by any homogeneous halfspace.",
          "link": "http://arxiv.org/abs/2012.10793",
          "publishedOn": "2021-06-24T01:51:44.890Z",
          "wordCount": 698,
          "title": "On the Power of Localized Perceptron for Label-Optimal Learning of Halfspaces with Adversarial Noise. (arXiv:2012.10793v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12041",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Hough_A/0/1/0/all/0/1\">Alana Hough</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wong_T/0/1/0/all/0/1\">Tony E. Wong</a>",
          "description": "Climate models are critical tools for developing strategies to manage the\nrisks posed by sea-level rise to coastal communities. While these models are\nnecessary for understanding climate risks, there is a level of uncertainty\ninherent in each parameter in the models. This model parametric uncertainty\nleads to uncertainty in future climate risks. Consequently, there is a need to\nunderstand how those parameter uncertainties impact our assessment of future\nclimate risks and the efficacy of strategies to manage them. Here, we use\nrandom forests to examine the parametric drivers of future climate risk and how\nthe relative importances of those drivers change over time. We find that the\nequilibrium climate sensitivity and a factor that scales the effect of aerosols\non radiative forcing are consistently the most important climate model\nparametric uncertainties throughout the 2020 to 2150 interval for both low and\nhigh radiative forcing scenarios. The near-term hazards of high-end sea-level\nrise are driven primarily by thermal expansion, while the longer-term hazards\nare associated with mass loss from the Antarctic and Greenland ice sheets. Our\nresults highlight the practical importance of considering time-evolving\nparametric uncertainties when developing strategies to manage future climate\nrisks.",
          "link": "http://arxiv.org/abs/2106.12041",
          "publishedOn": "2021-06-24T01:51:44.883Z",
          "wordCount": 636,
          "title": "Analysis of the Evolution of Parametric Drivers of High-End Sea-Level Hazards. (arXiv:2106.12041v1 [physics.ao-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yunjey Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Sungjoo Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1\">Youngjung Uh</a>",
          "description": "Generative adversarial networks (GANs) synthesize realistic images from\nrandom latent vectors. Although manipulating the latent vectors controls the\nsynthesized outputs, editing real images with GANs suffers from i)\ntime-consuming optimization for projecting real images to the latent vectors,\nii) or inaccurate embedding through an encoder. We propose StyleMapGAN: the\nintermediate latent space has spatial dimensions, and a spatially variant\nmodulation replaces AdaIN. It makes the embedding through an encoder more\naccurate than existing optimization-based methods while maintaining the\nproperties of GANs. Experimental results demonstrate that our method\nsignificantly outperforms state-of-the-art models in various image manipulation\ntasks such as local editing and image interpolation. Last but not least,\nconventional editing methods on GANs are still valid on our StyleMapGAN. Source\ncode is available at https://github.com/naver-ai/StyleMapGAN.",
          "link": "http://arxiv.org/abs/2104.14754",
          "publishedOn": "2021-06-24T01:51:44.878Z",
          "wordCount": 608,
          "title": "Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing. (arXiv:2104.14754v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07405",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1\">Wu Lin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1\">Frank Nielsen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Emtiyaz Khan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1\">Mark Schmidt</a>",
          "description": "Natural-gradient descent on structured parameter spaces (e.g., low-rank\ncovariances) is computationally challenging due to complicated inverse\nFisher-matrix computations. We address this issue for optimization, inference,\nand search problems by using \\emph{local-parameter coordinates}. Our method\ngeneralizes an existing evolutionary-strategy method, recovers Newton and\nRiemannian-gradient methods as special cases, and also yields new tractable\nnatural-gradient algorithms for learning flexible covariance structures of\nGaussian and Wishart-based distributions via \\emph{matrix groups}. We show\nresults on a range of applications on deep learning, variational inference, and\nevolution strategies. Our work opens a new direction for scalable structured\ngeometric methods via local parameterizations.",
          "link": "http://arxiv.org/abs/2102.07405",
          "publishedOn": "2021-06-24T01:51:44.872Z",
          "wordCount": 585,
          "title": "Tractable structured natural gradient descent using local parameterizations. (arXiv:2102.07405v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.12380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomariz_A/0/1/0/all/0/1\">Alvaro Gomariz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portenier_T/0/1/0/all/0/1\">Tiziano Portenier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helbling_P/0/1/0/all/0/1\">Patrick M. Helbling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isringhausen_S/0/1/0/all/0/1\">Stephan Isringhausen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suessbier_U/0/1/0/all/0/1\">Ute Suessbier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nombela_Arrieta_C/0/1/0/all/0/1\">C&#xe9;sar Nombela-Arrieta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goksel_O/0/1/0/all/0/1\">Orcun Goksel</a>",
          "description": "Fluorescence microscopy allows for a detailed inspection of cells, cellular\nnetworks, and anatomical landmarks by staining with a variety of\ncarefully-selected markers visualized as color channels. Quantitative\ncharacterization of structures in acquired images often relies on automatic\nimage analysis methods. Despite the success of deep learning methods in other\nvision applications, their potential for fluorescence image analysis remains\nunderexploited. One reason lies in the considerable workload required to train\naccurate models, which are normally specific for a given combination of\nmarkers, and therefore applicable to a very restricted number of experimental\nsettings. We herein propose Marker Sampling and Excite, a neural network\napproach with a modality sampling strategy and a novel attention module that\ntogether enable (i) flexible training with heterogeneous datasets with\ncombinations of markers and (ii) successful utility of learned models on\narbitrary subsets of markers prospectively. We show that our single neural\nnetwork solution performs comparably to an upper bound scenario where an\nensemble of many networks is na\\\"ively trained for each possible marker\ncombination separately. In addition, we demonstrate the feasibility of this\nframework in high-throughput biological analysis by revising a recent\nquantitative characterization of bone marrow vasculature in 3D confocal\nmicroscopy datasets and further confirm the validity of our approach on an\nadditional, significantly different dataset of microvessels in fetal liver\ntissues. Not only can our work substantially ameliorate the use of deep\nlearning in fluorescence microscopy analysis, but it can also be utilized in\nother fields with incomplete data acquisitions and missing modalities.",
          "link": "http://arxiv.org/abs/2008.12380",
          "publishedOn": "2021-06-24T01:51:44.867Z",
          "wordCount": 758,
          "title": "Modality Attention and Sampling Enables Deep Learning with Heterogeneous Marker Combinations in Fluorescence Microscopy. (arXiv:2008.12380v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shangqian Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "In this paper, we design a novel Bregman gradient policy optimization\nframework for reinforcement learning based on Bregman divergences and momentum\ntechniques. Specifically, we propose a Bregman gradient policy optimization\n(BGPO) algorithm based on the basic momentum technique and mirror descent\niteration. At the same time, we present an accelerated Bregman gradient policy\noptimization (VR-BGPO) algorithm based on a momentum variance-reduced\ntechnique. Moreover, we introduce a convergence analysis framework for our\nBregman gradient policy optimization under the nonconvex setting. Specifically,\nwe prove that BGPO achieves the sample complexity of $\\tilde{O}(\\epsilon^{-4})$\nfor finding $\\epsilon$-stationary point only requiring one trajectory at each\niteration, and VR-BGPO reaches the best known sample complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point, which\nalso only requires one trajectory at each iteration. In particular, by using\ndifferent Bregman divergences, our methods unify many existing policy\noptimization algorithms and their new variants such as the existing\n(variance-reduced) policy gradient algorithms and (variance-reduced) natural\npolicy gradient algorithms. Extensive experimental results on multiple\nreinforcement learning tasks demonstrate the efficiency of our new algorithms.",
          "link": "http://arxiv.org/abs/2106.12112",
          "publishedOn": "2021-06-24T01:51:44.852Z",
          "wordCount": 606,
          "title": "Bregman Gradient Policy Optimization. (arXiv:2106.12112v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.08740",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuhang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joe_Wong_C/0/1/0/all/0/1\">Carlee Joe-Wong</a>",
          "description": "We study the problem of clustering nodes in a dynamic graph, where the\nconnections between nodes and nodes' cluster memberships may change over time,\ne.g., due to community migration. We first propose a dynamic stochastic block\nmodel that captures these changes, and a simple decay-based clustering\nalgorithm that clusters nodes based on weighted connections between them, where\nthe weight decreases at a fixed rate over time. This decay rate can then be\ninterpreted as signifying the importance of including historical connection\ninformation in the clustering. However, the optimal decay rate may differ for\nclusters with different rates of turnover. We characterize the optimal decay\nrate for each cluster and propose a clustering method that achieves almost\nexact recovery of the true clusters. We then demonstrate the efficacy of our\nclustering algorithm with optimized decay rates on simulated graph data.\nRecurrent neural networks (RNNs), a popular algorithm for sequence learning,\nuse a similar decay-based method, and we use this insight to propose two new\nRNN-GCN (graph convolutional network) architectures for semi-supervised graph\nclustering. We finally demonstrate that the proposed architectures perform well\non real data compared to state-of-the-art graph clustering algorithms.",
          "link": "http://arxiv.org/abs/2012.08740",
          "publishedOn": "2021-06-24T01:51:44.835Z",
          "wordCount": 661,
          "title": "Interpretable Clustering on Dynamic Graphs with Recurrent Graph Neural Networks. (arXiv:2012.08740v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07900",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Yang_C/0/1/0/all/0/1\">Chaoqi Yang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Qian_C/0/1/0/all/0/1\">Cheng Qian</a>, <a href=\"http://arxiv.org/find/math/1/au:+Singh_N/0/1/0/all/0/1\">Navjot Singh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xiao_C/0/1/0/all/0/1\">Cao Xiao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Westover_M/0/1/0/all/0/1\">M Brandon Westover</a>, <a href=\"http://arxiv.org/find/math/1/au:+Solomonik_E/0/1/0/all/0/1\">Edgar Solomonik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>",
          "description": "Tensor decompositions are powerful tools for dimensionality reduction and\nfeature interpretation of multidimensional data such as signals. Existing\ntensor decomposition objectives (e.g., Frobenius norm) are designed for fitting\nraw data under statistical assumptions, which may not align with downstream\nclassification tasks. Also, real-world tensor data are usually high-ordered and\nhave large dimensions with millions or billions of entries. Thus, it is\nexpensive to decompose the whole tensor with traditional algorithms. In\npractice, raw tensor data also contains redundant information while data\naugmentation techniques may be used to smooth out noise in samples. This paper\naddresses the above challenges by proposing augmented tensor decomposition\n(ATD), which effectively incorporates data augmentations to boost downstream\nclassification. To reduce the memory footprint of the decomposition, we propose\na stochastic algorithm that updates the factor matrices in a batch fashion. We\nevaluate ATD on multiple signal datasets. It shows comparable or better\nperformance (e.g., up to 15% in accuracy) over self-supervised and autoencoder\nbaselines with less than 5% of model parameters, achieves 0.6% ~ 1.3% accuracy\ngain over other tensor-based baselines, and reduces the memory footprint by 9X\nwhen compared to standard tensor decomposition algorithms.",
          "link": "http://arxiv.org/abs/2106.07900",
          "publishedOn": "2021-06-24T01:51:44.828Z",
          "wordCount": 658,
          "title": "Augmented Tensor Decomposition with Stochastic Optimization. (arXiv:2106.07900v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.04806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alexander Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mengye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard S. Zemel</a>",
          "description": "Sketch drawings capture the salient information of visual concepts. Previous\nwork has shown that neural networks are capable of producing sketches of\nnatural objects drawn from a small number of classes. While earlier approaches\nfocus on generation quality or retrieval, we explore properties of image\nrepresentations learned by training a model to produce sketches of images. We\nshow that this generative, class-agnostic model produces informative embeddings\nof images from novel examples, classes, and even novel datasets in a few-shot\nsetting. Additionally, we find that these learned representations exhibit\ninteresting structure and compositionality.",
          "link": "http://arxiv.org/abs/2009.04806",
          "publishedOn": "2021-06-24T01:51:44.821Z",
          "wordCount": 589,
          "title": "SketchEmbedNet: Learning Novel Concepts by Imitating Drawings. (arXiv:2009.04806v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.06015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shih_S/0/1/0/all/0/1\">Sheng-Min Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tien_P/0/1/0/all/0/1\">Pin-Ju Tien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karnin_Z/0/1/0/all/0/1\">Zohar Karnin</a>",
          "description": "Attribution methods have been shown as promising approaches for identifying\nkey features that led to learned model predictions. While most existing\nattribution methods rely on a baseline input for performing feature\nperturbations, limited research has been conducted to address the baseline\nselection issues. Poor choices of baselines limit the ability of one-vs-one\n(1-vs-1) explanations for multi-class classifiers, which means the attribution\nmethods were not able to explain why an input belongs to its original class but\nnot the other specified target class. 1-vs-1 explanation is crucial when\ncertain classes are more similar than others, e.g. two bird types among\nmultiple animals, by focusing on key differentiating features rather than\nshared features across classes. In this paper, we present GAN-based Model\nEXplainability (GANMEX), a novel approach applying Generative Adversarial\nNetworks (GAN) by incorporating the to-be-explained classifier as part of the\nadversarial networks. Our approach effectively selects the counterfactual\nbaseline as the closest realistic sample belong to the target class, which\nallows attribution methods to provide true 1-vs-1 explanations. We showed that\nGANMEX baselines improved the saliency maps and led to stronger performance on\nperturbation-based evaluation metrics over the existing baselines. Existing\nattribution results are known for being insensitive to model randomization, and\nwe demonstrated that GANMEX baselines led to better outcome under the cascading\nrandomization of the model.",
          "link": "http://arxiv.org/abs/2011.06015",
          "publishedOn": "2021-06-24T01:51:44.816Z",
          "wordCount": 698,
          "title": "GANMEX: One-vs-One Attributions Guided by GAN-based Counterfactual Explanation Baselines. (arXiv:2011.06015v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1\">Shreyas Malakarjun Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dovrolis_C/0/1/0/all/0/1\">Constantine Dovrolis</a>",
          "description": "Methods that sparsify a network at initialization are important in practice\nbecause they greatly improve the efficiency of both learning and inference. Our\nwork is based on a recently proposed decomposition of the Neural Tangent Kernel\n(NTK) that has decoupled the dynamics of the training process into a\ndata-dependent component and an architecture-dependent kernel - the latter\nreferred to as Path Kernel. That work has shown how to design sparse neural\nnetworks for faster convergence, without any training data, using the\nSynflow-L2 algorithm. We first show that even though Synflow-L2 is optimal in\nterms of convergence, for a given network density, it results in sub-networks\nwith \"bottleneck\" (narrow) layers - leading to poor performance as compared to\nother data-agnostic methods that use the same number of parameters. Then we\npropose a new method to construct sparse networks, without any training data,\nreferred to as Paths with Higher-Edge Weights (PHEW). PHEW is a probabilistic\nnetwork formation method based on biased random walks that only depends on the\ninitial weights. It has similar path kernel properties as Synflow-L2 but it\ngenerates much wider layers, resulting in better generalization and\nperformance. PHEW achieves significant improvements over the data-independent\nSynFlow and SynFlow-L2 methods at a wide range of network densities.",
          "link": "http://arxiv.org/abs/2010.11354",
          "publishedOn": "2021-06-24T01:51:44.795Z",
          "wordCount": 685,
          "title": "PHEW: Constructing Sparse Networks that Learn Fast and Generalize Well without Training Data. (arXiv:2010.11354v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.14567",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chavdarova_T/0/1/0/all/0/1\">Tatjana Chavdarova</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pagliardini_M/0/1/0/all/0/1\">Matteo Pagliardini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Stich_S/0/1/0/all/0/1\">Sebastian U. Stich</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fleuret_F/0/1/0/all/0/1\">Francois Fleuret</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>",
          "description": "Generative Adversarial Networks are notoriously challenging to train. The\nunderlying minmax optimization is highly susceptible to the variance of the\nstochastic gradient and the rotational component of the associated game vector\nfield. To tackle these challenges, we propose the Lookahead algorithm for\nminmax optimization, originally developed for single objective minimization\nonly. The backtracking step of our Lookahead-minmax naturally handles the\nrotational game dynamics, a property which was identified to be key for\nenabling gradient ascent descent methods to converge on challenging examples\noften analyzed in the literature. Moreover, it implicitly handles high variance\nwithout using large mini-batches, known to be essential for reaching state of\nthe art performance. Experimental results on MNIST, SVHN, CIFAR-10, and\nImageNet demonstrate a clear advantage of combining Lookahead-minmax with Adam\nor extragradient, in terms of performance and improved stability, for\nnegligible memory and computational cost. Using 30-fold fewer parameters and\n16-fold smaller minibatches we outperform the reported performance of the\nclass-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the\nclass labels, bringing state-of-the-art GAN training within reach of common\ncomputational resources.",
          "link": "http://arxiv.org/abs/2006.14567",
          "publishedOn": "2021-06-24T01:51:44.784Z",
          "wordCount": 638,
          "title": "Taming GANs with Lookahead-Minmax. (arXiv:2006.14567v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09258",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Durkan_C/0/1/0/all/0/1\">Conor Durkan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Murray_I/0/1/0/all/0/1\">Iain Murray</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>",
          "description": "Score-based diffusion models synthesize samples by reversing a stochastic\nprocess that diffuses data to noise, and are trained by minimizing a weighted\ncombination of score matching losses. The log-likelihood of score-based models\ncan be tractably computed through a connection to continuous normalizing flows,\nbut log-likelihood is not directly optimized by the weighted combination of\nscore matching losses. We show that for a specific weighting scheme, the\nobjective upper bounds the negative log-likelihood, thus enabling approximate\nmaximum likelihood training of score-based models. We empirically observe that\nmaximum likelihood training consistently improves the likelihood of score-based\nmodels across multiple datasets, stochastic processes, and model architectures.\nOur best models achieve negative log-likelihoods of 2.74 and 3.76 bits/dim on\nCIFAR-10 and ImageNet 32x32, outperforming autoregressive models on these\ntasks.",
          "link": "http://arxiv.org/abs/2101.09258",
          "publishedOn": "2021-06-24T01:51:44.779Z",
          "wordCount": 583,
          "title": "Maximum Likelihood Training of Score-Based Diffusion Models. (arXiv:2101.09258v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.11533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xueyuan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasquier_T/0/1/0/all/0/1\">Thomas Pasquier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Ding Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_J/0/1/0/all/0/1\">Junghwan Rhee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mickens_J/0/1/0/all/0/1\">James Mickens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1\">Margo Seltzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>",
          "description": "Many users implicitly assume that software can only be exploited after it is\ninstalled. However, recent supply-chain attacks demonstrate that application\nintegrity must be ensured during installation itself. We introduce SIGL, a new\ntool for detecting malicious behavior during software installation. SIGL\ncollects traces of system call activity, building a data provenance graph that\nit analyzes using a novel autoencoder architecture with a graph long short-term\nmemory network (graph LSTM) for the encoder and a standard multilayer\nperceptron for the decoder. SIGL flags suspicious installations as well as the\nspecific installation-time processes that are likely to be malicious. Using a\ntest corpus of 625 malicious installers containing real-world malware, we\ndemonstrate that SIGL has a detection accuracy of 96%, outperforming similar\nsystems from industry and academia by up to 87% in precision and recall and 45%\nin accuracy. We also demonstrate that SIGL can pinpoint the processes most\nlikely to have triggered malicious behavior, works on different audit platforms\nand operating systems, and is robust to training data contamination and\nadversarial attack. It can be used with application-specific models, even in\nthe presence of new software versions, as well as application-agnostic\nmeta-models that encompass a wide range of applications and installers.",
          "link": "http://arxiv.org/abs/2008.11533",
          "publishedOn": "2021-06-24T01:51:44.754Z",
          "wordCount": 692,
          "title": "SIGL: Securing Software Installations Through Deep Graph Learning. (arXiv:2008.11533v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all the previously\nmentioned drawbacks). The entropic out-of-distribution detection solution\ncomprises the IsoMax loss for training and the entropic score for\nout-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in\nreplacement because swapping the SoftMax loss with the IsoMax loss requires no\nchanges in the model's architecture or training procedures/hyperparameters. In\nthis paper, we propose to perform what we call an isometrization of the\ndistances used in the IsoMax loss. Additionally, we propose to replace the\nentropic score with the minimum distance score. Our experiments showed that\nthese simple modifications increase out-of-distribution detection performance\nwhile keeping the solution seamless.",
          "link": "http://arxiv.org/abs/2105.14399",
          "publishedOn": "2021-06-24T01:51:44.749Z",
          "wordCount": 610,
          "title": "Improving Entropic Out-of-Distribution Detection using Isometric Distances and the Minimum Distance Score. (arXiv:2105.14399v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1\">Branislav Kveton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konobeev_M/0/1/0/all/0/1\">Mikhail Konobeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chih-wei Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mladenov_M/0/1/0/all/0/1\">Martin Mladenov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutilier_C/0/1/0/all/0/1\">Craig Boutilier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1\">Csaba Szepesvari</a>",
          "description": "Efficient exploration in bandits is a fundamental online learning problem. We\npropose a variant of Thompson sampling that learns to explore better as it\ninteracts with bandit instances drawn from an unknown prior. The algorithm\nmeta-learns the prior and thus we call it MetaTS. We propose several efficient\nimplementations of MetaTS and analyze it in Gaussian bandits. Our analysis\nshows the benefit of meta-learning and is of a broader interest, because we\nderive a novel prior-dependent Bayes regret bound for Thompson sampling. Our\ntheory is complemented by empirical evaluation, which shows that MetaTS quickly\nadapts to the unknown prior.",
          "link": "http://arxiv.org/abs/2102.06129",
          "publishedOn": "2021-06-24T01:51:44.744Z",
          "wordCount": 569,
          "title": "Meta-Thompson Sampling. (arXiv:2102.06129v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shmileva_E/0/1/0/all/0/1\">Elena Shmileva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarzhan_V/0/1/0/all/0/1\">Viktor Sarzhan</a>",
          "description": "This work is devoted to the clustering of check-in sequences from a geosocial\nnetwork. We used the mixture Markov chain process as a mathematical model for\ntime-dependent types of data. For clustering, we adjusted the\nExpectation-Maximization (EM) algorithm. As a result, we obtained highly\ndetailed communities (clusters) of users of the now defunct geosocial network,\nWeeplaces.",
          "link": "http://arxiv.org/abs/2106.12039",
          "publishedOn": "2021-06-24T01:51:44.729Z",
          "wordCount": 503,
          "title": "Clustering of check-in sequences using the mixture Markov chain process. (arXiv:2106.12039v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08721",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Pokhrel_P/0/1/0/all/0/1\">Pujan Pokhrel</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hoque_M/0/1/0/all/0/1\">Md Tamjidul Hoque</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Abdelguerfi_M/0/1/0/all/0/1\">Mahdi Abdelguerfi</a>",
          "description": "In this paper, we propose a Light Gradient Boosting (LightGBM) to forecast\ndominant wave periods in oceanic waters. First, we use the data collected from\nCDIP buoys and apply various data filtering methods. The data filtering methods\nallow us to obtain a high-quality dataset for training and validation purposes.\nWe then extract various wave-based features like wave heights, periods,\nskewness, kurtosis, etc., and atmospheric features like humidity, pressure, and\nair temperature for the buoys. Afterward, we train algorithms that use LightGBM\nand Extra Trees through a hv-block cross-validation scheme to forecast dominant\nwave periods for up to 30 days ahead. LightGBM has the R2 score of 0.94, 0.94,\nand 0.94 for 1-day ahead, 15-day ahead, and 30-day ahead prediction. Similarly,\nExtra Trees (ET) has an R2 score of 0.88, 0.86, and 0.85 for 1-day ahead,\n15-day ahead, and 30 day ahead prediction. In case of the test dataset,\nLightGBM has R2 score of 0.94, 0.94, and 0.94 for 1-day ahead, 15-day ahead and\n30-day ahead prediction. ET has R2 score of 0.88, 0.86, and 0.85 for 1-day\nahead, 15-day ahead, and 30-day ahead prediction. A similar R2 score for both\ntraining and the test dataset suggests that the machine learning models\ndeveloped in this paper are robust. Since the LightGBM algorithm outperforms ET\nfor all the windows tested, it is taken as the final algorithm. Note that the\nperformance of both methods does not decrease significantly as the forecast\nhorizon increases. Likewise, the proposed method outperforms the numerical\napproaches included in this paper in the test dataset. For 1 day ahead\nprediction, the proposed algorithm has SI, Bias, CC, and RMSE of 0.09, 0.00,\n0.97, and 1.78 compared to 0.268, 0.40, 0.63, and 2.18 for the European Centre\nfor Medium-range Weather Forecasts (ECMWF) model, which outperforms all the\nother methods in the test dataset.",
          "link": "http://arxiv.org/abs/2105.08721",
          "publishedOn": "2021-06-24T01:51:44.724Z",
          "wordCount": 779,
          "title": "A LightGBM based Forecasting of Dominant Wave Periods in Oceanic Waters. (arXiv:2105.08721v3 [physics.ao-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12472",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Ziatdinov_M/0/1/0/all/0/1\">Maxim Ziatdinov</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Wong_C/0/1/0/all/0/1\">Chun Yin Wong</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kalinin_S/0/1/0/all/0/1\">Sergei V. Kalinin</a>",
          "description": "Recent advances in scanning tunneling and transmission electron microscopies\n(STM and STEM) have allowed routine generation of large volumes of imaging data\ncontaining information on the structure and functionality of materials. The\nexperimental data sets contain signatures of long-range phenomena such as\nphysical order parameter fields, polarization and strain gradients in STEM, or\nstanding electronic waves and carrier-mediated exchange interactions in STM,\nall superimposed onto scanning system distortions and gradual changes of\ncontrast due to drift and/or mis-tilt effects. Correspondingly, while the human\neye can readily identify certain patterns in the images such as lattice\nperiodicities, repeating structural elements, or microstructures, their\nautomatic extraction and classification are highly non-trivial and universal\npathways to accomplish such analyses are absent. We pose that the most\ndistinctive elements of the patterns observed in STM and (S)TEM images are\nsimilarity and (almost-) periodicity, behaviors stemming directly from the\nparsimony of elementary atomic structures, superimposed on the gradual changes\nreflective of order parameter distributions. However, the discovery of these\nelements via global Fourier methods is non-trivial due to variability and lack\nof ideal discrete translation symmetry. To address this problem, we develop\nshift-invariant variational autoencoders (shift-VAE) that allow disentangling\ncharacteristic repeating features in the images, their variations, and shifts\ninevitable for random sampling of image space. Shift-VAEs balance the\nuncertainty in the position of the object of interest with the uncertainty in\nshape reconstruction. This approach is illustrated for model 1D data, and\nfurther extended to synthetic and experimental STM and STEM 2D data.",
          "link": "http://arxiv.org/abs/2106.12472",
          "publishedOn": "2021-06-24T01:51:44.718Z",
          "wordCount": 700,
          "title": "Finding simplicity: unsupervised discovery of features, patterns, and order parameters via shift-invariant variational autoencoders. (arXiv:2106.12472v1 [cond-mat.dis-nn])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bie_X/0/1/0/all/0/1\">Xiaoyu Bie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leglaive_S/0/1/0/all/0/1\">Simon Leglaive</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girin_L/0/1/0/all/0/1\">Laurent Girin</a>",
          "description": "Dynamical variational auto-encoders (DVAEs) are a class of deep generative\nmodels with latent variables, dedicated to time series data modeling. DVAEs can\nbe considered as extensions of the variational autoencoder (VAE) that include\nthe modeling of temporal dependencies between successive observed and/or latent\nvectors in data sequences. Previous work has shown the interest of DVAEs and\ntheir better performance over the VAE for speech signals (spectrogram)\nmodeling. Independently, the VAE has been successfully applied to speech\nenhancement in noise, in an unsupervised noise-agnostic set-up that does not\nrequire the use of a parallel dataset of clean and noisy speech samples for\ntraining, but only requires clean speech signals. In this paper, we extend\nthose works to DVAE-based single-channel unsupervised speech enhancement, hence\nexploiting both speech signals unsupervised representation learning and\ndynamics modeling. We propose an unsupervised speech enhancement algorithm\nbased on the most general form of DVAEs, that we then adapt to three specific\nDVAE models to illustrate the versatility of the framework. More precisely, we\ncombine DVAE-based speech priors with a noise model based on nonnegative matrix\nfactorization, and we derive a variational expectation-maximization (VEM)\nalgorithm to perform speech enhancement. Experimental results show that the\nproposed approach based on DVAEs outperforms its VAE counterpart and a\nsupervised speech enhancement baseline.",
          "link": "http://arxiv.org/abs/2106.12271",
          "publishedOn": "2021-06-24T01:51:44.707Z",
          "wordCount": 652,
          "title": "Unsupervised Speech Enhancement using Dynamical Variational Auto-Encoders. (arXiv:2106.12271v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2011.10687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Somanath_G/0/1/0/all/0/1\">Gowri Somanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1\">Daniel Kurz</a>",
          "description": "We present a method to estimate an HDR environment map from a narrow\nfield-of-view LDR camera image in real-time. This enables perceptually\nappealing reflections and shading on virtual objects of any material finish,\nfrom mirror to diffuse, rendered into a real physical environment using\naugmented reality. Our method is based on our efficient convolutional neural\nnetwork architecture, EnvMapNet, trained end-to-end with two novel losses,\nProjectionLoss for the generated image, and ClusterLoss for adversarial\ntraining. Through qualitative and quantitative comparison to state-of-the-art\nmethods, we demonstrate that our algorithm reduces the directional error of\nestimated light sources by more than 50%, and achieves 3.7 times lower Frechet\nInception Distance (FID). We further showcase a mobile application that is able\nto run our neural network model in under 9 ms on an iPhone XS, and render in\nreal-time, visually coherent virtual objects in previously unseen real-world\nenvironments.",
          "link": "http://arxiv.org/abs/2011.10687",
          "publishedOn": "2021-06-24T01:51:44.693Z",
          "wordCount": 649,
          "title": "HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.08045",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Azhand_D/0/1/0/all/0/1\">Dr. Arash Azhand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rabe_D/0/1/0/all/0/1\">Dr. Sophie Rabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muller_D/0/1/0/all/0/1\">Dr. Swantje M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sattler_I/0/1/0/all/0/1\">Igor Sattler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Steinert_D/0/1/0/all/0/1\">Dr. Anika Steinert</a>",
          "description": "Despite its paramount importance for manifold use cases (e.g., in the health\ncare industry, sports, rehabilitation and fitness assessment), sufficiently\nvalid and reliable gait parameter measurement is still limited to high-tech\ngait laboratories mostly. Here, we demonstrate the excellent validity and\ntest-retest repeatability of a novel gait assessment system which is built upon\nmodern convolutional neural networks to extract three-dimensional skeleton\njoints from monocular frontal-view videos of walking humans. The validity study\nis based on a comparison to the GAITRite pressure-sensitive walkway system. All\nmeasured gait parameters (gait speed, cadence, step length and step time)\nshowed excellent concurrent validity for multiple walk trials at normal and\nfast gait speeds. The test-retest-repeatability is on the same level as the\nGAITRite system. In conclusion, we are convinced that our results can pave the\nway for cost, space and operationally effective gait analysis in broad\nmainstream applications. Most sensor-based systems are costly, must be operated\nby extensively trained personnel (e.g., motion capture systems) or - even if\nnot quite as costly - still possess considerable complexity (e.g., wearable\nsensors). In contrast, a video sufficient for the assessment method presented\nhere can be obtained by anyone, without much training, via a smartphone camera.",
          "link": "http://arxiv.org/abs/2008.08045",
          "publishedOn": "2021-06-24T01:51:44.688Z",
          "wordCount": 695,
          "title": "Algorithm Based on One Monocular Video Delivers Highly Valid and Reliable Gait Parameters. (arXiv:2008.08045v5 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1\">Rhea Sanjay Sukthanker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiwu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Suryansh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "Flow-based generative models have shown excellent ability to explicitly learn\nthe probability density function of data via a sequence of invertible\ntransformations. Yet, modeling long-range dependencies over normalizing flows\nremains understudied. To fill the gap, in this paper, we introduce two types of\ninvertible attention mechanisms for generative flow models. To be precise, we\npropose map-based and scaled dot-product attention for unconditional and\nconditional generative flow models. The key idea is to exploit split-based\nattention mechanisms to learn the attention weights and input representations\non every two splits of flow feature maps. Our method provides invertible\nattention modules with tractable Jacobian determinants, enabling seamless\nintegration of it at any positions of the flow-based models. The proposed\nattention mechanism can model the global data dependencies, leading to more\ncomprehensive flow models. Evaluation on multiple generation tasks demonstrates\nthat the introduced attention flow idea results in efficient flow models and\ncompares favorably against the state-of-the-art unconditional and conditional\ngenerative flow methods.",
          "link": "http://arxiv.org/abs/2106.03959",
          "publishedOn": "2021-06-24T01:51:44.683Z",
          "wordCount": 611,
          "title": "Generative Flows with Invertible Attentions. (arXiv:2106.03959v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shenghao Xu</a>",
          "description": "Multi-armed bandits (MAB) provide a principled online learning approach to\nattain the balance between exploration and exploitation. Due to the superior\nperformance and low feedback learning without the learning to act in multiple\nsituations, Multi-armed Bandits drawing widespread attention in applications\nranging such as recommender systems. Likewise, within the recommender system,\ncollaborative filtering (CF) is arguably the earliest and most influential\nmethod in the recommender system. Crucially, new users and an ever-changing\npool of recommended items are the challenges that recommender systems need to\naddress. For collaborative filtering, the classical method is training the\nmodel offline, then perform the online testing, but this approach can no longer\nhandle the dynamic changes in user preferences which is the so-called cold\nstart. So how to effectively recommend items to users in the absence of\neffective information? To address the aforementioned problems, a multi-armed\nbandit based collaborative filtering recommender system has been proposed,\nnamed BanditMF. BanditMF is designed to address two challenges in the\nmulti-armed bandits algorithm and collaborative filtering: (1) how to solve the\ncold start problem for collaborative filtering under the condition of scarcity\nof valid information, (2) how to solve the sub-optimal problem of bandit\nalgorithms in strong social relations domains caused by independently\nestimating unknown parameters associated with each user and ignoring\ncorrelations between users.",
          "link": "http://arxiv.org/abs/2106.10898",
          "publishedOn": "2021-06-24T01:51:44.678Z",
          "wordCount": 664,
          "title": "BanditMF: Multi-Armed Bandit Based Matrix Factorization Recommender System. (arXiv:2106.10898v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12012",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendler_Dunner_C/0/1/0/all/0/1\">Celestine Mendler-D&#xfc;nner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wenshuo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1\">Stephen Bates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "An increasingly common setting in machine learning involves multiple parties,\neach with their own data, who want to jointly make predictions on future test\npoints. Agents wish to benefit from the collective expertise of the full set of\nagents to make better predictions than they would individually, but may not be\nwilling to release their data or model parameters. In this work, we explore a\ndecentralized mechanism to make collective predictions at test time, leveraging\neach agent's pre-trained model without relying on external validation, model\nretraining, or data pooling. Our approach takes inspiration from the literature\nin social science on human consensus-making. We analyze our mechanism\ntheoretically, showing that it converges to inverse meansquared-error (MSE)\nweighting in the large-sample limit. To compute error bars on the collective\npredictions we propose a decentralized Jackknife procedure that evaluates the\nsensitivity of our mechanism to a single agent's prediction. Empirically, we\ndemonstrate that our scheme effectively combines models with differing quality\nacross the input space. The proposed consensus prediction achieves significant\ngains over classical model averaging, and even outperforms weighted averaging\nschemes that have access to additional validation data.",
          "link": "http://arxiv.org/abs/2106.12012",
          "publishedOn": "2021-06-24T01:51:44.673Z",
          "wordCount": 618,
          "title": "Test-time Collective Prediction. (arXiv:2106.12012v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yapar_C/0/1/0/all/0/1\">&#xc7;a&#x11f;kan Yapar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levie_R/0/1/0/all/0/1\">Ron Levie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1\">Gitta Kutyniok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caire_G/0/1/0/all/0/1\">Giuseppe Caire</a>",
          "description": "This paper deals with the problem of localization in a cellular network in a\ndense urban scenario. Global Navigation Satellite Systems typically perform\npoorly in urban environments, where the likelihood of line-of-sight conditions\nbetween the devices and the satellites is low, and thus alternative\nlocalization methods are required for good accuracy. We present a deep learning\nmethod for localization, based merely on pathloss, which does not require any\nincrease in computation complexity at the user devices with respect to the\ndevice standard operations, unlike methods that rely on time of arrival or\nangle of arrival information. In a wireless network, user devices scan the base\nstation beacon slots and identify the few strongest base station signals for\nhandover and user-base station association purposes. In the proposed method,\nthe user to be localized simply reports such received signal strengths to a\ncentral processing unit, which may be located in the cloud. For each base\nstation we have good approximation of the pathloss at every location in a dense\ngrid in the map. This approximation is provided by RadioUNet, a deep\nlearning-based simulator of pathloss functions in urban environment, that we\nhave previously proposed and published. Using the estimated pathloss radio maps\nof all base stations and the corresponding reported signal strengths, the\nproposed deep learning algorithm can extract a very accurate localization of\nthe user. The proposed method, called LocUNet, enjoys high robustness to\ninaccuracies in the estimated radio maps. We demonstrate this by numerical\nexperiments, which obtain state-of-the-art results.",
          "link": "http://arxiv.org/abs/2106.12556",
          "publishedOn": "2021-06-24T01:51:44.668Z",
          "wordCount": 694,
          "title": "Real-time Outdoor Localization Using Radio Maps: A Deep Learning Approach. (arXiv:2106.12556v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.02490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feurer_M/0/1/0/all/0/1\">Matthias Feurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijn_J/0/1/0/all/0/1\">Jan N. van Rijn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadra_A/0/1/0/all/0/1\">Arlind Kadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gijsbers_P/0/1/0/all/0/1\">Pieter Gijsbers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallik_N/0/1/0/all/0/1\">Neeratyoy Mallik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Sahithya Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_A/0/1/0/all/0/1\">Andreas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanschoren_J/0/1/0/all/0/1\">Joaquin Vanschoren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>",
          "description": "OpenML is an online platform for open science collaboration in machine\nlearning, used to share datasets and results of machine learning experiments.\nIn this paper we introduce OpenML-Python, a client API for Python, opening up\nthe OpenML platform for a wide range of Python-based tools. It provides easy\naccess to all datasets, tasks and experiments on OpenML from within Python. It\nalso provides functionality to conduct machine learning experiments, upload the\nresults to OpenML, and reproduce results which are stored on OpenML.\nFurthermore, it comes with a scikit-learn plugin and a plugin mechanism to\neasily integrate other machine learning libraries written in Python into the\nOpenML ecosystem. Source code and documentation is available at\nhttps://github.com/openml/openml-python/.",
          "link": "http://arxiv.org/abs/1911.02490",
          "publishedOn": "2021-06-24T01:51:44.653Z",
          "wordCount": 604,
          "title": "OpenML-Python: an extensible Python API for OpenML. (arXiv:1911.02490v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1\">Marcel C. B&#xfc;hler</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1\">Abhimitra Meka</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gengyan Li</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1\">Thabo Beeler</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a> (1) ((1) ETH Zurich, (2) Google)",
          "description": "Deep generative models have recently demonstrated the ability to synthesize\nphotorealistic images of human faces with novel identities. A key challenge to\nthe wide applicability of such techniques is to provide independent control\nover semantically meaningful parameters: appearance, head pose, face shape, and\nfacial expressions. In this paper, we propose VariTex - to the best of our\nknowledge the first method that learns a variational latent feature space of\nneural face textures, which allows sampling of novel identities. We combine\nthis generative model with a parametric face model and gain explicit control\nover head pose and facial expressions. To generate images of complete human\nheads, we propose an additive decoder that generates plausible additional\ndetails such as hair. A novel training scheme enforces a pose independent\nlatent space and in consequence, allows learning of a one-to-many mapping\nbetween latent codes and pose-conditioned exterior regions. The resulting\nmethod can generate geometrically consistent images of novel identities\nallowing fine-grained control over head pose, face shape, and facial\nexpressions, facilitating a broad range of downstream tasks, like sampling\nnovel identities, re-posing, expression transfer, and more.",
          "link": "http://arxiv.org/abs/2104.05988",
          "publishedOn": "2021-06-24T01:51:44.648Z",
          "wordCount": 665,
          "title": "VariTex: Variational Neural Face Textures. (arXiv:2104.05988v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08817",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Graf_F/0/1/0/all/0/1\">Florian Graf</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hofer_C/0/1/0/all/0/1\">Christoph D. Hofer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kwitt_R/0/1/0/all/0/1\">Roland Kwitt</a>",
          "description": "Minimizing cross-entropy over the softmax scores of a linear map composed\nwith a high-capacity encoder is arguably the most popular choice for training\nneural networks on supervised learning tasks. However, recent works show that\none can directly optimize the encoder instead, to obtain equally (or even more)\ndiscriminative representations via a supervised variant of a contrastive\nobjective. In this work, we address the question whether there are fundamental\ndifferences in the sought-for representation geometry in the output space of\nthe encoder at minimal loss. Specifically, we prove, under mild assumptions,\nthat both losses attain their minimum once the representations of each class\ncollapse to the vertices of a regular simplex, inscribed in a hypersphere. We\nprovide empirical evidence that this configuration is attained in practice and\nthat reaching a close-to-optimal state typically indicates good generalization\nperformance. Yet, the two losses show remarkably different optimization\nbehavior. The number of iterations required to perfectly fit to data scales\nsuperlinearly with the amount of randomly flipped labels for the supervised\ncontrastive loss. This is in contrast to the approximately linear scaling\npreviously reported for networks trained with cross-entropy.",
          "link": "http://arxiv.org/abs/2102.08817",
          "publishedOn": "2021-06-24T01:51:44.642Z",
          "wordCount": 635,
          "title": "Dissecting Supervised Constrastive Learning. (arXiv:2102.08817v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1\">Kamal Ndousse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eck_D/0/1/0/all/0/1\">Douglas Eck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaques_N/0/1/0/all/0/1\">Natasha Jaques</a>",
          "description": "Social learning is a key component of human and animal intelligence. By\ntaking cues from the behavior of experts in their environment, social learners\ncan acquire sophisticated behavior and rapidly adapt to new circumstances. This\npaper investigates whether independent reinforcement learning (RL) agents in a\nmulti-agent environment can learn to use social learning to improve their\nperformance. We find that in most circumstances, vanilla model-free RL agents\ndo not use social learning. We analyze the reasons for this deficiency, and\nshow that by imposing constraints on the training environment and introducing a\nmodel-based auxiliary loss we are able to obtain generalized social learning\npolicies which enable agents to: i) discover complex skills that are not\nlearned from single-agent training, and ii) adapt online to novel environments\nby taking cues from experts present in the new environment. In contrast, agents\ntrained with model-free RL or imitation learning generalize poorly and do not\nsucceed in the transfer tasks. By mixing multi-agent and solo training, we can\nobtain agents that use social learning to gain skills that they can deploy when\nalone, even out-performing agents trained alone from the start.",
          "link": "http://arxiv.org/abs/2010.00581",
          "publishedOn": "2021-06-24T01:51:44.637Z",
          "wordCount": 676,
          "title": "Emergent Social Learning via Multi-agent Reinforcement Learning. (arXiv:2010.00581v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.06085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mandlekar_A/0/1/0/all/0/1\">Ajay Mandlekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Danfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>",
          "description": "Imitation learning is an effective and safe technique to train robot policies\nin the real world because it does not depend on an expensive random exploration\nprocess. However, due to the lack of exploration, learning policies that\ngeneralize beyond the demonstrated behaviors is still an open challenge. We\npresent a novel imitation learning framework to enable robots to 1) learn\ncomplex real world manipulation tasks efficiently from a small number of human\ndemonstrations, and 2) synthesize new behaviors not contained in the collected\ndemonstrations. Our key insight is that multi-task domains often present a\nlatent structure, where demonstrated trajectories for different tasks intersect\nat common regions of the state space. We present Generalization Through\nImitation (GTI), a two-stage offline imitation learning algorithm that exploits\nthis intersecting structure to train goal-directed policies that generalize to\nunseen start and goal state combinations. In the first stage of GTI, we train a\nstochastic policy that leverages trajectory intersections to have the capacity\nto compose behaviors from different demonstration trajectories together. In the\nsecond stage of GTI, we collect a small set of rollouts from the unconditioned\nstochastic policy of the first stage, and train a goal-directed agent to\ngeneralize to novel start and goal configurations. We validate GTI in both\nsimulated domains and a challenging long-horizon robotic manipulation domain in\nthe real world. Additional results and videos are available at\nhttps://sites.google.com/view/gti2020/ .",
          "link": "http://arxiv.org/abs/2003.06085",
          "publishedOn": "2021-06-24T01:51:44.623Z",
          "wordCount": 709,
          "title": "Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations. (arXiv:2003.06085v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slack_D/0/1/0/all/0/1\">Dylan Slack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilgard_S/0/1/0/all/0/1\">Sophie Hilgard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Hima Lakkaraju</a>",
          "description": "As machine learning models are increasingly used in critical decision-making\nsettings (e.g., healthcare, finance), there has been a growing emphasis on\ndeveloping methods to explain model predictions. Such \\textit{explanations} are\nused to understand and establish trust in models and are vital components in\nmachine learning pipelines. Though explanations are a critical piece in these\nsystems, there is little understanding about how they are vulnerable to\nmanipulation by adversaries. In this paper, we discuss how two broad classes of\nexplanations are vulnerable to manipulation. We demonstrate how adversaries can\ndesign biased models that manipulate model agnostic feature attribution methods\n(e.g., LIME \\& SHAP) and counterfactual explanations that hill-climb during the\ncounterfactual search (e.g., Wachter's Algorithm \\& DiCE) into\n\\textit{concealing} the model's biases. These vulnerabilities allow an\nadversary to deploy a biased model, yet explanations will not reveal this bias,\nthereby deceiving stakeholders into trusting the model. We evaluate the\nmanipulations on real world data sets, including COMPAS and Communities \\&\nCrime, and find explanations can be manipulated in practice.",
          "link": "http://arxiv.org/abs/2106.12563",
          "publishedOn": "2021-06-24T01:51:44.617Z",
          "wordCount": 605,
          "title": "Feature Attributions and Counterfactual Explanations Can Be Manipulated. (arXiv:2106.12563v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03832",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Deligiannidis_S/0/1/0/all/0/1\">Stavros Deligiannidis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mesaritakis_C/0/1/0/all/0/1\">Charis Mesaritakis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bogris_A/0/1/0/all/0/1\">Adonis Bogris</a>",
          "description": "We investigate the complexity and performance of recurrent neural network\n(RNN) models as post-processing units for the compensation of fibre\nnonlinearities in digital coherent systems carrying polarization multiplexed\n16-QAM and 32-QAM signals. We evaluate three bi-directional RNN models, namely\nthe bi-LSTM, bi-GRU and bi-Vanilla-RNN and show that all of them are promising\nnonlinearity compensators especially in dispersion unmanaged systems. Our\nsimulations show that during inference the three models provide similar\ncompensation performance, therefore in real-life systems the simplest scheme\nbased on Vanilla-RNN units should be preferred. We compare bi-Vanilla-RNN with\nVolterra nonlinear equalizers and exhibit its superiority both in terms of\nperformance and complexity, thus highlighting that RNN processing is a very\npromising pathway for the upgrade of long-haul optical communication systems\nutilizing coherent detection.",
          "link": "http://arxiv.org/abs/2103.03832",
          "publishedOn": "2021-06-24T01:51:44.591Z",
          "wordCount": 601,
          "title": "Performance and Complexity Analysis of bi-directional Recurrent Neural Network Models vs. Volterra Nonlinear Equalizers in Digital Coherent Systems. (arXiv:2103.03832v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02869",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Demir_U/0/1/0/all/0/1\">Ugur Demir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Irmakci_I/0/1/0/all/0/1\">Ismail Irmakci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keles_E/0/1/0/all/0/1\">Elif Keles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Topcu_A/0/1/0/all/0/1\">Ahmet Topcu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyue Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Spampinato_C/0/1/0/all/0/1\">Concetto Spampinato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jambawalikar_S/0/1/0/all/0/1\">Sachin Jambawalikar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turkbey_E/0/1/0/all/0/1\">Evrim Turkbey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turkbey_B/0/1/0/all/0/1\">Baris Turkbey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1\">Ulas Bagci</a>",
          "description": "Visual explanation methods have an important role in the prognosis of the\npatients where the annotated data is limited or unavailable. There have been\nseveral attempts to use gradient-based attribution methods to localize\npathology from medical scans without using segmentation labels. This research\ndirection has been impeded by the lack of robustness and reliability. These\nmethods are highly sensitive to the network parameters. In this study, we\nintroduce a robust visual explanation method to address this problem for\nmedical applications. We provide an innovative visual explanation algorithm for\ngeneral purpose and as an example application, we demonstrate its effectiveness\nfor quantifying lesions in the lungs caused by the Covid-19 with high accuracy\nand robustness without using dense segmentation labels. This approach overcomes\nthe drawbacks of commonly used Grad-CAM and its extended versions. The premise\nbehind our proposed strategy is that the information flow is minimized while\nensuring the classifier prediction stays similar. Our findings indicate that\nthe bottleneck condition provides a more stable severity estimation than the\nsimilar attribution methods.",
          "link": "http://arxiv.org/abs/2104.02869",
          "publishedOn": "2021-06-24T01:51:44.578Z",
          "wordCount": 697,
          "title": "Information Bottleneck Attribution for Visual Explanations of Diagnosis and Prognosis. (arXiv:2104.02869v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.04784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1\">Xiao Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>",
          "description": "Deep neural networks, while generalize well, are known to be sensitive to\nsmall adversarial perturbations. This phenomenon poses severe security threat\nand calls for in-depth investigation of the robustness of deep learning models.\nWith the emergence of neural networks for graph structured data, similar\ninvestigations are urged to understand their robustness. It has been found that\nadversarially perturbing the graph structure and/or node features may result in\na significant degradation of the model performance. In this work, we show from\na different angle that such fragility similarly occurs if the graph contains a\nfew bad-actor nodes, which compromise a trained graph neural network through\nflipping the connections to any targeted victim. Worse, the bad actors found\nfor one graph model severely compromise other models as well. We call the bad\nactors ``anchor nodes'' and propose an algorithm, named GUA, to identify them.\nThorough empirical investigations suggest an interesting finding that the\nanchor nodes often belong to the same class; and they also corroborate the\nintuitive trade-off between the number of anchor nodes and the attack success\nrate. For the dataset Cora which contains 2708 nodes, as few as six anchor\nnodes will result in an attack success rate higher than 80\\% for GCN and other\nthree models.",
          "link": "http://arxiv.org/abs/2002.04784",
          "publishedOn": "2021-06-24T01:51:44.572Z",
          "wordCount": 699,
          "title": "Graph Universal Adversarial Attacks: A Few Bad Actors Ruin Graph Learning Models. (arXiv:2002.04784v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06097",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lyu_Y/0/1/0/all/0/1\">Yueming Lyu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor Tsang</a>",
          "description": "Recent studies show a close connection between neural networks (NN) and\nkernel methods. However, most of these analyses (e.g., NTK) focus on the\ninfluence of (infinite) width instead of the depth of NN models. There remains\na gap between theory and practical network designs that benefit from the depth.\nThis paper first proposes a novel kernel family named Neural Optimization\nKernel (NOK). Our kernel is defined as the inner product between two $T$-step\nupdated functionals in RKHS w.r.t. a regularized optimization problem.\nTheoretically, we proved the monotonic descent property of our update rule for\nboth convex and non-convex problems, and a $O(1/T)$ convergence rate of our\nupdates for convex problems. Moreover, we propose a data-dependent structured\napproximation of our NOK, which builds the connection between training deep NNs\nand kernel methods associated with NOK. The resultant computational graph is a\nResNet-type finite width NN. Our structured approximation preserved the\nmonotonic descent property and $O(1/T)$ convergence rate. Namely, a $T$-layer\nNN performs $T$-step monotonic descent updates. Notably, we show our\n$T$-layered structured NN with ReLU maintains a $O(1/T)$ convergence rate\nw.r.t. a convex regularized problem, which explains the success of ReLU on\ntraining deep NN from a NN architecture optimization perspective. For the\nunsupervised learning and the shared parameter case, we show the equivalence of\ntraining structured NN with GD and performing functional gradient descent in\nRKHS associated with a fixed (data-dependent) NOK at an infinity-width regime.\nFor finite NOKs, we prove generalization bounds. Remarkably, we show that\noverparameterized deep NN (NOK) can increase the expressive power to reduce\nempirical risk and reduce the generalization bound at the same time. Extensive\nexperiments verify the robustness of our structured NOK blocks.",
          "link": "http://arxiv.org/abs/2106.06097",
          "publishedOn": "2021-06-24T01:51:44.565Z",
          "wordCount": 736,
          "title": "Neural Optimization Kernel: Towards Robust Deep Learning. (arXiv:2106.06097v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samragh_M/0/1/0/all/0/1\">Mohammad Samragh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_H/0/1/0/all/0/1\">Hossein Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triastcyn_A/0/1/0/all/0/1\">Aleksei Triastcyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azarian_K/0/1/0/all/0/1\">Kambiz Azarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soriaga_J/0/1/0/all/0/1\">Joseph Soriaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1\">Farinaz Koushanfar</a>",
          "description": "Splitting network computations between the edge device and a server enables\nlow edge-compute inference of neural networks but might expose sensitive\ninformation about the test query to the server. To address this problem,\nexisting techniques train the model to minimize information leakage for a given\nset of sensitive attributes. In practice, however, the test queries might\ncontain attributes that are not foreseen during training. We propose instead an\nunsupervised obfuscation method to discard the information irrelevant to the\nmain task. We formulate the problem via an information theoretical framework\nand derive an analytical solution for a given distortion to the model output.\nIn our method, the edge device runs the model up to a split layer determined\nbased on its computational capacity. It then obfuscates the obtained feature\nvector based on the first layer of the server model by removing the components\nin the null space as well as the low-energy components of the remaining signal.\nOur experimental results show that our method outperforms existing techniques\nin removing the information of the irrelevant attributes and maintaining the\naccuracy on the target label. We also show that our method reduces the\ncommunication cost and incurs only a small computational overhead.",
          "link": "http://arxiv.org/abs/2104.11413",
          "publishedOn": "2021-06-24T01:51:44.521Z",
          "wordCount": 670,
          "title": "Unsupervised Information Obfuscation for Split Inference of Neural Networks. (arXiv:2104.11413v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10492",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hanin_B/0/1/0/all/0/1\">Boris Hanin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jeong_R/0/1/0/all/0/1\">Ryan Jeong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rolnick_D/0/1/0/all/0/1\">David Rolnick</a>",
          "description": "Assessing the complexity of functions computed by a neural network helps us\nunderstand how the network will learn and generalize. One natural measure of\ncomplexity is how the network distorts length - if the network takes a\nunit-length curve as input, what is the length of the resulting curve of\noutputs? It has been widely believed that this length grows exponentially in\nnetwork depth. We prove that in fact this is not the case: the expected length\ndistortion does not grow with depth, and indeed shrinks slightly, for ReLU\nnetworks with standard random initialization. We also generalize this result by\nproving upper bounds both for higher moments of the length distortion and for\nthe distortion of higher-dimensional volumes. These theoretical results are\ncorroborated by our experiments.",
          "link": "http://arxiv.org/abs/2102.10492",
          "publishedOn": "2021-06-24T01:51:44.505Z",
          "wordCount": 575,
          "title": "Deep ReLU Networks Preserve Expected Length. (arXiv:2102.10492v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dorr_L/0/1/0/all/0/1\">Laura D&#xf6;rr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandt_F/0/1/0/all/0/1\">Felix Brandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_A/0/1/0/all/0/1\">Alexander Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pouls_M/0/1/0/all/0/1\">Martin Pouls</a>",
          "description": "While common image object detection tasks focus on bounding boxes or\nsegmentation masks as object representations, we consider the problem of\nfinding objects based on four arbitrary vertices. We propose a novel model,\nnamed TetraPackNet, to tackle this problem. TetraPackNet is based on CornerNet\nand uses similar algorithms and ideas. It is designated for applications\nrequiring high-accuracy detection of regularly shaped objects, which is the\ncase in the logistics use-case of packaging structure recognition. We evaluate\nour model on our specific real-world dataset for this use-case. Baselined\nagainst a previous solution, consisting of a Mask R-CNN model and suitable\npost-processing steps, TetraPackNet achieves superior results (9% higher in\naccuracy) in the sub-task of four-corner based transport unit side detection.",
          "link": "http://arxiv.org/abs/2104.09123",
          "publishedOn": "2021-06-24T01:51:44.491Z",
          "wordCount": 586,
          "title": "TetraPackNet: Four-Corner-Based Object Detection in Logistics Use-Cases. (arXiv:2104.09123v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08583",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Pokhrel_P/0/1/0/all/0/1\">Pujan Pokhrel</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hoque_M/0/1/0/all/0/1\">Md Tamjidul Hoque</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Abdelguerfi_M/0/1/0/all/0/1\">Mahdi Abdelguerfi</a>",
          "description": "This paper proposes a machine learning method based on the Extra Trees (ET)\nalgorithm for forecasting Significant Wave Heights in oceanic waters. To derive\nmultiple features from the CDIP buoys, which make point measurements, we first\nnowcast various parameters and then forecast them at 30-min intervals. The\nproposed algorithm has Scatter Index (SI), Bias, Correlation Coefficient, Root\nMean Squared Error (RMSE) of 0.130, -0.002, 0.97, and 0.14, respectively, for\none day ahead prediction and 0.110, -0.001, 0.98, and 0.122, respectively, for\n14-day ahead prediction on the testing dataset. While other state-of-the-art\nmethods can only forecast up to 120 hours ahead, we extend it further to 14\ndays. Our proposed setup includes spectral features, hv-block cross-validation,\nand stringent QC criteria. The proposed algorithm performs significantly better\nthan the state-of-the-art methods commonly used for significant wave height\nforecasting for one-day ahead prediction. Moreover, the improved performance of\nthe proposed machine learning method compared to the numerical methods shows\nthat this performance can be extended to even longer periods allowing for early\nprediction of significant wave heights in oceanic waters.",
          "link": "http://arxiv.org/abs/2105.08583",
          "publishedOn": "2021-06-24T01:51:44.477Z",
          "wordCount": 644,
          "title": "Machine Learning in weakly nonlinear systems: A Case study on Significant wave heights. (arXiv:2105.08583v2 [physics.ao-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimeno_F/0/1/0/all/0/1\">Felix Gimeno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andrew Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Joao Carreira</a>",
          "description": "Biological systems perceive the world by simultaneously processing\nhigh-dimensional inputs from modalities as diverse as vision, audition, touch,\nproprioception, etc. The perception models used in deep learning on the other\nhand are designed for individual modalities, often relying on domain-specific\nassumptions such as the local grid structures exploited by virtually all\nexisting vision models. These priors introduce helpful inductive biases, but\nalso lock models to individual modalities. In this paper we introduce the\nPerceiver - a model that builds upon Transformers and hence makes few\narchitectural assumptions about the relationship between its inputs, but that\nalso scales to hundreds of thousands of inputs, like ConvNets. The model\nleverages an asymmetric attention mechanism to iteratively distill inputs into\na tight latent bottleneck, allowing it to scale to handle very large inputs. We\nshow that this architecture is competitive with or outperforms strong,\nspecialized models on classification tasks across various modalities: images,\npoint clouds, audio, video, and video+audio. The Perceiver obtains performance\ncomparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly\nattending to 50,000 pixels. It is also competitive in all modalities in\nAudioSet.",
          "link": "http://arxiv.org/abs/2103.03206",
          "publishedOn": "2021-06-24T01:51:44.471Z",
          "wordCount": 679,
          "title": "Perceiver: General Perception with Iterative Attention. (arXiv:2103.03206v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aydore_S/0/1/0/all/0/1\">Sergul Aydore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_W/0/1/0/all/0/1\">William Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kearns_M/0/1/0/all/0/1\">Michael Kearns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenthapadi_K/0/1/0/all/0/1\">Krishnaram Kenthapadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melis_L/0/1/0/all/0/1\">Luca Melis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1\">Aaron Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siva_A/0/1/0/all/0/1\">Ankit Siva</a>",
          "description": "We propose, implement, and evaluate a new algorithm for releasing answers to\nvery large numbers of statistical queries like $k$-way marginals, subject to\ndifferential privacy. Our algorithm makes adaptive use of a continuous\nrelaxation of the Projection Mechanism, which answers queries on the private\ndataset using simple perturbation, and then attempts to find the synthetic\ndataset that most closely matches the noisy answers. We use a continuous\nrelaxation of the synthetic dataset domain which makes the projection loss\ndifferentiable, and allows us to use efficient ML optimization techniques and\ntooling. Rather than answering all queries up front, we make judicious use of\nour privacy budget by iteratively and adaptively finding queries for which our\n(relaxed) synthetic data has high error, and then repeating the projection. We\nperform extensive experimental evaluations across a range of parameters and\ndatasets, and find that our method outperforms existing algorithms in many\ncases, especially when the privacy budget is small or the query class is large.",
          "link": "http://arxiv.org/abs/2103.06641",
          "publishedOn": "2021-06-24T01:51:44.466Z",
          "wordCount": 631,
          "title": "Differentially Private Query Release Through Adaptive Projection. (arXiv:2103.06641v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.01350",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Yang_X/0/1/0/all/0/1\">Xiangyu Yang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_J/0/1/0/all/0/1\">Jiashan Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>",
          "description": "This paper primarily focuses on computing the Euclidean projection of a\nvector onto the $\\ell_{p}$ ball in which $p\\in(0,1)$. Such a problem emerges as\nthe core building block in statistical machine learning and signal processing\ntasks because of its ability to promote sparsity. However, efficient numerical\nalgorithms for finding the projections are still not available, particularly in\nlarge-scale optimization. To meet this challenge, we first derive the\nfirst-order necessary optimality conditions of this problem using Fr\\'echet\nnormal cone. Based on this characterization, we develop a novel numerical\napproach for computing the stationary point through solving a sequence of\nprojections onto the reweighted $\\ell_{1}$-balls. This method is practically\nsimple to implement and computationally efficient. Moreover, the proposed\nalgorithm is shown to converge uniquely under mild conditions and has a\nworst-case $O(1/\\sqrt{k})$ convergence rate. Numerical experiments demonstrate\nthe efficiency of our proposed algorithm.",
          "link": "http://arxiv.org/abs/2101.01350",
          "publishedOn": "2021-06-24T01:51:44.448Z",
          "wordCount": 643,
          "title": "Towards an efficient approach for the nonconvex $\\ell_p$-ball projection: algorithm and analysis. (arXiv:2101.01350v4 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suneja_S/0/1/0/all/0/1\">Sahil Suneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yunhui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yufan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laredo_J/0/1/0/all/0/1\">Jim Laredo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morari_A/0/1/0/all/0/1\">Alessandro Morari</a>",
          "description": "This work explores the signal awareness of AI models for source code\nunderstanding. Using a software vulnerability detection use case, we evaluate\nthe models' ability to capture the correct vulnerability signals to produce\ntheir predictions. Our prediction-preserving input minimization (P2IM) approach\nsystematically reduces the original source code to a minimal snippet which a\nmodel needs to maintain its prediction. The model's reliance on incorrect\nsignals is then uncovered when the vulnerability in the original code is\nmissing in the minimal snippet, both of which the model however predicts as\nbeing vulnerable. We measure the signal awareness of models using a new metric\nwe propose- Signal-aware Recall (SAR). We apply P2IM on three different neural\nnetwork architectures across multiple datasets. The results show a sharp drop\nin the model's Recall from the high 90s to sub-60s with the new metric,\nhighlighting that the models are presumably picking up a lot of noise or\ndataset nuances while learning their vulnerability detection logic. Although\nthe drop in model performance may be perceived as an adversarial attack, but\nthis isn't P2IM's objective. The idea is rather to uncover the signal-awareness\nof a black-box model in a data-driven manner via controlled queries. SAR's\npurpose is to measure the impact of task-agnostic model training, and not to\nsuggest a shortcoming in the Recall metric. The expectation, in fact, is for\nSAR to match Recall in the ideal scenario where the model truly captures\ntask-specific signals.",
          "link": "http://arxiv.org/abs/2011.14934",
          "publishedOn": "2021-06-24T01:51:44.442Z",
          "wordCount": 720,
          "title": "Probing Model Signal-Awareness via Prediction-Preserving Input Minimization. (arXiv:2011.14934v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12312",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Scognamiglio_S/0/1/0/all/0/1\">Salvatore Scognamiglio</a>",
          "description": "This paper introduces a neural network approach for fitting the Lee-Carter\nand the Poisson Lee-Carter model on multiple populations. We develop some\nneural networks that replicate the structure of the individual LC models and\nallow their joint fitting by analysing the mortality data of all the considered\npopulations simultaneously. The neural network architecture is specifically\ndesigned to calibrate each individual model using all available information\ninstead of using a population-specific subset of data as in the traditional\nestimation schemes. A large set of numerical experiments performed on all the\ncountries of the Human Mortality Database (HMD) shows the effectiveness of our\napproach. In particular, the resulting parameter estimates appear smooth and\nless sensitive to the random fluctuations often present in the mortality rates'\ndata, especially for low-population countries. In addition, the forecasting\nperformance results significantly improved as well.",
          "link": "http://arxiv.org/abs/2106.12312",
          "publishedOn": "2021-06-24T01:51:44.436Z",
          "wordCount": 573,
          "title": "Calibrating the Lee-Carter and the Poisson Lee-Carter models via Neural Networks. (arXiv:2106.12312v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2012.10988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tomani_C/0/1/0/all/0/1\">Christian Tomani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruber_S/0/1/0/all/0/1\">Sebastian Gruber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_M/0/1/0/all/0/1\">Muhammed Ebrar Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buettner_F/0/1/0/all/0/1\">Florian Buettner</a>",
          "description": "We address the problem of uncertainty calibration. While standard deep neural\nnetworks typically yield uncalibrated predictions, calibrated confidence scores\nthat are representative of the true likelihood of a prediction can be achieved\nusing post-hoc calibration methods. However, to date the focus of these\napproaches has been on in-domain calibration. Our contribution is two-fold.\nFirst, we show that existing post-hoc calibration methods yield highly\nover-confident predictions under domain shift. Second, we introduce a simple\nstrategy where perturbations are applied to samples in the validation set\nbefore performing the post-hoc calibration step. In extensive experiments, we\ndemonstrate that this perturbation step results in substantially better\ncalibration under domain shift on a wide range of architectures and modelling\ntasks.",
          "link": "http://arxiv.org/abs/2012.10988",
          "publishedOn": "2021-06-24T01:51:44.429Z",
          "wordCount": 604,
          "title": "Post-hoc Uncertainty Calibration for Domain Drift Scenarios. (arXiv:2012.10988v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12785",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Goel_G/0/1/0/all/0/1\">Gautam Goel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassibi_B/0/1/0/all/0/1\">Babak Hassibi</a>",
          "description": "We consider measurement-feedback control in linear dynamical systems from the\nperspective of regret minimization. Unlike most prior work in this area, we\nfocus on the problem of designing an online controller which competes with the\noptimal dynamic sequence of control actions selected in hindsight, instead of\nthe best controller in some specific class of controllers. This formulation of\nregret is attractive when the environment changes over time and no single\ncontroller achieves good performance over the entire time horizon. We show that\nin the measurement-feedback setting, unlike in the full-information setting,\nthere is no single offline controller which outperforms every other offline\ncontroller on every disturbance, and propose a new $H_2$-optimal offline\ncontroller as a benchmark for the online controller to compete against. We show\nthat the corresponding regret-optimal online controller can be found via a\nnovel reduction to the classical Nehari problem from robust control and present\na tight data-dependent bound on its regret.",
          "link": "http://arxiv.org/abs/2011.12785",
          "publishedOn": "2021-06-24T01:51:44.423Z",
          "wordCount": 614,
          "title": "Regret-optimal measurement-feedback control. (arXiv:2011.12785v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>",
          "description": "We develop an algorithm for sequential adaptation of a classifier that is\ntrained for a source domain to generalize in an unannotated target domain. We\nconsider that the model has been trained on the source domain annotated data\nand then it needs to be adapted using the target domain unannotated data when\nthe source domain data is not accessible. We align the distributions of the\nsource and the target domains in a discriminative embedding space via an\nintermediate internal distribution. This distribution is estimated using the\nsource data representations in the embedding. We conduct experiments on four\nbenchmarks to demonstrate the method is effective and compares favorably\nagainst existing methods.",
          "link": "http://arxiv.org/abs/2007.00197",
          "publishedOn": "2021-06-24T01:51:44.407Z",
          "wordCount": 593,
          "title": "Sequential Model Adaptation Using Domain Agnostic Internal Distributions. (arXiv:2007.00197v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1\">Seyed Saeed Changiz Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Fred X. Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1\">Mohammad Salameh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1\">Keith Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1\">Shuo Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1\">Shangling Jui</a>",
          "description": "Despite the empirical success of neural architecture search (NAS) in deep\nlearning applications, the optimality, reproducibility and cost of NAS schemes\nremain hard to assess. In this paper, we propose Generative Adversarial NAS\n(GA-NAS) with theoretically provable convergence guarantees, promoting\nstability and reproducibility in neural architecture search. Inspired by\nimportance sampling, GA-NAS iteratively fits a generator to previously\ndiscovered top architectures, thus increasingly focusing on important parts of\na large search space. Furthermore, we propose an efficient adversarial learning\napproach, where the generator is trained by reinforcement learning based on\nrewards provided by a discriminator, thus being able to explore the search\nspace without evaluating a large number of architectures. Extensive experiments\nshow that GA-NAS beats the best published results under several cases on three\npublic NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search\nconstraints and search spaces. We show that GA-NAS can be used to improve\nalready optimized baselines found by other NAS methods, including EfficientNet\nand ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in\ntheir original search space.",
          "link": "http://arxiv.org/abs/2105.09356",
          "publishedOn": "2021-06-24T01:51:44.402Z",
          "wordCount": 670,
          "title": "Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12372",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1\">Thomas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rousselle_F/0/1/0/all/0/1\">Fabrice Rousselle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novak_J/0/1/0/all/0/1\">Jan Nov&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_A/0/1/0/all/0/1\">Alexander Keller</a>",
          "description": "We present a real-time neural radiance caching method for path-traced global\nillumination. Our system is designed to handle fully dynamic scenes, and makes\nno assumptions about the lighting, geometry, and materials. The data-driven\nnature of our approach sidesteps many difficulties of caching algorithms, such\nas locating, interpolating, and updating cache points. Since pretraining neural\nnetworks to handle novel, dynamic scenes is a formidable generalization\nchallenge, we do away with pretraining and instead achieve generalization via\nadaptation, i.e. we opt for training the radiance cache while rendering. We\nemploy self-training to provide low-noise training targets and simulate\ninfinite-bounce transport by merely iterating few-bounce training updates. The\nupdates and cache queries incur a mild overhead -- about 2.6ms on full HD\nresolution -- thanks to a streaming implementation of the neural network that\nfully exploits modern hardware. We demonstrate significant noise reduction at\nthe cost of little induced bias, and report state-of-the-art, real-time\nperformance on a number of challenging scenarios.",
          "link": "http://arxiv.org/abs/2106.12372",
          "publishedOn": "2021-06-24T01:51:44.397Z",
          "wordCount": 600,
          "title": "Real-time Neural Radiance Caching for Path Tracing. (arXiv:2106.12372v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henning_C/0/1/0/all/0/1\">Christian Henning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cervera_M/0/1/0/all/0/1\">Maria R. Cervera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAngelo_F/0/1/0/all/0/1\">Francesco D&#x27;Angelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_J/0/1/0/all/0/1\">Johannes von Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Traber_R/0/1/0/all/0/1\">Regina Traber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehret_B/0/1/0/all/0/1\">Benjamin Ehret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_S/0/1/0/all/0/1\">Seijin Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacramento_J/0/1/0/all/0/1\">Jo&#xe3;o Sacramento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grewe_B/0/1/0/all/0/1\">Benjamin F. Grewe</a>",
          "description": "Learning a sequence of tasks without access to i.i.d. observations is a\nwidely studied form of continual learning (CL) that remains challenging. In\nprinciple, Bayesian learning directly applies to this setting, since recursive\nand one-off Bayesian updates yield the same result. In practice, however,\nrecursive updating often leads to poor trade-off solutions across tasks because\napproximate inference is necessary for most models of interest. Here, we\ndescribe an alternative Bayesian approach where task-conditioned parameter\ndistributions are continually inferred from data. We offer a practical deep\nlearning implementation of our framework based on probabilistic\ntask-conditioned hypernetworks, an approach we term \"posterior meta-replay\".\nExperiments on standard benchmarks show that our probabilistic hypernetworks\ncompress sequences of posterior parameter distributions with virtually no\nforgetting. We obtain considerable performance gains compared to existing\nBayesian CL methods, and identify task inference as our major limiting factor.\nThis limitation has several causes that are independent of the considered\nsequential setting, opening up new avenues for progress in CL.",
          "link": "http://arxiv.org/abs/2103.01133",
          "publishedOn": "2021-06-24T01:51:44.391Z",
          "wordCount": 633,
          "title": "Posterior Meta-Replay for Continual Learning. (arXiv:2103.01133v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.11561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polo_F/0/1/0/all/0/1\">Felipe Maia Polo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciochetti_I/0/1/0/all/0/1\">Itamar Ciochetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertolo_E/0/1/0/all/0/1\">Emerson Bertolo</a>",
          "description": "The objective of this paper is to develop predictive models to classify\nBrazilian legal proceedings in three possible classes of status: (i) archived\nproceedings, (ii) active proceedings, and (iii) suspended proceedings. This\nproblem's resolution is intended to assist public and private institutions in\nmanaging large portfolios of legal proceedings, providing gains in scale and\nefficiency. In this paper, legal proceedings are made up of sequences of short\ntexts called \"motions.\" We combined several natural language processing (NLP)\nand machine learning techniques to solve the problem. Although working with\nPortuguese NLP, which can be challenging due to lack of resources, our\napproaches performed remarkably well in the classification task, achieving\nmaximum accuracy of .93 and top average F1 Scores of .89 (macro) and .93\n(weighted). Furthermore, we could extract and interpret the patterns learned by\none of our models besides quantifying how those patterns relate to the\nclassification task. The interpretability step is important among machine\nlearning legal applications and gives us an exciting insight into how black-box\nmodels make decisions.",
          "link": "http://arxiv.org/abs/2003.11561",
          "publishedOn": "2021-06-24T01:51:44.385Z",
          "wordCount": 677,
          "title": "Predicting Legal Proceedings Status: Approaches Based on Sequential Text Data. (arXiv:2003.11561v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04290",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ma_X/0/1/0/all/0/1\">Xingchen Ma</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Blaschko_M/0/1/0/all/0/1\">Matthew B. Blaschko</a>",
          "description": "In many applications, it is desirable that a classifier not only makes\naccurate predictions, but also outputs calibrated posterior probabilities.\nHowever, many existing classifiers, especially deep neural network classifiers,\ntend to be uncalibrated. Post-hoc calibration is a technique to recalibrate a\nmodel by learning a calibration map. Existing approaches mostly focus on\nconstructing calibration maps with low calibration errors, however, this\nquality is inadequate for a calibrator being useful. In this paper, we\nintroduce two constraints that are worth consideration in designing a\ncalibration map for post-hoc calibration. Then we present Meta-Cal, which is\nbuilt from a base calibrator and a ranking model. Under some mild assumptions,\ntwo high-probability bounds are given with respect to these constraints.\nEmpirical results on CIFAR-10, CIFAR-100 and ImageNet and a range of popular\nnetwork architectures show our proposed method significantly outperforms the\ncurrent state of the art for post-hoc multi-class classification calibration.",
          "link": "http://arxiv.org/abs/2105.04290",
          "publishedOn": "2021-06-24T01:51:44.369Z",
          "wordCount": 592,
          "title": "Meta-Cal: Well-controlled Post-hoc Calibration by Ranking. (arXiv:2105.04290v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12288",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_M/0/1/0/all/0/1\">Ming Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu-Dong Liu</a>",
          "description": "Detecting the newly emerging malware variants in real time is crucial for\nmitigating cyber risks and proactively blocking intrusions. In this paper, we\npropose MG-DVD, a novel detection framework based on dynamic heterogeneous\ngraph learning, to detect malware variants in real time. Particularly, MG-DVD\nfirst models the fine-grained execution event streams of malware variants into\ndynamic heterogeneous graphs and investigates real-world meta-graphs between\nmalware objects, which can effectively characterize more discriminative\nmalicious evolutionary patterns between malware and their variants. Then,\nMG-DVD presents two dynamic walk-based heterogeneous graph learning methods to\nlearn more comprehensive representations of malware variants, which\nsignificantly reduces the cost of the entire graph retraining. As a result,\nMG-DVD is equipped with the ability to detect malware variants in real time,\nand it presents better interpretability by introducing meaningful meta-graphs.\nComprehensive experiments on large-scale samples prove that our proposed MG-DVD\noutperforms state-of-the-art methods in detecting malware variants in terms of\neffectiveness and efficiency.",
          "link": "http://arxiv.org/abs/2106.12288",
          "publishedOn": "2021-06-24T01:51:44.363Z",
          "wordCount": 618,
          "title": "MG-DVD: A Real-time Framework for Malware Variant Detection Based on Dynamic Heterogeneous Graph Learning. (arXiv:2106.12288v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">Fanhua Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongying Liu</a>",
          "description": "Federated Learning (FL) has become an active and promising distributed\nmachine learning paradigm. As a result of statistical heterogeneity, recent\nstudies clearly show that the performance of popular FL methods (e.g., FedAvg)\ndeteriorates dramatically due to the client drift caused by local updates. This\npaper proposes a novel Federated Learning algorithm (called IGFL), which\nleverages both Individual and Group behaviors to mimic distribution, thereby\nimproving the ability to deal with heterogeneity. Unlike existing FL methods,\nour IGFL can be applied to both client and server optimization. As a\nby-product, we propose a new attention-based federated learning in the server\noptimization of IGFL. To the best of our knowledge, this is the first time to\nincorporate attention mechanisms into federated optimization. We conduct\nextensive experiments and show that IGFL can significantly improve the\nperformance of existing federated learning methods. Especially when the\ndistributions of data among individuals are diverse, IGFL can improve the\nclassification accuracy by about 13% compared with prior baselines.",
          "link": "http://arxiv.org/abs/2106.12300",
          "publishedOn": "2021-06-24T01:51:44.358Z",
          "wordCount": 632,
          "title": "Behavior Mimics Distribution: Combining Individual and Group Behaviors for Federated Learning. (arXiv:2106.12300v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05748",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1\">Chao Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dulay_J/0/1/0/all/0/1\">Justin Dulay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rolwes_G/0/1/0/all/0/1\">Gregory Rolwes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauli_D/0/1/0/all/0/1\">Duke Pauli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakoor_N/0/1/0/all/0/1\">Nadia Shakoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1\">Abby Stylianou</a>",
          "description": "Automated high throughput plant phenotyping involves leveraging sensors, such\nas RGB, thermal and hyperspectral cameras (among others), to make large scale\nand rapid measurements of the physical properties of plants for the purpose of\nbetter understanding the difference between crops and facilitating rapid plant\nbreeding programs. One of the most basic phenotyping tasks is to determine the\ncultivar, or species, in a particular sensor product. This simple phenotype can\nbe used to detect errors in planting and to learn the most differentiating\nfeatures between cultivars. It is also a challenging visual recognition task,\nas a large number of highly related crops are grown simultaneously, leading to\na classification problem with low inter-class variance. In this paper, we\nintroduce the Sorghum-100 dataset, a large dataset of RGB imagery of sorghum\ncaptured by a state-of-the-art gantry system, a multi-resolution network\narchitecture that learns both global and fine-grained features on the crops,\nand a new global pooling strategy called Dynamic Outlier Pooling which\noutperforms standard global pooling strategies on this task.",
          "link": "http://arxiv.org/abs/2106.05748",
          "publishedOn": "2021-06-24T01:51:44.353Z",
          "wordCount": 630,
          "title": "Multi-resolution Outlier Pooling for Sorghum Classification. (arXiv:2106.05748v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15341",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Kim_S/0/1/0/all/0/1\">Suyong Kim</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ji_W/0/1/0/all/0/1\">Weiqi Ji</a>, <a href=\"http://arxiv.org/find/math/1/au:+Deng_S/0/1/0/all/0/1\">Sili Deng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ma_Y/0/1/0/all/0/1\">Yingbo Ma</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rackauckas_C/0/1/0/all/0/1\">Christopher Rackauckas</a>",
          "description": "Neural Ordinary Differential Equations (ODE) are a promising approach to\nlearn dynamic models from time-series data in science and engineering\napplications. This work aims at learning Neural ODE for stiff systems, which\nare usually raised from chemical kinetic modeling in chemical and biological\nsystems. We first show the challenges of learning neural ODE in the classical\nstiff ODE systems of Robertson's problem and propose techniques to mitigate the\nchallenges associated with scale separations in stiff systems. We then present\nsuccessful demonstrations in stiff systems of Robertson's problem and an air\npollution problem. The demonstrations show that the usage of deep networks with\nrectified activations, proper scaling of the network outputs as well as loss\nfunctions, and stabilized gradient calculations are the key techniques enabling\nthe learning of stiff neural ODE. The success of learning stiff neural ODE\nopens up possibilities of using neural ODEs in applications with widely varying\ntime-scales, like chemical dynamics in energy conversion, environmental\nengineering, and the life sciences.",
          "link": "http://arxiv.org/abs/2103.15341",
          "publishedOn": "2021-06-24T01:51:44.347Z",
          "wordCount": 609,
          "title": "Stiff Neural Ordinary Differential Equations. (arXiv:2103.15341v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aviv_R/0/1/0/all/0/1\">Rotem Zamir Aviv</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hakimi_I/0/1/0/all/0/1\">Ido Hakimi</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_A/0/1/0/all/0/1\">Assaf Schuster</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Levy_K/0/1/0/all/0/1\">Kfir Y. Levy</a> (1 and 3) ((1) Department of Electrical and Computer Engineering, Technion, (2) Department of Computer Science, Technion, (3) A Viterbi Fellow)",
          "description": "We consider stochastic convex optimization problems, where several machines\nact asynchronously in parallel while sharing a common memory. We propose a\nrobust training method for the constrained setting and derive non asymptotic\nconvergence guarantees that do not depend on prior knowledge of update delays,\nobjective smoothness, and gradient variance. Conversely, existing methods for\nthis setting crucially rely on this prior knowledge, which render them\nunsuitable for essentially all shared-resources computational environments,\nsuch as clouds and data centers. Concretely, existing approaches are unable to\naccommodate changes in the delays which result from dynamic allocation of the\nmachines, while our method implicitly adapts to such changes.",
          "link": "http://arxiv.org/abs/2106.12261",
          "publishedOn": "2021-06-24T01:51:44.341Z",
          "wordCount": 569,
          "title": "Learning Under Delayed Feedback: Implicitly Adapting to Gradient Delays. (arXiv:2106.12261v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaocheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chunlin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yinyu Ye</a>",
          "description": "In this paper, we study the bandits with knapsacks (BwK) problem and develop\na primal-dual based algorithm that achieves a problem-dependent logarithmic\nregret bound. The BwK problem extends the multi-arm bandit (MAB) problem to\nmodel the resource consumption associated with playing each arm, and the\nexisting BwK literature has been mainly focused on deriving asymptotically\noptimal distribution-free regret bounds. We first study the primal and dual\nlinear programs underlying the BwK problem. From this primal-dual perspective,\nwe discover symmetry between arms and knapsacks, and then propose a new notion\nof sub-optimality measure for the BwK problem. The sub-optimality measure\nhighlights the important role of knapsacks in determining algorithm regret and\ninspires the design of our two-phase algorithm. In the first phase, the\nalgorithm identifies the optimal arms and the binding knapsacks, and in the\nsecond phase, it exhausts the binding knapsacks via playing the optimal arms\nthrough an adaptive procedure. Our regret upper bound involves the proposed\nsub-optimality measure and it has a logarithmic dependence on length of horizon\n$T$ and a polynomial dependence on $m$ (the numbers of arms) and $d$ (the\nnumber of knapsacks). To the best of our knowledge, this is the first\nproblem-dependent logarithmic regret bound for solving the general BwK problem.",
          "link": "http://arxiv.org/abs/2102.06385",
          "publishedOn": "2021-06-24T01:51:44.326Z",
          "wordCount": 687,
          "title": "The Symmetry between Arms and Knapsacks: A Primal-Dual Approach for Bandits with Knapsacks. (arXiv:2102.06385v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06304",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Maurer_A/0/1/0/all/0/1\">Andreas Maurer</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pontil_M/0/1/0/all/0/1\">Massimiliano Pontil</a>",
          "description": "We prove concentration inequalities for functions of independent random\nvariables {under} sub-gaussian and sub-exponential conditions. The utility of\nthe inequalities is demonstrated by an extension of the now classical method of\nRademacher complexities to Lipschitz function classes and unbounded\nsub-exponential distribution.",
          "link": "http://arxiv.org/abs/2102.06304",
          "publishedOn": "2021-06-24T01:51:44.319Z",
          "wordCount": 506,
          "title": "Some Hoeffding- and Bernstein-type Concentration Inequalities. (arXiv:2102.06304v4 [math.PR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.02912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingzhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1\">Christopher De Sa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1\">Sam Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>",
          "description": "Variational inference (VI) plays an essential role in approximate Bayesian\ninference due to its computational efficiency and broad applicability. Crucial\nto the performance of VI is the selection of the associated divergence measure,\nas VI approximates the intractable distribution by minimizing this divergence.\nIn this paper we propose a meta-learning algorithm to learn the divergence\nmetric suited for the task of interest, automating the design of VI methods. In\naddition, we learn the initialization of the variational parameters without\nadditional cost when our method is deployed in the few-shot learning scenarios.\nWe demonstrate our approach outperforms standard VI on Gaussian mixture\ndistribution approximation, Bayesian neural network regression, image\ngeneration with variational autoencoders and recommender systems with a partial\nvariational autoencoder.",
          "link": "http://arxiv.org/abs/2007.02912",
          "publishedOn": "2021-06-24T01:51:44.314Z",
          "wordCount": 588,
          "title": "Meta-Learning Divergences of Variational Inference. (arXiv:2007.02912v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Galhotra_S/0/1/0/all/0/1\">Sainyam Galhotra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_R/0/1/0/all/0/1\">Romila Pradhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimi_B/0/1/0/all/0/1\">Babak Salimi</a>",
          "description": "There has been a recent resurgence of interest in explainable artificial\nintelligence (XAI) that aims to reduce the opaqueness of AI-based\ndecision-making systems, allowing humans to scrutinize and trust them. Prior\nwork in this context has focused on the attribution of responsibility for an\nalgorithm's decisions to its inputs wherein responsibility is typically\napproached as a purely associational concept. In this paper, we propose a\nprincipled causality-based approach for explaining black-box decision-making\nsystems that addresses limitations of existing methods in XAI. At the core of\nour framework lies probabilistic contrastive counterfactuals, a concept that\ncan be traced back to philosophical, cognitive, and social foundations of\ntheories on how humans generate and select explanations. We show how such\ncounterfactuals can quantify the direct and indirect influences of a variable\non decisions made by an algorithm, and provide actionable recourse for\nindividuals negatively affected by the algorithm's decision. Unlike prior work,\nour system, LEWIS: (1)can compute provably effective explanations and recourse\nat local, global and contextual levels (2)is designed to work with users with\nvarying levels of background knowledge of the underlying causal model and\n(3)makes no assumptions about the internals of an algorithmic system except for\nthe availability of its input-output data. We empirically evaluate LEWIS on\nthree real-world datasets and show that it generates human-understandable\nexplanations that improve upon state-of-the-art approaches in XAI, including\nthe popular LIME and SHAP. Experiments on synthetic data further demonstrate\nthe correctness of LEWIS's explanations and the scalability of its recourse\nalgorithm.",
          "link": "http://arxiv.org/abs/2103.11972",
          "publishedOn": "2021-06-24T01:51:44.308Z",
          "wordCount": 719,
          "title": "Explaining Black-Box Algorithms Using Probabilistic Contrastive Counterfactuals. (arXiv:2103.11972v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trosser_F/0/1/0/all/0/1\">Fulya Tr&#xf6;sser</a> (MIAT INRA), <a href=\"http://arxiv.org/find/cs/1/au:+Givry_S/0/1/0/all/0/1\">Simon de Givry</a> (MIAT INRA), <a href=\"http://arxiv.org/find/cs/1/au:+Katsirelos_G/0/1/0/all/0/1\">George Katsirelos</a> (MIA-Paris)",
          "description": "Bayesian networks are probabilistic graphical models with a wide range of\napplication areas including gene regulatory networks inference, risk analysis\nand image processing. Learning the structure of a Bayesian network (BNSL) from\ndiscrete data is known to be an NP-hard task with a superexponential search\nspace of directed acyclic graphs. In this work, we propose a new polynomial\ntime algorithm for discovering a subset of all possible cluster cuts, a greedy\nalgorithm for approximately solving the resulting linear program, and a\ngeneralised arc consistency algorithm for the acyclicity constraint. We embed\nthese in the constraint programmingbased branch-and-bound solver CPBayes and\nshow that, despite being suboptimal, they improve performance by orders of\nmagnitude. The resulting solver also compares favourably with GOBNILP, a\nstate-of-the-art solver for the BNSL problem which solves an NP-hard problem to\ndiscover each cut and solves the linear program exactly.",
          "link": "http://arxiv.org/abs/2106.12269",
          "publishedOn": "2021-06-24T01:51:44.302Z",
          "wordCount": 604,
          "title": "Improved Acyclicity Reasoning for Bayesian Network Structure Learning with Constraint Programming. (arXiv:2106.12269v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Garcia_M/0/1/0/all/0/1\">Miguel Ruiz-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoenholz_S/0/1/0/all/0/1\">Samuel S. Schoenholz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Andrea J. Liu</a>",
          "description": "We show that learning can be improved by using loss functions that evolve\ncyclically during training to emphasize one class at a time. In\nunderparameterized networks, such dynamical loss functions can lead to\nsuccessful training for networks that fail to find a deep minima of the\nstandard cross-entropy loss. In overparameterized networks, dynamical loss\nfunctions can lead to better generalization. Improvement arises from the\ninterplay of the changing loss landscape with the dynamics of the system as it\nevolves to minimize the loss. In particular, as the loss function oscillates,\ninstabilities develop in the form of bifurcation cascades, which we study using\nthe Hessian and Neural Tangent Kernel. Valleys in the landscape widen and\ndeepen, and then narrow and rise as the loss landscape changes during a cycle.\nAs the landscape narrows, the learning rate becomes too large and the network\nbecomes unstable and bounces around the valley. This process ultimately pushes\nthe system into deeper and wider regions of the loss landscape and is\ncharacterized by decreasing eigenvalues of the Hessian. This results in better\nregularized models with improved generalization performance.",
          "link": "http://arxiv.org/abs/2102.03793",
          "publishedOn": "2021-06-24T01:51:44.286Z",
          "wordCount": 668,
          "title": "Tilting the playing field: Dynamical loss functions for machine learning. (arXiv:2102.03793v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deza_A/0/1/0/all/0/1\">Arturo Deza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konkle_T/0/1/0/all/0/1\">Talia Konkle</a>",
          "description": "The goal of this work is to characterize the representational impact that\nfoveation operations have for machine vision systems, inspired by the foveated\nhuman visual system, which has higher acuity at the center of gaze and\ntexture-like encoding in the periphery. To do so, we introduce models\nconsisting of a first-stage \\textit{fixed} image transform followed by a\nsecond-stage \\textit{learnable} convolutional neural network, and we varied the\nfirst stage component. The primary model has a foveated-textural input stage,\nwhich we compare to a model with foveated-blurred input and a model with\nspatially-uniform blurred input (both matched for perceptual compression), and\na final reference model with minimal input-based compression. We find that: 1)\nthe foveated-texture model shows similar scene classification accuracy as the\nreference model despite its compressed input, with greater i.i.d.\ngeneralization than the other models; 2) the foveated-texture model has greater\nsensitivity to high-spatial frequency information and greater robustness to\nocclusion, w.r.t the comparison models; 3) both the foveated systems, show a\nstronger center image-bias relative to the spatially-uniform systems even with\na weight sharing constraint. Critically, these results are preserved over\ndifferent classical CNN architectures throughout their learning dynamics.\nAltogether, this suggests that foveation with peripheral texture-based\ncomputations yields an efficient, distinct, and robust representational format\nof scene information, and provides symbiotic computational insight into the\nrepresentational consequences that texture-based peripheral encoding may have\nfor processing in the human visual system, while also potentially inspiring the\nnext generation of computer vision models via spatially-adaptive computation.\nCode + Data available here: https://github.com/ArturoDeza/EmergentProperties",
          "link": "http://arxiv.org/abs/2006.07991",
          "publishedOn": "2021-06-24T01:51:44.281Z",
          "wordCount": 752,
          "title": "Emergent Properties of Foveated Perceptual Systems. (arXiv:2006.07991v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Han Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itoh_A/0/1/0/all/0/1\">Asami Itoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakamoto_R/0/1/0/all/0/1\">Ryota Sakamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimaoka_M/0/1/0/all/0/1\">Motomu Shimaoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sano_A/0/1/0/all/0/1\">Akane Sano</a>",
          "description": "Shift workers who are essential contributors to our society, face high risks\nof poor health and wellbeing. To help with their problems, we collected and\nanalyzed physiological and behavioral wearable sensor data from shift working\nnurses and doctors, as well as their behavioral questionnaire data and their\nself-reported daily health and wellbeing labels, including alertness,\nhappiness, energy, health, and stress. We found the similarities and\ndifferences between the responses of nurses and doctors. According to the\ndifferences in self-reported health and wellbeing labels between nurses and\ndoctors, and the correlations among their labels, we proposed a job-role based\nmultitask and multilabel deep learning model, where we modeled physiological\nand behavioral data for nurses and doctors simultaneously to predict\nparticipants' next day's multidimensional self-reported health and wellbeing\nstatus. Our model showed significantly better performances than baseline models\nand previous state-of-the-art models in the evaluations of binary/3-class\nclassification and regression prediction tasks. We also found features related\nto heart rate, sleep, and work shift contributed to shift workers' health and\nwellbeing.",
          "link": "http://arxiv.org/abs/2106.12081",
          "publishedOn": "2021-06-24T01:51:44.275Z",
          "wordCount": 622,
          "title": "Forecasting Health and Wellbeing for Shift Workers Using Job-role Based Deep Neural Network. (arXiv:2106.12081v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haddad_M/0/1/0/all/0/1\">Maroun Haddad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouguessa_M/0/1/0/all/0/1\">Mohamed Bouguessa</a>",
          "description": "While representation learning has yielded a great success on many graph\nlearning tasks, there is little understanding behind the structures that are\nbeing captured by these embeddings. For example, we wonder if the topological\nfeatures, such as the Triangle Count, the Degree of the node and other\ncentrality measures are concretely encoded in the embeddings. Furthermore, we\nask if the presence of these structures in the embeddings is necessary for a\nbetter performance on the downstream tasks, such as clustering and\nclassification. To address these questions, we conduct an extensive empirical\nstudy over three classes of unsupervised graph embedding models and seven\ndifferent variants of Graph Autoencoders. Our results show that five\ntopological features: the Degree, the Local Clustering Score, the Betweenness\nCentrality, the Eigenvector Centrality, and Triangle Count are concretely\npreserved in the first layer of the graph autoencoder that employs the SUM\naggregation rule, under the condition that the model preserves the second-order\nproximity. We supplement further evidence for the presence of these features by\nrevealing a hierarchy in the distribution of the topological features in the\nembeddings of the aforementioned model. We also show that a model with such\nproperties can outperform other models on certain downstream tasks, especially\nwhen the preserved features are relevant to the task at hand. Finally, we\nevaluate the suitability of our findings through a test case study related to\nsocial influence prediction.",
          "link": "http://arxiv.org/abs/2106.12005",
          "publishedOn": "2021-06-24T01:51:44.270Z",
          "wordCount": 660,
          "title": "Exploring the Representational Power of Graph Autoencoder. (arXiv:2106.12005v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>",
          "description": "Transfer learning has become a common solution to address training data\nscarcity in practice. It trains a specified student model by reusing or\nfine-tuning early layers of a well-trained teacher model that is usually\npublicly available. However, besides utility improvement, the transferred\npublic knowledge also brings potential threats to model confidentiality, and\neven further raises other security and privacy issues.\n\nIn this paper, we present the first comprehensive investigation of the\nteacher model exposure threat in the transfer learning context, aiming to gain\na deeper insight into the tension between public knowledge and model\nconfidentiality. To this end, we propose a teacher model fingerprinting attack\nto infer the origin of a student model, i.e., the teacher model it transfers\nfrom. Specifically, we propose a novel optimization-based method to carefully\ngenerate queries to probe the student model to realize our attack. Unlike\nexisting model reverse engineering approaches, our proposed fingerprinting\nmethod neither relies on fine-grained model outputs, e.g., posteriors, nor\nauxiliary information of the model architecture or training dataset. We\nsystematically evaluate the effectiveness of our proposed attack. The empirical\nresults demonstrate that our attack can accurately identify the model origin\nwith few probing queries. Moreover, we show that the proposed attack can serve\nas a stepping stone to facilitating other attacks against machine learning\nmodels, such as model stealing.",
          "link": "http://arxiv.org/abs/2106.12478",
          "publishedOn": "2021-06-24T01:51:44.264Z",
          "wordCount": 652,
          "title": "Teacher Model Fingerprinting Attacks Against Transfer Learning. (arXiv:2106.12478v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12511",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Duffy_G/0/1/0/all/0/1\">Grant Duffy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1\">Paul P Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_N/0/1/0/all/0/1\">Neal Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_B/0/1/0/all/0/1\">Bryan He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kwan_A/0/1/0/all/0/1\">Alan C. Kwan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shun_Shin_M/0/1/0/all/0/1\">Matthew J. Shun-Shin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alexander_K/0/1/0/all/0/1\">Kevin M. Alexander</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebinger_J/0/1/0/all/0/1\">Joseph Ebinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rader_F/0/1/0/all/0/1\">Florian Rader</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">David H. Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schnittger_I/0/1/0/all/0/1\">Ingela Schnittger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ashley_E/0/1/0/all/0/1\">Euan A. Ashley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zou_J/0/1/0/all/0/1\">James Y. Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_J/0/1/0/all/0/1\">Jignesh Patel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Witteles_R/0/1/0/all/0/1\">Ronald Witteles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_S/0/1/0/all/0/1\">Susan Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ouyang_D/0/1/0/all/0/1\">David Ouyang</a>",
          "description": "Left ventricular hypertrophy (LVH) results from chronic remodeling caused by\na broad range of systemic and cardiovascular disease including hypertension,\naortic stenosis, hypertrophic cardiomyopathy, and cardiac amyloidosis. Early\ndetection and characterization of LVH can significantly impact patient care but\nis limited by under-recognition of hypertrophy, measurement error and\nvariability, and difficulty differentiating etiologies of LVH. To overcome this\nchallenge, we present EchoNet-LVH - a deep learning workflow that automatically\nquantifies ventricular hypertrophy with precision equal to human experts and\npredicts etiology of LVH. Trained on 28,201 echocardiogram videos, our model\naccurately measures intraventricular wall thickness (mean absolute error [MAE]\n1.4mm, 95% CI 1.2-1.5mm), left ventricular diameter (MAE 2.4mm, 95% CI\n2.2-2.6mm), and posterior wall thickness (MAE 1.2mm, 95% CI 1.1-1.3mm) and\nclassifies cardiac amyloidosis (area under the curve of 0.83) and hypertrophic\ncardiomyopathy (AUC 0.98) from other etiologies of LVH. In external datasets\nfrom independent domestic and international healthcare systems, EchoNet-LVH\naccurately quantified ventricular parameters (R2 of 0.96 and 0.90 respectively)\nand detected cardiac amyloidosis (AUC 0.79) and hypertrophic cardiomyopathy\n(AUC 0.89) on the domestic external validation site. Leveraging measurements\nacross multiple heart beats, our model can more accurately identify subtle\nchanges in LV geometry and its causal etiologies. Compared to human experts,\nEchoNet-LVH is fully automated, allowing for reproducible, precise\nmeasurements, and lays the foundation for precision diagnosis of cardiac\nhypertrophy. As a resource to promote further innovation, we also make publicly\navailable a large dataset of 23,212 annotated echocardiogram videos.",
          "link": "http://arxiv.org/abs/2106.12511",
          "publishedOn": "2021-06-24T01:51:44.249Z",
          "wordCount": 731,
          "title": "High-Throughput Precision Phenotyping of Left Ventricular Hypertrophy with Cardiovascular Deep Learning. (arXiv:2106.12511v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.11005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_K/0/1/0/all/0/1\">Kaichao You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>",
          "description": "This paper studies task adaptive pre-trained model selection, an\nunderexplored problem of assessing pre-trained models for the target task and\nselect best ones from the model zoo \\emph{without fine-tuning}. A few pilot\nworks addressed the problem in transferring supervised pre-trained models to\nclassification tasks, but they cannot handle emerging unsupervised pre-trained\nmodels or regression tasks. In pursuit of a practical assessment method, we\npropose to estimate the maximum value of label evidence given features\nextracted by pre-trained models. Unlike the maximum likelihood, the maximum\nevidence is \\emph{immune to over-fitting}, while its expensive computation can\nbe dramatically reduced by our carefully designed algorithm. The Logarithm of\nMaximum Evidence (LogME) can be used to assess pre-trained models for transfer\nlearning: a pre-trained model with a high LogME value is likely to have good\ntransfer performance. LogME is \\emph{fast, accurate, and general},\ncharacterizing itself as the first practical method for assessing pre-trained\nmodels. Compared with brute-force fine-tuning, LogME brings at most\n$3000\\times$ speedup in wall-clock time and requires only $1\\%$ memory\nfootprint. It outperforms prior methods by a large margin in their setting and\nis applicable to new settings. It is general enough for diverse pre-trained\nmodels (supervised pre-trained and unsupervised pre-trained), downstream tasks\n(classification and regression), and modalities (vision and language). Code is\navailable at this repository:\n\\href{https://github.com/thuml/LogME}{https://github.com/thuml/LogME}.",
          "link": "http://arxiv.org/abs/2102.11005",
          "publishedOn": "2021-06-24T01:51:44.244Z",
          "wordCount": 701,
          "title": "LogME: Practical Assessment of Pre-trained Models for Transfer Learning. (arXiv:2102.11005v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Sewoong Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_C/0/1/0/all/0/1\">Chang-Han Rhee</a>",
          "description": "The empirical success of deep learning is often attributed to SGD's\nmysterious ability to avoid sharp local minima in the loss landscape, as sharp\nminima are known to lead to poor generalization. Recently, empirical evidence\nof heavy-tailed gradient noise was reported in many deep learning tasks, and it\nwas shown in \\c{S}im\\c{s}ekli (2019a,b) that SGD can escape sharp local minima\nunder the presence of such heavy-tailed gradient noise, providing a partial\nsolution to the mystery. In this work, we analyze a popular variant of SGD\nwhere gradients are truncated above a fixed threshold. We show that it achieves\na stronger notion of avoiding sharp minima: it can effectively eliminate sharp\nlocal minima entirely from its training trajectory. We characterize the\ndynamics of truncated SGD driven by heavy-tailed noises. First, we show that\nthe truncation threshold and width of the attraction field dictate the order of\nthe first exit time from the associated local minimum. Moreover, when the\nobjective function satisfies appropriate structural conditions, we prove that\nas the learning rate decreases, the dynamics of heavy-tailed truncated SGD\nclosely resemble those of a continuous-time Markov chain that never visits any\nsharp minima. Real data experiments on deep learning confirm our theoretical\nprediction that heavy-tailed SGD with gradient clipping finds a \"flatter\" local\nminima and achieves better generalization.",
          "link": "http://arxiv.org/abs/2102.04297",
          "publishedOn": "2021-06-24T01:51:44.238Z",
          "wordCount": 713,
          "title": "Eliminating Sharp Minima from SGD with Truncated Heavy-tailed Noise. (arXiv:2102.04297v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Widdicombe_A/0/1/0/all/0/1\">Amy Widdicombe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Julier_S/0/1/0/all/0/1\">Simon J. Julier</a>",
          "description": "Binarized Neural Networks (BNNs) have the potential to revolutionize the way\nthat deep learning is carried out in edge computing platforms. However, the\neffectiveness of interpretability methods on these networks has not been\nassessed.\n\nIn this paper, we compare the performance of several widely used saliency\nmap-based interpretabilty techniques (Gradient, SmoothGrad and GradCAM), when\napplied to Binarized or Full Precision Neural Networks (FPNNs). We found that\nthe basic Gradient method produces very similar-looking maps for both types of\nnetwork. However, SmoothGrad produces significantly noisier maps for BNNs.\nGradCAM also produces saliency maps which differ between network types, with\nsome of the BNNs having seemingly nonsensical explanations. We comment on\npossible reasons for these differences in explanations and present it as an\nexample of why interpretability techniques should be tested on a wider range of\nnetwork types.",
          "link": "http://arxiv.org/abs/2106.12569",
          "publishedOn": "2021-06-24T01:51:44.232Z",
          "wordCount": 592,
          "title": "Gradient-Based Interpretability Methods and Binarized Neural Networks. (arXiv:2106.12569v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.05437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hashemizadeh_M/0/1/0/all/0/1\">Meraj Hashemizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Michelle Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1\">Jacob Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1\">Guillaume Rabusseau</a>",
          "description": "Tensor Networks (TN) offer a powerful framework to efficiently represent very\nhigh-dimensional objects. TN have recently shown their potential for machine\nlearning applications and offer a unifying view of common tensor decomposition\nmodels such as Tucker, tensor train (TT) and tensor ring (TR). However,\nidentifying the best tensor network structure from data for a given task is\nchallenging. In this work, we leverage the TN formalism to develop a generic\nand efficient adaptive algorithm to jointly learn the structure and the\nparameters of a TN from data. Our method is based on a simple greedy approach\nstarting from a rank one tensor and successively identifying the most promising\ntensor network edges for small rank increments. Our algorithm can adaptively\nidentify TN structures with small number of parameters that effectively\noptimize any differentiable objective function. Experiments on tensor\ndecomposition, tensor completion and model compression tasks demonstrate the\neffectiveness of the proposed algorithm. In particular, our method outperforms\nthe state-of-the-art evolutionary topology search [Li and Sun, 2020] for tensor\ndecomposition of images (while being orders of magnitude faster) and finds\nefficient tensor network structures to compress neural networks outperforming\npopular TT based approaches [Novikov et al., 2015].",
          "link": "http://arxiv.org/abs/2008.05437",
          "publishedOn": "2021-06-24T01:51:44.226Z",
          "wordCount": 653,
          "title": "Adaptive Learning of Tensor Network Structures. (arXiv:2008.05437v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deja_K/0/1/0/all/0/1\">Kamil Deja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wawrzynski_P/0/1/0/all/0/1\">Pawe&#x142; Wawrzy&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marczak_D/0/1/0/all/0/1\">Daniel Marczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masarczyk_W/0/1/0/all/0/1\">Wojciech Masarczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>",
          "description": "We propose a new method for unsupervised continual knowledge consolidation in\ngenerative models that relies on the partitioning of Variational Autoencoder's\nlatent space. Acquiring knowledge about new data samples without forgetting\nprevious ones is a critical problem of continual learning. Currently proposed\nmethods achieve this goal by extending the existing model while constraining\nits behavior not to degrade on the past data, which does not exploit the full\npotential of relations within the entire training dataset. In this work, we\nidentify this limitation and posit the goal of continual learning as a\nknowledge accumulation task. We solve it by continuously re-aligning latent\nspace partitions that we call bands which are representations of samples seen\nin different tasks, driven by the similarity of the information they contain.\nIn addition, we introduce a simple yet effective method for controlled\nforgetting of past data that improves the quality of reconstructions encoded in\nlatent bands and a latent space disentanglement technique that improves\nknowledge consolidation. On top of the standard continual learning evaluation\nbenchmarks, we evaluate our method on a new knowledge consolidation scenario\nand show that the proposed approach outperforms state-of-the-art by up to\ntwofold across all testing scenarios.",
          "link": "http://arxiv.org/abs/2106.12196",
          "publishedOn": "2021-06-24T01:51:44.211Z",
          "wordCount": 634,
          "title": "Multiband VAE: Latent Space Partitioning for Knowledge Consolidation in Continual Learning. (arXiv:2106.12196v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prasse_P/0/1/0/all/0/1\">Paul Prasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brabec_J/0/1/0/all/0/1\">Jan Brabec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohout_J/0/1/0/all/0/1\">Jan Kohout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopp_M/0/1/0/all/0/1\">Martin Kopp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajer_L/0/1/0/all/0/1\">Lukas Bajer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheffer_T/0/1/0/all/0/1\">Tobias Scheffer</a>",
          "description": "We address the problems of identifying malware in network telemetry logs and\nproviding \\emph{indicators of compromise} -- comprehensible explanations of\nbehavioral patterns that identify the threat. In our system, an array of\nspecialized detectors abstracts network-flow data into comprehensible\n\\emph{network events} in a first step. We develop a neural network that\nprocesses this sequence of events and identifies specific threats, malware\nfamilies and broad categories of malware. We then use the\n\\emph{integrated-gradients} method to highlight events that jointly constitute\nthe characteristic behavioral pattern of the threat. We compare network\narchitectures based on CNNs, LSTMs, and transformers, and explore the efficacy\nof unsupervised pre-training experimentally on large-scale telemetry data. We\ndemonstrate how this system detects njRAT and other malware based on behavioral\npatterns.",
          "link": "http://arxiv.org/abs/2106.12328",
          "publishedOn": "2021-06-24T01:51:44.205Z",
          "wordCount": 572,
          "title": "Learning Explainable Representations of Malware Behavior. (arXiv:2106.12328v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Richter_M/0/1/0/all/0/1\">Mats L. Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoning_J/0/1/0/all/0/1\">Julius Sch&#xf6;ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krumnack_U/0/1/0/all/0/1\">Ulf Krumnack</a>",
          "description": "Applying artificial neural networks (ANN) to specific tasks, researchers,\nprogrammers, and other specialists usually overshot the number of convolutional\nlayers in their designs. By implication, these ANNs hold too many parameters,\nwhich needed unnecessarily trained without impacting the result. The features,\na convolutional layer can process, are strictly limited by its receptive field.\nBy layer-wise analyzing the expansion of the receptive fields, we can reliably\npredict sequences of layers that will not contribute qualitatively to the\ninference in thegiven ANN architecture. Based on these analyses, we propose\ndesign strategies to resolve these inefficiencies, optimizing the\nexplainability and the computational performance of ANNs. Since neither the\nstrategies nor the analysis requires training of the actual model, these\ninsights allow for a very efficient design process of ANNs architectures which\nmight be automated in the future.",
          "link": "http://arxiv.org/abs/2106.12307",
          "publishedOn": "2021-06-24T01:51:44.186Z",
          "wordCount": 593,
          "title": "Should You Go Deeper? Optimizing Convolutional Neural Network Architectures without Training by Receptive Field Analysis. (arXiv:2106.12307v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farias_T/0/1/0/all/0/1\">Tiago de Souza Farias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maziero_J/0/1/0/all/0/1\">Jonas Maziero</a>",
          "description": "We introduce feature alignment, a technique for obtaining approximate\nreversibility in artificial neural networks. By means of feature extraction, we\ncan train a neural network to learn an estimated map for its reverse process\nfrom outputs to inputs. Combined with variational autoencoders, we can generate\nnew samples from the same statistics as the training data. Improvements of the\nresults are obtained by using concepts from generative adversarial networks.\nFinally, we show that the technique can be modified for training neural\nnetworks locally, saving computational memory resources. Applying these\ntechniques, we report results for three vision generative tasks: MNIST,\nCIFAR-10, and celebA.",
          "link": "http://arxiv.org/abs/2106.12562",
          "publishedOn": "2021-06-24T01:51:44.178Z",
          "wordCount": 540,
          "title": "Feature Alignment for Approximated Reversibility in Neural Networks. (arXiv:2106.12562v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.03802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Riley_P/0/1/0/all/0/1\">Parker Riley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mandy Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Girish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uthus_D/0/1/0/all/0/1\">David Uthus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_Z/0/1/0/all/0/1\">Zarana Parekh</a>",
          "description": "We present a novel approach to the problem of text style transfer. Unlike\nprevious approaches requiring style-labeled training data, our method makes use\nof readily-available unlabeled text by relying on the implicit connection in\nstyle between adjacent sentences, and uses labeled data only at inference time.\nWe adapt T5 (Raffel et al., 2020), a strong pretrained text-to-text model, to\nextract a style vector from text and use it to condition the decoder to perform\nstyle transfer. As our label-free training results in a style vector space\nencoding many facets of style, we recast transfers as \"targeted restyling\"\nvector operations that adjust specific attributes of the input while preserving\nothers. We demonstrate that training on unlabeled Amazon reviews data results\nin a model that is competitive on sentiment transfer, even compared to models\ntrained fully on labeled data. Furthermore, applying our novel method to a\ndiverse corpus of unlabeled web text results in a single model capable of\ntransferring along multiple dimensions of style (dialect, emotiveness,\nformality, politeness, sentiment) despite no additional training and using only\na handful of exemplars at inference time.",
          "link": "http://arxiv.org/abs/2010.03802",
          "publishedOn": "2021-06-24T01:51:44.123Z",
          "wordCount": 662,
          "title": "TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling. (arXiv:2010.03802v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12506",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Shi_J/0/1/0/all/0/1\">Jiaxin Shi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>",
          "description": "We introduce a new family of particle evolution samplers suitable for\nconstrained domains and non-Euclidean geometries. Stein Variational Mirror\nDescent and Mirrored Stein Variational Gradient Descent minimize the\nKullback-Leibler (KL) divergence to constrained target distributions by\nevolving particles in a dual space defined by a mirror map. Stein Variational\nNatural Gradient exploits non-Euclidean geometry to more efficiently minimize\nthe KL divergence to unconstrained targets. We derive these samplers from a new\nclass of mirrored Stein operators and adaptive kernels developed in this work.\nWe demonstrate that these new samplers yield accurate approximations to\ndistributions on the simplex, deliver valid confidence intervals in\npost-selection inference, and converge more rapidly than prior methods in\nlarge-scale unconstrained posterior inference. Finally, we establish the\nconvergence of our new procedures under verifiable conditions on the target\ndistribution.",
          "link": "http://arxiv.org/abs/2106.12506",
          "publishedOn": "2021-06-24T01:51:44.116Z",
          "wordCount": 561,
          "title": "Sampling with Mirrored Stein Operators. (arXiv:2106.12506v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drichel_A/0/1/0/all/0/1\">Arthur Drichel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drury_V/0/1/0/all/0/1\">Vincent Drury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandt_J/0/1/0/all/0/1\">Justus von Brandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_U/0/1/0/all/0/1\">Ulrike Meyer</a>",
          "description": "Current popular phishing prevention techniques mainly utilize reactive\nblocklists, which leave a ``window of opportunity'' for attackers during which\nvictims are unprotected. One possible approach to shorten this window aims to\ndetect phishing attacks earlier, during website preparation, by monitoring\nCertificate Transparency (CT) logs. Previous attempts to work with CT log data\nfor phishing classification exist, however they lack evaluations on actual CT\nlog data. In this paper, we present a pipeline that facilitates such\nevaluations by addressing a number of problems when working with CT log data.\nThe pipeline includes dataset creation, training, and past or live\nclassification of CT logs. Its modular structure makes it possible to easily\nexchange classifiers or verification sources to support ground truth labeling\nefforts and classifier comparisons. We test the pipeline on a number of new and\nexisting classifiers, and find a general potential to improve classifiers for\nthis scenario in the future. We publish the source code of the pipeline and the\nused datasets along with this paper\n(https://gitlab.com/rwth-itsec/ctl-pipeline), thus making future research in\nthis direction more accessible.",
          "link": "http://arxiv.org/abs/2106.12343",
          "publishedOn": "2021-06-24T01:51:44.111Z",
          "wordCount": 657,
          "title": "Finding Phish in a Haystack: A Pipeline for Phishing Classification on Certificate Transparency Logs. (arXiv:2106.12343v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khodaverdian_Z/0/1/0/all/0/1\">Zeinab Khodaverdian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadr_H/0/1/0/all/0/1\">Hossein Sadr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edalatpanah_S/0/1/0/all/0/1\">Seyed Ahmad Edalatpanah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solimandarabi_M/0/1/0/all/0/1\">Mojdeh Nazari Solimandarabi</a>",
          "description": "Cloud computing service models have experienced rapid growth and inefficient\nresource usage is known as one of the greatest causes of high energy\nconsumption in cloud data centers. Resource allocation in cloud data centers\naiming to reduce energy consumption has been conducted using live migration of\nVirtual Machines (VMs) and their consolidation into the small number of\nPhysical Machines (PMs). However, the selection of the appropriate VM for\nmigration is an important challenge. To solve this issue, VMs can be classified\naccording to the pattern of user requests into sensitive or insensitive classes\nto latency, and thereafter suitable VMs can be selected for migration. In this\npaper, the combination of Convolution Neural Network (CNN) and Gated Recurrent\nUnit (GRU) is utilized for the classification of VMs in the Microsoft Azure\ndataset. Due to the fact the majority of VMs in this dataset are labeled as\ninsensitive to latency, migration of more VMs in this group not only reduces\nenergy consumption but also decreases the violation of Service Level Agreements\n(SLA). Based on the empirical results, the proposed model obtained an accuracy\nof 95.18which clearly demonstrates the superiority of our proposed model\ncompared to other existing models.",
          "link": "http://arxiv.org/abs/2106.12178",
          "publishedOn": "2021-06-24T01:51:44.106Z",
          "wordCount": 655,
          "title": "Combination of Convolutional Neural Network and Gated Recurrent Unit for Energy Aware Resource Allocation. (arXiv:2106.12178v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1\">Mikhail Galkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiapeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denis_E/0/1/0/all/0/1\">Etienne Denis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1\">William L. Hamilton</a>",
          "description": "Conventional representation learning algorithms for knowledge graphs (KG) map\neach entity to a unique embedding vector. Such a shallow lookup results in a\nlinear growth of memory consumption for storing the embedding matrix and incurs\nhigh computational costs when working with real-world KGs. Drawing parallels\nwith subword tokenization commonly used in NLP, we explore the landscape of\nmore parameter-efficient node embedding strategies with possibly sublinear\nmemory requirements. To this end, we propose NodePiece, an anchor-based\napproach to learn a fixed-size entity vocabulary. In NodePiece, a vocabulary of\nsubword/sub-entity units is constructed from anchor nodes in a graph with known\nrelation types. Given such a fixed-size vocabulary, it is possible to bootstrap\nan encoding and embedding for any entity, including those unseen during\ntraining. Experiments show that NodePiece performs competitively in node\nclassification, link prediction, and relation prediction tasks while retaining\nless than 10% of explicit nodes in a graph as anchors and often having 10x\nfewer parameters.",
          "link": "http://arxiv.org/abs/2106.12144",
          "publishedOn": "2021-06-24T01:51:44.101Z",
          "wordCount": 600,
          "title": "NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs. (arXiv:2106.12144v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goel_G/0/1/0/all/0/1\">Gautam Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassibi_B/0/1/0/all/0/1\">Babak Hassibi</a>",
          "description": "We consider estimation and control in linear time-varying dynamical systems\nfrom the perspective of regret minimization. Unlike most prior work in this\narea, we focus on the problem of designing causal estimators and controllers\nwhich compete against a clairvoyant noncausal policy, instead of the best\npolicy selected in hindsight from some fixed parametric class. We show that the\nregret-optimal estimator and regret-optimal controller can be derived in\nstate-space form using operator-theoretic techniques from robust control and\npresent tight,data-dependent bounds on the regret incurred by our algorithms in\nterms of the energy of the disturbances. Our results can be viewed as extending\ntraditional robust estimation and control, which focuses on minimizing\nworst-case cost, to minimizing worst-case regret. We propose regret-optimal\nanalogs of Model-Predictive Control (MPC) and the Extended KalmanFilter (EKF)\nfor systems with nonlinear dynamics and present numerical experiments which\nshow that our regret-optimal algorithms can significantly outperform standard\napproaches to estimation and control.",
          "link": "http://arxiv.org/abs/2106.12097",
          "publishedOn": "2021-06-24T01:51:44.085Z",
          "wordCount": 581,
          "title": "Regret-optimal Estimation and Control. (arXiv:2106.12097v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2003.07849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1\">Takuhiro Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>",
          "description": "Generative adversarial networks (GANs) have gained considerable attention\nowing to their ability to reproduce images. However, they can recreate training\nimages faithfully despite image degradation in the form of blur, noise, and\ncompression, generating similarly degraded images. To solve this problem, the\nrecently proposed noise robust GAN (NR-GAN) provides a partial solution by\ndemonstrating the ability to learn a clean image generator directly from noisy\nimages using a two-generator model comprising image and noise generators.\nHowever, its application is limited to noise, which is relatively easy to\ndecompose owing to its additive and reversible characteristics, and its\napplication to irreversible image degradation, in the form of blur,\ncompression, and combination of all, remains a challenge. To address these\nproblems, we propose blur, noise, and compression robust GAN (BNCR-GAN) that\ncan learn a clean image generator directly from degraded images without\nknowledge of degradation parameters (e.g., blur kernel types, noise amounts, or\nquality factor values). Inspired by NR-GAN, BNCR-GAN uses a multiple-generator\nmodel composed of image, blur-kernel, noise, and quality-factor generators.\nHowever, in contrast to NR-GAN, to address irreversible characteristics, we\nintroduce masking architectures adjusting degradation strength values in a\ndata-driven manner using bypasses before and after degradation. Furthermore, to\nsuppress uncertainty caused by the combination of blur, noise, and compression,\nwe introduce adaptive consistency losses imposing consistency between\nirreversible degradation processes according to the degradation strengths. We\ndemonstrate the effectiveness of BNCR-GAN through large-scale comparative\nstudies on CIFAR-10 and a generality analysis on FFHQ. In addition, we\ndemonstrate the applicability of BNCR-GAN in image restoration.",
          "link": "http://arxiv.org/abs/2003.07849",
          "publishedOn": "2021-06-24T01:51:44.080Z",
          "wordCount": 740,
          "title": "Blur, Noise, and Compression Robust Generative Adversarial Networks. (arXiv:2003.07849v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12534",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wada_K/0/1/0/all/0/1\">Kentaro Wada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laidlow_T/0/1/0/all/0/1\">Tristan Laidlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>",
          "description": "Reflecting on the last few years, the biggest breakthroughs in deep\nreinforcement learning (RL) have been in the discrete action domain. Robotic\nmanipulation, however, is inherently a continuous control environment, but\nthese continuous control reinforcement learning algorithms often depend on\nactor-critic methods that are sample-inefficient and inherently difficult to\ntrain, due to the joint optimisation of the actor and critic. To that end, we\nexplore how we can bring the stability of discrete action RL algorithms to the\nrobot manipulation domain. We extend the recently released ARM algorithm, by\nreplacing the continuous next-best pose agent with a discrete next-best pose\nagent. Discretisation of rotation is trivial given its bounded nature, while\ntranslation is inherently unbounded, making discretisation difficult. We\nformulate the translation prediction as the voxel prediction problem by\ndiscretising the 3D space; however, voxelisation of a large workspace is memory\nintensive and would not work with a high density of voxels, crucial to\nobtaining the resolution needed for robotic manipulation. We therefore propose\nto apply this voxel prediction in a coarse-to-fine manner by gradually\nincreasing the resolution. In each step, we extract the highest valued voxel as\nthe predicted location, which is then used as the centre of the\nhigher-resolution voxelisation in the next step. This coarse-to-fine prediction\nis applied over several steps, giving a near-lossless prediction of the\ntranslation. We show that our new coarse-to-fine algorithm is able to\naccomplish RLBench tasks much more efficiently than the continuous control\nequivalent, and even train some real-world tasks, tabular rasa, in less than 7\nminutes, with only 3 demonstrations. Moreover, we show that by moving to a\nvoxel representation, we are able to easily incorporate observations from\nmultiple cameras.",
          "link": "http://arxiv.org/abs/2106.12534",
          "publishedOn": "2021-06-24T01:51:44.075Z",
          "wordCount": 736,
          "title": "Coarse-to-Fine Q-attention: Efficient Learning for Visual Robotic Manipulation via Discretisation. (arXiv:2106.12534v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Boyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1\">Sunny Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianlong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor Tsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fang Chen</a>",
          "description": "Imitation learning aims to extract knowledge from human experts'\ndemonstrations or artificially created agents in order to replicate their\nbehaviors. Its success has been demonstrated in areas such as video games,\nautonomous driving, robotic simulations and object manipulation. However, this\nreplicating process could be problematic, such as the performance is highly\ndependent on the demonstration quality, and most trained agents are limited to\nperform well in task-specific environments. In this survey, we provide a\nsystematic review on imitation learning. We first introduce the background\nknowledge from development history and preliminaries, followed by presenting\ndifferent taxonomies within Imitation Learning and key milestones of the field.\nWe then detail challenges in learning strategies and present research\nopportunities with learning policy from suboptimal demonstration, voice\ninstructions and other associated optimization schemes.",
          "link": "http://arxiv.org/abs/2106.12177",
          "publishedOn": "2021-06-24T01:51:44.069Z",
          "wordCount": 557,
          "title": "Imitation Learning: Progress, Taxonomies and Opportunities. (arXiv:2106.12177v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Killian_J/0/1/0/all/0/1\">Jackson A. Killian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_A/0/1/0/all/0/1\">Arpita Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Sanket Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1\">Milind Tambe</a>",
          "description": "Multi-action restless multi-armed bandits (RMABs) are a powerful framework\nfor constrained resource allocation in which $N$ independent processes are\nmanaged. However, previous work only study the offline setting where problem\ndynamics are known. We address this restrictive assumption, designing the first\nalgorithms for learning good policies for Multi-action RMABs online using\ncombinations of Lagrangian relaxation and Q-learning. Our first approach,\nMAIQL, extends a method for Q-learning the Whittle index in binary-action RMABs\nto the multi-action setting. We derive a generalized update rule and\nconvergence proof and establish that, under standard assumptions, MAIQL\nconverges to the asymptotically optimal multi-action RMAB policy as\n$t\\rightarrow{}\\infty$. However, MAIQL relies on learning Q-functions and\nindexes on two timescales which leads to slow convergence and requires problem\nstructure to perform well. Thus, we design a second algorithm, LPQL, which\nlearns the well-performing and more general Lagrange policy for multi-action\nRMABs by learning to minimize the Lagrange bound through a variant of\nQ-learning. To ensure fast convergence, we take an approximation strategy that\nenables learning on a single timescale, then give a guarantee relating the\napproximation's precision to an upper bound of LPQL's return as\n$t\\rightarrow{}\\infty$. Finally, we show that our approaches always outperform\nbaselines across multiple settings, including one derived from real-world\nmedication adherence data.",
          "link": "http://arxiv.org/abs/2106.12024",
          "publishedOn": "2021-06-24T01:51:44.053Z",
          "wordCount": 665,
          "title": "Q-Learning Lagrange Policies for Multi-Action Restless Bandits. (arXiv:2106.12024v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.07805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiankang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "The transition matrix, denoting the transition relationship from clean labels\nto noisy labels, is essential to build statistically consistent classifiers in\nlabel-noise learning. Existing methods for estimating the transition matrix\nrely heavily on estimating the noisy class posterior. However, the estimation\nerror for noisy class posterior could be large due to the randomness of label\nnoise, which would lead the transition matrix to be poorly estimated.\nTherefore, in this paper, we aim to solve this problem by exploiting the\ndivide-and-conquer paradigm. Specifically, we introduce an intermediate class\nto avoid directly estimating the noisy class posterior. By this intermediate\nclass, the original transition matrix can then be factorized into the product\nof two easy-to-estimate transition matrices. We term the proposed method the\ndual-T estimator. Both theoretical analyses and empirical results illustrate\nthe effectiveness of the dual-T estimator for estimating transition matrices,\nleading to better classification performances.",
          "link": "http://arxiv.org/abs/2006.07805",
          "publishedOn": "2021-06-24T01:51:44.048Z",
          "wordCount": 632,
          "title": "Dual T: Reducing Estimation Error for Transition Matrix in Label-noise Learning. (arXiv:2006.07805v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lin Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">Fanhua Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongying Liu</a>",
          "description": "Recently, the study on learned iterative shrinkage thresholding algorithm\n(LISTA) has attracted increasing attentions. A large number of experiments as\nwell as some theories have proved the high efficiency of LISTA for solving\nsparse coding problems. However, existing LISTA methods are all serial\nconnection. To address this issue, we propose a novel extragradient based LISTA\n(ELISTA), which has a residual structure and theoretical guarantees. In\nparticular, our algorithm can also provide the interpretability for Res-Net to\na certain extent. From a theoretical perspective, we prove that our method\nattains linear convergence. In practice, extensive empirical results verify the\nadvantages of our method.",
          "link": "http://arxiv.org/abs/2106.11970",
          "publishedOn": "2021-06-24T01:51:44.043Z",
          "wordCount": 561,
          "title": "Learned Interpretable Residual Extragradient ISTA for Sparse Coding. (arXiv:2106.11970v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hagmann_M/0/1/0/all/0/1\">Michael Hagmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>",
          "description": "Machine learning algorithms train models from patterns of input data and\ntarget outputs, with the goal of predicting correct outputs for unseen test\ninputs. Here we demonstrate a problem of machine learning in vital application\nareas such as medical informatics or patent law that consists of the inclusion\nof measurements on which target outputs are deterministically defined in the\nrepresentations of input data. This leads to perfect, but circular predictions\nbased on a machine reconstruction of the known target definition, but fails on\nreal-world data where the defining measurements may not or only incompletely be\navailable. We present a circularity test that shows, for given datasets and\nblack-box machine learning models, whether the target functional definition can\nbe reconstructed and has been used in training. We argue that a transfer of\nresearch results to real-world applications requires to avoid circularity by\nseparating measurements that define target outcomes from data representations\nin machine learning.",
          "link": "http://arxiv.org/abs/2106.12417",
          "publishedOn": "2021-06-24T01:51:44.037Z",
          "wordCount": 593,
          "title": "False perfection in machine prediction: Detecting and assessing circularity problems in machine learning. (arXiv:2106.12417v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1\">Weituo Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spell_G/0/1/0/all/0/1\">Gregory Spell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>",
          "description": "The outbreak of COVID-19 Disease due to the novel coronavirus has caused a\nshortage of medical resources. To aid and accelerate the diagnosis process,\nautomatic diagnosis of COVID-19 via deep learning models has recently been\nexplored by researchers across the world. While different data-driven deep\nlearning models have been developed to mitigate the diagnosis of COVID-19, the\ndata itself is still scarce due to patient privacy concerns. Federated Learning\n(FL) is a natural solution because it allows different organizations to\ncooperatively learn an effective deep learning model without sharing raw data.\nHowever, recent studies show that FL still lacks privacy protection and may\ncause data leakage. We investigate this challenging problem by proposing a\nsimple yet effective algorithm, named \\textbf{F}ederated \\textbf{L}earning\n\\textbf{o}n Medical Datasets using \\textbf{P}artial Networks (FLOP), that\nshares only a partial model between the server and clients. Extensive\nexperiments on benchmark data and real-world healthcare tasks show that our\napproach achieves comparable or better performance while reducing the privacy\nand security risks. Of particular interest, we conduct experiments on the\nCOVID-19 dataset and find that our FLOP algorithm can allow different hospitals\nto collaboratively and effectively train a partially shared model without\nsharing local patients' data.",
          "link": "http://arxiv.org/abs/2102.05218",
          "publishedOn": "2021-06-24T01:51:44.032Z",
          "wordCount": 722,
          "title": "FLOP: Federated Learning on Medical Datasets using Partial Networks. (arXiv:2102.05218v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peste_A/0/1/0/all/0/1\">Alexandra Peste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iofinova_E/0/1/0/all/0/1\">Eugenia Iofinova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vladu_A/0/1/0/all/0/1\">Adrian Vladu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>",
          "description": "The increasing computational requirements of deep neural networks (DNNs) have\nled to significant interest in obtaining DNN models that are sparse, yet\naccurate. Recent work has investigated the even harder case of sparse training,\nwhere the DNN weights are, for as much as possible, already sparse to reduce\ncomputational costs during training.\n\nExisting sparse training methods are mainly empirical and often have lower\naccuracy relative to the dense baseline. In this paper, we present a general\napproach called Alternating Compressed/DeCompressed (AC/DC) training of DNNs,\ndemonstrate convergence for a variant of the algorithm, and show that AC/DC\noutperforms existing sparse training methods in accuracy at similar\ncomputational budgets; at high sparsity levels, AC/DC even outperforms existing\nmethods that rely on accurate pre-trained dense models. An important property\nof AC/DC is that it allows co-training of dense and sparse models, yielding\naccurate sparse-dense model pairs at the end of the training process. This is\nuseful in practice, where compressed variants may be desirable for deployment\nin resource-constrained settings without re-doing the entire training flow, and\nalso provides us with insights into the accuracy gap between dense and\ncompressed models.",
          "link": "http://arxiv.org/abs/2106.12379",
          "publishedOn": "2021-06-24T01:51:44.026Z",
          "wordCount": 621,
          "title": "AC/DC: Alternating Compressed/DeCompressed Training of Deep Neural Networks. (arXiv:2106.12379v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chzhen_E/0/1/0/all/0/1\">Evgenii Chzhen</a> (LMO, CELESTE), <a href=\"http://arxiv.org/find/cs/1/au:+Giraud_C/0/1/0/all/0/1\">Christophe Giraud</a> (LMO, CELESTE), <a href=\"http://arxiv.org/find/cs/1/au:+Stoltz_G/0/1/0/all/0/1\">Gilles Stoltz</a> (LMO, CELESTE)",
          "description": "We provide a setting and a general approach to fair online learning with\nstochastic sensitive and non-sensitive contexts. The setting is a repeated game\nbetween the Player and Nature, where at each stage both pick actions based on\nthe contexts. Inspired by the notion of unawareness, we assume that the Player\ncan only access the non-sensitive context before making a decision, while we\ndiscuss both cases of Nature accessing the sensitive contexts and Nature\nunaware of the sensitive contexts. Adapting Blackwell's approachability theory\nto handle the case of an unknown contexts' distribution, we provide a general\nnecessary and sufficient condition for learning objectives to be compatible\nwith some fairness constraints. This condition is instantiated on (group-wise)\nno-regret and (group-wise) calibration objectives, and on demographic parity as\nan additional constraint. When the objective is not compatible with the\nconstraint, the provided framework permits to characterise the optimal\ntrade-off between the two.",
          "link": "http://arxiv.org/abs/2106.12242",
          "publishedOn": "2021-06-24T01:51:44.010Z",
          "wordCount": 607,
          "title": "A Unified Approach to Fair Online Learning via Blackwell Approachability. (arXiv:2106.12242v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09056",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Padmanabha_G/0/1/0/all/0/1\">Govinda Anantha Padmanabha</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zabaras_N/0/1/0/all/0/1\">Nicholas Zabaras</a>",
          "description": "Fine-scale simulation of complex systems governed by multiscale partial\ndifferential equations (PDEs) is computationally expensive and various\nmultiscale methods have been developed for addressing such problems. In\naddition, it is challenging to develop accurate surrogate and uncertainty\nquantification models for high-dimensional problems governed by stochastic\nmultiscale PDEs using limited training data. In this work to address these\nchallenges, we introduce a novel hybrid deep-learning and multiscale approach\nfor stochastic multiscale PDEs with limited training data. For demonstration\npurposes, we focus on a porous media flow problem. We use an image-to-image\nsupervised deep learning model to learn the mapping between the input\npermeability field and the multiscale basis functions. We introduce a Bayesian\napproach to this hybrid framework to allow us to perform uncertainty\nquantification and propagation tasks. The performance of this hybrid approach\nis evaluated with varying intrinsic dimensionality of the permeability field.\nNumerical results indicate that the hybrid network can efficiently predict well\nfor high-dimensional inputs.",
          "link": "http://arxiv.org/abs/2103.09056",
          "publishedOn": "2021-06-24T01:51:44.005Z",
          "wordCount": 629,
          "title": "A Bayesian Multiscale Deep Learning Framework for Flows in Random Media. (arXiv:2103.09056v2 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.15207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yingyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>",
          "description": "Detecting out-of-distribution (OOD) inputs is critical for safely deploying\ndeep learning models in an open-world setting. However, existing OOD detection\nsolutions can be brittle in the open world, facing various types of adversarial\nOOD inputs. While methods leveraging auxiliary OOD data have emerged, our\nanalysis on illuminative examples reveals a key insight that the majority of\nauxiliary OOD examples may not meaningfully improve or even hurt the decision\nboundary of the OOD detector, which is also observed in empirical results on\nreal data. In this paper, we provide a theoretically motivated method,\nAdversarial Training with informative Outlier Mining (ATOM), which improves the\nrobustness of OOD detection. We show that, by mining informative auxiliary OOD\ndata, one can significantly improve OOD detection performance, and somewhat\nsurprisingly, generalize to unseen adversarial attacks. ATOM achieves\nstate-of-the-art performance under a broad family of classic and adversarial\nOOD evaluation tasks. For example, on the CIFAR-10 in-distribution dataset,\nATOM reduces the FPR (at TPR 95%) by up to 57.99% under adversarial OOD inputs,\nsurpassing the previous best baseline by a large margin.",
          "link": "http://arxiv.org/abs/2006.15207",
          "publishedOn": "2021-06-24T01:51:43.999Z",
          "wordCount": 660,
          "title": "ATOM: Robustifying Out-of-distribution Detection Using Outlier Mining. (arXiv:2006.15207v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12532",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Harilal_N/0/1/0/all/0/1\">Nidhin Harilal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_U/0/1/0/all/0/1\">Udit Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1\">Auroop R. Ganguly</a>",
          "description": "Advances in neural architecture search, as well as explainability and\ninterpretability of connectionist architectures, have been reported in the\nrecent literature. However, our understanding of how to design Bayesian Deep\nLearning (BDL) hyperparameters, specifically, the depth, width and ensemble\nsize, for robust function mapping with uncertainty quantification, is still\nemerging. This paper attempts to further our understanding by mapping Bayesian\nconnectionist representations to polynomials of different orders with varying\nnoise types and ratios. We examine the noise-contaminated polynomials to search\nfor the combination of hyperparameters that can extract the underlying\npolynomial signals while quantifying uncertainties based on the noise\nattributes. Specifically, we attempt to study the question that an appropriate\nneural architecture and ensemble configuration can be found to detect a signal\nof any n-th order polynomial contaminated with noise having different\ndistributions and signal-to-noise (SNR) ratios and varying noise attributes.\nOur results suggest the possible existence of an optimal network depth as well\nas an optimal number of ensembles for prediction skills and uncertainty\nquantification, respectively. However, optimality is not discernible for width,\neven though the performance gain reduces with increasing width at high values\nof width. Our experiments and insights can be directional to understand\ntheoretical properties of BDL representations and to design practical\nsolutions.",
          "link": "http://arxiv.org/abs/2106.12532",
          "publishedOn": "2021-06-24T01:51:43.993Z",
          "wordCount": 650,
          "title": "Bayesian Deep Learning Hyperparameter Search for Robust Function Mapping to Polynomials with Noise. (arXiv:2106.12532v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "One of the distinct features of this century has been the population of older\nadults which has been on a constant rise. Elderly people have several needs and\nrequirements due to physical disabilities, cognitive issues, weakened memory\nand disorganized behavior, that they face with increasing age. The extent of\nthese limitations also differs according to the varying diversities in elderly,\nwhich include age, gender, background, experience, skills, knowledge and so on.\nThese varying needs and challenges with increasing age, limits abilities of\nolder adults to perform Activities of Daily Living (ADLs) in an independent\nmanner. To add to it, the shortage of caregivers creates a looming need for\ntechnology-based services for elderly people, to assist them in performing\ntheir daily routine tasks to sustain their independent living and active aging.\nTo address these needs, this work consists of making three major contributions\nin this field. First, it provides a rather comprehensive review of assisted\nliving technologies aimed at helping elderly people to perform ADLs. Second,\nthe work discusses the challenges identified through this review, that\ncurrently exist in the context of implementation of assisted living services\nfor elderly care in Smart Homes and Smart Cities. Finally, the work also\noutlines an approach for implementation, extension and integration of the\nexisting works in this field for development of a much-needed framework that\ncan provide personalized assistance and user-centered behavior interventions to\nelderly as per their varying and ever-changing needs.",
          "link": "http://arxiv.org/abs/2106.12183",
          "publishedOn": "2021-06-24T01:51:43.979Z",
          "wordCount": 707,
          "title": "A Review of Assistive Technologies for Activities of Daily Living of Elderly. (arXiv:2106.12183v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14152",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qiujia Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1\">Liangliang Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>",
          "description": "End-to-end models with auto-regressive decoders have shown impressive results\nfor automatic speech recognition (ASR). These models formulate the\nsequence-level probability as a product of the conditional probabilities of all\nindividual tokens given their histories. However, the performance of locally\nnormalised models can be sub-optimal because of factors such as exposure bias.\nConsequently, the model distribution differs from the underlying data\ndistribution. In this paper, the residual energy-based model (R-EBM) is\nproposed to complement the auto-regressive ASR model to close the gap between\nthe two distributions. Meanwhile, R-EBMs can also be regarded as\nutterance-level confidence estimators, which may benefit many downstream tasks.\nExperiments on a 100hr LibriSpeech dataset show that R-EBMs can reduce the word\nerror rates (WERs) by 8.2%/6.7% while improving areas under precision-recall\ncurves of confidence scores by 12.6%/28.4% on test-clean/test-other sets.\nFurthermore, on a state-of-the-art model using self-supervised learning\n(wav2vec 2.0), R-EBMs still significantly improves both the WER and confidence\nestimation performance.",
          "link": "http://arxiv.org/abs/2103.14152",
          "publishedOn": "2021-06-24T01:51:43.973Z",
          "wordCount": 629,
          "title": "Residual Energy-Based Models for End-to-End Speech Recognition. (arXiv:2103.14152v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_D/0/1/0/all/0/1\">Deeparnab Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negahbani_M/0/1/0/all/0/1\">Maryam Negahbani</a>",
          "description": "We study data clustering problems with $\\ell_p$-norm objectives (e.g.\n$k$-Median and $k$-Means) in the context of individual fairness. The dataset\nconsists of $n$ points, and we want to find $k$ centers such that (a) the\nobjective is minimized, while (b) respecting the individual fairness constraint\nthat every point $v$ has a center within a distance at most $r(v)$, where\n$r(v)$ is $v$'s distance to its $(n/k)$th nearest point. Jung, Kannan, and Lutz\n[FORC 2020] introduced this concept and designed a clustering algorithm with\nprovable (approximate) fairness and objective guarantees for the $\\ell_\\infty$\nor $k$-Center objective. Mahabadi and Vakilian [ICML 2020] revisited this\nproblem to give a local-search algorithm for all $\\ell_p$-norms. Empirically,\ntheir algorithms outperform Jung et. al.'s by a large margin in terms of cost\n(for $k$-Median and $k$-Means), but they incur a reasonable loss in fairness.\nIn this paper, our main contribution is to use Linear Programming (LP)\ntechniques to obtain better algorithms for this problem, both in theory and in\npractice. We prove that by modifying known LP rounding techniques, one gets a\nworst-case guarantee on the objective which is much better than in MV20, and\nempirically, this objective is extremely close to the optimal. Furthermore, our\ntheoretical fairness guarantees are comparable with MV20 in theory, and\nempirically, we obtain noticeably fairer solutions. Although solving the LP\n{\\em exactly} might be prohibitive, we demonstrate that in practice, a simple\nsparsification technique drastically improves the run-time of our algorithm.",
          "link": "http://arxiv.org/abs/2106.12150",
          "publishedOn": "2021-06-24T01:51:43.969Z",
          "wordCount": 670,
          "title": "Better Algorithms for Individually Fair $k$-Clustering. (arXiv:2106.12150v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2006.09461",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jalal_A/0/1/0/all/0/1\">Ajil Jalal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1\">Liu Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Caramanis_C/0/1/0/all/0/1\">Constantine Caramanis</a>",
          "description": "The goal of compressed sensing is to estimate a high dimensional vector from\nan underdetermined system of noisy linear equations. In analogy to classical\ncompressed sensing, here we assume a generative model as a prior, that is, we\nassume the vector is represented by a deep generative model $G: \\mathbb{R}^k\n\\rightarrow \\mathbb{R}^n$. Classical recovery approaches such as empirical risk\nminimization (ERM) are guaranteed to succeed when the measurement matrix is\nsub-Gaussian. However, when the measurement matrix and measurements are\nheavy-tailed or have outliers, recovery may fail dramatically. In this paper we\npropose an algorithm inspired by the Median-of-Means (MOM). Our algorithm\nguarantees recovery for heavy-tailed data, even in the presence of outliers.\nTheoretically, our results show our novel MOM-based algorithm enjoys the same\nsample complexity guarantees as ERM under sub-Gaussian assumptions. Our\nexperiments validate both aspects of our claims: other algorithms are indeed\nfragile and fail under heavy-tailed and/or corrupted data, while our approach\nexhibits the predicted robustness.",
          "link": "http://arxiv.org/abs/2006.09461",
          "publishedOn": "2021-06-24T01:51:43.962Z",
          "wordCount": 621,
          "title": "Robust Compressed Sensing using Generative Models. (arXiv:2006.09461v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_G/0/1/0/all/0/1\">Gang Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>",
          "description": "Being able to efficiently and accurately select the top-$k$ elements with\ndifferential privacy is an integral component of various private data analysis\ntasks. In this paper, we present the oneshot Laplace mechanism, which\ngeneralizes the well-known Report Noisy Max mechanism to reporting noisy\ntop-$k$ elements. We show that the oneshot Laplace mechanism with a noise level\nof $\\widetilde{O}(\\sqrt{k}/\\eps)$ is approximately differentially private.\nCompared to the previous peeling approach of running Report Noisy Max $k$\ntimes, the oneshot Laplace mechanism only adds noises and computes the top $k$\nelements once, hence much more efficient for large $k$. In addition, our proof\nof privacy relies on a novel coupling technique that bypasses the use of\ncomposition theorems. Finally, we present a novel application of efficient\ntop-$k$ selection in the classical problem of ranking from pairwise\ncomparisons.",
          "link": "http://arxiv.org/abs/2105.08233",
          "publishedOn": "2021-06-24T01:51:43.957Z",
          "wordCount": 601,
          "title": "Oneshot Differentially Private Top-k Selection. (arXiv:2105.08233v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tayal_K/0/1/0/all/0/1\">Kshitij Tayal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manekar_R/0/1/0/all/0/1\">Raunak Manekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Zhong Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">David Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vipin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_F/0/1/0/all/0/1\">Felix Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Ju Sun</a>",
          "description": "Several deep learning methods for phase retrieval exist, but most of them\nfail on realistic data without precise support information. We propose a novel\nmethod based on single-instance deep generative prior that works well on\ncomplex-valued crystal data.",
          "link": "http://arxiv.org/abs/2106.04812",
          "publishedOn": "2021-06-24T01:51:43.951Z",
          "wordCount": 495,
          "title": "Phase Retrieval using Single-Instance Deep Generative Prior. (arXiv:2106.04812v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02526",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Garcia_A/0/1/0/all/0/1\">Alex Hernandez-Garcia</a>",
          "description": "The renaissance of artificial neural networks was catalysed by the success of\nclassification models, tagged by the community with the broader term supervised\nlearning. The extraordinary results gave rise to a hype loaded with ambitious\npromises and overstatements. Soon the community realised that the success owed\nmuch to the availability of thousands of labelled examples and supervised\nlearning went, for many, from glory to shame: Some criticised deep learning as\na whole and others proclaimed that the way forward had to be alternatives to\nsupervised learning: predictive, unsupervised, semi-supervised and, more\nrecently, self-supervised learning. However, all these seem brand names, rather\nthan actual categories of a theoretically grounded taxonomy. Moreover, the call\nto banish supervised learning was motivated by the questionable claim that\nhumans learn with little or no supervision and are capable of robust\nout-of-distribution generalisation. Here, we review insights about learning and\nsupervision in nature, revisit the notion that learning and generalisation are\nnot possible without supervision or inductive biases and argue that we will\nmake better progress if we just call it by its name.",
          "link": "http://arxiv.org/abs/2012.02526",
          "publishedOn": "2021-06-24T01:51:43.937Z",
          "wordCount": 666,
          "title": "Rethinking supervised learning: insights from biological learning and from calling it by its name. (arXiv:2012.02526v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boukhers_Z/0/1/0/all/0/1\">Zeyd Boukhers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayr_P/0/1/0/all/0/1\">Philipp Mayr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peroni_S/0/1/0/all/0/1\">Silvio Peroni</a>",
          "description": "Automatic processing of bibliographic data becomes very important in digital\nlibraries, data science and machine learning due to its importance in keeping\npace with the significant increase of published papers every year from one side\nand to the inherent challenges from the other side. This processing has several\naspects including but not limited to I) Automatic extraction of references from\nPDF documents, II) Building an accurate citation graph, III) Author name\ndisambiguation, etc. Bibliographic data is heterogeneous by nature and occurs\nin both structured (e.g. citation graph) and unstructured (e.g. publications)\nformats. Therefore, it requires data science and machine learning techniques to\nbe processed and analysed. Here we introduce BiblioDAP'21: The 1st Workshop on\nBibliographic Data Analysis and Processing.",
          "link": "http://arxiv.org/abs/2106.12320",
          "publishedOn": "2021-06-24T01:51:43.932Z",
          "wordCount": 572,
          "title": "BiblioDAP: The 1st Workshop on Bibliographic Data Analysis and Processing. (arXiv:2106.12320v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.08414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borrageiro_G/0/1/0/all/0/1\">Gabriel Borrageiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firoozye_N/0/1/0/all/0/1\">Nick Firoozye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barucca_P/0/1/0/all/0/1\">Paolo Barucca</a>",
          "description": "We investigate the benefits of feature selection, nonlinear modelling and\nonline learning when forecasting in financial time series. We consider the\nsequential and continual learning sub-genres of online learning. The\nexperiments we conduct show that there is a benefit to online transfer\nlearning, in the form of radial basis function networks, beyond the sequential\nupdating of recursive least-squares models. We show that the radial basis\nfunction networks, which make use of clustering algorithms to construct a\nkernel Gram matrix, are more beneficial than treating each training vector as\nseparate basis functions, as occurs with kernel Ridge regression. We\ndemonstrate quantitative procedures to determine the very structure of the\nradial basis function networks. Finally, we conduct experiments on the log\nreturns of financial time series and show that the online learning models,\nparticularly the radial basis function networks, are able to outperform a\nrandom walk baseline, whereas the offline learning models struggle to do so.",
          "link": "http://arxiv.org/abs/2103.08414",
          "publishedOn": "2021-06-24T01:51:43.926Z",
          "wordCount": 624,
          "title": "Online Learning with Radial Basis Function Networks. (arXiv:2103.08414v2 [cs.CE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1\">Ajil Jalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmalkar_S/0/1/0/all/0/1\">Sushrut Karmalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">Jessica Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1\">Eric Price</a>",
          "description": "This work tackles the issue of fairness in the context of generative\nprocedures, such as image super-resolution, which entail different definitions\nfrom the standard classification setting. Moreover, while traditional group\nfairness definitions are typically defined with respect to specified protected\ngroups -- camouflaging the fact that these groupings are artificial and carry\nhistorical and political motivations -- we emphasize that there are no ground\ntruth identities. For instance, should South and East Asians be viewed as a\nsingle group or separate groups? Should we consider one race as a whole or\nfurther split by gender? Choosing which groups are valid and who belongs in\nthem is an impossible dilemma and being ``fair'' with respect to Asians may\nrequire being ``unfair'' with respect to South Asians. This motivates the\nintroduction of definitions that allow algorithms to be \\emph{oblivious} to the\nrelevant groupings.\n\nWe define several intuitive notions of group fairness and study their\nincompatibilities and trade-offs. We show that the natural extension of\ndemographic parity is strongly dependent on the grouping, and \\emph{impossible}\nto achieve obliviously. On the other hand, the conceptually new definition we\nintroduce, Conditional Proportional Representation, can be achieved obliviously\nthrough Posterior Sampling. Our experiments validate our theoretical results\nand achieve fair image reconstruction using state-of-the-art generative models.",
          "link": "http://arxiv.org/abs/2106.12182",
          "publishedOn": "2021-06-24T01:51:43.921Z",
          "wordCount": 653,
          "title": "Fairness for Image Generation with Uncertain Sensitive Attributes. (arXiv:2106.12182v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.05961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shetty_M/0/1/0/all/0/1\">Manish Shetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_C/0/1/0/all/0/1\">Chetan Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sumit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_N/0/1/0/all/0/1\">Nikitha Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagappan_N/0/1/0/all/0/1\">Nachiappan Nagappan</a>",
          "description": "The move from boxed products to services and the widespread adoption of cloud\ncomputing has had a huge impact on the software development life cycle and\nDevOps processes. Particularly, incident management has become critical for\ndeveloping and operating large-scale services. Prior work on incident\nmanagement has heavily focused on the challenges with incident triaging and\nde-duplication. In this work, we address the fundamental problem of structured\nknowledge extraction from service incidents. We have built SoftNER, a framework\nfor mining Knowledge Graphs from incident reports. First, we build a novel\nmulti-task learning based BiLSTM-CRF model which leverages not just the\nsemantic context but also the data-types for extracting factual information in\nthe form of named entities. Next, we present an approach to mine relations\nbetween the named entities for automatically constructing knowledge graphs. We\nhave deployed SoftNER at Microsoft, a major cloud service provider and have\nevaluated it on more than 2 months of cloud incidents. We show that the\nunsupervised machine learning pipeline has a high precision of 0.96. Our\nmulti-task learning based deep learning model also outperforms the\nstate-of-the-art NER models. Lastly, using the knowledge extracted by SoftNER,\nwe are able to build accurate models for applications such as incident triaging\nand recommending entities based on their relevance to incident titles.",
          "link": "http://arxiv.org/abs/2101.05961",
          "publishedOn": "2021-06-24T01:51:43.905Z",
          "wordCount": 691,
          "title": "SoftNER: Mining Knowledge Graphs From Cloud Incidents. (arXiv:2101.05961v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Werling_K/0/1/0/all/0/1\">Keenon Werling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omens_D/0/1/0/all/0/1\">Dalton Omens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jeongseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Exarchos_I/0/1/0/all/0/1\">Ioannis Exarchos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>",
          "description": "We present a fast and feature-complete differentiable physics engine, Nimble\n(nimblephysics.org), that supports Lagrangian dynamics and hard contact\nconstraints for articulated rigid body simulation. Our differentiable physics\nengine offers a complete set of features that are typically only available in\nnon-differentiable physics simulators commonly used by robotics applications.\nWe solve contact constraints precisely using linear complementarity problems\n(LCPs). We present efficient and novel analytical gradients through the LCP\nformulation of inelastic contact that exploit the sparsity of the LCP solution.\nWe support complex contact geometry, and gradients approximating\ncontinuous-time elastic collision. We also introduce a novel method to compute\ncomplementarity-aware gradients that help downstream optimization tasks avoid\nstalling in saddle points. We show that an implementation of this combination\nin an existing physics engine (DART) is capable of a 87x single-core speedup\nover finite-differencing in computing analytical Jacobians for a single\ntimestep, while preserving all the expressiveness of original DART.",
          "link": "http://arxiv.org/abs/2103.16021",
          "publishedOn": "2021-06-24T01:51:43.900Z",
          "wordCount": 643,
          "title": "Fast and Feature-Complete Differentiable Physics for Articulated Rigid Bodies with Contact. (arXiv:2103.16021v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08174",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Viviano_D/0/1/0/all/0/1\">Davide Viviano</a>",
          "description": "This paper discusses experimental design to estimate welfare-maximizing\npolicies. We consider a setting where units are organized into large, finitely\nmany independent clusters and interact over unobserved dimensions within each\ncluster. The contribution of this paper is two-fold. First, we construct a test\nfor whether a welfare-improving treatment configuration exists and hence worth\nlearning by conducting a larger scale experiment. Second, we introduce an\nadaptive randomization procedure to estimate welfare-maximizing individual\ntreatment allocation rules valid under unobserved interference. We derive\nasymptotic properties of the marginal effects estimators and finite-sample\nregret guarantees of the policy. Finally, we illustrate the method's advantage\nin simulations calibrated to an existing experiment on information diffusion.",
          "link": "http://arxiv.org/abs/2011.08174",
          "publishedOn": "2021-06-24T01:51:43.894Z",
          "wordCount": 574,
          "title": "Policy choice in experiments with unknown interference. (arXiv:2011.08174v4 [econ.EM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05163",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mestav_K/0/1/0/all/0/1\">Kursat Rasim Mestav</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tong_L/0/1/0/all/0/1\">Lang Tong</a>",
          "description": "A deep learning approach is proposed to detect data and system anomalies\nusing high-resolution continuous point-on-wave (CPOW) or phasor measurements.\nBoth the anomaly and anomaly-free measurement models are assumed to have\nunknown temporal dependencies and probability distributions. Historical\ntraining samples are assumed for the anomaly-free model, while no training\nsamples are available for the anomaly measurements. By transforming the\nanomaly-free observations into uniform independent and identically distributed\nsequences via a generative adversarial network, the proposed approach deploys a\nuniformity test for anomaly detection at the sensor level. A distributed\ndetection scheme that combines sensor level detections at the control center is\nalso proposed that combines local detections to form more reliable detections.\nNumerical results demonstrate significant improvement over the state-of-the-art\nsolutions for various bad-data cases using real and synthetic CPOW and PMU data\nsets.",
          "link": "http://arxiv.org/abs/2012.05163",
          "publishedOn": "2021-06-24T01:51:43.814Z",
          "wordCount": 613,
          "title": "A Deep Learning Approach to Anomaly Sequence Detection for High-Resolution Monitoring of Power Systems. (arXiv:2012.05163v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandagale_S/0/1/0/all/0/1\">Sujay Khandagale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1\">Colin White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1\">Willie Neiswanger</a>",
          "description": "As machine learning models grow more complex and their applications become\nmore high-stakes, tools for explaining model predictions have become\nincreasingly important. Despite the widespread use of explainability\ntechniques, evaluating and comparing different feature attribution methods\nremains challenging: evaluations ideally require human studies, and empirical\nevaluation metrics are often computationally prohibitive on real-world\ndatasets. In this work, we address this issue by releasing XAI-Bench: a suite\nof synthetic datasets along with a library for benchmarking feature attribution\nalgorithms. Unlike real-world datasets, synthetic datasets allow the efficient\ncomputation of conditional expected values that are needed to evaluate\nground-truth Shapley values and other metrics. The synthetic datasets we\nrelease offer a wide variety of parameters that can be configured to simulate\nreal-world data. We demonstrate the power of our library by benchmarking\npopular explainability techniques across several evaluation metrics and\nidentifying failure modes for popular explainers. The efficiency of our library\nwill help bring new explainability methods from development to deployment.",
          "link": "http://arxiv.org/abs/2106.12543",
          "publishedOn": "2021-06-24T01:51:43.808Z",
          "wordCount": 600,
          "title": "Synthetic Benchmarks for Scientific Research in Explainable Machine Learning. (arXiv:2106.12543v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1\">Nawshad Farruque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1\">Randy Goebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivapalan_S/0/1/0/all/0/1\">Sudhakar Sivapalan</a>",
          "description": "We focus on exploring various approaches of Zero-Shot Learning (ZSL) and\ntheir explainability for a challenging yet important supervised learning task\nnotorious for training data scarcity, i.e. Depression Symptoms Detection (DSD)\nfrom text. We start with a comprehensive synthesis of different components of\nour ZSL modeling and analysis of our ground truth samples and Depression\nsymptom clues curation process with the help of a practicing clinician. We next\nanalyze the accuracy of various state-of-the-art ZSL models and their potential\nenhancements for our task. Further, we sketch a framework for the use of ZSL\nfor hierarchical text-based explanation mechanism, which we call, Syntax\nTree-Guided Semantic Explanation (STEP). Finally, we summarize experiments from\nwhich we conclude that we can use ZSL models and achieve reasonable accuracy\nand explainability, measured by a proposed Explainability Index (EI). This work\nis, to our knowledge, the first work to exhaustively explore the efficacy of\nZSL models for DSD task, both in terms of accuracy and explainability.",
          "link": "http://arxiv.org/abs/2106.10928",
          "publishedOn": "2021-06-24T01:51:43.802Z",
          "wordCount": 640,
          "title": "STEP-EZ: Syntax Tree guided semantic ExPlanation for Explainable Zero-shot modeling of clinical depression symptoms from text. (arXiv:2106.10928v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yunfeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mingming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>",
          "description": "Recently, visual Transformer (ViT) and its following works abandon the\nconvolution and exploit the self-attention operation, attaining a comparable or\neven higher accuracy than CNNs. More recently, MLP-Mixer abandons both the\nconvolution and the self-attention operation, proposing an architecture\ncontaining only MLP layers. To achieve cross-patch communications, it devises\nan additional token-mixing MLP besides the channel-mixing MLP. It achieves\npromising results when training on an extremely large-scale dataset. But it\ncannot achieve as outstanding performance as its CNN and ViT counterparts when\ntraining on medium-scale datasets such as ImageNet1K and ImageNet21K. The\nperformance drop of MLP-Mixer motivates us to rethink the token-mixing MLP. We\ndiscover that the token-mixing MLP is a variant of the depthwise convolution\nwith a global reception field and spatial-specific configuration. But the\nglobal reception field and the spatial-specific property make token-mixing MLP\nprone to over-fitting. In this paper, we propose a novel pure MLP architecture,\nspatial-shift MLP (S$^2$-MLP). Different from MLP-Mixer, our S$^2$-MLP only\ncontains channel-mixing MLP. We utilize a spatial-shift operation for\ncommunications between patches. It has a local reception field and is\nspatial-agnostic. It is parameter-free and efficient for computation. The\nproposed S$^2$-MLP attains higher recognition accuracy than MLP-Mixer when\ntraining on ImageNet-1K dataset. Meanwhile, S$^2$-MLP accomplishes as excellent\nperformance as ViT on ImageNet-1K dataset with considerably simpler\narchitecture and fewer FLOPs and parameters.",
          "link": "http://arxiv.org/abs/2106.07477",
          "publishedOn": "2021-06-24T01:51:43.786Z",
          "wordCount": 678,
          "title": "S$^2$-MLP: Spatial-Shift MLP Architecture for Vision. (arXiv:2106.07477v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1\">Navid Kardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Ankit Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1\">Kenneth O. Stanley</a>",
          "description": "Deep neural networks are behind many of the recent successes in machine\nlearning applications. However, these models can produce overconfident\ndecisions while encountering out-of-distribution (OOD) examples or making a\nwrong prediction. This inconsistent predictive confidence limits the\nintegration of independently-trained learning models into a larger system. This\npaper introduces separable concept learning framework to realistically measure\nthe performance of classifiers in presence of OOD examples. In this setup,\nseveral instances of a classifier are trained on different parts of a partition\nof the set of classes. Later, the performance of the combination of these\nmodels is evaluated on a separate test set. Unlike current OOD detection\ntechniques, this framework does not require auxiliary OOD datasets and does not\nseparate classification from detection performance. Furthermore, we present a\nnew strong baseline for more consistent predictive confidence in deep models,\ncalled fitted ensembles, where overconfident predictions are rectified by\ntransformed versions of the original classification task. Fitted ensembles can\nnaturally detect OOD examples without requiring auxiliary data by observing\ncontradicting predictions among its components. Experiments on MNIST, SVHN,\nCIFAR-10/100, and ImageNet show fitted ensemble significantly outperform\nconventional ensembles on OOD examples and are possible to scale.",
          "link": "http://arxiv.org/abs/2106.12070",
          "publishedOn": "2021-06-24T01:51:43.781Z",
          "wordCount": 635,
          "title": "Towards Consistent Predictive Confidence through Fitted Ensembles. (arXiv:2106.12070v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joy_T/0/1/0/all/0/1\">Tom Joy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuge Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1\">Tom Rainforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmon_S/0/1/0/all/0/1\">Sebastian M. Schmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>",
          "description": "Multimodal VAEs seek to model the joint distribution over heterogeneous data\n(e.g.\\ vision, language), whilst also capturing a shared representation across\nsuch modalities. Prior work has typically combined information from the\nmodalities by reconciling idiosyncratic representations directly in the\nrecognition model through explicit products, mixtures, or other such\nfactorisations. Here we introduce a novel alternative, the MEME, that avoids\nsuch explicit combinations by repurposing semi-supervised VAEs to combine\ninformation between modalities implicitly through mutual supervision. This\nformulation naturally allows learning from partially-observed data where some\nmodalities can be entirely missing -- something that most existing approaches\neither cannot handle, or do so to a limited extent. We demonstrate that MEME\noutperforms baselines on standard metrics across both partial and complete\nobservation schemes on the MNIST-SVHN (image--image) and CUB (image--text)\ndatasets. We also contrast the quality of the representations learnt by mutual\nsupervision against standard approaches and observe interesting trends in its\nability to capture relatedness between data.",
          "link": "http://arxiv.org/abs/2106.12570",
          "publishedOn": "2021-06-24T01:51:43.776Z",
          "wordCount": 589,
          "title": "Learning Multimodal VAEs through Mutual Supervision. (arXiv:2106.12570v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12147",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hwang_H/0/1/0/all/0/1\">Hyung Ju Hwang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Son_H/0/1/0/all/0/1\">Hwijae Son</a>",
          "description": "In this paper, we propose a novel conservative formulation for solving\nkinetic equations via neural networks. More precisely, we formulate the\nlearning problem as a constrained optimization problem with constraints that\nrepresent the physical conservation laws. The constraints are relaxed toward\nthe residual loss function by the Lagrangian duality. By imposing physical\nconservation properties of the solution as constraints of the learning problem,\nwe demonstrate far more accurate approximations of the solutions in terms of\nerrors and the conservation laws, for the kinetic Fokker-Planck equation and\nthe homogeneous Boltzmann equation.",
          "link": "http://arxiv.org/abs/2106.12147",
          "publishedOn": "2021-06-24T01:51:43.769Z",
          "wordCount": 528,
          "title": "Lagrangian dual framework for conservative neural network solutions of kinetic equations. (arXiv:2106.12147v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aliee_H/0/1/0/all/0/1\">Hananeh Aliee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theis_F/0/1/0/all/0/1\">Fabian J. Theis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilbertus_N/0/1/0/all/0/1\">Niki Kilbertus</a>",
          "description": "Spurred by tremendous success in pattern matching and prediction tasks,\nresearchers increasingly resort to machine learning to aid original scientific\ndiscovery. Given large amounts of observational data about a system, can we\nuncover the rules that govern its evolution? Solving this task holds the great\npromise of fully understanding the causal interactions and being able to make\nreliable predictions about the system's behavior under interventions. We take a\nstep towards answering this question for time-series data generated from\nsystems of ordinary differential equations (ODEs). While the governing ODEs\nmight not be identifiable from data alone, we show that combining simple\nregularization schemes with flexible neural ODEs can robustly recover the\ndynamics and causal structures from time-series data. Our results on a variety\nof (non)-linear first and second order systems as well as real data validate\nour method. We conclude by showing that we can also make accurate predictions\nunder interventions on variables or the system itself.",
          "link": "http://arxiv.org/abs/2106.12430",
          "publishedOn": "2021-06-24T01:51:43.754Z",
          "wordCount": 589,
          "title": "Beyond Predictions in Neural ODEs: Identification and Interventions. (arXiv:2106.12430v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12545",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Odeh_I/0/1/0/all/0/1\">Israa Odeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alkasassbeh_M/0/1/0/all/0/1\">Mouhammd Alkasassbeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alauthman_M/0/1/0/all/0/1\">Mohammad Alauthman</a>",
          "description": "Diabetic Retinopathy (DR) is among the worlds leading vision loss causes in\ndiabetic patients. DR is a microvascular disease that affects the eye retina,\nwhich causes vessel blockage and therefore cuts the main source of nutrition\nfor the retina tissues. Treatment for this visual disorder is most effective\nwhen it is detected in its earliest stages, as severe DR can result in\nirreversible blindness. Nonetheless, DR identification requires the expertise\nof Ophthalmologists which is often expensive and time-consuming. Therefore,\nautomatic detection systems were introduced aiming to facilitate the\nidentification process, making it available globally in a time and\ncost-efficient manner. However, due to the limited reliable datasets and\nmedical records for this particular eye disease, the obtained predictions\naccuracies were relatively unsatisfying for eye specialists to rely on them as\ndiagnostic systems. Thus, we explored an ensemble-based learning strategy,\nmerging a substantial selection of well-known classification algorithms in one\nsophisticated diagnostic model. The proposed framework achieved the highest\naccuracy rates among all other common classification algorithms in the area. 4\nsubdatasets were generated to contain the top 5 and top 10 features of the\nMessidor dataset, selected by InfoGainEval. and WrapperSubsetEval., accuracies\nof 70.7% and 75.1% were achieved on the InfoGainEval. top 5 and original\ndataset respectively. The results imply the impressive performance of the\nsubdataset, which significantly conduces to a less complex classification\nprocess",
          "link": "http://arxiv.org/abs/2106.12545",
          "publishedOn": "2021-06-24T01:51:43.748Z",
          "wordCount": 669,
          "title": "Diabetic Retinopathy Detection using Ensemble Machine Learning. (arXiv:2106.12545v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12045",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Singh_M/0/1/0/all/0/1\">Manmeet Singh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kumar_B/0/1/0/all/0/1\">Bipin Kumar</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Niyogi_D/0/1/0/all/0/1\">Dev Niyogi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rao_S/0/1/0/all/0/1\">Suryachandra Rao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gill_S/0/1/0/all/0/1\">Sukhpal Singh Gill</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chattopadhyay_R/0/1/0/all/0/1\">Rajib Chattopadhyay</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nanjundiah_R/0/1/0/all/0/1\">Ravi S Nanjundiah</a>",
          "description": "The formation of precipitation in state-of-the-art weather and climate models\nis an important process. The understanding of its relationship with other\nvariables can lead to endless benefits, particularly for the world's monsoon\nregions dependent on rainfall as a support for livelihood. Various factors play\na crucial role in the formation of rainfall, and those physical processes are\nleading to significant biases in the operational weather forecasts. We use the\nUNET architecture of a deep convolutional neural network with residual learning\nas a proof of concept to learn global data-driven models of precipitation. The\nmodels are trained on reanalysis datasets projected on the cubed-sphere\nprojection to minimize errors due to spherical distortion. The results are\ncompared with the operational dynamical model used by the India Meteorological\nDepartment. The theoretical deep learning-based model shows doubling of the\ngrid point, as well as area averaged skill measured in Pearson correlation\ncoefficients relative to operational system. This study is a proof-of-concept\nshowing that residual learning-based UNET can unravel physical relationships to\ntarget precipitation, and those physical constraints can be used in the\ndynamical operational models towards improved precipitation forecasts. Our\nresults pave the way for the development of online, hybrid models in the\nfuture.",
          "link": "http://arxiv.org/abs/2106.12045",
          "publishedOn": "2021-06-24T01:51:43.743Z",
          "wordCount": 650,
          "title": "Deep learning for improved global precipitation in numerical weather prediction systems. (arXiv:2106.12045v1 [physics.ao-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengchun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kettimuthu_R/0/1/0/all/0/1\">Rajkumar Kettimuthu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papka_M/0/1/0/all/0/1\">Michael E. Papka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_I/0/1/0/all/0/1\">Ian Foster</a>",
          "description": "Supercomputer FCFS-based scheduling policies result in many transient idle\nnodes, a phenomenon that is only partially alleviated by backfill scheduling\nmethods that promote small jobs to run before large jobs. Here we describe how\nto realize a novel use for these otherwise wasted resources, namely, deep\nneural network (DNN) training. This important workload is easily organized as\nmany small fragments that can be configured dynamically to fit essentially any\nnode*time hole in a supercomputer's schedule. We describe how the task of\nrescaling suitable DNN training tasks to fit dynamically changing holes can be\nformulated as a deterministic mixed integer linear programming (MILP)-based\nresource allocation algorithm, and show that this MILP problem can be solved\nefficiently at run time. We show further how this MILP problem can be adapted\nto optimize for administrator- or user-defined metrics. We validate our method\nwith supercomputer scheduler logs and different DNN training scenarios, and\ndemonstrate efficiencies of up to 93% compared with running the same training\ntasks on dedicated nodes. Our method thus enables substantial supercomputer\nresources to be allocated to DNN training with no impact on other applications.",
          "link": "http://arxiv.org/abs/2106.12091",
          "publishedOn": "2021-06-24T01:51:43.736Z",
          "wordCount": 629,
          "title": "BFTrainer: Low-Cost Training of Neural Networks on Unfillable Supercomputer Nodes. (arXiv:2106.12091v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1\">Maureen Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1\">Jiachen Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timothy_R/0/1/0/all/0/1\">Reese Timothy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1\">Jerry L. Prince</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "Self-training based unsupervised domain adaptation (UDA) has shown great\npotential to address the problem of domain shift, when applying a trained deep\nlearning model in a source domain to unlabeled target domains. However, while\nthe self-training UDA has demonstrated its effectiveness on discriminative\ntasks, such as classification and segmentation, via the reliable pseudo-label\nselection based on the softmax discrete histogram, the self-training UDA for\ngenerative tasks, such as image synthesis, is not fully investigated. In this\nwork, we propose a novel generative self-training (GST) UDA framework with\ncontinuous value prediction and regression objective for cross-domain image\nsynthesis. Specifically, we propose to filter the pseudo-label with an\nuncertainty mask, and quantify the predictive confidence of generated images\nwith practical variational Bayes learning. The fast test-time adaptation is\nachieved by a round-based alternative optimization scheme. We validated our\nframework on the tagged-to-cine magnetic resonance imaging (MRI) synthesis\nproblem, where datasets in the source and target domains were acquired from\ndifferent scanners or centers. Extensive validations were carried out to verify\nour framework against popular adversarial training UDA methods. Results show\nthat our GST, with tagged MRI of test subjects in new target domains, improved\nthe synthesis quality by a large margin, compared with the adversarial training\nUDA methods.",
          "link": "http://arxiv.org/abs/2106.12499",
          "publishedOn": "2021-06-24T01:51:43.719Z",
          "wordCount": 671,
          "title": "Generative Self-training for Cross-domain Unsupervised Tagged-to-Cine MRI Synthesis. (arXiv:2106.12499v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stan_S/0/1/0/all/0/1\">Serban Stan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>",
          "description": "Multi-source unsupervised domain adaptation (MUDA) is a recently explored\nlearning framework, where the goal is to address the challenge of labeled data\nscarcity in a target domain via transferring knowledge from multiple source\ndomains with annotated data. Since the source data is distributed, the privacy\nof source domains' data can be a natural concern. We benefit from the idea of\ndomain alignment in an embedding space to address the privacy concern for MUDA.\nOur method is based on aligning the sources and target distributions indirectly\nvia internally learned distributions, without communicating data samples\nbetween domains. We justify our approach theoretically and perform extensive\nexperiments to demonstrate that our method is effective and compares favorably\nagainst existing methods.",
          "link": "http://arxiv.org/abs/2106.12124",
          "publishedOn": "2021-06-24T01:51:43.713Z",
          "wordCount": 538,
          "title": "Secure Domain Adaptation with Multiple Sources. (arXiv:2106.12124v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drichel_A/0/1/0/all/0/1\">Arthur Drichel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faerber_N/0/1/0/all/0/1\">Nils Faerber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_U/0/1/0/all/0/1\">Ulrike Meyer</a>",
          "description": "Numerous malware families rely on domain generation algorithms (DGAs) to\nestablish a connection to their command and control (C2) server. Counteracting\nDGAs, several machine learning classifiers have been proposed enabling the\nidentification of the DGA that generated a specific domain name and thus\ntriggering targeted remediation measures. However, the proposed\nstate-of-the-art classifiers are based on deep learning models. The black box\nnature of these makes it difficult to evaluate their reasoning. The resulting\nlack of confidence makes the utilization of such models impracticable. In this\npaper, we propose EXPLAIN, a feature-based and contextless DGA multiclass\nclassifier. We comparatively evaluate several combinations of feature sets and\nhyperparameters for our approach against several state-of-the-art classifiers\nin a unified setting on the same real-world data. Our classifier achieves\ncompetitive results, is real-time capable, and its predictions are easier to\ntrace back to features than the predictions made by the DGA multiclass\nclassifiers proposed in related work.",
          "link": "http://arxiv.org/abs/2106.12336",
          "publishedOn": "2021-06-24T01:51:43.669Z",
          "wordCount": 616,
          "title": "First Step Towards EXPLAINable DGA Multiclass Classification. (arXiv:2106.12336v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Connor_M/0/1/0/all/0/1\">Marissa Connor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallah_K/0/1/0/all/0/1\">Kion Fallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozell_C/0/1/0/all/0/1\">Christopher Rozell</a>",
          "description": "Many machine learning techniques incorporate identity-preserving\ntransformations into their models to generalize their performance to previously\nunseen data. These transformations are typically selected from a set of\nfunctions that are known to maintain the identity of an input when applied\n(e.g., rotation, translation, flipping, and scaling). However, there are many\nnatural variations that cannot be labeled for supervision or defined through\nexamination of the data. As suggested by the manifold hypothesis, many of these\nnatural variations live on or near a low-dimensional, nonlinear manifold.\nSeveral techniques represent manifold variations through a set of learned Lie\ngroup operators that define directions of motion on the manifold. However\ntheses approaches are limited because they require transformation labels when\ntraining their models and they lack a method for determining which regions of\nthe manifold are appropriate for applying each specific operator. We address\nthese limitations by introducing a learning strategy that does not require\ntransformation labels and developing a method that learns the local regions\nwhere each operator is likely to be used while preserving the identity of\ninputs. Experiments on MNIST and Fashion MNIST highlight our model's ability to\nlearn identity-preserving transformations on multi-class datasets.\nAdditionally, we train on CelebA to showcase our model's ability to learn\nsemantically meaningful transformations on complex datasets in an unsupervised\nmanner.",
          "link": "http://arxiv.org/abs/2106.12096",
          "publishedOn": "2021-06-24T01:51:43.652Z",
          "wordCount": 644,
          "title": "Learning Identity-Preserving Transformations on Data Manifolds. (arXiv:2106.12096v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1\">Tero Karras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1\">Miika Aittala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1\">Samuli Laine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1\">Erik H&#xe4;rk&#xf6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1\">Janne Hellsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1\">Jaakko Lehtinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1\">Timo Aila</a>",
          "description": "We observe that despite their hierarchical convolutional nature, the\nsynthesis process of typical generative adversarial networks depends on\nabsolute pixel coordinates in an unhealthy manner. This manifests itself as,\ne.g., detail appearing to be glued to image coordinates instead of the surfaces\nof depicted objects. We trace the root cause to careless signal processing that\ncauses aliasing in the generator network. Interpreting all signals in the\nnetwork as continuous, we derive generally applicable, small architectural\nchanges that guarantee that unwanted information cannot leak into the\nhierarchical synthesis process. The resulting networks match the FID of\nStyleGAN2 but differ dramatically in their internal representations, and they\nare fully equivariant to translation and rotation even at subpixel scales. Our\nresults pave the way for generative models better suited for video and\nanimation.",
          "link": "http://arxiv.org/abs/2106.12423",
          "publishedOn": "2021-06-24T01:51:43.645Z",
          "wordCount": 583,
          "title": "Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirsch_A/0/1/0/all/0/1\">Andreas Kirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>",
          "description": "Information theory is of importance to machine learning, but the notation for\ninformation-theoretic quantities is sometimes opaque. The right notation can\nconvey valuable intuitions and concisely express new ideas. We propose such a\nnotation for machine learning users and expand it to include\ninformation-theoretic quantities between events (outcomes) and random\nvariables. We apply this notation to a popular information-theoretic\nacquisition function in Bayesian active learning which selects the most\ninformative (unlabelled) samples to be labelled by an expert. We demonstrate\nthe value of our notation when extending the acquisition function to the\ncore-set problem, which consists of selecting the most informative samples\n\\emph{given} the labels.",
          "link": "http://arxiv.org/abs/2106.12062",
          "publishedOn": "2021-06-24T01:51:43.618Z",
          "wordCount": 535,
          "title": "A Practical & Unified Notation for Information-Theoretic Quantities in ML. (arXiv:2106.12062v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hengrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qitian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1\">David Wipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "We introduce a conceptually simple yet effective model for self-supervised\nrepresentation learning with graph data. It follows the previous methods that\ngenerate two views of an input graph through data augmentation. However, unlike\ncontrastive methods that focus on instance-level discrimination, we optimize an\ninnovative feature-level objective inspired by classical Canonical Correlation\nAnalysis. Compared with other works, our approach requires none of the\nparameterized mutual information estimator, additional projector, asymmetric\nstructures, and most importantly, negative samples which can be costly. We show\nthat the new objective essentially 1) aims at discarding augmentation-variant\ninformation by learning invariant representations, and 2) can prevent\ndegenerated solutions by decorrelating features in different dimensions. Our\ntheoretical analysis further provides an understanding for the new objective\nwhich can be equivalently seen as an instantiation of the Information\nBottleneck Principle under the self-supervised setting. Despite its simplicity,\nour method performs competitively on seven public graph datasets.",
          "link": "http://arxiv.org/abs/2106.12484",
          "publishedOn": "2021-06-24T01:51:43.607Z",
          "wordCount": 583,
          "title": "From Canonical Correlation Analysis to Self-supervised Graph Neural Networks. (arXiv:2106.12484v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shao-Bo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Ding-Xuan Zhou</a>",
          "description": "Compared with avid research activities of deep convolutional neural networks\n(DCNNs) in practice, the study of theoretical behaviors of DCNNs lags heavily\nbehind. In particular, the universal consistency of DCNNs remains open. In this\npaper, we prove that implementing empirical risk minimization on DCNNs with\nexpansive convolution (with zero-padding) is strongly universally consistent.\nMotivated by the universal consistency, we conduct a series of experiments to\nshow that without any fully connected layers, DCNNs with expansive convolution\nperform not worse than the widely used deep neural networks with hybrid\nstructure containing contracting (without zero-padding) convolution layers and\nseveral fully connected layers.",
          "link": "http://arxiv.org/abs/2106.12498",
          "publishedOn": "2021-06-24T01:51:43.593Z",
          "wordCount": 537,
          "title": "Universal Consistency of Deep Convolutional Neural Networks. (arXiv:2106.12498v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1\">Charaf Eddine Benarab</a>",
          "description": "Knowledge is acquired by humans through experience, and no boundary is set\nbetween the kinds of knowledge or skill levels we can achieve on different\ntasks at the same time. When it comes to Neural Networks, that is not the case,\nthe major breakthroughs in the field are extremely task and domain specific.\nVision and language are dealt with in separate manners, using separate methods\nand different datasets. In this work, we propose to use knowledge acquired by\nbenchmark Vision Models which are trained on ImageNet to help a much smaller\narchitecture learn to classify text. After transforming the textual data\ncontained in the IMDB dataset to gray scale images. An analysis of different\ndomains and the Transfer Learning method is carried out. Despite the challenge\nposed by the very different datasets, promising results are achieved. The main\ncontribution of this work is a novel approach which links large pretrained\nmodels on both language and vision to achieve state-of-the-art results in\ndifferent sub-fields from the original task. Without needing high compute\ncapacity resources. Specifically, Sentiment Analysis is achieved after\ntransferring knowledge between vision and language models. BERT embeddings are\ntransformed into grayscale images, these images are then used as training\nexamples for pretrained vision models such as VGG16 and ResNet\n\nIndex Terms: Natural language, Vision, BERT, Transfer Learning, CNN, Domain\nAdaptation.",
          "link": "http://arxiv.org/abs/2106.12479",
          "publishedOn": "2021-06-24T01:51:43.588Z",
          "wordCount": 675,
          "title": "Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12228",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jullum_M/0/1/0/all/0/1\">Martin Jullum</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Redelmeier_A/0/1/0/all/0/1\">Annabelle Redelmeier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aas_K/0/1/0/all/0/1\">Kjersti Aas</a>",
          "description": "Shapley values has established itself as one of the most appropriate and\ntheoretically sound frameworks for explaining predictions from complex machine\nlearning models. The popularity of Shapley values in the explanation setting is\nprobably due to its unique theoretical properties. The main drawback with\nShapley values, however, is that its computational complexity grows\nexponentially in the number of input features, making it unfeasible in many\nreal world situations where there could be hundreds or thousands of features.\nFurthermore, with many (dependent) features, presenting/visualizing and\ninterpreting the computed Shapley values also becomes challenging. The present\npaper introduces groupShapley: a conceptually simple approach for dealing with\nthe aforementioned bottlenecks. The idea is to group the features, for example\nby type or dependence, and then compute and present Shapley values for these\ngroups instead of for all individual features. Reducing hundreds or thousands\nof features to half a dozen or so, makes precise computations practically\nfeasible and the presentation and knowledge extraction greatly simplified. We\nprove that under certain conditions, groupShapley is equivalent to summing the\nfeature-wise Shapley values within each feature group. Moreover, we provide a\nsimulation study exemplifying the differences when these conditions are not\nmet. We illustrate the usability of the approach in a real world car insurance\nexample, where groupShapley is used to provide simple and intuitive\nexplanations.",
          "link": "http://arxiv.org/abs/2106.12228",
          "publishedOn": "2021-06-24T01:51:43.567Z",
          "wordCount": 659,
          "title": "groupShapley: Efficient prediction explanation with Shapley values for feature groups. (arXiv:2106.12228v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from\na labeled source domain to an unlabeled and unseen target domain, which is\nusually trained on data from both domains. Access to the source domain data at\nthe adaptation stage, however, is often limited, due to data storage or privacy\nissues. To alleviate this, in this work, we target source free UDA for\nsegmentation, and propose to adapt an ``off-the-shelf\" segmentation model\npre-trained in the source domain to the target domain, with an adaptive\nbatch-wise normalization statistics adaptation framework. Specifically, the\ndomain-specific low-order batch statistics, i.e., mean and variance, are\ngradually adapted with an exponential momentum decay scheme, while the\nconsistency of domain shareable high-order batch statistics, i.e., scaling and\nshifting parameters, is explicitly enforced by our optimization objective. The\ntransferability of each channel is adaptively measured first from which to\nbalance the contribution of each channel. Moreover, the proposed source free\nUDA framework is orthogonal to unsupervised learning methods, e.g.,\nself-entropy minimization, which can thus be simply added on top of our\nframework. Extensive experiments on the BraTS 2018 database show that our\nsource free UDA framework outperformed existing source-relaxed UDA methods for\nthe cross-subtype UDA segmentation task and yielded comparable results for the\ncross-modality UDA segmentation task, compared with a supervised UDA methods\nwith the source data.",
          "link": "http://arxiv.org/abs/2106.12497",
          "publishedOn": "2021-06-24T01:51:43.562Z",
          "wordCount": 673,
          "title": "Adapting Off-the-Shelf Source Segmenter for Target Medical Image Segmentation. (arXiv:2106.12497v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12034",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1\">Yinglun Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jiang_R/0/1/0/all/0/1\">Ruoxi Jiang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Willett_R/0/1/0/all/0/1\">Rebecca Willett</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nowak_R/0/1/0/all/0/1\">Robert Nowak</a>",
          "description": "We study pure exploration in bandits, where the dimension of the feature\nrepresentation can be much larger than the number of arms. To overcome the\ncurse of dimensionality, we propose to adaptively embed the feature\nrepresentation of each arm into a lower-dimensional space and carefully deal\nwith the induced model misspecifications. Our approach is conceptually very\ndifferent from existing works that can either only handle low-dimensional\nlinear bandits or passively deal with model misspecifications. We showcase the\napplication of our approach to two pure exploration settings that were\npreviously under-studied: (1) the reward function belongs to a possibly\ninfinite-dimensional Reproducing Kernel Hilbert Space, and (2) the reward\nfunction is nonlinear and can be approximated by neural networks. Our main\nresults provide sample complexity guarantees that only depend on the effective\ndimension of the feature spaces in the kernel or neural representations.\nExtensive experiments conducted on both synthetic and real-world datasets\ndemonstrate the efficacy of our methods.",
          "link": "http://arxiv.org/abs/2106.12034",
          "publishedOn": "2021-06-24T01:51:43.556Z",
          "wordCount": 592,
          "title": "Pure Exploration in Kernel and Neural Bandits. (arXiv:2106.12034v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+T_B/0/1/0/all/0/1\">Balamurali B T</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hee_H/0/1/0/all/0/1\">Hwan Ing Hee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1\">Saumitra Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teoh_O/0/1/0/all/0/1\">Oon Hoe Teoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1\">Sung Shin Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Khai Pin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1\">Dorien Herremans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jer Ming Chen</a>",
          "description": "Intelligent systems are transforming the world, as well as our healthcare\nsystem. We propose a deep learning-based cough sound classification model that\ncan distinguish between children with healthy versus pathological coughs such\nas asthma, upper respiratory tract infection (URTI), and lower respiratory\ntract infection (LRTI). In order to train a deep neural network model, we\ncollected a new dataset of cough sounds, labelled with clinician's diagnosis.\nThe chosen model is a bidirectional long-short term memory network (BiLSTM)\nbased on Mel Frequency Cepstral Coefficients (MFCCs) features. The resulting\ntrained model when trained for classifying two classes of coughs -- healthy or\npathology (in general or belonging to a specific respiratory pathology),\nreaches accuracy exceeding 84\\% when classifying cough to the label provided by\nthe physicians' diagnosis. In order to classify subject's respiratory pathology\ncondition, results of multiple cough epochs per subject were combined. The\nresulting prediction accuracy exceeds 91\\% for all three respiratory\npathologies. However, when the model is trained to classify and discriminate\namong the four classes of coughs, overall accuracy dropped: one class of\npathological coughs are often misclassified as other. However, if one consider\nthe healthy cough classified as healthy and pathological cough classified to\nhave some kind of pathologies, then the overall accuracy of four class model is\nabove 84\\%. A longitudinal study of MFCC feature space when comparing\npathological and recovered coughs collected from the same subjects revealed the\nfact that pathological cough irrespective of the underlying conditions occupy\nthe same feature space making it harder to differentiate only using MFCC\nfeatures.",
          "link": "http://arxiv.org/abs/2106.12174",
          "publishedOn": "2021-06-24T01:51:43.551Z",
          "wordCount": 722,
          "title": "Deep Neural Network Based Respiratory Pathology Classification Using Cough Sounds. (arXiv:2106.12174v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jingda Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Chen Lv</a>",
          "description": "To further improve the learning efficiency and performance of reinforcement\nlearning (RL), in this paper we propose a novel uncertainty-aware model-based\nRL (UA-MBRL) framework, and then implement and validate it in autonomous\ndriving under various task scenarios. First, an action-conditioned ensemble\nmodel with the ability of uncertainty assessment is established as the virtual\nenvironment model. Then, a novel uncertainty-aware model-based RL framework is\ndeveloped based on the adaptive truncation approach, providing virtual\ninteractions between the agent and environment model, and improving RL's\ntraining efficiency and performance. The developed algorithms are then\nimplemented in end-to-end autonomous vehicle control tasks, validated and\ncompared with state-of-the-art methods under various driving scenarios. The\nvalidation results suggest that the proposed UA-MBRL method surpasses the\nexisting model-based and model-free RL approaches, in terms of learning\nefficiency and achieved performance. The results also demonstrate the good\nability of the proposed method with respect to the adaptiveness and robustness,\nunder various autonomous driving scenarios.",
          "link": "http://arxiv.org/abs/2106.12194",
          "publishedOn": "2021-06-24T01:51:43.546Z",
          "wordCount": 588,
          "title": "Uncertainty-Aware Model-Based Reinforcement Learning with Application to Autonomous Driving. (arXiv:2106.12194v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rouillard_L/0/1/0/all/0/1\">Louis Rouillard</a> (PARIETAL, Inria, CEA), <a href=\"http://arxiv.org/find/cs/1/au:+Wassermann_D/0/1/0/all/0/1\">Demian Wassermann</a> (PARIETAL, Inria, CEA)",
          "description": "Frequently, population studies feature pyramidally-organized data represented\nusing Hierarchical Bayesian Models (HBM) enriched with plates. These models can\nbecome prohibitively large in settings such as neuroimaging, where a sample is\ncomposed of a functional MRI signal measured on 64 thousand brain locations,\nacross 4 measurement sessions, and at least tens of subjects. Even a reduced\nexample on a specific cortical region of 300 brain locations features around 1\nmillion parameters, hampering the usage of modern density estimation techniques\nsuch as Simulation-Based Inference (SBI). To infer parameter posterior\ndistributions in this challenging class of problems, we designed a novel\nmethodology that automatically produces a variational family dual to a target\nHBM. This variatonal family, represented as a neural network, consists in the\ncombination of an attention-based hierarchical encoder feeding summary\nstatistics to a set of normalizing flows. Our automatically-derived neural\nnetwork exploits exchangeability in the plate-enriched HBM and factorizes its\nparameter space. The resulting architecture reduces by orders of magnitude its\nparameterization with respect to that of a typical SBI representation, while\nmaintaining expressivity. Our method performs inference on the specified HBM in\nan amortized setup: once trained, it can readily be applied to a new data\nsample to compute the parameters' full posterior. We demonstrate the capability\nof our method on simulated data, as well as a challenging high-dimensional\nbrain parcellation experiment. We also open up several questions that lie at\nthe intersection between SBI techniques and structured Variational Inference.",
          "link": "http://arxiv.org/abs/2106.12248",
          "publishedOn": "2021-06-24T01:51:43.530Z",
          "wordCount": 701,
          "title": "ADAVI: Automatic Dual Amortized Variational Inference Applied To Pyramidal Bayesian Models. (arXiv:2106.12248v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirsch_A/0/1/0/all/0/1\">Andreas Kirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farquhar_S/0/1/0/all/0/1\">Sebastian Farquhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>",
          "description": "In active learning, new labels are commonly acquired in batches. However,\ncommon acquisition functions are only meant for one-sample acquisition rounds\nat a time, and when their scores are used naively for batch acquisition, they\nresult in batches lacking diversity, which deteriorates performance. On the\nother hand, state-of-the-art batch acquisition functions are costly to compute.\nIn this paper, we present a novel class of stochastic acquisition functions\nthat extend one-sample acquisition functions to the batch setting by observing\nhow one-sample acquisition scores change as additional samples are acquired and\nmodelling this difference for additional batch samples. We simply acquire new\nsamples by sampling from the pool set using a Gibbs distribution based on the\nacquisition scores. Our acquisition functions are both vastly cheaper to\ncompute and out-perform other batch acquisition functions.",
          "link": "http://arxiv.org/abs/2106.12059",
          "publishedOn": "2021-06-24T01:51:43.512Z",
          "wordCount": 570,
          "title": "A Simple Baseline for Batch Active Learning with Stochastic Acquisition Functions. (arXiv:2106.12059v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuantao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jie Ding</a>",
          "description": "Multi-layer feedforward networks have been used to approximate a wide range\nof nonlinear functions. An important and fundamental problem is to understand\nthe learnability of a network model through its statistical risk, or the\nexpected prediction error on future data. To the best of our knowledge, the\nrate of convergence of neural networks shown by existing works is bounded by at\nmost the order of $n^{-1/4}$ for a sample size of $n$. In this paper, we show\nthat a class of variation-constrained neural networks, with arbitrary width,\ncan achieve near-parametric rate $n^{-1/2+\\delta}$ for an arbitrarily small\npositive constant $\\delta$. It is equivalent to $n^{-1 +2\\delta}$ under the\nmean squared error. This rate is also observed by numerical experiments. The\nresult indicates that the neural function space needed for approximating smooth\nfunctions may not be as large as what is often perceived. Our result also\nprovides insight to the phenomena that deep neural networks do not easily\nsuffer from overfitting when the number of neurons and learning parameters\nrapidly grow with $n$ or even surpass $n$. We also discuss the rate of\nconvergence regarding other network parameters, including the input dimension,\nnetwork layer, and coefficient norm.",
          "link": "http://arxiv.org/abs/2106.12068",
          "publishedOn": "2021-06-24T01:51:43.507Z",
          "wordCount": 636,
          "title": "The Rate of Convergence of Variation-Constrained Deep Neural Networks. (arXiv:2106.12068v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12231",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Carratino_L/0/1/0/all/0/1\">Luigi Carratino</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vigogna_S/0/1/0/all/0/1\">Stefano Vigogna</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Calandriello_D/0/1/0/all/0/1\">Daniele Calandriello</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rosasco_L/0/1/0/all/0/1\">Lorenzo Rosasco</a>",
          "description": "We introduce ParK, a new large-scale solver for kernel ridge regression. Our\napproach combines partitioning with random projections and iterative\noptimization to reduce space and time complexity while provably maintaining the\nsame statistical accuracy. In particular, constructing suitable partitions\ndirectly in the feature space rather than in the input space, we promote\northogonality between the local estimators, thus ensuring that key quantities\nsuch as local effective dimension and bias remain under control. We\ncharacterize the statistical-computational tradeoff of our model, and\ndemonstrate the effectiveness of our method by numerical experiments on\nlarge-scale datasets.",
          "link": "http://arxiv.org/abs/2106.12231",
          "publishedOn": "2021-06-24T01:51:43.491Z",
          "wordCount": 534,
          "title": "ParK: Sound and Efficient Kernel Ridge Regression by Feature Space Partitions. (arXiv:2106.12231v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jakkala_K/0/1/0/all/0/1\">Kalvik Jakkala</a>",
          "description": "Gaussian processes are one of the dominant approaches in Bayesian learning.\nAlthough the approach has been applied to numerous problems with great success,\nit has a few fundamental limitations. Multiple methods in literature have\naddressed these limitations. However, there has not been a comprehensive survey\nof the topics as of yet. Most existing surveys focus on only one particular\nvariant of Gaussian processes and their derivatives. This survey details the\ncore motivations for using Gaussian processes, their mathematical formulations,\nlimitations, and research themes that have flourished over the years to address\nsaid limitations. Furthermore, one particular research area is Deep Gaussian\nProcesses (DGPs), it has improved substantially in the past decade. The\nsignificant publications that advanced the forefront of this research area are\noutlined in their survey. Finally, a brief discussion on open problems and\nresearch directions for future work is presented at the end.",
          "link": "http://arxiv.org/abs/2106.12135",
          "publishedOn": "2021-06-24T01:51:43.474Z",
          "wordCount": 572,
          "title": "Deep Gaussian Processes: A Survey. (arXiv:2106.12135v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1\">Qi Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason D. Lee</a>",
          "description": "Transfer learning is essential when sufficient data comes from the source\ndomain, with scarce labeled data from the target domain. We develop estimators\nthat achieve minimax linear risk for linear regression problems under\ndistribution shift. Our algorithms cover different transfer learning settings\nincluding covariate shift and model shift. We also consider when data are\ngenerated from either linear or general nonlinear models. We show that linear\nminimax estimators are within an absolute constant of the minimax risk even\namong nonlinear estimators for various source/target distributions.",
          "link": "http://arxiv.org/abs/2106.12108",
          "publishedOn": "2021-06-24T01:51:43.442Z",
          "wordCount": 517,
          "title": "Near-Optimal Linear Regression under Distribution Shift. (arXiv:2106.12108v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11950",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Behne_J/0/1/0/all/0/1\">Joshua K. Behne</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Reeves_G/0/1/0/all/0/1\">Galen Reeves</a>",
          "description": "We study the problem of estimating a rank-one matrix from Gaussian\nobservations where different blocks of the matrix are observed under different\nnoise levels. This problem is motivated by applications in clustering and\ncommunity detection where latent variables can be partitioned into a fixed\nnumber of known groups (e.g., users and items) and the blocks of the matrix\ncorrespond to different types of pairwise interactions (e.g., user-user,\nuser-item, or item-item interactions). In the setting where the number of\nblocks is fixed while the number of variables tends to infinity, we prove\nasymptotically exact formulas for the minimum mean-squared error in estimating\nboth the matrix and the latent variables. These formulas describe the weak\nrecovery thresholds for the problem and reveal invariance properties with\nrespect to certain scalings of the noise variance. We also derive an\napproximate message passing algorithm and a gradient descent algorithm and show\nempirically that these algorithms achieve the information-theoretic limits in\ncertain regimes.",
          "link": "http://arxiv.org/abs/2106.11950",
          "publishedOn": "2021-06-23T01:48:42.476Z",
          "wordCount": 589,
          "title": "Rank-one matrix estimation with groupwise heteroskedasticity. (arXiv:2106.11950v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1906.04648",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Tan_S/0/1/0/all/0/1\">Sandra S. Y. Tan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Varvitsiotis_A/0/1/0/all/0/1\">Antonios Varvitsiotis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>",
          "description": "We introduce a new framework for unifying and systematizing the performance\nanalysis of first-order black-box optimization algorithms for unconstrained\nconvex minimization. The low-cost iteration complexity enjoyed by first-order\nalgorithms renders them particularly relevant for applications in machine\nlearning and large-scale data analysis. Relying on sum-of-squares (SOS)\noptimization, we introduce a hierarchy of semidefinite programs that give\nincreasingly better convergence bounds for higher levels of the hierarchy.\nAlluding to the power of the SOS hierarchy, we show that the (dual of the)\nfirst level corresponds to the Performance Estimation Problem (PEP) introduced\nby Drori and Teboulle [Math. Program., 145(1):451--482, 2014], a powerful\nframework for determining convergence rates of first-order optimization\nalgorithms. Consequently, many results obtained within the PEP framework can be\nreinterpreted as degree-1 SOS proofs, and thus, the SOS framework provides a\npromising new approach for certifying improved rates of convergence by means of\nhigher-order SOS certificates. To determine analytical rate bounds, in this\nwork we use the first level of the SOS hierarchy and derive new result{s} for\nnoisy gradient descent with inexact line search methods (Armijo, Wolfe, and\nGoldstein).",
          "link": "http://arxiv.org/abs/1906.04648",
          "publishedOn": "2021-06-23T01:48:42.470Z",
          "wordCount": 698,
          "title": "Analysis of Optimization Algorithms via Sum-of-Squares. (arXiv:1906.04648v4 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chorowski_J/0/1/0/all/0/1\">Jan Chorowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciesielski_G/0/1/0/all/0/1\">Grzegorz Ciesielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dzikowski_J/0/1/0/all/0/1\">Jaros&#x142;aw Dzikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lancucki_A/0/1/0/all/0/1\">Adrian &#x141;a&#x144;cucki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marxer_R/0/1/0/all/0/1\">Ricard Marxer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opala_M/0/1/0/all/0/1\">Mateusz Opala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pusz_P/0/1/0/all/0/1\">Piotr Pusz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rychlikowski_P/0/1/0/all/0/1\">Pawe&#x142; Rychlikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stypulkowski_M/0/1/0/all/0/1\">Micha&#x142; Stypu&#x142;kowski</a>",
          "description": "We investigate the possibility of forcing a self-supervised model trained\nusing a contrastive predictive loss to extract slowly varying latent\nrepresentations. Rather than producing individual predictions for each of the\nfuture representations, the model emits a sequence of predictions shorter than\nthat of the upcoming representations to which they will be aligned. In this\nway, the prediction network solves a simpler task of predicting the next\nsymbols, but not their exact timing, while the encoding network is trained to\nproduce piece-wise constant latent codes. We evaluate the model on a speech\ncoding task and demonstrate that the proposed Aligned Contrastive Predictive\nCoding (ACPC) leads to higher linear phone prediction accuracy and lower ABX\nerror rates, while being slightly faster to train due to the reduced number of\nprediction heads.",
          "link": "http://arxiv.org/abs/2104.11946",
          "publishedOn": "2021-06-23T01:48:42.386Z",
          "wordCount": 614,
          "title": "Aligned Contrastive Predictive Coding. (arXiv:2104.11946v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13945",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Abel_S/0/1/0/all/0/1\">Steve Abel</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Blance_A/0/1/0/all/0/1\">Andrew Blance</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Spannowsky_M/0/1/0/all/0/1\">Michael Spannowsky</a>",
          "description": "We perform an in-depth comparison of quantum annealing with several classical\noptimisation techniques, namely thermal annealing, Nelder-Mead, and gradient\ndescent. We begin with a direct study of the 2D Ising model on a quantum\nannealer, and compare its properties directly with those of the thermal 2D\nIsing model. These properties include an Ising-like phase transition that can\nbe induced by either a change in 'quantum-ness' of the theory, or by a scaling\nthe Ising couplings up or down. This behaviour is in accord with what is\nexpected from the physical understanding of the quantum system. We then go on\nto demonstrate the efficacy of the quantum annealer at minimising several\nincreasingly hard two dimensional potentials. For all the potentials we find\nthe general behaviour that Nelder-Mead and gradient descent methods are very\nsusceptible to becoming trapped in false minima, while the thermal anneal\nmethod is somewhat better at discovering the true minimum. However, and despite\ncurrent limitations on its size, the quantum annealer performs a minimisation\nvery markedly better than any of these classical techniques. A quantum anneal\ncan be designed so that the system almost never gets trapped in a false\nminimum, and rapidly and successfully minimises the potentials.",
          "link": "http://arxiv.org/abs/2105.13945",
          "publishedOn": "2021-06-23T01:48:42.369Z",
          "wordCount": 690,
          "title": "Quantum Optimisation of Complex Systems with a Quantum Annealer. (arXiv:2105.13945v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Franceschelli_G/0/1/0/all/0/1\">Giorgio Franceschelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1\">Mirco Musolesi</a>",
          "description": "Machine-generated artworks are now part of the contemporary art scene: they\nare attracting significant investments and they are presented in exhibitions\ntogether with those created by human artists. These artworks are mainly based\non generative deep learning techniques. Also given their success, several legal\nproblems arise when working with these techniques.\n\nIn this article we consider a set of key questions in the area of generative\ndeep learning for the arts. Is it possible to use copyrighted works as training\nset for generative models? How do we legally store their copies in order to\nperform the training process? And then, who (if someone) will own the copyright\non the generated data? We try to answer these questions considering the law in\nforce in both US and EU and the future alternatives, trying to define a set of\nguidelines for artists and developers working on deep learning generated art.",
          "link": "http://arxiv.org/abs/2105.09266",
          "publishedOn": "2021-06-23T01:48:42.362Z",
          "wordCount": 630,
          "title": "Copyright in Generative Deep Learning. (arXiv:2105.09266v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zinc Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hutchin Huang</a>",
          "description": "Hateful content detection is one of the areas where deep learning can and\nshould make a significant difference. The Hateful Memes Challenge from Facebook\nhelps fulfill such potential by challenging the contestants to detect hateful\nspeech in multi-modal memes using deep learning algorithms. In this paper, we\nutilize multi-modal, pre-trained models VilBERT and Visual BERT. We improved\nmodels' performance by adding training datasets generated from data\naugmentation. Enlarging the training data set helped us get a more than 2%\nboost in terms of AUROC with the Visual BERT model. Our approach achieved\n0.7439 AUROC along with an accuracy of 0.7037 on the challenge's test set,\nwhich revealed remarkable progress.",
          "link": "http://arxiv.org/abs/2105.13132",
          "publishedOn": "2021-06-23T01:48:42.356Z",
          "wordCount": 591,
          "title": "Enhance Multimodal Model Performance with Data Augmentation: Facebook Hateful Meme Challenge Solution. (arXiv:2105.13132v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11170",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Song_Y/0/1/0/all/0/1\">Yonghao Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_X/0/1/0/all/0/1\">Xueyu Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1\">Lie Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1\">Longhan Xie</a>",
          "description": "At present, people usually use some methods based on convolutional neural\nnetworks (CNNs) for Electroencephalograph (EEG) decoding. However, CNNs have\nlimitations in perceiving global dependencies, which is not adequate for common\nEEG paradigms with a strong overall relationship. Regarding this issue, we\npropose a novel EEG decoding method that mainly relies on the attention\nmechanism. The EEG data is firstly preprocessed and spatially filtered. And\nthen, we apply attention transforming on the feature-channel dimension so that\nthe model can enhance more relevant spatial features. The most crucial step is\nto slice the data in the time dimension for attention transforming, and finally\nobtain a highly distinguishable representation. At this time, global averaging\npooling and a simple fully-connected layer are used to classify different\ncategories of EEG data. Experiments on two public datasets indicate that the\nstrategy of attention transforming effectively utilizes spatial and temporal\nfeatures. And we have reached the level of the state-of-the-art in\nmulti-classification of EEG, with fewer parameters. As far as we know, it is\nthe first time that a detailed and complete method based on the transformer\nidea has been proposed in this field. It has good potential to promote the\npracticality of brain-computer interface (BCI). The source code can be found\nat: \\textit{https://github.com/anranknight/EEG-Transformer}.",
          "link": "http://arxiv.org/abs/2106.11170",
          "publishedOn": "2021-06-23T01:48:42.350Z",
          "wordCount": 655,
          "title": "Transformer-based Spatial-Temporal Feature Learning for EEG Decoding. (arXiv:2106.11170v1 [eess.SP] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Buchel_J/0/1/0/all/0/1\">Julian B&#xfc;chel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faber_F/0/1/0/all/0/1\">Fynn Faber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muir_D/0/1/0/all/0/1\">Dylan R. Muir</a>",
          "description": "Neuromorphic neural network processors, in the form of compute-in-memory\ncrossbar arrays of memristors, or in the form of subthreshold analog and\nmixed-signal ASICs, promise enormous advantages in compute density and energy\nefficiency for NN-based ML tasks. However, these technologies are prone to\ncomputational non-idealities, due to process variation and intrinsic device\nphysics. This degrades the task performance of networks deployed to the\nprocessor, by introducing parameter noise into the deployed model. While it is\npossible to calibrate each device, or train networks individually for each\nprocessor, these approaches are expensive and impractical for commercial\ndeployment. Alternative methods are therefore needed to train networks that are\ninherently robust against parameter variation, as a consequence of network\narchitecture and parameters. We present a new adversarial network optimisation\nalgorithm that attacks network parameters during training, and promotes robust\nperformance during inference in the face of parameter variation. Our approach\nintroduces a regularization term penalising the susceptibility of a network to\nweight perturbation. We compare against previous approaches for producing\nparameter insensitivity such as dropout, weight smoothing and introducing\nparameter noise during training. We show that our approach produces models that\nare more robust to targeted parameter variation, and equally robust to random\nparameter variation. Our approach finds minima in flatter locations in the\nweight-loss landscape compared with other approaches, highlighting that the\nnetworks found by our technique are less sensitive to parameter perturbation.\nOur work provides an approach to deploy neural network architectures to\ninference devices that suffer from computational non-idealities, with minimal\nloss of performance. ...",
          "link": "http://arxiv.org/abs/2106.05009",
          "publishedOn": "2021-06-23T01:48:42.334Z",
          "wordCount": 699,
          "title": "Network insensitivity to parameter noise via adversarial regularization. (arXiv:2106.05009v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manduchi_L/0/1/0/all/0/1\">Laura Manduchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_Cheong_K/0/1/0/all/0/1\">Kieran Chin-Cheong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_H/0/1/0/all/0/1\">Holger Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wellmann_S/0/1/0/all/0/1\">Sven Wellmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1\">Julia E. Vogt</a>",
          "description": "Constrained clustering has gained significant attention in the field of\nmachine learning as it can leverage prior information on a growing amount of\nonly partially labeled data. Following recent advances in deep generative\nmodels, we propose a novel framework for constrained clustering that is\nintuitive, interpretable, and can be trained efficiently in the framework of\nstochastic gradient variational inference. By explicitly integrating domain\nknowledge in the form of probabilistic relations, our proposed model (DC-GMM)\nuncovers the underlying distribution of data conditioned on prior clustering\npreferences, expressed as pairwise constraints. These constraints guide the\nclustering process towards a desirable partition of the data by indicating\nwhich samples should or should not belong to the same cluster. We provide\nextensive experiments to demonstrate that DC-GMM shows superior clustering\nperformances and robustness compared to state-of-the-art deep constrained\nclustering methods on a wide range of data sets. We further demonstrate the\nusefulness of our approach on two challenging real-world applications.",
          "link": "http://arxiv.org/abs/2106.06385",
          "publishedOn": "2021-06-23T01:48:42.318Z",
          "wordCount": 605,
          "title": "Deep Conditional Gaussian Mixture Model for Constrained Clustering. (arXiv:2106.06385v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07467",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yeung_M/0/1/0/all/0/1\">Michael Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sala_E/0/1/0/all/0/1\">Evis Sala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1\">Leonardo Rundo</a>",
          "description": "Background: Colonoscopy remains the gold-standard screening for colorectal\ncancer. However, significant miss rates for polyps have been reported,\nparticularly when there are multiple small adenomas. This presents an\nopportunity to leverage computer-aided systems to support clinicians and reduce\nthe number of polyps missed.\n\nMethod: In this work we introduce the Focus U-Net, a novel dual\nattention-gated deep neural network, which combines efficient spatial and\nchannel-based attention into a single Focus Gate module to encourage selective\nlearning of polyp features. The Focus U-Net further incorporates short-range\nskip connections and deep supervision. Furthermore, we introduce the Hybrid\nFocal loss, a new compound loss function based on the Focal loss and Focal\nTversky loss, to handle class-imbalanced image segmentation. For our\nexperiments, we selected five public datasets containing images of polyps\nobtained during optical colonoscopy: CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB,\nETIS-Larib PolypDB and EndoScene test set. To evaluate model performance, we\nuse the Dice similarity coefficient (DSC) and Intersection over Union (IoU)\nmetrics.\n\nResults: Our model achieves state-of-the-art results for both CVC-ClinicDB\nand Kvasir-SEG, with a mean DSC of 0.941 and 0.910, respectively. When\nevaluated on a combination of five public polyp datasets, our model similarly\nachieves state-of-the-art results with a mean DSC of 0.878 and mean IoU of\n0.809, a 14% and 15% improvement over the previous state-of-the-art results of\n0.768 and 0.702, respectively.\n\nConclusions: This study shows the potential for deep learning to provide fast\nand accurate polyp segmentation results for use during colonoscopy. The Focus\nU-Net may be adapted for future use in newer non-invasive screening and more\nbroadly to other biomedical image segmentation tasks involving class imbalance\nand requiring efficiency.",
          "link": "http://arxiv.org/abs/2105.07467",
          "publishedOn": "2021-06-23T01:48:42.310Z",
          "wordCount": 745,
          "title": "Focus U-Net: A novel dual attention-gated CNN for polyp segmentation during colonoscopy. (arXiv:2105.07467v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Wei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Spiking Neural Networks (SNNs) have been attached great importance due to\ntheir biological plausibility and high energy-efficiency on neuromorphic chips.\nAs these chips are usually resource-constrained, the compression of SNNs is\nthus crucial along the road of practical use of SNNs. Most existing methods\ndirectly apply pruning approaches in artificial neural networks (ANNs) to SNNs,\nwhich ignore the difference between ANNs and SNNs, thus limiting the\nperformance of the pruned SNNs. Besides, these methods are only suitable for\nshallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination\nin the neural system, we propose gradient rewiring (Grad R), a joint learning\nalgorithm of connectivity and weight for SNNs, that enables us to seamlessly\noptimize network structure without retraining. Our key innovation is to\nredefine the gradient to a new synaptic parameter, allowing better exploration\nof network structures by taking full advantage of the competition between\npruning and regrowth of connections. The experimental results show that the\nproposed method achieves minimal loss of SNNs' performance on MNIST and\nCIFAR-10 dataset so far. Moreover, it reaches a $\\sim$3.5% accuracy loss under\nunprecedented 0.73% connectivity, which reveals remarkable structure refining\ncapability in SNNs. Our work suggests that there exists extremely high\nredundancy in deep SNNs. Our codes are available at\nhttps://github.com/Yanqi-Chen/Gradient-Rewiring.",
          "link": "http://arxiv.org/abs/2105.04916",
          "publishedOn": "2021-06-23T01:48:42.302Z",
          "wordCount": 722,
          "title": "Pruning of Deep Spiking Neural Networks through Gradient Rewiring. (arXiv:2105.04916v3 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Javed_Z/0/1/0/all/0/1\">Zaynah Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">Daniel S. Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Satvik Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jerry Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishna_A/0/1/0/all/0/1\">Ashwin Balakrishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrik_M/0/1/0/all/0/1\">Marek Petrik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1\">Anca D. Dragan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1\">Ken Goldberg</a>",
          "description": "The difficulty in specifying rewards for many real-world problems has led to\nan increased focus on learning rewards from human feedback, such as\ndemonstrations. However, there are often many different reward functions that\nexplain the human feedback, leaving agents with uncertainty over what the true\nreward function is. While most policy optimization approaches handle this\nuncertainty by optimizing for expected performance, many applications demand\nrisk-averse behavior. We derive a novel policy gradient-style robust\noptimization approach, PG-BROIL, that optimizes a soft-robust objective that\nbalances expected performance and risk. To the best of our knowledge, PG-BROIL\nis the first policy optimization algorithm robust to a distribution of reward\nhypotheses which can scale to continuous MDPs. Results suggest that PG-BROIL\ncan produce a family of behaviors ranging from risk-neutral to risk-averse and\noutperforms state-of-the-art imitation learning algorithms when learning from\nambiguous demonstrations by hedging against uncertainty, rather than seeking to\nuniquely identify the demonstrator's reward function.",
          "link": "http://arxiv.org/abs/2106.06499",
          "publishedOn": "2021-06-23T01:48:42.292Z",
          "wordCount": 626,
          "title": "Policy Gradient Bayesian Robust Optimization for Imitation Learning. (arXiv:2106.06499v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gower_R/0/1/0/all/0/1\">Robert M. Gower</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Defazio_A/0/1/0/all/0/1\">Aaron Defazio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1\">Michael Rabbat</a>",
          "description": "We propose a new stochastic gradient method that uses recorded past loss\nvalues to reduce the variance. Our method can be interpreted as a new\nstochastic variant of the Polyak Stepsize that converges globally without\nassuming interpolation. Our method introduces auxiliary variables, one for each\ndata point, that track the loss value for each data point. We provide a global\nconvergence theory for our method by showing that it can be interpreted as a\nspecial variant of online SGD. The new method only stores a single scalar per\ndata point, opening up new applications for variance reduction where memory is\nthe bottleneck.",
          "link": "http://arxiv.org/abs/2106.11851",
          "publishedOn": "2021-06-23T01:48:42.250Z",
          "wordCount": 556,
          "title": "Stochastic Polyak Stepsize with a Moving Target. (arXiv:2106.11851v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gaherwar_P/0/1/0/all/0/1\">Prutha Gaherwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Shraddha Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khengare_R/0/1/0/all/0/1\">Rahul Khengare</a>",
          "description": "With an increase in mobile and camera devices' popularity, digital content in\nthe form of images has increased drastically. As personal life is being\ncontinuously documented in pictures, the risk of losing it to eavesdroppers is\na matter of grave concern. Secondary storage is the most preferred medium for\nthe storage of personal and other images. Our work is concerned with the\nsecurity of such images. While encryption is the best way to ensure image\nsecurity, full encryption and decryption is a computationally-intensive\nprocess. Moreover, as cameras are getting better every day, image quality, and\nthus, the pixel density has increased considerably. The increased pixel density\nmakes encryption and decryption more expensive. We, therefore, delve into\nselective encryption and selective blurring based on the region of interest.\nInstead of encrypting or blurring the entire photograph, we only encode\nselected regions of the image. We present a comparative analysis of the partial\nand full encryption of the photos. This kind of encoding will help us lower the\nencryption overhead without compromising security. The applications utilizing\nthis technique will become more usable due to the reduction in the decryption\ntime. Additionally, blurred images being more readable than encrypted ones,\nallowed us to define the level of security. We leverage the machine learning\nalgorithms like Mask-RCNN (Region-based convolutional neural network) and YOLO\n(You Only Look Once) to select the region of interest. These algorithms have\nset new benchmarks for object recognition. We develop an end to end system to\ndemonstrate our idea of selective encryption.",
          "link": "http://arxiv.org/abs/2106.11770",
          "publishedOn": "2021-06-23T01:48:42.233Z",
          "wordCount": 689,
          "title": "SISA: Securing Images by Selective Alteration. (arXiv:2106.11770v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jordana_A/0/1/0/all/0/1\">Armand Jordana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carpentier_J/0/1/0/all/0/1\">Justin Carpentier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Righetti_L/0/1/0/all/0/1\">Ludovic Righetti</a>",
          "description": "Modeling dynamical systems plays a crucial role in capturing and\nunderstanding complex physical phenomena. When physical models are not\nsufficiently accurate or hardly describable by analytical formulas, one can use\ngeneric function approximators such as neural networks to capture the system\ndynamics directly from sensor measurements. As for now, current methods to\nlearn the parameters of these neural networks are highly sensitive to the\ninherent instability of most dynamical systems of interest, which in turn\nprevents the study of very long sequences. In this work, we introduce a generic\nand scalable method based on multiple shooting to learn latent representations\nof indirectly observed dynamical systems. We achieve state-of-the-art\nperformances on systems observed directly from raw images. Further, we\ndemonstrate that our method is robust to noisy measurements and can handle\ncomplex dynamical systems, such as chaotic ones.",
          "link": "http://arxiv.org/abs/2106.11712",
          "publishedOn": "2021-06-23T01:48:42.226Z",
          "wordCount": 573,
          "title": "Learning Dynamical Systems from Noisy Sensor Measurements using Multiple Shooting. (arXiv:2106.11712v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maile_K/0/1/0/all/0/1\">Kaitlin Maile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lecarpentier_E/0/1/0/all/0/1\">Erwan Lecarpentier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luga_H/0/1/0/all/0/1\">Herv&#xe9; Luga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1\">Dennis G. Wilson</a>",
          "description": "Differentiable Architecture Search (DARTS) is a recently proposed neural\narchitecture search (NAS) method based on a differentiable relaxation. Due to\nits success, numerous variants analyzing and improving parts of the DARTS\nframework have recently been proposed. By considering the problem as a\nconstrained bilevel optimization, we propose and analyze three improvements to\narchitectural weight competition, update scheduling, and regularization towards\ndiscretization. First, we introduce a new approach to the activation of\narchitecture weights, which prevents confounding competition within an edge and\nallows for fair comparison across edges to aid in discretization. Next, we\npropose a dynamic schedule based on per-minibatch network information to make\narchitecture updates more informed. Finally, we consider two regularizations,\nbased on proximity to discretization and the Alternating Directions Method of\nMultipliers (ADMM) algorithm, to promote early discretization. Our results show\nthat this new activation scheme reduces final architecture size and the\nregularizations improve reliability in search results while maintaining\ncomparable performance to state-of-the-art in NAS, especially when used with\nour new dynamic informed schedule.",
          "link": "http://arxiv.org/abs/2106.11655",
          "publishedOn": "2021-06-23T01:48:42.219Z",
          "wordCount": 602,
          "title": "On Constrained Optimization in Differentiable Neural Architecture Search. (arXiv:2106.11655v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1\">Arun S. Maiya</a>",
          "description": "The vast majority of existing methods and systems for causal inference assume\nthat all variables under consideration are categorical or numerical (e.g.,\ngender, price, blood pressure, enrollment). In this paper, we present\nCausalNLP, a toolkit for inferring causality from observational data that\nincludes text in addition to traditional numerical and categorical variables.\nCausalNLP employs the use of meta-learners for treatment effect estimation and\nsupports using raw text and its linguistic properties as both a treatment and a\n\"controlled-for\" variable (e.g., confounder). The library is open-source and\navailable at: https://github.com/amaiya/causalnlp.",
          "link": "http://arxiv.org/abs/2106.08043",
          "publishedOn": "2021-06-23T01:48:41.861Z",
          "wordCount": 544,
          "title": "CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hammerbacher_T/0/1/0/all/0/1\">Tom Hammerbacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_Hegermann_M/0/1/0/all/0/1\">Markus Lange-Hegermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platz_G/0/1/0/all/0/1\">Gorden Platz</a>",
          "description": "Digitalization leads to data transparency for production systems that we can\nbenefit from with data-driven analysis methods like neural networks. For\nexample, automated anomaly detection enables saving resources and optimizing\nthe production. We study using rarely occurring information about labeled\nanomalies into Variational Autoencoder neural network structures to overcome\ninformation deficits of supervised and unsupervised approaches. This method\noutperforms all other models in terms of accuracy, precision, and recall. We\nevaluate the following methods: Principal Component Analysis, Isolation Forest,\nClassifying Neural Networks, and Variational Autoencoders on seven time series\ndatasets to find the best performing detection methods. We extend this idea to\ninclude more infrequently occurring meta information about production\nprocesses. This use of sparse labels, both of anomalies or production data,\nallows to harness any additional information available for increasing anomaly\ndetection performance.",
          "link": "http://arxiv.org/abs/2103.12998",
          "publishedOn": "2021-06-23T01:48:41.855Z",
          "wordCount": 605,
          "title": "Including Sparse Production Knowledge into Variational Autoencoders to Increase Anomaly Detection Reliability. (arXiv:2103.12998v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.07092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ni_R/0/1/0/all/0/1\">Renkun Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharaf_A/0/1/0/all/0/1\">Amr Sharaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1\">Kezhi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "Conventional image classifiers are trained by randomly sampling mini-batches\nof images. To achieve state-of-the-art performance, practitioners use\nsophisticated data augmentation schemes to expand the amount of training data\navailable for sampling. In contrast, meta-learning algorithms sample support\ndata, query data, and tasks on each training step. In this complex sampling\nscenario, data augmentation can be used not only to expand the number of images\navailable per class, but also to generate entirely new classes/tasks. We\nsystematically dissect the meta-learning pipeline and investigate the distinct\nways in which data augmentation can be integrated at both the image and class\nlevels. Our proposed meta-specific data augmentation significantly improves the\nperformance of meta-learners on few-shot classification benchmarks.",
          "link": "http://arxiv.org/abs/2010.07092",
          "publishedOn": "2021-06-23T01:48:41.849Z",
          "wordCount": 583,
          "title": "Data Augmentation for Meta-Learning. (arXiv:2010.07092v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1\">Pietro Barbiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1\">Gabriele Ciravegna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1\">Francesco Giannini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1\">Marco Gori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1\">Stefano Melacci</a>",
          "description": "Explainable artificial intelligence has rapidly emerged since lawmakers have\nstarted requiring interpretable models for safety-critical domains.\nConcept-based neural networks have arisen as explainable-by-design methods as\nthey leverage human-understandable symbols (i.e. concepts) to predict class\nmemberships. However, most of these approaches focus on the identification of\nthe most relevant concepts but do not provide concise, formal explanations of\nhow such concepts are leveraged by the classifier to make predictions. In this\npaper, we propose a novel end-to-end differentiable approach enabling the\nextraction of logic explanations from neural networks using the formalism of\nFirst-Order Logic. The method relies on an entropy-based criterion which\nautomatically identifies the most relevant concepts. We consider four different\ncase studies to demonstrate that: (i) this entropy-based criterion enables the\ndistillation of concise logic explanations in safety-critical domains from\nclinical data to computer vision; (ii) the proposed approach outperforms\nstate-of-the-art white-box models in terms of classification accuracy.",
          "link": "http://arxiv.org/abs/2106.06804",
          "publishedOn": "2021-06-23T01:48:41.843Z",
          "wordCount": 622,
          "title": "Entropy-based Logic Explanations of Neural Networks. (arXiv:2106.06804v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1\">Ayaan Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1\">Viraaj Reddi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giallanza_T/0/1/0/all/0/1\">Tyler Giallanza</a>",
          "description": "Early detection of suicidal ideation in depressed individuals can allow for\nadequate medical attention and support, which in many cases is life-saving.\nRecent NLP research focuses on classifying, from a given piece of text, if an\nindividual is suicidal or clinically healthy. However, there have been no major\nattempts to differentiate between depression and suicidal ideation, which is an\nimportant clinical challenge. Due to the scarce availability of EHR data,\nsuicide notes, or other similar verified sources, web query data has emerged as\na promising alternative. Online sources, such as Reddit, allow for anonymity\nthat prompts honest disclosure of symptoms, making it a plausible source even\nin a clinical setting. However, these online datasets also result in lower\nperformance, which can be attributed to the inherent noise in web-scraped\nlabels, which necessitates a noise-removal process. Thus, we propose SDCNL, a\nsuicide versus depression classification method through a deep learning\napproach. We utilize online content from Reddit to train our algorithm, and to\nverify and correct noisy labels, we propose a novel unsupervised label\ncorrection method which, unlike previous work, does not require prior noise\ndistribution information. Our extensive experimentation with multiple deep word\nembedding models and classifiers display the strong performance of the method\nin anew, challenging classification application. We make our code and dataset\navailable at https://github.com/ayaanzhaque/SDCNL",
          "link": "http://arxiv.org/abs/2102.09427",
          "publishedOn": "2021-06-23T01:48:41.837Z",
          "wordCount": 685,
          "title": "Deep Learning for Suicide and Depression Identification with Unsupervised Label Correction. (arXiv:2102.09427v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangrun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangcong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>",
          "description": "Self-supervised learning (especially contrastive learning) has attracted\ngreat interest due to its tremendous potentials in learning discriminative\nrepresentations in an unsupervised manner. Despite the acknowledged successes,\nexisting contrastive learning methods suffer from very low learning efficiency,\ne.g., taking about ten times more training epochs than supervised learning for\ncomparable recognition accuracy. In this paper, we discover two contradictory\nphenomena in contrastive learning that we call under-clustering and\nover-clustering problems, which are major obstacles to learning efficiency.\nUnder-clustering means that the model cannot efficiently learn to discover the\ndissimilarity between inter-class samples when the negative sample pairs for\ncontrastive learning are insufficient to differentiate all the actual object\ncategories. Over-clustering implies that the model cannot efficiently learn the\nfeature representation from excessive negative sample pairs, which enforces the\nmodel to over-cluster samples of the same actual categories into different\nclusters. To simultaneously overcome these two problems, we propose a novel\nself-supervised learning framework using a median triplet loss. Precisely, we\nemploy a triplet loss tending to maximize the relative distance between the\npositive pair and negative pairs to address the under-clustering problem; and\nwe construct the negative pair by selecting the negative sample of a median\nsimilarity score from all negative samples to avoid the over-clustering\nproblem, guaranteed by the Bernoulli Distribution model. We extensively\nevaluate our proposed framework in several large-scale benchmarks (e.g.,\nImageNet, SYSU-30k, and COCO). The results demonstrate the superior performance\n(e.g., the learning efficiency) of our model over the latest state-of-the-art\nmethods by a clear margin. Codes available at:\nhttps://github.com/wanggrun/triplet.",
          "link": "http://arxiv.org/abs/2104.08760",
          "publishedOn": "2021-06-23T01:48:41.830Z",
          "wordCount": 739,
          "title": "Towards Solving Inefficiency of Self-supervised Representation Learning. (arXiv:2104.08760v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02601",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Kotary_J/0/1/0/all/0/1\">James Kotary</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fioretto_F/0/1/0/all/0/1\">Ferdinando Fioretto</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hentenryck_P/0/1/0/all/0/1\">Pascal Van Hentenryck</a>",
          "description": "Optimization problems are ubiquitous in our societies and are present in\nalmost every segment of the economy. Most of these optimization problems are\nNP-hard and computationally demanding, often requiring approximate solutions\nfor large-scale instances. Machine learning frameworks that learn to\napproximate solutions to such hard optimization problems are a potentially\npromising avenue to address these difficulties, particularly when many closely\nrelated problem instances must be solved repeatedly. Supervised learning\nframeworks can train a model using the outputs of pre-solved instances.\nHowever, when the outputs are themselves approximations, when the optimization\nproblem has symmetric solutions, and/or when the solver uses randomization,\nsolutions to closely related instances may exhibit large differences and the\nlearning task can become inherently more difficult. This paper demonstrates\nthis critical challenge, connects the volatility of the training data to the\nability of a model to approximate it, and proposes a method for producing\n(exact or approximate) solutions to optimization problems that are more\namenable to supervised learning tasks. The effectiveness of the method is\ntested on hard non-linear nonconvex and discrete combinatorial problems.",
          "link": "http://arxiv.org/abs/2106.02601",
          "publishedOn": "2021-06-23T01:48:41.824Z",
          "wordCount": 628,
          "title": "Learning Hard Optimization Problems: A Data Generation Perspective. (arXiv:2106.02601v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulyono_H/0/1/0/all/0/1\">Hermawan Mulyono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1\">Desmond Chan</a>",
          "description": "Climate change has largely impacted our daily lives. As one of its\nconsequences, we are experiencing more wildfires. In the year 2020, wildfires\nburned a record number of 8,888,297 acres in the US. To awaken people's\nattention to climate change, and to visualize the current risk of wildfires, We\ndeveloped RtFPS, \"Real-Time Fire Prediction System\". It provides a real-time\nprediction visualization of wildfire risk at specific locations base on a\nMachine Learning model. It also provides interactive map features that show the\nhistorical wildfire events with environmental info.",
          "link": "http://arxiv.org/abs/2105.10880",
          "publishedOn": "2021-06-23T01:48:41.816Z",
          "wordCount": 577,
          "title": "RtFPS: An Interactive Map that Visualizes and Predicts Wildfires in the US. (arXiv:2105.10880v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jingxiu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shepperd_M/0/1/0/all/0/1\">Martin Shepperd</a>",
          "description": "Context: Software engineering researchers have undertaken many experiments\ninvestigating the potential of software defect prediction algorithms.\nUnfortunately, some widely used performance metrics are known to be\nproblematic, most notably F1, but nevertheless F1 is widely used.\n\nObjective: To investigate the potential impact of using F1 on the validity of\nthis large body of research.\n\nMethod: We undertook a systematic review to locate relevant experiments and\nthen extract all pairwise comparisons of defect prediction performance using F1\nand the un-biased Matthews correlation coefficient (MCC).\n\nResults: We found a total of 38 primary studies. These contain 12,471 pairs\nof results. Of these, 21.95% changed direction when the MCC metric is used\ninstead of the biased F1 metric. Unfortunately, we also found evidence\nsuggesting that F1 remains widely used in software defect prediction research.\n\nConclusions: We reiterate the concerns of statisticians that the F1 is a\nproblematic metric outside of an information retrieval context, since we are\nconcerned about both classes (defect-prone and not defect-prone units). This\ninappropriate usage has led to a substantial number (more than one fifth) of\nerroneous (in terms of direction) results. Therefore we urge researchers to (i)\nuse an unbiased metric and (ii) publish detailed results including confusion\nmatrices such that alternative analyses become possible.",
          "link": "http://arxiv.org/abs/2103.10201",
          "publishedOn": "2021-06-23T01:48:41.809Z",
          "wordCount": 721,
          "title": "The impact of using biased performance metrics on software defect prediction research. (arXiv:2103.10201v4 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1\">MohammadJavad Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_S/0/1/0/all/0/1\">Sheldon M Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyu Zhang</a>",
          "description": "We consider the problem of finding, through adaptive sampling, which of $n$\noptions (arms) has the largest mean. Our objective is to determine a rule which\nidentifies the best arm with a fixed minimum confidence using as few\nobservations as possible, i.e. this is a fixed-confidence (FC) best arm\nidentification (BAI) in multi-armed bandits. We study such problems under the\nBayesian setting with both Bernoulli and Gaussian arms. We propose to use the\nclassical \"vector at a time\" (VT) rule, which samples each remaining arm once\nin each round. We show how VT can be implemented and analyzed in our Bayesian\nsetting and be improved by early elimination. Our analysis show that these\nalgorithms guarantee an optimal strategy under the prior. We also propose and\nanalyze a variant of the classical \"play the winner\" (PW) algorithm. Numerical\nresults show that these rules compare favorably with state-of-art algorithms.",
          "link": "http://arxiv.org/abs/2106.06848",
          "publishedOn": "2021-06-23T01:48:41.767Z",
          "wordCount": 619,
          "title": "Guaranteed Fixed-Confidence Best Arm Identification in Multi-Armed Bandits: Simple Sequential Elimination Algorithms. (arXiv:2106.06848v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Ting Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1\">C&#xe9;dric F&#xe9;votte</a>",
          "description": "We consider an adversarially-trained version of the nonnegative matrix\nfactorization, a popular latent dimensionality reduction technique. In our\nformulation, an attacker adds an arbitrary matrix of bounded norm to the given\ndata matrix. We design efficient algorithms inspired by adversarial training to\noptimize for dictionary and coefficient matrices with enhanced generalization\nabilities. Extensive simulations on synthetic and benchmark datasets\ndemonstrate the superior predictive performance on matrix completion tasks of\nour proposed method compared to state-of-the-art competitors, including other\nvariants of adversarial nonnegative matrix factorization.",
          "link": "http://arxiv.org/abs/2104.04757",
          "publishedOn": "2021-06-23T01:48:41.757Z",
          "wordCount": 552,
          "title": "Adversarially-Trained Nonnegative Matrix Factorization. (arXiv:2104.04757v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joanne Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamichhane_B/0/1/0/all/0/1\">Bishal Lamichhane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Zeev_D/0/1/0/all/0/1\">Dror Ben-Zeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_A/0/1/0/all/0/1\">Andrew Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sano_A/0/1/0/all/0/1\">Akane Sano</a>",
          "description": "We aim to develop clustering models to obtain behavioral representations from\ncontinuous multimodal mobile sensing data towards relapse prediction tasks. The\nidentified clusters could represent different routine behavioral trends related\nto daily living of patients as well as atypical behavioral trends associated\nwith impending relapse.\n\nWe used the mobile sensing data obtained in the CrossCheck project for our\nanalysis. Continuous data from six different mobile sensing-based modalities\n(e.g. ambient light, sound/conversation, acceleration etc.) obtained from a\ntotal of 63 schizophrenia patients, each monitored for up to a year, were used\nfor the clustering models and relapse prediction evaluation. Two clustering\nmodels, Gaussian Mixture Model (GMM) and Partition Around Medoids (PAM), were\nused to obtain behavioral representations from the mobile sensing data. The\nfeatures obtained from the clustering models were used to train and evaluate a\npersonalized relapse prediction model using Balanced Random Forest. The\npersonalization was done by identifying optimal features for a given patient\nbased on a personalization subset consisting of other patients who are of\nsimilar age.\n\nThe clusters identified using the GMM and PAM models were found to represent\ndifferent behavioral patterns (such as clusters representing sedentary days,\nactive but with low communications days, etc.). Significant changes near the\nrelapse periods were seen in the obtained behavioral representation features\nfrom the clustering models. The clustering model based features, together with\nother features characterizing the mobile sensing data, resulted in an F2 score\nof 0.24 for the relapse prediction task in a leave-one-patient-out evaluation\nsetting. This obtained F2 score is significantly higher than a random\nclassification baseline with an average F2 score of 0.042.",
          "link": "http://arxiv.org/abs/2106.11487",
          "publishedOn": "2021-06-23T01:48:41.748Z",
          "wordCount": 709,
          "title": "Routine Clustering of Mobile Sensor Data Facilitates Psychotic Relapse Prediction in Schizophrenia Patients. (arXiv:2106.11487v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14602",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chettri_B/0/1/0/all/0/1\">Bhusan Chettri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hautamaki_R/0/1/0/all/0/1\">Rosa Gonz&#xe1;lez Hautam&#xe4;ki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sahidullah_M/0/1/0/all/0/1\">Md Sahidullah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kinnunen_T/0/1/0/all/0/1\">Tomi Kinnunen</a>",
          "description": "Voice anti-spoofing aims at classifying a given utterance either as a\nbonafide human sample, or a spoofing attack (e.g. synthetic or replayed\nsample). Many anti-spoofing methods have been proposed but most of them fail to\ngeneralize across domains (corpora) -- and we do not know \\emph{why}. We\noutline a novel interpretative framework for gauging the impact of data quality\nupon anti-spoofing performance. Our within- and between-domain experiments pool\ndata from seven public corpora and three anti-spoofing methods based on\nGaussian mixture and convolutive neural network models. We assess the impacts\nof long-term spectral information, speaker population (through x-vector speaker\nembeddings), signal-to-noise ratio, and selected voice quality features.",
          "link": "http://arxiv.org/abs/2103.14602",
          "publishedOn": "2021-06-23T01:48:41.733Z",
          "wordCount": 579,
          "title": "Data Quality as Predictor of Voice Anti-Spoofing Generalization. (arXiv:2103.14602v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11612",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiafan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We study reinforcement learning (RL) with linear function approximation.\nExisting algorithms for this problem only have high-probability regret and/or\nProbably Approximately Correct (PAC) sample complexity guarantees, which cannot\nguarantee the convergence to the optimal policy. In this paper, in order to\novercome the limitation of existing algorithms, we propose a new algorithm\ncalled FLUTE, which enjoys uniform-PAC convergence to the optimal policy with\nhigh probability. The uniform-PAC guarantee is the strongest possible guarantee\nfor reinforcement learning in the literature, which can directly imply both PAC\nand high probability regret bounds, making our algorithm superior to all\nexisting algorithms with linear function approximation. At the core of our\nalgorithm is a novel minimax value function estimator and a multi-level\npartition scheme to select the training samples from historical observations.\nBoth of these techniques are new and of independent interest.",
          "link": "http://arxiv.org/abs/2106.11612",
          "publishedOn": "2021-06-23T01:48:41.718Z",
          "wordCount": 582,
          "title": "Uniform-PAC Bounds for Reinforcement Learning with Linear Function Approximation. (arXiv:2106.11612v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leo_J/0/1/0/all/0/1\">Justin Leo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1\">Jugal Kalita</a>",
          "description": "Most modern neural networks for classification fail to take into account the\nconcept of the unknown. Trained neural networks are usually tested in an\nunrealistic scenario with only examples from a closed set of known classes. In\nan attempt to develop a more realistic model, the concept of working in an open\nset environment has been introduced. This in turn leads to the concept of\nincremental learning where a model with its own architecture and initial\ntrained set of data can identify unknown classes during the testing phase and\nautonomously update itself if evidence of a new class is detected. Some\nproblems that arise in incremental learning are inefficient use of resources to\nretrain the classifier repeatedly and the decrease of classification accuracy\nas multiple classes are added over time. This process of instantiating new\nclasses is repeated as many times as necessary, accruing errors. To address\nthese problems, this paper proposes the Classification Confidence Threshold\napproach to prime neural networks for incremental learning to keep accuracies\nhigh by limiting forgetting. A lean method is also used to reduce resources\nused in the retraining of the neural network. The proposed method is based on\nthe idea that a network is able to incrementally learn a new class even when\nexposed to a limited number samples associated with the new class. This method\ncan be applied to most existing neural networks with minimal changes to network\narchitecture.",
          "link": "http://arxiv.org/abs/2106.11437",
          "publishedOn": "2021-06-23T01:48:41.688Z",
          "wordCount": 683,
          "title": "Incremental Deep Neural Network Learning using Classification Confidence Thresholding. (arXiv:2106.11437v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11519",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dann_C/0/1/0/all/0/1\">Christoph Dann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1\">Yishay Mansour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1\">Mehryar Mohri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekhari_A/0/1/0/all/0/1\">Ayush Sekhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_K/0/1/0/all/0/1\">Karthik Sridharan</a>",
          "description": "There have been many recent advances on provably efficient Reinforcement\nLearning (RL) in problems with rich observation spaces. However, all these\nworks share a strong realizability assumption about the optimal value function\nof the true MDP. Such realizability assumptions are often too strong to hold in\npractice. In this work, we consider the more realistic setting of agnostic RL\nwith rich observation spaces and a fixed class of policies $\\Pi$ that may not\ncontain any near-optimal policy. We provide an algorithm for this setting whose\nerror is bounded in terms of the rank $d$ of the underlying MDP. Specifically,\nour algorithm enjoys a sample complexity bound of $\\widetilde{O}\\left((H^{4d}\nK^{3d} \\log |\\Pi|)/\\epsilon^2\\right)$ where $H$ is the length of episodes, $K$\nis the number of actions and $\\epsilon>0$ is the desired sub-optimality. We\nalso provide a nearly matching lower bound for this agnostic setting that shows\nthat the exponential dependence on rank is unavoidable, without further\nassumptions.",
          "link": "http://arxiv.org/abs/2106.11519",
          "publishedOn": "2021-06-23T01:48:41.679Z",
          "wordCount": 600,
          "title": "Agnostic Reinforcement Learning with Low-Rank MDPs and Rich Observations. (arXiv:2106.11519v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.10763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kruse_J/0/1/0/all/0/1\">Jakob Kruse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardizzone_L/0/1/0/all/0/1\">Lynton Ardizzone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1\">Carsten Rother</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothe_U/0/1/0/all/0/1\">Ullrich K&#xf6;the</a>",
          "description": "Recent work demonstrated that flow-based invertible neural networks are\npromising tools for solving ambiguous inverse problems. Following up on this,\nwe investigate how ten invertible architectures and related models fare on two\nintuitive, low-dimensional benchmark problems, obtaining the best results with\ncoupling layers and simple autoencoders. We hope that our initial efforts\ninspire other researchers to evaluate their invertible architectures in the\nsame setting and put forth additional benchmarks, so our evaluation may\neventually grow into an official community challenge.",
          "link": "http://arxiv.org/abs/2101.10763",
          "publishedOn": "2021-06-23T01:48:41.672Z",
          "wordCount": 558,
          "title": "Benchmarking Invertible Architectures on Inverse Problems. (arXiv:2101.10763v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Solanki_P/0/1/0/all/0/1\">Prashant Solanki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">Kwan Hui Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwood_A/0/1/0/all/0/1\">Aaron Harwood</a>",
          "description": "With the prevalence of online social networking sites (OSNs) and mobile\ndevices, people are increasingly reliant on a variety of OSNs for keeping in\ntouch with family and friends, and using it as a source of information. For\nexample, a user might utilise multiple OSNs for different purposes, such as\nusing Flickr to share holiday pictures with family and friends, and Twitter to\npost short messages about their thoughts. Identifying the same user across\nmultiple OSNs is an important task as this allows us to understand the usage\npatterns of users among different OSNs, make recommendations when a user\nregisters for a new OSN, and various other useful applications. To address this\nproblem, we proposed an algorithm based on the multilayer perceptron using\nvarious types of features, namely: (i) user profile, such as name, location,\ndescription; (ii) temporal distribution of user generated content; and (iii)\nembedding based on user name, real name and description. Using a Twitter and\nFlickr dataset of users and their posting activities, we perform an empirical\nstudy on how these features affect the performance of user identification\nacross the two OSNs and discuss our main findings based on the different\nfeatures.",
          "link": "http://arxiv.org/abs/2106.11815",
          "publishedOn": "2021-06-23T01:48:41.665Z",
          "wordCount": 656,
          "title": "User Identification across Social Networking Sites using User Profiles and Posting Patterns. (arXiv:2106.11815v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McLaughlin_N/0/1/0/all/0/1\">Niall McLaughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rincon_J/0/1/0/all/0/1\">Jesus Martinez del Rincon</a>",
          "description": "Data augmentation has been successfully used in many areas of deep-learning\nto significantly improve model performance. Typically data augmentation\nsimulates realistic variations in data in order to increase the apparent\ndiversity of the training-set. However, for opcode-based malware analysis,\nwhere deep learning methods are already achieving state of the art performance,\nit is not immediately clear how to apply data augmentation. In this paper we\nstudy different methods of data augmentation starting with basic methods using\nfixed transformations and moving to methods that adapt to the data. We propose\na novel data augmentation method based on using an opcode embedding layer\nwithin the network and its corresponding opcode embedding matrix to perform\nadaptive data augmentation during training. To the best of our knowledge this\nis the first paper to carry out a systematic study of different augmentation\nmethods applied to opcode sequence based malware classification.",
          "link": "http://arxiv.org/abs/2106.11821",
          "publishedOn": "2021-06-23T01:48:41.659Z",
          "wordCount": 590,
          "title": "Data Augmentation for Opcode Sequence Based Malware Detection. (arXiv:2106.11821v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2012.04053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen-Yu Wei</a>",
          "description": "We study the stochastic shortest path problem with adversarial costs and\nknown transition, and show that the minimax regret is\n$\\widetilde{O}(\\sqrt{DT^\\star K})$ and $\\widetilde{O}(\\sqrt{DT^\\star SA K})$\nfor the full-information setting and the bandit feedback setting respectively,\nwhere $D$ is the diameter, $T^\\star$ is the expected hitting time of the\noptimal policy, $S$ is the number of states, $A$ is the number of actions, and\n$K$ is the number of episodes. Our results significantly improve upon the\nexisting work of (Rosenberg and Mansour, 2020) which only considers the\nfull-information setting and achieves suboptimal regret. Our work is also the\nfirst to consider bandit feedback with adversarial costs.\n\nOur algorithms are built on top of the Online Mirror Descent framework with a\nvariety of new techniques that might be of independent interest, including an\nimproved multi-scale expert algorithm, a reduction from general stochastic\nshortest path to a special loop-free case, a skewed occupancy measure space,\nand a novel correction term added to the cost estimators. Interestingly, the\nlast two elements reduce the variance of the learner via positive bias and the\nvariance of the optimal policy via negative bias respectively, and having them\nsimultaneously is critical for obtaining the optimal high-probability bound in\nthe bandit feedback setting.",
          "link": "http://arxiv.org/abs/2012.04053",
          "publishedOn": "2021-06-23T01:48:41.640Z",
          "wordCount": 679,
          "title": "Minimax Regret for Stochastic Shortest Path with Adversarial Costs and Known Transition. (arXiv:2012.04053v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirsch_A/0/1/0/all/0/1\">Andreas Kirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1\">Tom Rainforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>",
          "description": "Active Learning is essential for more label-efficient deep learning. Bayesian\nActive Learning has focused on BALD, which reduces model parameter uncertainty.\nHowever, we show that BALD gets stuck on out-of-distribution or junk data that\nis not relevant for the task. We examine a novel *Expected Predictive\nInformation Gain (EPIG)* to deal with distribution shifts of the pool set. EPIG\nreduces the uncertainty of *predictions* on an unlabelled *evaluation set*\nsampled from the test data distribution whose distribution might be different\nto the pool set distribution. Based on this, our new EPIG-BALD acquisition\nfunction for Bayesian Neural Networks selects samples to improve the\nperformance on the test data distribution instead of selecting samples that\nreduce model uncertainty everywhere, including for out-of-distribution regions\nwith low density in the test data distribution. Our method outperforms\nstate-of-the-art Bayesian active learning methods on high-dimensional datasets\nand avoids out-of-distribution junk data in cases where current\nstate-of-the-art methods fail.",
          "link": "http://arxiv.org/abs/2106.11719",
          "publishedOn": "2021-06-23T01:48:41.632Z",
          "wordCount": 589,
          "title": "Active Learning under Pool Set Distribution Shift and Noisy Data. (arXiv:2106.11719v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11531",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Suhang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>",
          "description": "Routing methods in capsule networks often learn a hierarchical relationship\nfor capsules in successive layers, but the intra-relation between capsules in\nthe same layer is less studied, while this intra-relation is a key factor for\nthe semantic understanding in text data. Therefore, in this paper, we introduce\na new capsule network with graph routing to learn both relationships, where\ncapsules in each layer are treated as the nodes of a graph. We investigate\nstrategies to yield adjacency and degree matrix with three different distances\nfrom a layer of capsules, and propose the graph routing mechanism between those\ncapsules. We validate our approach on five text classification datasets, and\nour findings suggest that the approach combining bottom-up routing and top-down\nattention performs the best. Such an approach demonstrates generalization\ncapability across datasets. Compared to the state-of-the-art routing methods,\nthe improvements in accuracy in the five datasets we used were 0.82, 0.39,\n0.07, 1.01, and 0.02, respectively.",
          "link": "http://arxiv.org/abs/2106.11531",
          "publishedOn": "2021-06-23T01:48:41.626Z",
          "wordCount": 592,
          "title": "Graph Routing between Capsules. (arXiv:2106.11531v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.07348",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lydia T. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_F/0/1/0/all/0/1\">Feng Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mania_H/0/1/0/all/0/1\">Horia Mania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "We study two-sided matching markets in which one side of the market (the\nplayers) does not have a priori knowledge about its preferences for the other\nside (the arms) and is required to learn its preferences from experience. Also,\nwe assume the players have no direct means of communication. This model extends\nthe standard stochastic multi-armed bandit framework to a decentralized\nmultiple player setting with competition. We introduce a new algorithm for this\nsetting that, over a time horizon $T$, attains $\\mathcal{O}(\\log(T))$ stable\nregret when preferences of the arms over players are shared, and\n$\\mathcal{O}(\\log(T)^2)$ regret when there are no assumptions on the\npreferences on either side. Moreover, in the setting where a single player may\ndeviate, we show that the algorithm is incentive compatible whenever the arms'\npreferences are shared, but not necessarily so when preferences are fully\ngeneral.",
          "link": "http://arxiv.org/abs/2012.07348",
          "publishedOn": "2021-06-23T01:48:41.619Z",
          "wordCount": 637,
          "title": "Bandit Learning in Decentralized Matching Markets. (arXiv:2012.07348v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen-Yu Wei</a>",
          "description": "We resolve the long-standing \"impossible tuning\" issue for the classic expert\nproblem and show that, it is in fact possible to achieve regret\n$O\\left(\\sqrt{(\\ln d)\\sum_t \\ell_{t,i}^2}\\right)$ simultaneously for all expert\n$i$ in a $T$-round $d$-expert problem where $\\ell_{t,i}$ is the loss for expert\n$i$ in round $t$. Our algorithm is based on the Mirror Descent framework with a\ncorrection term and a weighted entropy regularizer. While natural, the\nalgorithm has not been studied before and requires a careful analysis. We also\ngeneralize the bound to $O\\left(\\sqrt{(\\ln d)\\sum_t\n(\\ell_{t,i}-m_{t,i})^2}\\right)$ for any prediction vector $m_t$ that the\nlearner receives, and recover or improve many existing results by choosing\ndifferent $m_t$. Furthermore, we use the same framework to create a master\nalgorithm that combines a set of base algorithms and learns the best one with\nlittle overhead. The new guarantee of our master allows us to derive many new\nresults for both the expert problem and more generally Online Linear\nOptimization.",
          "link": "http://arxiv.org/abs/2102.01046",
          "publishedOn": "2021-06-23T01:48:41.612Z",
          "wordCount": 625,
          "title": "Impossible Tuning Made Possible: A New Expert Algorithm and Its Applications. (arXiv:2102.01046v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.10399",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this work, we present a novel neural network to generate high resolution\nimages. We replace the decoder of VAE with a discriminator while using the\nencoder as it is. The encoder is fed data from a normal distribution while the\ngenerator is fed from a gaussian distribution. The combination from both is\ngiven to a discriminator which tells whether the generated image is correct or\nnot. We evaluate our network on 3 different datasets: MNIST, LSUN and CelebA\ndataset. Our network beats the previous state of the art using MMD, SSIM, log\nlikelihood, reconstruction error, ELBO and KL divergence as the evaluation\nmetrics while generating much sharper images. This work is potentially very\nexciting as we are able to combine the advantages of generative models and\ninference models in a principled bayesian manner.",
          "link": "http://arxiv.org/abs/2008.10399",
          "publishedOn": "2021-06-23T01:48:41.592Z",
          "wordCount": 622,
          "title": "Generate High Resolution Images With Generative Variational Autoencoder. (arXiv:2008.10399v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamouei_M/0/1/0/all/0/1\">Mohammad Mamouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimi_Khorshidi_G/0/1/0/all/0/1\">Gholamreza Salimi-Khorshidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1\">Shishir Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassaine_A/0/1/0/all/0/1\">Abdelaali Hassaine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canoy_D/0/1/0/all/0/1\">Dexter Canoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_K/0/1/0/all/0/1\">Kazem Rahimi</a>",
          "description": "Electronic health records represent a holistic overview of patients'\ntrajectories. Their increasing availability has fueled new hopes to leverage\nthem and develop accurate risk prediction models for a wide range of diseases.\nGiven the complex interrelationships of medical records and patient outcomes,\ndeep learning models have shown clear merits in achieving this goal. However, a\nkey limitation of these models remains their capacity in processing long\nsequences. Capturing the whole history of medical encounters is expected to\nlead to more accurate predictions, but the inclusion of records collected for\ndecades and from multiple resources can inevitably exceed the receptive field\nof the existing deep learning architectures. This can result in missing\ncrucial, long-term dependencies. To address this gap, we present Hi-BEHRT, a\nhierarchical Transformer-based model that can significantly expand the\nreceptive field of Transformers and extract associations from much longer\nsequences. Using a multimodal large-scale linked longitudinal electronic health\nrecords, the Hi-BEHRT exceeds the state-of-the-art BEHRT 1% to 5% for area\nunder the receiver operating characteristic (AUROC) curve and 3% to 6% for area\nunder the precision recall (AUPRC) curve on average, and 3% to 6% (AUROC) and\n3% to 11% (AUPRC) for patients with long medical history for 5-year heart\nfailure, diabetes, chronic kidney disease, and stroke risk prediction.\nAdditionally, because pretraining for hierarchical Transformer is not\nwell-established, we provide an effective end-to-end contrastive pre-training\nstrategy for Hi-BEHRT using EHR, improving its transferability on predicting\nclinical events with relatively small training dataset.",
          "link": "http://arxiv.org/abs/2106.11360",
          "publishedOn": "2021-06-23T01:48:41.584Z",
          "wordCount": 696,
          "title": "Hi-BEHRT: Hierarchical Transformer-based model for accurate prediction of clinical events using multimodal longitudinal electronic health records. (arXiv:2106.11360v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.07532",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1\">Michael Poli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massaroli_S/0/1/0/all/0/1\">Stefano Massaroli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_A/0/1/0/all/0/1\">Atsushi Yamashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asama_H/0/1/0/all/0/1\">Hajime Asama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinkyoo Park</a>",
          "description": "We introduce the framework of continuous--depth graph neural networks (GNNs).\nGraph neural ordinary differential equations (GDEs) are formalized as the\ncounterpart to GNNs where the input-output relationship is determined by a\ncontinuum of GNN layers, blending discrete topological structures and\ndifferential equations. The proposed framework is shown to be compatible with\nvarious static and autoregressive GNN models. Results prove general\neffectiveness of GDEs: in static settings they offer computational advantages\nby incorporating numerical methods in their forward pass; in dynamic settings,\non the other hand, they are shown to improve performance by exploiting the\ngeometry of the underlying dynamics.",
          "link": "http://arxiv.org/abs/1911.07532",
          "publishedOn": "2021-06-23T01:48:41.576Z",
          "wordCount": 601,
          "title": "Graph Neural Ordinary Differential Equations. (arXiv:1911.07532v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Metzen_J/0/1/0/all/0/1\">Jan Hendrik Metzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finnie_N/0/1/0/all/0/1\">Nicole Finnie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutmacher_R/0/1/0/all/0/1\">Robin Hutmacher</a>",
          "description": "Recently demonstrated physical-world adversarial attacks have exposed\nvulnerabilities in perception systems that pose severe risks for\nsafety-critical applications such as autonomous driving. These attacks place\nadversarial artifacts in the physical world that indirectly cause the addition\nof a universal patch to inputs of a model that can fool it in a variety of\ncontexts. Adversarial training is the most effective defense against\nimage-dependent adversarial attacks. However, tailoring adversarial training to\nuniversal patches is computationally expensive since the optimal universal\npatch depends on the model weights which change during training. We propose\nmeta adversarial training (MAT), a novel combination of adversarial training\nwith meta-learning, which overcomes this challenge by meta-learning universal\npatches along with model training. MAT requires little extra computation while\ncontinuously adapting a large set of patches to the current model. MAT\nconsiderably increases robustness against universal patch attacks on image\nclassification and traffic-light detection.",
          "link": "http://arxiv.org/abs/2101.11453",
          "publishedOn": "2021-06-23T01:48:41.568Z",
          "wordCount": 634,
          "title": "Meta Adversarial Training against Universal Patches. (arXiv:2101.11453v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singhal_T/0/1/0/all/0/1\">Trisha Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blessing_L/0/1/0/all/0/1\">Lucienne T. M. Blessing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">Kwan Hui Lim</a>",
          "description": "The advent of social media platforms has been a catalyst for the development\nof digital photography that engendered a boom in vision applications. With this\nmotivation, we introduce a large-scale dataset termed 'Photozilla', which\nincludes over 990k images belonging to 10 different photographic styles. The\ndataset is then used to train 3 classification models to automatically classify\nthe images into the relevant style which resulted in an accuracy of ~96%. With\nthe rapid evolution of digital photography, we have seen new types of\nphotography styles emerging at an exponential rate. On that account, we present\na novel Siamese-based network that uses the trained classification models as\nthe base architecture to adapt and classify unseen styles with only 25 training\nsamples. We report an accuracy of over 68% for identifying 10 other distinct\ntypes of photography styles. This dataset can be found at\nhttps://trisha025.github.io/Photozilla/",
          "link": "http://arxiv.org/abs/2106.11359",
          "publishedOn": "2021-06-23T01:48:41.559Z",
          "wordCount": 617,
          "title": "Photozilla: A Large-Scale Photography Dataset and Visual Embedding for 20 Photography Styles. (arXiv:2106.11359v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2002.12873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narayanamurthy_P/0/1/0/all/0/1\">Praneeth Narayanamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaswani_N/0/1/0/all/0/1\">Namrata Vaswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthy_A/0/1/0/all/0/1\">Aditya Ramamoorthy</a>",
          "description": "Subspace tracking (ST) with missing data (ST-miss) or outliers (Robust ST) or\nboth (Robust ST-miss) has been extensively studied in the last many years. This\nwork provides a new simple algorithm and guarantee for both ST with missing\ndata (ST-miss) and RST-miss. Unlike past work on this topic, the algorithm is\nmuch simpler (uses fewer parameters) and the guarantee does not make the\nartificial assumption of piecewise constant subspace change, although it still\nhandles that setting. Secondly, we extend our approach and its analysis to\nprovably solving these problems when the raw data is federated and when the\nover-air data communication modality is used for information exchange between\nthe $K$ peer nodes and the center.",
          "link": "http://arxiv.org/abs/2002.12873",
          "publishedOn": "2021-06-23T01:48:41.549Z",
          "wordCount": 616,
          "title": "Federated Over-Air Subspace Tracking from Incomplete and Corrupted Data. (arXiv:2002.12873v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.00482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lijie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huanyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1\">Marco Gaboardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinhui Xu</a>",
          "description": "In this paper, we study the problem of estimating smooth Generalized Linear\nModels (GLM) in the Non-interactive Local Differential Privacy (NLDP) model.\nDifferent from its classical setting, our model allows the server to access\nsome additional public but unlabeled data. By using Stein's lemma and its\nvariants, we first show that there is an $(\\epsilon, \\delta)$-NLDP algorithm\nfor GLM (under some mild assumptions), if each data record is i.i.d sampled\nfrom some sub-Gaussian distribution with bounded $\\ell_1$-norm. Then with high\nprobability, the sample complexity of the public and private data, for the\nalgorithm to achieve an $\\alpha$ estimation error (in $\\ell_\\infty$-norm), is\n$O(p^2\\alpha^{-2})$ and ${O}(p^2\\alpha^{-2}\\epsilon^{-2})$, respectively, if\n$\\alpha$ is not too small ({\\em i.e.,} $\\alpha\\geq\n\\Omega(\\frac{1}{\\sqrt{p}})$), where $p$ is the dimensionality of the data. This\nis a significant improvement over the previously known quasi-polynomial (in\n$\\alpha$) or exponential (in $p$) complexity of GLM with no public data. Also,\nour algorithm can answer multiple (at most $\\exp(O(p))$) GLM queries with the\nsame sample complexities as in the one GLM query case with at least constant\nprobability. We then extend our idea to the non-linear regression problem and\nshow a similar phenomenon for it. Finally, we demonstrate the effectiveness of\nour algorithms through experiments on both synthetic and real world datasets.\nTo our best knowledge, this is the first paper showing the existence of\nefficient and effective algorithms for GLM and non-linear regression in the\nNLDP model with public unlabeled data.",
          "link": "http://arxiv.org/abs/1910.00482",
          "publishedOn": "2021-06-23T01:48:41.527Z",
          "wordCount": 728,
          "title": "Estimating Smooth GLM in Non-interactive Local Differential Privacy Model with Public Unlabeled Data. (arXiv:1910.00482v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1902.01635",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Shustin_B/0/1/0/all/0/1\">Boris Shustin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Avron_H/0/1/0/all/0/1\">Haim Avron</a>",
          "description": "Optimization problems on the generalized Stiefel manifold (and products of\nit) are prevalent across science and engineering. For example, in computational\nscience they arise in the symmetric (generalized) eigenvalue problem, in\nnonlinear eigenvalue problems, and in electronic structures computations, to\nname a few problems. In statistics and machine learning, they arise, for\nexample, in various dimensionality reduction techniques such as canonical\ncorrelation analysis. In deep learning, regularization and improved stability\ncan be obtained by constraining some layers to have parameter matrices that\nbelong to the Stiefel manifold. Solving problems on the generalized Stiefel\nmanifold can be approached via the tools of Riemannian optimization. However,\nusing the standard geometric components for the generalized Stiefel manifold\nhas two possible shortcoming: computing some of the geometric components can be\ntoo expensive and converge can be rather slow in certain cases. Both\nshortcomings can be addressed using a technique called Riemannian\npreconditioning, which amounts to using geometric components derived using a\nprecoditioner that defines a Riemannian metric on the constraint manifold. In\nthis paper we develop the geometric components required to perform Riemannian\noptimization on the generalized Stiefel manifold equipped with a non-standard\nmetric, and illustrate theoretically and numerically the use of those\ncomponents and the effect of Riemannian preconditioning for solving\noptimization problems on the generalized Stiefel manifold.",
          "link": "http://arxiv.org/abs/1902.01635",
          "publishedOn": "2021-06-23T01:48:41.520Z",
          "wordCount": 678,
          "title": "Preconditioned Riemannian Optimization on the Generalized Stiefel Manifold. (arXiv:1902.01635v3 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gorishniy_Y/0/1/0/all/0/1\">Yury Gorishniy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubachev_I/0/1/0/all/0/1\">Ivan Rubachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khrulkov_V/0/1/0/all/0/1\">Valentin Khrulkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1\">Artem Babenko</a>",
          "description": "The necessity of deep learning for tabular data is still an unanswered\nquestion addressed by a large number of research efforts. The recent literature\non tabular DL proposes several deep architectures reported to be superior to\ntraditional \"shallow\" models like Gradient Boosted Decision Trees. However,\nsince existing works often use different benchmarks and tuning protocols, it is\nunclear if the proposed models universally outperform GBDT. Moreover, the\nmodels are often not compared to each other, therefore, it is challenging to\nidentify the best deep model for practitioners.\n\nIn this work, we start from a thorough review of the main families of DL\nmodels recently developed for tabular data. We carefully tune and evaluate them\non a wide range of datasets and reveal two significant findings. First, we show\nthat the choice between GBDT and DL models highly depends on data and there is\nstill no universally superior solution. Second, we demonstrate that a simple\nResNet-like architecture is a surprisingly effective baseline, which\noutperforms most of the sophisticated models from the DL literature. Finally,\nwe design a simple adaptation of the Transformer architecture for tabular data\nthat becomes a new strong DL baseline and reduces the gap between GBDT and DL\nmodels on datasets where GBDT dominates.",
          "link": "http://arxiv.org/abs/2106.11959",
          "publishedOn": "2021-06-23T01:48:41.510Z",
          "wordCount": 639,
          "title": "Revisiting Deep Learning Models for Tabular Data. (arXiv:2106.11959v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuxin Yang</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xitong Zhang</a> (1 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Q/0/1/0/all/0/1\">Qiang Guan</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Youzuo Lin</a> (1) ((1) Earth and Environmental Sciences Division, Los Alamos National Laboratory, (2) Department of Computer Science, Kent State University, (3) Department of Computational Mathematics, Science and Engineering, Michigan State University)",
          "description": "Deep learning and data-driven approaches have shown great potential in\nscientific domains. The promise of data-driven techniques relies on the\navailability of a large volume of high-quality training datasets. Due to the\nhigh cost of obtaining data through expensive physical experiments,\ninstruments, and simulations, data augmentation techniques for scientific\napplications have emerged as a new direction for obtaining scientific data\nrecently. However, existing data augmentation techniques originating from\ncomputer vision, yield physically unacceptable data samples that are not\nhelpful for the domain problems that we are interested in. In this paper, we\ndevelop new physics-informed data augmentation techniques based on\nconvolutional neural networks. Specifically, our generative models leverage\ndifferent physics knowledge (such as governing equations, observable\nperception, and physics phenomena) to improve the quality of the synthetic\ndata. To validate the effectiveness of our data augmentation techniques, we\napply them to solve a subsurface seismic full-waveform inversion using\nsimulated CO$_2$ leakage data. Our interest is to invert for subsurface\nvelocity models associated with very small CO$_2$ leakage. We validate the\nperformance of our methods using comprehensive numerical tests. Via comparison\nand analysis, we show that data-driven seismic imaging can be significantly\nenhanced by using our physics-informed data augmentation techniques.\nParticularly, the imaging quality has been improved by 15% in test scenarios of\ngeneral-sized leakage and 17% in small-sized leakage when using an augmented\ntraining set obtained with our techniques.",
          "link": "http://arxiv.org/abs/2106.11892",
          "publishedOn": "2021-06-23T01:48:41.503Z",
          "wordCount": 713,
          "title": "Making Invisible Visible: Data-Driven Seismic Inversion with Physics-Informed Data Augmentation. (arXiv:2106.11892v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arumugam_D/0/1/0/all/0/1\">Dilip Arumugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1\">Benjamin Van Roy</a>",
          "description": "Agents that learn to select optimal actions represent a prominent focus of\nthe sequential decision-making literature. In the face of a complex environment\nor constraints on time and resources, however, aiming to synthesize such an\noptimal policy can become infeasible. These scenarios give rise to an important\ntrade-off between the information an agent must acquire to learn and the\nsub-optimality of the resulting policy. While an agent designer has a\npreference for how this trade-off is resolved, existing approaches further\nrequire that the designer translate these preferences into a fixed learning\ntarget for the agent. In this work, leveraging rate-distortion theory, we\nautomate this process such that the designer need only express their\npreferences via a single hyperparameter and the agent is endowed with the\nability to compute its own learning targets that best achieve the desired\ntrade-off. We establish a general bound on expected discounted regret for an\nagent that decides what to learn in this manner along with computational\nexperiments that illustrate the expressiveness of designer preferences and even\nshow improvements over Thompson sampling in identifying an optimal policy.",
          "link": "http://arxiv.org/abs/2101.06197",
          "publishedOn": "2021-06-23T01:48:41.494Z",
          "wordCount": 648,
          "title": "Deciding What to Learn: A Rate-Distortion Approach. (arXiv:2101.06197v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_O/0/1/0/all/0/1\">Orchid Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1\">Alessandro Achille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1\">Marzia Polito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "Few-shot learning aims to transfer information from one task to enable\ngeneralization on novel tasks given a few examples. This information is present\nboth in the domain and the class labels. In this work we investigate the\ncomplementary roles of these two sources of information by combining\ninstance-discriminative contrastive learning and supervised learning in a\nsingle framework called Supervised Momentum Contrastive learning (SUPMOCO). Our\napproach avoids a problem observed in supervised learning where information in\nimages not relevant to the task is discarded, which hampers their\ngeneralization to novel tasks. We show that (self-supervised) contrastive\nlearning and supervised learning are mutually beneficial, leading to a new\nstate-of-the-art on the META-DATASET - a recently introduced benchmark for\nfew-shot learning. Our method is based on a simple modification of MOCO and\nscales better than prior work on combining supervised and self-supervised\nlearning. This allows us to easily combine data from multiple domains leading\nto further improvements.",
          "link": "http://arxiv.org/abs/2101.11058",
          "publishedOn": "2021-06-23T01:48:41.468Z",
          "wordCount": 634,
          "title": "Supervised Momentum Contrastive Learning for Few-Shot Classification. (arXiv:2101.11058v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.03979",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_W/0/1/0/all/0/1\">Wei Biao Wu</a>",
          "description": "The stochastic gradient descent (SGD) algorithm is widely used for parameter\nestimation, especially for huge data sets and online learning. While this\nrecursive algorithm is popular for computation and memory efficiency,\nquantifying variability and randomness of the solutions has been rarely\nstudied. This paper aims at conducting statistical inference of SGD-based\nestimates in an online setting. In particular, we propose a fully online\nestimator for the covariance matrix of averaged SGD iterates (ASGD) only using\nthe iterates from SGD. We formally establish our online estimator's consistency\nand show that the convergence rate is comparable to offline counterparts. Based\non the classic asymptotic normality results of ASGD, we construct\nasymptotically valid confidence intervals for model parameters. Upon receiving\nnew observations, we can quickly update the covariance matrix estimate and the\nconfidence intervals. This approach fits in an online setting and takes full\nadvantage of SGD: efficiency in computation and memory.",
          "link": "http://arxiv.org/abs/2002.03979",
          "publishedOn": "2021-06-23T01:48:41.459Z",
          "wordCount": 608,
          "title": "Online Covariance Matrix Estimation in Stochastic Gradient Descent. (arXiv:2002.03979v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Redefined_A/0/1/0/all/0/1\">AI Redefined</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottipati_S/0/1/0/all/0/1\">Sai Krishna Gottipati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurandwad_S/0/1/0/all/0/1\">Sagar Kurandwad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mars_C/0/1/0/all/0/1\">Clod&#xe9;ric Mars</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szriftgiser_G/0/1/0/all/0/1\">Gregory Szriftgiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chabot_F/0/1/0/all/0/1\">Fran&#xe7;ois Chabot</a>",
          "description": "Involving humans directly for the benefit of AI agents' training is getting\ntraction thanks to several advances in reinforcement learning and\nhuman-in-the-loop learning. Humans can provide rewards to the agent,\ndemonstrate tasks, design a curriculum, or act in the environment, but these\nbenefits also come with architectural, functional design and engineering\ncomplexities. We present Cogment, a unifying open-source framework that\nintroduces an actor formalism to support a variety of humans-agents\ncollaboration typologies and training approaches. It is also scalable out of\nthe box thanks to a distributed micro service architecture, and offers\nsolutions to the aforementioned complexities.",
          "link": "http://arxiv.org/abs/2106.11345",
          "publishedOn": "2021-06-23T01:48:41.451Z",
          "wordCount": 559,
          "title": "Cogment: Open Source Framework For Distributed Multi-actor Training, Deployment & Operations. (arXiv:2106.11345v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/1903.12561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Shaokai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaidi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambrechts_J/0/1/0/all/0/1\">Jan-Henrik Lambrechts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aojun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaisheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>",
          "description": "It is well known that deep neural networks (DNNs) are vulnerable to\nadversarial attacks, which are implemented by adding crafted perturbations onto\nbenign examples. Min-max robust optimization based adversarial training can\nprovide a notion of security against adversarial attacks. However, adversarial\nrobustness requires a significantly larger capacity of the network than that\nfor the natural training with only benign examples. This paper proposes a\nframework of concurrent adversarial training and weight pruning that enables\nmodel compression while still preserving the adversarial robustness and\nessentially tackles the dilemma of adversarial training. Furthermore, this work\nstudies two hypotheses about weight pruning in the conventional setting and\nfinds that weight pruning is essential for reducing the network model size in\nthe adversarial setting, training a small model from scratch even with\ninherited initialization from the large model cannot achieve both adversarial\nrobustness and high standard accuracy. Code is available at\nhttps://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM.",
          "link": "http://arxiv.org/abs/1903.12561",
          "publishedOn": "2021-06-23T01:48:41.444Z",
          "wordCount": 666,
          "title": "Adversarial Robustness vs Model Compression, or Both?. (arXiv:1903.12561v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moondra_J/0/1/0/all/0/1\">Jai Moondra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortagy_H/0/1/0/all/0/1\">Hassan Mortagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Swati Gupta</a>",
          "description": "Optimization algorithms such as projected Newton's method, FISTA, mirror\ndescent and its variants enjoy near-optimal regret bounds and convergence\nrates, but suffer from a computational bottleneck of computing \"projections''\nin potentially each iteration (e.g., $O(T^{1/2})$ regret of online mirror\ndescent). On the other hand, conditional gradient variants solve a linear\noptimization in each iteration, but result in suboptimal rates (e.g.,\n$O(T^{3/4})$ regret of online Frank-Wolfe). Motivated by this trade-off in\nruntime v/s convergence rates, we consider iterative projections of close-by\npoints over widely-prevalent submodular base polytopes $B(f)$. We develop a\ntoolkit to speed up the computation of projections using both discrete and\ncontinuous perspectives. We subsequently adapt the away-step Frank-Wolfe\nalgorithm to use this information and enable early termination. For the special\ncase of cardinality based submodular polytopes, we improve the runtime of\ncomputing certain Bregman projections by a factor of $\\Omega(n/\\log(n))$. Our\ntheoretical results show orders of magnitude reduction in runtime in\npreliminary computational experiments.",
          "link": "http://arxiv.org/abs/2106.11943",
          "publishedOn": "2021-06-23T01:48:41.436Z",
          "wordCount": 594,
          "title": "Reusing Combinatorial Structure: Faster Iterative Projections over Submodular Base Polytopes. (arXiv:2106.11943v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1910.02684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Ziang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jieming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengzhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zengfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>",
          "description": "Graph neural networks (GNNs) are designed for semi-supervised node\nclassification on graphs where only a small subset of nodes have class labels.\nHowever, under extreme cases when very few labels are available (e.g., 1\nlabeled node per class), GNNs suffer from severe result quality degradation.\nSeveral existing studies make an initial effort to ease this situation, but are\nstill far from satisfactory.\n\nIn this paper, on few-labeled graph data, we propose an effective framework\nABN that is readily applicable to both shallow and deep GNN architectures and\nsignificantly boosts classification accuracy. In particular, on a benchmark\ndataset Cora with only 1 labeled node per class, while the classic graph\nconvolutional network (GCN) only has 44.6% accuracy, an immediate instantiation\nof ABN over GCN achieves 62.5% accuracy; when applied to a deep architecture\nDAGNN, ABN improves accuracy from 59.8% to 66.4%, which is state of the art.\n\nABN obtains superior performance through three main algorithmic designs.\nFirst, it selects high-quality unlabeled nodes via an adaptive pseudo labeling\ntechnique, so as to adaptively enhance the training process of GNNs. Second,\nABN balances the labels of the selected nodes on real-world skewed graph data\nby pseudo label balancing. Finally, a negative sampling regularizer is designed\nfor ABN to further utilize the unlabeled nodes. The effectiveness of the three\ntechniques in ABN is well-validated by both theoretical and empirical analysis.\nExtensive experiments, comparing 12 existing approaches on 4 benchmark\ndatasets, demonstrate that ABN achieves state-of-the-art performance.",
          "link": "http://arxiv.org/abs/1910.02684",
          "publishedOn": "2021-06-23T01:48:41.416Z",
          "wordCount": 707,
          "title": "Effective Semi-Supervised Node Classification on Few-Labeled Graph Data. (arXiv:1910.02684v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eriksson_D/0/1/0/all/0/1\">David Eriksson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_P/0/1/0/all/0/1\">Pierce I-Jen Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daulton_S/0/1/0/all/0/1\">Sam Daulton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aly_A/0/1/0/all/0/1\">Ahmed Aly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1\">Arun Babu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Akshat Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1\">Peng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shicong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1\">Ganesh Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balandat_M/0/1/0/all/0/1\">Maximilian Balandat</a>",
          "description": "When tuning the architecture and hyperparameters of large machine learning\nmodels for on-device deployment, it is desirable to understand the optimal\ntrade-offs between on-device latency and model accuracy. In this work, we\nleverage recent methodological advances in Bayesian optimization over\nhigh-dimensional search spaces and multi-objective Bayesian optimization to\nefficiently explore these trade-offs for a production-scale on-device natural\nlanguage understanding model at Facebook.",
          "link": "http://arxiv.org/abs/2106.11890",
          "publishedOn": "2021-06-23T01:48:41.408Z",
          "wordCount": 519,
          "title": "Latency-Aware Neural Architecture Search with Multi-Objective Bayesian Optimization. (arXiv:2106.11890v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Olin_Ammentorp_W/0/1/0/all/0/1\">Wilkie Olin-Ammentorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazhenov_M/0/1/0/all/0/1\">Maxim Bazhenov</a>",
          "description": "In this work, we extend standard neural networks by building upon an\nassumption that neuronal activations correspond to the angle of a complex\nnumber lying on the unit circle, or 'phasor.' Each layer in such a network\nproduces new activations by taking a weighted superposition of the previous\nlayer's phases and calculating the new phase value. This generalized\narchitecture allows models to reach high accuracy and carries the singular\nadvantage that mathematically equivalent versions of the network can be\nexecuted with or without regard to a temporal variable. Importantly, the value\nof a phase angle in the temporal domain can be sparsely represented by a\nperiodically repeating series of delta functions or 'spikes'. We demonstrate\nthe atemporal training of a phasor network on standard deep learning tasks and\nshow that these networks can then be executed in either the traditional\natemporal domain or spiking temporal domain with no conversion step needed.\nThis provides a novel basis for constructing deep networkswhich operate via\ntemporal, spike-based calculations suitable for neuromorphic computing\nhardware.",
          "link": "http://arxiv.org/abs/2106.11908",
          "publishedOn": "2021-06-23T01:48:41.402Z",
          "wordCount": 610,
          "title": "Deep Phasor Networks: Connecting Conventional and Spiking Neural Networks. (arXiv:2106.11908v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11865",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_I/0/1/0/all/0/1\">I-Chung Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng-Te Li</a>",
          "description": "Recent advances in protecting node privacy on graph data and attacking graph\nneural networks (GNNs) gain much attention. The eye does not bring these two\nessential tasks together yet. Imagine an adversary can utilize the powerful\nGNNs to infer users' private labels in a social network. How can we\nadversarially defend against such privacy attacks while maintaining the utility\nof perturbed graphs? In this work, we propose a novel research task,\nadversarial defenses against GNN-based privacy attacks, and present a graph\nperturbation-based approach, NetFense, to achieve the goal. NetFense can\nsimultaneously keep graph data unnoticeability (i.e., having limited changes on\nthe graph structure), maintain the prediction confidence of targeted label\nclassification (i.e., preserving data utility), and reduce the prediction\nconfidence of private label classification (i.e., protecting the privacy of\nnodes). Experiments conducted on single- and multiple-target perturbations\nusing three real graph data exhibit that the perturbed graphs by NetFense can\neffectively maintain data utility (i.e., model unnoticeability) on targeted\nlabel classification and significantly decrease the prediction confidence of\nprivate label classification (i.e., privacy protection). Extensive studies also\nbring several insights, such as the flexibility of NetFense, preserving local\nneighborhoods in data unnoticeability, and better privacy protection for\nhigh-degree nodes.",
          "link": "http://arxiv.org/abs/2106.11865",
          "publishedOn": "2021-06-23T01:48:41.396Z",
          "wordCount": 670,
          "title": "NetFense: Adversarial Defenses against Privacy Attacks on Neural Networks for Graph Data. (arXiv:2106.11865v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Izmailov_P/0/1/0/all/0/1\">Pavel Izmailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicholson_P/0/1/0/all/0/1\">Patrick Nicholson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotfi_S/0/1/0/all/0/1\">Sanae Lotfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1\">Andrew Gordon Wilson</a>",
          "description": "Approximate Bayesian inference for neural networks is considered a robust\nalternative to standard training, often providing good performance on\nout-of-distribution data. However, Bayesian neural networks (BNNs) with\nhigh-fidelity approximate inference via full-batch Hamiltonian Monte Carlo\nachieve poor generalization under covariate shift, even underperforming\nclassical estimation. We explain this surprising result, showing how a Bayesian\nmodel average can in fact be problematic under covariate shift, particularly in\ncases where linear dependencies in the input features cause a lack of posterior\ncontraction. We additionally show why the same issue does not affect many\napproximate inference procedures, or classical maximum a-posteriori (MAP)\ntraining. Finally, we propose novel priors that improve the robustness of BNNs\nto many sources of covariate shift.",
          "link": "http://arxiv.org/abs/2106.11905",
          "publishedOn": "2021-06-23T01:48:41.388Z",
          "wordCount": 553,
          "title": "Dangers of Bayesian Model Averaging under Covariate Shift. (arXiv:2106.11905v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weien Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>",
          "description": "Temperature monitoring during the life time of heat-source components in\nengineering systems becomes essential to ensure the normal work and even the\nlong working life of the heat sources. However, prior methods, which mainly use\nthe interpolate estimation, require large amounts of temperature tensors for an\naccurate estimation. To solve this problem, this work develops a novel\nphysics-informed deep surrogate models for temperature field reconstruction.\nFirst, we defines the temperature field reconstruction task of heat-source\nsystems. Then, this work develops the deep surrogate model mapping for the\nproposed task. Finally, considering the physical properties of heat transfer,\nthis work proposes four different losses and joint learns the deep surrogate\nmodel with these losses. Experimental studies have conducted over typical\ntwo-dimensional heat-source systems to demonstrate the effectiveness and\nefficiency of the proposed physics-informed deep surrogate models for\ntemperature field reconstruction.",
          "link": "http://arxiv.org/abs/2106.11929",
          "publishedOn": "2021-06-23T01:48:41.368Z",
          "wordCount": 582,
          "title": "Physics-Informed Deep Reversible Regression Model for Temperature Field Reconstruction of Heat-Source Systems. (arXiv:2106.11929v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_Y/0/1/0/all/0/1\">Yousef Saad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zechen Zhang</a>",
          "description": "The general method of graph coarsening or graph reduction has been a\nremarkably useful and ubiquitous tool in scientific computing and it is now\njust starting to have a similar impact in machine learning. The goal of this\npaper is to take a broad look into coarsening techniques that have been\nsuccessfully deployed in scientific computing and see how similar principles\nare finding their way in more recent applications related to machine learning.\nIn scientific computing, coarsening plays a central role in algebraic multigrid\nmethods as well as the related class of multilevel incomplete LU\nfactorizations. In machine learning, graph coarsening goes under various names,\ne.g., graph downsampling or graph reduction. Its goal in most cases is to\nreplace some original graph by one which has fewer nodes, but whose structure\nand characteristics are similar to those of the original graph. As will be\nseen, a common strategy in these methods is to rely on spectral properties to\ndefine the coarse graph.",
          "link": "http://arxiv.org/abs/2106.11863",
          "publishedOn": "2021-06-23T01:48:41.361Z",
          "wordCount": 601,
          "title": "Graph coarsening: From scientific computing to machine learning. (arXiv:2106.11863v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1706.07180",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gribonval_R/0/1/0/all/0/1\">R&#xe9;mi Gribonval</a> (PANAMA, DANTE), <a href=\"http://arxiv.org/find/stat/1/au:+Blanchard_G/0/1/0/all/0/1\">Gilles Blanchard</a> (DATASHAPE, LMO), <a href=\"http://arxiv.org/find/stat/1/au:+Keriven_N/0/1/0/all/0/1\">Nicolas Keriven</a> (PANAMA, GIPSA-GAIA), <a href=\"http://arxiv.org/find/stat/1/au:+Traonmilin_Y/0/1/0/all/0/1\">Yann Traonmilin</a> (PANAMA, IMB)",
          "description": "We describe a general framework -- compressive statistical learning -- for\nresource-efficient large-scale learning: the training collection is compressed\nin one pass into a low-dimensional sketch (a vector of random empirical\ngeneralized moments) that captures the information relevant to the considered\nlearning task. A near-minimizer of the risk is computed from the sketch through\nthe solution of a nonlinear least squares problem. We investigate sufficient\nsketch sizes to control the generalization error of this procedure. The\nframework is illustrated on compressive PCA, compressive clustering, and\ncompressive Gaussian mixture Modeling with fixed known variance. The latter two\nare further developed in a companion paper.",
          "link": "http://arxiv.org/abs/1706.07180",
          "publishedOn": "2021-06-23T01:48:41.355Z",
          "wordCount": 666,
          "title": "Compressive Statistical Learning with Random Feature Moments. (arXiv:1706.07180v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11918",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pekpe_K/0/1/0/all/0/1\">Komi Midzodzi P&#xe9;kp&#xe9;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zitouni_D/0/1/0/all/0/1\">Djamel Zitouni</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gasso_G/0/1/0/all/0/1\">Gilles Gasso</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dhifli_W/0/1/0/all/0/1\">Wajdi Dhifli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Guinhouya_B/0/1/0/all/0/1\">Benjamin C. Guinhouya</a>",
          "description": "Common compartmental modeling for COVID-19 is based on a priori knowledge and\nnumerous assumptions. Additionally, they do not systematically incorporate\nasymptomatic cases. Our study aimed at providing a framework for data-driven\napproaches, by leveraging the strengths of the grey-box system theory or\ngrey-box identification, known for its robustness in problem solving under\npartial, incomplete, or uncertain data. Empirical data on confirmed cases and\ndeaths, extracted from an open source repository were used to develop the\nSEAIRD compartment model. Adjustments were made to fit current knowledge on the\nCOVID-19 behavior. The model was implemented and solved using an Ordinary\nDifferential Equation solver and an optimization tool. A cross-validation\ntechnique was applied, and the coefficient of determination $R^2$ was computed\nin order to evaluate the goodness-of-fit of the model. %to the data. Key\nepidemiological parameters were finally estimated and we provided the rationale\nfor the construction of SEAIRD model. When applied to Brazil's cases, SEAIRD\nproduced an excellent agreement to the data, with an %coefficient of\ndetermination $R^2$ $\\geq 90\\%$. The probability of COVID-19 transmission was\ngenerally high ($\\geq 95\\%$). On the basis of a 20-day modeling data, the\nincidence rate of COVID-19 was as low as 3 infected cases per 100,000 exposed\npersons in Brazil and France. Within the same time frame, the fatality rate of\nCOVID-19 was the highest in France (16.4\\%) followed by Brazil (6.9\\%), and the\nlowest in Russia ($\\leq 1\\%$). SEAIRD represents an asset for modeling\ninfectious diseases in their dynamical stable phase, especially for new viruses\nwhen pathophysiology knowledge is very limited.",
          "link": "http://arxiv.org/abs/2106.11918",
          "publishedOn": "2021-06-23T01:48:41.347Z",
          "wordCount": 763,
          "title": "From SIR to SEAIRD: a novel data-driven modeling approach based on the Grey-box System Theory to predict the dynamics of COVID-19. (arXiv:2106.11918v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11879",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cohen_A/0/1/0/all/0/1\">Alon Cohen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Daniely_A/0/1/0/all/0/1\">Amit Daniely</a>, <a href=\"http://arxiv.org/find/math/1/au:+Drori_Y/0/1/0/all/0/1\">Yoel Drori</a>, <a href=\"http://arxiv.org/find/math/1/au:+Koren_T/0/1/0/all/0/1\">Tomer Koren</a>, <a href=\"http://arxiv.org/find/math/1/au:+Schain_M/0/1/0/all/0/1\">Mariano Schain</a>",
          "description": "We consider stochastic optimization with delayed gradients where, at each\ntime step $t$, the algorithm makes an update using a stale stochastic gradient\nfrom step $t - d_t$ for some arbitrary delay $d_t$. This setting abstracts\nasynchronous distributed optimization where a central server receives gradient\nupdates computed by worker machines. These machines can experience computation\nand communication loads that might vary significantly over time. In the general\nnon-convex smooth optimization setting, we give a simple and efficient\nalgorithm that requires $O( \\sigma^2/\\epsilon^4 + \\tau/\\epsilon^2 )$ steps for\nfinding an $\\epsilon$-stationary point $x$, where $\\tau$ is the \\emph{average}\ndelay $\\smash{\\frac{1}{T}\\sum_{t=1}^T d_t}$ and $\\sigma^2$ is the variance of\nthe stochastic gradients. This improves over previous work, which showed that\nstochastic gradient decent achieves the same rate but with respect to the\n\\emph{maximal} delay $\\max_{t} d_t$, that can be significantly larger than the\naverage delay especially in heterogeneous distributed systems. Our experiments\ndemonstrate the efficacy and robustness of our algorithm in cases where the\ndelay distribution is skewed or heavy-tailed.",
          "link": "http://arxiv.org/abs/2106.11879",
          "publishedOn": "2021-06-23T01:48:41.338Z",
          "wordCount": 602,
          "title": "Asynchronous Stochastic Optimization Robust to Arbitrary Delays. (arXiv:2106.11879v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1\">Abraham George Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_J/0/1/0/all/0/1\">Jens Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terrones_Campos_C/0/1/0/all/0/1\">Cynthia Terrones-Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berthelsen_A/0/1/0/all/0/1\">Anne Kiil Berthelsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forbes_N/0/1/0/all/0/1\">Nora Jarrett Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darkner_S/0/1/0/all/0/1\">Sune Darkner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specht_L/0/1/0/all/0/1\">Lena Specht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelius_I/0/1/0/all/0/1\">Ivan Richter Vogelius</a>",
          "description": "Organ-at-risk contouring is still a bottleneck in radiotherapy, with many\ndeep learning methods falling short of promised results when evaluated on\nclinical data. We investigate the accuracy and time-savings resulting from the\nuse of an interactive-machine-learning method for an organ-at-risk contouring\ntask. We compare the method to the Eclipse contouring software and find strong\nagreement with manual delineations, with a dice score of 0.95. The annotations\ncreated using corrective-annotation also take less time to create as more\nimages are annotated, resulting in substantial time savings compared to manual\nmethods, with hearts that take 2 minutes and 2 seconds to delineate on average,\nafter 923 images have been delineated, compared to 7 minutes and 1 seconds when\ndelineating manually. Our experiment demonstrates that\ninteractive-machine-learning with corrective-annotation provides a fast and\naccessible way for non computer-scientists to train deep-learning models to\nsegment their own structures of interest as part of routine clinical workflows.\n\nSource code is available at\n\\href{https://github.com/Abe404/RootPainter3D}{this HTTPS URL}.",
          "link": "http://arxiv.org/abs/2106.11942",
          "publishedOn": "2021-06-23T01:48:41.317Z",
          "wordCount": 618,
          "title": "RootPainter3D: Interactive-machine-learning enables rapid and accurate contouring for radiotherapy. (arXiv:2106.11942v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weitong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiafan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Amy Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "The success of deep reinforcement learning (DRL) is due to the power of\nlearning a representation that is suitable for the underlying exploration and\nexploitation task. However, existing provable reinforcement learning algorithms\nwith linear function approximation often assume the feature representation is\nknown and fixed. In order to understand how representation learning can improve\nthe efficiency of RL, we study representation learning for a class of low-rank\nMarkov Decision Processes (MDPs) where the transition kernel can be represented\nin a bilinear form. We propose a provably efficient algorithm called ReLEX that\ncan simultaneously learn the representation and perform exploration. We show\nthat ReLEX always performs no worse than a state-of-the-art algorithm without\nrepresentation learning, and will be strictly better in terms of sample\nefficiency if the function class of representations enjoys a certain mild\n\"coverage'' property over the whole state-action space.",
          "link": "http://arxiv.org/abs/2106.11935",
          "publishedOn": "2021-06-23T01:48:41.309Z",
          "wordCount": 589,
          "title": "Provably Efficient Representation Learning in Low-rank Markov Decision Processes. (arXiv:2106.11935v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dimanov_D/0/1/0/all/0/1\">Daniel Dimanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaguer_Ballester_E/0/1/0/all/0/1\">Emili Balaguer-Ballester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singleton_C/0/1/0/all/0/1\">Colin Singleton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostami_S/0/1/0/all/0/1\">Shahin Rostami</a>",
          "description": "In this paper, we present a novel neuroevolutionary method to identify the\narchitecture and hyperparameters of convolutional autoencoders. Remarkably, we\nused a hypervolume indicator in the context of neural architecture search for\nautoencoders, for the first time to our current knowledge. Results show that\nimages were compressed by a factor of more than 10, while still retaining\nenough information to achieve image classification for the majority of the\ntasks. Thus, this new approach can be used to speed up the AutoML pipeline for\nimage compression.",
          "link": "http://arxiv.org/abs/2106.11914",
          "publishedOn": "2021-06-23T01:48:41.303Z",
          "wordCount": 531,
          "title": "MONCAE: Multi-Objective Neuroevolution of Convolutional Autoencoders. (arXiv:2106.11914v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hand_D/0/1/0/all/0/1\">D. J. Hand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anagnostopoulos_C/0/1/0/all/0/1\">C. Anagnostopoulos</a>",
          "description": "The H-measure is a classifier performance measure which takes into account\nthe context of application without requiring a rigid value of relative\nmisclassification costs to be set. Since its introduction in 2009 it has become\nwidely adopted. This paper answers various queries which users have raised\nsince its introduction, including questions about its interpretation, the\nchoice of a weighting function, whether it is strictly proper, and its\ncoherence, and relates the measure to other work.",
          "link": "http://arxiv.org/abs/2106.11888",
          "publishedOn": "2021-06-23T01:48:41.293Z",
          "wordCount": 502,
          "title": "Notes on the H-measure of classifier performance. (arXiv:2106.11888v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1\">Sarah M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohr_A/0/1/0/all/0/1\">Alexander von Rohr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trimpe_S/0/1/0/all/0/1\">Sebastian Trimpe</a>",
          "description": "Reinforcement learning (RL) aims to find an optimal policy by interaction\nwith an environment. Consequently, learning complex behavior requires a vast\nnumber of samples, which can be prohibitive in practice. Nevertheless, instead\nof systematically reasoning and actively choosing informative samples, policy\ngradients for local search are often obtained from random perturbations. These\nrandom samples yield high variance estimates and hence are sub-optimal in terms\nof sample complexity. Actively selecting informative samples is at the core of\nBayesian optimization, which constructs a probabilistic surrogate of the\nobjective from past samples to reason about informative subsequent ones. In\nthis paper, we propose to join both worlds. We develop an algorithm utilizing a\nprobabilistic model of the objective function and its gradient. Based on the\nmodel, the algorithm decides where to query a noisy zeroth-order oracle to\nimprove the gradient estimates. The resulting algorithm is a novel type of\npolicy search method, which we compare to existing black-box algorithms. The\ncomparison reveals improved sample complexity and reduced variance in extensive\nempirical evaluations on synthetic objectives. Further, we highlight the\nbenefits of active sampling on popular RL benchmarks.",
          "link": "http://arxiv.org/abs/2106.11899",
          "publishedOn": "2021-06-23T01:48:41.286Z",
          "wordCount": 615,
          "title": "Local policy search with Bayesian optimization. (arXiv:2106.11899v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Beining Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhizhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuofan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jian Peng</a>",
          "description": "We study deep reinforcement learning (RL) algorithms with delayed rewards. In\nmany real-world tasks, instant rewards are often not readily accessible or even\ndefined immediately after the agent performs actions. In this work, we first\nformally define the environment with delayed rewards and discuss the challenges\nraised due to the non-Markovian nature of such environments. Then, we introduce\na general off-policy RL framework with a new Q-function formulation that can\nhandle the delayed rewards with theoretical convergence guarantees. For\npractical tasks with high dimensional state spaces, we further introduce the\nHC-decomposition rule of the Q-function in our framework which naturally leads\nto an approximation scheme that helps boost the training efficiency and\nstability. We finally conduct extensive experiments to demonstrate the superior\nperformance of our algorithms over the existing work and their variants.",
          "link": "http://arxiv.org/abs/2106.11854",
          "publishedOn": "2021-06-23T01:48:41.278Z",
          "wordCount": 564,
          "title": "Off-Policy Reinforcement Learning with Delayed Rewards. (arXiv:2106.11854v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinlu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callcut_R/0/1/0/all/0/1\">Rachael Callcut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petzold_L/0/1/0/all/0/1\">Linda Petzold</a>",
          "description": "Multiple organ failure (MOF) is a severe syndrome with a high mortality rate\namong Intensive Care Unit (ICU) patients. Early and precise detection is\ncritical for clinicians to make timely decisions. An essential challenge in\napplying machine learning models to electronic health records (EHRs) is the\npervasiveness of missing values. Most existing imputation methods are involved\nin the data preprocessing phase, failing to capture the relationship between\ndata and outcome for downstream predictions. In this paper, we propose\nclassifier-guided generative adversarial imputation networks Classifier-GAIN)\nfor MOF prediction to bridge this gap, by incorporating both observed data and\nlabel information. Specifically, the classifier takes imputed values from the\ngenerator(imputer) to predict task outcomes and provides additional supervision\nsignals to the generator by joint training. The classifier-guide generator\nimputes missing values with label-awareness during training, improving the\nclassifier's performance during inference. We conduct extensive experiments\nshowing that our approach consistently outperforms classical and state-of-art\nneural baselines across a range of missing data scenarios and evaluation\nmetrics.",
          "link": "http://arxiv.org/abs/2106.11878",
          "publishedOn": "2021-06-23T01:48:41.247Z",
          "wordCount": 603,
          "title": "Multiple Organ Failure Prediction with Classifier-Guided Generative Adversarial Imputation Networks. (arXiv:2106.11878v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11849",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Agarwal_N/0/1/0/all/0/1\">Nikita Agarwal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zeitler_J/0/1/0/all/0/1\">Jakob Zeitler</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mastouri_A/0/1/0/all/0/1\">Afsaneh Mastouri</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "Algorithmic recourse aims to provide actionable recommendations to\nindividuals to obtain a more favourable outcome from an automated\ndecision-making system. As it involves reasoning about interventions performed\nin the physical world, recourse is fundamentally a causal problem. Existing\nmethods compute the effect of recourse actions using a causal model learnt from\ndata under the assumption of no hidden confounding and modelling assumptions\nsuch as additive noise. Building on the seminal work of Balke and Pearl (1994),\nwe propose an alternative approach for discrete random variables which relaxes\nthese assumptions and allows for unobserved confounding and arbitrary\nstructural equations. The proposed approach only requires specification of the\ncausal graph and confounding structure and bounds the expected counterfactual\neffect of recourse actions. If the lower bound is above a certain threshold,\ni.e., on the other side of the decision boundary, recourse is guaranteed in\nexpectation.",
          "link": "http://arxiv.org/abs/2106.11849",
          "publishedOn": "2021-06-23T01:48:41.236Z",
          "wordCount": 600,
          "title": "Algorithmic Recourse in Partially and Fully Confounded Settings Through Bounding Counterfactual Effects. (arXiv:2106.11849v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1\">Ismail Elezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taixe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>",
          "description": "Deep neural networks have reached very high accuracy on object detection but\ntheir success hinges on large amounts of labeled data. To reduce the dependency\non labels, various active-learning strategies have been proposed, typically\nbased on the confidence of the detector. However, these methods are biased\ntowards best-performing classes and can lead to acquired datasets that are not\ngood representatives of the data in the testing set. In this work, we propose a\nunified framework for active learning, that considers both the uncertainty and\nthe robustness of the detector, ensuring that the network performs accurately\nin all classes. Furthermore, our method is able to pseudo-label the very\nconfident predictions, suppressing a potential distribution drift while further\nboosting the performance of the model. Experiments show that our method\ncomprehensively outperforms a wide range of active-learning methods on PASCAL\nVOC07+12 and MS-COCO, having up to a 7.7% relative improvement, or up to 82%\nreduction in labeling cost.",
          "link": "http://arxiv.org/abs/2106.11921",
          "publishedOn": "2021-06-23T01:48:41.227Z",
          "wordCount": 601,
          "title": "Towards Reducing Labeling Cost in Deep Object Detection. (arXiv:2106.11921v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11881",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Vlantis_P/0/1/0/all/0/1\">Panagiotis Vlantis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zavlanos_M/0/1/0/all/0/1\">Michael M. Zavlanos</a>",
          "description": "In this work, we consider the problem of learning a feed-forward neural\nnetwork (NN) controller to safely steer an arbitrarily shaped planar robot in a\ncompact and obstacle-occluded workspace. Unlike existing methods that depend\nstrongly on the density of data points close to the boundary of the safe state\nspace to train NN controllers with closed-loop safety guarantees, we propose an\napproach that lifts such assumptions on the data that are hard to satisfy in\npractice and instead allows for graceful safety violations, i.e., of a bounded\nmagnitude that can be spatially controlled. To do so, we employ reachability\nanalysis methods to encapsulate safety constraints in the training process.\nSpecifically, to obtain a computationally efficient over-approximation of the\nforward reachable set of the closed-loop system, we partition the robot's state\nspace into cells and adaptively subdivide the cells that contain states which\nmay escape the safe set under the trained control law. To do so, we first\ndesign appropriate under- and over-approximations of the robot's footprint to\nadaptively subdivide the configuration space into cells. Then, using the\noverlap between each cell's forward reachable set and the set of infeasible\nrobot configurations as a measure for safety violations, we introduce penalty\nterms into the loss function that penalize this overlap in the training\nprocess. As a result, our method can learn a safe vector field for the\nclosed-loop system and, at the same time, provide numerical worst-case bounds\non safety violation over the whole configuration space, defined by the overlap\nbetween the over-approximation of the forward reachable set of the closed-loop\nsystem and the set of unsafe states. Moreover, it can control the tradeoff\nbetween computational complexity and tightness of these bounds. Finally, we\nprovide a simulation study that verifies the efficacy of the proposed scheme.",
          "link": "http://arxiv.org/abs/2106.11881",
          "publishedOn": "2021-06-23T01:48:41.219Z",
          "wordCount": 742,
          "title": "Failing with Grace: Learning Neural Network Controllers that are Boundedly Unsafe. (arXiv:2106.11881v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Prieto_A/0/1/0/all/0/1\">&#xc1;ngel Gonz&#xe1;lez-Prieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bru_A/0/1/0/all/0/1\">Antonio Br&#xfa;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nuno_J/0/1/0/all/0/1\">Juan Carlos Nu&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Alvarez_J/0/1/0/all/0/1\">Jos&#xe9; Luis Gonz&#xe1;lez-&#xc1;lvarez</a>",
          "description": "Gender-based crime is one of the most concerning scourges of contemporary\nsociety. Governments worldwide have invested lots of economic and human\nresources to radically eliminate this threat. Despite these efforts, providing\naccurate predictions of the risk that a victim of gender violence has of being\nattacked again is still a very hard open problem. The development of new\nmethods for issuing accurate, fair and quick predictions would allow police\nforces to select the most appropriate measures to prevent recidivism. In this\nwork, we propose to apply Machine Learning (ML) techniques to create models\nthat accurately predict the recidivism risk of a gender-violence offender. The\nrelevance of the contribution of this work is threefold: (i) the proposed ML\nmethod outperforms the preexisting risk assessment algorithm based on classical\nstatistical techniques, (ii) the study has been conducted through an official\nspecific-purpose database with more than 40,000 reports of gender violence, and\n(iii) two new quality measures are proposed for assessing the effective police\nprotection that a model supplies and the overload in the invested resources\nthat it generates. Additionally, we propose a hybrid model that combines the\nstatistical prediction methods with the ML method, permitting authorities to\nimplement a smooth transition from the preexisting model to the ML-based model.\nThis hybrid nature enables a decision-making process to optimally balance\nbetween the efficiency of the police system and aggressiveness of the\nprotection measures taken.",
          "link": "http://arxiv.org/abs/2106.11847",
          "publishedOn": "2021-06-23T01:48:41.212Z",
          "wordCount": 712,
          "title": "Machine learning for risk assessment in gender-based crime. (arXiv:2106.11847v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1\">Deepthi Sreenivasaiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1\">Thomas Wollmann</a>",
          "description": "Image segmentation is a common and challenging task in autonomous driving.\nAvailability of sufficient pixel-level annotations for the training data is a\nhurdle. Active learning helps learning from small amounts of data by suggesting\nthe most promising samples for labeling. In this work, we propose a new\npool-based method for active learning, which proposes promising image regions,\nin each acquisition step. The problem is framed in an exploration-exploitation\nframework by combining an embedding based on Uniform Manifold Approximation to\nmodel representativeness with entropy as uncertainty measure to model\ninformativeness. We applied our proposed method to the challenging autonomous\ndriving data sets CamVid and Cityscapes and performed a quantitative comparison\nwith state-of-the-art methods. We find that our active learning method achieves\nbetter performance on CamVid compared to other methods, while on Cityscapes,\nthe performance lift was negligible.",
          "link": "http://arxiv.org/abs/2106.11858",
          "publishedOn": "2021-06-23T01:48:41.189Z",
          "wordCount": 565,
          "title": "MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuntian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yingtao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongxiao Zhang</a>",
          "description": "Partial differential equations (PDEs) are concise and understandable\nrepresentations of domain knowledge, which are essential for deepening our\nunderstanding of physical processes and predicting future responses. However,\nthe PDEs of many real-world problems are uncertain, which calls for PDE\ndiscovery. We propose the symbolic genetic algorithm (SGA-PDE) to discover\nopen-form PDEs directly from data without prior knowledge about the equation\nstructure. SGA-PDE focuses on the representation and optimization of PDE.\nFirstly, SGA-PDE uses symbolic mathematics to realize the flexible\nrepresentation of any given PDE, transforms a PDE into a forest, and converts\neach function term into a binary tree. Secondly, SGA-PDE adopts a specially\ndesigned genetic algorithm to efficiently optimize the binary trees by\niteratively updating the tree topology and node attributes. The SGA-PDE is\ngradient-free, which is a desirable characteristic in PDE discovery since it is\ndifficult to obtain the gradient between the PDE loss and the PDE structure. In\nthe experiment, SGA-PDE not only successfully discovered nonlinear Burgers'\nequation, Korteweg-de Vries (KdV) equation, and Chafee-Infante equation, but\nalso handled PDEs with fractional structure and compound functions that cannot\nbe solved by conventional PDE discovery methods.",
          "link": "http://arxiv.org/abs/2106.11927",
          "publishedOn": "2021-06-23T01:48:41.183Z",
          "wordCount": 655,
          "title": "Any equation is a forest: Symbolic genetic algorithm for discovering open-form partial differential equations (SGA-PDE). (arXiv:2106.11927v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chitsazan_N/0/1/0/all/0/1\">Nima Chitsazan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharpe_S/0/1/0/all/0/1\">Samuel Sharpe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katariya_D/0/1/0/all/0/1\">Dwipam Katariya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qianyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasethupathy_K/0/1/0/all/0/1\">Karthik Rajasethupathy</a>",
          "description": "As financial services (FS) companies have experienced drastic technology\ndriven changes, the availability of new data streams provides the opportunity\nfor more comprehensive customer understanding. We propose Dynamic Customer\nEmbeddings (DCE), a framework that leverages customers' digital activity and a\nwide range of financial context to learn dense representations of customers in\nthe FS industry. Our method examines customer actions and pageviews within a\nmobile or web digital session, the sequencing of the sessions themselves, and\nsnapshots of common financial features across our organization at the time of\nlogin. We test our customer embeddings using real world data in three\nprediction problems: 1) the intent of a customer in their next digital session,\n2) the probability of a customer calling the call centers after a session, and\n3) the probability of a digital session to be fraudulent. DCE showed\nperformance lift in all three downstream problems.",
          "link": "http://arxiv.org/abs/2106.11880",
          "publishedOn": "2021-06-23T01:48:41.175Z",
          "wordCount": 592,
          "title": "Dynamic Customer Embeddings for Financial Service Applications. (arXiv:2106.11880v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sordello_M/0/1/0/all/0/1\">Matteo Sordello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiqi Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jinshuo Dong</a>",
          "description": "In this paper, we consider the framework of privacy amplification via\niteration, which is originally proposed by Feldman et al. and subsequently\nsimplified by Asoodeh et al. in their analysis via the contraction coefficient.\nThis line of work focuses on the study of the privacy guarantees obtained by\nthe projected noisy stochastic gradient descent (PNSGD) algorithm with hidden\nintermediate updates. A limitation in the existing literature is that only the\nearly stopped PNSGD has been studied, while no result has been proved on the\nmore widely-used PNSGD applied on a shuffled dataset. Moreover, no scheme has\nbeen yet proposed regarding how to decrease the injected noise when new data\nare received in an online fashion. In this work, we first prove a privacy\nguarantee for shuffled PNSGD, which is investigated asymptotically when the\nnoise is fixed for each sample size $n$ but reduced at a predetermined rate\nwhen $n$ increases, in order to achieve the convergence of privacy loss. We\nthen analyze the online setting and provide a faster decaying scheme for the\nmagnitude of the injected noise that also guarantees the convergence of privacy\nloss.",
          "link": "http://arxiv.org/abs/2106.11767",
          "publishedOn": "2021-06-23T01:48:41.169Z",
          "wordCount": 633,
          "title": "Privacy Amplification via Iteration for Shuffled and Online PNSGD. (arXiv:2106.11767v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khavari_B/0/1/0/all/0/1\">Behnoush Khavari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1\">Guillaume Rabusseau</a>",
          "description": "Tensor network methods have been a key ingredient of advances in condensed\nmatter physics and have recently sparked interest in the machine learning\ncommunity for their ability to compactly represent very high-dimensional\nobjects. Tensor network methods can for example be used to efficiently learn\nlinear models in exponentially large feature spaces [Stoudenmire and Schwab,\n2016]. In this work, we derive upper and lower bounds on the VC dimension and\npseudo-dimension of a large class of tensor network models for classification,\nregression and completion. Our upper bounds hold for linear models\nparameterized by arbitrary tensor network structures, and we derive lower\nbounds for common tensor decomposition models~(CP, Tensor Train, Tensor Ring\nand Tucker) showing the tightness of our general upper bound. These results are\nused to derive a generalization bound which can be applied to classification\nwith low rank matrices as well as linear classifiers based on any of the\ncommonly used tensor decomposition models. As a corollary of our results, we\nobtain a bound on the VC dimension of the matrix product state classifier\nintroduced in [Stoudenmire and Schwab, 2016] as a function of the so-called\nbond dimension~(i.e. tensor train rank), which answers an open problem listed\nby Cirac, Garre-Rubio and P\\'erez-Garc\\'ia in [Cirac et al., 2019].",
          "link": "http://arxiv.org/abs/2106.11827",
          "publishedOn": "2021-06-23T01:48:41.161Z",
          "wordCount": 639,
          "title": "Lower and Upper Bounds on the VC-Dimension of Tensor Network Models. (arXiv:2106.11827v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>",
          "description": "Cross-silo federated learning (FL) is a distributed learning approach where\nclients train a global model cooperatively while keeping their local data\nprivate. Different from cross-device FL, clients in cross-silo FL are usually\norganizations or companies which may execute multiple cross-silo FL processes\nrepeatedly due to their time-varying local data sets, and aim to optimize their\nlong-term benefits by selfishly choosing their participation levels. While\nthere has been some work on incentivizing clients to join FL, the analysis of\nthe long-term selfish participation behaviors of clients in cross-silo FL\nremains largely unexplored. In this paper, we analyze the selfish participation\nbehaviors of heterogeneous clients in cross-silo FL. Specifically, we model the\nlong-term selfish participation behaviors of clients as an infinitely repeated\ngame, with the stage game being a selfish participation game in one cross-silo\nFL process (SPFL). For the stage game SPFL, we derive the unique Nash\nequilibrium (NE), and propose a distributed algorithm for each client to\ncalculate its equilibrium participation strategy. For the long-term\ninteractions among clients, we derive a cooperative strategy for clients which\nminimizes the number of free riders while increasing the amount of local data\nfor model training. We show that enforced by a punishment strategy, such a\ncooperative strategy is a SPNE of the infinitely repeated game, under which\nsome clients who are free riders at the NE of the stage game choose to be\n(partial) contributors. We further propose an algorithm to calculate the\noptimal SPNE which minimizes the number of free riders while maximizing the\namount of local data for model training. Simulation results show that our\nproposed cooperative strategy at the optimal SPNE can effectively reduce the\nnumber of free riders and increase the amount of local data for model training.",
          "link": "http://arxiv.org/abs/2106.11814",
          "publishedOn": "2021-06-23T01:48:41.141Z",
          "wordCount": 730,
          "title": "Enabling Long-Term Cooperation in Cross-Silo Federated Learning: A Repeated Game Perspective. (arXiv:2106.11814v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mora_A/0/1/0/all/0/1\">A.M. Mora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esparcia_Alcazar_A/0/1/0/all/0/1\">A.I. Esparcia-Alc&#xe1;zar</a>",
          "description": "Volumen with the Late-Breaking Abstracts submitted to the Evo* 2021\nConference, held online from 7 to 9 of April 2021. These papers present ongoing\nresearch and preliminary results investigating on the application of different\napproaches of Bioinspired Methods (mainly Evolutionary Computation) to\ndifferent problems, most of them real world ones.",
          "link": "http://arxiv.org/abs/2106.11804",
          "publishedOn": "2021-06-23T01:48:41.134Z",
          "wordCount": 503,
          "title": "Evo* 2021 -- Late-Breaking Abstracts Volume. (arXiv:2106.11804v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Ray Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahavy_T/0/1/0/all/0/1\">Tom Zahavy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhongwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Adam White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_M/0/1/0/all/0/1\">Matteo Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1\">Charles Blundell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasselt_H/0/1/0/all/0/1\">Hado van Hasselt</a>",
          "description": "Off-policy learning allows us to learn about possible policies of behavior\nfrom experience generated by a different behavior policy. Temporal difference\n(TD) learning algorithms can become unstable when combined with function\napproximation and off-policy sampling - this is known as the ''deadly triad''.\nEmphatic temporal difference (ETD($\\lambda$)) algorithm ensures convergence in\nthe linear case by appropriately weighting the TD($\\lambda$) updates. In this\npaper, we extend the use of emphatic methods to deep reinforcement learning\nagents. We show that naively adapting ETD($\\lambda$) to popular deep\nreinforcement learning algorithms, which use forward view multi-step returns,\nresults in poor performance. We then derive new emphatic algorithms for use in\nthe context of such algorithms, and we demonstrate that they provide noticeable\nbenefits in small problems designed to highlight the instability of TD methods.\nFinally, we observed improved performance when applying these algorithms at\nscale on classic Atari games from the Arcade Learning Environment.",
          "link": "http://arxiv.org/abs/2106.11779",
          "publishedOn": "2021-06-23T01:48:41.128Z",
          "wordCount": 602,
          "title": "Emphatic Algorithms for Deep Reinforcement Learning. (arXiv:2106.11779v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11853",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lienen_J/0/1/0/all/0/1\">Julian Lienen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>",
          "description": "Self-training is an effective approach to semi-supervised learning. The key\nidea is to let the learner itself iteratively generate \"pseudo-supervision\" for\nunlabeled instances based on its current hypothesis. In combination with\nconsistency regularization, pseudo-labeling has shown promising performance in\nvarious domains, for example in computer vision. To account for the\nhypothetical nature of the pseudo-labels, these are commonly provided in the\nform of probability distributions. Still, one may argue that even a probability\ndistribution represents an excessive level of informedness, as it suggests that\nthe learner precisely knows the ground-truth conditional probabilities. In our\napproach, we therefore allow the learner to label instances in the form of\ncredal sets, that is, sets of (candidate) probability distributions. Thanks to\nthis increased expressiveness, the learner is able to represent uncertainty and\na lack of knowledge in a more flexible and more faithful manner. To learn from\nweakly labeled data of that kind, we leverage methods that have recently been\nproposed in the realm of so-called superset learning. In an exhaustive\nempirical evaluation, we compare our methodology to state-of-the-art\nself-supervision approaches, showing competitive to superior performance\nespecially in low-label scenarios incorporating a high degree of uncertainty.",
          "link": "http://arxiv.org/abs/2106.11853",
          "publishedOn": "2021-06-23T01:48:41.121Z",
          "wordCount": 623,
          "title": "Credal Self-Supervised Learning. (arXiv:2106.11853v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1\">Gustavo H. de Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Papa</a>",
          "description": "A graph-inspired classifier, known as Optimum-Path Forest (OPF), has proven\nto be a state-of-the-art algorithm comparable to Logistic Regressors, Support\nVector Machines in a wide variety of tasks. Recently, its Python-based version,\ndenoted as OPFython, has been proposed to provide a more friendly framework and\na faster prototyping environment. Nevertheless, Python-based algorithms are\nslower than their counterpart C-based algorithms, impacting their performance\nwhen confronted with large amounts of data. Therefore, this paper proposed a\nsimple yet highly efficient speed up using the Numba package, which accelerates\nNumpy-based calculations and attempts to increase the algorithm's overall\nperformance. Experimental results showed that the proposed approach achieved\nbetter results than the na\\\"ive Python-based OPF and speeded up its distance\nmeasurement calculation.",
          "link": "http://arxiv.org/abs/2106.11828",
          "publishedOn": "2021-06-23T01:48:41.114Z",
          "wordCount": 558,
          "title": "Speeding Up OPFython with Numba. (arXiv:2106.11828v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soutif__Cormerais_A/0/1/0/all/0/1\">Albin Soutif--Cormerais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masana_M/0/1/0/all/0/1\">Marc Masana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost Van de Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1\">Bart&#x142;omiej Twardowski</a>",
          "description": "In class-incremental learning, an agent with limited resources needs to learn\na sequence of classification tasks, forming an ever growing classification\nproblem, with the constraint of not being able to access data from previous\ntasks. The main difference with task-incremental learning, where a task-ID is\navailable at inference time, is that the learner also needs to perform\ncross-task discrimination, i.e. distinguish between classes that have not been\nseen together. Approaches to tackle this problem are numerous and mostly make\nuse of an external memory (buffer) of non-negligible size. In this paper, we\nablate the learning of cross-task features and study its influence on the\nperformance of basic replay strategies used for class-IL. We also define a new\nforgetting measure for class-incremental learning, and see that forgetting is\nnot the principal cause of low performance. Our experimental results show that\nfuture algorithms for class-incremental learning should not only prevent\nforgetting, but also aim to improve the quality of the cross-task features.\nThis is especially important when the number of classes per task is small.",
          "link": "http://arxiv.org/abs/2106.11930",
          "publishedOn": "2021-06-23T01:48:41.095Z",
          "wordCount": 618,
          "title": "On the importance of cross-task features for class-incremental learning. (arXiv:2106.11930v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11759",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mitra_V/0/1/0/all/0/1\">Vikramjit Mitra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zifang Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lea_C/0/1/0/all/0/1\">Colin Lea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tooley_L/0/1/0/all/0/1\">Lauren Tooley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1\">Sarah Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Botten_D/0/1/0/all/0/1\">Darren Botten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Palekar_A/0/1/0/all/0/1\">Ashwini Palekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thelapurath_S/0/1/0/all/0/1\">Shrinath Thelapurath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Georgiou_P/0/1/0/all/0/1\">Panayiotis Georgiou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kajarekar_S/0/1/0/all/0/1\">Sachin Kajarekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bigham_J/0/1/0/all/0/1\">Jefferey Bigham</a>",
          "description": "Dysfluencies and variations in speech pronunciation can severely degrade\nspeech recognition performance, and for many individuals with\nmoderate-to-severe speech disorders, voice operated systems do not work.\nCurrent speech recognition systems are trained primarily with data from fluent\nspeakers and as a consequence do not generalize well to speech with\ndysfluencies such as sound or word repetitions, sound prolongations, or audible\nblocks. The focus of this work is on quantitative analysis of a consumer speech\nrecognition system on individuals who stutter and production-oriented\napproaches for improving performance for common voice assistant tasks (i.e.,\n\"what is the weather?\"). At baseline, this system introduces a significant\nnumber of insertion and substitution errors resulting in intended speech Word\nError Rates (isWER) that are 13.64\\% worse (absolute) for individuals with\nfluency disorders. We show that by simply tuning the decoding parameters in an\nexisting hybrid speech recognition system one can improve isWER by 24\\%\n(relative) for individuals with fluency disorders. Tuning these parameters\ntranslates to 3.6\\% better domain recognition and 1.7\\% better intent\nrecognition relative to the default setup for the 18 study participants across\nall stuttering severities.",
          "link": "http://arxiv.org/abs/2106.11759",
          "publishedOn": "2021-06-23T01:48:41.087Z",
          "wordCount": 671,
          "title": "Analysis and Tuning of a Voice Assistant System for Dysfluent Speech. (arXiv:2106.11759v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xuyang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homaifar_A/0/1/0/all/0/1\">Abdollah Homaifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_M/0/1/0/all/0/1\">Mrinmoy Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girma_A/0/1/0/all/0/1\">Abenezer Girma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunstel_E/0/1/0/all/0/1\">Edward Tunstel</a>",
          "description": "The non-stationary nature of data streams strongly challenges traditional\nmachine learning techniques. Although some solutions have been proposed to\nextend traditional machine learning techniques for handling data streams, these\napproaches either require an initial label set or rely on specialized design\nparameters. The overlap among classes and the labeling of data streams\nconstitute other major challenges for classifying data streams. In this paper,\nwe proposed a clustering-based data stream classification framework to handle\nnon-stationary data streams without utilizing an initial label set. A\ndensity-based stream clustering procedure is used to capture novel concepts\nwith a dynamic threshold and an effective active label querying strategy is\nintroduced to continuously learn the new concepts from the data streams. The\nsub-cluster structure of each cluster is explored to handle the overlap among\nclasses. Experimental results and quantitative comparison studies reveal that\nthe proposed method provides statistically better or comparable performance\nthan the existing methods.",
          "link": "http://arxiv.org/abs/2106.11823",
          "publishedOn": "2021-06-23T01:48:41.080Z",
          "wordCount": 593,
          "title": "A Clustering-based Framework for Classifying Data Streams. (arXiv:2106.11823v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+BK_V/0/1/0/all/0/1\">Vanya BK</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_B/0/1/0/all/0/1\">Balaji Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_A/0/1/0/all/0/1\">Aniket Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Devbrat Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Arvind Agarwal</a>",
          "description": "Explaining Graph Neural Networks predictions to end users of AI applications\nin easily understandable terms remains an unsolved problem. In particular, we\ndo not have well developed methods for automatically evaluating explanations,\nin ways that are closer to how users consume those explanations. Based on\nrecent application trends and our own experiences in real world problems, we\npropose automatic evaluation approaches for GNN Explanations.",
          "link": "http://arxiv.org/abs/2106.11864",
          "publishedOn": "2021-06-23T01:48:41.073Z",
          "wordCount": 512,
          "title": "Towards Automated Evaluation of Explanations in Graph Neural Networks. (arXiv:2106.11864v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11936",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tod_G/0/1/0/all/0/1\">Georges Tod</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Both_G/0/1/0/all/0/1\">Gert-Jan Both</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kusters_R/0/1/0/all/0/1\">Remy Kusters</a>",
          "description": "Discovering the partial differential equations underlying a spatio-temporal\ndatasets from very limited observations is of paramount interest in many\nscientific fields. However, it remains an open question to know when model\ndiscovery algorithms based on sparse regression can actually recover the\nunderlying physical processes. We trace back the poor of performance of Lasso\nbased model discovery algorithms to its potential variable selection\ninconsistency: meaning that even if the true model is present in the library,\nit might not be selected. By first revisiting the irrepresentability condition\n(IRC) of the Lasso, we gain some insights of when this might occur. We then\nshow that the adaptive Lasso will have more chances of verifying the IRC than\nthe Lasso and propose to integrate it within a deep learning model discovery\nframework with stability selection and error control. Experimental results show\nwe can recover several nonlinear and chaotic canonical PDEs with a single set\nof hyperparameters from a very limited number of samples at high noise levels.",
          "link": "http://arxiv.org/abs/2106.11936",
          "publishedOn": "2021-06-23T01:48:41.066Z",
          "wordCount": 590,
          "title": "Sparsistent Model Discovery. (arXiv:2106.11936v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iofinova_E/0/1/0/all/0/1\">Eugenia Iofinova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konstantinov_N/0/1/0/all/0/1\">Nikola Konstantinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1\">Christoph H. Lampert</a>",
          "description": "Fairness-aware learning aims at constructing classifiers that not only make\naccurate predictions, but do not discriminate against specific groups. It is a\nfast-growing area of machine learning with far-reaching societal impact.\nHowever, existing fair learning methods are vulnerable to accidental or\nmalicious artifacts in the training data, which can cause them to unknowingly\nproduce unfair classifiers. In this work we address the problem of fair\nlearning from unreliable training data in the robust multisource setting, where\nthe available training data comes from multiple sources, a fraction of which\nmight be not representative of the true data distribution. We introduce FLEA, a\nfiltering-based algorithm that allows the learning system to identify and\nsuppress those data sources that would have a negative impact on fairness or\naccuracy if they were used for training. We show the effectiveness of our\napproach by a diverse range of experiments on multiple datasets. Additionally\nwe prove formally that, given enough data, FLEA protects the learner against\nunreliable data as long as the fraction of affected data sources is less than\nhalf.",
          "link": "http://arxiv.org/abs/2106.11732",
          "publishedOn": "2021-06-23T01:48:41.060Z",
          "wordCount": 606,
          "title": "FLEA: Provably Fair Multisource Learning from Unreliable Training Data. (arXiv:2106.11732v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen-Chi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raskin_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Raskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raedt_L/0/1/0/all/0/1\">Luc De Raedt</a>",
          "description": "Model checking has been developed for verifying the behaviour of systems with\nstochastic and non-deterministic behavior. It is used to provide guarantees\nabout such systems. While most model checking methods focus on propositional\nmodels, various probabilistic planning and reinforcement frameworks deal with\nrelational domains, for instance, STRIPS planning and relational Markov\nDecision Processes. Using propositional model checking in relational settings\nrequires one to ground the model, which leads to the well known state explosion\nproblem and intractability. We present pCTL-REBEL, a lifted model checking\napproach for verifying pCTL properties on relational MDPs. It extends REBEL,\nthe relational Bellman update operator, which is a lifted value iteration\napproach for model-based relational reinforcement learning, toward relational\nmodel-checking. PCTL-REBEL is lifted, which means that rather than grounding,\nthe model exploits symmetries and reasons at an abstract relational level.\nTheoretically, we show that the pCTL model checking approach is decidable for\nrelational MDPs even for possibly infinite domains provided that the states\nhave a bounded size. Practically, we contribute algorithms and an\nimplementation of lifted relational model checking, and we show that the lifted\napproach improves the scalability of the model checking approach.",
          "link": "http://arxiv.org/abs/2106.11735",
          "publishedOn": "2021-06-23T01:48:41.041Z",
          "wordCount": 623,
          "title": "Lifted Model Checking for Relational MDPs. (arXiv:2106.11735v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepti Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Maanak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_S/0/1/0/all/0/1\">Smriti Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tosun_A/0/1/0/all/0/1\">Ali Saman Tosun</a>",
          "description": "The growth in Remote Patient Monitoring (RPM) services using wearable and\nnon-wearable Internet of Medical Things (IoMT) promises to improve the quality\nof diagnosis and facilitate timely treatment for a gamut of medical conditions.\nAt the same time, the proliferation of IoMT devices increases the potential for\nmalicious activities that can lead to catastrophic results including theft of\npersonal information, data breach, and compromised medical devices, putting\nhuman lives at risk. IoMT devices generate tremendous amount of data that\nreflect user behavior patterns including both personal and day-to-day social\nactivities along with daily routine health monitoring. In this context, there\nare possibilities of anomalies generated due to various reasons including\nunexpected user behavior, faulty sensor, or abnormal values from\nmalicious/compromised devices. To address this problem, there is an imminent\nneed to develop a framework for securing the smart health care infrastructure\nto identify and mitigate anomalies. In this paper, we present an anomaly\ndetection model for RPM utilizing IoMT and smart home devices. We propose\nHidden Markov Model (HMM) based anomaly detection that analyzes normal user\nbehavior in the context of RPM comprising both smart home and smart health\ndevices, and identifies anomalous user behavior. We design a testbed with\nmultiple IoMT devices and home sensors to collect data and use the HMM model to\ntrain using network and user behavioral data. Proposed HMM based anomaly\ndetection model achieved over 98% accuracy in identifying the anomalies in the\ncontext of RPM.",
          "link": "http://arxiv.org/abs/2106.11844",
          "publishedOn": "2021-06-23T01:48:41.032Z",
          "wordCount": 676,
          "title": "Detecting Anomalous User Behavior in Remote Patient Monitoring. (arXiv:2106.11844v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sipka_T/0/1/0/all/0/1\">Tomas Sipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulc_M/0/1/0/all/0/1\">Milan Sulc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>",
          "description": "In many computer vision classification tasks, class priors at test time often\ndiffer from priors on the training set. In the case of such prior shift,\nclassifiers must be adapted correspondingly to maintain close to optimal\nperformance. This paper analyzes methods for adaptation of probabilistic\nclassifiers to new priors and for estimating new priors on an unlabeled test\nset. We propose a novel method to address a known issue of prior estimation\nmethods based on confusion matrices, where inconsistent estimates of decision\nprobabilities and confusion matrices lead to negative values in the estimated\npriors. Experiments on fine-grained image classification datasets provide\ninsight into the best practice of prior shift estimation and classifier\nadaptation and show that the proposed method achieves state-of-the-art results\nin prior adaptation. Applying the best practice to two tasks with naturally\nimbalanced priors, learning from web-crawled images and plant species\nclassification, increased the recognition accuracy by 1.1% and 3.4%\nrespectively.",
          "link": "http://arxiv.org/abs/2106.11695",
          "publishedOn": "2021-06-23T01:48:41.023Z",
          "wordCount": 592,
          "title": "The Hitchhiker's Guide to Prior-Shift Adaptation. (arXiv:2106.11695v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11740",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weihao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zihang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qibin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "Modern pre-trained language models are mostly built upon backbones stacking\nself-attention and feed-forward layers in an interleaved order. In this paper,\nbeyond this stereotyped layer pattern, we aim to improve pre-trained models by\nexploiting layer variety from two aspects: the layer type set and the layer\norder. Specifically, besides the original self-attention and feed-forward\nlayers, we introduce convolution into the layer type set, which is\nexperimentally found beneficial to pre-trained models. Furthermore, beyond the\noriginal interleaved order, we explore more layer orders to discover more\npowerful architectures. However, the introduced layer variety leads to a large\narchitecture space of more than billions of candidates, while training a single\ncandidate model from scratch already requires huge computation cost, making it\nnot affordable to search such a space by directly training large amounts of\ncandidate models. To solve this problem, we first pre-train a supernet from\nwhich the weights of all candidate models can be inherited, and then adopt an\nevolutionary algorithm guided by pre-training accuracy to find the optimal\narchitecture. Extensive experiments show that LV-BERT model obtained by our\nmethod outperforms BERT and its variants on various downstream tasks. For\nexample, LV-BERT-small achieves 78.8 on the GLUE testing set, 1.8 higher than\nthe strong baseline ELECTRA-small.",
          "link": "http://arxiv.org/abs/2106.11740",
          "publishedOn": "2021-06-23T01:48:41.016Z",
          "wordCount": 661,
          "title": "LV-BERT: Exploiting Layer Variety for BERT. (arXiv:2106.11740v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11769",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Haiyang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jihan Zhang</a>",
          "description": "Speech production is a dynamic procedure, which involved multi human organs\nincluding the tongue, jaw and lips. Modeling the dynamics of the vocal tract\ndeformation is a fundamental problem to understand the speech, which is the\nmost common way for human daily communication. Researchers employ several\nsensory streams to describe the process simultaneously, which are\nincontrovertibly statistically related to other streams. In this paper, we\naddress the following question: given an observable image sequences of lips,\ncan we picture the corresponding tongue motion. We formulated this problem as\nthe self-supervised learning problem, and employ the two-stream convolutional\nnetwork and long-short memory network for the learning task, with the attention\nmechanism. We evaluate the performance of the proposed method by leveraging the\nunlabeled lip videos to predict an upcoming ultrasound tongue image sequence.\nThe results show that our model is able to generate images that close to the\nreal ultrasound tongue images, and results in the matching between two imaging\nmodalities.",
          "link": "http://arxiv.org/abs/2106.11769",
          "publishedOn": "2021-06-23T01:48:40.998Z",
          "wordCount": 633,
          "title": "Improving Ultrasound Tongue Image Reconstruction from Lip Images Using Self-supervised Learning and Attention Mechanism. (arXiv:2106.11769v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Creagh_A/0/1/0/all/0/1\">Andrew P. Creagh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipsmeier_F/0/1/0/all/0/1\">Florian Lipsmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindemann_M/0/1/0/all/0/1\">Michael Lindemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vos_M/0/1/0/all/0/1\">Maarten De Vos</a>",
          "description": "The emergence of digital technologies such as smartphones in healthcare\napplications have demonstrated the possibility of developing rich, continuous,\nand objective measures of multiple sclerosis (MS) disability that can be\nadministered remotely and out-of-clinic. In this work, deep convolutional\nneural networks (DCNN) applied to smartphone inertial sensor data were shown to\nbetter distinguish healthy from MS participant ambulation, compared to standard\nSupport Vector Machine (SVM) feature-based methodologies. To overcome the\ntypical limitations associated with remotely generated health data, such as low\nsubject numbers, sparsity, and heterogeneous data, a transfer learning (TL)\nmodel from similar large open-source datasets was proposed. Our TL framework\nutilised the ambulatory information learned on Human Activity Recognition (HAR)\ntasks collected from similar smartphone-based sensor data. A lack of\ntransparency of \"black-box\" deep networks remains one of the largest stumbling\nblocks to the wider acceptance of deep learning for clinical applications.\nEnsuing work therefore aimed to visualise DCNN decisions attributed by\nrelevance heatmaps using Layer-Wise Relevance Propagation (LRP). Through the\nLRP framework, the patterns captured from smartphone-based inertial sensor data\nthat were reflective of those who are healthy versus persons with MS (PwMS)\ncould begin to be established and understood. Interpretations suggested that\ncadence-based measures, gait speed, and ambulation-related signal perturbations\nwere distinct characteristics that distinguished MS disability from healthy\nparticipants. Robust and interpretable outcomes, generated from high-frequency\nout-of-clinic assessments, could greatly augment the current in-clinic\nassessment picture for PwMS, to inform better disease management techniques,\nand enable the development of better therapeutic interventions.",
          "link": "http://arxiv.org/abs/2103.09171",
          "publishedOn": "2021-06-23T01:48:40.991Z",
          "wordCount": 729,
          "title": "Interpretable Deep Learning for the Remote Characterisation of Ambulation in Multiple Sclerosis using Smartphones. (arXiv:2103.09171v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1\">Yifei Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We study the off-policy evaluation (OPE) problem in reinforcement learning\nwith linear function approximation, which aims to estimate the value function\nof a target policy based on the offline data collected by a behavior policy. We\npropose to incorporate the variance information of the value function to\nimprove the sample efficiency of OPE. More specifically, for time-inhomogeneous\nepisodic linear Markov decision processes (MDPs), we propose an algorithm,\nVA-OPE, which uses the estimated variance of the value function to reweight the\nBellman residual in Fitted Q-Iteration. We show that our algorithm achieves a\ntighter error bound than the best-known result. We also provide a fine-grained\ncharacterization of the distribution shift between the behavior policy and the\ntarget policy. Extensive numerical experiments corroborate our theory.",
          "link": "http://arxiv.org/abs/2106.11960",
          "publishedOn": "2021-06-23T01:48:40.958Z",
          "wordCount": 567,
          "title": "Variance-Aware Off-Policy Evaluation with Linear Function Approximation. (arXiv:2106.11960v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11731",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Langner_T/0/1/0/all/0/1\">Taro Langner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mora_A/0/1/0/all/0/1\">Andr&#xe9;s Mart&#xed;nez Mora</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strand_R/0/1/0/all/0/1\">Robin Strand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahlstrom_H/0/1/0/all/0/1\">H&#xe5;kan Ahlstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kullberg_J/0/1/0/all/0/1\">Joel Kullberg</a>",
          "description": "UK Biobank (UKB) is conducting a large-scale study of more than half a\nmillion volunteers, collecting health-related information on genetics,\nlifestyle, blood biochemistry, and more. Medical imaging furthermore targets\n100,000 subjects, with 70,000 follow-up sessions, enabling measurements of\norgans, muscle, and body composition. With up to 170,000 mounting MR images,\nvarious methodologies are accordingly engaged in large-scale image analysis.\nThis work presents an experimental inference engine that can automatically\npredict a comprehensive profile of subject metadata from UKB neck-to-knee body\nMRI. In cross-validation, it accurately inferred baseline characteristics such\nas age, height, weight, and sex, but also emulated measurements of body\ncomposition by DXA, organ volumes, and abstract properties like grip strength,\npulse rate, and type 2 diabetic status (AUC: 0.866). The proposed system can\nautomatically analyze thousands of subjects within hours and provide individual\nconfidence intervals. The underlying methodology is based on convolutional\nneural networks for image-based mean-variance regression on two-dimensional\nrepresentations of the MRI data. This work aims to make the proposed system\navailable for free to researchers, who can use it to obtain fast and\nfully-automated estimates of 72 different measurements immediately upon release\nof new UK Biobank image data.",
          "link": "http://arxiv.org/abs/2106.11731",
          "publishedOn": "2021-06-23T01:48:40.951Z",
          "wordCount": 650,
          "title": "MIMIR: Deep Regression for Automated Analysis of UK Biobank Body MRI. (arXiv:2106.11731v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chennu_S/0/1/0/all/0/1\">Srivas Chennu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1\">Jamie Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liyanagama_P/0/1/0/all/0/1\">Puli Liyanagama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohr_P/0/1/0/all/0/1\">Phil Mohr</a>",
          "description": "Stochastic delays in feedback lead to unstable sequential learning using\nmulti-armed bandits. Recently, empirical Bayesian shrinkage has been shown to\nimprove reward estimation in bandit learning. Here, we propose a novel\nadaptation to shrinkage that estimates smoothed reward estimates from windowed\ncumulative inputs, to deal with incomplete knowledge from delayed feedback and\nnon-stationary rewards. Using numerical simulations, we show that this\nadaptation retains the benefits of shrinkage, and improves the stability of\nreward estimation by more than 50%. Our proposal reduces variability in\ntreatment allocations to the best arm by up to 3.8x, and improves statistical\naccuracy - with up to 8% improvement in true positive rates and 37% reduction\nin false positive rates. Together, these advantages enable control of the\ntrade-off between speed and stability of adaptation, and facilitate\nhuman-in-the-loop sequential optimisation.",
          "link": "http://arxiv.org/abs/2106.11294",
          "publishedOn": "2021-06-23T01:48:40.944Z",
          "wordCount": 599,
          "title": "Smooth Sequential Optimisation with Delayed Feedback. (arXiv:2106.11294v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06671",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_Tang_T/0/1/0/all/0/1\">Thanh Nguyen-Tang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tran_The_H/0/1/0/all/0/1\">Hung Tran-The</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>",
          "description": "We study the statistical theory of offline reinforcement learning (RL) with\ndeep ReLU network function approximation. We analyze a variant of fitted-Q\niteration (FQI) algorithm under a new dynamic condition that we call Besov\ndynamic closure, which encompasses the conditions from prior analyses for deep\nneural network function approximation. Under Besov dynamic closure, we prove\nthat the FQI-type algorithm enjoys the sample complexity of\n$\\tilde{\\mathcal{O}}\\left( \\kappa^{1 + d/\\alpha} \\cdot \\epsilon^{-2 -\n2d/\\alpha} \\right)$ where $\\kappa$ is a distribution shift measure, $d$ is the\ndimensionality of the state-action space, $\\alpha$ is the (possibly fractional)\nsmoothness parameter of the underlying MDP, and $\\epsilon$ is a user-specified\nprecision. This is an improvement over the sample complexity of\n$\\tilde{\\mathcal{O}}\\left( K \\cdot \\kappa^{2 + d/\\alpha} \\cdot \\epsilon^{-2 -\nd/\\alpha} \\right)$ in the prior result [Yang et al., 2019] where $K$ is an\nalgorithmic iteration number which is arbitrarily large in practice.\nImportantly, our sample complexity is obtained under the new general dynamic\ncondition and a data-dependent structure where the latter is either ignored in\nprior algorithms or improperly handled by prior analyses. This is the first\ncomprehensive analysis for offline RL with deep ReLU network function\napproximation under a general setting.",
          "link": "http://arxiv.org/abs/2103.06671",
          "publishedOn": "2021-06-23T01:48:40.933Z",
          "wordCount": 654,
          "title": "Sample Complexity of Offline Reinforcement Learning with Deep ReLU Networks. (arXiv:2103.06671v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stankevicius_L/0/1/0/all/0/1\">Lukas Stankevi&#x10d;ius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukosevicius_M/0/1/0/all/0/1\">Mantas Luko&#x161;evi&#x10d;ius</a>",
          "description": "In this work, we train the first monolingual Lithuanian transformer model on\na relatively large corpus of Lithuanian news articles and compare various\noutput decoding algorithms for abstractive news summarization. We achieve an\naverage ROUGE-2 score 0.163, generated summaries are coherent and look\nimpressive at first glance. However, some of them contain misleading\ninformation that is not so easy to spot. We describe all the technical details\nand share our trained model and accompanying code in an online open-source\nrepository, as well as some characteristic samples of the generated summaries.",
          "link": "http://arxiv.org/abs/2105.03279",
          "publishedOn": "2021-06-23T01:48:40.910Z",
          "wordCount": 569,
          "title": "Generating abstractive summaries of Lithuanian news articles using a transformer model. (arXiv:2105.03279v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1\">Baoyu Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>",
          "description": "Networks have been widely used to represent the relations between objects\nsuch as academic networks and social networks, and learning embedding for\nnetworks has thus garnered plenty of research attention. Self-supervised\nnetwork representation learning aims at extracting node embedding without\nexternal supervision. Recently, maximizing the mutual information between the\nlocal node embedding and the global summary (e.g. Deep Graph Infomax, or DGI\nfor short) has shown promising results on many downstream tasks such as node\nclassification. However, there are two major limitations of DGI. Firstly, DGI\nmerely considers the extrinsic supervision signal (i.e., the mutual information\nbetween node embedding and global summary) while ignores the intrinsic signal\n(i.e., the mutual dependence between node embedding and node attributes).\nSecondly, nodes in a real-world network are usually connected by multiple edges\nwith different relations, while DGI does not fully explore the various\nrelations among nodes. To address the above-mentioned problems, we propose a\nnovel framework, called High-order Deep Multiplex Infomax (HDMI), for learning\nnode embedding on multiplex networks in a self-supervised way. To be more\nspecific, we first design a joint supervision signal containing both extrinsic\nand intrinsic mutual information by high-order mutual information, and we\npropose a High-order Deep Infomax (HDI) to optimize the proposed supervision\nsignal. Then we propose an attention based fusion module to combine node\nembedding from different layers of the multiplex network. Finally, we evaluate\nthe proposed HDMI on various downstream tasks such as unsupervised clustering\nand supervised classification. The experimental results show that HDMI achieves\nstate-of-the-art performance on these tasks.",
          "link": "http://arxiv.org/abs/2102.07810",
          "publishedOn": "2021-06-23T01:48:40.903Z",
          "wordCount": 742,
          "title": "HDMI: High-order Deep Multiplex Infomax. (arXiv:2102.07810v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jambulapati_A/0/1/0/all/0/1\">Arun Jambulapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jerry Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schramm_T/0/1/0/all/0/1\">Tselil Schramm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_K/0/1/0/all/0/1\">Kevin Tian</a>",
          "description": "We study fast algorithms for statistical regression problems under the strong\ncontamination model, where the goal is to approximately optimize a generalized\nlinear model (GLM) given adversarially corrupted samples. Prior works in this\nline of research were based on the robust gradient descent framework of Prasad\net. al., a first-order method using biased gradient queries, or the Sever\nframework of Diakonikolas et. al., an iterative outlier-removal method calling\na stationary point finder.\n\nWe present nearly-linear time algorithms for robust regression problems with\nimproved runtime or estimation guarantees compared to the state-of-the-art. For\nthe general case of smooth GLMs (e.g. logistic regression), we show that the\nrobust gradient descent framework of Prasad et. al. can be accelerated, and\nshow our algorithm extends to optimizing the Moreau envelopes of Lipschitz GLMs\n(e.g. support vector machines), answering several open questions in the\nliterature.\n\nFor the well-studied case of robust linear regression, we present an\nalternative approach obtaining improved estimation rates over prior\nnearly-linear time algorithms. Interestingly, our method starts with an\nidentifiability proof introduced in the context of the sum-of-squares algorithm\nof Bakshi and Prasad, which achieved optimal error rates while requiring large\npolynomial runtime and sample complexity. We reinterpret their proof within the\nSever framework and obtain a dramatically faster and more sample-efficient\nalgorithm under fewer distributional assumptions.",
          "link": "http://arxiv.org/abs/2106.11938",
          "publishedOn": "2021-06-23T01:48:40.883Z",
          "wordCount": 668,
          "title": "Robust Regression Revisited: Acceleration and Improved Estimation Rates. (arXiv:2106.11938v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2005.08140",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ober_S/0/1/0/all/0/1\">Sebastian W. Ober</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1\">Laurence Aitchison</a>",
          "description": "We consider the optimal approximate posterior over the top-layer weights in a\nBayesian neural network for regression, and show that it exhibits strong\ndependencies on the lower-layer weights. We adapt this result to develop a\ncorrelated approximate posterior over the weights at all layers in a Bayesian\nneural network. We extend this approach to deep Gaussian processes, unifying\ninference in the two model classes. Our approximate posterior uses learned\n\"global\" inducing points, which are defined only at the input layer and\npropagated through the network to obtain inducing inputs at subsequent layers.\nBy contrast, standard, \"local\", inducing point methods from the deep Gaussian\nprocess literature optimise a separate set of inducing inputs at every layer,\nand thus do not model correlations across layers. Our method gives\nstate-of-the-art performance for a variational Bayesian method, without data\naugmentation or tempering, on CIFAR-10 of 86.7%, which is comparable to SGMCMC\nwithout tempering but with data augmentation (88% in Wenzel et al. 2020).",
          "link": "http://arxiv.org/abs/2005.08140",
          "publishedOn": "2021-06-23T01:48:40.877Z",
          "wordCount": 662,
          "title": "Global inducing point variational posteriors for Bayesian neural networks and deep Gaussian processes. (arXiv:2005.08140v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guanlin_L/0/1/0/all/0/1\">Li Guanlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangwei_G/0/1/0/all/0/1\">Guo Shangwei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Run_W/0/1/0/all/0/1\">Wang Run</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guowen_X/0/1/0/all/0/1\">Xu Guowen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tianwei_Z/0/1/0/all/0/1\">Zhang Tianwei</a>",
          "description": "This paper presents a novel fingerprinting methodology for the Intellectual\nProperty protection of generative models. Prior solutions for discriminative\nmodels usually adopt adversarial examples as the fingerprints, which give\nanomalous inference behaviors and prediction results. Hence, these methods are\nnot stealthy and can be easily recognized by the adversary. Our approach\nleverages the invisible backdoor technique to overcome the above limitation.\nSpecifically, we design verification samples, whose model outputs look normal\nbut can trigger a backdoor classifier to make abnormal predictions. We propose\na new backdoor embedding approach with Unique-Triplet Loss and fine-grained\ncategorization to enhance the effectiveness of our fingerprints. Extensive\nevaluations show that this solution can outperform other strategies with higher\nrobustness, uniqueness and stealthiness for various GAN models.",
          "link": "http://arxiv.org/abs/2106.11760",
          "publishedOn": "2021-06-23T01:48:40.870Z",
          "wordCount": 571,
          "title": "A Stealthy and Robust Fingerprinting Scheme for Generative Models. (arXiv:2106.11760v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_Z/0/1/0/all/0/1\">Zahra Ghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reagen_B/0/1/0/all/0/1\">Brandon Reagen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Siddharth Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1\">Chinmay Hegde</a>",
          "description": "The emergence of deep learning has been accompanied by privacy concerns\nsurrounding users' data and service providers' models. We focus on private\ninference (PI), where the goal is to perform inference on a user's data sample\nusing a service provider's model. Existing PI methods for deep networks enable\ncryptographically secure inference with little drop in functionality; however,\nthey incur severe latency costs, primarily caused by non-linear network\noperations (such as ReLUs). This paper presents Sphynx, a ReLU-efficient\nnetwork design method based on micro-search strategies for convolutional cell\ndesign. Sphynx achieves Pareto dominance over all existing private inference\nmethods on CIFAR-100. We also design large-scale networks that support\ncryptographically private inference on Tiny-ImageNet and ImageNet.",
          "link": "http://arxiv.org/abs/2106.11755",
          "publishedOn": "2021-06-23T01:48:40.863Z",
          "wordCount": 549,
          "title": "Sphynx: ReLU-Efficient Network Design for Private Inference. (arXiv:2106.11755v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+David_M/0/1/0/all/0/1\">Marco David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehats_F/0/1/0/all/0/1\">Florian M&#xe9;hats</a>",
          "description": "Machine learning methods are widely used in the natural sciences to model and\npredict physical systems from observation data. Yet, they are often used as\npoorly understood \"black boxes,\" disregarding existing mathematical structure\nand invariants of the problem. Recently, the proposal of Hamiltonian Neural\nNetworks (HNNs) took a first step towards a unified \"gray box\" approach, using\nphysical insight to improve performance for Hamiltonian systems. In this paper,\nwe explore a significantly improved training method for HNNs, exploiting the\nsymplectic structure of Hamiltonian systems with a different loss function.\nThis frees the loss from an artificial lower bound. We mathematically guarantee\nthe existence of an exact Hamiltonian function which the HNN can learn. This\nallows us to prove and numerically analyze the errors made by HNNs which, in\nturn, renders them fully explainable. Finally, we present a novel post-training\ncorrection to obtain the true Hamiltonian only from discretized observation\ndata, up to an arbitrary order.",
          "link": "http://arxiv.org/abs/2106.11753",
          "publishedOn": "2021-06-23T01:48:40.856Z",
          "wordCount": 598,
          "title": "Symplectic Learning for Hamiltonian Neural Networks. (arXiv:2106.11753v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weihua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fey_M/0/1/0/all/0/1\">Matthias Fey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakata_M/0/1/0/all/0/1\">Maho Nakata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>",
          "description": "Enabling effective and efficient machine learning (ML) over large-scale graph\ndata (e.g., graphs with billions of edges) can have a huge impact on both\nindustrial and scientific applications. However, community efforts to advance\nlarge-scale graph ML have been severely limited by the lack of a suitable\npublic benchmark. For KDD Cup 2021, we present OGB Large-Scale Challenge\n(OGB-LSC), a collection of three real-world datasets for advancing the\nstate-of-the-art in large-scale graph ML. OGB-LSC provides graph datasets that\nare orders of magnitude larger than existing ones and covers three core graph\nlearning tasks -- link prediction, graph regression, and node classification.\nFurthermore, OGB-LSC provides dedicated baseline experiments, scaling up\nexpressive graph ML models to the massive datasets. We show that the expressive\nmodels significantly outperform simple scalable baselines, indicating an\nopportunity for dedicated efforts to further improve graph ML at scale. Our\ndatasets and baseline code are released and maintained as part of our OGB\ninitiative (Hu et al., 2020). We hope OGB-LSC at KDD Cup 2021 can empower the\ncommunity to discover innovative solutions for large-scale graph ML.",
          "link": "http://arxiv.org/abs/2103.09430",
          "publishedOn": "2021-06-23T01:48:40.849Z",
          "wordCount": 650,
          "title": "OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs. (arXiv:2103.09430v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11926",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mouradi_R/0/1/0/all/0/1\">Rem-Sophia Mouradi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Goeury_C/0/1/0/all/0/1\">C&#xe9;dric Goeury</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thual_O/0/1/0/all/0/1\">Olivier Thual</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zaoui_F/0/1/0/all/0/1\">Fabrice Zaoui</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tassi_P/0/1/0/all/0/1\">Pablo Tassi</a>",
          "description": "Data assimilation (DA) is widely used to combine physical knowledge and\nobservations. It is nowadays commonly used in geosciences to perform parametric\ncalibration. In a context of climate change, old calibrations can not\nnecessarily be used for new scenarios. This raises the question of DA\ncomputational cost, as costly physics-based numerical models need to be\nreanalyzed. Reduction and metamodelling represent therefore interesting\nperspectives, for example proposed in recent contributions as hybridization\nbetween ensemble and variational methods, to combine their advantages\n(efficiency, non-linear framework). They are however often based on Monte Carlo\n(MC) type sampling, which often requires considerable increase of the ensemble\nsize for better efficiency, therefore representing a computational burden in\nensemble-based methods as well. To address these issues, two methods to replace\nthe complex model by a surrogate are proposed and confronted : (i) PODEn3DVAR\ndirectly inspired from PODEn4DVAR, relies on an ensemble-based joint\nparameter-state Proper Orthogonal Decomposition (POD), which provides a linear\nmetamodel ; (ii) POD-PCE-3DVAR, where the model states are POD reduced then\nlearned using Polynomial Chaos Expansion (PCE), resulting in a non-linear\nmetamodel. Both metamodels allow to write an approximate cost function whose\nminimum can be analytically computed, or deduced by a gradient descent at\nnegligible cost. Furthermore, adapted metamodelling error covariance matrix is\ngiven for POD-PCE-3DVAR, allowing to substantially improve the metamodel-based\nDA analysis. Proposed methods are confronted on a twin experiment, and compared\nto classical 3DVAR on a measurement-based problem. Results are promising, in\nparticular superior with POD-PCE-3DVAR, showing good convergence to classical\n3DVAR and robustness to noise.",
          "link": "http://arxiv.org/abs/2106.11926",
          "publishedOn": "2021-06-23T01:48:40.842Z",
          "wordCount": 700,
          "title": "Surrogate-based variational data assimilation for tidal modelling. (arXiv:2106.11926v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>",
          "description": "We consider repair tasks: given a critic (e.g., compiler) that assesses the\nquality of an input, the goal is to train a fixer that converts a bad example\n(e.g., code with syntax errors) into a good one (e.g., code with no syntax\nerrors). Existing works create training data consisting of (bad, good) pairs by\ncorrupting good examples using heuristics (e.g., dropping tokens). However,\nfixers trained on this synthetically-generated data do not extrapolate well to\nthe real distribution of bad inputs. To bridge this gap, we propose a new\ntraining approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use\nthe critic to check a fixer's output on real bad inputs and add good (fixed)\noutputs to the training data, and (ii) we train a breaker to generate realistic\nbad code from good code. Based on these ideas, we iteratively update the\nbreaker and the fixer while using them in conjunction to generate more paired\ndata. We evaluate BIFI on two code repair datasets: GitHub-Python, a new\ndataset we introduce where the goal is to repair Python code with AST parse\nerrors; and DeepFix, where the goal is to repair C code with compiler errors.\nBIFI outperforms existing methods, obtaining 90.5% repair accuracy on\nGitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%). Notably, BIFI does not\nrequire any labeled data; we hope it will be a strong starting point for\nunsupervised learning of various repair tasks.",
          "link": "http://arxiv.org/abs/2106.06600",
          "publishedOn": "2021-06-23T01:48:40.835Z",
          "wordCount": 693,
          "title": "Break-It-Fix-It: Unsupervised Learning for Program Repair. (arXiv:2106.06600v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baeta_F/0/1/0/all/0/1\">Francisco Baeta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correia_J/0/1/0/all/0/1\">Jo&#xe3;o Correia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_T/0/1/0/all/0/1\">Tiago Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_P/0/1/0/all/0/1\">Penousal Machado</a>",
          "description": "Genetic Programming (GP) is known to suffer from the burden of being\ncomputationally expensive by design. While, over the years, many techniques\nhave been developed to mitigate this issue, data vectorization, in particular,\nis arguably still the most attractive strategy due to the parallel nature of\nGP. In this work, we employ a series of benchmarks meant to compare both the\nperformance and evolution capabilities of different vectorized and iterative\nimplementation approaches across several existing frameworks. Namely, TensorGP,\na novel open-source engine written in Python, is shown to greatly benefit from\nthe TensorFlow library to accelerate the domain evaluation phase in GP. The\npresented performance benchmarks demonstrate that the TensorGP engine manages\nto pull ahead, with relative speedups above two orders of magnitude for\nproblems with a higher number of fitness cases. Additionally, as a consequence\nof being able to compute larger domains, we argue that TensorGP performance\ngains aid the discovery of more accurate candidate solutions.",
          "link": "http://arxiv.org/abs/2106.11919",
          "publishedOn": "2021-06-23T01:48:40.815Z",
          "wordCount": 596,
          "title": "Speed Benchmarking of Genetic Programming Frameworks. (arXiv:2106.11919v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>",
          "description": "Topic modeling is an unsupervised method for revealing the hidden semantic\nstructure of a corpus. It has been increasingly widely adopted as a tool in the\nsocial sciences, including political science, digital humanities and\nsociological research in general. One desirable property of topic models is to\nallow users to find topics describing a specific aspect of the corpus. A\npossible solution is to incorporate domain-specific knowledge into topic\nmodeling, but this requires a specification from domain experts. We propose a\nnovel query-driven topic model that allows users to specify a simple query in\nwords or phrases and return query-related topics, thus avoiding tedious work\nfrom domain experts. Our proposed approach is particularly attractive when the\nuser-specified query has a low occurrence in a text corpus, making it difficult\nfor traditional topic models built on word cooccurrence patterns to identify\nrelevant topics. Experimental results demonstrate the effectiveness of our\nmodel in comparison with both classical topic models and neural topic models.",
          "link": "http://arxiv.org/abs/2106.07346",
          "publishedOn": "2021-06-23T01:48:40.805Z",
          "wordCount": 611,
          "title": "A Query-Driven Topic Model. (arXiv:2106.07346v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_D/0/1/0/all/0/1\">Donglin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuaiwen Leon Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>",
          "description": "The quest for determinism in machine learning has disproportionately focused\non characterizing the impact of noise introduced by algorithmic design choices.\nIn this work, we address a less well understood and studied question: how does\nour choice of tooling introduce randomness to deep neural network training. We\nconduct large scale experiments across different types of hardware,\naccelerators, state of art networks, and open-source datasets, to characterize\nhow tooling choices contribute to the level of non-determinism in a system, the\nimpact of said non-determinism, and the cost of eliminating different sources\nof noise.\n\nOur findings are surprising, and suggest that the impact of non-determinism\nin nuanced. While top-line metrics such as top-1 accuracy are not noticeably\nimpacted, model performance on certain parts of the data distribution is far\nmore sensitive to the introduction of randomness. Our results suggest that\ndeterministic tooling is critical for AI safety. However, we also find that the\ncost of ensuring determinism varies dramatically between neural network\narchitectures and hardware types, e.g., with overhead up to $746\\%$, $241\\%$,\nand $196\\%$ on a spectrum of widely used GPU accelerator architectures,\nrelative to non-deterministic training. The source code used in this paper is\navailable at https://github.com/usyd-fsalab/NeuralNetworkRandomness.",
          "link": "http://arxiv.org/abs/2106.11872",
          "publishedOn": "2021-06-23T01:48:40.795Z",
          "wordCount": 644,
          "title": "Randomness In Neural Network Training: Characterizing The Impact of Tooling. (arXiv:2106.11872v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.17132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mutschler_M/0/1/0/all/0/1\">Maximus Mutschler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1\">Andreas Zell</a>",
          "description": "Optimization in Deep Learning is mainly guided by vague intuitions and strong\nassumptions, with a limited understanding how and why these work in practice.\nTo shed more light on this, our work provides some deeper understandings of how\nSGD behaves by empirically analyzing the trajectory taken by SGD from a line\nsearch perspective. Specifically, a costly quantitative analysis of the\nfull-batch loss along SGD trajectories from common used models trained on a\nsubset of CIFAR-10 is performed. Our core results include that the full-batch\nloss along lines in update step direction is highly parabolically. Further on,\nwe show that there exists a learning rate with which SGD always performs almost\nexact line searches on the full-batch loss. Finally, we provide a different\nperspective why increasing the batch size has almost the same effect as\ndecreasing the learning rate by the same factor.",
          "link": "http://arxiv.org/abs/2103.17132",
          "publishedOn": "2021-06-23T01:48:40.786Z",
          "wordCount": 618,
          "title": "Empirically explaining SGD from a line search perspective. (arXiv:2103.17132v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_F/0/1/0/all/0/1\">Fu-Shun Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shang-Ran Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chien-Wen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuan-Ren Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Chieh Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_J/0/1/0/all/0/1\">Jack Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chung-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1\">Feipei Lai</a>",
          "description": "A continuous real-time respiratory sound automated analysis system is needed\nin clinical practice. Previously, we established an open access lung sound\ndatabase, HF_Lung_V1, and automated lung sound analysis algorithms capable of\ndetecting inhalation, exhalation, continuous adventitious sounds (CASs) and\ndiscontinuous adventitious sounds (DASs). In this study, HF-Lung-V1 has been\nfurther expanded to HF-Lung-V2 with 1.45 times of increase in audio files. The\nconvolutional neural network (CNN)-bidirectional gated recurrent unit (BiGRU)\nmodel was separately trained with training datasets of HF_Lung_V1 (V1_Train)\nand HF_Lung_V2 (V2_Train), and then were used for the performance comparisons\nof segment detection and event detection on both test datasets of HF_Lung_V1\n(V1_Test) and HF_Lung_V2 (V2_Test). The performance of segment detection was\nmeasured by accuracy, predictive positive value (PPV), sensitivity,\nspecificity, F1 score, receiver operating characteristic (ROC) curve and area\nunder the curve (AUC), whereas that of event detection was evaluated with PPV,\nsensitivity, and F1 score. Results indicate that the model performance trained\nby V2_Train showed improvement on both V1_Test and V2_Test in inhalation, CASs\nand DASs, particularly in CASs, as well as on V1_Test in exhalation.",
          "link": "http://arxiv.org/abs/2102.04062",
          "publishedOn": "2021-06-23T01:48:40.774Z",
          "wordCount": 670,
          "title": "An Update of a Progressively Expanded Database for Automated Lung Sound Analysis. (arXiv:2102.04062v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jha_N/0/1/0/all/0/1\">Nandan Kumar Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_Z/0/1/0/all/0/1\">Zahra Ghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Siddharth Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reagen_B/0/1/0/all/0/1\">Brandon Reagen</a>",
          "description": "The recent rise of privacy concerns has led researchers to devise methods for\nprivate neural inference -- where inferences are made directly on encrypted\ndata, never seeing inputs. The primary challenge facing private inference is\nthat computing on encrypted data levies an impractically-high latency penalty,\nstemming mostly from non-linear operators like ReLU. Enabling practical and\nprivate inference requires new optimization methods that minimize network ReLU\ncounts while preserving accuracy. This paper proposes DeepReDuce: a set of\noptimizations for the judicious removal of ReLUs to reduce private inference\nlatency. The key insight is that not all ReLUs contribute equally to accuracy.\nWe leverage this insight to drop, or remove, ReLUs from classic networks to\nsignificantly reduce inference latency and maintain high accuracy. Given a\ntarget network, DeepReDuce outputs a Pareto frontier of networks that tradeoff\nthe number of ReLUs and accuracy. Compared to the state-of-the-art for private\ninference DeepReDuce improves accuracy and reduces ReLU count by up to 3.5%\n(iso-ReLU count) and 3.5$\\times$ (iso-accuracy), respectively.",
          "link": "http://arxiv.org/abs/2103.01396",
          "publishedOn": "2021-06-23T01:48:40.746Z",
          "wordCount": 633,
          "title": "DeepReDuce: ReLU Reduction for Fast Private Inference. (arXiv:2103.01396v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04279",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ding_Z/0/1/0/all/0/1\">Zhiyan Ding</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_Q/0/1/0/all/0/1\">Qin Li</a>",
          "description": "The classical Langevin Monte Carlo method looks for i.i.d. samples from a\ntarget distribution by descending along the gradient of the target\ndistribution. It is popular partially due to its fast convergence rate.\nHowever, the numerical cost is sometimes high because the gradient can be hard\nto obtain. One approach to eliminate the gradient computation is to employ the\nconcept of \"ensemble\", where a large number of particles are evolved together\nso that the neighboring particles provide gradient information to each other.\nIn this article, we discuss two algorithms that integrate the ensemble feature\ninto LMC, and the associated properties. There are two sides of our discovery:\n\n1. By directly surrogating the gradient using the ensemble approximation, we\ndevelop Ensemble Langevin Monte Carlo. We show that this method is unstable due\nto a potentially small denominator that induces high variance. We provide a\ncounterexample to explicitly show this instability.\n\n2. We then change the strategy and enact the ensemble approximation to the\ngradient only in a constrained manner, to eliminate the unstable points. The\nalgorithm is termed Constrained Ensemble Langevin Monte Carlo. We show that,\nwith a proper tuning, the surrogation takes place often enough to bring the\nreasonable numerical saving, while the induced error is still low enough for us\nto maintain the fast convergence rate, up to a controllable discretization and\nensemble error.\n\nSuch combination of ensemble method and LMC shed light on inventing\ngradient-free algorithms that produce i.i.d. samples almost exponentially fast.",
          "link": "http://arxiv.org/abs/2102.04279",
          "publishedOn": "2021-06-23T01:48:40.728Z",
          "wordCount": 692,
          "title": "Constrained Ensemble Langevin Monte Carlo. (arXiv:2102.04279v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08902",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Okati_N/0/1/0/all/0/1\">Nastaran Okati</a>, <a href=\"http://arxiv.org/find/stat/1/au:+De_A/0/1/0/all/0/1\">Abir De</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gomez_Rodriguez_M/0/1/0/all/0/1\">Manuel Gomez-Rodriguez</a>",
          "description": "Multiple lines of evidence suggest that predictive models may benefit from\nalgorithmic triage. Under algorithmic triage, a predictive model does not\npredict all instances but instead defers some of them to human experts.\nHowever, the interplay between the prediction accuracy of the model and the\nhuman experts under algorithmic triage is not well understood. In this work, we\nstart by formally characterizing under which circumstances a predictive model\nmay benefit from algorithmic triage. In doing so, we also demonstrate that\nmodels trained for full automation may be suboptimal under triage. Then, given\nany model and desired level of triage, we show that the optimal triage policy\nis a deterministic threshold rule in which triage decisions are derived\ndeterministically by thresholding the difference between the model and human\nerrors on a per-instance level. Building upon these results, we introduce a\npractical gradient-based algorithm that is guaranteed to find a sequence of\ntriage policies and predictive models of increasing performance. Experiments on\na wide variety of supervised learning tasks using synthetic and real data from\ntwo important applications -- content moderation and scientific discovery --\nillustrate our theoretical results and show that the models and triage policies\nprovided by our gradient-based algorithm outperform those provided by several\ncompetitive baselines.",
          "link": "http://arxiv.org/abs/2103.08902",
          "publishedOn": "2021-06-23T01:48:40.708Z",
          "wordCount": 648,
          "title": "Differentiable Learning Under Triage. (arXiv:2103.08902v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chengli Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiangshe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junmin Liu</a>",
          "description": "\\textit{Stochastic gradient descent} (SGD) is of fundamental importance in\ndeep learning. Despite its simplicity, elucidating its efficacy remains\nchallenging. Conventionally, the success of SGD is attributed to the\n\\textit{stochastic gradient noise} (SGN) incurred in the training process.\nBased on this general consensus, SGD is frequently treated and analyzed as the\nEuler-Maruyama discretization of a \\textit{stochastic differential equation}\n(SDE) driven by either Brownian or L\\'evy stable motion. In this study, we\nargue that SGN is neither Gaussian nor stable. Instead, inspired by the\nlong-time correlation emerging in SGN series, we propose that SGD can be viewed\nas a discretization of an SDE driven by \\textit{fractional Brownian motion}\n(FBM). Accordingly, the different convergence behavior of SGD dynamics is well\ngrounded. Moreover, the first passage time of an SDE driven by FBM is\napproximately derived. This indicates a lower escaping rate for a larger Hurst\nparameter, and thus SGD stays longer in flat minima. This happens to coincide\nwith the well-known phenomenon that SGD favors flat minima that generalize\nwell. Four groups of experiments are conducted to validate our conjecture, and\nit is demonstrated that long-range memory effects persist across various model\narchitectures, datasets, and training strategies. Our study opens up a new\nperspective and may contribute to a better understanding of SGD.",
          "link": "http://arxiv.org/abs/2105.02062",
          "publishedOn": "2021-06-23T01:48:40.701Z",
          "wordCount": 708,
          "title": "Understanding Long Range Memory Effects in Deep Neural Networks. (arXiv:2105.02062v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1\">Ayaan Haque</a>",
          "description": "Semi-supervised learning has been gaining attention as it allows for\nperforming image analysis tasks such as classification with limited labeled\ndata. Some popular algorithms using Generative Adversarial Networks (GANs) for\nsemi-supervised classification share a single architecture for classification\nand discrimination. However, this may require a model to converge to a separate\ndata distribution for each task, which may reduce overall performance. While\nprogress in semi-supervised learning has been made, less addressed are\nsmall-scale, fully-supervised tasks where even unlabeled data is unavailable\nand unattainable. We therefore, propose a novel GAN model namely External\nClassifier GAN (EC-GAN), that utilizes GANs and semi-supervised algorithms to\nimprove classification in fully-supervised regimes. Our method leverages a GAN\nto generate artificial data used to supplement supervised classification. More\nspecifically, we attach an external classifier, hence the name EC-GAN, to the\nGAN's generator, as opposed to sharing an architecture with the discriminator.\nOur experiments demonstrate that EC-GAN's performance is comparable to the\nshared architecture method, far superior to the standard data augmentation and\nregularization-based approach, and effective on a small, realistic dataset.",
          "link": "http://arxiv.org/abs/2012.15864",
          "publishedOn": "2021-06-23T01:48:40.682Z",
          "wordCount": 647,
          "title": "EC-GAN: Low-Sample Classification using Semi-Supervised Algorithms and GANs. (arXiv:2012.15864v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosasco_A/0/1/0/all/0/1\">Andrea Rosasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1\">Antonio Carta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1\">Andrea Cossu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1\">Vincenzo Lomonaco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>",
          "description": "Replay strategies are Continual Learning techniques which mitigate\ncatastrophic forgetting by keeping a buffer of patterns from previous\nexperiences, which are interleaved with new data during training. The amount of\npatterns stored in the buffer is a critical parameter which largely influences\nthe final performance and the memory footprint of the approach. This work\nintroduces Distilled Replay, a novel replay strategy for Continual Learning\nwhich is able to mitigate forgetting by keeping a very small buffer (1 pattern\nper class) of highly informative samples. Distilled Replay builds the buffer\nthrough a distillation process which compresses a large dataset into a tiny set\nof informative examples. We show the effectiveness of our Distilled Replay\nagainst popular replay-based strategies on four Continual Learning benchmarks.",
          "link": "http://arxiv.org/abs/2103.15851",
          "publishedOn": "2021-06-23T01:48:40.675Z",
          "wordCount": 583,
          "title": "Distilled Replay: Overcoming Forgetting through Synthetic Samples. (arXiv:2103.15851v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thames_Q/0/1/0/all/0/1\">Quin Thames</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpur_A/0/1/0/all/0/1\">Arjun Karpur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norris_W/0/1/0/all/0/1\">Wade Norris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fangting Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panait_L/0/1/0/all/0/1\">Liviu Panait</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyand_T/0/1/0/all/0/1\">Tobias Weyand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_J/0/1/0/all/0/1\">Jack Sim</a>",
          "description": "Understanding the nutritional content of food from visual data is a\nchallenging computer vision problem, with the potential to have a positive and\nwidespread impact on public health. Studies in this area are limited to\nexisting datasets in the field that lack sufficient diversity or labels\nrequired for training models with nutritional understanding capability. We\nintroduce Nutrition5k, a novel dataset of 5k diverse, real world food dishes\nwith corresponding video streams, depth images, component weights, and high\naccuracy nutritional content annotation. We demonstrate the potential of this\ndataset by training a computer vision algorithm capable of predicting the\ncaloric and macronutrient values of a complex, real world dish at an accuracy\nthat outperforms professional nutritionists. Further we present a baseline for\nincorporating depth sensor data to improve nutrition predictions. We will\npublicly release Nutrition5k in the hope that it will accelerate innovation in\nthe space of nutritional understanding.",
          "link": "http://arxiv.org/abs/2103.03375",
          "publishedOn": "2021-06-23T01:48:40.666Z",
          "wordCount": 630,
          "title": "Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food. (arXiv:2103.03375v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_R/0/1/0/all/0/1\">Rachana Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharpe_S/0/1/0/all/0/1\">Samuel Sharpe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barr_B/0/1/0/all/0/1\">Brian Barr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wittenbach_J/0/1/0/all/0/1\">Jason Wittenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruss_C/0/1/0/all/0/1\">C. Bayan Bruss</a>",
          "description": "In the environment of fair lending laws and the General Data Protection\nRegulation (GDPR), the ability to explain a model's prediction is of paramount\nimportance. High quality explanations are the first step in assessing fairness.\nCounterfactuals are valuable tools for explainability. They provide actionable,\ncomprehensible explanations for the individual who is subject to decisions made\nfrom the prediction. It is important to find a baseline for producing them. We\npropose a simple method for generating counterfactuals by using gradient\ndescent to search in the latent space of an autoencoder and benchmark our\nmethod against approaches that search for counterfactuals in feature space.\nAdditionally, we implement metrics to concretely evaluate the quality of the\ncounterfactuals. We show that latent space counterfactual generation strikes a\nbalance between the speed of basic feature gradient descent methods and the\nsparseness and authenticity of counterfactuals generated by more complex\nfeature space oriented techniques.",
          "link": "http://arxiv.org/abs/2012.09301",
          "publishedOn": "2021-06-23T01:48:40.659Z",
          "wordCount": 608,
          "title": "Latent-CF: A Simple Baseline for Reverse Counterfactual Explanations. (arXiv:2012.09301v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.07606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Shubhada Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koolen_W/0/1/0/all/0/1\">Wouter M. Koolen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juneja_S/0/1/0/all/0/1\">Sandeep Juneja</a>",
          "description": "Conditional value-at-risk (CVaR) and value-at-risk (VaR) are popular\ntail-risk measures in finance and insurance industries as well as in highly\nreliable, safety-critical uncertain environments where often the underlying\nprobability distributions are heavy-tailed. We use the multi-armed bandit\nbest-arm identification framework and consider the problem of identifying the\narm from amongst finitely many that has the smallest CVaR, VaR, or weighted sum\nof CVaR and mean. The latter captures the risk-return trade-off common in\nfinance. Our main contribution is an optimal $\\delta$-correct algorithm that\nacts on general arms, including heavy-tailed distributions, and matches the\nlower bound on the expected number of samples needed, asymptotically (as\n$\\delta$ approaches $0$). The algorithm requires solving a non-convex\noptimization problem in the space of probability measures, that requires\ndelicate analysis. En-route, we develop new non-asymptotic empirical\nlikelihood-based concentration inequalities for tail-risk measures which are\ntighter than those for popular truncation-based empirical estimators.",
          "link": "http://arxiv.org/abs/2008.07606",
          "publishedOn": "2021-06-23T01:48:40.648Z",
          "wordCount": 621,
          "title": "Optimal Best-Arm Identification Methods for Tail-Risk Measures. (arXiv:2008.07606v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.01250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this work, we address the problem of 3D object detection from point cloud\ndata in real time. For autonomous vehicles to work, it is very important for\nthe perception component to detect the real world objects with both high\naccuracy and fast inference. We propose a novel neural network architecture\nalong with the training and optimization details for detecting 3D objects in\npoint cloud data. We compare the results with different backbone architectures\nincluding the standard ones like VGG, ResNet, Inception with our backbone. Also\nwe present the optimization and ablation studies including designing an\nefficient anchor. We use the Kitti 3D Birds Eye View dataset for benchmarking\nand validating our results. Our work surpasses the state of the art in this\ndomain both in terms of average precision and speed running at > 30 FPS. This\nmakes it a feasible option to be deployed in real time applications including\nself driving cars.",
          "link": "http://arxiv.org/abs/2006.01250",
          "publishedOn": "2021-06-23T01:48:40.634Z",
          "wordCount": 684,
          "title": "RUHSNet: 3D Object Detection Using Lidar Data in Real Time. (arXiv:2006.01250v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.03197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_A/0/1/0/all/0/1\">Anshuman Chhabra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vashishth_V/0/1/0/all/0/1\">Vidushi Vashishth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_P/0/1/0/all/0/1\">Prasant Mohapatra</a>",
          "description": "Hierarchical Agglomerative Clustering (HAC) algorithms are extensively\nutilized in modern data science, and seek to partition the dataset into\nclusters while generating a hierarchical relationship between the data samples.\nHAC algorithms are employed in many applications, such as biology, natural\nlanguage processing, and recommender systems. Thus, it is imperative to ensure\nthat these algorithms are fair -- even if the dataset contains biases against\ncertain protected groups, the cluster outputs generated should not discriminate\nagainst samples from any of these groups. However, recent work in clustering\nfairness has mostly focused on center-based clustering algorithms, such as\nk-median and k-means clustering. In this paper, we propose fair algorithms for\nperforming HAC that enforce fairness constraints 1) irrespective of the\ndistance linkage criteria used, 2) generalize to any natural measures of\nclustering fairness for HAC, 3) work for multiple protected groups, and 4) have\ncompetitive running times to vanilla HAC. Through extensive experiments on\nmultiple real-world UCI datasets, we show that our proposed algorithm finds\nfairer clusterings compared to vanilla HAC as well as other state-of-the-art\nfair clustering approaches.",
          "link": "http://arxiv.org/abs/2005.03197",
          "publishedOn": "2021-06-23T01:48:40.615Z",
          "wordCount": 642,
          "title": "Fair Algorithms for Hierarchical Agglomerative Clustering. (arXiv:2005.03197v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nozad_S/0/1/0/all/0/1\">Sayyed Ahmad Naghavi Nozad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haeri_M/0/1/0/all/0/1\">Maryam Amir Haeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Folino_G/0/1/0/all/0/1\">Gianluigi Folino</a>",
          "description": "This paper presents a batch-wise density-based clustering approach for local\noutlier detection in massive-scale datasets. Unlike the well-known traditional\nalgorithms, which assume that all the data is memory-resident, our proposed\nmethod is scalable and processes the input data chunk-by-chunk within the\nconfines of a limited memory buffer. A temporary clustering model is built at\nthe first phase; then, it is gradually updated by analyzing consecutive memory\nloads of points. Subsequently, at the end of scalable clustering, the\napproximate structure of the original clusters is obtained. Finally, by another\nscan of the entire dataset and using a suitable criterion, an outlying score is\nassigned to each object called SDCOR (Scalable Density-based Clustering\nOutlierness Ratio). Evaluations on real-life and synthetic datasets demonstrate\nthat the proposed method has a low linear time complexity and is more effective\nand efficient compared to best-known conventional density-based methods, which\nneed to load all data into the memory; and also, to some fast distance-based\nmethods, which can perform on data resident in the disk.",
          "link": "http://arxiv.org/abs/2006.07616",
          "publishedOn": "2021-06-23T01:48:40.608Z",
          "wordCount": 740,
          "title": "SDCOR: Scalable Density-based Clustering for Local Outlier Detection in Massive-Scale Datasets. (arXiv:2006.07616v10 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_N/0/1/0/all/0/1\">Naeun Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We propose a novel and effective input transformation based adversarial\ndefense method against gray- and black-box attack, which is computationally\nefficient and does not require any adversarial training or retraining of a\nclassification model. We first show that a very simple iterative Gaussian\nsmoothing can effectively wash out adversarial noise and achieve substantially\nhigh robust accuracy. Based on the observation, we propose Self-Supervised\nIterative Contextual Smoothing (SSICS), which aims to reconstruct the original\ndiscriminative features from the Gaussian-smoothed image in context-adaptive\nmanner, while still smoothing out the adversarial noise. From the experiments\non ImageNet, we show that our SSICS achieves both high standard accuracy and\nvery competitive robust accuracy for the gray- and black-box attacks; e.g.,\ntransfer-based PGD-attack and score-based attack. A note-worthy point to stress\nis that our defense is free of computationally expensive adversarial training,\nyet, can approach its robust accuracy via input transformation.",
          "link": "http://arxiv.org/abs/2106.11644",
          "publishedOn": "2021-06-23T01:48:40.601Z",
          "wordCount": 599,
          "title": "Self-Supervised Iterative Contextual Smoothing for Efficient Adversarial Defense against Gray- and Black-Box Attack. (arXiv:2106.11644v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.07587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "Bayesian neural networks perform variational inference over the weights\nhowever calculation of the posterior distribution remains a challenge. Our work\nbuilds on variational inference techniques for bayesian neural networks using\nthe original Evidence Lower Bound. In this paper, we present a stochastic\nbayesian neural network in which we maximize Evidence Lower Bound using a new\nobjective function which we name as Stochastic Evidence Lower Bound. We\nevaluate our network on 5 publicly available UCI datasets using test RMSE and\nlog likelihood as the evaluation metrics. We demonstrate that our work not only\nbeats the previous state of the art algorithms but is also scalable to larger\ndatasets.",
          "link": "http://arxiv.org/abs/2008.07587",
          "publishedOn": "2021-06-23T01:48:40.594Z",
          "wordCount": 580,
          "title": "Stochastic Bayesian Neural Networks. (arXiv:2008.07587v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.08453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "The goal of bayesian approach used in variational inference is to minimize\nthe KL divergence between variational distribution and unknown posterior\ndistribution. This is done by maximizing the Evidence Lower Bound (ELBO). A\nneural network is used to parametrize these distributions using Stochastic\nGradient Descent. This work extends the work done by others by deriving the\nvariational inference models. We show how SGD can be applied on bayesian neural\nnetworks by gradient estimation techniques. For validation, we have tested our\nmodel on 5 UCI datasets and the metrics chosen for evaluation are Root Mean\nSquare Error (RMSE) error and negative log likelihood. Our work considerably\nbeats the previous state of the art approaches for regression using bayesian\nneural networks.",
          "link": "http://arxiv.org/abs/2006.08453",
          "publishedOn": "2021-06-23T01:48:40.575Z",
          "wordCount": 615,
          "title": "Bayesian Neural Network via Stochastic Gradient Descent. (arXiv:2006.08453v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11723",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mital_N/0/1/0/all/0/1\">Nitish Mital</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozyilkan_E/0/1/0/all/0/1\">Ezgi Ozyilkan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garjani_A/0/1/0/all/0/1\">Ali Garjani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>",
          "description": "We present a novel deep neural network (DNN) architecture for compressing an\nimage when a correlated image is available as side information only at the\ndecoder. This problem is known as distributed source coding (DSC) in\ninformation theory. In particular, we consider a pair of stereo images, which\ngenerally have high correlation with each other due to overlapping fields of\nview, and assume that one image of the pair is to be compressed and\ntransmitted, while the other image is available only at the decoder. In the\nproposed architecture, the encoder maps the input image to a latent space,\nquantizes the latent representation, and compresses it using entropy coding.\nThe decoder is trained to extract the Wyner's common information between the\ninput image and the correlated image from the latter. The received latent\nrepresentation and the locally generated common information are passed through\na decoder network to obtain an enhanced reconstruction of the input image. The\ncommon information provides a succinct representation of the relevant\ninformation at the receiver. We train and demonstrate the effectiveness of the\nproposed approach on the KITTI dataset of stereo image pairs. Our results show\nthat the proposed architecture is capable of exploiting the decoder-only side\ninformation, and outperforms previous work on stereo image compression with\ndecoder side information.",
          "link": "http://arxiv.org/abs/2106.11723",
          "publishedOn": "2021-06-23T01:48:40.568Z",
          "wordCount": 675,
          "title": "Deep Stereo Image Compression with Decoder Side Information using Wyner Common Information. (arXiv:2106.11723v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1\">Zhiyong Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yixuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Huihua Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1\">Hsiao-Dong Chiang</a>",
          "description": "Recent progress on deep learning relies heavily on the quality and efficiency\nof training algorithms. In this paper, we develop a fast training method\nmotivated by the nonlinear Conjugate Gradient (CG) framework. We propose the\nConjugate Gradient with Quadratic line-search (CGQ) method. On the one hand, a\nquadratic line-search determines the step size according to current loss\nlandscape. On the other hand, the momentum factor is dynamically updated in\ncomputing the conjugate gradient parameter (like Polak-Ribiere). Theoretical\nresults to ensure the convergence of our method in strong convex settings is\ndeveloped. And experiments in image classification datasets show that our\nmethod yields faster convergence than other local solvers and has better\ngeneralization capability (test set accuracy). One major advantage of the paper\nmethod is that tedious hand tuning of hyperparameters like the learning rate\nand momentum is avoided.",
          "link": "http://arxiv.org/abs/2106.11548",
          "publishedOn": "2021-06-23T01:48:40.560Z",
          "wordCount": 571,
          "title": "Adaptive Learning Rate and Momentum for Training Deep Neural Networks. (arXiv:2106.11548v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.02227",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Ji_S/0/1/0/all/0/1\">Shaolin Ji</a>, <a href=\"http://arxiv.org/find/math/1/au:+Peng_S/0/1/0/all/0/1\">Shige Peng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Peng_Y/0/1/0/all/0/1\">Ying Peng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_X/0/1/0/all/0/1\">Xichuan Zhang</a>",
          "description": "In this paper, we aim to solve the high dimensional stochastic optimal\ncontrol problem from the view of the stochastic maximum principle via deep\nlearning. By introducing the extended Hamiltonian system which is essentially\nan FBSDE with a maximum condition, we reformulate the original control problem\nas a new one. Three algorithms are proposed to solve the new control problem.\nNumerical results for different examples demonstrate the effectiveness of our\nproposed algorithms, especially in high dimensional cases. And an important\napplication of this method is to calculate the sub-linear expectations, which\ncorrespond to a kind of fully nonlinear PDEs.",
          "link": "http://arxiv.org/abs/2007.02227",
          "publishedOn": "2021-06-23T01:48:40.553Z",
          "wordCount": 589,
          "title": "Solving stochastic optimal control problem via stochastic maximum principle with deep learning method. (arXiv:2007.02227v5 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Ye Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aker_A/0/1/0/all/0/1\">Ahmet Aker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1\">Kalina Bontcheva</a>",
          "description": "The spreading COVID-19 misinformation over social media already draws the\nattention of many researchers. According to Google Scholar, about 26000\nCOVID-19 related misinformation studies have been published to date. Most of\nthese studies focusing on 1) detect and/or 2) analysing the characteristics of\nCOVID-19 related misinformation. However, the study of the social behaviours\nrelated to misinformation is often neglected. In this paper, we introduce a\nfine-grained annotated misinformation tweets dataset including social\nbehaviours annotation (e.g. comment or question to the misinformation). The\ndataset not only allows social behaviours analysis but also suitable for both\nevidence-based or non-evidence-based misinformation classification task. In\naddition, we introduce leave claim out validation in our experiments and\ndemonstrate the misinformation classification performance could be\nsignificantly different when applying to real-world unseen misinformation.",
          "link": "http://arxiv.org/abs/2106.11702",
          "publishedOn": "2021-06-23T01:48:40.547Z",
          "wordCount": 621,
          "title": "Categorising Fine-to-Coarse Grained Misinformation: An Empirical Study of COVID-19 Infodemic. (arXiv:2106.11702v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anand_M/0/1/0/all/0/1\">Mrinal Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kayal_P/0/1/0/all/0/1\">Pratik Kayal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>",
          "description": "Automatic code synthesis from natural language descriptions is a challenging\ntask. We witness massive progress in developing code generation systems for\ndomain-specific languages (DSLs) employing sequence-to-sequence deep learning\ntechniques in the recent past. In this paper, we specifically experiment with\n\\textsc{AlgoLisp} DSL-based generative models and showcase the existence of\nsignificant dataset bias through different classes of adversarial examples. We\nalso experiment with two variants of Transformer-based models that outperform\nall existing \\textsc{AlgoLisp} DSL-based code generation baselines. Consistent\nwith the current state-of-the-art systems, our proposed models, too, achieve\npoor performance under adversarial settings. Therefore, we propose several\ndataset augmentation techniques to reduce bias and showcase their efficacy\nusing robust experimentation.",
          "link": "http://arxiv.org/abs/2106.11629",
          "publishedOn": "2021-06-23T01:48:40.539Z",
          "wordCount": 538,
          "title": "On Adversarial Robustness of Synthetic Code Generation. (arXiv:2106.11629v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DAngelo_F/0/1/0/all/0/1\">Francesco D&#x27;Angelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>",
          "description": "Deep ensembles have recently gained popularity in the deep learning community\nfor their conceptual simplicity and efficiency. However, maintaining functional\ndiversity between ensemble members that are independently trained with gradient\ndescent is challenging. This can lead to pathologies when adding more ensemble\nmembers, such as a saturation of the ensemble performance, which converges to\nthe performance of a single model. Moreover, this does not only affect the\nquality of its predictions, but even more so the uncertainty estimates of the\nensemble, and thus its performance on out-of-distribution data. We hypothesize\nthat this limitation can be overcome by discouraging different ensemble members\nfrom collapsing to the same function. To this end, we introduce a kernelized\nrepulsive term in the update rule of the deep ensembles. We show that this\nsimple modification not only enforces and maintains diversity among the members\nbut, even more importantly, transforms the maximum a posteriori inference into\nproper Bayesian inference. Namely, we show that the training dynamics of our\nproposed repulsive ensembles follow a Wasserstein gradient flow of the KL\ndivergence with the true posterior. We study repulsive terms in weight and\nfunction space and empirically compare their performance to standard ensembles\nand Bayesian baselines on synthetic and real-world prediction tasks.",
          "link": "http://arxiv.org/abs/2106.11642",
          "publishedOn": "2021-06-23T01:48:40.521Z",
          "wordCount": 629,
          "title": "Repulsive Deep Ensembles are Bayesian. (arXiv:2106.11642v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ni_X/0/1/0/all/0/1\">Xiang Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaolong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Changhua Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqiang Wang</a>",
          "description": "Recently, Graph Neural Network (GNN) has achieved remarkable success in\nvarious real-world problems on graph data. However in most industries, data\nexists in the form of isolated islands and the data privacy and security is\nalso an important issue. In this paper, we propose FedVGCN, a federated GCN\nlearning paradigm for privacy-preserving node classification task under data\nvertically partitioned setting, which can be generalized to existing GCN\nmodels. Specifically, we split the computation graph data into two parts. For\neach iteration of the training process, the two parties transfer intermediate\nresults to each other under homomorphic encryption. We conduct experiments on\nbenchmark data and the results demonstrate the effectiveness of FedVGCN in the\ncase of GraphSage.",
          "link": "http://arxiv.org/abs/2106.11593",
          "publishedOn": "2021-06-23T01:48:40.504Z",
          "wordCount": 553,
          "title": "A Vertical Federated Learning Framework for Graph Convolutional Network. (arXiv:2106.11593v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1\">Michael Poli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massaroli_S/0/1/0/all/0/1\">Stefano Massaroli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabideau_C/0/1/0/all/0/1\">Clayton M. Rabideau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_A/0/1/0/all/0/1\">Atsushi Yamashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asama_H/0/1/0/all/0/1\">Hajime Asama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinkyoo Park</a>",
          "description": "We introduce the framework of continuous-depth graph neural networks (GNNs).\nNeural graph differential equations (Neural GDEs) are formalized as the\ncounterpart to GNNs where the input-output relationship is determined by a\ncontinuum of GNN layers, blending discrete topological structures and\ndifferential equations. The proposed framework is shown to be compatible with\nstatic GNN models and is extended to dynamic and stochastic settings through\nhybrid dynamical system theory. Here, Neural GDEs improve performance by\nexploiting the underlying dynamics geometry, further introducing the ability to\naccommodate irregularly sampled data. Results prove the effectiveness of the\nproposed models across applications, such as traffic forecasting or prediction\nin genetic regulatory networks.",
          "link": "http://arxiv.org/abs/2106.11581",
          "publishedOn": "2021-06-23T01:48:40.497Z",
          "wordCount": 571,
          "title": "Continuous-Depth Neural Models for Dynamic Graph Prediction. (arXiv:2106.11581v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Sin Kit Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qinghua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paik_H/0/1/0/all/0/1\">Hye-Young Paik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Liming Zhu</a>",
          "description": "Federated learning is an emerging machine learning paradigm that enables\nmultiple devices to train models locally and formulate a global model, without\nsharing the clients' local data. A federated learning system can be viewed as a\nlarge-scale distributed system, involving different components and stakeholders\nwith diverse requirements and constraints. Hence, developing a federated\nlearning system requires both software system design thinking and machine\nlearning knowledge. Although much effort has been put into federated learning\nfrom the machine learning perspectives, our previous systematic literature\nreview on the area shows that there is a distinct lack of considerations for\nsoftware architecture design for federated learning. In this paper, we propose\nFLRA, a reference architecture for federated learning systems, which provides a\ntemplate design for federated learning-based solutions. The proposed FLRA\nreference architecture is based on an extensive review of existing patterns of\nfederated learning systems found in the literature and existing industrial\nimplementation. The FLRA reference architecture consists of a pool of\narchitectural patterns that could address the frequently recurring design\nproblems in federated learning architectures. The FLRA reference architecture\ncan serve as a design guideline to assist architects and developers with\npractical solutions for their problems, which can be further customised.",
          "link": "http://arxiv.org/abs/2106.11570",
          "publishedOn": "2021-06-23T01:48:40.489Z",
          "wordCount": 648,
          "title": "FLRA: A Reference Architecture for Federated Learning Systems. (arXiv:2106.11570v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunchang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Han Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcelon_E/0/1/0/all/0/1\">Evrard Garcelon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirotta_M/0/1/0/all/0/1\">Matteo Pirotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1\">Alessandro Lazaric</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon S. Du</a>",
          "description": "We study bandits and reinforcement learning (RL) subject to a conservative\nconstraint where the agent is asked to perform at least as well as a given\nbaseline policy. This setting is particular relevant in real-world domains\nincluding digital marketing, healthcare, production, finance, etc. For\nmulti-armed bandits, linear bandits and tabular RL, specialized algorithms and\ntheoretical analyses were proposed in previous work. In this paper, we present\na unified framework for conservative bandits and RL, in which our core\ntechnique is to calculate the necessary and sufficient budget obtained from\nrunning the baseline policy. For lower bounds, our framework gives a black-box\nreduction that turns a certain lower bound in the nonconservative setting into\na new lower bound in the conservative setting. We strengthen the existing lower\nbound for conservative multi-armed bandits and obtain new lower bounds for\nconservative linear bandits, tabular RL and low-rank MDP. For upper bounds, our\nframework turns a certain nonconservative upper-confidence-bound (UCB)\nalgorithm into a conservative algorithm with a simple analysis. For multi-armed\nbandits, linear bandits and tabular RL, our new upper bounds tighten or match\nexisting ones with significantly simpler analyses. We also obtain a new upper\nbound for conservative low-rank MDP.",
          "link": "http://arxiv.org/abs/2106.11692",
          "publishedOn": "2021-06-23T01:48:40.480Z",
          "wordCount": 637,
          "title": "A Unified Framework for Conservative Exploration. (arXiv:2106.11692v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hanxuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1\">Qingchao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Wenji Mao</a>",
          "description": "Graph representation learning is a fundamental problem for modeling\nrelational data and benefits a number of downstream applications. Traditional\nBayesian-based graph models and recent deep learning based GNN either suffer\nfrom impracticability or lack interpretability, thus combined models for\nundirected graphs have been proposed to overcome the weaknesses. As a large\nportion of real-world graphs are directed graphs (of which undirected graphs\nare special cases), in this paper, we propose a Deep Latent Space Model (DLSM)\nfor directed graphs to incorporate the traditional latent variable based\ngenerative model into deep learning frameworks. Our proposed model consists of\na graph convolutional network (GCN) encoder and a stochastic decoder, which are\nlayer-wise connected by a hierarchical variational auto-encoder architecture.\nBy specifically modeling the degree heterogeneity using node random factors,\nour model possesses better interpretability in both community structure and\ndegree heterogeneity. For fast inference, the stochastic gradient variational\nBayes (SGVB) is adopted using a non-iterative recognition model, which is much\nmore scalable than traditional MCMC-based methods. The experiments on\nreal-world datasets show that the proposed model achieves the state-of-the-art\nperformances on both link prediction and community detection tasks while\nlearning interpretable node embeddings. The source code is available at\nhttps://github.com/upperr/DLSM.",
          "link": "http://arxiv.org/abs/2106.11721",
          "publishedOn": "2021-06-23T01:48:40.460Z",
          "wordCount": 635,
          "title": "A Deep Latent Space Model for Graph Representation Learning. (arXiv:2106.11721v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2003.00563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bun_M/0/1/0/all/0/1\">Mark Bun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livni_R/0/1/0/all/0/1\">Roi Livni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1\">Shay Moran</a>",
          "description": "We prove that every concept class with finite Littlestone dimension can be\nlearned by an (approximate) differentially-private algorithm. This answers an\nopen question of Alon et al. (STOC 2019) who proved the converse statement\n(this question was also asked by Neel et al.~(FOCS 2019)). Together these two\nresults yield an equivalence between online learnability and private PAC\nlearnability.\n\nWe introduce a new notion of algorithmic stability called \"global stability\"\nwhich is essential to our proof and may be of independent interest. We also\ndiscuss an application of our results to boosting the privacy and accuracy\nparameters of differentially-private learners.",
          "link": "http://arxiv.org/abs/2003.00563",
          "publishedOn": "2021-06-23T01:48:40.452Z",
          "wordCount": 615,
          "title": "An Equivalence Between Private Classification and Online Prediction. (arXiv:2003.00563v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dapeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yunpeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_G/0/1/0/all/0/1\">Guoliang Fan</a>",
          "description": "In the real world, many tasks require multiple agents to cooperate with each\nother under the condition of local observations. To solve such problems, many\nmulti-agent reinforcement learning methods based on Centralized Training with\nDecentralized Execution have been proposed. One representative class of work is\nvalue decomposition, which decomposes the global joint Q-value $Q_\\text{jt}$\ninto individual Q-values $Q_a$ to guide individuals' behaviors, e.g. VDN\n(Value-Decomposition Networks) and QMIX. However, these baselines often ignore\nthe randomness in the situation. We propose MMD-MIX, a method that combines\ndistributional reinforcement learning and value decomposition to alleviate the\nabove weaknesses. Besides, to improve data sampling efficiency, we were\ninspired by REM (Random Ensemble Mixture) which is a robust RL algorithm to\nexplicitly introduce randomness into the MMD-MIX. The experiments demonstrate\nthat MMD-MIX outperforms prior baselines in the StarCraft Multi-Agent Challenge\n(SMAC) environment.",
          "link": "http://arxiv.org/abs/2106.11652",
          "publishedOn": "2021-06-23T01:48:40.445Z",
          "wordCount": 597,
          "title": "MMD-MIX: Value Function Factorisation with Maximum Mean Discrepancy for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2106.11652v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11560",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Abhin Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1\">Karthikeyan Shanmugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1\">Kartik Ahuja</a>",
          "description": "Treatment effect estimation from observational data is a fundamental problem\nin causal inference. There are two very different schools of thought that have\ntackled this problem. On the one hand, the Pearlian framework commonly assumes\nstructural knowledge (provided by an expert) in the form of Directed Acyclic\nGraphs (DAGs) and provides graphical criteria such as the back-door criterion\nto identify the valid adjustment sets. On the other hand, the potential\noutcomes (PO) framework commonly assumes that all the observed features satisfy\nignorability (i.e., no hidden confounding), which in general is untestable. In\nthis work, we take steps to bridge these two frameworks. We show that even if\nwe know only one parent of the treatment variable (provided by an expert), then\nquite remarkably it suffices to test a broad class of (but not all) back-door\ncriteria. Importantly, we also cover the non-trivial case where the entire set\nof observed features is not ignorable (generalizing the PO framework) without\nrequiring all the parents of the treatment variable to be observed. Our key\ntechnical idea involves a more general result -- Given a synthetic sub-sampling\n(or environment) variable that is a function of the parent variable, we show\nthat an invariance test involving this sub-sampling variable is equivalent to\ntesting a broad class of back-door criteria. We demonstrate our approach on\nsynthetic data as well as real causal effect estimation benchmarks.",
          "link": "http://arxiv.org/abs/2106.11560",
          "publishedOn": "2021-06-23T01:48:40.438Z",
          "wordCount": 659,
          "title": "Finding Valid Adjustments under Non-ignorability with Minimal DAG Knowledge. (arXiv:2106.11560v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11690",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rapp_M/0/1/0/all/0/1\">Michael Rapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mencia_E/0/1/0/all/0/1\">Eneldo Loza Menc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1\">Johannes F&#xfc;rnkranz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>",
          "description": "In multi-label classification, where a single example may be associated with\nseveral class labels at the same time, the ability to model dependencies\nbetween labels is considered crucial to effectively optimize non-decomposable\nevaluation measures, such as the Subset 0/1 loss. The gradient boosting\nframework provides a well-studied foundation for learning models that are\nspecifically tailored to such a loss function and recent research attests the\nability to achieve high predictive accuracy in the multi-label setting. The\nutilization of second-order derivatives, as used by many recent boosting\napproaches, helps to guide the minimization of non-decomposable losses, due to\nthe information about pairs of labels it incorporates into the optimization\nprocess. On the downside, this comes with high computational costs, even if the\nnumber of labels is small. In this work, we address the computational\nbottleneck of such approach -- the need to solve a system of linear equations\n-- by integrating a novel approximation technique into the boosting procedure.\nBased on the derivatives computed during training, we dynamically group the\nlabels into a predefined number of bins to impose an upper bound on the\ndimensionality of the linear system. Our experiments, using an existing\nrule-based algorithm, suggest that this may boost the speed of training,\nwithout any significant loss in predictive performance.",
          "link": "http://arxiv.org/abs/2106.11690",
          "publishedOn": "2021-06-23T01:48:40.431Z",
          "wordCount": 638,
          "title": "Gradient-based Label Binning in Multi-label Classification. (arXiv:2106.11690v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yue Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Can Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>",
          "description": "Adaptive gradient methods, such as \\textsc{Adam}, have achieved tremendous\nsuccess in machine learning. Scaling gradients by square roots of the running\naverages of squared past gradients, such methods are able to attain rapid\ntraining of modern deep neural networks. Nevertheless, they are observed to\ngeneralize worse than stochastic gradient descent (\\textsc{SGD}) and tend to be\ntrapped in local minima at an early stage during training. Intriguingly, we\ndiscover that substituting the gradient in the preconditioner term with the\nmomentumized version in \\textsc{Adam} can well solve the issues. The intuition\nis that gradient with momentum contains more accurate directional information\nand therefore its second moment estimation is a better choice for scaling than\nraw gradient's. Thereby we propose \\textsc{AdaMomentum} as a new optimizer\nreaching the goal of training faster while generalizing better. We further\ndevelop a theory to back up the improvement in optimization and generalization\nand provide convergence guarantee under both convex and nonconvex settings.\nExtensive experiments on various models and tasks demonstrate that\n\\textsc{AdaMomentum} exhibits comparable performance to \\textsc{SGD} on vision\ntasks, and achieves state-of-the-art results consistently on other tasks\nincluding language processing.",
          "link": "http://arxiv.org/abs/2106.11514",
          "publishedOn": "2021-06-23T01:48:40.412Z",
          "wordCount": 630,
          "title": "Adapting Stepsizes by Momentumized Gradients Improves Optimization and Generalization. (arXiv:2106.11514v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kansal_R/0/1/0/all/0/1\">Raghav Kansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duarte_J/0/1/0/all/0/1\">Javier Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orzari_B/0/1/0/all/0/1\">Breno Orzari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomei_T/0/1/0/all/0/1\">Thiago Tomei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierini_M/0/1/0/all/0/1\">Maurizio Pierini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Touranakou_M/0/1/0/all/0/1\">Mary Touranakou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlimant_J/0/1/0/all/0/1\">Jean-Roch Vlimant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunopulos_D/0/1/0/all/0/1\">Dimitrios Gunopulos</a>",
          "description": "In high energy physics (HEP), jets are collections of correlated particles\nproduced ubiquitously in particle collisions such as those at the CERN Large\nHadron Collider (LHC). Machine-learning-based generative models, such as\ngenerative adversarial networks (GANs), have the potential to significantly\naccelerate LHC jet simulations. However, despite jets having a natural\nrepresentation as a set of particles in momentum-space, a.k.a. a particle\ncloud, to our knowledge there exist no generative models applied to such a\ndataset. We introduce a new particle cloud dataset (JetNet), and, due to\nsimilarities between particle and point clouds, apply to it existing point\ncloud GANs. Results are evaluated using (1) the 1-Wasserstein distance between\nhigh- and low-level feature distributions, (2) a newly developed Fr\\'{e}chet\nParticleNet Distance, and (3) the coverage and (4) minimum matching distance\nmetrics. Existing GANs are found to be inadequate for physics applications,\nhence we develop a new message passing GAN (MPGAN), which outperforms existing\npoint cloud GANs on virtually every metric and shows promise for use in HEP. We\npropose JetNet as a novel point-cloud-style dataset for the machine learning\ncommunity to experiment with, and set MPGAN as a benchmark to improve upon for\nfuture generative models.",
          "link": "http://arxiv.org/abs/2106.11535",
          "publishedOn": "2021-06-23T01:48:40.405Z",
          "wordCount": 657,
          "title": "Particle Cloud Generation with Message Passing Generative Adversarial Networks. (arXiv:2106.11535v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11633",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Boas_B/0/1/0/all/0/1\">Brenda Vilas Boas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zirwas_W/0/1/0/all/0/1\">Wolfgang Zirwas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haardt_M/0/1/0/all/0/1\">Martin Haardt</a>",
          "description": "A variety of wireless channel estimation methods, e.g., MUSIC and ESPRIT,\nrely on prior knowledge of the model order. Therefore, it is important to\ncorrectly estimate the number of multipath components (MPCs) which compose such\nchannels. However, environments with many scatterers may generate MPCs which\nare closely spaced. This clustering of MPCs in addition to noise makes the\nmodel order selection task difficult in practice to currently known algorithms.\nIn this paper, we exploit the multidimensional characteristics of MIMO\northogonal frequency division multiplexing (OFDM) systems and propose a machine\nlearning (ML) method capable of determining the number of MPCs with a higher\naccuracy than state of the art methods in almost coherent scenarios. Moreover,\nour results show that our proposed ML method has an enhanced reliability.",
          "link": "http://arxiv.org/abs/2106.11633",
          "publishedOn": "2021-06-23T01:48:40.398Z",
          "wordCount": 572,
          "title": "Machine Learning for Model Order Selection in MIMO OFDM Systems. (arXiv:2106.11633v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chorowski_J/0/1/0/all/0/1\">Jan Chorowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciesielski_G/0/1/0/all/0/1\">Grzegorz Ciesielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dzikowski_J/0/1/0/all/0/1\">Jaros&#x142;aw Dzikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lancucki_A/0/1/0/all/0/1\">Adrian &#x141;a&#x144;cucki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marxer_R/0/1/0/all/0/1\">Ricard Marxer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opala_M/0/1/0/all/0/1\">Mateusz Opala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pusz_P/0/1/0/all/0/1\">Piotr Pusz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rychlikowski_P/0/1/0/all/0/1\">Pawe&#x142; Rychlikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stypulkowski_M/0/1/0/all/0/1\">Micha&#x142; Stypu&#x142;kowski</a>",
          "description": "We present a number of low-resource approaches to the tasks of the Zero\nResource Speech Challenge 2021. We build on the unsupervised representations of\nspeech proposed by the organizers as a baseline, derived from CPC and clustered\nwith the k-means algorithm. We demonstrate that simple methods of refining\nthose representations can narrow the gap, or even improve upon the solutions\nwhich use a high computational budget. The results lead to the conclusion that\nthe CPC-derived representations are still too noisy for training language\nmodels, but stable enough for simpler forms of pattern matching and retrieval.",
          "link": "http://arxiv.org/abs/2106.11603",
          "publishedOn": "2021-06-23T01:48:40.391Z",
          "wordCount": 557,
          "title": "Information Retrieval for ZeroSpeech 2021: The Submission by University of Wroclaw. (arXiv:2106.11603v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadeghzadeh_A/0/1/0/all/0/1\">Amir Mahdi Sadeghzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghan_F/0/1/0/all/0/1\">Faezeh Dehghan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sobhanian_A/0/1/0/all/0/1\">Amir Mohammad Sobhanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalili_R/0/1/0/all/0/1\">Rasool Jalili</a>",
          "description": "Several recent studies have shown that Deep Neural Network (DNN)-based\nclassifiers are vulnerable against model extraction attacks. In model\nextraction attacks, an adversary exploits the target classifier to create a\nsurrogate classifier imitating the target classifier with respect to some\ncriteria. In this paper, we investigate the hardness degree of samples and\ndemonstrate that the hardness degree histogram of model extraction attacks\nsamples is distinguishable from the hardness degree histogram of normal\nsamples. Normal samples come from the target classifier's training data\ndistribution. As the training process of DNN-based classifiers is done in\nseveral epochs, we can consider this process as a sequence of subclassifiers so\nthat each subclassifier is created at the end of an epoch. We use the sequence\nof subclassifiers to calculate the hardness degree of samples. We investigate\nthe relation between hardness degree of samples and the trust in the classifier\noutputs. We propose Hardness-Oriented Detection Approach (HODA) to detect the\nsample sequences of model extraction attacks. The results demonstrate that HODA\ncan detect the sample sequences of model extraction attacks with a high success\nrate by only watching 100 attack samples. We also investigate the hardness\ndegree of adversarial examples and indicate that the hardness degree histogram\nof adversarial examples is distinct from the hardness degree histogram of\nnormal samples.",
          "link": "http://arxiv.org/abs/2106.11424",
          "publishedOn": "2021-06-23T01:48:40.383Z",
          "wordCount": 673,
          "title": "Hardness of Samples Is All You Need: Protecting Deep Learning Models Using Hardness of Samples. (arXiv:2106.11424v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mary_P/0/1/0/all/0/1\">Philippe Mary</a> (IETR), <a href=\"http://arxiv.org/find/cs/1/au:+Koivunen_V/0/1/0/all/0/1\">Visa Koivunen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moy_C/0/1/0/all/0/1\">Christophe Moy</a> (IETR)",
          "description": "In this chapter, we will give comprehensive examples of applying RL in\noptimizing the physical layer of wireless communications by defining different\nclass of problems and the possible solutions to handle them. In Section 9.2, we\npresent all the basic theory needed to address a RL problem, i.e. Markov\ndecision process (MDP), Partially observable Markov decision process (POMDP),\nbut also two very important and widely used algorithms for RL, i.e. the\nQ-learning and SARSA algorithms. We also introduce the deep reinforcement\nlearning (DRL) paradigm and the section ends with an introduction to the\nmulti-armed bandits (MAB) framework. Section 9.3 focuses on some toy examples\nto illustrate how the basic concepts of RL are employed in communication\nsystems. We present applications extracted from literature with simplified\nsystem models using similar notation as in Section 9.2 of this Chapter. In\nSection 9.3, we also focus on modeling RL problems, i.e. how action and state\nspaces and rewards are chosen. The Chapter is concluded in Section 9.4 with a\nprospective thought on RL trends and it ends with a review of a broader state\nof the art in Section 9.5.",
          "link": "http://arxiv.org/abs/2106.11595",
          "publishedOn": "2021-06-23T01:48:40.362Z",
          "wordCount": 639,
          "title": "Reinforcement learning for PHY layer communications. (arXiv:2106.11595v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zargari_A/0/1/0/all/0/1\">Amir Hosein Afandizadeh Zargari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aqajari_S/0/1/0/all/0/1\">Seyed Amir Hossein Aqajari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khodabandeh_H/0/1/0/all/0/1\">Hadi Khodabandeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1\">Amir M. Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurdahi_F/0/1/0/all/0/1\">Fadi Kurdahi</a>",
          "description": "A photoplethysmography (PPG) is an uncomplicated and inexpensive optical\ntechnique widely used in the healthcare domain to extract valuable\nhealth-related information, e.g., heart rate variability, blood pressure, and\nrespiration rate. PPG signals can easily be collected continuously and remotely\nusing portable wearable devices. However, these measuring devices are\nvulnerable to motion artifacts caused by daily life activities. The most common\nways to eliminate motion artifacts use extra accelerometer sensors, which\nsuffer from two limitations: i) high power consumption and ii) the need to\nintegrate an accelerometer sensor in a wearable device (which is not required\nin certain wearables). This paper proposes a low-power non-accelerometer-based\nPPG motion artifacts removal method outperforming the accuracy of the existing\nmethods. We use Cycle Generative Adversarial Network to reconstruct clean PPG\nsignals from noisy PPG signals. Our novel machine-learning-based technique\nachieves 9.5 times improvement in motion artifact removal compared to the\nstate-of-the-art without using extra sensors such as an accelerometer.",
          "link": "http://arxiv.org/abs/2106.11512",
          "publishedOn": "2021-06-23T01:48:40.355Z",
          "wordCount": 601,
          "title": "An Accurate Non-accelerometer-based PPG Motion Artifact Removal Technique using CycleGAN. (arXiv:2106.11512v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungmin Cha. Beomyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We consider a class-incremental semantic segmentation (CISS) problem. While\nsome recently proposed algorithms utilized variants of knowledge distillation\n(KD) technique to tackle the problem, they only partially addressed the key\nadditional challenges in CISS that causes the catastrophic forgetting; i.e.,\nthe semantic drift of the background class and multi-label prediction issue. To\nbetter address these challenges, we propose a new method, dubbed as SSUL-M\n(Semantic Segmentation with Unknown Label with Memory), by carefully combining\nseveral techniques tailored for semantic segmentation. More specifically, we\nmake three main contributions; (1) modeling unknown class within the background\nclass to help learning future classes (help plasticity), (2) freezing backbone\nnetwork and past classifiers with binary cross-entropy loss and pseudo-labeling\nto overcome catastrophic forgetting (help stability), and (3) utilizing tiny\nexemplar memory for the first time in CISS to improve both plasticity and\nstability. As a result, we show our method achieves significantly better\nperformance than the recent state-of-the-art baselines on the standard\nbenchmark datasets. Furthermore, we justify our contributions with thorough and\nextensive ablation analyses and discuss different natures of the CISS problem\ncompared to the standard class-incremental learning for classification.",
          "link": "http://arxiv.org/abs/2106.11562",
          "publishedOn": "2021-06-23T01:48:40.347Z",
          "wordCount": 631,
          "title": "SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning. (arXiv:2106.11562v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11396",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Bilevel optimization recently has attracted increased interest in machine\nlearning due to its many applications such as hyper-parameter optimization and\npolicy optimization. Although some methods recently have been proposed to solve\nthe bilevel problems, these methods do not consider using adaptive learning\nrates. To fill this gap, in the paper, we propose a class of fast and effective\nadaptive methods for solving bilevel optimization problems that the outer\nproblem is possibly nonconvex and the inner problem is strongly-convex.\nSpecifically, we propose a fast single-loop BiAdam algorithm based on the basic\nmomentum technique, which achieves a sample complexity of\n$\\tilde{O}(\\epsilon^{-4})$ for finding an $\\epsilon$-stationary point. At the\nsame time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by\nusing variance reduced technique, which reaches the best known sample\ncomplexity of $\\tilde{O}(\\epsilon^{-3})$. To further reduce computation in\nestimating derivatives, we propose a fast single-loop stochastic approximated\nBiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still\nachieves a sample complexity of $\\tilde{O}(\\epsilon^{-4})$ without large\nbatches. We further present an accelerated version of saBiAdam algorithm\n(VR-saBiAdam), which also reaches the best known sample complexity of\n$\\tilde{O}(\\epsilon^{-3})$. We apply the unified adaptive matrices to our\nmethods as the SUPER-ADAM \\citep{huang2021super}, which including many types of\nadaptive learning rates. Moreover, our framework can flexibly use the momentum\nand variance reduced techniques. In particular, we provide a useful convergence\nanalysis framework for both the constrained and unconstrained bilevel\noptimization. To the best of our knowledge, we first study the adaptive bilevel\noptimization methods with adaptive learning rates.",
          "link": "http://arxiv.org/abs/2106.11396",
          "publishedOn": "2021-06-23T01:48:40.340Z",
          "wordCount": 692,
          "title": "BiAdam: Fast Adaptive Bilevel Optimization Methods. (arXiv:2106.11396v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trautner_M/0/1/0/all/0/1\">Margaret Trautner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravela_S/0/1/0/all/0/1\">Sai Ravela</a>",
          "description": "The optimal design of neural networks is a critical problem in many\napplications. Here, we investigate how dynamical systems with polynomial\nnonlinearities can inform the design of neural systems that seek to emulate\nthem. We propose a Learnability metric and its associated features to quantify\nthe near-equilibrium behavior of learning dynamics. Equating the Learnability\nof neural systems with equivalent parameter estimation metric of the reference\nsystem establishes bounds on network structure. In this way, norms from theory\nprovide a good first guess for neural structure, which may then further adapt\nwith data. The proposed approach neither requires training nor training data.\nIt reveals exact sizing for a class of neural networks with multiplicative\nnodes that mimic continuous- or discrete-time polynomial dynamics. It also\nprovides relatively tight lower size bounds for classical feed-forward networks\nthat is consistent with simulated assessments.",
          "link": "http://arxiv.org/abs/2106.11409",
          "publishedOn": "2021-06-23T01:48:40.332Z",
          "wordCount": 592,
          "title": "Learn Like The Pro: Norms from Theory to Size Neural Computation. (arXiv:2106.11409v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>",
          "description": "Logical relations widely exist in human activities. Human use them for making\njudgement and decision according to various conditions, which are embodied in\nthe form of \\emph{if-then} rules. As an important kind of cognitive\nintelligence, it is prerequisite of representing and storing logical relations\nrightly into computer systems so as to make automatic judgement and decision,\nespecially for high-risk domains like medical diagnosis. However, current\nnumeric ANN (Artificial Neural Network) models are good at perceptual\nintelligence such as image recognition while they are not good at cognitive\nintelligence such as logical representation, blocking the further application\nof ANN. To solve it, researchers have tried to design logical ANN models to\nrepresent and store logical relations. Although there are some advances in this\nresearch area, recent works still have disadvantages because the structures of\nthese logical ANN models still don't map more directly with logical relations\nwhich will cause the corresponding logical relations cannot be read out from\ntheir network structures. Therefore, in order to represent logical relations\nmore clearly by the neural network structure and to read out logical relations\nfrom it, this paper proposes a novel logical ANN model by designing the new\nlogical neurons and links in demand of logical representation. Compared with\nthe recent works on logical ANN models, this logical ANN model has more clear\ncorresponding with logical relations using the more direct mapping method\nherein, thus logical relations can be read out following the connection\npatterns of the network structure. Additionally, less neurons are used.",
          "link": "http://arxiv.org/abs/2106.11463",
          "publishedOn": "2021-06-23T01:48:40.312Z",
          "wordCount": 694,
          "title": "A Logical Neural Network Structure With More Direct Mapping From Logical Relations. (arXiv:2106.11463v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debapriya Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lygerakis_F/0/1/0/all/0/1\">Fotios Lygerakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makedon_F/0/1/0/all/0/1\">Fillia Makedon</a>",
          "description": "Multi-modal sentiment analysis plays an important role for providing better\ninteractive experiences to users. Each modality in multi-modal data can provide\ndifferent viewpoints or reveal unique aspects of a user's emotional state. In\nthis work, we use text, audio and visual modalities from MOSI dataset and we\npropose a novel fusion technique using a multi-head attention LSTM network.\nFinally, we perform a classification task and evaluate its performance.",
          "link": "http://arxiv.org/abs/2106.11473",
          "publishedOn": "2021-06-23T01:48:40.305Z",
          "wordCount": 502,
          "title": "Sequential Late Fusion Technique for Multi-modal Sentiment Analysis. (arXiv:2106.11473v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hafez_A/0/1/0/all/0/1\">Ahmad Hafez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Praphul_A/0/1/0/all/0/1\">Atulya Praphul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaradt_Y/0/1/0/all/0/1\">Yousef Jaradt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godwin_E/0/1/0/all/0/1\">Ezani Godwin</a>",
          "description": "Learning node representations on temporal graphs is a fundamental step to\nlearn real-word dynamic graphs efficiently. Real-world graphs have the nature\nof continuously evolving over time, such as changing edges weights, removing\nand adding nodes and appearing and disappearing of edges, while previous graph\nrepresentation learning methods focused generally on static graphs. We present\nConvDySAT as an enhancement of DySAT, one of the state-of-the-art dynamic\nmethods, by augmenting convolution neural networks with the self-attention\nmechanism, the employed method in DySAT to express the structural and temporal\nevolution. We conducted single-step link prediction on a communication network\nand rating network, Experimental results show significant performance gains for\nConvDySAT over various state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.11430",
          "publishedOn": "2021-06-23T01:48:40.299Z",
          "wordCount": 553,
          "title": "ConvDySAT: Deep Neural Representation Learning on Dynamic Graphs via Self-Attention and Convolutional Neural Networks. (arXiv:2106.11430v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>",
          "description": "Place Recognition is a crucial capability for mobile robot localization and\nnavigation. Image-based or Visual Place Recognition (VPR) is a challenging\nproblem as scene appearance and camera viewpoint can change significantly when\nplaces are revisited. Recent VPR methods based on ``sequential\nrepresentations'' have shown promising results as compared to traditional\nsequence score aggregation or single image based techniques. In parallel to\nthese endeavors, 3D point clouds based place recognition is also being explored\nfollowing the advances in deep learning based point cloud processing. However,\na key question remains: is an explicit 3D structure based place representation\nalways superior to an implicit ``spatial'' representation based on sequence of\nRGB images which can inherently learn scene structure. In this extended\nabstract, we attempt to compare these two types of methods by considering a\nsimilar ``metric span'' to represent places. We compare a 3D point cloud based\nmethod (PointNetVLAD) with image sequence based methods (SeqNet and others) and\nshowcase that image sequence based techniques approach, and can even surpass,\nthe performance achieved by point cloud based methods for a given metric span.\nThese performance variations can be attributed to differences in data richness\nof input sensors as well as data accumulation strategies for a mobile robot.\nWhile a perfect apple-to-apple comparison may not be feasible for these two\ndifferent modalities, the presented comparison takes a step in the direction of\nanswering deeper questions regarding spatial representations, relevant to\nseveral applications like Autonomous Driving and Augmented/Virtual Reality.\nSource code available publicly https://github.com/oravus/seqNet.",
          "link": "http://arxiv.org/abs/2106.11481",
          "publishedOn": "2021-06-23T01:48:40.291Z",
          "wordCount": 722,
          "title": "SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for Day-Night Place Recognition. (arXiv:2106.11481v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1\">Ajil Jalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmalkar_S/0/1/0/all/0/1\">Sushrut Karmalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1\">Eric Price</a>",
          "description": "We characterize the measurement complexity of compressed sensing of signals\ndrawn from a known prior distribution, even when the support of the prior is\nthe entire space (rather than, say, sparse vectors). We show for Gaussian\nmeasurements and \\emph{any} prior distribution on the signal, that the\nposterior sampling estimator achieves near-optimal recovery guarantees.\nMoreover, this result is robust to model mismatch, as long as the distribution\nestimate (e.g., from an invertible generative model) is close to the true\ndistribution in Wasserstein distance. We implement the posterior sampling\nestimator for deep generative priors using Langevin dynamics, and empirically\nfind that it produces accurate estimates with more diversity than MAP.",
          "link": "http://arxiv.org/abs/2106.11438",
          "publishedOn": "2021-06-23T01:48:40.285Z",
          "wordCount": 545,
          "title": "Instance-Optimal Compressed Sensing via Posterior Sampling. (arXiv:2106.11438v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Treven_L/0/1/0/all/0/1\">Lenart Treven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenk_P/0/1/0/all/0/1\">Philippe Wenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorfler_F/0/1/0/all/0/1\">Florian D&#xf6;rfler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1\">Andreas Krause</a>",
          "description": "Differential equations in general and neural ODEs in particular are an\nessential technique in continuous-time system identification. While many\ndeterministic learning algorithms have been designed based on numerical\nintegration via the adjoint method, many downstream tasks such as active\nlearning, exploration in reinforcement learning, robust control, or filtering\nrequire accurate estimates of predictive uncertainties. In this work, we\npropose a novel approach towards estimating epistemically uncertain neural\nODEs, avoiding the numerical integration bottleneck. Instead of modeling\nuncertainty in the ODE parameters, we directly model uncertainties in the state\nspace. Our algorithm - distributional gradient matching (DGM) - jointly trains\na smoother and a dynamics model and matches their gradients via minimizing a\nWasserstein loss. Our experiments show that, compared to traditional\napproximate inference methods based on numerical integration, our approach is\nfaster to train, faster at predicting previously unseen trajectories, and in\nthe context of neural ODEs, significantly more accurate.",
          "link": "http://arxiv.org/abs/2106.11609",
          "publishedOn": "2021-06-23T01:48:40.263Z",
          "wordCount": 592,
          "title": "Distributional Gradient Matching for Learning Uncertain Neural Dynamics Models. (arXiv:2106.11609v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Doan_T/0/1/0/all/0/1\">Tung Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takasu_A/0/1/0/all/0/1\">Atsuhiro Takasu</a>",
          "description": "Kernel segmentation aims at partitioning a data sequence into several\nnon-overlapping segments that may have nonlinear and complex structures. In\ngeneral, it is formulated as a discrete optimization problem with combinatorial\nconstraints. A popular algorithm for optimally solving this problem is dynamic\nprogramming (DP), which has quadratic computation and memory requirements.\nGiven that sequences in practice are too long, this algorithm is not a\npractical approach. Although many heuristic algorithms have been proposed to\napproximate the optimal segmentation, they have no guarantee on the quality of\ntheir solutions. In this paper, we take a differentiable approach to alleviate\nthe aforementioned issues. First, we introduce a novel sigmoid-based\nregularization to smoothly approximate the combinatorial constraints. Combining\nit with objective of the balanced kernel clustering, we formulate a\ndifferentiable model termed Kernel clustering with sigmoid-based regularization\n(KCSR), where the gradient-based algorithm can be exploited to obtain the\noptimal segmentation. Second, we develop a stochastic variant of the proposed\nmodel. By using the stochastic gradient descent algorithm, which has much lower\ntime and space complexities, for optimization, the second model can perform\nsegmentation on overlong data sequences. Finally, for simultaneously segmenting\nmultiple data sequences, we slightly modify the sigmoid-based regularization to\nfurther introduce an extended variant of the proposed model. Through extensive\nexperiments on various types of data sequences performances of our models are\nevaluated and compared with those of the existing methods. The experimental\nresults validate advantages of the proposed models. Our Matlab source code is\navailable on github.",
          "link": "http://arxiv.org/abs/2106.11541",
          "publishedOn": "2021-06-23T01:48:40.253Z",
          "wordCount": 686,
          "title": "Kernel Clustering with Sigmoid-based Regularization for Efficient Segmentation of Sequential Data. (arXiv:2106.11541v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11451",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+He_Q/0/1/0/all/0/1\">QiZhi He</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Stinis_P/0/1/0/all/0/1\">Panos Stinis</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tartakovsky_A/0/1/0/all/0/1\">Alexandre Tartakovsky</a>",
          "description": "In this paper, we present a physics-constrained deep neural network (PCDNN)\nmethod for parameter estimation in the zero-dimensional (0D) model of the\nvanadium redox flow battery (VRFB). In this approach, we use deep neural\nnetworks (DNNs) to approximate the model parameters as functions of the\noperating conditions. This method allows the integration of the VRFB\ncomputational models as the physical constraints in the parameter learning\nprocess, leading to enhanced accuracy of parameter estimation and cell voltage\nprediction. Using an experimental dataset, we demonstrate that the PCDNN method\ncan estimate model parameters for a range of operating conditions and improve\nthe 0D model prediction of voltage compared to the 0D model prediction with\nconstant operation-condition-independent parameters estimated with traditional\ninverse methods. We also demonstrate that the PCDNN approach has an improved\ngeneralization ability for estimating parameter values for operating conditions\nnot used in the DNN training.",
          "link": "http://arxiv.org/abs/2106.11451",
          "publishedOn": "2021-06-23T01:48:40.247Z",
          "wordCount": 587,
          "title": "Physics-constrained deep neural network method for estimating parameters in a redox flow battery. (arXiv:2106.11451v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Steven Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>",
          "description": "With leveraging the weight-sharing and continuous relaxation to enable\ngradient-descent to alternately optimize the supernet weights and the\narchitecture parameters through a bi-level optimization paradigm,\n\\textit{Differentiable ARchiTecture Search} (DARTS) has become the mainstream\nmethod in Neural Architecture Search (NAS) due to its simplicity and\nefficiency. However, more recent works found that the performance of the\nsearched architecture barely increases with the optimization proceeding in\nDARTS. In addition, several concurrent works show that the NAS could find more\ncompetitive architectures without labels. The above observations reveal that\nthe supervision signal in DARTS may be a poor indicator for architecture\noptimization, inspiring a foundational question: instead of using the\nsupervision signal to perform bi-level optimization, \\textit{can we find\nhigh-quality architectures \\textbf{without any training nor labels}}? We\nprovide an affirmative answer by customizing the NAS as a network pruning at\ninitialization problem. By leveraging recent techniques on the network pruning\nat initialization, we designed a FreeFlow proxy to score the importance of\ncandidate operations in NAS without any training nor labels, and proposed a\nnovel framework called \\textit{training and label free neural architecture\nsearch} (\\textbf{FreeNAS}) accordingly. We show that, without any training nor\nlabels, FreeNAS with the proposed FreeFlow proxy can outperform most NAS\nbaselines. More importantly, our framework is extremely efficient, which\ncompletes the architecture search within only \\textbf{3.6s} and \\textbf{79s} on\na single GPU for the NAS-Bench-201 and DARTS search space, respectively. We\nhope our work inspires more attempts in solving NAS from the perspective of\npruning at initialization.",
          "link": "http://arxiv.org/abs/2106.11542",
          "publishedOn": "2021-06-23T01:48:40.240Z",
          "wordCount": 693,
          "title": "Differentiable Architecture Search Without Training Nor Labels: A Pruning Perspective. (arXiv:2106.11542v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeongho Kim</a>",
          "description": "The author of this work proposes an overview of the recent semi-supervised\nlearning approaches and related works. Despite the remarkable success of neural\nnetworks in various applications, there exist few formidable constraints\nincluding the need for a large amount of labeled data. Therefore,\nsemi-supervised learning, which is a learning scheme in which the scarce labels\nand a larger amount of unlabeled data are utilized to train models (e.g., deep\nneural networks) is getting more important. Based on the key assumptions of\nsemi-supervised learning, which are the manifold assumption, cluster\nassumption, and continuity assumption, the work reviews the recent\nsemi-supervised learning approaches. In particular, the methods in regard to\nusing deep neural networks in a semi-supervised learning setting are primarily\ndiscussed. In addition, the existing works are first classified based on the\nunderlying idea and explained, and then the holistic approaches that unify the\naforementioned ideas are detailed.",
          "link": "http://arxiv.org/abs/2106.11528",
          "publishedOn": "2021-06-23T01:48:40.232Z",
          "wordCount": 582,
          "title": "Recent Deep Semi-supervised Learning Approaches and Related Works. (arXiv:2106.11528v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11447",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Silva_J/0/1/0/all/0/1\">Jo&#xe3;o Louren&#xe7;o Silva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menezes_M/0/1/0/all/0/1\">Miguel Nobre Menezes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodrigues_T/0/1/0/all/0/1\">Tiago Rodrigues</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Silva_B/0/1/0/all/0/1\">Beatriz Silva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pinto_F/0/1/0/all/0/1\">Fausto J. Pinto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_A/0/1/0/all/0/1\">Arlindo L. Oliveira</a>",
          "description": "Coronary X-ray angiography is a crucial clinical procedure for the diagnosis\nand treatment of coronary artery disease, which accounts for roughly 16% of\nglobal deaths every year. However, the images acquired in these procedures have\nlow resolution and poor contrast, making lesion detection and assessment\nchallenging. Accurate coronary artery segmentation not only helps mitigate\nthese problems, but also allows the extraction of relevant anatomical features\nfor further analysis by quantitative methods. Although automated segmentation\nof coronary arteries has been proposed before, previous approaches have used\nnon-optimal segmentation criteria, leading to less useful results. Most methods\neither segment only the major vessel, discarding important information from the\nremaining ones, or segment the whole coronary tree based mostly on contrast\ninformation, producing a noisy output that includes vessels that are not\nrelevant for diagnosis. We adopt a better-suited clinical criterion and segment\nvessels according to their clinical relevance. Additionally, we simultaneously\nperform catheter segmentation, which may be useful for diagnosis due to the\nscale factor provided by the catheter's known diameter, and is a task that has\nnot yet been performed with good results. To derive the optimal approach, we\nconducted an extensive comparative study of encoder-decoder architectures\ntrained on a combination of focal loss and a variant of generalized dice loss.\nBased on the EfficientNet and the UNet++ architectures, we propose a line of\nefficient and high-performance segmentation models using a new decoder\narchitecture, the EfficientUNet++, whose best-performing version achieved\naverage dice scores of 0.8904 and 0.7526 for the artery and catheter classes,\nrespectively, and an average generalized dice score of 0.9234.",
          "link": "http://arxiv.org/abs/2106.11447",
          "publishedOn": "2021-06-23T01:48:40.214Z",
          "wordCount": 719,
          "title": "Encoder-Decoder Architectures for Clinically Relevant Coronary Artery Segmentation. (arXiv:2106.11447v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Duo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fekri_F/0/1/0/all/0/1\">Faramarz Fekri</a>",
          "description": "Recently deep reinforcement learning has achieved tremendous success in wide\nranges of applications. However, it notoriously lacks data-efficiency and\ninterpretability. Data-efficiency is important as interacting with the\nenvironment is expensive. Further, interpretability can increase the\ntransparency of the black-box-style deep RL models and hence gain trust from\nthe users. In this work, we propose a new hierarchical framework via symbolic\nRL, leveraging a symbolic transition model to improve the data-efficiency and\nintroduce the interpretability for learned policy. This framework consists of a\nhigh-level agent, a subtask solver and a symbolic transition model. Without\nassuming any prior knowledge on the state transition, we adopt inductive logic\nprogramming (ILP) to learn the rules of symbolic state transitions, introducing\ninterpretability and making the learned behavior understandable to users. In\nempirical experiments, we confirmed that the proposed framework offers\napproximately between 30\\% to 40\\% more data efficiency over previous methods.",
          "link": "http://arxiv.org/abs/2106.11417",
          "publishedOn": "2021-06-23T01:48:40.204Z",
          "wordCount": 573,
          "title": "Interpretable Model-based Hierarchical Reinforcement Learning using Inductive Logic Programming. (arXiv:2106.11417v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_F/0/1/0/all/0/1\">Freddie Bickford Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roads_B/0/1/0/all/0/1\">Brett D Roads</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiaoliang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Love_B/0/1/0/all/0/1\">Bradley C Love</a>",
          "description": "Top-down attention allows neural networks, both artificial and biological, to\nfocus on the information most relevant for a given task. This is known to\nenhance performance in visual perception. But it remains unclear how attention\nbrings about its perceptual boost, especially when it comes to naturalistic\nsettings like recognising an object in an everyday scene. What aspects of a\nvisual task does attention help to deal with? We aim to answer this with a\ncomputational experiment based on a general framework called task-oriented\nablation design. First we define a broad range of visual tasks and identify six\nfactors that underlie task variability. Then on each task we compare the\nperformance of two neural networks, one with top-down attention and one\nwithout. These comparisons reveal the task-dependence of attention's perceptual\nboost, giving a clearer idea of the role attention plays. Whereas many existing\ncognitive accounts link attention to stimulus-level variables, such as visual\nclutter and object scale, we find greater explanatory power in system-level\nvariables that capture the interaction between the model, the distribution of\ntraining data and the task format. This finding suggests a shift in how\nattention is studied could be fruitful. We make publicly available our code and\nresults, along with statistics relevant to ImageNet-based experiments beyond\nthis one. Our contribution serves to support the development of more human-like\nvision models and the design of more informative machine-learning experiments.",
          "link": "http://arxiv.org/abs/2106.11339",
          "publishedOn": "2021-06-23T01:48:40.114Z",
          "wordCount": 672,
          "title": "Understanding top-down attention using task-oriented ablation design. (arXiv:2106.11339v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1\">Ye Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_C/0/1/0/all/0/1\">Chun Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yiping Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1\">Shaunak Chatterjee</a>",
          "description": "Social media platforms bring together content creators and content consumers\nthrough recommender systems like newsfeed. The focus of such recommender\nsystems has thus far been primarily on modeling the content consumer\npreferences and optimizing for their experience. However, it is equally\ncritical to nurture content creation by prioritizing the creators' interests,\nas quality content forms the seed for sustainable engagement and conversations,\nbringing in new consumers while retaining existing ones. In this work, we\npropose a modeling approach to predict how feedback from content consumers\nincentivizes creators. We then leverage this model to optimize the newsfeed\nexperience for content creators by reshaping the feedback distribution, leading\nto a more active content ecosystem. Practically, we discuss how we balance the\nuser experience for both consumers and creators, and how we carry out online\nA/B tests with strong network effects. We present a deployed use case on the\nLinkedIn newsfeed, where we used this approach to improve content creation\nsignificantly without compromising the consumers' experience.",
          "link": "http://arxiv.org/abs/2106.11312",
          "publishedOn": "2021-06-23T01:48:40.105Z",
          "wordCount": 626,
          "title": "Feedback Shaping: A Modeling Approach to Nurture Content Creation. (arXiv:2106.11312v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Eslam Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1\">Ahmed El-Sallab</a>",
          "description": "Moving objects have special importance for Autonomous Driving tasks.\nDetecting moving objects can be posed as Moving Object Segmentation, by\nsegmenting the object pixels, or Moving Object Detection, by generating a\nbounding box for the moving targets. In this paper, we present a Multi-Task\nLearning architecture, based on Transformers, to jointly perform both tasks\nthrough one network. Due to the importance of the motion features to the task,\nthe whole setup is based on a Spatio-Temporal aggregation. We evaluate the\nperformance of the individual tasks architecture versus the MTL setup, both\nwith early shared encoders, and late shared encoder-decoder transformers. For\nthe latter, we present a novel joint tasks query decoder transformer, that\nenables us to have tasks dedicated heads out of the shared model. To evaluate\nour approach, we use the KITTI MOD [29] data set. Results show1.5% mAP\nimprovement for Moving Object Detection, and 2%IoU improvement for Moving\nObject Segmentation, over the individual tasks networks.",
          "link": "http://arxiv.org/abs/2106.11401",
          "publishedOn": "2021-06-23T01:48:40.099Z",
          "wordCount": 599,
          "title": "Spatio-Temporal Multi-Task Learning Transformer for Joint Moving Object Detection and Segmentation. (arXiv:2106.11401v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11374",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bhogi_K/0/1/0/all/0/1\">Keerthana Bhogi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saha_C/0/1/0/all/0/1\">Chiranjib Saha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dhillon_H/0/1/0/all/0/1\">Harpreet S. Dhillon</a>",
          "description": "This paper develops an efficient procedure for designing low-complexity\ncodebooks for precoding in a full-dimension (FD) multiple-input multiple-output\n(MIMO) system with a uniform planar array (UPA) antenna at the transmitter (Tx)\nusing tensor learning. In particular, instead of using statistical channel\nmodels, we utilize a model-free data-driven approach with foundations in\nmachine learning to generate codebooks that adapt to the surrounding\npropagation conditions. We use a tensor representation of the FD-MIMO channel\nand exploit its properties to design quantized version of the channel\nprecoders. We find the best representation of the optimal precoder as a\nfunction of Kronecker Product (KP) of two low-dimensional precoders,\nrespectively corresponding to the horizontal and vertical dimensions of the\nUPA, obtained from the tensor decomposition of the channel. We then quantize\nthis precoder to design product codebooks such that an average loss in mutual\ninformation due to quantization of channel state information (CSI) is\nminimized. The key technical contribution lies in exploiting the constraints on\nthe precoders to reduce the product codebook design problem to an unsupervised\nclustering problem on a Cartesian Product Grassmann manifold (CPM), where the\ncluster centroids form a finite-sized precoder codebook. This codebook can be\nfound efficiently by running a $K$-means clustering on the CPM. With a suitable\ninduced distance metric on the CPM, we show that the construction of product\ncodebooks is equivalent to finding the optimal set of centroids on the factor\nmanifolds corresponding to the horizontal and vertical dimensions. Simulation\nresults are presented to demonstrate the capability of the proposed design\ncriterion in learning the codebooks and the attractive performance of the\ndesigned codebooks.",
          "link": "http://arxiv.org/abs/2106.11374",
          "publishedOn": "2021-06-23T01:48:40.082Z",
          "wordCount": 712,
          "title": "Tensor Learning-based Precoder Codebooks for FD-MIMO Systems. (arXiv:2106.11374v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Eslam Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1\">Ahmad El-Sallab</a>",
          "description": "Moving Object Detection (MOD) is a crucial task for the Autonomous Driving\npipeline. MOD is usually handled via 2-stream convolutional architectures that\nincorporates both appearance and motion cues, without considering the\ninter-relations between the spatial or motion features. In this paper, we\ntackle this problem through multi-head attention mechanisms, both across the\nspatial and motion streams. We propose MODETR; a Moving Object DEtection\nTRansformer network, comprised of multi-stream transformer encoders for both\nspatial and motion modalities, and an object transformer decoder that produces\nthe moving objects bounding boxes using set predictions. The whole architecture\nis trained end-to-end using bi-partite loss. Several methods of incorporating\nmotion cues with the Transformer model are explored, including two-stream RGB\nand Optical Flow (OF) methods, and multi-stream architectures that take\nadvantage of sequence information. To incorporate the temporal information, we\npropose a new Temporal Positional Encoding (TPE) approach to extend the Spatial\nPositional Encoding(SPE) in DETR. We explore two architectural choices for\nthat, balancing between speed and time. To evaluate the our network, we perform\nthe MOD task on the KITTI MOD [6] data set. Results show significant 5% mAP of\nthe Transformer network for MOD over the state-of-the art methods. Moreover,\nthe proposed TPE encoding provides 10% mAP improvement over the SPE baseline.",
          "link": "http://arxiv.org/abs/2106.11422",
          "publishedOn": "2021-06-23T01:48:40.074Z",
          "wordCount": 662,
          "title": "MODETR: Moving Object Detection with Transformers. (arXiv:2106.11422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1\">Saeed Mahloujifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1\">Huseyin A. Inan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chase_M/0/1/0/all/0/1\">Melissa Chase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_E/0/1/0/all/0/1\">Esha Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_M/0/1/0/all/0/1\">Marcello Hasegawa</a>",
          "description": "In the text processing context, most ML models are built on word embeddings.\nThese embeddings are themselves trained on some datasets, potentially\ncontaining sensitive data. In some cases this training is done independently,\nin other cases, it occurs as part of training a larger, task-specific model. In\neither case, it is of interest to consider membership inference attacks based\non the embedding layer as a way of understanding sensitive information leakage.\nBut, somewhat surprisingly, membership inference attacks on word embeddings and\ntheir effect in other natural language processing (NLP) tasks that use these\nembeddings, have remained relatively unexplored.\n\nIn this work, we show that word embeddings are vulnerable to black-box\nmembership inference attacks under realistic assumptions. Furthermore, we show\nthat this leakage persists through two other major NLP applications:\nclassification and text-generation, even when the embedding layer is not\nexposed to the attacker. We show that our MI attack achieves high attack\naccuracy against a classifier model and an LSTM-based language model. Indeed,\nour attack is a cheaper membership inference attack on text-generative models,\nwhich does not require the knowledge of the target model or any expensive\ntraining of text-generative models as shadow models.",
          "link": "http://arxiv.org/abs/2106.11384",
          "publishedOn": "2021-06-23T01:48:40.067Z",
          "wordCount": 640,
          "title": "Membership Inference on Word Embedding and Beyond. (arXiv:2106.11384v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zichang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coleman_B/0/1/0/all/0/1\">Benjamin Coleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Anshumali Shrivastava</a>",
          "description": "Large machine learning models achieve unprecedented performance on various\ntasks and have evolved as the go-to technique. However, deploying these compute\nand memory hungry models on resource constraint environments poses new\nchallenges. In this work, we propose mathematically provable Representer\nSketch, a concise set of count arrays that can approximate the inference\nprocedure with simple hashing computations and aggregations. Representer Sketch\nbuilds upon the popular Representer Theorem from kernel literature, hence the\nname, providing a generic fundamental alternative to the problem of efficient\ninference that goes beyond the popular approach such as quantization, iterative\npruning and knowledge distillation. A neural network function is transformed to\nits weighted kernel density representation, which can be very efficiently\nestimated with our sketching algorithm. Empirically, we show that Representer\nSketch achieves up to 114x reduction in storage requirement and 59x reduction\nin computation complexity without any drop in accuracy.",
          "link": "http://arxiv.org/abs/2106.11426",
          "publishedOn": "2021-06-23T01:48:40.061Z",
          "wordCount": 578,
          "title": "Efficient Inference via Universal LSH Kernel. (arXiv:2106.11426v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11428",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Celentano_M/0/1/0/all/0/1\">Michael Celentano</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fan_Z/0/1/0/all/0/1\">Zhou Fan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mei_S/0/1/0/all/0/1\">Song Mei</a>",
          "description": "We study mean-field variational Bayesian inference using the TAP approach,\nfor Z2-synchronization as a prototypical example of a high-dimensional Bayesian\nmodel. We show that for any signal strength $\\lambda > 1$ (the weak-recovery\nthreshold), there exists a unique local minimizer of the TAP free energy\nfunctional near the mean of the Bayes posterior law. Furthermore, the TAP free\nenergy in a local neighborhood of this minimizer is strongly convex.\nConsequently, a natural-gradient/mirror-descent algorithm achieves linear\nconvergence to this minimizer from a local initialization, which may be\nobtained by a finite number of iterates of Approximate Message Passing (AMP).\nThis provides a rigorous foundation for variational inference in high\ndimensions via minimization of the TAP free energy.\n\nWe also analyze the finite-sample convergence of AMP, showing that AMP is\nasymptotically stable at the TAP minimizer for any $\\lambda > 1$, and is\nlinearly convergent to this minimizer from a spectral initialization for\nsufficiently large $\\lambda$. Such a guarantee is stronger than results\nobtainable by state evolution analyses, which only describe a fixed number of\nAMP iterations in the infinite-sample limit.\n\nOur proofs combine the Kac-Rice formula and Sudakov-Fernique Gaussian\ncomparison inequality to analyze the complexity of critical points that satisfy\nstrong convexity and stability conditions within their local neighborhoods.",
          "link": "http://arxiv.org/abs/2106.11428",
          "publishedOn": "2021-06-23T01:48:40.054Z",
          "wordCount": 656,
          "title": "Local convexity of the TAP free energy and AMP convergence for Z2-synchronization. (arXiv:2106.11428v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>",
          "description": "This open-source book represents our attempt to make deep learning\napproachable, teaching readers the concepts, the context, and the code. The\nentire book is drafted in Jupyter notebooks, seamlessly integrating exposition\nfigures, math, and interactive examples with self-contained code. Our goal is\nto offer a resource that could (i) be freely available for everyone; (ii) offer\nsufficient technical depth to provide a starting point on the path to actually\nbecoming an applied machine learning scientist; (iii) include runnable code,\nshowing readers how to solve problems in practice; (iv) allow for rapid\nupdates, both by us and also by the community at large; (v) be complemented by\na forum for interactive discussion of technical details and to answer\nquestions.",
          "link": "http://arxiv.org/abs/2106.11342",
          "publishedOn": "2021-06-23T01:48:40.025Z",
          "wordCount": 565,
          "title": "Dive into Deep Learning. (arXiv:2106.11342v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aounon Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_A/0/1/0/all/0/1\">Alexander Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>",
          "description": "The study of provable adversarial robustness for deep neural network (DNN)\nmodels has mainly focused on static supervised learning tasks such as image\nclassification. However, DNNs have been used extensively in real-world adaptive\ntasks such as reinforcement learning (RL), making RL systems vulnerable to\nadversarial attacks. The key challenge in adversarial RL is that the attacker\ncan adapt itself to the defense strategy used by the agent in previous\ntime-steps to strengthen its attack in future steps. In this work, we study the\nprovable robustness of RL against norm-bounded adversarial perturbations of the\ninputs. We focus on smoothing-based provable defenses and propose policy\nsmoothing where the agent adds a Gaussian noise to its observation at each\ntime-step before applying the policy network to make itself less sensitive to\nadversarial perturbations of its inputs. Our main theoretical contribution is\nto prove an adaptive version of the Neyman-Pearson Lemma where the adversarial\nperturbation at a particular time can be a stochastic function of current and\nprevious observations and states as well as previously observed actions. Using\nthis lemma, we adapt the robustness certificates produced by randomized\nsmoothing in the static setting of image classification to the dynamic setting\nof RL. We generate certificates that guarantee that the total reward obtained\nby the smoothed policy will not fall below a certain threshold under a\nnorm-bounded adversarial perturbation of the input. We show that our\ncertificates are tight by constructing a worst-case setting that achieves the\nbounds derived in our analysis. In our experiments, we show that this method\ncan yield meaningful certificates in complex environments demonstrating its\neffectiveness against adversarial attacks.",
          "link": "http://arxiv.org/abs/2106.11420",
          "publishedOn": "2021-06-23T01:48:40.010Z",
          "wordCount": 693,
          "title": "Policy Smoothing for Provably Robust Reinforcement Learning. (arXiv:2106.11420v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tejaswin_P/0/1/0/all/0/1\">Priyam Tejaswin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_D/0/1/0/all/0/1\">Dhruv Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>",
          "description": "State-of-the-art summarization systems are trained and evaluated on massive\ndatasets scraped from the web. Despite their prevalence, we know very little\nabout the underlying characteristics (data noise, summarization complexity,\netc.) of these datasets, and how these affect system performance and the\nreliability of automatic metrics like ROUGE. In this study, we manually analyze\n600 samples from three popular summarization datasets. Our study is driven by a\nsix-class typology which captures different noise types (missing facts,\nentities) and degrees of summarization difficulty (extractive, abstractive). We\nfollow with a thorough analysis of 27 state-of-the-art summarization models and\n5 popular metrics, and report our key insights: (1) Datasets have distinct data\nquality and complexity distributions, which can be traced back to their\ncollection process. (2) The performance of models and reliability of metrics is\ndependent on sample complexity. (3) Faithful summaries often receive low scores\nbecause of the poor diversity of references. We release the code, annotated\ndata and model outputs.",
          "link": "http://arxiv.org/abs/2106.11388",
          "publishedOn": "2021-06-23T01:48:40.003Z",
          "wordCount": 598,
          "title": "How well do you know your summarization datasets?. (arXiv:2106.11388v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Acuna_D/0/1/0/all/0/1\">David Acuna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guojun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1\">Marc T. Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>",
          "description": "Unsupervised domain adaptation is used in many machine learning applications\nwhere, during training, a model has access to unlabeled data in the target\ndomain, and a related labeled dataset. In this paper, we introduce a novel and\ngeneral domain-adversarial framework. Specifically, we derive a novel\ngeneralization bound for domain adaptation that exploits a new measure of\ndiscrepancy between distributions based on a variational characterization of\nf-divergences. It recovers the theoretical results from Ben-David et al.\n(2010a) as a special case and supports divergences used in practice. Based on\nthis bound, we derive a new algorithmic framework that introduces a key\ncorrection in the original adversarial training method of Ganin et al. (2016).\nWe show that many regularizers and ad-hoc objectives introduced over the last\nyears in this framework are then not required to achieve performance comparable\nto (if not better than) state-of-the-art domain-adversarial methods.\nExperimental analysis conducted on real-world natural language and computer\nvision datasets show that our framework outperforms existing baselines, and\nobtains the best results for f-divergences that were not considered previously\nin domain-adversarial learning.",
          "link": "http://arxiv.org/abs/2106.11344",
          "publishedOn": "2021-06-23T01:48:39.968Z",
          "wordCount": 616,
          "title": "f-Domain-Adversarial Learning: Theory and Algorithms. (arXiv:2106.11344v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wicker_M/0/1/0/all/0/1\">Matthew Wicker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurenti_L/0/1/0/all/0/1\">Luca Laurenti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patane_A/0/1/0/all/0/1\">Andrea Patane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paoletti_N/0/1/0/all/0/1\">Nicola Paoletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1\">Alessandro Abate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1\">Marta Kwiatkowska</a>",
          "description": "We consider the problem of computing reach-avoid probabilities for iterative\npredictions made with Bayesian neural network (BNN) models. Specifically, we\nleverage bound propagation techniques and backward recursion to compute lower\nbounds for the probability that trajectories of the BNN model reach a given set\nof states while avoiding a set of unsafe states. We use the lower bounds in the\ncontext of control and reinforcement learning to provide safety certification\nfor given control policies, as well as to synthesize control policies that\nimprove the certification bounds. On a set of benchmarks, we demonstrate that\nour framework can be employed to certify policies over BNNs predictions for\nproblems of more than $10$ dimensions, and to effectively synthesize policies\nthat significantly increase the lower bound on the satisfaction probability.",
          "link": "http://arxiv.org/abs/2105.10134",
          "publishedOn": "2021-06-22T01:57:13.990Z",
          "wordCount": 597,
          "title": "Certification of Iterative Predictions in Bayesian Neural Networks. (arXiv:2105.10134v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1\">Nawshad Farruque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1\">Randy Goebel</a>",
          "description": "In this paper, we present empirical analysis on basic and depression specific\nmulti-emotion mining in Tweets with the help of state of the art multi-label\nclassifiers. We choose our basic emotions from a hybrid emotion model\nconsisting of the common emotions from four highly regarded psychological\nmodels of emotions. Moreover, we augment that emotion model with new emotion\ncategories because of their importance in the analysis of depression. Most of\nthose additional emotions have not been used in previous emotion mining\nresearch. Our experimental analyses show that a cost sensitive RankSVM\nalgorithm and a Deep Learning model are both robust, measured by both Macro\nF-measures and Micro F-measures. This suggests that these algorithms are\nsuperior in addressing the widely known data imbalance problem in multi-label\nlearning. Moreover, our application of Deep Learning performs the best, giving\nit an edge in modeling deep semantic features of our extended emotional\ncategories.",
          "link": "http://arxiv.org/abs/2105.12364",
          "publishedOn": "2021-06-22T01:57:13.943Z",
          "wordCount": 620,
          "title": "Basic and Depression Specific Emotion Identification in Tweets: Multi-label Classification Experiments. (arXiv:2105.12364v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07524",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kollias_D/0/1/0/all/0/1\">Dimitrios Kollias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arsenos_A/0/1/0/all/0/1\">Anastasios Arsenos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soukissian_L/0/1/0/all/0/1\">Levon Soukissian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kollias_S/0/1/0/all/0/1\">Stefanos Kollias</a>",
          "description": "Early and reliable COVID-19 diagnosis based on chest 3-D CT scans can assist\nmedical specialists in vital circumstances. Deep learning methodologies\nconstitute a main approach for chest CT scan analysis and disease prediction.\nHowever, large annotated databases are necessary for developing deep learning\nmodels that are able to provide COVID-19 diagnosis across various medical\nenvironments in different countries. Due to privacy issues, publicly available\nCOVID-19 CT datasets are highly difficult to obtain, which hinders the research\nand development of AI-enabled diagnosis methods of COVID-19 based on CT scans.\nIn this paper we present the COV19-CT-DB database which is annotated for\nCOVID-19, consisting of about 5,000 3-D CT scans, We have split the database in\ntraining, validation and test datasets. The former two datasets can be used for\ntraining and validation of machine learning models, while the latter will be\nused for evaluation of the developed models. We also present a deep learning\napproach, based on a CNN-RNN network and report its performance on the\nCOVID19-CT-DB database.",
          "link": "http://arxiv.org/abs/2106.07524",
          "publishedOn": "2021-06-22T01:57:13.916Z",
          "wordCount": 678,
          "title": "MIA-COV19D: COVID-19 Detection through 3-D Chest CT Image Analysis. (arXiv:2106.07524v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lowe_E/0/1/0/all/0/1\">Evan Lowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guvenc_L/0/1/0/all/0/1\">Levent Guven&#xe7;</a>",
          "description": "As passenger vehicle technologies have advanced, so have their capabilities\nto avoid obstacles, especially with developments in tires, suspensions,\nsteering, as well as safety technologies like ABS, ESC, and more recently, ADAS\nsystems. However, environments around passenger vehicles have also become more\ncomplex, and dangerous. There have previously been studies that outline driver\ntendencies and performance capabilities when attempting to avoid obstacles\nwhile driving passenger vehicles. Now that autonomous vehicles are being\ndeveloped with obstacle avoidance capabilities, it is important to target\nperformance that meets or exceeds that of human drivers. This manuscript\nhighlights systems that are crucial for an emergency obstacle avoidance\nmaneuver (EOAM) and identifies the state-of-the-art for each of the related\nsystems, while considering the nuances of traveling at highway speeds. Some of\nthe primary EOAM-related systems/areas that are discussed in this review are:\ngeneral path planning methods, system hierarchies, decision-making, trajectory\ngeneration, and trajectory-tracking control methods. After concluding remarks,\nsuggestions for future work which could lead to an ideal EOAM development, are\ndiscussed.",
          "link": "http://arxiv.org/abs/2105.09446",
          "publishedOn": "2021-06-22T01:57:13.911Z",
          "wordCount": 665,
          "title": "A Review of Autonomous Road Vehicle Integrated Approaches to an Emergency Obstacle Avoidance Maneuver. (arXiv:2105.09446v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">George Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jingjing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khisti_A/0/1/0/all/0/1\">Ashish Khisti</a>",
          "description": "In the context of lossy compression, Blau & Michaeli (2019) adopt a\nmathematical notion of perceptual quality and define the information\nrate-distortion-perception function, generalizing the classical rate-distortion\ntradeoff. We consider the notion of universal representations in which one may\nfix an encoder and vary the decoder to achieve any point within a collection of\ndistortion and perception constraints. We prove that the corresponding\ninformation-theoretic universal rate-distortion-perception function is\noperationally achievable in an approximate sense. Under MSE distortion, we show\nthat the entire distortion-perception tradeoff of a Gaussian source can be\nachieved by a single encoder of the same rate asymptotically. We then\ncharacterize the achievable distortion-perception region for a fixed\nrepresentation in the case of arbitrary distributions, identify conditions\nunder which the aforementioned results continue to hold approximately, and\nstudy the case when the rate is not fixed in advance. This motivates the study\nof practical constructions that are approximately universal across the RDP\ntradeoff, thereby alleviating the need to design a new encoder for each\nobjective. We provide experimental results on MNIST and SVHN suggesting that on\nimage compression tasks, the operational tradeoffs achieved by machine learning\nmodels with a fixed encoder suffer only a small penalty when compared to their\nvariable encoder counterparts.",
          "link": "http://arxiv.org/abs/2106.10311",
          "publishedOn": "2021-06-22T01:57:13.900Z",
          "wordCount": 637,
          "title": "Universal Rate-Distortion-Perception Representations for Lossy Compression. (arXiv:2106.10311v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Phuc_L/0/1/0/all/0/1\">Luu Huu Phuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeuchi_K/0/1/0/all/0/1\">Koh Takeuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okajima_S/0/1/0/all/0/1\">Seiji Okajima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolmachev_A/0/1/0/all/0/1\">Arseny Tolmachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takebayashi_T/0/1/0/all/0/1\">Tomoyoshi Takebayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maruhashi_K/0/1/0/all/0/1\">Koji Maruhashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1\">Hisashi Kashima</a>",
          "description": "Multi-relational graph is a ubiquitous and important data structure, allowing\nflexible representation of multiple types of interactions and relations between\nentities. Similar to other graph-structured data, link prediction is one of the\nmost important tasks on multi-relational graphs and is often used for knowledge\ncompletion. When related graphs coexist, it is of great benefit to build a\nlarger graph via integrating the smaller ones. The integration requires\npredicting hidden relational connections between entities belonged to different\ngraphs (inter-domain link prediction). However, this poses a real challenge to\nexisting methods that are exclusively designed for link prediction between\nentities of the same graph only (intra-domain link prediction). In this study,\nwe propose a new approach to tackle the inter-domain link prediction problem by\nsoftly aligning the entity distributions between different domains with optimal\ntransport and maximum mean discrepancy regularizers. Experiments on real-world\ndatasets show that optimal transport regularizer is beneficial and considerably\nimproves the performance of baseline methods.",
          "link": "http://arxiv.org/abs/2106.06171",
          "publishedOn": "2021-06-22T01:57:13.886Z",
          "wordCount": 614,
          "title": "Inter-domain Multi-relational Link Prediction. (arXiv:2106.06171v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07135",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Yang_C/0/1/0/all/0/1\">Chaoqi Yang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Singh_N/0/1/0/all/0/1\">Navjot Singh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xiao_C/0/1/0/all/0/1\">Cao Xiao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Qian_C/0/1/0/all/0/1\">Cheng Qian</a>, <a href=\"http://arxiv.org/find/math/1/au:+Solomonik_E/0/1/0/all/0/1\">Edgar Solomonik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>",
          "description": "Existing tensor completion formulation mostly relies on partial observations\nfrom a single tensor. However, tensors extracted from real-world data are often\nmore complex due to: (i) Partial observation: Only a small subset (e.g., 5%) of\ntensor elements are available. (ii) Coarse observation: Some tensor modes only\npresent coarse and aggregated patterns (e.g., monthly summary instead of daily\nreports). In this paper, we are given a subset of the tensor and some\naggregated/coarse observations (along one or more modes) and seek to recover\nthe original fine-granular tensor with low-rank factorization. We formulate a\ncoupled tensor completion problem and propose an efficient Multi-resolution\nTensor Completion model (MTC) to solve the problem. Our MTC model explores\ntensor mode properties and leverages the hierarchy of resolutions to\nrecursively initialize an optimization setup, and optimizes on the coupled\nsystem using alternating least squares. MTC ensures low computational and space\ncomplexity. We evaluate our model on two COVID-19 related spatio-temporal\ntensors. The experiments show that MTC could provide 65.20% and 75.79%\npercentage of fitness (PoF) in tensor completion with only 5% fine granular\nobservations, which is 27.96% relative improvement over the best baseline. To\nevaluate the learned low-rank factors, we also design a tensor prediction task\nfor daily and cumulative disease case predictions, where MTC achieves 50% in\nPoF and 30% relative improvements over the best baseline.",
          "link": "http://arxiv.org/abs/2106.07135",
          "publishedOn": "2021-06-22T01:57:13.868Z",
          "wordCount": 733,
          "title": "MTC: Multiresolution Tensor Completion from Partial and Coarse Observations. (arXiv:2106.07135v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cachay_S/0/1/0/all/0/1\">Salva R&#xfc;hling Cachay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boecking_B/0/1/0/all/0/1\">Benedikt Boecking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubrawski_A/0/1/0/all/0/1\">Artur Dubrawski</a>",
          "description": "Data programming (DP) has proven to be an attractive alternative to costly\nhand-labeling of data.\n\nIn DP, users encode domain knowledge into \\emph{labeling functions} (LF),\nheuristics that label a subset of the data noisily and may have complex\ndependencies. A label model is then fit to the LFs to produce an estimate of\nthe unknown class label.\n\nThe effects of label model misspecification on test set performance of a\ndownstream classifier are understudied. This presents a serious awareness gap\nto practitioners, in particular since the dependency structure among LFs is\nfrequently ignored in field applications of DP.\n\nWe analyse modeling errors due to structure over-specification.\n\nWe derive novel theoretical bounds on the modeling error and empirically show\nthat this error can be substantial, even when modeling a seemingly sensible\nstructure.",
          "link": "http://arxiv.org/abs/2106.10302",
          "publishedOn": "2021-06-22T01:57:13.856Z",
          "wordCount": 581,
          "title": "Dependency Structure Misspecification in Multi-Source Weak Supervision Models. (arXiv:2106.10302v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06285",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Elliott_T/0/1/0/all/0/1\">Thomas J. Elliott</a>",
          "description": "Stochastic modelling is an essential component of the quantitative sciences,\nwith hidden Markov models (HMMs) often playing a central role. Concurrently,\nthe rise of quantum technologies promises a host of advantages in computational\nproblems, typically in terms of the scaling of requisite resources such as time\nand memory. HMMs are no exception to this, with recent results highlighting\nquantum implementations of deterministic HMMs exhibiting superior memory and\nthermal efficiency relative to their classical counterparts. In many contexts\nhowever, non-deterministic HMMs are viable alternatives; compared to them the\nadvantages of current quantum implementations do not always hold. Here, we\nprovide a systematic prescription for constructing quantum implementations of\nnon-deterministic HMMs that re-establish the quantum advantages against this\nbroader class. Crucially, we show that whenever the classical implementation\nsuffers from thermal dissipation due to its need to process information in a\ntime-local manner, our quantum implementations will both mitigate some of this\ndissipation, and achieve an advantage in memory compression.",
          "link": "http://arxiv.org/abs/2105.06285",
          "publishedOn": "2021-06-22T01:57:13.839Z",
          "wordCount": 646,
          "title": "Memory compression and thermal efficiency of quantum implementations of non-deterministic hidden Markov models. (arXiv:2105.06285v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roland S. Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1\">Yash Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1\">Steffen Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>",
          "description": "Contrastive learning has recently seen tremendous success in self-supervised\nlearning. So far, however, it is largely unclear why the learned\nrepresentations generalize so effectively to a large variety of downstream\ntasks. We here prove that feedforward models trained with objectives belonging\nto the commonly used InfoNCE family learn to implicitly invert the underlying\ngenerative model of the observed data. While the proofs make certain\nstatistical assumptions about the generative model, we observe empirically that\nour findings hold even if these assumptions are severely violated. Our theory\nhighlights a fundamental connection between contrastive learning, generative\nmodeling, and nonlinear independent component analysis, thereby furthering our\nunderstanding of the learned representations as well as providing a theoretical\nfoundation to derive more effective contrastive losses.",
          "link": "http://arxiv.org/abs/2102.08850",
          "publishedOn": "2021-06-22T01:57:13.719Z",
          "wordCount": 624,
          "title": "Contrastive Learning Inverts the Data Generating Process. (arXiv:2102.08850v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03361",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Parhi_R/0/1/0/all/0/1\">Rahul Parhi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nowak_R/0/1/0/all/0/1\">Robert D. Nowak</a>",
          "description": "We develop a variational framework to understand the properties of functions\nlearned by deep neural networks with ReLU activation functions fit to data. We\npropose a new function space, which is reminiscent of classical bounded\nvariation spaces, that captures the compositional structure associated with\ndeep neural networks. We derive a representer theorem showing that deep ReLU\nnetworks are solutions to regularized data fitting problems in this function\nspace. The function space consists of compositions of functions from the\n(non-reflexive) Banach spaces of second-order bounded variation in the Radon\ndomain. These are Banach spaces with sparsity-promoting norms, giving insight\ninto the role of sparsity in deep neural networks. The neural network solutions\nhave skip connections and rank bounded weight matrices, providing new\ntheoretical support for these common architectural choices. The variational\nproblem we study can be recast as a finite-dimensional neural network training\nproblem with regularization schemes related to the notions of weight decay and\npath-norm regularization. Finally, our analysis builds on techniques from\nvariational spline theory, providing new connections between deep neural\nnetworks and splines.",
          "link": "http://arxiv.org/abs/2105.03361",
          "publishedOn": "2021-06-22T01:57:13.714Z",
          "wordCount": 635,
          "title": "What Kinds of Functions do Deep Neural Networks Learn? Insights from Variational Spline Theory. (arXiv:2105.03361v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aldahdooh_A/0/1/0/all/0/1\">Ahmed Aldahdooh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1\">Olivier D&#xe9;forges</a>",
          "description": "Security-sensitive applications that rely on Deep Neural Networks (DNNs) are\nvulnerable to small perturbations that are crafted to generate Adversarial\nExamples(AEs). The AEs are imperceptible to humans and cause DNN to misclassify\nthem. Many defense and detection techniques have been proposed. Model's\nconfidences and Dropout, as a popular way to estimate the model's uncertainty,\nhave been used for AE detection but they showed limited success against black-\nand gray-box attacks. Moreover, the state-of-the-art detection techniques have\nbeen designed for specific attacks or broken by others, need knowledge about\nthe attacks, are not consistent, increase model parameters overhead, are\ntime-consuming, or have latency in inference time. To trade off these factors,\nwe revisit the model's uncertainty and confidences and propose a novel\nunsupervised ensemble AE detection mechanism that 1) uses the uncertainty\nmethod called SelectiveNet, 2) processes model layers outputs, i.e.feature\nmaps, to generate new confidence probabilities. The detection method is called\nSelective and Feature based Adversarial Detection (SFAD). Experimental results\nshow that the proposed approach achieves better performance against black- and\ngray-box attacks than the state-of-the-art methods and achieves comparable\nperformance against white-box attacks. Moreover, results show that SFAD is\nfully robust against High Confidence Attacks (HCAs) for MNIST and partially\nrobust for CIFAR10 datasets.",
          "link": "http://arxiv.org/abs/2103.05354",
          "publishedOn": "2021-06-22T01:57:13.708Z",
          "wordCount": 677,
          "title": "Revisiting Model's Uncertainty and Confidences for Adversarial Example Detection. (arXiv:2103.05354v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01450",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Park_J/0/1/0/all/0/1\">Ji Won Park</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Villar_A/0/1/0/all/0/1\">Ashley Villar</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Jiang_Y/0/1/0/all/0/1\">Yan-Fei Jiang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Ho_S/0/1/0/all/0/1\">Shirley Ho</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Lin_J/0/1/0/all/0/1\">Joshua Yao-Yu Lin</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Marshall_P/0/1/0/all/0/1\">Philip J. Marshall</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Roodman_A/0/1/0/all/0/1\">Aaron Roodman</a>",
          "description": "Among the most extreme objects in the Universe, active galactic nuclei (AGN)\nare luminous centers of galaxies where a black hole feeds on surrounding\nmatter. The variability patterns of the light emitted by an AGN contain\ninformation about the physical properties of the underlying black hole.\nUpcoming telescopes will observe over 100 million AGN in multiple broadband\nwavelengths, yielding a large sample of multivariate time series with long gaps\nand irregular sampling. We present a method that reconstructs the AGN time\nseries and simultaneously infers the posterior probability density distribution\n(PDF) over the physical quantities of the black hole, including its mass and\nluminosity. We apply this method to a simulated dataset of 11,000 AGN and\nreport precision and accuracy of 0.4 dex and 0.3 dex in the inferred black hole\nmass. This work is the first to address probabilistic time series\nreconstruction and parameter inference for AGN in an end-to-end fashion.",
          "link": "http://arxiv.org/abs/2106.01450",
          "publishedOn": "2021-06-22T01:57:13.703Z",
          "wordCount": 674,
          "title": "Inferring Black Hole Properties from Astronomical Multivariate Time Series with Bayesian Attentive Neural Processes. (arXiv:2106.01450v2 [astro-ph.IM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koskela_A/0/1/0/all/0/1\">Antti Koskela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honkela_A/0/1/0/all/0/1\">Antti Honkela</a>",
          "description": "The recently proposed Fast Fourier Transform (FFT)-based accountant for\nevaluating $(\\varepsilon,\\delta)$-differential privacy guarantees using the\nprivacy loss distribution formalism has been shown to give tighter bounds than\ncommonly used methods such as R\\'enyi accountants when applied to homogeneous\ncompositions, i.e., to compositions of identical mechanisms. In this paper, we\nextend this approach to heterogeneous compositions. We carry out a full error\nanalysis that allows choosing the parameters of the algorithm such that a\ndesired accuracy is obtained. The analysis also extends previous results by\ntaking into account all the parameters of the algorithm. Using the error\nanalysis, we also give a bound for the computational complexity in terms of the\nerror which is analogous to and slightly tightens the one given by Murtagh and\nVadhan (2018). We also show how to speed up the evaluation of tight privacy\nguarantees using the Plancherel theorem at the cost of increased\npre-computation and memory usage.",
          "link": "http://arxiv.org/abs/2102.12412",
          "publishedOn": "2021-06-22T01:57:13.688Z",
          "wordCount": 622,
          "title": "Computing Differential Privacy Guarantees for Heterogeneous Compositions Using FFT. (arXiv:2102.12412v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1\">James Lee-Thorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_I/0/1/0/all/0/1\">Ilya Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Ontanon</a>",
          "description": "We show that Transformer encoder architectures can be massively sped up, with\nlimited accuracy costs, by replacing the self-attention sublayers with simple\nlinear transformations that \"mix\" input tokens. These linear transformations,\nalong with standard nonlinearities in feed-forward layers, prove competent at\nmodeling semantic relationships in several text classification tasks. Most\nsurprisingly, we find that replacing the self-attention sublayer in a\nTransformer encoder with a standard, unparameterized Fourier Transform achieves\n92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains\nnearly seven times faster on GPUs and twice as fast on TPUs. The resulting\nmodel, FNet, also scales very efficiently to long inputs. Specifically, when\ncompared to the \"efficient\" Transformers on the Long Range Arena benchmark,\nFNet matches the accuracy of the most accurate models, but is faster than the\nfastest models across all sequence lengths on GPUs (and across relatively\nshorter lengths on TPUs). Finally, FNet has a light memory footprint and is\nparticularly efficient at smaller model sizes: for a fixed speed and accuracy\nbudget, small FNet models outperform Transformer counterparts.",
          "link": "http://arxiv.org/abs/2105.03824",
          "publishedOn": "2021-06-22T01:57:13.683Z",
          "wordCount": 639,
          "title": "FNet: Mixing Tokens with Fourier Transforms. (arXiv:2105.03824v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1\">Serguei Barannikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sotnikov_G/0/1/0/all/0/1\">Grigorii Sotnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trofimov_I/0/1/0/all/0/1\">Ilya Trofimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korotin_A/0/1/0/all/0/1\">Alexander Korotin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>",
          "description": "We apply methods of topological data analysis to loss functions to gain\ninsights on learning of deep neural networks and their generalization\nproperties. We study global properties of the loss function gradient flow. We\nuse topological data analysis of the loss function and its Morse complex to\nrelate local behavior along gradient trajectories with global properties of the\nloss surface. We define neural network Topological Obstructions score,\nTO-score, with help of robust topological invariants, barcodes of loss\nfunction, that quantify the badness of local minima for gradient-based\noptimization. We have made several experiments for computing these invariants,\nfor small neural networks, and for fully connected, convolutional and\nResNet-like neural networks on different datasets: MNIST, Fashion MNIST,\nCIFAR10, SVHN. Our two principal observations are as follows. Firstly, the\nneural network barcode and TO-score decrease with the increase of the neural\nnetwork depth and width. Secondly, there is an intriguing connection between\nthe length of minima segments in the barcode and the minima generalization\nerror.",
          "link": "http://arxiv.org/abs/2012.15834",
          "publishedOn": "2021-06-22T01:57:13.676Z",
          "wordCount": 611,
          "title": "Topological obstructions in neural networks learning. (arXiv:2012.15834v1 [cs.LG] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Richards_S/0/1/0/all/0/1\">Spencer M. Richards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizan_N/0/1/0/all/0/1\">Navid Azizan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slotine_J/0/1/0/all/0/1\">Jean-Jacques Slotine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>",
          "description": "Real-time adaptation is imperative to the control of robots operating in\ncomplex, dynamic environments. Adaptive control laws can endow even nonlinear\nsystems with good trajectory tracking performance, provided that any uncertain\ndynamics terms are linearly parameterizable with known nonlinear features.\nHowever, it is often difficult to specify such features a priori, such as for\naerodynamic disturbances on rotorcraft or interaction forces between a\nmanipulator arm and various objects. In this paper, we turn to data-driven\nmodeling with neural networks to learn, offline from past data, an adaptive\ncontroller with an internal parametric model of these nonlinear features. Our\nkey insight is that we can better prepare the controller for deployment with\ncontrol-oriented meta-learning of features in closed-loop simulation, rather\nthan regression-oriented meta-learning of features to fit input-output data.\nSpecifically, we meta-learn the adaptive controller with closed-loop tracking\nsimulation as the base-learner and the average tracking error as the\nmeta-objective. With a nonlinear planar rotorcraft subject to wind, we\ndemonstrate that our adaptive controller outperforms other controllers trained\nwith regression-oriented meta-learning when deployed in closed-loop for\ntrajectory tracking control.",
          "link": "http://arxiv.org/abs/2103.04490",
          "publishedOn": "2021-06-22T01:57:13.669Z",
          "wordCount": 650,
          "title": "Adaptive-Control-Oriented Meta-Learning for Nonlinear Systems. (arXiv:2103.04490v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baher_H/0/1/0/all/0/1\">Hugo Le Baher</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lemaire_V/0/1/0/all/0/1\">Vincent Lemaire</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Trinquart_R/0/1/0/all/0/1\">Romain Trinquart</a> (2) ((1) Polytech Nantes (France), (2) Orange Labs (France))",
          "description": "In some industrial applications such as fraud detection, the performance of\ncommon supervision techniques may be affected by the poor quality of the\navailable labels : in actual operational use-cases, these labels may be weak in\nquantity, quality or trustworthiness. We propose a benchmark to evaluate the\nnatural robustness of different algorithms taken from various paradigms on\nartificially corrupted datasets, with a focus on noisy labels. This paper\nstudies the intrinsic robustness of some leading classifiers. The algorithms\nunder scrutiny include SVM, logistic regression, random forests, XGBoost,\nKhiops. Furthermore, building on results from recent literature, the study is\nsupplemented with an investigation into the opportunity to enhance some\nalgorithms with symmetric loss functions.",
          "link": "http://arxiv.org/abs/2010.13570",
          "publishedOn": "2021-06-22T01:57:13.663Z",
          "wordCount": 632,
          "title": "On the intrinsic robustness to noise of some leading classifiers and symmetric loss function -- an empirical evaluation. (arXiv:2010.13570v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bar_N/0/1/0/all/0/1\">Noga Bar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1\">Tomer Koren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>",
          "description": "Deep neural networks are widespread due to their powerful performance. Yet,\nthey suffer from degraded performance in the presence of noisy labels at train\ntime or adversarial examples during inference. Inspired by the setting of\nlearning with expert advice, where multiplicative weights (MW) updates were\nrecently shown to be robust to moderate adversarial corruptions, we propose to\nuse MW for reweighting examples during neural networks optimization. We\nestablish the convergence of our method when used with gradient descent and\nshow its advantage in two simple examples. We then validate empirically our\nfindings by demonstrating that MW improve networks accuracy in the presence of\nlabel noise on CIFAR-10, CIFAR-100 and Clothing1M, and leads to better\nrobustness to adversarial attacks.",
          "link": "http://arxiv.org/abs/2102.12192",
          "publishedOn": "2021-06-22T01:57:13.648Z",
          "wordCount": 585,
          "title": "Multiplicative Reweighting for Robust Neural Network Optimization. (arXiv:2102.12192v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00666",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuxin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1\">Bencheng Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiemin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiyang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1\">Jianwei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>",
          "description": "Can Transformer perform $2\\mathrm{D}$ object-level recognition from a pure\nsequence-to-sequence perspective with minimal knowledge about the $2\\mathrm{D}$\nspatial structure? To answer this question, we present You Only Look at One\nSequence (YOLOS), a series of object detection models based on the na\\\"ive\nVision Transformer with the fewest possible modifications as well as inductive\nbiases. We find that YOLOS pre-trained on the mid-sized ImageNet-$1k$ dataset\nonly can already achieve competitive object detection performance on COCO,\n\\textit{e.g.}, YOLOS-Base directly adopted from BERT-Base can achieve $42.0$\nbox AP. We also discuss the impacts as well as limitations of current pre-train\nschemes and model scaling strategies for Transformer in vision through object\ndetection. Code and model weights are available at\n\\url{https://github.com/hustvl/YOLOS}.",
          "link": "http://arxiv.org/abs/2106.00666",
          "publishedOn": "2021-06-22T01:57:13.643Z",
          "wordCount": 611,
          "title": "You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection. (arXiv:2106.00666v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01933",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gaddy_D/0/1/0/all/0/1\">David Gaddy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>",
          "description": "In this paper, we present an improved model for voicing silent speech, where\naudio is synthesized from facial electromyography (EMG) signals. To give our\nmodel greater flexibility to learn its own input features, we directly use EMG\nsignals as input in the place of hand-designed features used by prior work. Our\nmodel uses convolutional layers to extract features from the signals and\nTransformer layers to propagate information across longer distances. To provide\nbetter signal for learning, we also introduce an auxiliary task of predicting\nphoneme labels in addition to predicting speech audio features. On an open\nvocabulary intelligibility evaluation, our model improves the state of the art\nfor this task by an absolute 25.8%.",
          "link": "http://arxiv.org/abs/2106.01933",
          "publishedOn": "2021-06-22T01:57:13.637Z",
          "wordCount": 578,
          "title": "An Improved Model for Voicing Silent Speech. (arXiv:2106.01933v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08609",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Borra_F/0/1/0/all/0/1\">Francesco Borra</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Biferale_L/0/1/0/all/0/1\">Luca Biferale</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Cencini_M/0/1/0/all/0/1\">Massimo Cencini</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Celani_A/0/1/0/all/0/1\">Antonio Celani</a>",
          "description": "Aquatic organisms can use hydrodynamic cues to navigate, find their preys and\nescape from predators. We consider a model of two competing microswimmers\nengaged in a pursue-evasion task while immersed in a low-Reynolds-number\nenvironment. The players have limited abilities: they can only sense\nhydrodynamic disturbances, which provide some cue about the opponent's\nposition, and perform simple manoeuvres. The goal of the pursuer is to\ncapturethe evader in the shortest possible time. Conversely the evader aims at\ndeferring capture as much as possible. We show that by means of Reinforcement\nLearning the players find efficient and physically explainable strategies which\nnon-trivially exploit the hydrodynamic environment. This Letter offers a\nproof-of-concept for the use of Reinforcement Learning to discover\nprey-predator strategies in aquatic environments, with potential applications\nto underwater robotics.",
          "link": "http://arxiv.org/abs/2106.08609",
          "publishedOn": "2021-06-22T01:57:13.631Z",
          "wordCount": 596,
          "title": "Reinforcement learning for pursuit and evasion of microswimmers at low Reynolds number. (arXiv:2106.08609v1 [physics.flu-dyn] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tavakoli_A/0/1/0/all/0/1\">Arash Tavakoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatemi_M/0/1/0/all/0/1\">Mehdi Fatemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kormushev_P/0/1/0/all/0/1\">Petar Kormushev</a>",
          "description": "Action-value estimation is a critical component of many reinforcement\nlearning (RL) methods whereby sample complexity relies heavily on how fast a\ngood estimator for action value can be learned. By viewing this problem through\nthe lens of representation learning, good representations of both state and\naction can facilitate action-value estimation. While advances in deep learning\nhave seamlessly driven progress in learning state representations, given the\nspecificity of the notion of agency to RL, little attention has been paid to\nlearning action representations. We conjecture that leveraging the\ncombinatorial structure of multi-dimensional action spaces is a key ingredient\nfor learning good representations of action. To test this, we set forth the\naction hypergraph networks framework -- a class of functions for learning\naction representations in multi-dimensional discrete action spaces with a\nstructural inductive bias. Using this framework we realise an agent class based\non a combination with deep Q-networks, which we dub hypergraph Q-networks. We\nshow the effectiveness of our approach on a myriad of domains: illustrative\nprediction problems under minimal confounding effects, Atari 2600 games, and\ndiscretised physical control benchmarks.",
          "link": "http://arxiv.org/abs/2010.14680",
          "publishedOn": "2021-06-22T01:57:13.625Z",
          "wordCount": 656,
          "title": "Learning to Represent Action Values as a Hypergraph on the Action Vertices. (arXiv:2010.14680v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaobin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>",
          "description": "Recently, considerable literature has grown up around the theme of few-shot\nnamed entity recognition (NER), but little published benchmark data\nspecifically focused on the practical and challenging task. Current approaches\ncollect existing supervised NER datasets and re-organize them to the few-shot\nsetting for empirical study. These strategies conventionally aim to recognize\ncoarse-grained entity types with few examples, while in practice, most unseen\nentity types are fine-grained. In this paper, we present Few-NERD, a\nlarge-scale human-annotated few-shot NER dataset with a hierarchy of 8\ncoarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238\nsentences from Wikipedia, 4,601,160 words are included and each is annotated as\ncontext or a part of a two-level entity type. To the best of our knowledge,\nthis is the first few-shot NER dataset and the largest human-crafted NER\ndataset. We construct benchmark tasks with different emphases to\ncomprehensively assess the generalization capability of models. Extensive\nempirical results and analysis show that Few-NERD is challenging and the\nproblem requires further research. We make Few-NERD public at\nhttps://ningding97.github.io/fewnerd/.",
          "link": "http://arxiv.org/abs/2105.07464",
          "publishedOn": "2021-06-22T01:57:13.611Z",
          "wordCount": 683,
          "title": "Few-NERD: A Few-Shot Named Entity Recognition Dataset. (arXiv:2105.07464v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.13697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1\">Mustafa Hajij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Istvan_K/0/1/0/all/0/1\">Kyle Istvan</a>",
          "description": "We utilize classical facts from topology to show that the classification\nproblem in machine learning is always solvable under very mild conditions.\nFurthermore, we show that a softmax classification network acts on an input\ntopological space by a finite sequence of topological moves to achieve the\nclassification task. Moreover, given a training dataset, we show how\ntopological formalism can be used to suggest the appropriate architectural\nchoices for neural networks designed to be trained as classifiers on the data.\nFinally, we show how the architecture of a neural network cannot be chosen\nindependently from the shape of the underlying data. To demonstrate these\nresults, we provide example datasets and show how they are acted upon by neural\nnets from this topological perspective.",
          "link": "http://arxiv.org/abs/2008.13697",
          "publishedOn": "2021-06-22T01:57:13.603Z",
          "wordCount": 683,
          "title": "A Topological Framework for Deep Learning. (arXiv:2008.13697v13 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Levi_M/0/1/0/all/0/1\">Matan Levi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attias_I/0/1/0/all/0/1\">Idan Attias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kontorovich_A/0/1/0/all/0/1\">Aryeh Kontorovich</a>",
          "description": "The phenomenon of adversarial examples illustrates one of the most basic\nvulnerabilities of deep neural networks. Among the variety of techniques\nintroduced to surmount this inherent weakness, adversarial training has emerged\nas the most common and efficient strategy to achieve robustness. Typically,\nthis is achieved by balancing robust and natural objectives. In this work, we\naim to achieve better trade-off between robust and natural performances by\nenforcing a domain-invariant feature representation. We present a new\nadversarial training method, Domain Invariant Adversarial Learning (DIAL),\nwhich learns a feature representation which is both robust and domain\ninvariant. DIAL uses a variant of Domain Adversarial Neural Network (DANN) on\nthe natural domain and its corresponding adversarial domain. In a case where\nthe source domain consists of natural examples and the target domain is the\nadversarially perturbed examples, our method learns a feature representation\nconstrained not to discriminate between the natural and adversarial examples,\nand can therefore achieve a more robust representation. Our experiments\nindicate that our method improves both robustness and natural accuracy, when\ncompared to current state-of-the-art adversarial training methods.",
          "link": "http://arxiv.org/abs/2104.00322",
          "publishedOn": "2021-06-22T01:57:13.597Z",
          "wordCount": 633,
          "title": "Domain Invariant Adversarial Learning. (arXiv:2104.00322v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>",
          "description": "This paper serves as a survey of recent advances in large margin training and\nits theoretical foundations, mostly for (nonlinear) deep neural networks (DNNs)\nthat are probably the most prominent machine learning models for large-scale\ndata in the community over the past decade. We generalize the formulation of\nclassification margins from classical research to latest DNNs, summarize\ntheoretical connections between the margin, network generalization, and\nrobustness, and introduce recent efforts in enlarging the margins for DNNs\ncomprehensively. Since the viewpoint of different methods is discrepant, we\ncategorize them into groups for ease of comparison and discussion in the paper.\nHopefully, our discussions and overview inspire new research work in the\ncommunity that aim to improve the performance of DNNs, and we also point to\ndirections where the large margin principle can be verified to provide\ntheoretical evidence why certain regularizations for DNNs function well in\npractice. We managed to shorten the paper such that the crucial spirit of large\nmargin learning and related methods are better emphasized.",
          "link": "http://arxiv.org/abs/2103.13598",
          "publishedOn": "2021-06-22T01:57:13.592Z",
          "wordCount": null,
          "title": "Recent Advances in Large Margin Learning. (arXiv:2103.13598v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Calvin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lala Li</a>",
          "description": "Contrastive loss and its variants have become very popular recently for\nlearning visual representations without supervision. In this work, we study\nthree intriguing properties of contrastive learning. We first generalize the\nstandard contrastive loss to a broader family of losses, and we find that\nvarious instantiations of the generalized loss perform similarly under the\npresence of a multi-layer non-linear projection head. We then study if\ninstance-based contrastive learning (such as in SimCLR, MoCo, BYOL, and so on,\nwhich are based on global image representation) can learn well on images with\nmultiple objects present. We find that meaningful hierarchical local features\ncan be learned despite the fact that these objectives operate on global\ninstance-level features.\n\nFinally, we study an intriguing phenomenon of feature suppression among\ncompeting features shared across augmented views, such as \"color distribution\"\nvs \"object class\". We construct datasets with explicit and controllable\ncompeting features, and show that, for contrastive learning, a few bits of\neasy-to-learn shared features can suppress, and even fully prevent, the\nlearning of other sets of competing features. In scenarios where there are\nmultiple objects in an image, the dominant object would suppress the learning\nof smaller objects. Existing contrastive learning methods critically rely on\ndata augmentation to favor certain sets of features over others, and face\npotential limitation for scenarios where existing augmentations cannot fully\naddress the feature suppression. This poses open challenges to existing\ncontrastive learning techniques.",
          "link": "http://arxiv.org/abs/2011.02803",
          "publishedOn": "2021-06-22T01:57:13.590Z",
          "wordCount": 707,
          "title": "Intriguing Properties of Contrastive Losses. (arXiv:2011.02803v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Isaac Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1\">Guillermo Valle P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louis_A/0/1/0/all/0/1\">Ard Louis</a>",
          "description": "The intuition that local flatness of the loss landscape is correlated with\nbetter generalization for deep neural networks (DNNs) has been explored for\ndecades, spawning many different flatness measures. Recently, this link with\ngeneralization has been called into question by a demonstration that many\nmeasures of flatness are vulnerable to parameter re-scaling which arbitrarily\nchanges their value without changing neural network outputs.\n\nHere we show that, in addition, some popular variants of SGD such as Adam and\nEntropy-SGD, can also break the flatness-generalization correlation. As an\nalternative to flatness measures, we use a function based picture and propose\nusing the log of Bayesian prior upon initialization, $\\log P(f)$, as a\npredictor of the generalization when a DNN converges on function $f$ after\ntraining to zero error. The prior is directly proportional to the Bayesian\nposterior for functions that give zero error on a test set. For the case of\nimage classification, we show that $\\log P(f)$ is a significantly more robust\npredictor of generalization than flatness measures are.\n\nWhilst local flatness measures fail under parameter re-scaling, the\nprior/posterior, which is global quantity, remains invariant under re-scaling.\nMoreover, the correlation with generalization as a function of data complexity\nremains good for different variants of SGD.",
          "link": "http://arxiv.org/abs/2103.06219",
          "publishedOn": "2021-06-22T01:57:13.585Z",
          "wordCount": 678,
          "title": "Why flatness does and does not correlate with generalization for deep neural networks. (arXiv:2103.06219v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lockhart_B/0/1/0/all/0/1\">Brandon Lockhart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jinglin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weiyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1\">Eugene Wu</a>",
          "description": "Obtaining an explanation for an SQL query result can enrich the analysis\nexperience, reveal data errors, and provide deeper insight into the data.\nInference query explanation seeks to explain unexpected aggregate query results\non inference data; such queries are challenging to explain because an\nexplanation may need to be derived from the source, training, or inference data\nin an ML pipeline. In this paper, we model an objective function as a black-box\nfunction and propose BOExplain, a novel framework for explaining inference\nqueries using Bayesian optimization (BO). An explanation is a predicate\ndefining the input tuples that should be removed so that the query result of\ninterest is significantly affected. BO - a technique for finding the global\noptimum of a black-box function - is used to find the best predicate. We\ndevelop two new techniques (individual contribution encoding and warm start) to\nhandle categorical variables. We perform experiments showing that the\npredicates found by BOExplain have a higher degree of explanation compared to\nthose found by the state-of-the-art query explanation engines. We also show\nthat BOExplain is effective at deriving explanations for inference queries from\nsource and training data on a variety of real-world datasets. BOExplain is\nopen-sourced as a Python package at https://github.com/sfu-db/BOExplain.",
          "link": "http://arxiv.org/abs/2102.05308",
          "publishedOn": "2021-06-22T01:57:13.570Z",
          "wordCount": 665,
          "title": "Explaining Inference Queries with Bayesian Optimization. (arXiv:2102.05308v2 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01907",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kozodoi_N/0/1/0/all/0/1\">Nikita Kozodoi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jacob_J/0/1/0/all/0/1\">Johannes Jacob</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lessmann_S/0/1/0/all/0/1\">Stefan Lessmann</a>",
          "description": "The rise of algorithmic decision-making has spawned much research on fair\nmachine learning (ML). Financial institutions use ML for building risk\nscorecards that support a range of credit-related decisions. Yet, the\nliterature on fair ML in credit scoring is scarce. The paper makes three\ncontributions. First, we revisit statistical fairness criteria and examine\ntheir adequacy for credit scoring. Second, we catalog algorithmic options for\nincorporating fairness goals in the ML model development pipeline. Last, we\nempirically compare different fairness processors in a profit-oriented credit\nscoring context using real-world data. The empirical results substantiate the\nevaluation of fairness measures, identify suitable options to implement fair\ncredit scoring, and clarify the profit-fairness trade-off in lending decisions.\nWe find that multiple fairness criteria can be approximately satisfied at once\nand recommend separation as a proper criterion for measuring the fairness of a\nscorecard. We also find fair in-processors to deliver a good balance between\nprofit and fairness and show that algorithmic discrimination can be reduced to\na reasonable level at a relatively low cost. The codes corresponding to the\npaper are available on GitHub.",
          "link": "http://arxiv.org/abs/2103.01907",
          "publishedOn": "2021-06-22T01:57:13.564Z",
          "wordCount": 656,
          "title": "Fairness in Credit Scoring: Assessment, Implementation and Profit Implications. (arXiv:2103.01907v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qendro_L/0/1/0/all/0/1\">Lorena Qendro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Ting Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascolo_C/0/1/0/all/0/1\">Cecilia Mascolo</a>",
          "description": "Recently, sound-based COVID-19 detection studies have shown great promise to\nachieve scalable and prompt digital pre-screening. However, there are still two\nunsolved issues hindering the practice. First, collected datasets for model\ntraining are often imbalanced, with a considerably smaller proportion of users\ntested positive, making it harder to learn representative and robust features.\nSecond, deep learning models are generally overconfident in their predictions.\nClinically, false predictions aggravate healthcare costs. Estimation of the\nuncertainty of screening would aid this. To handle these issues, we propose an\nensemble framework where multiple deep learning models for sound-based COVID-19\ndetection are developed from different but balanced subsets from original data.\nAs such, data are utilized more effectively compared to traditional up-sampling\nand down-sampling approaches: an AUC of 0.74 with a sensitivity of 0.68 and a\nspecificity of 0.69 is achieved. Simultaneously, we estimate uncertainty from\nthe disagreement across multiple models. It is shown that false predictions\noften yield higher uncertainty, enabling us to suggest the users with certainty\nhigher than a threshold to repeat the audio test on their phones or to take\nclinical tests if digital diagnosis still fails. This study paves the way for a\nmore robust sound-based COVID-19 automated screening system.",
          "link": "http://arxiv.org/abs/2104.02005",
          "publishedOn": "2021-06-22T01:57:13.559Z",
          "wordCount": 716,
          "title": "Uncertainty-Aware COVID-19 Detection from Imbalanced Sound Data. (arXiv:2104.02005v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1\">Harshay Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Prateek Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1\">Praneeth Netrapalli</a>",
          "description": "Post-hoc gradient-based interpretability methods [Simonyan et al., 2013,\nSmilkov et al., 2017] that provide instance-specific explanations of model\npredictions are often based on assumption (A): magnitude of input gradients --\ngradients of logits with respect to input -- noisily highlight discriminative\ntask-relevant features. In this work, we test the validity of assumption (A)\nusing a three-pronged approach. First, we develop an evaluation framework,\nDiffROAR, to test assumption (A) on four image classification benchmarks. Our\nresults suggest that (i) input gradients of standard models (i.e., trained on\noriginal data) may grossly violate (A), whereas (ii) input gradients of\nadversarially robust models satisfy (A). Second, we then introduce BlockMNIST,\nan MNIST-based semi-real dataset, that by design encodes a priori knowledge of\ndiscriminative features. Our analysis on BlockMNIST leverages this information\nto validate as well as characterize differences between input gradient\nattributions of standard and robust models. Finally, we theoretically prove\nthat our empirical findings hold on a simplified version of the BlockMNIST\ndataset. Specifically, we prove that input gradients of standard\none-hidden-layer MLPs trained on this dataset do not highlight\ninstance-specific signal coordinates, thus grossly violating assumption (A).\nOur findings motivate the need to formalize and test common assumptions in\ninterpretability in a falsifiable manner [Leavitt and Morcos, 2020].\nAdditionally, we believe that the DiffROAR evaluation framework and\nBlockMNIST-based datasets can serve as sanity checks to audit instance-specific\ninterpretability methods.",
          "link": "http://arxiv.org/abs/2102.12781",
          "publishedOn": "2021-06-22T01:57:13.553Z",
          "wordCount": 700,
          "title": "Do Input Gradients Highlight Discriminative Features?. (arXiv:2102.12781v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1\">Krzysztof Choromanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1\">Deepali Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1\">Jack Parker-Holder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyou Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likhosherstov_V/0/1/0/all/0/1\">Valerii Likhosherstov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santara_A/0/1/0/all/0/1\">Anirban Santara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1\">Aldo Pacchiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yunhao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>",
          "description": "There has recently been significant interest in training reinforcement\nlearning (RL) agents in vision-based environments. This poses many challenges,\nsuch as high dimensionality and potential for observational overfitting through\nspurious correlations. A promising approach to solve both of these problems is\na self-attention bottleneck, which provides a simple and effective framework\nfor learning high performing policies, even in the presence of distractions.\nHowever, due to poor scalability of attention architectures, these methods do\nnot scale beyond low resolution visual inputs, using large patches (thus small\nattention matrices). In this paper we make use of new efficient attention\nalgorithms, recently shown to be highly effective for Transformers, and\ndemonstrate that these new techniques can be applied in the RL setting. This\nallows our attention-based controllers to scale to larger visual inputs, and\nfacilitate the use of smaller patches, even individual pixels, improving\ngeneralization. In addition, we propose a new efficient algorithm approximating\nsoftmax attention with what we call hybrid random features, leveraging the\ntheory of angular kernels. We show theoretically and empirically that hybrid\nrandom features is a promising approach when using attention for vision-based\nRL.",
          "link": "http://arxiv.org/abs/2102.04353",
          "publishedOn": "2021-06-22T01:57:13.547Z",
          "wordCount": 678,
          "title": "Unlocking Pixels for Reinforcement Learning via Implicit Attention. (arXiv:2102.04353v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bohan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1\">Suraj Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Martin-Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>",
          "description": "A video prediction model that generalizes to diverse scenes would enable\nintelligent agents such as robots to perform a variety of tasks via planning\nwith the model. However, while existing video prediction models have produced\npromising results on small datasets, they suffer from severe underfitting when\ntrained on large and diverse datasets. To address this underfitting challenge,\nwe first observe that the ability to train larger video prediction models is\noften bottlenecked by the memory constraints of GPUs or TPUs. In parallel, deep\nhierarchical latent variable models can produce higher quality predictions by\ncapturing the multi-level stochasticity of future observations, but end-to-end\noptimization of such models is notably difficult. Our key insight is that\ngreedy and modular optimization of hierarchical autoencoders can simultaneously\naddress both the memory constraints and the optimization challenges of\nlarge-scale video prediction. We introduce Greedy Hierarchical Variational\nAutoencoders (GHVAEs), a method that learns high-fidelity video predictions by\ngreedily training each level of a hierarchical autoencoder. In comparison to\nstate-of-the-art models, GHVAEs provide 17-55% gains in prediction performance\non four video datasets, a 35-40% higher success rate on real robot tasks, and\ncan improve performance monotonically by simply adding more modules.",
          "link": "http://arxiv.org/abs/2103.04174",
          "publishedOn": "2021-06-22T01:57:13.533Z",
          "wordCount": 691,
          "title": "Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction. (arXiv:2103.04174v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00351",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chung_M/0/1/0/all/0/1\">Moo K. Chung</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ombao_H/0/1/0/all/0/1\">Hernando Ombao</a>",
          "description": "Topological data analysis, including persistent homology, has undergone\nsignificant development in recent years. However, one outstanding challenge is\nto build a coherent statistical inference procedure on persistent diagrams. The\npaired dependent data structure, which are the births and deaths in persistent\ndiagrams, adds complexity to statistical inference. In this paper, we present a\nnew lattice path representation for persistent diagrams. A new exact\nstatistical inference procedure is developed for lattice paths via\ncombinatorial enumerations. The proposed lattice path method is applied to\nstudy the topological characterization of the protein structures of the\nCOVID-19 virus. We demonstrate that there are topological changes during the\nconformational change of spike proteins, a necessary step in infecting host\ncells.",
          "link": "http://arxiv.org/abs/2105.00351",
          "publishedOn": "2021-06-22T01:57:13.527Z",
          "wordCount": 629,
          "title": "Lattice Paths for Persistent Diagrams with Application to COVID-19 Virus Spike Proteins. (arXiv:2105.00351v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sidheekh_S/0/1/0/all/0/1\">Sahil Sidheekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aimen_A/0/1/0/all/0/1\">Aroof Aimen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">Narayanan C. Krishnan</a>",
          "description": "Despite the accomplishments of Generative Adversarial Networks (GANs) in\nmodeling data distributions, training them remains a challenging task. A\ncontributing factor to this difficulty is the non-intuitive nature of the GAN\nloss curves, which necessitates a subjective evaluation of the generated output\nto infer training progress. Recently, motivated by game theory, duality gap has\nbeen proposed as a domain agnostic measure to monitor GAN training. However, it\nis restricted to the setting when the GAN converges to a Nash equilibrium. But\nGANs need not always converge to a Nash equilibrium to model the data\ndistribution. In this work, we extend the notion of duality gap to proximal\nduality gap that is applicable to the general context of training GANs where\nNash equilibria may not exist. We show theoretically that the proximal duality\ngap is capable of monitoring the convergence of GANs to a wider spectrum of\nequilibria that subsumes Nash equilibria. We also theoretically establish the\nrelationship between the proximal duality gap and the divergence between the\nreal and generated data distributions for different GAN formulations. Our\nresults provide new insights into the nature of GAN convergence. Finally, we\nvalidate experimentally the usefulness of proximal duality gap for monitoring\nand influencing GAN training.",
          "link": "http://arxiv.org/abs/2105.04801",
          "publishedOn": "2021-06-22T01:57:13.521Z",
          "wordCount": 663,
          "title": "On Characterizing GAN Convergence Through Proximal Duality Gap. (arXiv:2105.04801v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05109",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Perrone_V/0/1/0/all/0/1\">Valerio Perrone</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Donini_M/0/1/0/all/0/1\">Michele Donini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zafar_M/0/1/0/all/0/1\">Muhammad Bilal Zafar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schmucker_R/0/1/0/all/0/1\">Robin Schmucker</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kenthapadi_K/0/1/0/all/0/1\">Krishnaram Kenthapadi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Archambeau_C/0/1/0/all/0/1\">C&#xe9;dric Archambeau</a>",
          "description": "Given the increasing importance of machine learning (ML) in our lives,\nseveral algorithmic fairness techniques have been proposed to mitigate biases\nin the outcomes of the ML models. However, most of these techniques are\nspecialized to cater to a single family of ML models and a specific definition\nof fairness, limiting their adaptibility in practice. We introduce a general\nconstrained Bayesian optimization (BO) framework to optimize the performance of\nany ML model while enforcing one or multiple fairness constraints. BO is a\nmodel-agnostic optimization method that has been successfully applied to\nautomatically tune the hyperparameters of ML models. We apply BO with fairness\nconstraints to a range of popular models, including random forests, gradient\nboosting, and neural networks, showing that we can obtain accurate and fair\nsolutions by acting solely on the hyperparameters. We also show empirically\nthat our approach is competitive with specialized techniques that enforce\nmodel-specific fairness constraints, and outperforms preprocessing methods that\nlearn fair representations of the input data. Moreover, our method can be used\nin synergy with such specialized fairness techniques to tune their\nhyperparameters. Finally, we study the relationship between fairness and the\nhyperparameters selected by BO. We observe a correlation between regularization\nand unbiased models, explaining why acting on the hyperparameters leads to ML\nmodels that generalize well and are fair.",
          "link": "http://arxiv.org/abs/2006.05109",
          "publishedOn": "2021-06-22T01:57:13.515Z",
          "wordCount": 672,
          "title": "Fair Bayesian Optimization. (arXiv:2006.05109v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morris_C/0/1/0/all/0/1\">Christopher Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fey_M/0/1/0/all/0/1\">Matthias Fey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kriege_N/0/1/0/all/0/1\">Nils M. Kriege</a>",
          "description": "In recent years, algorithms and neural architectures based on the\nWeisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism\nproblem, emerged as a powerful tool for (supervised) machine learning with\ngraphs and relational data. Here, we give a comprehensive overview of the\nalgorithm's use in a machine learning setting. We discuss the theoretical\nbackground, show how to use it for supervised graph- and node classification,\ndiscuss recent extensions, and its connection to neural architectures.\nMoreover, we give an overview of current applications and future directions to\nstimulate research.",
          "link": "http://arxiv.org/abs/2105.05911",
          "publishedOn": "2021-06-22T01:57:13.509Z",
          "wordCount": 575,
          "title": "The Power of the Weisfeiler-Leman Algorithm for Machine Learning with Graphs. (arXiv:2105.05911v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramnath_K/0/1/0/all/0/1\">Kiran Ramnath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1\">Mark Hasegawa-Johnson</a>",
          "description": "Fact-based Visual Question Answering (FVQA), a challenging variant of VQA,\nrequires a QA-system to include facts from a diverse knowledge graph (KG) in\nits reasoning process to produce an answer. Large KGs, especially common-sense\nKGs, are known to be incomplete, i.e., not all non-existent facts are always\nincorrect. Therefore, being able to reason over incomplete KGs for QA is a\ncritical requirement in real-world applications that has not been addressed\nextensively in the literature. We develop a novel QA architecture that allows\nus to reason over incomplete KGs, something current FVQA state-of-the-art\n(SOTA) approaches lack due to their critical reliance on fact retrieval. We use\nKG Embeddings, a technique widely used for KG completion, for the downstream\ntask of FVQA. We also employ a new image representation technique we call\n'Image-as-Knowledge' to enable this capability, alongside a simple one-step\nCoAttention mechanism to attend to text and image during QA. Our FVQA\narchitecture is faster during inference time, being O(m), as opposed to\nexisting FVQA SOTA methods which are O(N log N), where m = number of vertices,\nN = number of edges = O(m^2). KG embeddings are shown to hold complementary\ninformation to word embeddings: a combination of both metrics permits\nperformance comparable to SOTA methods in the standard answer retrieval task,\nand significantly better (26% absolute) in the proposed missing-edge reasoning\ntask.",
          "link": "http://arxiv.org/abs/2012.15484",
          "publishedOn": "2021-06-22T01:57:13.494Z",
          "wordCount": null,
          "title": "Seeing is Knowing! Fact-based Visual Question Answering using Knowledge Graph Embeddings. (arXiv:2012.15484v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1\">Younggyo Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lili Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kimin Lee</a>",
          "description": "Recent exploration methods have proven to be a recipe for improving\nsample-efficiency in deep reinforcement learning (RL). However, efficient\nexploration in high-dimensional observation spaces still remains a challenge.\nThis paper presents Random Encoders for Efficient Exploration (RE3), an\nexploration method that utilizes state entropy as an intrinsic reward. In order\nto estimate state entropy in environments with high-dimensional observations,\nwe utilize a k-nearest neighbor entropy estimator in the low-dimensional\nrepresentation space of a convolutional encoder. In particular, we find that\nthe state entropy can be estimated in a stable and compute-efficient manner by\nutilizing a randomly initialized encoder, which is fixed throughout training.\nOur experiments show that RE3 significantly improves the sample-efficiency of\nboth model-free and model-based RL methods on locomotion and navigation tasks\nfrom DeepMind Control Suite and MiniGrid benchmarks. We also show that RE3\nallows learning diverse behaviors without extrinsic rewards, effectively\nimproving sample-efficiency in downstream tasks. Source code and videos are\navailable at https://sites.google.com/view/re3-rl.",
          "link": "http://arxiv.org/abs/2102.09430",
          "publishedOn": "2021-06-22T01:57:13.494Z",
          "wordCount": null,
          "title": "State Entropy Maximization with Random Encoders for Efficient Exploration. (arXiv:2102.09430v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00065",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1\">Jeremy M. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_S/0/1/0/all/0/1\">Simran Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1\">J. Zico Kolter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>",
          "description": "We empirically demonstrate that full-batch gradient descent on neural network\ntraining objectives typically operates in a regime we call the Edge of\nStability. In this regime, the maximum eigenvalue of the training loss Hessian\nhovers just above the numerical value $2 / \\text{(step size)}$, and the\ntraining loss behaves non-monotonically over short timescales, yet consistently\ndecreases over long timescales. Since this behavior is inconsistent with\nseveral widespread presumptions in the field of optimization, our findings\nraise questions as to whether these presumptions are relevant to neural network\ntraining. We hope that our findings will inspire future efforts aimed at\nrigorously understanding optimization at the Edge of Stability. Code is\navailable at https://github.com/locuslab/edge-of-stability.",
          "link": "http://arxiv.org/abs/2103.00065",
          "publishedOn": "2021-06-22T01:57:13.494Z",
          "wordCount": null,
          "title": "Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability. (arXiv:2103.00065v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stammer_W/0/1/0/all/0/1\">Wolfgang Stammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>",
          "description": "Most explanation methods in deep learning map importance estimates for a\nmodel's prediction back to the original input space. These \"visual\"\nexplanations are often insufficient, as the model's actual concept remains\nelusive. Moreover, without insights into the model's semantic concept, it is\ndifficult -- if not impossible -- to intervene on the model's behavior via its\nexplanations, called Explanatory Interactive Learning. Consequently, we propose\nto intervene on a Neuro-Symbolic scene representation, which allows one to\nrevise the model on the semantic level, e.g. \"never focus on the color to make\nyour decision\". We compiled a novel confounded visual scene data set, the\nCLEVR-Hans data set, capturing complex compositions of different objects. The\nresults of our experiments on CLEVR-Hans demonstrate that our semantic\nexplanations, i.e. compositional explanations at a per-object level, can\nidentify confounders that are not identifiable using \"visual\" explanations\nonly. More importantly, feedback on this semantic level makes it possible to\nrevise the model from focusing on these factors.",
          "link": "http://arxiv.org/abs/2011.12854",
          "publishedOn": "2021-06-22T01:57:13.493Z",
          "wordCount": null,
          "title": "Right for the Right Concept: Revising Neuro-Symbolic Concepts by Interacting with their Explanations. (arXiv:2011.12854v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banihashem_K/0/1/0/all/0/1\">Kiarash Banihashem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1\">Adish Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radanovic_G/0/1/0/all/0/1\">Goran Radanovic</a>",
          "description": "We study defense strategies against reward poisoning attacks in reinforcement\nlearning. As a threat model, we consider attacks that minimally alter rewards\nto make the attacker's target policy uniquely optimal under the poisoned\nrewards, with the optimality gap specified by an attack parameter. Our goal is\nto design agents that are robust against such attacks in terms of the\nworst-case utility w.r.t. the true, unpoisoned, rewards while computing their\npolicies under the poisoned rewards. We propose an optimization framework for\nderiving optimal defense policies, both when the attack parameter is known and\nunknown. Moreover, we show that defense policies that are solutions to the\nproposed optimization problems have provable performance guarantees. In\nparticular, we provide the following bounds with respect to the true,\nunpoisoned, rewards: a) lower bounds on the expected return of the defense\npolicies, and b) upper bounds on how suboptimal these defense policies are\ncompared to the attacker's target policy. We conclude the paper by illustrating\nthe intuitions behind our formal results, and showing that the derived bounds\nare non-trivial.",
          "link": "http://arxiv.org/abs/2102.05776",
          "publishedOn": "2021-06-22T01:57:13.490Z",
          "wordCount": 632,
          "title": "Defense Against Reward Poisoning Attacks in Reinforcement Learning. (arXiv:2102.05776v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>",
          "description": "Assessing an AI agent that can converse in human language and understand\nvisual content is challenging. Generation metrics, such as BLEU scores favor\ncorrect syntax over semantics. Hence a discriminative approach is often used,\nwhere an agent ranks a set of candidate options. The mean reciprocal rank (MRR)\nmetric evaluates the model performance by taking into account the rank of a\nsingle human-derived answer. This approach, however, raises a new challenge:\nthe ambiguity and synonymy of answers, for instance, semantic equivalence\n(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative\ngain (NDCG) metric has been used to capture the relevance of all the correct\nanswers via dense annotations. However, the NDCG metric favors the usually\napplicable uncertain answers such as `I don't know. Crafting a model that\nexcels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should\nanswer a human-like reply and validate the correctness of any answer. To\naddress this issue, we describe a two-step non-parametric ranking approach that\ncan merge strong MRR and NDCG models. Using our approach, we manage to keep\nmost MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG\nstate-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won\nthe recent Visual Dialog 2020 challenge. Source code is available at\nhttps://github.com/idansc/mrr-ndcg.",
          "link": "http://arxiv.org/abs/2104.07511",
          "publishedOn": "2021-06-22T01:57:13.485Z",
          "wordCount": 688,
          "title": "Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ilhan_E/0/1/0/all/0/1\">Ercument Ilhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gow_J/0/1/0/all/0/1\">Jeremy Gow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Liebana_D/0/1/0/all/0/1\">Diego Perez-Liebana</a>",
          "description": "Deep Reinforcement Learning (RL) techniques can benefit greatly from\nleveraging prior experience, which can be either self-generated or acquired\nfrom other entities. Action advising is a framework that provides a flexible\nway to transfer such knowledge in the form of actions between teacher-student\npeers. However, due to the realistic concerns, the number of these interactions\nis limited with a budget; therefore, it is crucial to perform these in the most\nappropriate moments. There have been several promising studies recently that\naddress this problem setting especially from the student's perspective. Despite\ntheir success, they have some shortcomings when it comes to the practical\napplicability and integrity as an overall solution to the learning from advice\nchallenge. In this paper, we extend the idea of advice reusing via teacher\nimitation to construct a unified approach that addresses both advice collection\nand advice utilisation problems. We also propose a method to automatically tune\nthe relevant hyperparameters of these components on-the-fly to make it able to\nadapt to any task with minimal human intervention. The experiments we performed\nin $5$ different Atari games verify that our algorithm either surpasses or\nperforms on-par with its top competitors while being far simpler to be\nemployed. Furthermore, its individual components are also found to be providing\nsignificant advantages alone.",
          "link": "http://arxiv.org/abs/2104.08440",
          "publishedOn": "2021-06-22T01:57:13.479Z",
          "wordCount": 669,
          "title": "Learning on a Budget via Teacher Imitation. (arXiv:2104.08440v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03287",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neely_M/0/1/0/all/0/1\">Michael Neely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schouten_S/0/1/0/all/0/1\">Stefan F. Schouten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bleeker_M/0/1/0/all/0/1\">Maurits J. R. Bleeker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_A/0/1/0/all/0/1\">Ana Lucic</a>",
          "description": "By computing the rank correlation between attention weights and\nfeature-additive explanation methods, previous analyses either invalidate or\nsupport the role of attention-based explanations as a faithful and plausible\nmeasure of salience. To investigate whether this approach is appropriate, we\ncompare LIME, Integrated Gradients, DeepLIFT, Grad-SHAP, Deep-SHAP, and\nattention-based explanations, applied to two neural architectures trained on\nsingle- and pair-sequence language tasks. In most cases, we find that none of\nour chosen methods agree. Based on our empirical observations and theoretical\nobjections, we conclude that rank correlation does not measure the quality of\nfeature-additive methods. Practitioners should instead use the numerous and\nrigorous diagnostic methods proposed by the community.",
          "link": "http://arxiv.org/abs/2105.03287",
          "publishedOn": "2021-06-22T01:57:13.472Z",
          "wordCount": 595,
          "title": "Order in the Court: Explainable AI Methods Prone to Disagreement. (arXiv:2105.03287v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1\">Igor L. Markov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jacqueline Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vagner_A/0/1/0/all/0/1\">Adam Vagner</a>",
          "description": "Text classifiers are at the core of many NLP applications and use a variety\nof algorithmic approaches and software. This paper introduces infrastructure\nand methodologies for text classifiers based on large-scale regular\nexpressions. In particular, we describe how Facebook determines if a given\npiece of text - anything from a hashtag to a post - belongs to a narrow topic\nsuch as COVID-19. To fully define a topic and evaluate classifier performance\nwe employ human-guided iterations of keyword discovery, but do not require\nlabeled data. For COVID-19, we build two sets of regular expressions: (1) for\n66 languages, with 99% precision and recall >50%, (2) for the 11 most common\nlanguages, with precision >90% and recall >90%. Regular expressions enable\nlow-latency queries from multiple platforms. Response to challenges like\nCOVID-19 is fast and so are revisions. Comparisons to a DNN classifier show\nexplainable results, higher precision and recall, and less overfitting. Our\nlearnings can be applied to other narrow-topic classifiers.",
          "link": "http://arxiv.org/abs/2102.09507",
          "publishedOn": "2021-06-22T01:57:13.467Z",
          "wordCount": 694,
          "title": "Regular Expressions for Fast-response COVID-19 Text Classification. (arXiv:2102.09507v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1\">Marios Savvides</a>",
          "description": "Previous studies dominantly target at self-supervised learning on real-valued\nnetworks and have achieved many promising results. However, on the more\nchallenging binary neural networks (BNNs), this task has not yet been fully\nexplored in the community. In this paper, we focus on this more difficult\nscenario: learning networks where both weights and activations are binary,\nmeanwhile, without any human annotated labels. We observe that the commonly\nused contrastive objective is not satisfying on BNNs for competitive accuracy,\nsince the backbone network contains relatively limited capacity and\nrepresentation ability. Hence instead of directly applying existing\nself-supervised methods, which cause a severe decline in performance, we\npresent a novel guided learning paradigm from real-valued to distill binary\nnetworks on the final prediction distribution, to minimize the loss and obtain\ndesirable accuracy. Our proposed method can boost the simple contrastive\nlearning baseline by an absolute gain of 5.5~15% on BNNs. We further reveal\nthat it is difficult for BNNs to recover the similar predictive distributions\nas real-valued models when training without labels. Thus, how to calibrate them\nis key to address the degradation in performance. Extensive experiments are\nconducted on the large-scale ImageNet and downstream datasets. Our method\nachieves substantial improvement over the simple contrastive learning baseline,\nand is even comparable to many mainstream supervised BNN methods. Code is\navailable at https://github.com/szq0214/S2-BNN.",
          "link": "http://arxiv.org/abs/2102.08946",
          "publishedOn": "2021-06-22T01:57:13.449Z",
          "wordCount": 738,
          "title": "S2-BNN: Bridging the Gap Between Self-Supervised Real and 1-bit Neural Networks via Guided Distribution Calibration. (arXiv:2102.08946v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10586",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaidi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldhahn_R/0/1/0/all/0/1\">Ryan Goldhahn</a>",
          "description": "To tackle the susceptibility of deep neural networks to examples, the\nadversarial training has been proposed which provides a notion of robust\nthrough an inner maximization problem presenting the first-order embedded\nwithin the outer minimization of the training loss. To generalize the\nadversarial robustness over different perturbation types, the adversarial\ntraining method has been augmented with the improved inner maximization\npresenting a union of multiple perturbations e.g., various $\\ell_p$\nnorm-bounded perturbations.",
          "link": "http://arxiv.org/abs/2104.10586",
          "publishedOn": "2021-06-22T01:57:13.437Z",
          "wordCount": 572,
          "title": "Mixture of Robust Experts (MoRE). (arXiv:2104.10586v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_A/0/1/0/all/0/1\">Adrian Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fanconi_C/0/1/0/all/0/1\">Claudio Fanconi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rade_R/0/1/0/all/0/1\">Rahul Rade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1\">Jonas Kohler</a>",
          "description": "Deep neural networks that yield human interpretable decisions by\narchitectural design have lately become an increasingly popular alternative to\npost hoc interpretation of traditional black-box models. Among these networks,\nthe arguably most widespread approach is so-called prototype learning, where\nsimilarities to learned latent prototypes serve as the basis of classifying an\nunseen data point. In this work, we point to an important shortcoming of such\napproaches. Namely, there is a semantic gap between similarity in latent space\nand similarity in input space, which can corrupt interpretability. We design\ntwo experiments that exemplify this issue on the so-called ProtoPNet.\nSpecifically, we find that this network's interpretability mechanism can be led\nastray by intentionally crafted or even JPEG compression artefacts, which can\nproduce incomprehensible decisions. We argue that practitioners ought to have\nthis shortcoming in mind when deploying prototype-based models in practice.",
          "link": "http://arxiv.org/abs/2105.02968",
          "publishedOn": "2021-06-22T01:57:13.431Z",
          "wordCount": 648,
          "title": "This Looks Like That... Does it? Shortcomings of Latent Space Prototype Interpretability in Deep Networks. (arXiv:2105.02968v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pinyan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chao Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaojin Zhang</a>",
          "description": "We study the problem of identifying the best arm in a stochastic multi-armed\nbandit game. Given a set of $n$ arms indexed from $1$ to $n$, each arm $i$ is\nassociated with an unknown reward distribution supported on $[0,1]$ with mean\n$\\theta_i$ and variance $\\sigma_i^2$. Assume $\\theta_1 > \\theta_2 \\geq \\cdots\n\\geq\\theta_n$. We propose an adaptive algorithm which explores the gaps and\nvariances of the rewards of the arms and makes future decisions based on the\ngathered information using a novel approach called \\textit{grouped median\nelimination}. The proposed algorithm guarantees to output the best arm with\nprobability $(1-\\delta)$ and uses at most $O \\left(\\sum_{i = 1}^n\n\\left(\\frac{\\sigma_i^2}{\\Delta_i^2} + \\frac{1}{\\Delta_i}\\right)(\\ln \\delta^{-1}\n+ \\ln \\ln \\Delta_i^{-1})\\right)$ samples, where $\\Delta_i$ ($i \\geq 2$) denotes\nthe reward gap between arm $i$ and the best arm and we define $\\Delta_1 =\n\\Delta_2$. This achieves a significant advantage over the variance-independent\nalgorithms in some favorable scenarios and is the first result that removes the\nextra $\\ln n$ factor on the best arm compared with the state-of-the-art. We\nfurther show that $\\Omega \\left( \\sum_{i = 1}^n \\left(\n\\frac{\\sigma_i^2}{\\Delta_i^2} + \\frac{1}{\\Delta_i} \\right) \\ln \\delta^{-1}\n\\right)$ samples are necessary for an algorithm to achieve the same goal,\nthereby illustrating that our algorithm is optimal up to doubly logarithmic\nterms.",
          "link": "http://arxiv.org/abs/2106.10417",
          "publishedOn": "2021-06-22T01:57:13.419Z",
          "wordCount": 635,
          "title": "Variance-Dependent Best Arm Identification. (arXiv:2106.10417v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.14785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dahlin_N/0/1/0/all/0/1\">Nathan Dahlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalagarla_K/0/1/0/all/0/1\">Krishna Chaitanya Kalagarla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_N/0/1/0/all/0/1\">Nikhil Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rahul Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nuzzo_P/0/1/0/all/0/1\">Pierluigi Nuzzo</a>",
          "description": "In an ever expanding set of research and application areas, deep neural\nnetworks (DNNs) set the bar for algorithm performance. However, depending upon\nadditional constraints such as processing power and execution time limits, or\nrequirements such as verifiable safety guarantees, it may not be feasible to\nactually use such high-performing DNNs in practice. Many techniques have been\ndeveloped in recent years to compress or distill complex DNNs into smaller,\nfaster or more understandable models and controllers. This work seeks to\nidentify reduced models that not only preserve a desired performance level, but\nalso, for example, succinctly explain the latent knowledge represented by a\nDNN. We illustrate the effectiveness of the proposed approach on the evaluation\nof decision tree variants and kernel machines in the context of benchmark\nreinforcement learning tasks.",
          "link": "http://arxiv.org/abs/2010.14785",
          "publishedOn": "2021-06-22T01:57:13.404Z",
          "wordCount": 594,
          "title": "Designing Interpretable Approximations to Deep Reinforcement Learning. (arXiv:2010.14785v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruiwen Li</a> (co-first author), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhibo Zhang</a> (co-first author), <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiani Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1\">Scott Sanner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Jongseong Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yeonjeong Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_D/0/1/0/all/0/1\">Dongsub Shim</a>",
          "description": "Recent years have seen the introduction of a range of methods for post-hoc\nexplainability of image classifier predictions. However, these post-hoc\nexplanations may not always align perfectly with classifier predictions, which\nposes a significant challenge when attempting to debug models based on such\nexplanations. To this end, we seek a methodology that can improve alignment\nbetween model predictions and explanation method that is both agnostic to the\nmodel and explanation classes and which does not require ground truth\nexplanations. We achieve this through a novel explanation-driven data\naugmentation (EDDA) method that augments the training data with occlusions of\nexisting data stemming from model-explanations; this is based on the simple\nmotivating principle that occluding salient regions for the model prediction\nshould decrease the model confidence in the prediction, while occluding\nnon-salient regions should not change the prediction -- if the model and\nexplainer are aligned. To verify that this augmentation method improves model\nand explainer alignment, we evaluate the methodology on a variety of datasets,\nimage classification models, and explanation methods. We verify in all cases\nthat our explanation-driven data augmentation method improves alignment of the\nmodel and explanation in comparison to no data augmentation and non-explanation\ndriven data augmentation methods. In conclusion, this approach provides a novel\nmodel- and explainer-agnostic methodology for improving alignment between model\npredictions and explanations, which we see as a critical step forward for\npractical deployment and debugging of image classification models.",
          "link": "http://arxiv.org/abs/2105.14162",
          "publishedOn": "2021-06-22T01:57:13.398Z",
          "wordCount": 707,
          "title": "EDDA: Explanation-driven Data Augmentation to Improve Model and Explanation Alignment. (arXiv:2105.14162v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grimm_C/0/1/0/all/0/1\">Christopher Grimm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barreto_A/0/1/0/all/0/1\">Andr&#xe9; Barreto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farquhar_G/0/1/0/all/0/1\">Gregory Farquhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silver_D/0/1/0/all/0/1\">David Silver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satinder Singh</a>",
          "description": "One of the main challenges in model-based reinforcement learning (RL) is to\ndecide which aspects of the environment should be modeled. The\nvalue-equivalence (VE) principle proposes a simple answer to this question: a\nmodel should capture the aspects of the environment that are relevant for\nvalue-based planning. Technically, VE distinguishes models based on a set of\npolicies and a set of functions: a model is said to be VE to the environment if\nthe Bellman operators it induces for the policies yield the correct result when\napplied to the functions. As the number of policies and functions increase, the\nset of VE models shrinks, eventually collapsing to a single point corresponding\nto a perfect model. A fundamental question underlying the VE principle is thus\nhow to select the smallest sets of policies and functions that are sufficient\nfor planning. In this paper we take an important step towards answering this\nquestion. We start by generalizing the concept of VE to order-$k$ counterparts\ndefined with respect to $k$ applications of the Bellman operator. This leads to\na family of VE classes that increase in size as $k \\rightarrow \\infty$. In the\nlimit, all functions become value functions, and we have a special\ninstantiation of VE which we call proper VE or simply PVE. Unlike VE, the PVE\nclass may contain multiple models even in the limit when all value functions\nare used. Crucially, all these models are sufficient for planning, meaning that\nthey will yield an optimal policy despite the fact that they may ignore many\naspects of the environment. We construct a loss function for learning PVE\nmodels and argue that popular algorithms such as MuZero and Muesli can be\nunderstood as minimizing an upper bound for this loss. We leverage this\nconnection to propose a modification to MuZero and show that it can lead to\nimproved performance in practice.",
          "link": "http://arxiv.org/abs/2106.10316",
          "publishedOn": "2021-06-22T01:57:13.392Z",
          "wordCount": 735,
          "title": "Proper Value Equivalence. (arXiv:2106.10316v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yulun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choma_N/0/1/0/all/0/1\">Nicholas Choma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Andrew Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cashman_M/0/1/0/all/0/1\">Mikaela Cashman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prates_E/0/1/0/all/0/1\">&#xc9;rica T. Prates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Manesh Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vergara_V/0/1/0/all/0/1\">Ver&#xf3;nica G. Melesse Vergara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clyde_A/0/1/0/all/0/1\">Austin Clyde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brettin_T/0/1/0/all/0/1\">Thomas S. Brettin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jong_W/0/1/0/all/0/1\">Wibe A. de Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Neeraj Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Head_M/0/1/0/all/0/1\">Martha S. Head</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_R/0/1/0/all/0/1\">Rick L. Stevens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nugent_P/0/1/0/all/0/1\">Peter Nugent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobson_D/0/1/0/all/0/1\">Daniel A. Jacobson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_J/0/1/0/all/0/1\">James B. Brown</a>",
          "description": "We developed Distilled Graph Attention Policy Networks (DGAPNs), a\ncuriosity-driven reinforcement learning model to generate novel\ngraph-structured chemical representations that optimize user-defined objectives\nby efficiently navigating a physically constrained domain. The framework is\nexamined on the task of generating molecules that are designed to bind,\nnoncovalently, to functional sites of SARS-CoV-2 proteins. We present a spatial\nGraph Attention Network (sGAT) that leverages self-attention over both node and\nedge attributes as well as encoding spatial structure -- this capability is of\nconsiderable interest in areas such as molecular and synthetic biology and drug\ndiscovery. An attentional policy network is then introduced to learn decision\nrules for a dynamic, fragment-based chemical environment, and state-of-the-art\npolicy gradient techniques are employed to train the network with enhanced\nstability. Exploration is efficiently encouraged by incorporating innovation\nreward bonuses learned and proposed by random network distillation. In\nexperiments, our framework achieved outstanding results compared to\nstate-of-the-art algorithms, while increasing the diversity of proposed\nmolecules and reducing the complexity of paths to chemical synthesis.",
          "link": "http://arxiv.org/abs/2106.02190",
          "publishedOn": "2021-06-22T01:57:13.386Z",
          "wordCount": 714,
          "title": "Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery. (arXiv:2106.02190v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1\">Parth Patwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_V/0/1/0/all/0/1\">Viswanatha Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukumaran_R/0/1/0/all/0/1\">Rohan Sukumaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+TV_S/0/1/0/all/0/1\">Sethuraman TV</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nashnoush_E/0/1/0/all/0/1\">Eptehal Nashnoush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_S/0/1/0/all/0/1\">Sheshank Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1\">Rishemjit Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Abhishek Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1\">Ramesh Raskar</a>",
          "description": "The COVID-19 pandemic has impacted lives and economies across the globe,\nleading to many deaths. While vaccination is an important intervention, its\nroll-out is slow and unequal across the globe. Therefore, extensive testing\nstill remains one of the key methods to monitor and contain the virus. Testing\non a large scale is expensive and arduous. Hence, we need alternate methods to\nestimate the number of cases. Online surveys have been shown to be an effective\nmethod for data collection amidst the pandemic. In this work, we develop\nmachine learning models to estimate the prevalence of COVID-19 using\nself-reported symptoms. Our best model predicts the daily cases with a mean\nabsolute error (MAE) of 226.30 (normalized MAE of 27.09%) per state, which\ndemonstrates the possibility of predicting the actual number of confirmed cases\nby utilizing self-reported symptoms. The models are developed at two levels of\ndata granularity - local models, which are trained at the state level, and a\nsingle global model which is trained on the combined data aggregated across all\nstates. Our results indicate a lower error on the local models as opposed to\nthe global model. In addition, we also show that the most important symptoms\n(features) vary considerably from state to state. This work demonstrates that\nthe models developed on crowd-sourced data, curated via online platforms, can\ncomplement the existing epidemiological surveillance infrastructure in a\ncost-effective manner. The code is publicly available at\nhttps://github.com/parthpatwa/Can-Self-Reported-Symptoms-Predict-Daily-COVID-19-Cases.",
          "link": "http://arxiv.org/abs/2105.08321",
          "publishedOn": "2021-06-22T01:57:13.372Z",
          "wordCount": 775,
          "title": "Can Self Reported Symptoms Predict Daily COVID-19 Cases?. (arXiv:2105.08321v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_R/0/1/0/all/0/1\">Runzhe Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chengchun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shikai Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>",
          "description": "Order dispatch is one of the central problems to ride-sharing platforms.\nRecently, value-based reinforcement learning algorithms have shown promising\nperformance on this problem. However, in real-world applications, the\nnon-stationarity of the demand-supply system poses challenges to re-utilizing\ndata generated in different time periods to learn the value function. In this\nwork, motivated by the fact that the relative relationship between the values\nof some states is largely stable across various environments, we propose a\npattern transfer learning framework for value-based reinforcement learning in\nthe order dispatch problem. Our method efficiently captures the value patterns\nby incorporating a concordance penalty. The superior performance of the\nproposed method is supported by experiments.",
          "link": "http://arxiv.org/abs/2105.13218",
          "publishedOn": "2021-06-22T01:57:13.366Z",
          "wordCount": 577,
          "title": "Pattern Transfer Learning for Reinforcement Learning in Order Dispatching. (arXiv:2105.13218v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziniu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>",
          "description": "This paper is dedicated to designing provably efficient adversarial imitation\nlearning (AIL) algorithms that directly optimize policies from expert\ndemonstrations. Firstly, we develop a transition-aware AIL algorithm named TAIL\nwith an expert sample complexity of $\\tilde{O}(H^{3/2} |S|/\\varepsilon)$ under\nthe known transition setting, where $H$ is the planning horizon, $|S|$ is the\nstate space size and $\\varepsilon$ is desired policy value gap. This improves\nupon the previous best bound of $\\tilde{O}(H^2 |S| / \\varepsilon^2)$ for AIL\nmethods and matches the lower bound of $\\tilde{\\Omega} (H^{3/2}\n|S|/\\varepsilon)$ in [Rajaraman et al., 2021] up to a logarithmic factor. The\nkey ingredient of TAIL is a fine-grained estimator for expert state-action\ndistribution, which explicitly utilizes the transition function information.\nSecondly, considering practical settings where the transition functions are\nusually unknown but environment interaction is allowed, we accordingly develop\na model-based transition-aware AIL algorithm named MB-TAIL. In particular,\nMB-TAIL builds an empirical transition model by interacting with the\nenvironment and performs imitation under the recovered empirical model. The\ninteraction complexity of MB-TAIL is $\\tilde{O} (H^3 |S|^2 |A| /\n\\varepsilon^2)$, which improves the best known result of $\\tilde{O} (H^4 |S|^2\n|A| / \\varepsilon^2)$ in [Shani et al., 2021]. Finally, our theoretical results\nare supported by numerical evaluation and detailed analysis on two challenging\nMDPs.",
          "link": "http://arxiv.org/abs/2106.10424",
          "publishedOn": "2021-06-22T01:57:13.360Z",
          "wordCount": 645,
          "title": "Nearly Minimax Optimal Adversarial Imitation Learning with Known and Unknown Transitions. (arXiv:2106.10424v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junda Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xupin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "While the long-term effects of COVID-19 are yet to be determined, its\nimmediate impact on crowdfunding is nonetheless significant. This study takes a\ncomputational approach to more deeply comprehend this change. Using a unique\ndata set of all the campaigns published over the past two years on GoFundMe, we\nexplore the factors that have led to the successful funding of a crowdfunding\nproject. In particular, we study a corpus of crowdfunded projects, analyzing\ncover images and other variables commonly present on crowdfunding sites.\nFurthermore, we construct a classifier and a regression model to assess the\nsignificance of features based on XGBoost. In addition, we employ\ncounterfactual analysis to investigate the causality between features and the\nsuccess of crowdfunding. More importantly, sentiment analysis and the paired\nsample t-test are performed to examine the differences in crowdfunding\ncampaigns before and after the COVID-19 outbreak that started in March 2020.\nFirst, we note that there is significant racial disparity in crowdfunding\nsuccess. Second, we find that sad emotion expressed through the campaign's\ndescription became significant after the COVID-19 outbreak. Considering all\nthese factors, our findings shed light on the impact of COVID-19 on\ncrowdfunding campaigns.",
          "link": "http://arxiv.org/abs/2106.09981",
          "publishedOn": "2021-06-22T01:57:13.355Z",
          "wordCount": 682,
          "title": "How COVID-19 Have Changed Crowdfunding: Evidence From GoFundMe. (arXiv:2106.09981v1 [cs.CY] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03156",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1\">Sokbae Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liao_Y/0/1/0/all/0/1\">Yuan Liao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Seo_M/0/1/0/all/0/1\">Myung Hwan Seo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shin_Y/0/1/0/all/0/1\">Youngki Shin</a>",
          "description": "We develop a new method of online inference for a vector of parameters\nestimated by the Polyak-Ruppert averaging procedure of stochastic gradient\ndescent (SGD) algorithms. We leverage insights from time series regression in\neconometrics and construct asymptotically pivotal statistics via random\nscaling. Our approach is fully operational with online data and is rigorously\nunderpinned by a functional central limit theorem. Our proposed inference\nmethod has a couple of key advantages over the existing methods. First, the\ntest statistic is computed in an online fashion with only SGD iterates and the\ncritical values can be obtained without any resampling methods, thereby\nallowing for efficient implementation suitable for massive online data. Second,\nthere is no need to estimate the asymptotic variance and our inference method\nis shown to be robust to changes in the tuning parameters for SGD algorithms in\nsimulation experiments with synthetic data.",
          "link": "http://arxiv.org/abs/2106.03156",
          "publishedOn": "2021-06-22T01:57:13.349Z",
          "wordCount": 629,
          "title": "Fast and Robust Online Inference with Stochastic Gradient Descent via Random Scaling. (arXiv:2106.03156v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sukumaran_R/0/1/0/all/0/1\">Rohan Sukumaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1\">Parth Patwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethuraman_T/0/1/0/all/0/1\">T V Sethuraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_S/0/1/0/all/0/1\">Sheshank Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanaparti_R/0/1/0/all/0/1\">Rishank Kanaparti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1\">Joseph Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_Y/0/1/0/all/0/1\">Yash Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Abhishek Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1\">Ayush Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Myungsun Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramaswamy_P/0/1/0/all/0/1\">Priya Ramaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1\">Ramesh Raskar</a>",
          "description": "It is crucial for policymakers to understand the community prevalence of\nCOVID-19 so combative resources can be effectively allocated and prioritized\nduring the COVID-19 pandemic. Traditionally, community prevalence has been\nassessed through diagnostic and antibody testing data. However, despite the\nincreasing availability of COVID-19 testing, the required level has not been\nmet in most parts of the globe, introducing a need for an alternative method\nfor communities to determine disease prevalence. This is further complicated by\nthe observation that COVID-19 prevalence and spread varies across different\nspatial, temporal, and demographics. In this study, we understand trends in the\nspread of COVID-19 by utilizing the results of self-reported COVID-19 symptoms\nsurveys as an alternative to COVID-19 testing reports. This allows us to assess\ncommunity disease prevalence, even in areas with low COVID-19 testing ability.\nUsing individually reported symptom data from various populations, our method\npredicts the likely percentage of the population that tested positive for\nCOVID-19. We do so with a Mean Absolute Error (MAE) of 1.14 and Mean Relative\nError (MRE) of 60.40\\% with 95\\% confidence interval as (60.12, 60.67). This\nimplies that our model predicts +/- 1140 cases than the original in a\npopulation of 1 million. In addition, we forecast the location-wise percentage\nof the population testing positive for the next 30 days using self-reported\nsymptoms data from previous days. The MAE for this method is as low as 0.15\n(MRE of 23.61\\% with 95\\% confidence interval as (23.6, 13.7)) for New York. We\npresent an analysis of these results, exposing various clinical attributes of\ninterest across different demographics. Lastly, we qualitatively analyze how\nvarious policy enactments (testing, curfew) affect the prevalence of COVID-19\nin a community.",
          "link": "http://arxiv.org/abs/2101.10266",
          "publishedOn": "2021-06-22T01:57:13.344Z",
          "wordCount": 823,
          "title": "COVID-19 Outbreak Prediction and Analysis using Self Reported Symptoms. (arXiv:2101.10266v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mysore_S/0/1/0/all/0/1\">Siddharth Mysore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mabsout_B/0/1/0/all/0/1\">Bassel Mabsout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancuso_R/0/1/0/all/0/1\">Renato Mancuso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>",
          "description": "Actors and critics in actor-critic reinforcement learning algorithms are\nfunctionally separate, yet they often use the same network architectures. This\ncase study explores the performance impact of network sizes when considering\nactor and critic architectures independently. By relaxing the assumption of\narchitectural symmetry, it is often possible for smaller actors to achieve\ncomparable policy performance to their symmetric counterparts. Our experiments\nshow up to 99% reduction in the number of network weights with an average\nreduction of 77% over multiple actor-critic algorithms on 9 independent tasks.\nGiven that reducing actor complexity results in a direct reduction of run-time\ninference cost, we believe configurations of actors and critics are aspects of\nactor-critic design that deserve to be considered independently, particularly\nin resource-constrained applications or when deploying multiple actors\nsimultaneously.",
          "link": "http://arxiv.org/abs/2102.11893",
          "publishedOn": "2021-06-22T01:57:13.337Z",
          "wordCount": 624,
          "title": "Honey, I Shrunk The Actor: A Case Study on Preserving Performance with Smaller Actors in Actor-Critic RL. (arXiv:2102.11893v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lamba_J/0/1/0/all/0/1\">Jatin Lamba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abhishek/0/1/0/all/0/1\">Abhishek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akula_J/0/1/0/all/0/1\">Jayaprakash Akula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabral_R/0/1/0/all/0/1\">Rishabh Dabral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>",
          "description": "In this paper, we present a novel approach to the audio-visual video parsing\n(AVVP) task that demarcates events from a video separately for audio and visual\nmodalities. The proposed parsing approach simultaneously detects the temporal\nboundaries in terms of start and end times of such events. We show how AVVP can\nbenefit from the following techniques geared towards effective cross-modal\nlearning: (i) adversarial training and skip connections (ii) global context\naware attention and, (iii) self-supervised pretraining using an audio-video\ngrounding objective to obtain cross-modal audio-video representations. We\npresent extensive experimental evaluations on the Look, Listen, and Parse (LLP)\ndataset and show that we outperform the state-of-the-art Hybrid Attention\nNetwork (HAN) on all five metrics proposed for AVVP. We also present several\nablations to validate the effect of pretraining, global attention and\nadversarial training.",
          "link": "http://arxiv.org/abs/2104.04598",
          "publishedOn": "2021-06-22T01:57:13.237Z",
          "wordCount": 618,
          "title": "Cross-Modal learning for Audio-Visual Video Parsing. (arXiv:2104.04598v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1\">Mustafa Hajij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batayneh_F/0/1/0/all/0/1\">Fawwaz Batayneh</a>",
          "description": "Topological Data Analysis (TDA) has emerged recently as a robust tool to\nextract and compare the structure of datasets. TDA identifies features in data\nsuch as connected components and holes and assigns a quantitative measure to\nthese features. Several studies reported that topological features extracted by\nTDA tools provide unique information about the data, discover new insights, and\ndetermine which feature is more related to the outcome. On the other hand, the\noverwhelming success of deep neural networks in learning patterns and\nrelationships has been proven on a vast array of data applications, images in\nparticular. To capture the characteristics of both powerful tools, we propose\n\\textit{TDA-Net}, a novel ensemble network that fuses topological and deep\nfeatures for the purpose of enhancing model generalizability and accuracy. We\napply the proposed \\textit{TDA-Net} to a critical application, which is the\nautomated detection of COVID-19 from CXR images. The experimental results\nshowed that the proposed network achieved excellent performance and suggests\nthe applicability of our method in practice.",
          "link": "http://arxiv.org/abs/2101.08398",
          "publishedOn": "2021-06-22T01:57:13.223Z",
          "wordCount": 697,
          "title": "TDA-Net: Fusion of Persistent Homology and Deep Learning Features for COVID-19 Detection in Chest X-Ray Images. (arXiv:2101.08398v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.08930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xintian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1\">Muhammad Usman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Rentuo Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1\">Pengfei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huanhuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>",
          "description": "Generative Adversarial Networks (GANs) have been widely applied in different\nscenarios thanks to the development of deep neural networks. The original GAN\nwas proposed based on the non-parametric assumption of the infinite capacity of\nnetworks. However, it is still unknown whether GANs can generate realistic\nsamples without any prior information. Due to the overconfident assumption,\nmany issues remain unaddressed in GANs' training, such as non-convergence, mode\ncollapses, gradient vanishing. Regularization and normalization are common\nmethods of introducing prior information to stabilize training and improve\ndiscrimination. Although a handful number of regularization and normalization\nmethods have been proposed for GANs, to the best of our knowledge, there exists\nno comprehensive survey which primarily focuses on objectives and development\nof these methods, apart from some in-comprehensive and limited scope studies.\nIn this work, we conduct a comprehensive survey on the regularization and\nnormalization techniques from different perspectives of GANs training. First,\nwe systematically describe different perspectives of GANs training and thus\nobtain the different objectives of regularization and normalization. Based on\nthese objectives, we propose a new taxonomy. Furthermore, we compare the\nperformance of the mainstream methods on different datasets and investigate the\nregularization and normalization techniques that have been frequently employed\nin SOTA GANs. Finally, we highlight potential future directions of research in\nthis domain.",
          "link": "http://arxiv.org/abs/2008.08930",
          "publishedOn": "2021-06-22T01:57:13.214Z",
          "wordCount": 721,
          "title": "A Systematic Survey of Regularization and Normalization in GANs. (arXiv:2008.08930v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schlembach_C/0/1/0/all/0/1\">Christoph Schlembach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_S/0/1/0/all/0/1\">Sascha L. Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schreyer_D/0/1/0/all/0/1\">Dominik Schreyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wunderlich_L/0/1/0/all/0/1\">Linus Wunderlich</a>",
          "description": "Forecasting the number of Olympic medals for each nation is highly relevant\nfor different stakeholders: Ex ante, sports betting companies can determine the\nodds while sponsors and media companies can allocate their resources to\npromising teams. Ex post, sports politicians and managers can benchmark the\nperformance of their teams and evaluate the drivers of success. To\nsignificantly increase the Olympic medal forecasting accuracy, we apply machine\nlearning, more specifically a two-staged Random Forest, thus outperforming more\ntraditional na\\\"ive forecast for three previous Olympics held between 2008 and\n2016 for the first time. Regarding the Tokyo 2020 Games in 2021, our model\nsuggests that the United States will lead the Olympic medal table, winning 120\nmedals, followed by China (87) and Great Britain (74). Intriguingly, we predict\nthat the current COVID-19 pandemic will not significantly alter the medal count\nas all countries suffer from the pandemic to some extent (data inherent) and\nlimited historical data points on comparable diseases (model inherent).",
          "link": "http://arxiv.org/abs/2012.04378",
          "publishedOn": "2021-06-22T01:57:13.201Z",
          "wordCount": 681,
          "title": "Forecasting the Olympic medal distribution during a pandemic: a socio-economic machine learning model. (arXiv:2012.04378v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13881",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Klusowski_J/0/1/0/all/0/1\">Jason M. Klusowski</a>",
          "description": "This paper shows that decision trees constructed with Classification and\nRegression Trees (CART) methodology are universally consistent in an additive\nmodel context, even when the number of predictor variables scales exponentially\nwith the sample size, under certain $1$-norm sparsity constraints. The\nconsistency is universal in the sense that there are no a priori assumptions on\nthe distribution of the predictor variables. Amazingly, this adaptivity to\n(approximate or exact) sparsity is achieved with a single tree, as opposed to\nwhat might be expected for an ensemble. Finally, we show that these qualitative\nproperties of individual trees are inherited by Breiman's random forests.\nAnother surprise is that consistency holds even when the \"mtry\" tuning\nparameter vanishes as a fraction of the number of predictor variables, thus\nspeeding up computation of the forest. A key step in the analysis is the\nestablishment of an oracle inequality, which precisely characterizes the\ngoodness-of-fit and complexity tradeoff for a misspecified model.",
          "link": "http://arxiv.org/abs/2104.13881",
          "publishedOn": "2021-06-22T01:57:13.177Z",
          "wordCount": 626,
          "title": "Universal Consistency of Decision Trees in High Dimensions. (arXiv:2104.13881v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10153",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Schmidt_J/0/1/0/all/0/1\">Jonathan Schmidt</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kramer_N/0/1/0/all/0/1\">Nicholas Kr&#xe4;mer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hennig_P/0/1/0/all/0/1\">Philipp Hennig</a>",
          "description": "Mechanistic models with differential equations are a key component of\nscientific applications of machine learning. Inference in such models is\nusually computationally demanding, because it involves repeatedly solving the\ndifferential equation. The main problem here is that the numerical solver is\nhard to combine with standard inference techniques. Recent work in\nprobabilistic numerics has developed a new class of solvers for ordinary\ndifferential equations (ODEs) that phrase the solution process directly in\nterms of Bayesian filtering. We here show that this allows such methods to be\ncombined very directly, with conceptual and numerical ease, with latent force\nmodels in the ODE itself. It then becomes possible to perform approximate\nBayesian inference on the latent force as well as the ODE solution in a single,\nlinear complexity pass of an extended Kalman filter / smoother - that is, at\nthe cost of computing a single ODE solution. We demonstrate the expressiveness\nand performance of the algorithm by training, among others, a non-parametric\nSIRD model on data from the COVID-19 outbreak.",
          "link": "http://arxiv.org/abs/2103.10153",
          "publishedOn": "2021-06-22T01:57:13.171Z",
          "wordCount": 685,
          "title": "A Probabilistic State Space Model for Joint Inference from Differential Equations and Data. (arXiv:2103.10153v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.07937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Woong-Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozger_M/0/1/0/all/0/1\">Mustafa Ozger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Challita_U/0/1/0/all/0/1\">Ursula Challita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_K/0/1/0/all/0/1\">Ki Won Sung</a>",
          "description": "This letter introduces a new denoiser that modifies the structure of\ndenoising autoencoder (DAE), namely noise learning based DAE (nlDAE). The\nproposed nlDAE learns the noise of the input data. Then, the denoising is\nperformed by subtracting the regenerated noise from the noisy input. Hence,\nnlDAE is more effective than DAE when the noise is simpler to regenerate than\nthe original data. To validate the performance of nlDAE, we provide three case\nstudies: signal restoration, symbol demodulation, and precise localization.\nNumerical results suggest that nlDAE requires smaller latent space dimension\nand smaller training dataset compared to DAE.",
          "link": "http://arxiv.org/abs/2101.07937",
          "publishedOn": "2021-06-22T01:57:13.164Z",
          "wordCount": 560,
          "title": "Noise Learning Based Denoising Autoencoder. (arXiv:2101.07937v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dognin_P/0/1/0/all/0/1\">Pierre Dognin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_I/0/1/0/all/0/1\">Igor Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mroueh_Y/0/1/0/all/0/1\">Youssef Mroueh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padhi_I/0/1/0/all/0/1\">Inkit Padhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigotti_M/0/1/0/all/0/1\">Mattia Rigotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_J/0/1/0/all/0/1\">Jarret Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiff_Y/0/1/0/all/0/1\">Yair Schiff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_R/0/1/0/all/0/1\">Richard A. Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belgodere_B/0/1/0/all/0/1\">Brian Belgodere</a>",
          "description": "Image captioning has recently demonstrated impressive progress largely owing\nto the introduction of neural network algorithms trained on curated dataset\nlike MS-COCO. Often work in this field is motivated by the promise of\ndeployment of captioning systems in practical applications. However, the\nscarcity of data and contexts in many competition datasets renders the utility\nof systems trained on these datasets limited as an assistive technology in\nreal-world settings, such as helping visually impaired people navigate and\naccomplish everyday tasks. This gap motivated the introduction of the novel\nVizWiz dataset, which consists of images taken by the visually impaired and\ncaptions that have useful, task-oriented information. In an attempt to help the\nmachine learning computer vision field realize its promise of producing\ntechnologies that have positive social impact, the curators of the VizWiz\ndataset host several competitions, including one for image captioning. This\nwork details the theory and engineering from our winning submission to the 2020\ncaptioning competition. Our work provides a step towards improved assistive\nimage captioning systems.",
          "link": "http://arxiv.org/abs/2012.11696",
          "publishedOn": "2021-06-22T01:57:13.156Z",
          "wordCount": 679,
          "title": "Image Captioning as an Assistive Technology: Lessons Learned from VizWiz 2020 Challenge. (arXiv:2012.11696v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xuebin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bingxin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junbin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Guang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Lio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Ming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montufar_G/0/1/0/all/0/1\">Guido Montufar</a>",
          "description": "This paper presents a new approach for assembling graph neural networks based\non framelet transforms. The latter provides a multi-scale representation for\ngraph-structured data. We decompose an input graph into low-pass and high-pass\nfrequencies coefficients for network training, which then defines a\nframelet-based graph convolution. The framelet decomposition naturally induces\na graph pooling strategy by aggregating the graph feature into low-pass and\nhigh-pass spectra, which considers both the feature values and geometry of the\ngraph data and conserves the total information. The graph neural networks with\nthe proposed framelet convolution and pooling achieve state-of-the-art\nperformance in many node and graph prediction tasks. Moreover, we propose\nshrinkage as a new activation for the framelet convolution, which thresholds\nhigh-frequency information at different scales. Compared to ReLU, shrinkage\nactivation improves model performance on denoising and signal compression:\nnoises in both node and structure can be significantly reduced by accurately\ncutting off the high-pass coefficients from framelet decomposition, and the\nsignal can be compressed to less than half its original size with\nwell-preserved prediction performance.",
          "link": "http://arxiv.org/abs/2102.06986",
          "publishedOn": "2021-06-22T01:57:13.150Z",
          "wordCount": 673,
          "title": "How Framelets Enhance Graph Neural Networks. (arXiv:2102.06986v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.10102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joy_T/0/1/0/all/0/1\">Tom Joy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmon_S/0/1/0/all/0/1\">Sebastian M. Schmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1\">Tom Rainforth</a>",
          "description": "We present a principled approach to incorporating labels in VAEs that\ncaptures the rich characteristic information associated with those labels.\nWhile prior work has typically conflated these by learning latent variables\nthat directly correspond to label values, we argue this is contrary to the\nintended effect of supervision in VAEs-capturing rich label characteristics\nwith the latents. For example, we may want to capture the characteristics of a\nface that make it look young, rather than just the age of the person. To this\nend, we develop the CCVAE, a novel VAE model and concomitant variational\nobjective which captures label characteristics explicitly in the latent space,\neschewing direct correspondences between label values and latents. Through\njudicious structuring of mappings between such characteristic latents and\nlabels, we show that the CCVAE can effectively learn meaningful representations\nof the characteristics of interest across a variety of supervision schemes. In\nparticular, we show that the CCVAE allows for more effective and more general\ninterventions to be performed, such as smooth traversals within the\ncharacteristics for a given label, diverse conditional generation, and\ntransferring characteristics across datapoints.",
          "link": "http://arxiv.org/abs/2006.10102",
          "publishedOn": "2021-06-22T01:57:13.134Z",
          "wordCount": 651,
          "title": "Capturing Label Characteristics in VAEs. (arXiv:2006.10102v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_J/0/1/0/all/0/1\">Joydeep Ghosh</a>",
          "description": "How can we find a subset of training samples that are most responsible for a\nspecific prediction made by a complex black-box machine learning model? More\ngenerally, how can we explain the model's decisions to end-users in a\ntransparent way? We propose a new model-agnostic algorithm to identify a\nminimal set of training samples that are indispensable for a given model's\ndecision at a particular test point, i.e., the model's decision would have\nchanged upon the removal of this subset from the training dataset. Our\nalgorithm identifies such a set of \"indispensable\" samples iteratively by\nsolving a constrained optimization problem. Further, we speed up the algorithm\nthrough efficient approximations and provide theoretical justification for its\nperformance. To demonstrate the applicability and effectiveness of our\napproach, we apply it to a variety of tasks including data poisoning detection,\ntraining set debugging and understanding loan decisions. The results show that\nour algorithm is an effective and easy-to-comprehend tool that helps to better\nunderstand local model behavior, and therefore facilitates the adoption of\nmachine learning in domains where such understanding is a requisite.",
          "link": "http://arxiv.org/abs/2011.00639",
          "publishedOn": "2021-06-22T01:57:13.127Z",
          "wordCount": 644,
          "title": "Model-Agnostic Explanations using Minimal Forcing Subsets. (arXiv:2011.00639v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.03258",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shlezinger_N/0/1/0/all/0/1\">Nir Shlezinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farsad_N/0/1/0/all/0/1\">Nariman Farsad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1\">Yonina C. Eldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldsmith_A/0/1/0/all/0/1\">Andrea J. Goldsmith</a>",
          "description": "The design of methods for inference from time sequences has traditionally\nrelied on statistical models that describe the relation between a latent\ndesired sequence and the observed one. A broad family of model-based algorithms\nhave been derived to carry out inference at controllable complexity using\nrecursive computations over the factor graph representing the underlying\ndistribution. An alternative model-agnostic approach utilizes machine learning\n(ML) methods. Here we propose a framework that combines model-based algorithms\nand data-driven ML tools for stationary time sequences. In the proposed\napproach, neural networks are developed to separately learn specific components\nof a factor graph describing the distribution of the time sequence, rather than\nthe complete inference task. By exploiting stationary properties of this\ndistribution, the resulting approach can be applied to sequences of varying\ntemporal duration. Learned factor graph can be realized using compact neural\nnetworks that are trainable using small training sets, or alternatively, be\nused to improve upon existing deep inference systems. We present an inference\nalgorithm based on learned stationary factor graphs, which learns to implement\nthe sum-product scheme from labeled data, and can be applied to sequences of\ndifferent lengths. Our experimental results demonstrate the ability of the\nproposed learned factor graphs to learn to carry out accurate inference from\nsmall training sets for sleep stage detection using the Sleep-EDF dataset, as\nwell as for symbol detection in digital communications with unknown channels.",
          "link": "http://arxiv.org/abs/2006.03258",
          "publishedOn": "2021-06-22T01:57:13.121Z",
          "wordCount": 716,
          "title": "Learned Factor Graphs for Inference from Stationary Time Sequences. (arXiv:2006.03258v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.07488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1\">Shounak Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mariottoni_E/0/1/0/all/0/1\">Eduardo B. Mariottoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dov_D/0/1/0/all/0/1\">David Dov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jammal_A/0/1/0/all/0/1\">Alessandro A. Jammal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medeiros_F/0/1/0/all/0/1\">Felipe A. Medeiros</a>",
          "description": "Glaucoma is the leading cause of irreversible blindness in the world,\naffecting over 70 million people. The cumbersome Standard Automated Perimetry\n(SAP) test is most frequently used to detect visual loss due to glaucoma. Due\nto the SAP test's innate difficulty and its high test-retest variability, we\npropose the RetiNerveNet, a deep convolutional recursive neural network for\nobtaining estimates of the SAP visual field. RetiNerveNet uses information from\nthe more objective Spectral-Domain Optical Coherence Tomography (SDOCT).\nRetiNerveNet attempts to trace-back the arcuate convergence of the retinal\nnerve fibers, starting from the Retinal Nerve Fiber Layer (RNFL) thickness\naround the optic disc, to estimate individual age-corrected 24-2 SAP values.\nRecursive passes through the proposed network sequentially yield estimates of\nthe visual locations progressively farther from the optic disc. While all the\nmethods used for our experiments exhibit lower performance for the advanced\ndisease group, the proposed network is observed to be more accurate than all\nthe baselines for estimating the individual visual field values. We further\naugment RetiNerveNet to additionally predict the SAP Mean Deviation values and\nalso create an ensemble of RetiNerveNets that further improves the performance,\nby increasingly weighting-up underrepresented parts of the training data.",
          "link": "http://arxiv.org/abs/2010.07488",
          "publishedOn": "2021-06-22T01:57:13.115Z",
          "wordCount": 695,
          "title": "RetiNerveNet: Using Recursive Deep Learning to Estimate Pointwise 24-2 Visual Field Data based on Retinal Structure. (arXiv:2010.07488v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15658",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Behboodi_A/0/1/0/all/0/1\">Arash Behboodi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rauhut_H/0/1/0/all/0/1\">Holger Rauhut</a>, <a href=\"http://arxiv.org/find/math/1/au:+Schnoor_E/0/1/0/all/0/1\">Ekkehard Schnoor</a>",
          "description": "Various iterative reconstruction algorithms for inverse problems can be\nunfolded as neural networks. Empirically, this approach has often led to\nimproved results, but theoretical guarantees are still scarce. While some\nprogress on generalization properties of neural networks have been made, great\nchallenges remain. In this chapter, we discuss and combine these topics to\npresent a generalization error analysis for a class of neural networks suitable\nfor sparse reconstruction from few linear measurements. The hypothesis class\nconsidered is inspired by the classical iterative soft-thresholding algorithm\n(ISTA). The neural networks in this class are obtained by unfolding iterations\nof ISTA and learning some of the weights. Based on training samples, we aim at\nlearning the optimal network parameters via empirical risk minimization and\nthereby the optimal network that reconstructs signals from their compressive\nlinear measurements. In particular, we may learn a sparsity basis that is\nshared by all of the iterations/layers and thereby obtain a new approach for\ndictionary learning. For this class of networks, we present a generalization\nbound, which is based on bounding the Rademacher complexity of hypothesis\nclasses consisting of such deep networks via Dudley's integral. Remarkably,\nunder realistic conditions, the generalization error scales only\nlogarithmically in the number of layers, and at most linear in number of\nmeasurements.",
          "link": "http://arxiv.org/abs/2010.15658",
          "publishedOn": "2021-06-22T01:57:13.110Z",
          "wordCount": 689,
          "title": "Compressive Sensing and Neural Networks from a Statistical Learning Perspective. (arXiv:2010.15658v3 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07134",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Koskela_A/0/1/0/all/0/1\">Antti Koskela</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jalko_J/0/1/0/all/0/1\">Joonas J&#xe4;lk&#xf6;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Prediger_L/0/1/0/all/0/1\">Lukas Prediger</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Honkela_A/0/1/0/all/0/1\">Antti Honkela</a>",
          "description": "We propose a numerical accountant for evaluating the tight\n$(\\varepsilon,\\delta)$-privacy loss for algorithms with discrete one\ndimensional output. The method is based on the privacy loss distribution\nformalism and it uses the recently introduced fast Fourier transform based\naccounting technique. We carry out an error analysis of the method in terms of\nmoment bounds of the privacy loss distribution which leads to rigorous lower\nand upper bounds for the true $(\\varepsilon,\\delta)$-values. As an application,\nwe present a novel approach to accurate privacy accounting of the subsampled\nGaussian mechanism. This completes the previously proposed analysis by giving\nstrict lower and upper bounds for the privacy parameters. We demonstrate the\nperformance of the accountant on the binomial mechanism and show that our\napproach allows decreasing noise variance up to 75 percent at equal privacy\ncompared to existing bounds in the literature. We also illustrate how to\ncompute tight bounds for the exponential mechanism applied to counting queries.",
          "link": "http://arxiv.org/abs/2006.07134",
          "publishedOn": "2021-06-22T01:57:13.091Z",
          "wordCount": 636,
          "title": "Tight Differential Privacy for Discrete-Valued Mechanisms and for the Subsampled Gaussian Mechanism Using FFT. (arXiv:2006.07134v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_D/0/1/0/all/0/1\">Divyam Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>",
          "description": "Adversarial learning has emerged as one of the successful techniques to\ncircumvent the susceptibility of existing methods against adversarial\nperturbations. However, the majority of existing defense methods are tailored\nto defend against a single category of adversarial perturbation (e.g.\n$\\ell_\\infty$-attack). In safety-critical applications, this makes these\nmethods extraneous as the attacker can adopt diverse adversaries to deceive the\nsystem. Moreover, training on multiple perturbations simultaneously\nsignificantly increases the computational overhead during training. To address\nthese challenges, we propose a novel meta-learning framework that explicitly\nlearns to generate noise to improve the model's robustness against multiple\ntypes of attacks. Its key component is Meta Noise Generator (MNG) that outputs\noptimal noise to stochastically perturb a given sample, such that it helps\nlower the error on diverse adversarial perturbations. By utilizing samples\ngenerated by MNG, we train a model by enforcing the label consistency across\nmultiple perturbations. We validate the robustness of models trained by our\nscheme on various datasets and against a wide variety of perturbations,\ndemonstrating that it significantly outperforms the baselines across multiple\nperturbations with a marginal computational cost.",
          "link": "http://arxiv.org/abs/2006.12135",
          "publishedOn": "2021-06-22T01:57:13.085Z",
          "wordCount": 672,
          "title": "Learning to Generate Noise for Multi-Attack Robustness. (arXiv:2006.12135v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.06158",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Chen_J/0/1/0/all/0/1\">Jiafeng Chen</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Chen_D/0/1/0/all/0/1\">Daniel L. Chen</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Lewis_G/0/1/0/all/0/1\">Greg Lewis</a>",
          "description": "We offer straightforward theoretical results that justify incorporating\nmachine learning in the standard linear instrumental variable setting. The key\nidea is to use machine learning, combined with sample-splitting, to predict the\ntreatment variable from the instrument and any exogenous covariates, and then\nuse this predicted treatment and the covariates as technical instruments to\nrecover the coefficients in the second-stage. This allows the researcher to\nextract non-linear co-variation between the treatment and instrument that may\ndramatically improve estimation precision and robustness by boosting instrument\nstrength. Importantly, we constrain the machine-learned predictions to be\nlinear in the exogenous covariates, thus avoiding spurious identification\narising from non-linear relationships between the treatment and the covariates.\nWe show that this approach delivers consistent and asymptotically normal\nestimates under weak conditions and that it may be adapted to be\nsemiparametrically efficient (Chamberlain, 1992). Our method preserves standard\nintuitions and interpretations of linear instrumental variable methods,\nincluding under weak identification, and provides a simple, user-friendly\nupgrade to the applied economics toolbox. We illustrate our method with an\nexample in law and criminal justice, examining the causal effect of appellate\ncourt reversals on district court sentencing decisions.",
          "link": "http://arxiv.org/abs/2011.06158",
          "publishedOn": "2021-06-22T01:57:13.080Z",
          "wordCount": 665,
          "title": "Mostly Harmless Machine Learning: Learning Optimal Instruments in Linear IV Models. (arXiv:2011.06158v3 [econ.EM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lemeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1\">Peter Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>",
          "description": "We propose firefly neural architecture descent, a general framework for\nprogressively and dynamically growing neural networks to jointly optimize the\nnetworks' parameters and architectures. Our method works in a steepest descent\nfashion, which iteratively finds the best network within a functional\nneighborhood of the original network that includes a diverse set of candidate\nnetwork structures. By using Taylor approximation, the optimal network\nstructure in the neighborhood can be found with a greedy selection procedure.\nWe show that firefly descent can flexibly grow networks both wider and deeper,\nand can be applied to learn accurate but resource-efficient neural\narchitectures that avoid catastrophic forgetting in continual learning.\nEmpirically, firefly descent achieves promising results on both neural\narchitecture search and continual learning. In particular, on a challenging\ncontinual image classification task, it learns networks that are smaller in\nsize but have higher average accuracy than those learned by the\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2102.08574",
          "publishedOn": "2021-06-22T01:57:13.074Z",
          "wordCount": 613,
          "title": "Firefly Neural Architecture Descent: a General Approach for Growing Neural Networks. (arXiv:2102.08574v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.04847",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kazemi_A/0/1/0/all/0/1\">Amir Kazemi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meidani_H/0/1/0/all/0/1\">Hadi Meidani</a>",
          "description": "Increasing use of sensor data in intelligent transportation systems calls for\naccurate imputation algorithms that can enable reliable traffic management in\nthe occasional absence of data. As one of the effective imputation approaches,\ngenerative adversarial networks (GANs) are implicit generative models that can\nbe used for data imputation, which is formulated as an unsupervised learning\nproblem. This work introduces a novel iterative GAN architecture, called\nIterative Generative Adversarial Networks for Imputation (IGANI), for data\nimputation. IGANI imputes data in two steps and maintains the invertibility of\nthe generative imputer, which will be shown to be a sufficient condition for\nthe convergence of the proposed GAN-based imputation. The performance of our\nproposed method is evaluated on (1) the imputation of traffic speed data\ncollected in the city of Guangzhou in China, and the training of short-term\ntraffic prediction models using imputed data, and (2) the imputation of\nmulti-variable traffic data of highways in Portland-Vancouver metropolitan\nregion which includes volume, occupancy, and speed with different missing rates\nfor each of them. It is shown that our proposed algorithm mostly produces more\naccurate results compared to those of previous GAN-based imputation\narchitectures.",
          "link": "http://arxiv.org/abs/2008.04847",
          "publishedOn": "2021-06-22T01:57:13.068Z",
          "wordCount": 656,
          "title": "IGANI: Iterative Generative Adversarial Networks for Imputation with Application to Traffic Data. (arXiv:2008.04847v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10836",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Senzaki_Y/0/1/0/all/0/1\">Yuya Senzaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamelain_C/0/1/0/all/0/1\">Christian Hamelain</a>",
          "description": "When dealing with deep neural network (DNN) applications on edge devices,\ncontinuously updating the model is important. Although updating a model with\nreal incoming data is ideal, using all of them is not always feasible due to\nlimits, such as labeling and communication costs. Thus, it is necessary to\nfilter and select the data to use for training (i.e., active learning) on the\ndevice. In this paper, we formalize a practical active learning problem for\nDNNs on edge devices and propose a general task-agnostic framework to tackle\nthis problem, which reduces it to a stream submodular maximization. This\nframework is light enough to be run with low computational resources, yet\nprovides solutions whose quality is theoretically guaranteed thanks to the\nsubmodular property. Through this framework, we can configure data selection\ncriteria flexibly, including using methods proposed in previous active learning\nstudies. We evaluate our approach on both classification and object detection\ntasks in a practical setting to simulate a real-life scenario. The results of\nour study show that the proposed framework outperforms all other methods in\nboth tasks, while running at a practical speed on real devices.",
          "link": "http://arxiv.org/abs/2106.10836",
          "publishedOn": "2021-06-22T01:57:13.052Z",
          "wordCount": 627,
          "title": "Active Learning for Deep Neural Networks on Edge Devices. (arXiv:2106.10836v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_E/0/1/0/all/0/1\">Enda Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1\">Dezun Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yemao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Shuo Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1\">Xiangke Liao</a>",
          "description": "Communication overhead is the key challenge for distributed training.\nGradient compression is a widely used approach to reduce communication traffic.\nWhen combining with parallel communication mechanism method like pipeline,\ngradient compression technique can greatly alleviate the impact of\ncommunication overhead. However, there exists two problems of gradient\ncompression technique to be solved. Firstly, gradient compression brings in\nextra computation cost, which will delay the next training iteration. Secondly,\ngradient compression usually leads to the decrease of convergence accuracy.",
          "link": "http://arxiv.org/abs/2106.10796",
          "publishedOn": "2021-06-22T01:57:13.045Z",
          "wordCount": 523,
          "title": "CD-SGD: Distributed Stochastic Gradient Descent with Compression and Delay Compensation. (arXiv:2106.10796v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_S/0/1/0/all/0/1\">Senlin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1\">Nan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Miao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1\">Bo An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "To alleviate the data requirement for training effective binary classifiers\nin binary classification, many weakly supervised learning settings have been\nproposed. Among them, some consider using pairwise but not pointwise labels,\nwhen pointwise labels are not accessible due to privacy, confidentiality, or\nsecurity reasons. However, as a pairwise label denotes whether or not two data\npoints share a pointwise label, it cannot be easily collected if either point\nis equally likely to be positive or negative. Thus, in this paper, we propose a\nnovel setting called pairwise comparison (Pcomp) classification, where we have\nonly pairs of unlabeled data that we know one is more likely to be positive\nthan the other. Firstly, we give a Pcomp data generation process, derive an\nunbiased risk estimator (URE) with theoretical guarantee, and further improve\nURE using correction functions. Secondly, we link Pcomp classification to\nnoisy-label learning to develop a progressive URE and improve it by imposing\nconsistency regularization. Finally, we demonstrate by experiments the\neffectiveness of our methods, which suggests Pcomp is a valuable and\npractically useful type of pairwise supervision besides the pairwise label.",
          "link": "http://arxiv.org/abs/2010.01875",
          "publishedOn": "2021-06-22T01:57:13.040Z",
          "wordCount": 668,
          "title": "Pointwise Binary Classification with Pairwise Confidence Comparisons. (arXiv:2010.01875v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zanette_A/0/1/0/all/0/1\">Andrea Zanette</a>",
          "description": "Several practical applications of reinforcement learning involve an agent\nlearning from past data without the possibility of further exploration. Often\nthese applications require us to 1) identify a near optimal policy or to 2)\nestimate the value of a target policy. For both tasks we derive\n\\emph{exponential} information-theoretic lower bounds in discounted infinite\nhorizon MDPs with a linear function representation for the action value\nfunction even if 1) \\emph{realizability} holds, 2) the batch algorithm observes\nthe exact reward and transition \\emph{functions}, and 3) the batch algorithm is\ngiven the \\emph{best} a priori data distribution for the problem class. Our\nwork introduces a new `oracle + batch algorithm' framework to prove lower\nbounds that hold for every distribution. The work shows an exponential\nseparation between batch and online reinforcement learning.",
          "link": "http://arxiv.org/abs/2012.08005",
          "publishedOn": "2021-06-22T01:57:13.030Z",
          "wordCount": 626,
          "title": "Exponential Lower Bounds for Batch Reinforcement Learning: Batch RL can be Exponentially Harder than Online RL. (arXiv:2012.08005v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jason Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_T/0/1/0/all/0/1\">Tony Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yung_D/0/1/0/all/0/1\">Dylan Yung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasseri_S/0/1/0/all/0/1\">S. Ali Nasseri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1\">Frank Wood</a>",
          "description": "There are currently many barriers that prevent non-experts from exploiting\nmachine learning solutions ranging from the lack of intuition on statistical\nlearning techniques to the trickiness of hyperparameter tuning. Such barriers\nhave led to an explosion of interest in automated machine learning (AutoML),\nwhereby an off-the-shelf system can take care of many of the steps for\nend-users without the need for expertise in machine learning. This paper\npresents Ensemble Squared (Ensemble$^2$), an AutoML system that ensembles the\nresults of state-of-the-art open-source AutoML systems. Ensemble$^2$ exploits\nthe diversity of existing AutoML systems by leveraging the differences in their\nmodel search space and heuristics. Empirically, we show that diversity of each\nAutoML system is sufficient to justify ensembling at the AutoML system level.\nIn demonstrating this, we also establish new state-of-the-art AutoML results on\nthe OpenML tabular classification benchmark.",
          "link": "http://arxiv.org/abs/2012.05390",
          "publishedOn": "2021-06-22T01:57:13.024Z",
          "wordCount": 606,
          "title": "Ensemble Squared: A Meta AutoML System. (arXiv:2012.05390v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parmar_P/0/1/0/all/0/1\">Paritosh Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_J/0/1/0/all/0/1\">Jaiden Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_B/0/1/0/all/0/1\">Brendan Morris</a>",
          "description": "Can a computer determine a piano player's skill level? Is it preferable to\nbase this assessment on visual analysis of the player's performance or should\nwe trust our ears over our eyes? Since current CNNs have difficulty processing\nlong video videos, how can shorter clips be sampled to best reflect the players\nskill level? In this work, we collect and release a first-of-its-kind dataset\nfor multimodal skill assessment focusing on assessing piano player's skill\nlevel, answer the asked questions, initiate work in automated evaluation of\npiano playing skills and provide baselines for future work. Dataset is\navailable from: https://github.com/ParitoshParmar/Piano-Skills-Assessment.",
          "link": "http://arxiv.org/abs/2101.04884",
          "publishedOn": "2021-06-22T01:57:13.004Z",
          "wordCount": 576,
          "title": "Piano Skills Assessment. (arXiv:2101.04884v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morrill_J/0/1/0/all/0/1\">James Morrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvi_C/0/1/0/all/0/1\">Cristopher Salvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kidger_P/0/1/0/all/0/1\">Patrick Kidger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">James Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyons_T/0/1/0/all/0/1\">Terry Lyons</a>",
          "description": "Neural controlled differential equations (CDEs) are the continuous-time\nanalogue of recurrent neural networks, as Neural ODEs are to residual networks,\nand offer a memory-efficient continuous-time way to model functions of\npotentially irregular time series. Existing methods for computing the forward\npass of a Neural CDE involve embedding the incoming time series into path\nspace, often via interpolation, and using evaluations of this path to drive the\nhidden state. Here, we use rough path theory to extend this formulation.\nInstead of directly embedding into path space, we instead represent the input\nsignal over small time intervals through its \\textit{log-signature}, which are\nstatistics describing how the signal drives a CDE. This is the approach for\nsolving \\textit{rough differential equations} (RDEs), and correspondingly we\ndescribe our main contribution as the introduction of Neural RDEs. This\nextension has a purpose: by generalising the Neural CDE approach to a broader\nclass of driving signals, we demonstrate particular advantages for tackling\nlong time series. In this regime, we demonstrate efficacy on problems of length\nup to 17k observations and observe significant training speed-ups, improvements\nin model performance, and reduced memory requirements compared to existing\napproaches.",
          "link": "http://arxiv.org/abs/2009.08295",
          "publishedOn": "2021-06-22T01:57:12.998Z",
          "wordCount": 686,
          "title": "Neural Rough Differential Equations for Long Time Series. (arXiv:2009.08295v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00146",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yap_P/0/1/0/all/0/1\">Pauching Yap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_H/0/1/0/all/0/1\">Hippolyt Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barber_D/0/1/0/all/0/1\">David Barber</a>",
          "description": "Neural networks are known to suffer from catastrophic forgetting when trained\non sequential datasets. While there have been numerous attempts to solve this\nproblem in large-scale supervised classification, little has been done to\novercome catastrophic forgetting in few-shot classification problems. We\ndemonstrate that the popular gradient-based model-agnostic meta-learning\nalgorithm (MAML) indeed suffers from catastrophic forgetting and introduce a\nBayesian online meta-learning framework that tackles this problem. Our\nframework utilises Bayesian online learning and meta-learning along with\nLaplace approximation and variational inference to overcome catastrophic\nforgetting in few-shot classification problems. The experimental evaluations\ndemonstrate that our framework can effectively achieve this goal in comparison\nwith various baselines. As an additional utility, we also demonstrate\nempirically that our framework is capable of meta-learning on sequentially\narriving few-shot tasks from a stationary task distribution.",
          "link": "http://arxiv.org/abs/2005.00146",
          "publishedOn": "2021-06-22T01:57:12.992Z",
          "wordCount": 599,
          "title": "Addressing Catastrophic Forgetting in Few-Shot Problems. (arXiv:2005.00146v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.05601",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tsai_K/0/1/0/all/0/1\">Katherine Tsai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kolar_M/0/1/0/all/0/1\">Mladen Kolar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Koyejo_O/0/1/0/all/0/1\">Oluwasanmi Koyejo</a>",
          "description": "We propose a flexible yet interpretable model for high-dimensional data with\ntime-varying second order statistics, motivated and applied to functional\nneuroimaging data. Motivated by the neuroscience literature, we factorize the\ncovariances into sparse spatial and smooth temporal components. While this\nfactorization results in both parsimony and domain interpretability, the\nresulting estimation problem is nonconvex. To this end, we design a two-stage\noptimization scheme with a carefully tailored spectral initialization, combined\nwith iteratively refined alternating projected gradient descent. We prove a\nlinear convergence rate up to a nontrivial statistical error for the proposed\ndescent scheme and establish sample complexity guarantees for the estimator. We\nfurther quantify the statistical error for the multivariate Gaussian case.\nEmpirical results using simulated and real brain imaging data illustrate that\nour approach outperforms existing baselines.",
          "link": "http://arxiv.org/abs/2011.05601",
          "publishedOn": "2021-06-22T01:57:12.986Z",
          "wordCount": 584,
          "title": "A Nonconvex Framework for Structured Dynamic Covariance Recovery. (arXiv:2011.05601v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ouali_Y/0/1/0/all/0/1\">Yassine Ouali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1\">C&#xe9;line Hudelot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tami_M/0/1/0/all/0/1\">Myriam Tami</a>",
          "description": "In this paper, we explore contrastive learning for few-shot classification,\nin which we propose to use it as an additional auxiliary training objective\nacting as a data-dependent regularizer to promote more general and transferable\nfeatures. In particular, we present a novel attention-based spatial contrastive\nobjective to learn locally discriminative and class-agnostic features. As a\nresult, our approach overcomes some of the limitations of the cross-entropy\nloss, such as its excessive discrimination towards seen classes, which reduces\nthe transferability of features to unseen classes. With extensive experiments,\nwe show that the proposed method outperforms state-of-the-art approaches,\nconfirming the importance of learning good and transferable embeddings for\nfew-shot learning.",
          "link": "http://arxiv.org/abs/2012.13831",
          "publishedOn": "2021-06-22T01:57:12.980Z",
          "wordCount": 580,
          "title": "Spatial Contrastive Learning for Few-Shot Classification. (arXiv:2012.13831v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13138",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hemati_S/0/1/0/all/0/1\">Sobhan Hemati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdizavareh_M/0/1/0/all/0/1\">Mohammad Hadi Mehdizavareh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenouri_S/0/1/0/all/0/1\">Shojaeddin Chenouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1\">Hamid R Tizhoosh</a>",
          "description": "In the era of big data, methods for improving memory and computational\nefficiency have become crucial for successful deployment of technologies.\nHashing is one of the most effective approaches to deal with computational\nlimitations that come with big data. One natural way for formulating this\nproblem is spectral hashing that directly incorporates affinity to learn binary\ncodes. However, due to binary constraints, the optimization becomes\nintractable. To mitigate this challenge, different relaxation approaches have\nbeen proposed to reduce the computational load of obtaining binary codes and\nstill attain a good solution. The problem with all existing relaxation methods\nis resorting to one or more additional auxiliary variables to attain high\nquality binary codes while relaxing the problem. The existence of auxiliary\nvariables leads to coordinate descent approach which increases the\ncomputational complexity. We argue that introducing these variables is\nunnecessary. To this end, we propose a novel relaxed formulation for spectral\nhashing that adds no additional variables to the problem. Furthermore, instead\nof solving the problem in original space where number of variables is equal to\nthe data points, we solve the problem in a much smaller space and retrieve the\nbinary codes from this solution. This trick reduces both the memory and\ncomputational complexity at the same time. We apply two optimization\ntechniques, namely projected gradient and optimization on manifold, to obtain\nthe solution. Using comprehensive experiments on four public datasets, we show\nthat the proposed efficient spectral hashing (ESH) algorithm achieves highly\ncompetitive retrieval performance compared with state of the art at low\ncomplexity.",
          "link": "http://arxiv.org/abs/2012.13138",
          "publishedOn": "2021-06-22T01:57:12.975Z",
          "wordCount": 745,
          "title": "A non-alternating graph hashing algorithm for large scale image search. (arXiv:2012.13138v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Simhayev_E/0/1/0/all/0/1\">Eli Simhayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1\">Gilad Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rokach_L/0/1/0/all/0/1\">Lior Rokach</a>",
          "description": "Improving the robustness of neural nets in regression tasks is key to their\napplication in multiple domains. Deep learning-based approaches aim to achieve\nthis goal either by improving their prediction of specific values (i.e., point\nprediction), or by producing prediction intervals (PIs) that quantify\nuncertainty. We present PIVEN, a deep neural network for producing both a PI\nand a value prediction. Our loss function expresses the value prediction as a\nfunction of the upper and lower bounds, thus ensuring that it falls within the\ninterval without increasing model complexity. Moreover, our approach makes no\nassumptions regarding data distribution within the PI, making its value\nprediction more effective for various real-world problems. Experiments and\nablation tests on known benchmarks show that our approach produces tighter\nuncertainty bounds than the current state-of-the-art approaches for producing\nPIs, while maintaining comparable performance to the state-of-the-art approach\nfor value-prediction. Additionally, we go beyond previous work and include\nlarge image datasets in our evaluation, where PIVEN is combined with modern\nneural nets.",
          "link": "http://arxiv.org/abs/2006.05139",
          "publishedOn": "2021-06-22T01:57:12.958Z",
          "wordCount": 645,
          "title": "PIVEN: A Deep Neural Network for Prediction Intervals with Specific Value Prediction. (arXiv:2006.05139v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.11949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>",
          "description": "We study the expressive power of deep ReLU neural networks for approximating\nfunctions in dilated shift-invariant spaces, which are widely used in signal\nprocessing, image processing, communications and so on. Approximation error\nbounds are estimated with respect to the width and depth of neural networks.\nThe network construction is based on the bit extraction and data-fitting\ncapacity of deep neural networks. As applications of our main results, the\napproximation rates of classical function spaces such as Sobolev spaces and\nBesov spaces are obtained. We also give lower bounds of the $L^p (1\\le p \\le\n\\infty)$ approximation error for Sobolev spaces, which show that our\nconstruction of neural network is asymptotically optimal up to a logarithmic\nfactor.",
          "link": "http://arxiv.org/abs/2005.11949",
          "publishedOn": "2021-06-22T01:57:12.953Z",
          "wordCount": 584,
          "title": "Approximation in shift-invariant spaces with deep ReLU neural networks. (arXiv:2005.11949v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06725",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narihira_T/0/1/0/all/0/1\">Takuya Narihira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonsogarcia_J/0/1/0/all/0/1\">Javier Alonsogarcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardinaux_F/0/1/0/all/0/1\">Fabien Cardinaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayakawa_A/0/1/0/all/0/1\">Akio Hayakawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_M/0/1/0/all/0/1\">Masato Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwaki_K/0/1/0/all/0/1\">Kazunori Iwaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemp_T/0/1/0/all/0/1\">Thomas Kemp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_Y/0/1/0/all/0/1\">Yoshiyuki Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mauch_L/0/1/0/all/0/1\">Lukas Mauch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_A/0/1/0/all/0/1\">Akira Nakamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obuchi_Y/0/1/0/all/0/1\">Yukio Obuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_A/0/1/0/all/0/1\">Andrew Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_K/0/1/0/all/0/1\">Kenji Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiedmann_S/0/1/0/all/0/1\">Stephen Tiedmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uhlich_S/0/1/0/all/0/1\">Stefan Uhlich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yashima_T/0/1/0/all/0/1\">Takuya Yashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshiyama_K/0/1/0/all/0/1\">Kazuki Yoshiyama</a>",
          "description": "While there exist a plethora of deep learning tools and frameworks, the\nfast-growing complexity of the field brings new demands and challenges, such as\nmore flexible network design, speedy computation on distributed setting, and\ncompatibility between different tools. In this paper, we introduce Neural\nNetwork Libraries (https://nnabla.org), a deep learning framework designed from\nengineer's perspective, with emphasis on usability and compatibility as its\ncore design principles. We elaborate on each of our design principles and its\nmerits, and validate our attempts via experiments.",
          "link": "http://arxiv.org/abs/2102.06725",
          "publishedOn": "2021-06-22T01:57:12.947Z",
          "wordCount": 587,
          "title": "Neural Network Libraries: A Deep Learning Framework Designed from Engineers' Perspectives. (arXiv:2102.06725v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.02684",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1\">Zhan Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Isufi_E/0/1/0/all/0/1\">Elvin Isufi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ribeiro_A/0/1/0/all/0/1\">Alejandro Ribeiro</a>",
          "description": "Graph neural networks (GNNs) model nonlinear representations in graph data\nwith applications in distributed agent coordination, control, and planning\namong others. Current GNN architectures assume ideal scenarios and ignore link\nfluctuations that occur due to environment, human factors, or external attacks.\nIn these situations, the GNN fails to address its distributed task if the\ntopological randomness is not considered accordingly. To overcome this issue,\nwe put forth the stochastic graph neural network (SGNN) model: a GNN where the\ndistributed graph convolution module accounts for the random network changes.\nSince stochasticity brings in a new learning paradigm, we conduct a statistical\nanalysis on the SGNN output variance to identify conditions the learned filters\nshould satisfy for achieving robust transference to perturbed scenarios,\nultimately revealing the explicit impact of random link losses. We further\ndevelop a stochastic gradient descent (SGD) based learning process for the SGNN\nand derive conditions on the learning rate under which this learning process\nconverges to a stationary point. Numerical results corroborate our theoretical\nfindings and compare the benefits of SGNN robust transference with a\nconventional GNN that ignores graph perturbations during learning.",
          "link": "http://arxiv.org/abs/2006.02684",
          "publishedOn": "2021-06-22T01:57:12.934Z",
          "wordCount": 630,
          "title": "Stochastic Graph Neural Networks. (arXiv:2006.02684v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.06934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lun Yiu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Cuiyun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhicong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>",
          "description": "Automatic generation of high-quality commit messages for code commits can\nsubstantially facilitate software developers' works and coordination. However,\nthe semantic gap between source code and natural language poses a major\nchallenge for the task. Several studies have been proposed to alleviate the\nchallenge but none explicitly involves code contextual information during\ncommit message generation. Specifically, existing research adopts static\nembedding for code tokens, which maps a token to the same vector regardless of\nits context. In this paper, we propose a novel Contextualized code\nrepresentation learning strategy for commit message Generation (CoreGen).\nCoreGen first learns contextualized code representations which exploit the\ncontextual information behind code commit sequences. The learned\nrepresentations of code commits built upon Transformer are then fine-tuned for\ndownstream commit message generation. Experiments on the benchmark dataset\ndemonstrate the superior effectiveness of our model over the baseline models\nwith at least 28.18% improvement in terms of BLEU-4 score. Furthermore, we also\nhighlight the future opportunities in training contextualized code\nrepresentations on larger code corpus as a solution to low-resource tasks and\nadapting the contextualized code representation framework to other code-to-text\ngeneration tasks.",
          "link": "http://arxiv.org/abs/2007.06934",
          "publishedOn": "2021-06-22T01:57:12.929Z",
          "wordCount": 673,
          "title": "CoreGen: Contextualized Code Representation Learning for Commit Message Generation. (arXiv:2007.06934v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.04515",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Shi_C/0/1/0/all/0/1\">C. Shi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1\">S. Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lu_W/0/1/0/all/0/1\">W. Lu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Song_R/0/1/0/all/0/1\">R. Song</a>",
          "description": "Reinforcement learning is a general technique that allows an agent to learn\nan optimal policy and interact with an environment in sequential decision\nmaking problems. The goodness of a policy is measured by its value function\nstarting from some initial state. The focus of this paper is to construct\nconfidence intervals (CIs) for a policy's value in infinite horizon settings\nwhere the number of decision points diverges to infinity. We propose to model\nthe action-value state function (Q-function) associated with a policy based on\nseries/sieve method to derive its confidence interval. When the target policy\ndepends on the observed data as well, we propose a SequentiAl Value Evaluation\n(SAVE) method to recursively update the estimated policy and its value\nestimator. As long as either the number of trajectories or the number of\ndecision points diverges to infinity, we show that the proposed CI achieves\nnominal coverage even in cases where the optimal policy is not unique.\nSimulation studies are conducted to back up our theoretical findings. We apply\nthe proposed method to a dataset from mobile health studies and find that\nreinforcement learning algorithms could help improve patient's health status. A\nPython implementation of the proposed procedure is available at\nhttps://github.com/shengzhang37/SAVE.",
          "link": "http://arxiv.org/abs/2001.04515",
          "publishedOn": "2021-06-22T01:57:12.913Z",
          "wordCount": 664,
          "title": "Statistical Inference of the Value Function for Reinforcement Learning in Infinite Horizon Settings. (arXiv:2001.04515v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1\">Aditya Golatkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1\">Alessandro Achille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1\">Marzia Polito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We show that the influence of a subset of the training samples can be removed\n-- or \"forgotten\" -- from the weights of a network trained on large-scale image\nclassification tasks, and we provide strong computable bounds on the amount of\nremaining information after forgetting. Inspired by real-world applications of\nforgetting techniques, we introduce a novel notion of forgetting in\nmixed-privacy setting, where we know that a \"core\" subset of the training\nsamples does not need to be forgotten. While this variation of the problem is\nconceptually simple, we show that working in this setting significantly\nimproves the accuracy and guarantees of forgetting methods applied to vision\nclassification tasks. Moreover, our method allows efficient removal of all\ninformation contained in non-core data by simply setting to zero a subset of\nthe weights with minimal loss in performance. We achieve these results by\nreplacing a standard deep network with a suitable linear approximation. With\nopportune changes to the network architecture and training procedure, we show\nthat such linear approximation achieves comparable performance to the original\nnetwork and that the forgetting problem becomes quadratic and can be solved\nefficiently even for large models. Unlike previous forgetting methods on deep\nnetworks, ours can achieve close to the state-of-the-art accuracy on large\nscale vision tasks. In particular, we show that our method allows forgetting\nwithout having to trade off the model accuracy.",
          "link": "http://arxiv.org/abs/2012.13431",
          "publishedOn": "2021-06-22T01:57:12.907Z",
          "wordCount": 696,
          "title": "Mixed-Privacy Forgetting in Deep Networks. (arXiv:2012.13431v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Renzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakala_P/0/1/0/all/0/1\">Prem Sakala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xu Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yeye He</a>",
          "description": "Entity matching (EM) refers to the problem of identifying tuple pairs in one\nor more relations that refer to the same real world entities. Supervised\nmachine learning (ML) approaches, and deep learning based approaches in\nparticular, typically achieve state-of-the-art matching results. However, these\napproaches require many labeled examples, in the form of matching and\nnon-matching pairs, which are expensive and time-consuming to label. In this\npaper, we introduce Panda, a weakly supervised system specifically designed for\nEM. Panda uses the same labeling function abstraction as Snorkel, where\nlabeling functions (LF) are user-provided programs that can generate large\namounts of (somewhat noisy) labels quickly and cheaply, which can then be\ncombined via a labeling model to generate accurate final predictions. To\nsupport users developing LFs for EM, Panda provides an integrated development\nenvironment (IDE) that lives in a modern browser architecture. Panda's IDE\nfacilitates the development, debugging, and life-cycle management of LFs in the\ncontext of EM tasks, similar to how IDEs such as Visual Studio or Eclipse excel\nin general-purpose programming. Panda's IDE includes many novel features\npurpose-built for EM, such as smart data sampling, a builtin library of EM\nutility functions, automatically generated LFs, visual debugging of LFs, and\nfinally, an EM-specific labeling model. We show in this demo that Panda IDE can\ngreatly accelerate the development of high-quality EM solutions using weak\nsupervision.",
          "link": "http://arxiv.org/abs/2106.10821",
          "publishedOn": "2021-06-22T01:57:12.901Z",
          "wordCount": 675,
          "title": "Demonstration of Panda: A Weakly Supervised Entity Matching System. (arXiv:2106.10821v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2006.10114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leimkuhler_B/0/1/0/all/0/1\">Benedict Leimkuhler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pouchon_T/0/1/0/all/0/1\">Timoth&#xe9;e Pouchon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlaar_T/0/1/0/all/0/1\">Tiffany Vlaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1\">Amos Storkey</a>",
          "description": "We propose a method for efficiently incorporating constraints into a\nstochastic gradient Langevin framework for the training of deep neural\nnetworks. Constraints allow direct control of the parameter space of the model.\nAppropriately designed, they reduce the vanishing/exploding gradient problem,\ncontrol weight magnitudes and stabilize deep neural networks and thus improve\nthe robustness of training algorithms and the generalization capabilities of\nthe trained neural network. We present examples of constrained training methods\nmotivated by orthogonality preservation for weight matrices and explicit weight\nnormalizations. We describe the methods in the overdamped formulation of\nLangevin dynamics and the underdamped form, in which momenta help to improve\nsampling efficiency. The methods are explored in test examples in image\nclassification and natural language processing.",
          "link": "http://arxiv.org/abs/2006.10114",
          "publishedOn": "2021-06-22T01:57:12.895Z",
          "wordCount": 601,
          "title": "Constraint-Based Regularization of Neural Networks. (arXiv:2006.10114v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1902.07247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rubies_Royo_V/0/1/0/all/0/1\">Vicenc Rubies-Royo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calandra_R/0/1/0/all/0/1\">Roberto Calandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stipanovic_D/0/1/0/all/0/1\">Dusan M. Stipanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomlin_C/0/1/0/all/0/1\">Claire Tomlin</a>",
          "description": "To use neural networks in safety-critical settings it is paramount to provide\nassurances on their runtime operation. Recent work on ReLU networks has sought\nto verify whether inputs belonging to a bounded box can ever yield some\nundesirable output. Input-splitting procedures, a particular type of\nverification mechanism, do so by recursively partitioning the input set into\nsmaller sets. The efficiency of these methods is largely determined by the\nnumber of splits the box must undergo before the property can be verified. In\nthis work, we propose a new technique based on shadow prices that fully\nexploits the information of the problem yielding a more efficient generation of\nsplits than the state-of-the-art. Results on the Airborne Collision Avoidance\nSystem (ACAS) benchmark verification tasks show a considerable reduction in the\npartitions generated which substantially reduces computation times. These\nresults open the door to improved verification methods for a wide variety of\nmachine learning applications including vision and control.",
          "link": "http://arxiv.org/abs/1902.07247",
          "publishedOn": "2021-06-22T01:57:12.888Z",
          "wordCount": 632,
          "title": "Fast Neural Network Verification via Shadow Prices. (arXiv:1902.07247v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Shabat_Y/0/1/0/all/0/1\">Yizhak Ben-Shabat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koneputugodage_C/0/1/0/all/0/1\">Chamin Hewa Koneputugodage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1\">Stephen Gould</a>",
          "description": "Neural shape representations have recently shown to be effective in shape\nanalysis and reconstruction tasks. Existing neural network methods require\npoint coordinates and corresponding normal vectors to learn the implicit level\nsets of the shape. Normal vectors are often not provided as raw data,\ntherefore, approximation and reorientation are required as pre-processing\nstages, both of which can introduce noise. In this paper, we propose a\ndivergence guided shape representation learning approach that does not require\nnormal vectors as input. We show that incorporating a soft constraint on the\ndivergence of the distance function favours smooth solutions that reliably\norients gradients to match the unknown normal at each point, in some cases even\nbetter than approaches that use ground truth normal vectors directly.\nAdditionally, we introduce a novel geometric initialization method for\nsinusoidal shape representation networks that further improves convergence to\nthe desired solution. We evaluate the effectiveness of our approach on the task\nof surface reconstruction and show state-of-the-art performance compared to\nother unoriented methods and on-par performance compared to oriented methods.",
          "link": "http://arxiv.org/abs/2106.10811",
          "publishedOn": "2021-06-22T01:57:12.869Z",
          "wordCount": 623,
          "title": "DiGS : Divergence guided shape implicit neural representation for unoriented point clouds. (arXiv:2106.10811v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.12391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bull_D/0/1/0/all/0/1\">David Bull</a>",
          "description": "This paper reviews the current state of the art in Artificial Intelligence\n(AI) technologies and applications in the context of the creative industries. A\nbrief background of AI, and specifically Machine Learning (ML) algorithms, is\nprovided including Convolutional Neural Network (CNNs), Generative Adversarial\nNetworks (GANs), Recurrent Neural Networks (RNNs) and Deep Reinforcement\nLearning (DRL). We categorise creative applications into five groups related to\nhow AI technologies are used: i) content creation, ii) information analysis,\niii) content enhancement and post production workflows, iv) information\nextraction and enhancement, and v) data compression. We critically examine the\nsuccesses and limitations of this rapidly advancing technology in each of these\nareas. We further differentiate between the use of AI as a creative tool and\nits potential as a creator in its own right. We foresee that, in the near\nfuture, machine learning-based AI will be adopted widely as a tool or\ncollaborative assistant for creativity. In contrast, we observe that the\nsuccesses of machine learning in domains with fewer constraints, where AI is\nthe `creator', remain modest. The potential of AI (or its developers) to win\nawards for its original creations in competition with human creatives is also\nlimited, based on contemporary technologies. We therefore conclude that, in the\ncontext of creative industries, maximum benefit from AI will be derived where\nits focus is human centric -- where it is designed to augment, rather than\nreplace, human creativity.",
          "link": "http://arxiv.org/abs/2007.12391",
          "publishedOn": "2021-06-22T01:57:12.863Z",
          "wordCount": 737,
          "title": "Artificial Intelligence in the Creative Industries: A Review. (arXiv:2007.12391v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1\">Masahiro Kato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasui_S/0/1/0/all/0/1\">Shota Yasui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAlinn_K/0/1/0/all/0/1\">Kenichiro McAlinn</a>",
          "description": "The doubly robust (DR) estimator, which consists of two nuisance parameters,\nthe conditional mean outcome and the logging policy (the probability of\nchoosing an action), is crucial in causal inference. This paper proposes a DR\nestimator for dependent samples obtained from adaptive experiments. To obtain\nan asymptotically normal semiparametric estimator from dependent samples with\nnon-Donsker nuisance estimators, we propose adaptive-fitting as a variant of\nsample-splitting. We also report an empirical paradox that our proposed DR\nestimator tends to show better performances compared to other estimators\nutilizing the true logging policy. While a similar phenomenon is known for\nestimators with i.i.d. samples, traditional explanations based on asymptotic\nefficiency cannot elucidate our case with dependent samples. We confirm this\nhypothesis through simulation studies.",
          "link": "http://arxiv.org/abs/2010.03792",
          "publishedOn": "2021-06-22T01:57:12.858Z",
          "wordCount": 632,
          "title": "The Adaptive Doubly Robust Estimator for Policy Evaluation in Adaptive Experiments and a Paradox Concerning Logging Policy. (arXiv:2010.03792v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tohalino_J/0/1/0/all/0/1\">Jorge A. V. Tohalino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amancio_D/0/1/0/all/0/1\">Diego R. Amancio</a>",
          "description": "Understanding the reasons associated with successful proposals is of\nparamount importance to improve evaluation processes. In this context, we\nanalyzed whether bibliometric features are able to predict the success of\nresearch grants. We extracted features aiming at characterizing the academic\nhistory of Brazilian researchers, including research topics, affiliations,\nnumber of publications and visibility. The extracted features were then used to\npredict grants productivity via machine learning in three major research areas,\nnamely Medicine, Dentistry and Veterinary Medicine. We found that research\nsubject and publication history play a role in predicting productivity. In\naddition, institution-based features turned out to be relevant when combined\nwith other features. While the best results outperformed text-based attributes,\nthe evaluated features were not highly discriminative. Our findings indicate\nthat predicting grants success, at least with the considered set of\nbibliometric features, is not a trivial task.",
          "link": "http://arxiv.org/abs/2106.10700",
          "publishedOn": "2021-06-22T01:57:12.853Z",
          "wordCount": 567,
          "title": "On predicting research grants productivity. (arXiv:2106.10700v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2003.10392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lemeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1\">Qi Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason D. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>",
          "description": "Developing efficient and principled neural architecture optimization methods\nis a critical challenge of modern deep learning. Recently, Liu et al.[19]\nproposed a splitting steepest descent (S2D) method that jointly optimizes the\nneural parameters and architectures based on progressively growing network\nstructures by splitting neurons into multiple copies in a steepest descent\nfashion. However, S2D suffers from a local optimality issue when all the\nneurons become \"splitting stable\", a concept akin to local stability in\nparametric optimization. In this work, we develop a significant and surprising\nextension of the splitting descent framework that addresses the local\noptimality issue. The idea is to observe that the original S2D is unnecessarily\nrestricted to splitting neurons into positive weighted copies. By simply\nallowing both positive and negative weights during splitting, we can eliminate\nthe appearance of splitting stability in S2D and hence escape the local optima\nto obtain better performance. By incorporating signed splittings, we\nsignificantly extend the optimization power of splitting steepest descent both\ntheoretically and empirically. We verify our method on various challenging\nbenchmarks such as CIFAR-100, ImageNet and ModelNet40, on which we outperform\nS2D and other advanced methods on learning accurate and energy-efficient neural\nnetworks.",
          "link": "http://arxiv.org/abs/2003.10392",
          "publishedOn": "2021-06-22T01:57:12.848Z",
          "wordCount": 698,
          "title": "Steepest Descent Neural Architecture Optimization: Escaping Local Optimum with Signed Neural Splitting. (arXiv:2003.10392v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.01663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ilahi_I/0/1/0/all/0/1\">Inaam Ilahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zia_H/0/1/0/all/0/1\">Hafiz Muhammad Abdullah Zia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahsan_M/0/1/0/all/0/1\">Muhammad Ahtazaz Ahsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabassam_R/0/1/0/all/0/1\">Rauf Tabassam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Armaghan Ahmed</a>",
          "description": "Recent advancements in deep learning have created many opportunities to solve\nreal-world problems that remained unsolved for more than a decade. Automatic\ncaption generation is a major research field, and the research community has\ndone a lot of work on it in most common languages like English. Urdu is the\nnational language of Pakistan and also much spoken and understood in the\nsub-continent region of Pakistan-India, and yet no work has been done for Urdu\nlanguage caption generation. Our research aims to fill this gap by developing\nan attention-based deep learning model using techniques of sequence modeling\nspecialized for the Urdu language. We have prepared a dataset in the Urdu\nlanguage by translating a subset of the \"Flickr8k\" dataset containing 700 'man'\nimages. We evaluate our proposed technique on this dataset and show that it can\nachieve a BLEU score of 0.83 in the Urdu language. We improve on the previous\nstate-of-the-art by using better CNN architectures and optimization techniques.\nFurthermore, we provide a discussion on how the generated captions can be made\ncorrect grammar-wise.",
          "link": "http://arxiv.org/abs/2008.01663",
          "publishedOn": "2021-06-22T01:57:12.829Z",
          "wordCount": 677,
          "title": "Efficient Urdu Caption Generation using Attention based LSTM. (arXiv:2008.01663v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.12162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaidi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>",
          "description": "Although deep neural networks (DNNs) have achieved a great success in various\ncomputer vision tasks, it is recently found that they are vulnerable to\nadversarial attacks. In this paper, we focus on the so-called \\textit{backdoor\nattack}, which injects a backdoor trigger to a small portion of training data\n(also known as data poisoning) such that the trained DNN induces\nmisclassification while facing examples with this trigger. To be specific, we\ncarefully study the effect of both real and synthetic backdoor attacks on the\ninternal response of vanilla and backdoored DNNs through the lens of Gard-CAM.\nMoreover, we show that the backdoor attack induces a significant bias in neuron\nactivation in terms of the $\\ell_\\infty$ norm of an activation map compared to\nits $\\ell_1$ and $\\ell_2$ norm. Spurred by our results, we propose the\n\\textit{$\\ell_\\infty$-based neuron pruning} to remove the backdoor from the\nbackdoored DNN. Experiments show that our method could effectively decrease the\nattack success rate, and also hold a high classification accuracy for clean\nimages.",
          "link": "http://arxiv.org/abs/2002.12162",
          "publishedOn": "2021-06-22T01:57:12.822Z",
          "wordCount": 649,
          "title": "Defending against Backdoor Attack on Deep Neural Networks. (arXiv:2002.12162v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.10159",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Coelho_C/0/1/0/all/0/1\">Claudionor N. Coelho Jr.</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kuusela_A/0/1/0/all/0/1\">Aki Kuusela</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_S/0/1/0/all/0/1\">Shan Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhuang_H/0/1/0/all/0/1\">Hao Zhuang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Aarrestad_T/0/1/0/all/0/1\">Thea Aarrestad</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Loncar_V/0/1/0/all/0/1\">Vladimir Loncar</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ngadiuba_J/0/1/0/all/0/1\">Jennifer Ngadiuba</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pierini_M/0/1/0/all/0/1\">Maurizio Pierini</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pol_A/0/1/0/all/0/1\">Adrian Alan Pol</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Summers_S/0/1/0/all/0/1\">Sioni Summers</a>",
          "description": "Although the quest for more accurate solutions is pushing deep learning\nresearch towards larger and more complex algorithms, edge devices demand\nefficient inference and therefore reduction in model size, latency and energy\nconsumption. One technique to limit model size is quantization, which implies\nusing fewer bits to represent weights and biases. Such an approach usually\nresults in a decline in performance. Here, we introduce a method for designing\noptimally heterogeneously quantized versions of deep neural network models for\nminimum-energy, high-accuracy, nanosecond inference and fully automated\ndeployment on chip. With a per-layer, per-parameter type automatic quantization\nprocedure, sampling from a wide range of quantizers, model energy consumption\nand size are minimized while high accuracy is maintained. This is crucial for\nthe event selection procedure in proton-proton collisions at the CERN Large\nHadron Collider, where resources are strictly limited and a latency of\n${\\mathcal O}(1)~\\mu$s is required. Nanosecond inference and a resource\nconsumption reduced by a factor of 50 when implemented on field-programmable\ngate array hardware are achieved.",
          "link": "http://arxiv.org/abs/2006.10159",
          "publishedOn": "2021-06-22T01:57:12.817Z",
          "wordCount": 689,
          "title": "Automatic heterogeneous quantization of deep neural networks for low-latency inference on the edge for particle detectors. (arXiv:2006.10159v3 [physics.ins-det] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.13135",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>",
          "description": "Studying the relationship between neuroanatomy and cognitive decline due to\nAlzheimer's has been a major research focus in the last decade. However, to\ninfer cause-effect relationships rather than simple associations from\nobservational data, we need to (i) express the causal relationships leading to\ncognitive decline in a graphical model, and (ii) ensure the causal effect of\ninterest is identifiable from the collected data. We derive a causal graph from\nthe current clinical knowledge on cause and effect in the Alzheimer's disease\ncontinuum, and show that identifiability of the causal effect requires all\nconfounders to be known and measured. However, in complex neuroimaging studies,\nwe neither know all potential confounders nor do we have data on them. To\nalleviate this requirement, we leverage the dependencies among multiple causes\nby deriving a substitute confounder via a probabilistic latent factor model. In\nour theoretical analysis, we prove that using the substitute confounder enables\nidentifiability of the causal effect of neuroanatomy on cognition. We\nquantitatively evaluate the effectiveness of our approach on semi-synthetic\ndata, where we know the true causal effects, and illustrate its use on real\ndata on the Alzheimer's disease continuum, where it reveals important causes\nthat otherwise would have been missed.",
          "link": "http://arxiv.org/abs/2006.13135",
          "publishedOn": "2021-06-22T01:57:12.801Z",
          "wordCount": 705,
          "title": "Estimation of Causal Effects in the Presence of Unobserved Confounding in the Alzheimer's Continuum. (arXiv:2006.13135v4 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.13136",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rodriguez_Deniz_H/0/1/0/all/0/1\">Hector Rodriguez-Deniz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Villani_M/0/1/0/all/0/1\">Mattias Villani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Voltes_Dorta_A/0/1/0/all/0/1\">Augusto Voltes-Dorta</a>",
          "description": "Dynamic transportation networks have been analyzed for years by means of\nstatic graph-based indicators in order to study the temporal evolution of\nrelevant network components, and to reveal complex dependencies that would not\nbe easily detected by a direct inspection of the data. This paper presents a\nstate-of-the-art latent network model to forecast multilayer dynamic graphs\nthat are increasingly common in transportation and proposes a community-based\nextension to reduce the computational burden. Flexible time series analysis is\nobtained by modeling the probability of edges between vertices through latent\nGaussian processes. The models and Bayesian inference are illustrated on a\nsample of 10-year data from four major airlines within the US air\ntransportation system. Results show how the estimated latent parameters from\nthe models are related to the airline's connectivity dynamics, and their\nability to project the multilayer graph into the future for out-of-sample full\nnetwork forecasts, while stochastic blockmodeling allows for the identification\nof relevant communities. Reliable network predictions would allow policy-makers\nto better understand the dynamics of the transport system, and help in their\nplanning on e.g. route development, or the deployment of new regulations.",
          "link": "http://arxiv.org/abs/1911.13136",
          "publishedOn": "2021-06-22T01:57:12.772Z",
          "wordCount": 668,
          "title": "A Multilayered Block Network Model to Forecast Large Dynamic Transportation Graphs: an Application to US Air Transport. (arXiv:1911.13136v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Queiruga_A/0/1/0/all/0/1\">Alejandro Queiruga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erichson_N/0/1/0/all/0/1\">N. Benjamin Erichson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodgkinson_L/0/1/0/all/0/1\">Liam Hodgkinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>",
          "description": "The recently-introduced class of ordinary differential equation networks\n(ODE-Nets) establishes a fruitful connection between deep learning and\ndynamical systems. In this work, we reconsider formulations of the weights as\ncontinuous-depth functions using linear combinations of basis functions. This\nperspective allows us to compress the weights through a change of basis,\nwithout retraining, while maintaining near state-of-the-art performance. In\nturn, both inference time and the memory footprint are reduced, enabling quick\nand rigorous adaptation between computational environments. Furthermore, our\nframework enables meaningful continuous-in-time batch normalization layers\nusing function projections. The performance of basis function compression is\ndemonstrated by applying continuous-depth models to (a) image classification\ntasks using convolutional units and (b) sentence-tagging tasks using\ntransformer encoder units.",
          "link": "http://arxiv.org/abs/2106.10820",
          "publishedOn": "2021-06-22T01:57:12.747Z",
          "wordCount": 552,
          "title": "Compressing Deep ODE-Nets using Basis Function Expansions. (arXiv:2106.10820v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1909.11564",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Aletti_G/0/1/0/all/0/1\">Giacomo Aletti</a>",
          "description": "This paper develops a new mathematical-statistical approach to analyze a\nclass of Flajolet-Martin algorithms (FMa), and provides analytical confidence\nintervals for the number F0 of distinct elements in a stream, based on Chernoff\nbounds. The class of FMa has reached a significant popularity in bigdata stream\nlearning, and the attention of the literature has mainly been based on\nalgorithmic aspects, basically complexity optimality, while the statistical\nanalysis of these class of algorithms has been often faced heuristically. The\nanalysis provided here shows deep connections with mathematical special\nfunctions and with extreme value theory. The latter connection may help in\nexplaining heuristic considerations, while the first opens many numerical\nissues, faced at the end of the present paper. Finally, the algorithms are\ntested on an anonymized real data stream and MonteCarlo simulations are\nprovided to support our analytical choice in this context.",
          "link": "http://arxiv.org/abs/1909.11564",
          "publishedOn": "2021-06-22T01:57:12.742Z",
          "wordCount": 620,
          "title": "Analytical confidence intervals for the number of different objects in data streams. (arXiv:1909.11564v3 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shenfeld_M/0/1/0/all/0/1\">Moshe Shenfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ligett_K/0/1/0/all/0/1\">Katrina Ligett</a>",
          "description": "Repeated use of a data sample via adaptively chosen queries can rapidly lead\nto overfitting, wherein the issued queries yield answers on the sample that\ndiffer wildly from the values of those queries on the underlying data\ndistribution. Differential privacy provides a tool to ensure generalization\ndespite adaptively-chosen queries, but its worst-case nature means that it\ncannot, for example, yield improved results for low-variance queries. In this\npaper, we give a simple new characterization that illuminates the core problem\nof adaptive data analysis. We show explicitly that the harms of adaptivity come\nfrom the covariance between the behavior of future queries and a Bayes\nfactor-based measure of how much information about the data sample was encoded\nin the responses given to past queries. We leverage this intuition to introduce\na new stability notion; we then use it to prove new generalization results for\nthe most basic noise-addition mechanisms (Laplace and Gaussian noise addition),\nwith guarantees that scale with the variance of the queries rather than the\nsquare of their range. Our characterization opens the door to new insights and\nnew algorithms for the fundamental problem of achieving generalization in\nadaptive data analysis.",
          "link": "http://arxiv.org/abs/2106.10761",
          "publishedOn": "2021-06-22T01:57:12.738Z",
          "wordCount": 625,
          "title": "Generalization in the Face of Adaptivity: A Bayesian Perspective. (arXiv:2106.10761v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fowl_L/0/1/0/all/0/1\">Liam Fowl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_P/0/1/0/all/0/1\">Ping-yeh Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czaja_W/0/1/0/all/0/1\">Wojtek Czaja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "The adversarial machine learning literature is largely partitioned into\nevasion attacks on testing data and poisoning attacks on training data. In this\nwork, we show that adversarial examples, originally intended for attacking\npre-trained models, are even more effective for data poisoning than recent\nmethods designed specifically for poisoning. Our findings indicate that\nadversarial examples, when assigned the original label of their natural base\nimage, cannot be used to train a classifier for natural images. Furthermore,\nwhen adversarial examples are assigned their adversarial class label, they are\nuseful for training. This suggests that adversarial examples contain useful\nsemantic content, just with the ``wrong'' labels (according to a network, but\nnot a human). Our method, adversarial poisoning, is substantially more\neffective than existing poisoning methods for secure dataset release, and we\nrelease a poisoned version of ImageNet, ImageNet-P, to encourage research into\nthe strength of this form of data obfuscation.",
          "link": "http://arxiv.org/abs/2106.10807",
          "publishedOn": "2021-06-22T01:57:12.732Z",
          "wordCount": 581,
          "title": "Adversarial Examples Make Strong Poisons. (arXiv:2106.10807v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Min Jae Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadik_I/0/1/0/all/0/1\">Ilias Zadik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>",
          "description": "We show a simple reduction which demonstrates the cryptographic hardness of\nlearning a single periodic neuron over isotropic Gaussian distributions in the\npresence of noise. More precisely, our reduction shows that any polynomial-time\nalgorithm (not necessarily gradient-based) for learning such functions under\nsmall noise implies a polynomial-time quantum algorithm for solving worst-case\nlattice problems, whose hardness form the foundation of lattice-based\ncryptography. Our core hard family of functions, which are well-approximated by\none-layer neural networks, take the general form of a univariate periodic\nfunction applied to an affine projection of the data. These functions have\nappeared in previous seminal works which demonstrate their hardness against\ngradient-based (Shamir'18), and Statistical Query (SQ) algorithms (Song et\nal.'17). We show that if (polynomially) small noise is added to the labels, the\nintractability of learning these functions applies to all polynomial-time\nalgorithms under the aforementioned cryptographic assumptions.\n\nMoreover, we demonstrate the necessity of noise in the hardness result by\ndesigning a polynomial-time algorithm for learning certain families of such\nfunctions under exponentially small adversarial noise. Our proposed algorithm\nis not a gradient-based or an SQ algorithm, but is rather based on the\ncelebrated Lenstra-Lenstra-Lov\\'asz (LLL) lattice basis reduction algorithm.\nFurthermore, in the absence of noise, this algorithm can be directly applied to\nsolve CLWE detection (Bruna et al.'21) and phase retrieval with an optimal\nsample complexity of $d+1$ samples. In the former case, this improves upon the\nquadratic-in-$d$ sample complexity required in (Bruna et al.'21). In the latter\ncase, this improves upon the state-of-the-art AMP-based algorithm, which\nrequires approximately $1.128d$ samples (Barbier et al.'19).",
          "link": "http://arxiv.org/abs/2106.10744",
          "publishedOn": "2021-06-22T01:57:12.717Z",
          "wordCount": 713,
          "title": "On the Cryptographic Hardness of Learning Single Periodic Neurons. (arXiv:2106.10744v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10771",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vlaar_T/0/1/0/all/0/1\">Tiffany Vlaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leimkuhler_B/0/1/0/all/0/1\">Benedict Leimkuhler</a>",
          "description": "We propose multirate training of neural networks: partitioning neural network\nparameters into \"fast\" and \"slow\" parts which are trained simultaneously using\ndifferent learning rates. By choosing appropriate partitionings we can obtain\nlarge computational speed-ups for transfer learning tasks. We show that for\nvarious transfer learning applications in vision and NLP we can fine-tune deep\nneural networks in almost half the time, without reducing the generalization\nperformance of the resulting model. We also discuss other splitting choices for\nthe neural network parameters which are beneficial in enhancing generalization\nperformance in settings where neural networks are trained from scratch.\nFinally, we propose an additional multirate technique which can learn different\nfeatures present in the data by training the full network on different time\nscales simultaneously. The benefits of using this approach are illustrated for\nResNet architectures on image data. Our paper unlocks the potential of using\nmultirate techniques for neural network training and provides many starting\npoints for future work in this area.",
          "link": "http://arxiv.org/abs/2106.10771",
          "publishedOn": "2021-06-22T01:57:12.688Z",
          "wordCount": 586,
          "title": "Multirate Training of Neural Networks. (arXiv:2106.10771v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renshen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1\">Siyang Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popat_A/0/1/0/all/0/1\">Ashok Popat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>",
          "description": "Natural reading orders of words are crucial for information extraction from\nform-like documents. Despite recent advances in Graph Convolutional Networks\n(GCNs) on modeling spatial layout patterns of documents, they have limited\nability to capture reading orders of given word-level node representations in a\ngraph. We propose Reading Order Equivariant Positional Encoding (ROPE), a new\npositional encoding technique designed to apprehend the sequential presentation\nof words in documents. ROPE generates unique reading order codes for\nneighboring words relative to the target word given a word-level graph\nconnectivity. We study two fundamental document entity extraction tasks\nincluding word labeling and word grouping on the public FUNSD dataset and a\nlarge-scale payment dataset. We show that ROPE consistently improves existing\nGCNs with a margin up to 8.4% F1-score.",
          "link": "http://arxiv.org/abs/2106.10786",
          "publishedOn": "2021-06-22T01:57:12.652Z",
          "wordCount": 580,
          "title": "ROPE: Reading Order Equivariant Positional Encoding for Graph-based Document Information Extraction. (arXiv:2106.10786v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1909.06865",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Miles Q. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_B/0/1/0/all/0/1\">Benjamin C. M. Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charland_P/0/1/0/all/0/1\">Philippe Charland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Steven H.H. Ding</a>",
          "description": "Malware currently presents a number of serious threats to computer users.\nSignature-based malware detection methods are limited in detecting new malware\nsamples that are significantly different from known ones. Therefore, machine\nlearning-based methods have been proposed, but there are two challenges these\nmethods face. The first is to model the full semantics behind the assembly code\nof malware. The second challenge is to provide interpretable results while\nkeeping excellent detection performance. In this paper, we propose an\nInterpretable MAlware Detector (I-MAD) that outperforms state-of-the-art static\nmalware detection models regarding accuracy with excellent interpretability. To\nimprove the detection performance, I-MAD incorporates a novel network component\ncalled the Galaxy Transformer network that can understand assembly code at the\nbasic block, function, and executable levels. It also incorporates our proposed\ninterpretable feed-forward neural network to provide interpretations for its\ndetection results by quantifying the impact of each feature with respect to the\nprediction. Experiment results show that our model significantly outperforms\nexisting state-of-the-art static malware detection models and presents\nmeaningful interpretations.",
          "link": "http://arxiv.org/abs/1909.06865",
          "publishedOn": "2021-06-22T01:57:12.646Z",
          "wordCount": 660,
          "title": "I-MAD: Interpretable Malware Detector Using Galaxy Transformer. (arXiv:1909.06865v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10714",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Ishtiaq_A/0/1/0/all/0/1\">Arhum Ishtiaq</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Mahmood_S/0/1/0/all/0/1\">Sara Mahmood</a>",
          "description": "For the last few decades, classical machine learning has allowed us to\nimprove the lives of many through automation, natural language processing,\npredictive analytics and much more. However, a major concern is the fact that\nwe're fast approach the threshold of the maximum possible computational\ncapacity available to us by the means of classical computing devices including\nCPUs, GPUs and Application Specific Integrated Circuits (ASICs). This is due to\nthe exponential increase in model sizes which now have parameters in the\nmagnitude of billions and trillions, requiring a significant amount of\ncomputing resources across a significant amount of time, just to converge one\nsingle model. To observe the efficacy of using quantum computing for certain\nmachine learning tasks and explore the improved potential of convergence, error\nreduction and robustness to noisy data, this paper will look forth to test and\nverify the aspects in which quantum machine learning can help improve over\nclassical machine learning approaches while also shedding light on the likely\nlimitations that have prevented quantum approaches to become the mainstream. A\nmajor focus will be to recreate the work by Farhi et al and conduct experiments\nusing their theory of performing machine learning in a quantum context, with\nassistance from the Tensorflow Quantum documentation.",
          "link": "http://arxiv.org/abs/2106.10714",
          "publishedOn": "2021-06-22T01:57:12.632Z",
          "wordCount": 631,
          "title": "Quantum Machine Learning: Fad or Future?. (arXiv:2106.10714v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/1707.08729",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zixing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Ding Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn Schuller</a>",
          "description": "Acoustic Event Classification (AEC) has become a significant task for\nmachines to perceive the surrounding auditory scene. However, extracting\neffective representations that capture the underlying characteristics of the\nacoustic events is still challenging. Previous methods mainly focused on\ndesigning the audio features in a `hand-crafted' manner. Interestingly,\ndata-learnt features have been recently reported to show better performance. Up\nto now, these were only considered on the frame level. In this article, we\npropose an unsupervised learning framework to learn a vector representation of\nan audio sequence for AEC. This framework consists of a Recurrent Neural\nNetwork (RNN) encoder and an RNN decoder, which respectively transforms the\nvariable-length audio sequence into a fixed-length vector and reconstructs the\ninput sequence on the generated vector. After training the encoder-decoder, we\nfeed the audio sequences to the encoder and then take the learnt vectors as the\naudio sequence representations. Compared with previous methods, the proposed\nmethod can not only deal with the problem of arbitrary-lengths of audio\nstreams, but also learn the salient information of the sequence. Extensive\nevaluation on a large-size acoustic event database is performed, and the\nempirical results demonstrate that the learnt audio sequence representation\nyields a significant performance improvement by a large margin compared with\nother state-of-the-art hand-crafted sequence features for AEC.",
          "link": "http://arxiv.org/abs/1707.08729",
          "publishedOn": "2021-06-22T01:57:12.625Z",
          "wordCount": 676,
          "title": "Learning audio sequence representations for acoustic event classification. (arXiv:1707.08729v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_S/0/1/0/all/0/1\">Sharu Theresa Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1\">Osvaldo Simeone</a>",
          "description": "Meta-learning optimizes the hyperparameters of a training procedure, such as\nits initialization, kernel, or learning rate, based on data sampled from a\nnumber of auxiliary tasks. A key underlying assumption is that the auxiliary\ntasks, known as meta-training tasks, share the same generating distribution as\nthe tasks to be encountered at deployment time, known as meta-test tasks. This\nmay, however, not be the case when the test environment differ from the\nmeta-training conditions. To address shifts in task generating distribution\nbetween meta-training and meta-testing phases, this paper introduces weighted\nfree energy minimization (WFEM) for transfer meta-learning. We instantiate the\nproposed approach for non-parametric Bayesian regression and classification via\nGaussian Processes (GPs). The method is validated on a toy sinusoidal\nregression problem, as well as on classification using miniImagenet and CUB\ndata sets, through comparison with standard meta-learning of GP priors as\nimplemented by PACOH.",
          "link": "http://arxiv.org/abs/2106.10711",
          "publishedOn": "2021-06-22T01:57:12.619Z",
          "wordCount": 595,
          "title": "Transfer Bayesian Meta-learning via Weighted Free Energy Minimization. (arXiv:2106.10711v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10633",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Massi_M/0/1/0/all/0/1\">Michela C. Massi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ieva_F/0/1/0/all/0/1\">Francesca Ieva</a>",
          "description": "EEG technology finds applications in several domains. Currently, most EEG\nsystems require subjects to wear several electrodes on the scalp to be\neffective. However, several channels might include noisy information, redundant\nsignals, induce longer preparation times and increase computational times of\nany automated system for EEG decoding. One way to reduce the signal-to-noise\nratio and improve classification accuracy is to combine channel selection with\nfeature extraction, but EEG signals are known to present high inter-subject\nvariability. In this work we introduce a novel algorithm for\nsubject-independent channel selection of EEG recordings. Considering\nmulti-channel trial recordings as statistical units and the EEG decoding task\nas the class of reference, the algorithm (i) exploits channel-specific\n1D-Convolutional Neural Networks (1D-CNNs) as feature extractors in a\nsupervised fashion to maximize class separability; (ii) it reduces a high\ndimensional multi-channel trial representation into a unique trial vector by\nconcatenating the channels' embeddings and (iii) recovers the complex\ninter-channel relationships during channel selection, by exploiting an ensemble\nof AutoEncoders (AE) to identify from these vectors the most relevant channels\nto perform classification. After training, the algorithm can be exploited by\ntransferring only the parametrized subgroup of selected channel-specific\n1D-CNNs to new signals from new subjects and obtain low-dimensional and highly\ninformative trial vectors to be fed to any classifier.",
          "link": "http://arxiv.org/abs/2106.10633",
          "publishedOn": "2021-06-22T01:57:12.614Z",
          "wordCount": 655,
          "title": "Learning Signal Representations for EEG Cross-Subject Channel Selection and Trial Classification. (arXiv:2106.10633v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Ling Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juang_J/0/1/0/all/0/1\">Jack Juang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiguradze_Z/0/1/0/all/0/1\">Zurab Kiguradze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_B/0/1/0/all/0/1\">Bo Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Shuai Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Songping Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_C/0/1/0/all/0/1\">Chulsoon Hwang</a>",
          "description": "Modeling and simulating a power distribution network (PDN) for printed\ncircuit boards (PCBs) with irregular board shapes and multi-layer stackup is\ncomputationally inefficient using full-wave simulations. This paper presents a\nnew concept of using deep learning for PDN impedance prediction. A boundary\nelement method (BEM) is applied to efficiently calculate the impedance for\narbitrary board shape and stackup. Then over one million boards with different\nshapes, stackup, IC location, and decap placement are randomly generated to\ntrain a deep neural network (DNN). The trained DNN can predict the impedance\naccurately for new board configurations that have not been used for training.\nThe consumed time using the trained DNN is only 0.1 seconds, which is over 100\ntimes faster than the BEM method and 5000 times faster than full-wave\nsimulations.",
          "link": "http://arxiv.org/abs/2106.10693",
          "publishedOn": "2021-06-22T01:57:12.608Z",
          "wordCount": 569,
          "title": "Fast PDN Impedance Prediction Using Deep Learning. (arXiv:2106.10693v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1910.10174",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lee_C/0/1/0/all/0/1\">Ciar&#xe1;n M. Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hart_C/0/1/0/all/0/1\">Christopher Hart</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Richens_J/0/1/0/all/0/1\">Jonathan G. Richens</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Johri_S/0/1/0/all/0/1\">Saurabh Johri</a>",
          "description": "The discovery of causal relationships is a fundamental problem in science and\nmedicine. In recent years, many elegant approaches to discovering causal\nrelationships between two variables from observational data have been proposed.\nHowever, most of these deal only with purely directed causal relationships and\ncannot detect latent common causes. Here, we devise a general heuristic which\ntakes a causal discovery algorithm that can only distinguish purely directed\ncausal relations and modifies it to also detect latent common causes. We apply\nour method to two directed causal discovery algorithms, the Information\nGeometric Causal Inference of (Daniusis et al., 2010) and the Kernel\nConditional Deviance for Causal Inference of (Mitrovic, Sejdinovic, & Teh,\n2018), and extensively test on synthetic data -- detecting latent common causes\nin additive, multiplicative and complex noise regimes -- and on real data,\nwhere we are able to detect known common causes. In addition to detecting\nlatent common causes, our experiments demonstrate that both the modified\nalgorithms preserve the performance of the original in distinguishing directed\ncausal relations.",
          "link": "http://arxiv.org/abs/1910.10174",
          "publishedOn": "2021-06-22T01:57:12.603Z",
          "wordCount": 655,
          "title": "Leveraging directed causal discovery to detect latent common causes. (arXiv:1910.10174v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10768",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Harper_D/0/1/0/all/0/1\">Daniel R. Harper</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nandy_A/0/1/0/all/0/1\">Aditya Nandy</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Arunachalam_N/0/1/0/all/0/1\">Naveen Arunachalam</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Duan_C/0/1/0/all/0/1\">Chenru Duan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Janet_J/0/1/0/all/0/1\">Jon Paul Janet</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kulik_H/0/1/0/all/0/1\">Heather J. Kulik</a>",
          "description": "Strategies for machine-learning(ML)-accelerated discovery that are general\nacross materials composition spaces are essential, but demonstrations of ML\nhave been primarily limited to narrow composition variations. By addressing the\nscarcity of data in promising regions of chemical space for challenging targets\nlike open-shell transition-metal complexes, general representations and\ntransferable ML models that leverage known relationships in existing data will\naccelerate discovery. Over a large set (ca. 1000) of isovalent transition-metal\ncomplexes, we quantify evident relationships for different properties (i.e.,\nspin-splitting and ligand dissociation) between rows of the periodic table\n(i.e., 3d/4d metals and 2p/3p ligands). We demonstrate an extension to\ngraph-based revised autocorrelation (RAC) representation (i.e., eRAC) that\nincorporates the effective nuclear charge alongside the nuclear charge\nheuristic that otherwise overestimates dissimilarity of isovalent complexes. To\naddress the common challenge of discovery in a new space where data is limited,\nwe introduce a transfer learning approach in which we seed models trained on a\nlarge amount of data from one row of the periodic table with a small number of\ndata points from the additional row. We demonstrate the synergistic value of\nthe eRACs alongside this transfer learning strategy to consistently improve\nmodel performance. Analysis of these models highlights how the approach\nsucceeds by reordering the distances between complexes to be more consistent\nwith the periodic table, a property we expect to be broadly useful for other\nmaterials domains.",
          "link": "http://arxiv.org/abs/2106.10768",
          "publishedOn": "2021-06-22T01:57:12.589Z",
          "wordCount": 678,
          "title": "Representations and Strategies for Transferable Machine Learning Models in Chemical Discovery. (arXiv:2106.10768v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2003.05738",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Devailly_F/0/1/0/all/0/1\">Fran&#xe7;ois-Xavier Devailly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larocque_D/0/1/0/all/0/1\">Denis Larocque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charlin_L/0/1/0/all/0/1\">Laurent Charlin</a>",
          "description": "Scaling adaptive traffic-signal control involves dealing with combinatorial\nstate and action spaces. Multi-agent reinforcement learning attempts to address\nthis challenge by distributing control to specialized agents. However,\nspecialization hinders generalization and transferability, and the\ncomputational graphs underlying neural-networks architectures -- dominating in\nthe multi-agent setting -- do not offer the flexibility to handle an arbitrary\nnumber of entities which changes both between road networks, and over time as\nvehicles traverse the network. We introduce Inductive Graph Reinforcement\nLearning (IG-RL) based on graph-convolutional networks which adapts to the\nstructure of any road network, to learn detailed representations of\ntraffic-controllers and their surroundings. Our decentralized approach enables\nlearning of a transferable-adaptive-traffic-signal-control policy. After being\ntrained on an arbitrary set of road networks, our model can generalize to new\nroad networks, traffic distributions, and traffic regimes, with no additional\ntraining and a constant number of parameters, enabling greater scalability\ncompared to prior methods. Furthermore, our approach can exploit the\ngranularity of available data by capturing the (dynamic) demand at both the\nlane and the vehicle levels. The proposed method is tested on both road\nnetworks and traffic settings never experienced during training. We compare\nIG-RL to multi-agent reinforcement learning and domain-specific baselines. In\nboth synthetic road networks and in a larger experiment involving the control\nof the 3,971 traffic signals of Manhattan, we show that different\ninstantiations of IG-RL outperform baselines.",
          "link": "http://arxiv.org/abs/2003.05738",
          "publishedOn": "2021-06-22T01:57:12.584Z",
          "wordCount": 734,
          "title": "IG-RL: Inductive Graph Reinforcement Learning for Massive-Scale Traffic Signal Control. (arXiv:2003.05738v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1\">Yann Dubois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloem_Reddy_B/0/1/0/all/0/1\">Benjamin Bloem-Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullrich_K/0/1/0/all/0/1\">Karen Ullrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddison_C/0/1/0/all/0/1\">Chris J. Maddison</a>",
          "description": "Most data is automatically collected and only ever \"seen\" by algorithms. Yet,\ndata compressors preserve perceptual fidelity rather than just the information\nneeded by algorithms performing downstream tasks. In this paper, we\ncharacterize the bit-rate required to ensure high performance on all predictive\ntasks that are invariant under a set of transformations, such as data\naugmentations. Based on our theory, we design unsupervised objectives for\ntraining neural compressors. Using these objectives, we train a generic image\ncompressor that achieves substantial rate savings (more than $1000\\times$ on\nImageNet) compared to JPEG on 8 datasets, without decreasing downstream\nclassification performance.",
          "link": "http://arxiv.org/abs/2106.10800",
          "publishedOn": "2021-06-22T01:57:12.577Z",
          "wordCount": 534,
          "title": "Lossy Compression for Lossless Prediction. (arXiv:2106.10800v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1\">Chi Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lydia Y. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roos_S/0/1/0/all/0/1\">Stefanie Roos</a>",
          "description": "Shapley Value is commonly adopted to measure and incentivize client\nparticipation in federated learning. In this paper, we show -- theoretically\nand through simulations -- that Shapley Value underestimates the contribution\nof a common type of client: the Maverick. Mavericks are clients that differ\nboth in data distribution and data quantity and can be the sole owners of\ncertain types of data. Selecting the right clients at the right moment is\nimportant for federated learning to reduce convergence times and improve\naccuracy. We propose FedEMD, an adaptive client selection strategy based on the\nWasserstein distance between the local and global data distributions. As FedEMD\nadapts the selection probability such that Mavericks are preferably selected\nwhen the model benefits from improvement on rare classes, it consistently\nensures the fast convergence in the presence of different types of Mavericks.\nCompared to existing strategies, including Shapley Value-based ones, FedEMD\nimproves the convergence of neural network classifiers by at least 26.9% for\nFedAvg aggregation compared with the state of the art.",
          "link": "http://arxiv.org/abs/2106.10734",
          "publishedOn": "2021-06-22T01:57:12.564Z",
          "wordCount": 614,
          "title": "Is Shapley Value fair? Improving Client Selection for Mavericks in Federated Learning. (arXiv:2106.10734v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leimkuhler_B/0/1/0/all/0/1\">Benedict Leimkuhler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlaar_T/0/1/0/all/0/1\">Tiffany Vlaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pouchon_T/0/1/0/all/0/1\">Timoth&#xe9;e Pouchon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1\">Amos Storkey</a>",
          "description": "We employ constraints to control the parameter space of deep neural networks\nthroughout training. The use of customized, appropriately designed constraints\ncan reduce the vanishing/exploding gradients problem, improve smoothness of\nclassification boundaries, control weight magnitudes and stabilize deep neural\nnetworks, and thus enhance the robustness of training algorithms and the\ngeneralization capabilities of neural networks. We provide a general approach\nto efficiently incorporate constraints into a stochastic gradient Langevin\nframework, allowing enhanced exploration of the loss landscape. We also present\nspecific examples of constrained training methods motivated by orthogonality\npreservation for weight matrices and explicit weight normalizations.\nDiscretization schemes are provided both for the overdamped formulation of\nLangevin dynamics and the underdamped form, in which momenta further improve\nsampling efficiency. These optimization schemes can be used directly, without\nneeding to adapt neural network architecture design choices or to modify the\nobjective with regularization terms, and see performance improvements in\nclassification tasks.",
          "link": "http://arxiv.org/abs/2106.10704",
          "publishedOn": "2021-06-22T01:57:12.559Z",
          "wordCount": 608,
          "title": "Better Training using Weight-Constrained Stochastic Dynamics. (arXiv:2106.10704v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1912.01238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Patrick H. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>",
          "description": "In this paper, we propose a new method to overcome catastrophic forgetting by\nadding generative regularization to Bayesian inference framework. Bayesian\nmethod provides a general framework for continual learning. We could further\nconstruct a generative regularization term for all given classification models\nby leveraging energy-based models and Langevin-dynamic sampling to enrich the\nfeatures learned in each task. By combining discriminative and generative loss\ntogether, we empirically show that the proposed method outperforms\nstate-of-the-art methods on a variety of tasks, avoiding catastrophic\nforgetting in continual learning. In particular, the proposed method\noutperforms baseline methods over 15% on the Fashion-MNIST dataset and 10% on\nthe CUB dataset",
          "link": "http://arxiv.org/abs/1912.01238",
          "publishedOn": "2021-06-22T01:57:12.553Z",
          "wordCount": 576,
          "title": "Overcoming Catastrophic Forgetting by Generative Regularization. (arXiv:1912.01238v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Freund_Y/0/1/0/all/0/1\">Yoav Freund</a>",
          "description": "We extend the drifting games analysis to continuous time and show that the\noptimal adversary, if the value function has strictly positive derivative up to\nfourth order is bronian motion.",
          "link": "http://arxiv.org/abs/2106.10717",
          "publishedOn": "2021-06-22T01:57:12.548Z",
          "wordCount": 452,
          "title": "Optimal Strategies for Decision Theoretic Online Learning. (arXiv:2106.10717v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1\">Pranesh Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karwande_A/0/1/0/all/0/1\">Atharva Karwande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolhe_T/0/1/0/all/0/1\">Tejas Kolhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamble_S/0/1/0/all/0/1\">Soham Kamble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Akshay Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wyawahare_M/0/1/0/all/0/1\">Medha Wyawahare</a>",
          "description": "One of the important and tedious task in agricultural practices is the\ndetection of the disease on crops. It requires huge time as well as skilled\nlabor. This paper proposes a smart and efficient technique for detection of\ncrop disease which uses computer vision and machine learning techniques. The\nproposed system is able to detect 20 different diseases of 5 common plants with\n93% accuracy.",
          "link": "http://arxiv.org/abs/2106.10698",
          "publishedOn": "2021-06-22T01:57:12.543Z",
          "wordCount": 513,
          "title": "Plant Disease Detection Using Image Processing and Machine Learning. (arXiv:2106.10698v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Steven Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasnejad_E/0/1/0/all/0/1\">Ehsan Abbasnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_R/0/1/0/all/0/1\">Reza Haffari</a>",
          "description": "\\textit{Differentiable ARchiTecture Search} (DARTS) has recently become the\nmainstream of neural architecture search (NAS) due to its efficiency and\nsimplicity. With a gradient-based bi-level optimization, DARTS alternately\noptimizes the inner model weights and the outer architecture parameter in a\nweight-sharing supernet. A key challenge to the scalability and quality of the\nlearned architectures is the need for differentiating through the inner-loop\noptimisation. While much has been discussed about several potentially fatal\nfactors in DARTS, the architecture gradient, a.k.a. hypergradient, has received\nless attention. In this paper, we tackle the hypergradient computation in DARTS\nbased on the implicit function theorem, making it only depends on the obtained\nsolution to the inner-loop optimization and agnostic to the optimization path.\nTo further reduce the computational requirements, we formulate a stochastic\nhypergradient approximation for differentiable NAS, and theoretically show that\nthe architecture optimization with the proposed method, named iDARTS, is\nexpected to converge to a stationary point. Comprehensive experiments on two\nNAS benchmark search spaces and the common NAS search space verify the\neffectiveness of our proposed method. It leads to architectures outperforming,\nwith large margins, those learned by the baseline methods.",
          "link": "http://arxiv.org/abs/2106.10784",
          "publishedOn": "2021-06-22T01:57:12.537Z",
          "wordCount": 624,
          "title": "iDARTS: Differentiable Architecture Search with Stochastic Implicit Gradients. (arXiv:2106.10784v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wenqing Lin</a>",
          "description": "Network embedding has been widely used in social recommendation and network\nanalysis, such as recommendation systems and anomaly detection with graphs.\nHowever, most of previous approaches cannot handle large graphs efficiently,\ndue to that (i) computation on graphs is often costly and (ii) the size of\ngraph or the intermediate results of vectors could be prohibitively large,\nrendering it difficult to be processed on a single machine. In this paper, we\npropose an efficient and effective distributed algorithm for network embedding\non large graphs using Apache Spark, which recursively partitions a graph into\nseveral small-sized subgraphs to capture the internal and external structural\ninformation of nodes, and then computes the network embedding for each subgraph\nin parallel. Finally, by aggregating the outputs on all subgraphs, we obtain\nthe embeddings of nodes in a linear cost. After that, we demonstrate in various\nexperiments that our proposed approach is able to handle graphs with billions\nof edges within a few hours and is at least 4 times faster than the\nstate-of-the-art approaches. Besides, it achieves up to $4.25\\%$ and $4.27\\%$\nimprovements on link prediction and node classification tasks respectively. In\nthe end, we deploy the proposed algorithms in two online games of Tencent with\nthe applications of friend recommendation and item recommendation, which\nimprove the competitors by up to $91.11\\%$ in running time and up to $12.80\\%$\nin the corresponding evaluation metrics.",
          "link": "http://arxiv.org/abs/2106.10620",
          "publishedOn": "2021-06-22T01:57:12.532Z",
          "wordCount": 663,
          "title": "Large-Scale Network Embedding in Apache Spark. (arXiv:2106.10620v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/1907.03452",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Beck_C/0/1/0/all/0/1\">Christian Beck</a>, <a href=\"http://arxiv.org/find/math/1/au:+Becker_S/0/1/0/all/0/1\">Sebastian Becker</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cheridito_P/0/1/0/all/0/1\">Patrick Cheridito</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jentzen_A/0/1/0/all/0/1\">Arnulf Jentzen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Neufeld_A/0/1/0/all/0/1\">Ariel Neufeld</a>",
          "description": "In this paper we introduce a numerical method for nonlinear parabolic PDEs\nthat combines operator splitting with deep learning. It divides the PDE\napproximation problem into a sequence of separate learning problems. Since the\ncomputational graph for each of the subproblems is comparatively small, the\napproach can handle extremely high-dimensional PDEs. We test the method on\ndifferent examples from physics, stochastic control and mathematical finance.\nIn all cases, it yields very good results in up to 10,000 dimensions with short\nrun times.",
          "link": "http://arxiv.org/abs/1907.03452",
          "publishedOn": "2021-06-22T01:57:12.526Z",
          "wordCount": 552,
          "title": "Deep splitting method for parabolic PDEs. (arXiv:1907.03452v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.06552",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Sannai_A/0/1/0/all/0/1\">Akiyoshi Sannai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Imaizumi_M/0/1/0/all/0/1\">Masaaki Imaizumi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kawano_M/0/1/0/all/0/1\">Makoto Kawano</a>",
          "description": "Numerous invariant (or equivariant) neural networks have succeeded in\nhandling invariant data such as point clouds and graphs. However, a\ngeneralization theory for the neural networks has not been well developed,\nbecause several essential factors for the theory, such as network size and\nmargin distribution, are not deeply connected to the invariance and\nequivariance. In this study, we develop a novel generalization error bound for\ninvariant and equivariant deep neural networks. To describe the effect of\ninvariance and equivariance on generalization, we develop a notion of a\n\\textit{quotient feature space}, which measures the effect of group actions for\nthe properties. Our main result proves that the volume of quotient feature\nspaces can describe the generalization error. Furthermore, the bound shows that\nthe invariance and equivariance significantly improve the leading term of the\nbound. We apply our result to specific invariant and equivariant networks, such\nas DeepSets (Zaheer et al. (2017)), and show that their generalization bound is\nconsiderably improved by $\\sqrt{n!}$, where $n!$ is the number of permutations.\nWe also discuss the expressive power of invariant DNNs and show that they can\nachieve an optimal approximation rate. Our experimental result supports our\ntheoretical claims.",
          "link": "http://arxiv.org/abs/1910.06552",
          "publishedOn": "2021-06-22T01:57:12.520Z",
          "wordCount": 676,
          "title": "Improved Generalization Bounds of Group Invariant / Equivariant Deep Networks via Quotient Feature Spaces. (arXiv:1910.06552v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.03924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mak_C/0/1/0/all/0/1\">Carol Mak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_C/0/1/0/all/0/1\">C.-H. Luke Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paquet_H/0/1/0/all/0/1\">Hugo Paquet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1\">Dominik Wagner</a>",
          "description": "We study the differential properties of higher-order statistical\nprobabilistic programs with recursion and conditioning. Our starting point is\nan open problem posed by Hongseok Yang: what class of statistical probabilistic\nprograms have densities that are differentiable almost everywhere? To formalise\nthe problem, we consider Statistical PCF (SPCF), an extension of call-by-value\nPCF with real numbers, and constructs for sampling and conditioning. We give\nSPCF a sampling-style operational semantics a la Borgstrom et al., and study\nthe associated weight (commonly referred to as the density) function and value\nfunction on the set of possible execution traces. Our main result is that\nalmost-surely terminating SPCF programs, generated from a set of primitive\nfunctions (e.g. the set of analytic functions) satisfying mild closure\nproperties, have weight and value functions that are almost-everywhere\ndifferentiable. We use a stochastic form of symbolic execution to reason about\nalmost-everywhere differentiability. A by-product of this work is that\nalmost-surely terminating deterministic (S)PCF programs with real parameters\ndenote functions that are almost-everywhere differentiable. Our result is of\npractical interest, as almost-everywhere differentiability of the density\nfunction is required to hold for the correctness of major gradient-based\ninference algorithms.",
          "link": "http://arxiv.org/abs/2004.03924",
          "publishedOn": "2021-06-22T01:57:12.496Z",
          "wordCount": 669,
          "title": "Densities of Almost Surely Terminating Probabilistic Programs are Differentiable Almost Everywhere. (arXiv:2004.03924v2 [cs.LO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moharrer_A/0/1/0/all/0/1\">Armin Moharrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamran_K/0/1/0/all/0/1\">Khashayar Kamran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_E/0/1/0/all/0/1\">Edmund Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ioannidis_S/0/1/0/all/0/1\">Stratis Ioannidis</a>",
          "description": "The mean squared error loss is widely used in many applications, including\nauto-encoders, multi-target regression, and matrix factorization, to name a\nfew. Despite computational advantages due to its differentiability, it is not\nrobust to outliers. In contrast, l_p norms are known to be robust, but cannot\nbe optimized via, e.g., stochastic gradient descent, as they are\nnon-differentiable. We propose an algorithm inspired by so-called model-based\noptimization (MBO) [35, 36], which replaces a non-convex objective with a\nconvex model function and alternates between optimizing the model function and\nupdating the solution. We apply this to robust regression, proposing SADM, a\nstochastic variant of the Online Alternating Direction Method of Multipliers\n(OADM) [50] to solve the inner optimization in MBO. We show that SADM converges\nwith the rate O(log T/T). Finally, we demonstrate experimentally (a) the\nrobustness of l_p norms to outliers and (b) the efficiency of our proposed\nmodel-based algorithms in comparison with gradient methods on autoencoders and\nmulti-target regression.",
          "link": "http://arxiv.org/abs/2106.10759",
          "publishedOn": "2021-06-22T01:57:12.473Z",
          "wordCount": 591,
          "title": "Robust Regression via Model Based Methods. (arXiv:2106.10759v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chanyaswad_T/0/1/0/all/0/1\">Thee Chanyaswad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">J. Morris Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kung_S/0/1/0/all/0/1\">S.Y. Kung</a>",
          "description": "As the analytic tools become more powerful, and more data are generated on a\ndaily basis, the issue of data privacy arises. This leads to the study of the\ndesign of privacy-preserving machine learning algorithms. Given two objectives,\nnamely, utility maximization and privacy-loss minimization, this work is based\non two previously non-intersecting regimes -- Compressive Privacy and\nmulti-kernel method. Compressive Privacy is a privacy framework that employs\nutility-preserving lossy-encoding scheme to protect the privacy of the data,\nwhile multi-kernel method is a kernel based machine learning regime that\nexplores the idea of using multiple kernels for building better predictors. The\ncompressive multi-kernel method proposed consists of two stages -- the\ncompression stage and the multi-kernel stage. The compression stage follows the\nCompressive Privacy paradigm to provide the desired privacy protection. Each\nkernel matrix is compressed with a lossy projection matrix derived from the\nDiscriminant Component Analysis (DCA). The multi-kernel stage uses the\nsignal-to-noise ratio (SNR) score of each kernel to non-uniformly combine\nmultiple compressive kernels. The proposed method is evaluated on two\nmobile-sensing datasets -- MHEALTH and HAR -- where activity recognition is\ndefined as utility and person identification is defined as privacy. The results\nshow that the compression regime is successful in privacy preservation as the\nprivacy classification accuracies are almost at the random-guess level in all\nexperiments. On the other hand, the novel SNR-based multi-kernel shows utility\nclassification accuracy improvement upon the state-of-the-art in both datasets.\nThese results indicate a promising direction for research in privacy-preserving\nmachine learning.",
          "link": "http://arxiv.org/abs/2106.10671",
          "publishedOn": "2021-06-22T01:57:12.464Z",
          "wordCount": 702,
          "title": "A compressive multi-kernel method for privacy-preserving machine learning. (arXiv:2106.10671v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10669",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+John_J/0/1/0/all/0/1\">Jacob John</a>",
          "description": "Outlier detection is a significant area in data mining. It can be either used\nto pre-process the data prior to an analysis or post the processing phase\n(before visualization) depending on the effectiveness of the outlier and its\nimportance. Outlier detection extends to several fields such as detection of\ncredit card fraud, network intrusions, machine failure prediction, potential\nterrorist attacks, and so on. Outliers are those data points with\ncharacteristics considerably different. They deviate from the data set causing\ninconsistencies, noise and anomalies during analysis and result in modification\nof the original points However, a common misconception is that outliers have to\nbe immediately eliminated or replaced from the data set. Such points could be\nconsidered useful if analyzed separately as they could be obtained from a\nseparate mechanism entirely making it important to the research question. This\nstudy surveys the different methods of outlier detection for spatial analysis.\nSpatial data or geospatial data are those that exhibit geographic properties or\nattributes such as position or areas. An example would be weather data such as\nprecipitation, temperature, wind velocity, and so on collected for a defined\nregion.",
          "link": "http://arxiv.org/abs/2106.10669",
          "publishedOn": "2021-06-22T01:57:12.456Z",
          "wordCount": 615,
          "title": "Outlier Detection and Spatial Analysis Algorithms. (arXiv:2106.10669v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jalwana_M/0/1/0/all/0/1\">Mohammad A. A. K. Jalwana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>",
          "description": "Backpropagation image saliency aims at explaining model predictions by\nestimating model-centric importance of individual pixels in the input. However,\nclass-insensitivity of the earlier layers in a network only allows saliency\ncomputation with low resolution activation maps of the deeper layers, resulting\nin compromised image saliency. Remedifying this can lead to sanity failures. We\npropose CAMERAS, a technique to compute high-fidelity backpropagation saliency\nmaps without requiring any external priors and preserving the map sanity. Our\nmethod systematically performs multi-scale accumulation and fusion of the\nactivation maps and backpropagated gradients to compute precise saliency maps.\nFrom accurate image saliency to articulation of relative importance of input\nfeatures for different models, and precise discrimination between model\nperception of visually similar objects, our high-resolution mapping offers\nmultiple novel insights into the black-box deep visual models, which are\npresented in the paper. We also demonstrate the utility of our saliency maps in\nadversarial setup by drastically reducing the norm of attack signals by\nfocusing them on the precise regions identified by our maps. Our method also\ninspires new evaluation metrics and a sanity check for this developing research\ndirection. Code is available here https://github.com/VisMIL/CAMERAS",
          "link": "http://arxiv.org/abs/2106.10649",
          "publishedOn": "2021-06-22T01:57:12.447Z",
          "wordCount": 652,
          "title": "CAMERAS: Enhanced Resolution And Sanity preserving Class Activation Mapping for image saliency. (arXiv:2106.10649v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Subhankar Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhiming Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>",
          "description": "In this paper, we address Novel Class Discovery (NCD), the task of unveiling\nnew classes in a set of unlabeled samples given a labeled dataset with known\nclasses. We exploit the peculiarities of NCD to build a new framework, named\nNeighborhood Contrastive Learning (NCL), to learn discriminative\nrepresentations that are important to clustering performance. Our contribution\nis twofold. First, we find that a feature extractor trained on the labeled set\ngenerates representations in which a generic query sample and its neighbors are\nlikely to share the same class. We exploit this observation to retrieve and\naggregate pseudo-positive pairs with contrastive learning, thus encouraging the\nmodel to learn more discriminative representations. Second, we notice that most\nof the instances are easily discriminated by the network, contributing less to\nthe contrastive loss. To overcome this issue, we propose to generate hard\nnegatives by mixing labeled and unlabeled samples in the feature space. We\nexperimentally demonstrate that these two ingredients significantly contribute\nto clustering performance and lead our model to outperform state-of-the-art\nmethods by a large margin (e.g., clustering accuracy +13% on CIFAR-100 and +8%\non ImageNet).",
          "link": "http://arxiv.org/abs/2106.10731",
          "publishedOn": "2021-06-22T01:57:12.431Z",
          "wordCount": 632,
          "title": "Neighborhood Contrastive Learning for Novel Class Discovery. (arXiv:2106.10731v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jozsa_M/0/1/0/all/0/1\">M&#xe1;t&#xe9; J&#xf3;zsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazar_A/0/1/0/all/0/1\">Alp&#xe1;r S. L&#xe1;z&#xe1;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazar_Z/0/1/0/all/0/1\">Zsolt I. L&#xe1;z&#xe1;r</a>",
          "description": "Based on a large dataset containing thousands of real-world networks ranging\nfrom genetic, protein interaction, and metabolic networks to brain, language,\necology, and social networks we search for defining structural measures of the\ndifferent complex network domains (CND). We calculate 208 measures for all\nnetworks and using a comprehensive and scrupulous workflow of statistical and\nmachine learning methods we investigated the limitations and possibilities of\nidentifying the key graph measures of CNDs. Our approach managed to identify\nwell distinguishable groups of network domains and confer their relevant\nfeatures. These features turn out to be CND specific and not unique even at the\nlevel of individual CNDs. The presented methodology may be applied to other\nsimilar scenarios involving highly unbalanced and skewed datasets.",
          "link": "http://arxiv.org/abs/2106.10753",
          "publishedOn": "2021-06-22T01:57:12.406Z",
          "wordCount": 577,
          "title": "Opportunities and challenges in partitioning the graph measure space of real-world networks. (arXiv:2106.10753v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Junwei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1\">Qiaozhu Mei</a>",
          "description": "Graph neural networks (GNNs) have attracted increasing interests. With broad\ndeployments of GNNs in real-world applications, there is an urgent need for\nunderstanding the robustness of GNNs under adversarial attacks, especially in\nrealistic setups. In this work, we study the problem of attacking GNNs in a\nrestricted and realistic setup, by perturbing the features of a small set of\nnodes, with no access to model parameters and model predictions. Our formal\nanalysis draws a connection between this type of attacks and an influence\nmaximization problem on the graph. This connection not only enhances our\nunderstanding on the problem of adversarial attack on GNNs, but also allows us\nto propose a group of effective and practical attack strategies. Our\nexperiments verify that the proposed attack strategies significantly degrade\nthe performance of three popular GNN models and outperform baseline adversarial\nattack strategies.",
          "link": "http://arxiv.org/abs/2106.10785",
          "publishedOn": "2021-06-22T01:57:12.400Z",
          "wordCount": 578,
          "title": "Adversarial Attack on Graph Neural Networks as An Influence Maximization Problem. (arXiv:2106.10785v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Runqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baochang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_L/0/1/0/all/0/1\">Li&#x27;an Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1\">David Doermann</a>",
          "description": "Conventional gradient descent methods compute the gradients for multiple\nvariables through the partial derivative. Treating the coupled variables\nindependently while ignoring the interaction, however, leads to an insufficient\noptimization for bilinear models. In this paper, we propose a dependable\nlearning based on Cogradient Descent (CoGD) algorithm to address the bilinear\noptimization problem, providing a systematic way to coordinate the gradients of\ncoupling variables based on a kernelized projection function. CoGD is\nintroduced to solve bilinear problems when one variable is with sparsity\nconstraint, as often occurs in modern learning paradigms. CoGD can also be used\nto decompose the association of features and weights, which further generalizes\nour method to better train convolutional neural networks (CNNs) and improve the\nmodel capacity. CoGD is applied in representative bilinear problems, including\nimage reconstruction, image inpainting, network pruning and CNN training.\nExtensive experiments show that CoGD improves the state-of-the-arts by\nsignificant margins. Code is available at\n{https://github.com/bczhangbczhang/CoGD}.",
          "link": "http://arxiv.org/abs/2106.10617",
          "publishedOn": "2021-06-22T01:57:12.373Z",
          "wordCount": 590,
          "title": "Cogradient Descent for Dependable Learning. (arXiv:2106.10617v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leist_A/0/1/0/all/0/1\">Anja K. Leist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klee_M/0/1/0/all/0/1\">Matthias Klee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jung Hyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehkopf_D/0/1/0/all/0/1\">David H. Rehkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bordas_S/0/1/0/all/0/1\">St&#xe9;phane P. A. Bordas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muniz_Terrera_G/0/1/0/all/0/1\">Graciela Muniz-Terrera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wade_S/0/1/0/all/0/1\">Sara Wade</a>",
          "description": "The uptake of machine learning (ML) approaches in the social and health\nsciences has been rather slow, and research using ML for social and health\nresearch questions remains fragmented. This may be due to the separate\ndevelopment of research in the computational/data versus social and health\nsciences as well as a lack of accessible overviews and adequate training in ML\ntechniques for non data science researchers. This paper provides a meta-mapping\nof research questions in the social and health sciences to appropriate ML\napproaches, by incorporating the necessary requirements to statistical analysis\nin these disciplines. We map the established classification into description,\nprediction, and causal inference to common research goals, such as estimating\nprevalence of adverse health or social outcomes, predicting the risk of an\nevent, and identifying risk factors or causes of adverse outcomes. This\nmeta-mapping aims at overcoming disciplinary barriers and starting a fluid\ndialogue between researchers from the social and health sciences and\nmethodologically trained researchers. Such mapping may also help to fully\nexploit the benefits of ML while considering domain-specific aspects relevant\nto the social and health sciences, and hopefully contribute to the acceleration\nof the uptake of ML applications to advance both basic and applied social and\nhealth sciences research.",
          "link": "http://arxiv.org/abs/2106.10716",
          "publishedOn": "2021-06-22T01:57:12.368Z",
          "wordCount": 644,
          "title": "Machine learning in the social and health sciences. (arXiv:2106.10716v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Nhan Khanh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1\">Quang Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangzhou Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1\">Quanwei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirche_S/0/1/0/all/0/1\">Sandra Hirche</a>",
          "description": "Federated learning is the distributed machine learning framework that enables\ncollaborative training across multiple parties while ensuring data privacy.\nPractical adaptation of XGBoost, the state-of-the-art tree boosting framework,\nto federated learning remains limited due to high cost incurred by conventional\nprivacy-preserving methods. To address the problem, we propose two variants of\nfederated XGBoost with privacy guarantee: FedXGBoost-SMM and FedXGBoost-LDP.\nOur first protocol FedXGBoost-SMM deploys enhanced secure matrix multiplication\nmethod to preserve privacy with lossless accuracy and lower overhead than\nencryption-based techniques. Developed independently, the second protocol\nFedXGBoost-LDP is heuristically designed with noise perturbation for local\ndifferential privacy, and empirically evaluated on real-world and synthetic\ndatasets.",
          "link": "http://arxiv.org/abs/2106.10662",
          "publishedOn": "2021-06-22T01:57:12.362Z",
          "wordCount": 544,
          "title": "FedXGBoost: Privacy-Preserving XGBoost for Federated Learning. (arXiv:2106.10662v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turgutlu_K/0/1/0/all/0/1\">Kerem Turgutlu</a>",
          "description": "Existing computer vision research in categorization struggles with\nfine-grained attributes recognition due to the inherently high intra-class\nvariances and low inter-class variances. SOTA methods tackle this challenge by\nlocating the most informative image regions and rely on them to classify the\ncomplete image. The most recent work, Vision Transformer (ViT), shows its\nstrong performance in both traditional and fine-grained classification tasks.\nIn this work, we propose a multi-stage ViT framework for fine-grained image\nclassification tasks, which localizes the informative image regions without\nrequiring architectural changes using the inherent multi-head self-attention\nmechanism. We also introduce attention-guided augmentations for improving the\nmodel's capabilities. We demonstrate the value of our approach by experimenting\nwith four popular fine-grained benchmarks: CUB-200-2011, Stanford Cars,\nStanford Dogs, and FGVC7 Plant Pathology. We also prove our model's\ninterpretability via qualitative results.",
          "link": "http://arxiv.org/abs/2106.10587",
          "publishedOn": "2021-06-22T01:57:12.347Z",
          "wordCount": 594,
          "title": "Exploring Vision Transformers for Fine-grained Classification. (arXiv:2106.10587v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingfeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>",
          "description": "Graphically-rich applications such as games are ubiquitous with attractive\nvisual effects of Graphical User Interface (GUI) that offers a bridge between\nsoftware applications and end-users. However, various types of graphical\nglitches may arise from such GUI complexity and have become one of the main\ncomponent of software compatibility issues. Our study on bug reports from game\ndevelopment teams in NetEase Inc. indicates that graphical glitches frequently\noccur during the GUI rendering and severely degrade the quality of\ngraphically-rich applications such as video games. Existing automated testing\ntechniques for such applications focus mainly on generating various GUI test\nsequences and check whether the test sequences can cause crashes. These\ntechniques require constant human attention to captures non-crashing bugs such\nas bugs causing graphical glitches. In this paper, we present the first step in\nautomating the test oracle for detecting non-crashing bugs in graphically-rich\napplications. Specifically, we propose \\texttt{GLIB} based on a code-based data\naugmentation technique to detect game GUI glitches. We perform an evaluation of\n\\texttt{GLIB} on 20 real-world game apps (with bug reports available) and the\nresult shows that \\texttt{GLIB} can achieve 100\\% precision and 99.5\\% recall\nin detecting non-crashing bugs such as game GUI glitches. Practical application\nof \\texttt{GLIB} on another 14 real-world games (without bug reports) further\ndemonstrates that \\texttt{GLIB} can effectively uncover GUI glitches, with 48\nof 53 bugs reported by \\texttt{GLIB} having been confirmed and fixed so far.",
          "link": "http://arxiv.org/abs/2106.10507",
          "publishedOn": "2021-06-22T01:57:11.808Z",
          "wordCount": 689,
          "title": "GLIB: Towards Automated Test Oracle for Graphically-Rich Applications. (arXiv:2106.10507v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2002.03580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haoyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kai Zheng</a>",
          "description": "In this paper, we investigate the non-stationary combinatorial semi-bandit\nproblem, both in the switching case and in the dynamic case. In the general\ncase where (a) the reward function is non-linear, (b) arms may be\nprobabilistically triggered, and (c) only approximate offline oracle exists\n\\cite{wang2017improving}, our algorithm achieves\n$\\tilde{\\mathcal{O}}(\\sqrt{\\mathcal{S} T})$ distribution-dependent regret in\nthe switching case, and $\\tilde{\\mathcal{O}}(\\mathcal{V}^{1/3}T^{2/3})$ in the\ndynamic case, where $\\mathcal S$ is the number of switchings and $\\mathcal V$\nis the sum of the total ``distribution changes''. The regret bounds in both\nscenarios are nearly optimal, but our algorithm needs to know the parameter\n$\\mathcal S$ or $\\mathcal V$ in advance.\n\nWe further show that by employing another technique, our algorithm no longer\nneeds to know the parameters $\\mathcal S$ or $\\mathcal V$ but the regret bounds\ncould become suboptimal.\n\nIn a special case where the reward function is linear and we have an exact\noracle, we design a parameter-free algorithm that achieves nearly optimal\nregret both in the switching case and in the dynamic case without knowing the\nparameters in advance.",
          "link": "http://arxiv.org/abs/2002.03580",
          "publishedOn": "2021-06-22T01:57:11.803Z",
          "wordCount": 655,
          "title": "Combinatorial Semi-Bandit in the Non-Stationary Environment. (arXiv:2002.03580v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiapeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1\">Guozhi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Weihong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1\">Kai Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yichao Huang</a>",
          "description": "Visual information extraction (VIE) has attracted increasing attention in\nrecent years. The existing methods usually first organized optical character\nrecognition (OCR) results into plain texts and then utilized token-level entity\nannotations as supervision to train a sequence tagging model. However, it\nexpends great annotation costs and may be exposed to label confusion, and the\nOCR errors will also significantly affect the final performance. In this paper,\nwe propose a unified weakly-supervised learning framework called TCPN (Tag,\nCopy or Predict Network), which introduces 1) an efficient encoder to\nsimultaneously model the semantic and layout information in 2D OCR results; 2)\na weakly-supervised training strategy that utilizes only key information\nsequences as supervision; and 3) a flexible and switchable decoder which\ncontains two inference modes: one (Copy or Predict Mode) is to output key\ninformation sequences of different categories by copying a token from the input\nor predicting one in each time step, and the other (Tag Mode) is to directly\ntag the input sequence in a single forward pass. Our method shows new\nstate-of-the-art performance on several public benchmarks, which fully proves\nits effectiveness.",
          "link": "http://arxiv.org/abs/2106.10681",
          "publishedOn": "2021-06-22T01:57:11.798Z",
          "wordCount": 656,
          "title": "Tag, Copy or Predict: A Unified Weakly-Supervised Learning Framework for Visual Information Extraction using Sequences. (arXiv:2106.10681v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jongmin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_W/0/1/0/all/0/1\">Wonseok Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Byung-Jun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kee-Eung Kim</a>",
          "description": "We consider the offline reinforcement learning (RL) setting where the agent\naims to optimize the policy solely from the data without further environment\ninteractions. In offline RL, the distributional shift becomes the primary\nsource of difficulty, which arises from the deviation of the target policy\nbeing optimized from the behavior policy used for data collection. This\ntypically causes overestimation of action values, which poses severe problems\nfor model-free algorithms that use bootstrapping. To mitigate the problem,\nprior offline RL algorithms often used sophisticated techniques that encourage\nunderestimation of action values, which introduces an additional set of\nhyperparameters that need to be tuned properly. In this paper, we present an\noffline RL algorithm that prevents overestimation in a more principled way. Our\nalgorithm, OptiDICE, directly estimates the stationary distribution corrections\nof the optimal policy and does not rely on policy-gradients, unlike previous\noffline RL algorithms. Using an extensive set of benchmark datasets for offline\nRL, we show that OptiDICE performs competitively with the state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2106.10783",
          "publishedOn": "2021-06-22T01:57:11.793Z",
          "wordCount": 610,
          "title": "OptiDICE: Offline Policy Optimization via Stationary Distribution Correction Estimation. (arXiv:2106.10783v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farseev_A/0/1/0/all/0/1\">Aleksandr Farseev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1\">Andrey Filchenkov</a>",
          "description": "Human personality traits are the key drivers behind our decision-making,\ninfluencing our life path on a daily basis. Inference of personality traits,\nsuch as Myers-Briggs Personality Type, as well as an understanding of\ndependencies between personality traits and users' behavior on various social\nmedia platforms is of crucial importance to modern research and industry\napplications. The emergence of diverse and cross-purpose social media avenues\nmakes it possible to perform user personality profiling automatically and\nefficiently based on data represented across multiple data modalities. However,\nthe research efforts on personality profiling from multi-source multi-modal\nsocial media data are relatively sparse, and the level of impact of different\nsocial network data on machine learning performance has yet to be\ncomprehensively evaluated. Furthermore, there is not such dataset in the\nresearch community to benchmark. This study is one of the first attempts\ntowards bridging such an important research gap. Specifically, in this work, we\ninfer the Myers-Briggs Personality Type indicators, by applying a novel\nmulti-view fusion framework, called \"PERS\" and comparing the performance\nresults not just across data modalities but also with respect to different\nsocial network data sources. Our experimental results demonstrate the PERS's\nability to learn from multi-view data for personality profiling by efficiently\nleveraging on the significantly different data arriving from diverse social\nmultimedia sources. We have also found that the selection of a machine learning\napproach is of crucial importance when choosing social network data sources and\nthat people tend to reveal multiple facets of their personality in different\nsocial media avenues. Our released social multimedia dataset facilitates future\nresearch on this direction.",
          "link": "http://arxiv.org/abs/2106.10673",
          "publishedOn": "2021-06-22T01:57:11.780Z",
          "wordCount": 714,
          "title": "Two-Faced Humans on Twitter and Facebook: Harvesting Social Multimedia for Human Personality Profiling. (arXiv:2106.10673v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shixiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiuyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yao Xie</a>",
          "description": "Self- and mutually-exciting point processes are popular models in machine\nlearning and statistics for dependent discrete event data. To date, most\nexisting models assume stationary kernels (including the classical Hawkes\nprocesses) and simple parametric models. Modern applications with complex event\ndata require more general point process models that can incorporate contextual\ninformation of the events, called marks, besides the temporal and location\ninformation. Moreover, such applications often require non-stationary models to\ncapture more complex spatio-temporal dependence. To tackle these challenges, a\nkey question is to devise a versatile influence kernel in the point process\nmodel. In this paper, we introduce a novel and general neural network-based\nnon-stationary influence kernel with high expressiveness for handling complex\ndiscrete events data while providing theoretical performance guarantees. We\ndemonstrate the superior performance of our proposed method compared with the\nstate-of-the-art on synthetic and real data.",
          "link": "http://arxiv.org/abs/2106.10773",
          "publishedOn": "2021-06-22T01:57:11.774Z",
          "wordCount": 570,
          "title": "Neural Spectral Marked Point Processes. (arXiv:2106.10773v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10533",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Djeumou_F/0/1/0/all/0/1\">Franck Djeumou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>",
          "description": "We develop a learning-based control algorithm for unknown dynamical systems\nunder very severe data limitations. Specifically, the algorithm has access to\nstreaming data only from a single and ongoing trial. Despite the scarcity of\ndata, we show -- through a series of examples -- that the algorithm can provide\nperformance comparable to reinforcement learning algorithms trained over\nmillions of environment interactions. It accomplishes such performance by\neffectively leveraging various forms of side information on the dynamics to\nreduce the sample complexity. Such side information typically comes from\nelementary laws of physics and qualitative properties of the system. More\nprecisely, the algorithm approximately solves an optimal control problem\nencoding the system's desired behavior. To this end, it constructs and refines\na differential inclusion that contains the unknown vector field of the\ndynamics. The differential inclusion, used in an interval Taylor-based method,\nenables to over-approximate the set of states the system may reach.\nTheoretically, we establish a bound on the suboptimality of the approximate\nsolution with respect to the case of known dynamics. We show that the longer\nthe trial or the more side information is available, the tighter the bound.\nEmpirically, experiments in a high-fidelity F-16 aircraft simulator and\nMuJoCo's environments such as the Reacher, Swimmer, and Cheetah illustrate the\nalgorithm's effectiveness.",
          "link": "http://arxiv.org/abs/2106.10533",
          "publishedOn": "2021-06-22T01:57:11.767Z",
          "wordCount": 682,
          "title": "Learning to Reach, Swim, Walk and Fly in One Trial: Data-Driven Control with Scarce Data and Side Information. (arXiv:2106.10533v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DAngelo_F/0/1/0/all/0/1\">Francesco D&#x27;Angelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenzel_F/0/1/0/all/0/1\">Florian Wenzel</a>",
          "description": "Ensembles of deep neural networks have achieved great success recently, but\nthey do not offer a proper Bayesian justification. Moreover, while they allow\nfor averaging of predictions over several hypotheses, they do not provide any\nguarantees for their diversity, leading to redundant solutions in function\nspace. In contrast, particle-based inference methods, such as Stein variational\ngradient descent (SVGD), offer a Bayesian framework, but rely on the choice of\na kernel to measure the similarity between ensemble members. In this work, we\nstudy different SVGD methods operating in the weight space, function space, and\nin a hybrid setting. % Defining the kernel directly on the neural network\nfunctions seems promising to overcome the limitations of deep ensembles. %\nHowever, ensuring diversity in function space while maintaining SVGD's\ntheoretical guarantees is not trivial. % In this work, we provide an overview\nover different ensembling and SVGD methods in weight space and function space\nand propose new and assess their theoretical and empirical properties on\nsynthetic and real-world tasks. We compare the SVGD approaches to other\nensembling-based methods in terms of their theoretical properties and assess\ntheir empirical performance on synthetic and real-world tasks. We find that\nSVGD using functional and hybrid kernels can overcome the limitations of deep\nensembles. It improves on functional diversity and uncertainty estimation and\napproaches the true Bayesian posterior more closely. Moreover, we show that\nusing stochastic SVGD updates, as opposed to the standard deterministic ones,\ncan further improve the performance.",
          "link": "http://arxiv.org/abs/2106.10760",
          "publishedOn": "2021-06-22T01:57:11.762Z",
          "wordCount": 667,
          "title": "On Stein Variational Neural Network Ensembles. (arXiv:2106.10760v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.07312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huber_T/0/1/0/all/0/1\">Tobias Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Limmer_B/0/1/0/all/0/1\">Benedikt Limmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1\">Elisabeth Andr&#xe9;</a>",
          "description": "Recent years saw a plethora of work on explaining complex intelligent agents.\nOne example is the development of several algorithms that generate saliency\nmaps which show how much each pixel attributed to the agents' decision.\nHowever, most evaluations of such saliency maps focus on image classification\ntasks. As far as we know, there is no work that thoroughly compares different\nsaliency maps for Deep Reinforcement Learning agents. This paper compares four\nperturbation-based approaches to create saliency maps for Deep Reinforcement\nLearning agents trained on four different Atari 2600 games. All four approaches\nwork by perturbing parts of the input and measuring how much this affects the\nagent's output. The approaches are compared using three computational metrics:\ndependence on the learned parameters of the agent (sanity checks), faithfulness\nto the agent's reasoning (input degradation), and run-time. In particular,\nduring the sanity checks we find issues with two approaches and propose a\nsolution to fix one of those issues.",
          "link": "http://arxiv.org/abs/2101.07312",
          "publishedOn": "2021-06-22T01:57:11.756Z",
          "wordCount": 621,
          "title": "Benchmarking Perturbation-based Saliency Maps for Explaining Atari Agents. (arXiv:2101.07312v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yicong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atia_G/0/1/0/all/0/1\">George K. Atia</a>",
          "description": "Tensor completion is the problem of estimating the missing values of\nhigh-order data from partially observed entries. Among several definitions of\ntensor rank, tensor ring rank affords the flexibility and accuracy needed to\nmodel tensors of different orders, which motivated recent efforts on\ntensor-ring completion. However, data corruption due to prevailing outliers\nposes major challenges to existing algorithms. In this paper, we develop a\nrobust approach to tensor ring completion that uses an M-estimator as its error\nstatistic, which can significantly alleviate the effect of outliers. Leveraging\na half-quadratic (HQ) method, we reformulate the problem as one of weighted\ntensor completion. We present two HQ-based algorithms based on truncated\nsingular value decomposition and matrix factorization along with their\nconvergence and complexity analysis. Extendibility of the proposed approach to\nalternative definitions of tensor rank is also discussed. The experimental\nresults demonstrate the superior performance of the proposed approach over\nstate-of-the-art robust algorithms for tensor completion.",
          "link": "http://arxiv.org/abs/2106.10422",
          "publishedOn": "2021-06-22T01:57:11.741Z",
          "wordCount": 588,
          "title": "Robust M-estimation-based Tensor Ring Completion: a Half-quadratic Minimization Approach. (arXiv:2106.10422v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stewart_C/0/1/0/all/0/1\">Charles V. Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parham_J/0/1/0/all/0/1\">Jason R. Parham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holmberg_J/0/1/0/all/0/1\">Jason Holmberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_Wolf_T/0/1/0/all/0/1\">Tanya Y. Berger-Wolf</a>",
          "description": "Hoping to stimulate new research in individual animal identification from\nimages, we propose to formulate the problem as the human-machine Continual\nCuration of images and animal identities. This is an open world recognition\nproblem, where most new animals enter the system after its algorithms are\ninitially trained and deployed. Continual Curation, as defined here, requires\n(1) an improvement in the effectiveness of current recognition methods, (2) a\npairwise verification algorithm that allows the possibility of no decision, and\n(3) an algorithmic decision mechanism that seeks human input to guide the\ncuration process. Error metrics must evaluate the ability of recognition\nalgorithms to identify not only animals that have been seen just once or twice\nbut also recognize new animals not in the database. An important measure of\noverall system performance is accuracy as a function of the amount of human\ninput required.",
          "link": "http://arxiv.org/abs/2106.10377",
          "publishedOn": "2021-06-22T01:57:11.736Z",
          "wordCount": 590,
          "title": "The Animal ID Problem: Continual Curation. (arXiv:2106.10377v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10558",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Webber_R/0/1/0/all/0/1\">Robert J. Webber</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lindsey_M/0/1/0/all/0/1\">Michael Lindsey</a>",
          "description": "Variational Monte Carlo (VMC) is an approach for computing ground-state\nwavefunctions that has recently become more powerful due to the introduction of\nneural network-based wavefunction parametrizations. However, efficiently\ntraining neural wavefunctions to converge to an energy minimum remains a\ndifficult problem. In this work, we analyze optimization and sampling methods\nused in VMC and introduce alterations to improve their performance. First,\nbased on theoretical convergence analysis in a noiseless setting, we motivate a\nnew optimizer that we call the Rayleigh-Gauss-Newton method, which can improve\nupon gradient descent and natural gradient descent to achieve superlinear\nconvergence. Second, in order to realize this favorable comparison in the\npresence of stochastic noise, we analyze the effect of sampling error on VMC\nparameter updates and experimentally demonstrate that it can be reduced by the\nparallel tempering method. In particular, we demonstrate that RGN can be made\nrobust to energy spikes that occur when new regions of configuration space\nbecome available to the sampler over the course of optimization. Finally,\nputting theory into practice, we apply our enhanced optimization and sampling\nmethods to the transverse-field Ising and XXZ models on large lattices,\nyielding ground-state energy estimates with remarkably high accuracy after just\n200-500 parameter updates.",
          "link": "http://arxiv.org/abs/2106.10558",
          "publishedOn": "2021-06-22T01:57:11.726Z",
          "wordCount": 649,
          "title": "Rayleigh-Gauss-Newton optimization with enhanced sampling for variational Monte Carlo. (arXiv:2106.10558v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2010.03250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuhui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Quanming Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Huan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>",
          "description": "In this paper, we propose a novel framework to automatically utilize\ntask-dependent semantic information which is encoded in heterogeneous\ninformation networks (HINs). Specifically, we search for a meta graph, which\ncan capture more complex semantic relations than a meta path, to determine how\ngraph neural networks (GNNs) propagate messages along different types of edges.\nWe formalize the problem within the framework of neural architecture search\n(NAS) and then perform the search in a differentiable manner. We design an\nexpressive search space in the form of a directed acyclic graph (DAG) to\nrepresent candidate meta graphs for a HIN, and we propose task-dependent type\nconstraint to filter out those edge types along which message passing has no\neffect on the representations of nodes that are related to the downstream task.\nThe size of the search space we define is huge, so we further propose a novel\nand efficient search algorithm to make the total search cost on a par with\ntraining a single GNN once. Compared with existing popular NAS algorithms, our\nproposed search algorithm improves the search efficiency. We conduct extensive\nexperiments on different HINs and downstream tasks to evaluate our method, and\nexperimental results show that our method can outperform state-of-the-art\nheterogeneous GNNs and also improves efficiency compared with those methods\nwhich can implicitly learn meta paths.",
          "link": "http://arxiv.org/abs/2010.03250",
          "publishedOn": "2021-06-22T01:57:11.721Z",
          "wordCount": 691,
          "title": "DiffMG: Differentiable Meta Graph Search for Heterogeneous Graph Neural Networks. (arXiv:2010.03250v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gefei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yuling Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Can Yang</a>",
          "description": "We propose to learn a generative model via entropy interpolation with a\nSchr\\\"{o}dinger Bridge. The generative learning task can be formulated as\ninterpolating between a reference distribution and a target distribution based\non the Kullback-Leibler divergence. At the population level, this entropy\ninterpolation is characterized via an SDE on $[0,1]$ with a time-varying drift\nterm. At the sample level, we derive our Schr\\\"{o}dinger Bridge algorithm by\nplugging the drift term estimated by a deep score estimator and a deep density\nratio estimator into the Euler-Maruyama method. Under some mild smoothness\nassumptions of the target distribution, we prove the consistency of both the\nscore estimator and the density ratio estimator, and then establish the\nconsistency of the proposed Schr\\\"{o}dinger Bridge approach. Our theoretical\nresults guarantee that the distribution learned by our approach converges to\nthe target distribution. Experimental results on multimodal synthetic data and\nbenchmark data support our theoretical findings and indicate that the\ngenerative model via Schr\\\"{o}dinger Bridge is comparable with state-of-the-art\nGANs, suggesting a new formulation of generative learning. We demonstrate its\nusefulness in image interpolation and image inpainting.",
          "link": "http://arxiv.org/abs/2106.10410",
          "publishedOn": "2021-06-22T01:57:11.705Z",
          "wordCount": 619,
          "title": "Deep Generative Learning via Schr\\\"{o}dinger Bridge. (arXiv:2106.10410v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10696",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Esmaeili_A/0/1/0/all/0/1\">Ashkan Esmaeili</a>",
          "description": "Deep compressed sensing assumes the data has sparse representation in a\nlatent space, i.e., it is intrinsically of low-dimension. The original data is\nassumed to be mapped from a low-dimensional space through a\nlow-to-high-dimensional generator. In this work, we propound how to design such\na low-to-high dimensional deep learning-based generator suiting for compressed\nsensing, while satisfying robustness to universal adversarial perturbations in\nthe latent domain. We also justify why the noise is considered in the latent\nspace. The work is also buttressed with theoretical analysis on the robustness\nof the trained generator to adversarial perturbations. Experiments on\nreal-world datasets are provided to substantiate the efficacy of the proposed\n\\emph{generative model adversarial training for deep compressed sensing.}",
          "link": "http://arxiv.org/abs/2106.10696",
          "publishedOn": "2021-06-22T01:57:11.700Z",
          "wordCount": 551,
          "title": "Generative Model Adversarial Training for Deep Compressed Sensing. (arXiv:2106.10696v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_K/0/1/0/all/0/1\">Kulin Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Amit Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Navin Goyal</a>",
          "description": "In supervised learning, it is known that overparameterized neural networks\nwith one hidden layer provably and efficiently learn and generalize, when\ntrained using stochastic gradient descent with sufficiently small learning rate\nand suitable initialization. In contrast, the benefit of overparameterization\nin unsupervised learning is not well understood. Normalizing flows (NFs)\nconstitute an important class of models in unsupervised learning for sampling\nand density estimation. In this paper, we theoretically and empirically analyze\nthese models when the underlying neural network is one-hidden-layer\noverparameterized network. Our main contributions are two-fold: (1) On the one\nhand, we provide theoretical and empirical evidence that for a class of NFs\ncontaining most of the existing NF models, overparametrization hurts training.\n(2) On the other hand, we prove that unconstrained NFs, a recently introduced\nmodel, can efficiently learn any reasonable data distribution under minimal\nassumptions when the underlying network is overparametrized.",
          "link": "http://arxiv.org/abs/2106.10535",
          "publishedOn": "2021-06-22T01:57:11.694Z",
          "wordCount": 579,
          "title": "Learning and Generalization in Overparameterized Normalizing Flows. (arXiv:2106.10535v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolk_A/0/1/0/all/0/1\">Agnieszka Wo&#x142;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chlasta_K/0/1/0/all/0/1\">Karol Chlasta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holas_P/0/1/0/all/0/1\">Pawe&#x142; Holas</a>",
          "description": "Sentiment and lexical analyses are widely used to detect depression or\nanxiety disorders. It has been documented that there are significant\ndifferences in the language used by a person with emotional disorders in\ncomparison to a healthy individual. Still, the effectiveness of these lexical\napproaches could be improved further because the current analysis focuses on\nwhat the social media entries are about, and not how they are written. In this\nstudy, we focus on aspects in which these short texts are similar to each\nother, and how they were created. We present an innovative approach to the\ndepression screening problem by applying Collgram analysis, which is a known\neffective method of obtaining linguistic information from texts. We compare\nthese results with sentiment analysis based on the BERT architecture. Finally,\nwe create a hybrid model achieving a diagnostic accuracy of 71%.",
          "link": "http://arxiv.org/abs/2106.10485",
          "publishedOn": "2021-06-22T01:57:11.689Z",
          "wordCount": 602,
          "title": "Hybrid approach to detecting symptoms of depression in social media entries. (arXiv:2106.10485v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guizhong Liu</a>",
          "description": "Metric learning is a widely used method for few shot learning in which the\nquality of prototypes plays a key role in the algorithm. In this paper we\npropose the trainable prototypes for distance measure instead of the artificial\nones within the meta-training and task-training framework. Also to avoid the\ndisadvantages that the episodic meta-training brought, we adopt non-episodic\nmeta-training based on self-supervised learning. Overall we solve the few-shot\ntasks in two phases: meta-training a transferable feature extractor via\nself-supervised learning and training the prototypes for metric classification.\nIn addition, the simple attention mechanism is used in both meta-training and\ntask-training. Our method achieves state-of-the-art performance in a variety of\nestablished few-shot tasks on the standard few-shot visual classification\ndataset, with about 20% increase compared to the available unsupervised\nfew-shot learning methods.",
          "link": "http://arxiv.org/abs/2106.10846",
          "publishedOn": "2021-06-22T01:57:11.683Z",
          "wordCount": 582,
          "title": "Trainable Class Prototypes for Few-Shot Learning. (arXiv:2106.10846v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McRae_P/0/1/0/all/0/1\">Paul-Aymeric McRae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1\">Prasanna Parthasarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assran_M/0/1/0/all/0/1\">Mahmoud Assran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>",
          "description": "Popular approaches for minimizing loss in data-driven learning often involve\nan abstraction or an explicit retention of the history of gradients for\nefficient parameter updates. The aggregated history of gradients nudges the\nparameter updates in the right direction even when the gradients at any given\nstep are not informative. Although the history of gradients summarized in\nmeta-parameters or explicitly stored in memory has been shown effective in\ntheory and practice, the question of whether $all$ or only a subset of the\ngradients in the history are sufficient in deciding the parameter updates\nremains unanswered. In this paper, we propose a framework of memory-augmented\ngradient descent optimizers that retain a limited view of their gradient\nhistory in their internal memory. Such optimizers scale well to large real-life\ndatasets, and our experiments show that the memory augmented extensions of\nstandard optimizers enjoy accelerated convergence and improved performance on a\nmajority of computer vision and language tasks that we considered.\nAdditionally, we prove that the proposed class of optimizers with fixed-size\nmemory converge under assumptions of strong convexity, regardless of which\ngradients are selected or how they are linearly combined to form the update\nstep.",
          "link": "http://arxiv.org/abs/2106.10708",
          "publishedOn": "2021-06-22T01:57:11.667Z",
          "wordCount": 629,
          "title": "Memory Augmented Optimizers for Deep Learning. (arXiv:2106.10708v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shirzad_H/0/1/0/all/0/1\">Hamed Shirzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajimirsadeghi_H/0/1/0/all/0/1\">Hossein Hajimirsadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdi_A/0/1/0/all/0/1\">Amir H. Abdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_G/0/1/0/all/0/1\">Greg Mori</a>",
          "description": "We propose TD-GEN, a graph generation framework based on tree decomposition,\nand introduce a reduced upper bound on the maximum number of decisions needed\nfor graph generation. The framework includes a permutation invariant tree\ngeneration model which forms the backbone of graph generation. Tree nodes are\nsupernodes, each representing a cluster of nodes in the graph. Graph nodes and\nedges are incrementally generated inside the clusters by traversing the tree\nsupernodes, respecting the structure of the tree decomposition, and following\nnode sharing decisions between the clusters. Finally, we discuss the\nshortcomings of standard evaluation criteria based on statistical properties of\nthe generated graphs as performance measures. We propose to compare the\nperformance of models based on likelihood. Empirical results on a variety of\nstandard graph generation datasets demonstrate the superior performance of our\nmethod.",
          "link": "http://arxiv.org/abs/2106.10656",
          "publishedOn": "2021-06-22T01:57:11.646Z",
          "wordCount": 572,
          "title": "TD-GEN: Graph Generation With Tree Decomposition. (arXiv:2106.10656v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengdi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Peide Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengpei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiacheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xuewei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguchi_K/0/1/0/all/0/1\">Kentaro Oguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_H/0/1/0/all/0/1\">Henry Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>",
          "description": "The evaluation of rare but high-stakes events remains one of the main\ndifficulties in obtaining reliable policies from intelligent agents, especially\nin large or continuous state/action spaces where limited scalability enforces\nthe use of a prohibitively large number of testing iterations. On the other\nhand, a biased or inaccurate policy evaluation in a safety-critical system\ncould potentially cause unexpected catastrophic failures during deployment. In\nthis paper, we propose the Accelerated Policy Evaluation (APE) method, which\nsimultaneously uncovers rare events and estimates the rare event probability in\nMarkov decision processes. The APE method treats the environment nature as an\nadversarial agent and learns towards, through adaptive importance sampling, the\nzero-variance sampling distribution for the policy evaluation. Moreover, APE is\nscalable to large discrete or continuous spaces by incorporating function\napproximators. We investigate the convergence properties of proposed algorithms\nunder suitable regularity conditions. Our empirical studies show that APE\nestimates rare event probability with a smaller variance while only using\norders of magnitude fewer samples compared to baseline methods in both\nmulti-agent and single-agent environments.",
          "link": "http://arxiv.org/abs/2106.10566",
          "publishedOn": "2021-06-22T01:57:11.641Z",
          "wordCount": 626,
          "title": "Accelerated Policy Evaluation: Learning Adversarial Environments with Adaptive Importance Sampling. (arXiv:2106.10566v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalwana_M/0/1/0/all/0/1\">Muhammad A. A. K. Jalwana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>",
          "description": "Deep visual models are susceptible to adversarial perturbations to inputs.\nAlthough these signals are carefully crafted, they still appear noise-like\npatterns to humans. This observation has led to the argument that deep visual\nrepresentation is misaligned with human perception. We counter-argue by\nproviding evidence of human-meaningful patterns in adversarial perturbations.\nWe first propose an attack that fools a network to confuse a whole category of\nobjects (source class) with a target label. Our attack also limits the\nunintended fooling by samples from non-sources classes, thereby circumscribing\nhuman-defined semantic notions for network fooling. We show that the proposed\nattack not only leads to the emergence of regular geometric patterns in the\nperturbations, but also reveals insightful information about the decision\nboundaries of deep models. Exploring this phenomenon further, we alter the\n`adversarial' objective of our attack to use it as a tool to `explain' deep\nvisual representation. We show that by careful channeling and projection of the\nperturbations computed by our method, we can visualize a model's understanding\nof human-defined semantic notions. Finally, we exploit the explanability\nproperties of our perturbations to perform image generation, inpainting and\ninteractive image manipulation by attacking adversarialy robust\n`classifiers'.In all, our major contribution is a novel pragmatic adversarial\nattack that is subsequently transformed into a tool to interpret the visual\nmodels. The article also makes secondary contributions in terms of establishing\nthe utility of our attack beyond the adversarial objective with multiple\ninteresting applications.",
          "link": "http://arxiv.org/abs/2106.10606",
          "publishedOn": "2021-06-22T01:57:11.635Z",
          "wordCount": 703,
          "title": "Attack to Fool and Explain Deep Networks. (arXiv:2106.10606v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zim_M/0/1/0/all/0/1\">Md Ziaul Haque Zim</a>",
          "description": "In recent decades, Machine Learning (ML) has become extremely important for\nmany computing applications. The pervasiveness of ultra-low-power embedded\ndevices such as ESP32 or ESP32 Cam with tiny Machine Learning (tinyML)\napplications will enable the mass proliferation of Artificial Intelligent\npowered Embedded IoT Devices. In the last few years, the microcontroller device\n(Espressif ESP32) became powerful enough to be used for small/tiny machine\nlearning (tinyML) tasks. The ease of use of platforms like Arduino IDE,\nMicroPython and TensorFlow Lite (TF) with tinyML application make it an\nindispensable topic of research for mobile robotics, modern computer science\nand electrical engineering. The goal of this paper is to analyze the speed of\nthe Xtensa dual core 32-bit LX6 microprocessor by running a neural network\napplication. The different number of inputs (9, 36, 144 and 576) inputted\nthrough the different number of neurons in neural networks with one and two\nhidden layers. Xtensa LX6 microprocessor has been analyzed because it comes\ninside with Espressif ESP32 and ESP32 Cam which are very easy to use, plug and\nplay IoT device. In this paper speed of the Xtensa LX6 microprocessor in\nfeed-forward mode has been analyzed.",
          "link": "http://arxiv.org/abs/2106.10652",
          "publishedOn": "2021-06-22T01:57:11.628Z",
          "wordCount": 633,
          "title": "TinyML: Analysis of Xtensa LX6 microprocessor for Neural Network Applications by ESP32 SoC. (arXiv:2106.10652v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yandong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hayoung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuanming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yong Zhou</a>",
          "description": "Massive access is a critical design challenge of Internet of Things (IoT)\nnetworks. In this paper, we consider the grant-free uplink transmission of an\nIoT network with a multiple-antenna base station (BS) and a large number of\nsingle-antenna IoT devices. Taking into account the sporadic nature of IoT\ndevices, we formulate the joint activity detection and channel estimation\n(JADCE) problem as a group-sparse matrix estimation problem. This problem can\nbe solved by applying the existing compressed sensing techniques, which however\neither suffer from high computational complexities or lack of algorithm\nrobustness. To this end, we propose a novel algorithm unrolling framework based\non the deep neural network to simultaneously achieve low computational\ncomplexity and high robustness for solving the JADCE problem. Specifically, we\nmap the original iterative shrinkage thresholding algorithm (ISTA) into an\nunrolled recurrent neural network (RNN), thereby improving the convergence rate\nand computational efficiency through end-to-end training. Moreover, the\nproposed algorithm unrolling approach inherits the structure and domain\nknowledge of the ISTA, thereby maintaining the algorithm robustness, which can\nhandle non-Gaussian preamble sequence matrix in massive access. With rigorous\ntheoretical analysis, we further simplify the unrolled network structure by\nreducing the redundant training parameters. Furthermore, we prove that the\nsimplified unrolled deep neural network structures enjoy a linear convergence\nrate. Extensive simulations based on various preamble signatures show that the\nproposed unrolled networks outperform the existing methods in terms of the\nconvergence rate, robustness and estimation accuracy.",
          "link": "http://arxiv.org/abs/2106.10426",
          "publishedOn": "2021-06-22T01:57:11.614Z",
          "wordCount": 703,
          "title": "Algorithm Unrolling for Massive Access via Deep Neural Network with Theoretical Guarantee. (arXiv:2106.10426v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10476",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yucun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murzakhanov_I/0/1/0/all/0/1\">Ilgiz Murzakhanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzivasileiadis_S/0/1/0/all/0/1\">Spyros Chatzivasileiadis</a>",
          "description": "With the rapid growth of renewable energy, lots of small photovoltaic (PV)\nprosumers emerge. Due to the uncertainty of solar power generation, there is a\nneed for aggregated prosumers to predict solar power generation and whether\nsolar power generation will be larger than load. This paper presents two\ninterpretable neural networks to solve the problem: one binary classification\nneural network and one regression neural network. The neural networks are built\nusing TensorFlow. The global feature importance and local feature contributions\nare examined by three gradient-based methods: Integrated Gradients, Expected\nGradients, and DeepLIFT. Moreover, we detect abnormal cases when predictions\nmight fail by estimating the prediction uncertainty using Bayesian neural\nnetworks. Neural networks, which are interpreted by gradient-based methods and\ncomplemented with uncertainty estimation, provide robust and explainable\nforecasting for decision-makers.",
          "link": "http://arxiv.org/abs/2106.10476",
          "publishedOn": "2021-06-22T01:57:11.609Z",
          "wordCount": 560,
          "title": "Neural network interpretability for forecasting of aggregated renewable generatiion. (arXiv:2106.10476v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hua Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1\">Deheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhenhui/0/1/0/all/0/1\">Zhenhui</a> (Jessie)Li",
          "description": "Offline reinforcement learning (RL) tries to learn the near-optimal policy\nwith recorded offline experience without online exploration. Current offline RL\nresearch includes: 1) generative modeling, i.e., approximating a policy using\nfixed data; and 2) learning the state-action value function. While most\nresearch focuses on the state-action function part through reducing the\nbootstrapping error in value function approximation induced by the distribution\nshift of training data, the effects of error propagation in generative modeling\nhave been neglected. In this paper, we analyze the error in generative\nmodeling. We propose AQL (action-conditioned Q-learning), a residual generative\nmodel to reduce policy approximation error for offline RL. We show that our\nmethod can learn more accurate policy approximations in different benchmark\ndatasets. In addition, we show that the proposed offline RL method can learn\nmore competitive AI agents in complex control tasks under the multiplayer\nonline battle arena (MOBA) game Honor of Kings.",
          "link": "http://arxiv.org/abs/2106.10411",
          "publishedOn": "2021-06-22T01:57:11.604Z",
          "wordCount": 611,
          "title": "Boosting Offline Reinforcement Learning with Residual Generative Modeling. (arXiv:2106.10411v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10314",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Scibior_A/0/1/0/all/0/1\">Adam &#x15a;cibior</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Masrani_V/0/1/0/all/0/1\">Vaden Masrani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wood_F/0/1/0/all/0/1\">Frank Wood</a>",
          "description": "In recent years particle filters have being used as components in systems\noptimized end-to-end with gradient descent. However, the resampling step in a\nparticle filter is not differentiable, which biases gradients and interferes\nwith optimization. To remedy this problem, several differentiable variants of\nresampling have been proposed, all of which modify the behavior of the particle\nfilter in significant and potentially undesirable ways. In this paper, we show\nhow to obtain unbiased estimators of the gradient of the marginal likelihood by\nonly modifying messages used in backpropagation, leaving the standard forward\npass of a particle filter unchanged. Our method is simple to implement, has a\nlow computational overhead, does not introduce additional hyperparameters, and\nextends to derivatives of higher orders. We call it stop-gradient resampling,\nsince it can easily be implemented with automatic differentiation libraries\nusing the stop-gradient operator instead of explicitly modifying the backward\nmessages.",
          "link": "http://arxiv.org/abs/2106.10314",
          "publishedOn": "2021-06-22T01:57:11.599Z",
          "wordCount": 582,
          "title": "Differentiable Particle Filtering without Modifying the Forward Pass. (arXiv:2106.10314v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wenga_C/0/1/0/all/0/1\">Carmel Wenga</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Fansi_M/0/1/0/all/0/1\">Majirus Fansi</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Chabrier_S/0/1/0/all/0/1\">S&#xe9;bastien Chabrier</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Mari_J/0/1/0/all/0/1\">Jean-Martial Mari</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gabillon_A/0/1/0/all/0/1\">Alban Gabillon</a> (1) ((1) University of French Polynesia, (2) NzhinuSoft)",
          "description": "Over the past two decades, recommender systems have attracted a lot of\ninterest due to the explosion in the amount of data in online applications. A\nparticular attention has been paid to collaborative filtering, which is the\nmost widely used in applications that involve information recommendations.\nCollaborative filtering (CF) uses the known preference of a group of users to\nmake predictions and recommendations about the unknown preferences of other\nusers (recommendations are made based on the past behavior of users). First\nintroduced in the 1990s, a wide variety of increasingly successful models have\nbeen proposed. Due to the success of machine learning techniques in many areas,\nthere has been a growing emphasis on the application of such algorithms in\nrecommendation systems. In this article, we present an overview of the CF\napproaches for recommender systems, their two main categories, and their\nevaluation metrics. We focus on the application of classical Machine Learning\nalgorithms to CF recommender systems by presenting their evolution from their\nfirst use-cases to advanced Machine Learning models. We attempt to provide a\ncomprehensive and comparative overview of CF systems (with python\nimplementations) that can serve as a guideline for research and practice in\nthis area.",
          "link": "http://arxiv.org/abs/2106.10679",
          "publishedOn": "2021-06-22T01:57:11.565Z",
          "wordCount": 663,
          "title": "A Comprehensive Review on Non-Neural Networks Collaborative Filtering Recommendation Systems. (arXiv:2106.10679v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xuanyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>",
          "description": "Training images with data transformations have been suggested as contrastive\nexamples to complement the testing set for generalization performance\nevaluation of deep neural networks (DNNs). In this work, we propose a practical\nframework ContRE (The word \"contre\" means \"against\" or \"versus\" in French.)\nthat uses Contrastive examples for DNN geneRalization performance Estimation.\nSpecifically, ContRE follows the assumption in contrastive learning that robust\nDNN models with good generalization performance are capable of extracting a\nconsistent set of features and making consistent predictions from the same\nimage under varying data transformations. Incorporating with a set of\nrandomized strategies for well-designed data transformations over the training\nset, ContRE adopts classification errors and Fisher ratios on the generated\ncontrastive examples to assess and analyze the generalization performance of\ndeep models in complement with a testing set. To show the effectiveness and the\nefficiency of ContRE, extensive experiments have been done using various DNN\nmodels on three open source benchmark datasets with thorough ablation studies\nand applicability analyses. Our experiment results confirm that (1) behaviors\nof deep models on contrastive examples are strongly correlated to what on the\ntesting set, and (2) ContRE is a robust measure of generalization performance\ncomplementing to the testing set in various settings.",
          "link": "http://arxiv.org/abs/2106.10653",
          "publishedOn": "2021-06-22T01:57:11.527Z",
          "wordCount": 652,
          "title": "Practical Assessment of Generalization Performance Robustness for Deep Networks via Contrastive Examples. (arXiv:2106.10653v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1\">Ondrej Bohdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy Hospedales</a>",
          "description": "Gradient-based meta-learning and hyperparameter optimization have seen\nsignificant progress recently, enabling practical end-to-end training of neural\nnetworks together with many hyperparameters. Nevertheless, existing approaches\nare relatively expensive as they need to compute second-order derivatives and\nstore a longer computational graph. This cost prevents scaling them to larger\nnetwork architectures. We present EvoGrad, a new approach to meta-learning that\ndraws upon evolutionary techniques to more efficiently compute hypergradients.\nEvoGrad estimates hypergradient with respect to hyperparameters without\ncalculating second-order gradients, or storing a longer computational graph,\nleading to significant improvements in efficiency. We evaluate EvoGrad on two\nsubstantial recent meta-learning applications, namely cross-domain few-shot\nlearning with feature-wise transformations and noisy label learning with\nMetaWeightNet. The results show that EvoGrad significantly improves efficiency\nand enables scaling meta-learning to bigger CNN architectures such as from\nResNet18 to ResNet34.",
          "link": "http://arxiv.org/abs/2106.10575",
          "publishedOn": "2021-06-22T01:57:11.509Z",
          "wordCount": 571,
          "title": "EvoGrad: Efficient Gradient-Based Meta-Learning and Hyperparameter Optimization. (arXiv:2106.10575v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10591",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Amiridi_M/0/1/0/all/0/1\">Magda Amiridi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kargas_N/0/1/0/all/0/1\">Nikos Kargas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sidiropoulos_N/0/1/0/all/0/1\">Nicholas D. Sidiropoulos</a>",
          "description": "Learning generative probabilistic models is a core problem in machine\nlearning, which presents significant challenges due to the curse of\ndimensionality. This paper proposes a joint dimensionality reduction and\nnon-parametric density estimation framework, using a novel estimator that can\nexplicitly capture the underlying distribution of appropriate reduced-dimension\nrepresentations of the input data. The idea is to jointly design a nonlinear\ndimensionality reducing auto-encoder to model the training data in terms of a\nparsimonious set of latent random variables, and learn a canonical low-rank\ntensor model of the joint distribution of the latent variables in the Fourier\ndomain. The proposed latent density model is non-parametric and universal, as\nopposed to the predefined prior that is assumed in variational auto-encoders.\nJoint optimization of the auto-encoder and the latent density estimator is\npursued via a formulation which learns both by minimizing a combination of the\nnegative log-likelihood in the latent domain and the auto-encoder\nreconstruction loss. We demonstrate that the proposed model achieves very\npromising results on toy, tabular, and image datasets on regression tasks,\nsampling, and anomaly detection.",
          "link": "http://arxiv.org/abs/2106.10591",
          "publishedOn": "2021-06-22T01:57:11.504Z",
          "wordCount": 616,
          "title": "Low-rank Characteristic Tensor Density Estimation Part II: Compression and Latent Density Estimation. (arXiv:2106.10591v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10370",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Awasthi_P/0/1/0/all/0/1\">Pranjal Awasthi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Das_A/0/1/0/all/0/1\">Abhimanyu Das</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sen_R/0/1/0/all/0/1\">Rajat Sen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Suresh_A/0/1/0/all/0/1\">Ananda Theertha Suresh</a>",
          "description": "We advocate for a practical Maximum Likelihood Estimation (MLE) approach for\nregression and forecasting, as an alternative to the typical approach of\nEmpirical Risk Minimization (ERM) for a specific target metric. This approach\nis better suited to capture inductive biases such as prior domain knowledge in\ndatasets, and can output post-hoc estimators at inference time that can\noptimize different types of target metrics. We present theoretical results to\ndemonstrate that our approach is always competitive with any estimator for the\ntarget metric under some general conditions, and in many practical settings\n(such as Poisson Regression) can actually be much superior to ERM. We\ndemonstrate empirically that our method instantiated with a well-designed\ngeneral purpose mixture likelihood family can obtain superior performance over\nERM for a variety of tasks across time-series forecasting and regression\ndatasets with different data distributions.",
          "link": "http://arxiv.org/abs/2106.10370",
          "publishedOn": "2021-06-22T01:57:11.499Z",
          "wordCount": 584,
          "title": "On the benefits of maximum likelihood estimation for Regression and Forecasting. (arXiv:2106.10370v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Palmieri_F/0/1/0/all/0/1\">Francesco A.N. Palmieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pattipati_K/0/1/0/all/0/1\">Krishna R. Pattipati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gennaro_G/0/1/0/all/0/1\">Giovanni Di Gennaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fioretti_G/0/1/0/all/0/1\">Giovanni Fioretti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verolla_F/0/1/0/all/0/1\">Francesco Verolla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buonanno_A/0/1/0/all/0/1\">Amedeo Buonanno</a>",
          "description": "Even if path planning can be solved using standard techniques from dynamic\nprogramming and control, the problem can also be approached using probabilistic\ninference. The algorithms that emerge using the latter framework bear some\nappealing characteristics that qualify the probabilistic approach as a powerful\nalternative to the more traditional control formulations. The idea of using\nestimation on stochastic models to solve control problems is not new and the\ninference approach considered here falls under the rubric of Active Inference\n(AI) and Control as Inference (CAI). In this work, we look at the specific\nrecursions that arise from various cost functions that, although they may\nappear similar in scope, bear noticeable differences, at least when applied to\ntypical path planning problems. We start by posing the path planning problem on\na probabilistic factor graph, and show how the various algorithms translate\ninto specific message composition rules. We then show how this unified\napproach, presented both in probability space and in log space, provides a very\ngeneral framework that includes the Sum-product, the Max-product, Dynamic\nprogramming and mixed Reward/Entropy criteria-based algorithms. The framework\nalso expands algorithmic design options for smoother or sharper policy\ndistributions, including generalized Sum/Max-product algorithm, a Smooth\nDynamic programming algorithm and modified versions of the Reward/Entropy\nrecursions. We provide a comprehensive table of recursions and a comparison\nthrough simulations, first on a synthetic small grid with a single goal with\nobstacles, and then on a grid extrapolated from a real-world scene with\nmultiple goals and a semantic map.",
          "link": "http://arxiv.org/abs/2106.10442",
          "publishedOn": "2021-06-22T01:57:11.494Z",
          "wordCount": 703,
          "title": "A Unified View of Algorithms for Path Planning Using Probabilistic Inference on Factor Graphs. (arXiv:2106.10442v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10543",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hanna_S/0/1/0/all/0/1\">Samer Hanna</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dick_C/0/1/0/all/0/1\">Chris Dick</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cabric_D/0/1/0/all/0/1\">Danijela Cabric</a>",
          "description": "Blindly decoding a signal requires estimating its unknown transmit\nparameters, compensating for the wireless channel impairments, and identifying\nthe modulation type. While deep learning can solve complex problems, digital\nsignal processing (DSP) is interpretable and can be more computationally\nefficient. To combine both, we propose the dual path network (DPN). It consists\nof a signal path of DSP operations that recover the signal, and a feature path\nof neural networks that estimate the unknown transmit parameters. By\ninterconnecting the paths over several recovery stages, later stages benefit\nfrom the recovered signals and reuse all the previously extracted features. The\nproposed design is demonstrated to provide 5% improvement in modulation\nclassification compared to alternative designs lacking either feature sharing\nor access to recovered signals. The estimation results of DPN along with its\nblind decoding performance are shown to outperform a blind signal processing\nalgorithm for BPSK and QPSK on a simulated dataset. An over-the-air\nsoftware-defined-radio capture was used to verify DPN results at high SNRs. DPN\ndesign can process variable length inputs and is shown to outperform relying on\nfixed length inputs with prediction averaging on longer signals by up to 15% in\nmodulation classification.",
          "link": "http://arxiv.org/abs/2106.10543",
          "publishedOn": "2021-06-22T01:57:11.481Z",
          "wordCount": 638,
          "title": "Signal Processing Based Deep Learning for Blind Symbol Decoding and Modulation Classification. (arXiv:2106.10543v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Seungyul Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Youngchul Sung</a>",
          "description": "In this paper, we propose a max-min entropy framework for reinforcement\nlearning (RL) to overcome the limitation of the maximum entropy RL framework in\nmodel-free sample-based learning. Whereas the maximum entropy RL framework\nguides learning for policies to reach states with high entropy in the future,\nthe proposed max-min entropy framework aims to learn to visit states with low\nentropy and maximize the entropy of these low-entropy states to promote\nexploration. For general Markov decision processes (MDPs), an efficient\nalgorithm is constructed under the proposed max-min entropy framework based on\ndisentanglement of exploration and exploitation. Numerical results show that\nthe proposed algorithm yields drastic performance improvement over the current\nstate-of-the-art RL algorithms.",
          "link": "http://arxiv.org/abs/2106.10517",
          "publishedOn": "2021-06-22T01:57:11.476Z",
          "wordCount": 545,
          "title": "A Max-Min Entropy Framework for Reinforcement Learning. (arXiv:2106.10517v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10437",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1\">Sieun Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_E/0/1/0/all/0/1\">Eunho Lee</a>",
          "description": "Super-resolution (SR) is a one-to-many task with multiple possible solutions.\nHowever, previous works were not concerned about this characteristic. For a\none-to-many pipeline, the generator should be able to generate multiple\nestimates of the reconstruction, and not be penalized for generating similar\nand equally realistic images. To achieve this, we propose adding weighted\npixel-wise noise after every Residual-in-Residual Dense Block (RRDB) to enable\nthe generator to generate various images. We modify the strict content loss to\nnot penalize the stochastic variation in reconstructed images as long as it has\nconsistent content. Additionally, we observe that there are out-of-focus\nregions in the DIV2K, DIV8K datasets that provide unhelpful guidelines. We\nfilter blurry regions in the training data using the method of [10]. Finally,\nwe modify the discriminator to receive the low-resolution image as a reference\nimage along with the target image to provide better feedback to the generator.\nUsing our proposed methods, we were able to improve the performance of ESRGAN\nin x4 perceptual SR and achieve the state-of-the-art LPIPS score in x16\nperceptual extreme SR.",
          "link": "http://arxiv.org/abs/2106.10437",
          "publishedOn": "2021-06-22T01:57:11.469Z",
          "wordCount": 613,
          "title": "One-to-many Approach for Improving Super-Resolution. (arXiv:2106.10437v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10471",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "Cross-entropy loss with softmax output is a standard choice to train neural\nnetwork classifiers. We give a new view of neural network classifiers with\nsoftmax and cross-entropy as mutual information evaluators. We show that when\nthe dataset is balanced, training a neural network with cross-entropy maximises\nthe mutual information between inputs and labels through a variational form of\nmutual information. Thereby, we develop a new form of softmax that also\nconverts a classifier to a mutual information evaluator when the dataset is\nimbalanced. Experimental results show that the new form leads to better\nclassification accuracy, in particular for imbalanced datasets.",
          "link": "http://arxiv.org/abs/2106.10471",
          "publishedOn": "2021-06-22T01:57:11.464Z",
          "wordCount": 541,
          "title": "Neural Network Classifier as Mutual Information Evaluator. (arXiv:2106.10471v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aoki_R/0/1/0/all/0/1\">Raquel Aoki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_F/0/1/0/all/0/1\">Frederick Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_G/0/1/0/all/0/1\">Gabriel L. Oliveira</a>",
          "description": "Predicting multiple heterogeneous biological and medical targets is a\nchallenge for traditional deep learning models. In contrast to single-task\nlearning, in which a separate model is trained for each target, multi-task\nlearning (MTL) optimizes a single model to predict multiple related targets\nsimultaneously. To address this challenge, we propose the Multi-gate\nMixture-of-Experts with Exclusivity (MMoEEx). Our work aims to tackle the\nheterogeneous MTL setting, in which the same model optimizes multiple tasks\nwith different characteristics. Such a scenario can overwhelm current MTL\napproaches due to the challenges in balancing shared and task-specific\nrepresentations and the need to optimize tasks with competing optimization\npaths. Our method makes two key contributions: first, we introduce an approach\nto induce more diversity among experts, thus creating representations more\nsuitable for highly imbalanced and heterogenous MTL learning; second, we adopt\na two-step optimization [6, 11] approach to balancing the tasks at the gradient\nlevel. We validate our method on three MTL benchmark datasets, including\nMedical Information Mart for Intensive Care (MIMIC-III) and PubChem BioAssay\n(PCBA).",
          "link": "http://arxiv.org/abs/2106.10595",
          "publishedOn": "2021-06-22T01:57:11.460Z",
          "wordCount": 600,
          "title": "Heterogeneous Multi-task Learning with Expert Diversity. (arXiv:2106.10595v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanduri_P/0/1/0/all/0/1\">Prashant Khanduri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Pranay Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1\">Mingyi Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajawat_K/0/1/0/all/0/1\">Ketan Rajawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_P/0/1/0/all/0/1\">Pramod K. Varshney</a>",
          "description": "Federated Learning (FL) refers to the paradigm where multiple worker nodes\n(WNs) build a joint model by using local data. Despite extensive research, for\na generic non-convex FL problem, it is not clear, how to choose the WNs' and\nthe server's update directions, the minibatch sizes, and the local update\nfrequency, so that the WNs use the minimum number of samples and communication\nrounds to achieve the desired solution. This work addresses the above question\nand considers a class of stochastic algorithms where the WNs perform a few\nlocal updates before communication. We show that when both the WN's and the\nserver's directions are chosen based on a stochastic momentum estimator, the\nalgorithm requires $\\tilde{\\mathcal{O}}(\\epsilon^{-3/2})$ samples and\n$\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ communication rounds to compute an\n$\\epsilon$-stationary solution. To the best of our knowledge, this is the first\nFL algorithm that achieves such {\\it near-optimal} sample and communication\ncomplexities simultaneously. Further, we show that there is a trade-off curve\nbetween local update frequencies and local minibatch sizes, on which the above\nsample and communication complexities can be maintained. Finally, we show that\nfor the classical FedAvg (a.k.a. Local SGD, which is a momentum-less special\ncase of the STEM), a similar trade-off curve exists, albeit with worse sample\nand communication complexities. Our insights on this trade-off provides\nguidelines for choosing the four important design elements for FL algorithms,\nthe update frequency, directions, and minibatch sizes to achieve the best\nperformance.",
          "link": "http://arxiv.org/abs/2106.10435",
          "publishedOn": "2021-06-22T01:57:11.445Z",
          "wordCount": 698,
          "title": "STEM: A Stochastic Two-Sided Momentum Algorithm Achieving Near-Optimal Sample and Communication Complexities for Federated Learning. (arXiv:2106.10435v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shao-Lun Huang</a>",
          "description": "Transferability estimation is an essential problem in transfer learning to\npredict how good the performance is when transfer a source model (source task)\nto a target task. Recent analytical transferability metrics have been widely\nused for source model selection and multi-task learning. Earlier metrics does\nnot work sufficiently well under the challenging cross-domain cross-task\ntransfer settings, but recent OTCE score achieves a noteworthy performance\nusing auxiliary tasks. A simplified version named OT-based NCE score sacrifices\naccuracy to be more efficient, but it can be further improved. Consequently, we\npropose a practical transferability metric called JC-NCE score to further\nimprove the cross-domain cross-task transferability estimation performance,\nwhich is more efficient than the OTCE score and more accurate than the OT-based\nNCE score. Specifically, we build the joint correspondences between source and\ntarget data via solving an optimal transport problem with considering both the\nsample distance and label distance, and then compute the transferability score\nas the negative conditional entropy. Extensive validations under the\nintra-dataset and inter-dataset transfer settings demonstrate that our JC-NCE\nscore outperforms the OT-based NCE score with about 7% and 12% gains,\nrespectively.",
          "link": "http://arxiv.org/abs/2106.10479",
          "publishedOn": "2021-06-22T01:57:11.440Z",
          "wordCount": 627,
          "title": "Practical Transferability Estimation for Image Classification Tasks. (arXiv:2106.10479v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10394",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Laidlaw_C/0/1/0/all/0/1\">Cassidy Laidlaw</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Russell_S/0/1/0/all/0/1\">Stuart Russell</a>",
          "description": "Existing observational approaches for learning human preferences, such as\ninverse reinforcement learning, usually make strong assumptions about the\nobservability of the human's environment. However, in reality, people make many\nimportant decisions under uncertainty. To better understand preference learning\nin these cases, we study the setting of inverse decision theory (IDT), a\npreviously proposed framework where a human is observed making non-sequential\nbinary decisions under uncertainty. In IDT, the human's preferences are\nconveyed through their loss function, which expresses a tradeoff between\ndifferent types of mistakes. We give the first statistical analysis of IDT,\nproviding conditions necessary to identify these preferences and characterizing\nthe sample complexity -- the number of decisions that must be observed to learn\nthe tradeoff the human is making to a desired precision. Interestingly, we show\nthat it is actually easier to identify preferences when the decision problem is\nmore uncertain. Furthermore, uncertain decision problems allow us to relax the\nunrealistic assumption that the human is an optimal decision maker but still\nidentify their exact preferences; we give sample complexities in this\nsuboptimal case as well. Our analysis contradicts the intuition that partial\nobservability should make preference learning more difficult. It also provides\na first step towards understanding and improving preference learning methods\nfor uncertain and suboptimal humans.",
          "link": "http://arxiv.org/abs/2106.10394",
          "publishedOn": "2021-06-22T01:57:11.435Z",
          "wordCount": 649,
          "title": "Learning the Preferences of Uncertain Humans with Inverse Decision Theory. (arXiv:2106.10394v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aimen_A/0/1/0/all/0/1\">Aroof Aimen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidheekh_S/0/1/0/all/0/1\">Sahil Sidheekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">Narayanan C. Krishnan</a>",
          "description": "Meta-learning (ML) has emerged as a promising direction in learning models\nunder constrained resource settings like few-shot learning. The popular\napproaches for ML either learn a generalizable initial model or a generic\nparametric optimizer through episodic training. The former approaches leverage\nthe knowledge from a batch of tasks to learn an optimal prior. In this work, we\nstudy the importance of a batch for ML. Specifically, we first incorporate a\nbatch episodic training regimen to improve the learning of the generic\nparametric optimizer. We also hypothesize that the common assumption in batch\nepisodic training that each task in a batch has an equal contribution to\nlearning an optimal meta-model need not be true. We propose to weight the tasks\nin a batch according to their \"importance\" in improving the meta-model's\nlearning. To this end, we introduce a training curriculum motivated by\nselective focus in humans, called task attended meta-training, to weight the\ntasks in a batch. Task attention is a standalone module that can be integrated\nwith any batch episodic training regimen. The comparisons of the models with\ntheir non-task-attended counterparts on complex datasets like miniImageNet and\ntieredImageNet validate its effectiveness.",
          "link": "http://arxiv.org/abs/2106.10642",
          "publishedOn": "2021-06-22T01:57:11.430Z",
          "wordCount": 623,
          "title": "Task Attended Meta-Learning for Few-Shot Learning. (arXiv:2106.10642v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bertossi_L/0/1/0/all/0/1\">Leopoldo Bertossi</a>",
          "description": "We describe some recent approaches to score-based explanations for query\nanswers in databases and outcomes from classification models in machine\nlearning. The focus is on work done by the author and collaborators. Special\nemphasis is placed on declarative approaches based on answer-set programming to\nthe use of counterfactual reasoning for score specification and computation.\nSeveral examples that illustrate the flexibility of these methods are shown.",
          "link": "http://arxiv.org/abs/2106.10562",
          "publishedOn": "2021-06-22T01:57:11.426Z",
          "wordCount": 533,
          "title": "Score-Based Explanations in Data Management and Machine Learning: An Answer-Set Programming Approach to Counterfactual Analysis. (arXiv:2106.10562v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_E/0/1/0/all/0/1\">En-Yu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun-Lin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hong-Liang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Duan-Bing Chen</a>",
          "description": "Many real-world systems can be expressed in temporal networks with nodes\nplaying far different roles in structure and function and edges representing\nthe relationships between nodes. Identifying critical nodes can help us control\nthe spread of public opinions or epidemics, predict leading figures in\nacademia, conduct advertisements for various commodities, and so on. However,\nit is rather difficult to identify critical nodes because the network structure\nchanges over time in temporal networks. In this paper, considering the sequence\ntopological information of temporal networks, a novel and effective learning\nframework based on the combination of special GCNs and RNNs is proposed to\nidentify nodes with the best spreading ability. The effectiveness of the\napproach is evaluated by weighted Susceptible-Infected-Recovered model.\nExperimental results on four real-world temporal networks demonstrate that the\nproposed method outperforms both traditional and deep learning benchmark\nmethods in terms of the Kendall $\\tau$ coefficient and top $k$ hit rate.",
          "link": "http://arxiv.org/abs/2106.10419",
          "publishedOn": "2021-06-22T01:57:11.410Z",
          "wordCount": 602,
          "title": "Predicting Critical Nodes in Temporal Networks by Dynamic Graph Convolutional Networks. (arXiv:2106.10419v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10494",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lukasik_M/0/1/0/all/0/1\">Michal Lukasik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1\">Srinadh Bhojanapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1\">Aditya Krishna Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>",
          "description": "Knowledge distillation is widely used as a means of improving the performance\nof a relatively simple student model using the predictions from a complex\nteacher model. Several works have shown that distillation significantly boosts\nthe student's overall performance; however, are these gains uniform across all\ndata subgroups? In this paper, we show that distillation can harm performance\non certain subgroups, e.g., classes with few associated samples. We trace this\nbehaviour to errors made by the teacher distribution being transferred to and\namplified by the student model. To mitigate this problem, we present techniques\nwhich soften the teacher influence for subgroups where it is less reliable.\nExperiments on several image classification benchmarks show that these\nmodifications of distillation maintain boost in overall accuracy, while\nadditionally ensuring improvement in subgroup performance.",
          "link": "http://arxiv.org/abs/2106.10494",
          "publishedOn": "2021-06-22T01:57:11.405Z",
          "wordCount": 565,
          "title": "Teacher's pet: understanding and mitigating biases in distillation. (arXiv:2106.10494v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10526",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isufi_E/0/1/0/all/0/1\">Elvin Isufi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1\">Alejandro Ribeiro</a>",
          "description": "Graph convolutional neural networks (GCNNs) are nonlinear processing tools to\nlearn representations from network data. A key property of GCNNs is their\nstability to graph perturbations. Current analysis considers deterministic\nperturbations but fails to provide relevant insights when topological changes\nare random. This paper investigates the stability of GCNNs to stochastic graph\nperturbations induced by link losses. In particular, it proves the expected\noutput difference between the GCNN over random perturbed graphs and the GCNN\nover the nominal graph is upper bounded by a factor that is linear in the link\nloss probability. We perform the stability analysis in the graph spectral\ndomain such that the result holds uniformly for any graph. This result also\nshows the role of the nonlinearity and the architecture width and depth, and\nallows identifying handle to improve the GCNN robustness. Numerical simulations\non source localization and robot swarm control corroborate our theoretical\nfindings.",
          "link": "http://arxiv.org/abs/2106.10526",
          "publishedOn": "2021-06-22T01:57:11.396Z",
          "wordCount": 584,
          "title": "Stability of Graph Convolutional Neural Networks to Stochastic Perturbations. (arXiv:2106.10526v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cameron_C/0/1/0/all/0/1\">Chris Cameron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartford_J/0/1/0/all/0/1\">Jason Hartford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundy_T/0/1/0/all/0/1\">Taylor Lundy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leyton_Brown_K/0/1/0/all/0/1\">Kevin Leyton-Brown</a>",
          "description": "Formulating real-world optimization problems often begins with making\npredictions from historical data (e.g., an optimizer that aims to recommend\nfast routes relies upon travel-time predictions). Typically, learning the\nprediction model used to generate the optimization problem and solving that\nproblem are performed in two separate stages. Recent work has showed how such\nprediction models can be learned end-to-end by differentiating through the\noptimization task. Such methods often yield empirical improvements, which are\ntypically attributed to end-to-end making better error tradeoffs than the\nstandard loss function used in a two-stage solution. We refine this explanation\nand more precisely characterize when end-to-end can improve performance. When\nprediction targets are stochastic, a two-stage solution must make an a priori\nchoice about which statistics of the target distribution to model -- we\nconsider expectations over prediction targets -- while an end-to-end solution\ncan make this choice adaptively. We show that the performance gap between a\ntwo-stage and end-to-end approach is closely related to the \\emph{price of\ncorrelation} concept in stochastic optimization and show the implications of\nsome existing POC results for our predict-then-optimize problem. We then\nconsider a novel and particularly practical setting, where coefficients in the\nobjective function depend on multiple prediction targets. We give explicit\nconstructions where (1) two-stage performs unboundedly worse than end-to-end;\nand (2) two-stage is optimal. We identify a large set of real-world\napplications whose objective functions rely on multiple prediction targets but\nwhich nevertheless deploy two-stage solutions. We also use simulations to\nexperimentally quantify performance gaps.",
          "link": "http://arxiv.org/abs/2106.10349",
          "publishedOn": "2021-06-22T01:57:11.391Z",
          "wordCount": 683,
          "title": "The Perils of Learning Before Optimizing. (arXiv:2106.10349v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummins_C/0/1/0/all/0/1\">Chris Cummins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Brandon Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steiner_B/0/1/0/all/0/1\">Benoit Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linnan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>",
          "description": "Path planning, the problem of efficiently discovering high-reward\ntrajectories, often requires optimizing a high-dimensional and multimodal\nreward function. Popular approaches like CEM and CMA-ES greedily focus on\npromising regions of the search space and may get trapped in local maxima. DOO\nand VOOT balance exploration and exploitation, but use space partitioning\nstrategies independent of the reward function to be optimized. Recently, LaMCTS\nempirically learns to partition the search space in a reward-sensitive manner\nfor black-box optimization. In this paper, we develop a novel formal regret\nanalysis for when and why such an adaptive region partitioning scheme works. We\nalso propose a new path planning method PlaLaM which improves the function\nvalue estimation within each sub-region, and uses a latent representation of\nthe search space. Empirically, PlaLaM outperforms existing path planning\nmethods in 2D navigation tasks, especially in the presence of\ndifficult-to-escape local optima, and shows benefits when plugged into\nmodel-based RL with planning components such as PETS. These gains transfer to\nhighly multimodal real-world tasks, where we outperform strong baselines in\ncompiler phase ordering by up to 245% and in molecular design by up to 0.4 on\nproperties on a 0-1 scale.",
          "link": "http://arxiv.org/abs/2106.10544",
          "publishedOn": "2021-06-22T01:57:11.385Z",
          "wordCount": 642,
          "title": "Learning Space Partitions for Path Planning. (arXiv:2106.10544v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chengyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>",
          "description": "Solving the optimal power flow (OPF) problem in real-time electricity market\nimproves the efficiency and reliability in the integration of low-carbon energy\nresources into the power grids. To address the scalability and adaptivity\nissues of existing end-to-end OPF learning solutions, we propose a new graph\nneural network (GNN) framework for predicting the electricity market prices\nfrom solving OPFs. The proposed GNN-for-OPF framework innovatively exploits the\nlocality property of prices and introduces physics-aware regularization, while\nattaining reduced model complexity and fast adaptivity to varying grid\ntopology. Numerical tests have validated the learning efficiency and adaptivity\nimprovements of our proposed method over existing approaches.",
          "link": "http://arxiv.org/abs/2106.10529",
          "publishedOn": "2021-06-22T01:57:11.371Z",
          "wordCount": 547,
          "title": "Graph Neural Networks for Learning Real-Time Prices in Electricity Market. (arXiv:2106.10529v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kazmierczak_S/0/1/0/all/0/1\">Stanis&#x142;aw Ka&#x17a;mierczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juszka_Z/0/1/0/all/0/1\">Zofia Juszka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fudalej_P/0/1/0/all/0/1\">Piotr Fudalej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandziuk_J/0/1/0/all/0/1\">Jacek Ma&#x144;dziuk</a>",
          "description": "First attempts of prediction of the facial growth (FG) direction were made\nover half of a century ago. Despite numerous attempts and elapsed time, a\nsatisfactory method has not been established yet and the problem still poses a\nchallenge for medical experts. To our knowledge, this paper is the first\nMachine Learning approach to the prediction of FG direction. Conducted data\nanalysis reveals the inherent complexity of the problem and explains the\nreasons of difficulty in FG direction prediction based on 2D X-ray images. To\nperform growth forecasting, we employ a wide range of algorithms, from logistic\nregression, through tree ensembles to neural networks and consider three,\nslightly different, problem formulations. The resulting classification accuracy\nvaries between 71% and 75%.",
          "link": "http://arxiv.org/abs/2106.10464",
          "publishedOn": "2021-06-22T01:57:11.365Z",
          "wordCount": 560,
          "title": "Prediction of the facial growth direction with Machine Learning methods. (arXiv:2106.10464v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "We study how to evaluate the quantitative information content of a region\nwithin an image for a particular label. To this end, we bridge class activation\nmaps with information theory. We develop an informative class activation map\n(infoCAM). Given a classification task, infoCAM depict how to accumulate\ninformation of partial regions to that of the entire image toward a label.\nThus, we can utilise infoCAM to locate the most informative features for a\nlabel. When applied to an image classification task, infoCAM performs better\nthan the traditional classification map in the weakly supervised object\nlocalisation task. We achieve state-of-the-art results on Tiny-ImageNet.",
          "link": "http://arxiv.org/abs/2106.10472",
          "publishedOn": "2021-06-22T01:57:11.338Z",
          "wordCount": 541,
          "title": "Informative Class Activation Maps. (arXiv:2106.10472v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zhihan Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Juanyong Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianmeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Congrui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bixiong Xu</a>",
          "description": "This paper presents TS2Vec, a universal framework for learning\ntimestamp-level representations of time series. Unlike existing methods, TS2Vec\nperforms timestamp-wise discrimination, which learns a contextual\nrepresentation vector directly for each timestamp. We find that the learned\nrepresentations have superior predictive ability. A linear regression trained\non top of the learned representations outperforms previous SOTAs for supervised\ntime series forecasting. Also, the instance-level representations can be simply\nobtained by applying a max pooling layer on top of learned representations of\nall timestamps. We conduct extensive experiments on time series classification\ntasks to evaluate the quality of instance-level representations. As a result,\nTS2Vec achieves significant improvement compared with existing SOTAs of\nunsupervised time series representation on 125 UCR datasets and 29 UEA\ndatasets. The source code is publicly available at\nhttps://github.com/yuezhihan/ts2vec.",
          "link": "http://arxiv.org/abs/2106.10466",
          "publishedOn": "2021-06-22T01:57:11.292Z",
          "wordCount": 578,
          "title": "Learning Timestamp-Level Representations for Time Series with Hierarchical Contrastive Loss. (arXiv:2106.10466v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10516",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1\">Athindran Ramesh Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramadge_P/0/1/0/all/0/1\">Peter J. Ramadge</a>",
          "description": "Since most industrial control applications use PID controllers, PID tuning\nand anti-windup measures are significant problems. This paper investigates\ntuning the feedback gains of a PID controller via back-calculation and\nautomatic differentiation tools. In particular, we episodically use a cost\nfunction to generate gradients and perform gradient descent to improve\ncontroller performance. We provide a theoretical framework for analyzing this\nnon-convex optimization and establish a relationship between back-calculation\nand disturbance feedback policies. We include numerical experiments on linear\nsystems with actuator saturation to show the efficacy of this approach.",
          "link": "http://arxiv.org/abs/2106.10516",
          "publishedOn": "2021-06-22T01:57:11.287Z",
          "wordCount": 558,
          "title": "DiffLoop: Tuning PID controllers by differentiating through the feedback loop. (arXiv:2106.10516v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goel_V/0/1/0/all/0/1\">Vidit Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Shubhika Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_H/0/1/0/all/0/1\">Harsh Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>",
          "description": "In this work we present a novel solution for Video Instance\nSegmentation(VIS), that is automatically generating instance level segmentation\nmasks along with object class and tracking them in a video. Our method improves\nthe masks from segmentation and propagation branches in an online manner using\nthe Mask Selection Network (MSN) hence limiting the noise accumulation during\nmask tracking. We propose an effective design of MSN by using patch-based\nconvolutional neural network. The network is able to distinguish between very\nsubtle differences between the masks and choose the better masks out of the\nassociated masks accurately. Further, we make use of temporal consistency and\nprocess the video sequences in both forward and reverse manner as a post\nprocessing step to recover lost objects. The proposed method can be used to\nadapt any video object segmentation method for the task of VIS. Our method\nachieves a score of 49.1 mAP on 2021 YouTube-VIS Challenge and was ranked third\nplace among more than 30 global teams. Our code will be available at\nhttps://github.com/SHI-Labs/Mask-Selection-Networks.",
          "link": "http://arxiv.org/abs/2106.10452",
          "publishedOn": "2021-06-22T01:57:11.281Z",
          "wordCount": 629,
          "title": "MSN: Efficient Online Mask Selection Network for Video Instance Segmentation. (arXiv:2106.10452v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Juyong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1\">Pradeep Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>",
          "description": "Compositional generalization is the ability to generalize systematically to a\nnew data distribution by combining known components. Although humans seem to\nhave a great ability to generalize compositionally, state-of-the-art neural\nmodels struggle to do so. In this work, we study compositional generalization\nin classification tasks and present two main contributions. First, we study\nways to convert a natural language sequence-to-sequence dataset to a\nclassification dataset that also requires compositional generalization. Second,\nwe show that providing structural hints (specifically, providing parse trees\nand entity links as attention masks for a Transformer model) helps\ncompositional generalization.",
          "link": "http://arxiv.org/abs/2106.10434",
          "publishedOn": "2021-06-22T01:57:11.266Z",
          "wordCount": 540,
          "title": "Improving Compositional Generalization in Classification Tasks via Structure Annotations. (arXiv:2106.10434v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10401",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhi Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_P/0/1/0/all/0/1\">Pengpeng Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_F/0/1/0/all/0/1\">Fulei Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_P/0/1/0/all/0/1\">Peihan Qi</a>",
          "description": "A neural network is essentially a high-dimensional complex mapping model by\nadjusting network weights for feature fitting. However, the spectral bias in\nnetwork training leads to unbearable training epochs for fitting the\nhigh-frequency components in broadband signals. To improve the fitting\nefficiency of high-frequency components, the PhaseDNN was proposed recently by\ncombining complex frequency band extraction and frequency shift techniques [Cai\net al. SIAM J. SCI. COMPUT. 42, A3285 (2020)]. Our paper is devoted to an\nalternative candidate for fitting complex signals with high-frequency\ncomponents. Here, a parallel frequency function-deep neural network (PFF-DNN)\nis proposed to suppress computational overhead while ensuring fitting accuracy\nby utilizing fast Fourier analysis of broadband signals and the spectral bias\nnature of neural networks. The effectiveness and efficiency of the proposed\nPFF-DNN method are verified based on detailed numerical experiments for six\ntypical broadband signals.",
          "link": "http://arxiv.org/abs/2106.10401",
          "publishedOn": "2021-06-22T01:57:11.257Z",
          "wordCount": 582,
          "title": "Parallel frequency function-deep neural network for efficient complex broadband signal approximation. (arXiv:2106.10401v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Ruiqing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Q/0/1/0/all/0/1\">Qiqiang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">He Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanlin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Leye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Man Huang</a>",
          "description": "The utilization of computer technology to solve problems in medical scenarios\nhas attracted considerable attention in recent years, which still has great\npotential and space for exploration. Among them, machine learning has been\nwidely used in the prediction, diagnosis and even treatment of Sepsis. However,\nstate-of-the-art methods require large amounts of labeled medical data for\nsupervised learning. In real-world applications, the lack of labeled data will\ncause enormous obstacles if one hospital wants to deploy a new Sepsis detection\nsystem. Different from the supervised learning setting, we need to use known\ninformation (e.g., from another hospital with rich labeled data) to help build\na model with acceptable performance, i.e., transfer learning. In this paper, we\npropose a semi-supervised optimal transport with self-paced ensemble framework\nfor Sepsis early detection, called SPSSOT, to transfer knowledge from the other\nthat has rich labeled data. In SPSSOT, we first extract the same clinical\nindicators from the source domain (e.g., hospital with rich labeled data) and\nthe target domain (e.g., hospital with little labeled data), then we combine\nthe semi-supervised domain adaptation based on optimal transport theory with\nself-paced under-sampling to avoid a negative transfer possibly caused by\ncovariate shift and class imbalance. On the whole, SPSSOT is an end-to-end\ntransfer learning method for Sepsis early detection which can automatically\nselect suitable samples from two domains respectively according to the number\nof iterations and align feature space of two domains. Extensive experiments on\ntwo open clinical datasets demonstrate that comparing with other methods, our\nproposed SPSSOT, can significantly improve the AUC values with only 1% labeled\ndata in the target domain in two transfer learning scenarios, MIMIC\n$rightarrow$ Challenge and Challenge $rightarrow$ MIMIC.",
          "link": "http://arxiv.org/abs/2106.10352",
          "publishedOn": "2021-06-22T01:57:11.252Z",
          "wordCount": 736,
          "title": "Semi-supervised Optimal Transport with Self-paced Ensemble for Cross-hospital Sepsis Early Detection. (arXiv:2106.10352v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antaris_S/0/1/0/all/0/1\">Stefanos Antaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafailidis_D/0/1/0/all/0/1\">Dimitrios Rafailidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arriaza_R/0/1/0/all/0/1\">Romina Arriaza</a>",
          "description": "Nowadays, live video streaming events have become a mainstay in viewer's\ncommunication in large international enterprises. Provided that viewers are\ndistributed worldwide, the main challenge resides on how to schedule the\noptimal event's time so as to improve both the viewer's engagement and\nadoption. In this paper we present a multi-task deep reinforcement learning\nmodel to select the time of a live video streaming event, aiming to optimize\nthe viewer's engagement and adoption at the same time. We consider the\nengagement and adoption of the viewers as independent tasks and formulate a\nunified loss function to learn a common policy. In addition, we account for the\nfact that each task might have different contribution to the training strategy\nof the agent. Therefore, to determine the contribution of each task to the\nagent's training, we design a Transformer's architecture for the state-action\ntransitions of each task. We evaluate our proposed model on four real-world\ndatasets, generated by the live video streaming events of four large\nenterprises spanning from January 2019 until March 2021. Our experiments\ndemonstrate the effectiveness of the proposed model when compared with several\nstate-of-the-art strategies. For reproduction purposes, our evaluation datasets\nand implementation are publicly available at\nhttps://github.com/stefanosantaris/merlin.",
          "link": "http://arxiv.org/abs/2106.10305",
          "publishedOn": "2021-06-22T01:57:11.199Z",
          "wordCount": 641,
          "title": "Multi-Task Learning for User Engagement and Adoption in Live Video Streaming Events. (arXiv:2106.10305v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2103.17228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Norelli_A/0/1/0/all/0/1\">Antonio Norelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panconesi_A/0/1/0/all/0/1\">Alessandro Panconesi</a>",
          "description": "We introduce OLIVAW, an AI Othello player adopting the design principles of\nthe famous AlphaGo series. The main motivation behind OLIVAW was to attain\nexceptional competence in a non-trivial board game at a tiny fraction of the\ncost of its illustrious predecessors. In this paper, we show how the AlphaGo\nZero's paradigm can be successfully applied to the popular game of Othello\nusing only commodity hardware and free cloud services. While being simpler than\nChess or Go, Othello maintains a considerable search space and difficulty in\nevaluating board positions. To achieve this result, OLIVAW implements some\nimprovements inspired by recent works to accelerate the standard AlphaGo Zero\nlearning process. The main modification implies doubling the positions\ncollected per game during the training phase, by including also positions not\nplayed but largely explored by the agent. We tested the strength of OLIVAW in\nthree different ways: by pitting it against Edax, the strongest open-source\nOthello engine, by playing anonymous games on the web platform OthelloQuest,\nand finally in two in-person matches against top-notch human players: a\nnational champion and a former world champion.",
          "link": "http://arxiv.org/abs/2103.17228",
          "publishedOn": "2021-06-22T01:57:10.873Z",
          "wordCount": 653,
          "title": "OLIVAW: Mastering Othello with neither Humans nor a Penny. (arXiv:2103.17228v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.07296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stojanov_S/0/1/0/all/0/1\">Stefan Stojanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thai_A/0/1/0/all/0/1\">Anh Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1\">James M. Rehg</a>",
          "description": "It is widely accepted that reasoning about object shape is important for\nobject recognition. However, the most powerful object recognition methods today\ndo not explicitly make use of object shape during learning. In this work,\nmotivated by recent developments in low-shot learning, findings in\ndevelopmental psychology, and the increased use of synthetic data in computer\nvision research, we investigate how reasoning about 3D shape can be used to\nimprove low-shot learning methods' generalization performance. We propose a new\nway to improve existing low-shot learning approaches by learning a\ndiscriminative embedding space using 3D object shape, and using this embedding\nby learning how to map images into it. Our new approach improves the\nperformance of image-only low-shot learning approaches on multiple datasets. We\nalso introduce Toys4K, a 3D object dataset with the largest number of object\ncategories currently available, which supports low-shot learning.",
          "link": "http://arxiv.org/abs/2101.07296",
          "publishedOn": "2021-06-22T01:57:10.867Z",
          "wordCount": 629,
          "title": "Using Shape to Categorize: Low-Shot Learning with an Explicit Shape Bias. (arXiv:2101.07296v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.09954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Runzhe Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>",
          "description": "In this paper, we explore the ability to model and infer personality types of\nopponents, predict their responses, and use this information to adapt a dialog\nagent's high-level strategy in negotiation tasks. Inspired by the idea of\nincorporating a theory of mind (ToM) into machines, we introduce a\nprobabilistic formulation to encapsulate the opponent's personality type during\nboth learning and inference. We test our approach on the CraigslistBargain\ndataset and show that our method using ToM inference achieves a 20% higher\ndialog agreement rate compared to baselines on a mixed population of opponents.\nWe also find that our model displays diverse negotiation behavior with\ndifferent types of opponents.",
          "link": "http://arxiv.org/abs/2010.09954",
          "publishedOn": "2021-06-22T01:57:10.861Z",
          "wordCount": 580,
          "title": "Improving Dialog Systems for Negotiation with Personality Modeling. (arXiv:2010.09954v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kongwudhikunakorn_S/0/1/0/all/0/1\">Supavit Kongwudhikunakorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiatthaveephong_S/0/1/0/all/0/1\">Suktipol Kiatthaveephong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thanontip_K/0/1/0/all/0/1\">Kamonwan Thanontip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leelaarporn_P/0/1/0/all/0/1\">Pitshaporn Leelaarporn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piriyajitakonkij_M/0/1/0/all/0/1\">Maytus Piriyajitakonkij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charoenpattarawut_T/0/1/0/all/0/1\">Thananya Charoenpattarawut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Autthasan_P/0/1/0/all/0/1\">Phairot Autthasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaisaen_R/0/1/0/all/0/1\">Rattanaphon Chaisaen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dujada_P/0/1/0/all/0/1\">Pathitta Dujada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudhawiyangkul_T/0/1/0/all/0/1\">Thapanun Sudhawiyangkul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senanarong_V/0/1/0/all/0/1\">Vorapun Senanarong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1\">Theerawit Wilaiprasitporn</a>",
          "description": "In the status quo, dementia is yet to be cured. Precise diagnosis prior to\nthe onset of the symptoms can prevent the rapid progression of the emerging\ncognitive impairment. Recent progress has shown that Electroencephalography\n(EEG) is the promising and cost-effective test to facilitate the detection of\nneurocognitive disorders. However, most of the existing works have been using\nonly resting-state EEG. The efficiencies of EEG signals from various cognitive\ntasks, for dementia classification, have yet to be thoroughly investigated. In\nthis study, we designed four cognitive tasks that engage different cognitive\nperformances: attention, working memory, and executive function. We\ninvestigated these tasks by using statistical analysis on both time and\nfrequency domains of EEG signals from three classes of human subjects: Dementia\n(DEM), Mild Cognitive Impairment (MCI), and Normal Control (NC). We also\nfurther evaluated the classification performances of two features extraction\nmethods: Principal Component Analysis (PCA) and Filter Bank Common Spatial\nPattern (FBCSP). We found that the working memory related tasks yielded good\nperformances for dementia recognition in both cases using PCA and FBCSP.\nMoreover, FBCSP with features combination from four tasks revealed the best\nsensitivity of 0.87 and the specificity of 0.80. To our best knowledge, this is\nthe first work that concurrently investigated several cognitive tasks for\ndementia recognition using both statistical analysis and classification scores.\nOur results give essential information to design and aid in conducting further\nexperimental tasks to early diagnose dementia patients.",
          "link": "http://arxiv.org/abs/2103.03854",
          "publishedOn": "2021-06-22T01:57:10.855Z",
          "wordCount": 733,
          "title": "A Pilot Study on Visually Stimulated Cognitive Tasks for EEG-Based Dementia Recognition. (arXiv:2103.03854v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolczyk_M/0/1/0/all/0/1\">Maciej Wo&#x142;czyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zajac_M/0/1/0/all/0/1\">Micha&#x142; Zaj&#x105;c</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kucinski_L/0/1/0/all/0/1\">&#x141;ukasz Kuci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milos_P/0/1/0/all/0/1\">Piotr Mi&#x142;o&#x15b;</a>",
          "description": "Continual learning (CL) -- the ability to continuously learn, building on\npreviously acquired knowledge -- is a natural requirement for long-lived\nautonomous reinforcement learning (RL) agents. While building such agents, one\nneeds to balance opposing desiderata, such as constraints on capacity and\ncompute, the ability to not catastrophically forget, and to exhibit positive\ntransfer on new tasks. Understanding the right trade-off is conceptually and\ncomputationally challenging, which we argue has led the community to overly\nfocus on catastrophic forgetting. In response to these issues, we advocate for\nthe need to prioritize forward transfer and propose Continual World, a\nbenchmark consisting of realistic and meaningfully diverse robotic tasks built\non top of Meta-World as a testbed. Following an in-depth empirical evaluation\nof existing CL methods, we pinpoint their limitations and highlight unique\nalgorithmic challenges in the RL setting. Our benchmark aims to provide a\nmeaningful and computationally inexpensive challenge for the community and thus\nhelp better understand the performance of existing and future solutions.",
          "link": "http://arxiv.org/abs/2105.10919",
          "publishedOn": "2021-06-22T01:57:10.838Z",
          "wordCount": 633,
          "title": "Continual World: A Robotic Benchmark For Continual Reinforcement Learning. (arXiv:2105.10919v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhangyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Cheng Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan.Z.Li</a>",
          "description": "Deep learning on graphs has recently achieved remarkable success on a variety\nof tasks while such success relies heavily on the massive and carefully labeled\ndata. However, precise annotations are generally very expensive and\ntime-consuming. To address this problem, self-supervised learning (SSL) is\nemerging as a new paradigm for extracting informative knowledge through\nwell-designed pretext tasks without relying on manual labels. In this survey,\nwe extend the concept of SSL, which first emerged in the fields of computer\nvision and natural language processing, to present a timely and comprehensive\nreview of the existing SSL techniques for graph data. Specifically, we divide\nexisting graph SSL methods into three categories: contrastive, generative, and\npredictive. More importantly, unlike many other surveys that only provide a\nhigh-level description of published research, we present an additional\nmathematical summary of the existing works in a unified framework. Furthermore,\nto facilitate methodological development and empirical comparisons, we also\nsummarize the commonly used datasets, evaluation metrics, downstream tasks, and\nopen-source implementations of various algorithms. Finally, we discuss the\ntechnical challenges and potential future directions for improving graph\nself-supervised learning.",
          "link": "http://arxiv.org/abs/2105.07342",
          "publishedOn": "2021-06-22T01:57:10.832Z",
          "wordCount": 649,
          "title": "Self-supervised on Graphs: Contrastive, Generative,or Predictive. (arXiv:2105.07342v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08489",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perrone_V/0/1/0/all/0/1\">Valerio Perrone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huibin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zolic_A/0/1/0/all/0/1\">Aida Zolic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shcherbatyi_I/0/1/0/all/0/1\">Iaroslav Shcherbatyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Amr Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_T/0/1/0/all/0/1\">Tanya Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donini_M/0/1/0/all/0/1\">Michele Donini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winkelmolen_F/0/1/0/all/0/1\">Fela Winkelmolen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1\">Rodolphe Jenatton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faddoul_J/0/1/0/all/0/1\">Jean Baptiste Faddoul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pogorzelska_B/0/1/0/all/0/1\">Barbara Pogorzelska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miladinovic_M/0/1/0/all/0/1\">Miroslav Miladinovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenthapadi_K/0/1/0/all/0/1\">Krishnaram Kenthapadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeger_M/0/1/0/all/0/1\">Matthias Seeger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Archambeau_C/0/1/0/all/0/1\">C&#xe9;dric Archambeau</a>",
          "description": "Tuning complex machine learning systems is challenging. Machine learning\ntypically requires to set hyperparameters, be it regularization, architecture,\nor optimization parameters, whose tuning is critical to achieve good predictive\nperformance. To democratize access to machine learning systems, it is essential\nto automate the tuning. This paper presents Amazon SageMaker Automatic Model\nTuning (AMT), a fully managed system for gradient-free optimization at scale.\nAMT finds the best version of a trained machine learning model by repeatedly\nevaluating it with different hyperparameter configurations. It leverages either\nrandom search or Bayesian optimization to choose the hyperparameter values\nresulting in the best model, as measured by the metric chosen by the user. AMT\ncan be used with built-in algorithms, custom algorithms, and Amazon SageMaker\npre-built containers for machine learning frameworks. We discuss the core\nfunctionality, system architecture, our design principles, and lessons learned.\nWe also describe more advanced features of AMT, such as automated early\nstopping and warm-starting, showing in experiments their benefits to users.",
          "link": "http://arxiv.org/abs/2012.08489",
          "publishedOn": "2021-06-22T01:57:10.827Z",
          "wordCount": 654,
          "title": "Amazon SageMaker Automatic Model Tuning: Scalable Gradient-Free Optimization. (arXiv:2012.08489v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Montesinos_J/0/1/0/all/0/1\">Juan F. Montesinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadandale_V/0/1/0/all/0/1\">Venkatesh S. Kadandale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haro_G/0/1/0/all/0/1\">Gloria Haro</a>",
          "description": "Music source separation can be interpreted as the estimation of the\nconstituent music sources that a music clip is composed of. In this work, we\nexplore the single-channel singing voice separation problem from a multimodal\nperspective, by jointly learning from audio and visual modalities. To do so, we\npresent Acappella, a dataset spanning around 46 hours of a cappella solo\nsinging videos sourced from YouTube. We propose Y-Net, an audio-visual\nconvolutional neural network which achieves state-of-the-art singing voice\nseparation results on the Acappella dataset and compare it against its\naudio-only counterpart, U-Net, and a state-of-the-art audio-visual speech\nseparation model. Singing voice separation can be particularly challenging when\nthe audio mixture also comprises of other accompaniment voices and background\nsounds along with the target voice of interest. We demonstrate that our model\ncan outperform the baseline models in the singing voice separation task in such\nchallenging scenarios. The code, the pre-trained models and the dataset will be\npublicly available at https://ipcv.github.io/Acappella/",
          "link": "http://arxiv.org/abs/2104.09946",
          "publishedOn": "2021-06-22T01:57:10.821Z",
          "wordCount": 645,
          "title": "A cappella: Audio-visual Singing Voice Separation. (arXiv:2104.09946v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08143",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Nikitin_O/0/1/0/all/0/1\">Oleg Nikitin</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lukyanova_O/0/1/0/all/0/1\">Olga Lukyanova</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kunin_A/0/1/0/all/0/1\">Alex Kunin</a>",
          "description": "Biological neurons have adaptive nature and perform complex computations\ninvolving the filtering of redundant information. However, most common neural\ncell models, including biologically plausible, such as Hodgkin-Huxley or\nIzhikevich, do not possess predictive dynamics on a single-cell level.\nMoreover, the modern rules of synaptic plasticity or interconnections weights\nadaptation also do not provide grounding for the ability of neurons to adapt to\nthe ever-changing input signal intensity. While natural neuron synaptic growth\nis precisely controlled and restricted by protein supply and recycling, weight\ncorrection rules such as widely used STDP are efficiently unlimited in change\nrate and scale. The present article introduces new mechanics of interconnection\nbetween neuron firing rate homeostasis and weight change through STDP growth\nbounded by abstract protein reserve, controlled by the intracellular\noptimization algorithm. We show how these cellular dynamics help neurons filter\nout the intense noise signals to help neurons keep a stable firing rate. We\nalso examine that such filtering does not affect the ability of neurons to\nrecognize the correlated inputs in unsupervised mode. Such an approach might be\nused in the machine learning domain to improve the robustness of AI systems.",
          "link": "http://arxiv.org/abs/2103.08143",
          "publishedOn": "2021-06-22T01:57:10.804Z",
          "wordCount": 677,
          "title": "Constrained plasticity reserve as a natural way to control frequency and weights in spiking neural networks. (arXiv:2103.08143v2 [q-bio.NC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baghi_B/0/1/0/all/0/1\">Bobak H. Baghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1\">Gregory Dudek</a>",
          "description": "In this paper, we present an algorithm to efficiently learn\nsocially-compliant navigation policies from observations of human trajectories.\nAs mobile robots come to inhabit and traffic social spaces, they must account\nfor social cues and behave in a socially compliant manner. We focus on learning\nsuch cues from examples. We describe an inverse reinforcement learning based\nalgorithm which learns from human trajectory observations without knowing their\nspecific actions. We increase the sample-efficiency of our approach over\nalternative methods by leveraging the notion of a replay buffer (found in many\noff-policy reinforcement learning methods) to eliminate the additional sample\ncomplexity associated with inverse reinforcement learning. We evaluate our\nmethod by training agents using publicly available pedestrian motion data sets\nand compare it to related methods. We show that our approach yields better\nperformance while also decreasing training time and sample complexity.",
          "link": "http://arxiv.org/abs/2106.10318",
          "publishedOn": "2021-06-22T01:57:10.798Z",
          "wordCount": 574,
          "title": "Sample Efficient Social Navigation Using Inverse Reinforcement Learning. (arXiv:2106.10318v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10324",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farnia_F/0/1/0/all/0/1\">Farzan Farnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghazadeh_A/0/1/0/all/0/1\">Amirali Aghazadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tse_D/0/1/0/all/0/1\">David Tse</a>",
          "description": "Robust training methods against perturbations to the input data have received\ngreat attention in the machine learning literature. A standard approach in this\ndirection is adversarial training which learns a model using\nadversarially-perturbed training samples. However, adversarial training\nperforms suboptimally against perturbations structured across samples such as\nuniversal and group-sparse shifts that are commonly present in biological data\nsuch as gene expression levels of different tissues. In this work, we seek to\nclose this optimality gap and introduce Group-Structured Adversarial Training\n(GSAT) which learns a model robust to perturbations structured across samples.\nWe formulate GSAT as a non-convex concave minimax optimization problem which\nminimizes a group-structured optimal transport cost. Specifically, we focus on\nthe applications of GSAT for group-sparse and rank-constrained perturbations\nmodeled using group and nuclear norm penalties. In order to solve GSAT's\nnon-smooth optimization problem in those cases, we propose a new minimax\noptimization algorithm called GDADMM by combining Gradient Descent Ascent (GDA)\nand Alternating Direction Method of Multipliers (ADMM). We present several\napplications of the GSAT framework to gain robustness against structured\nperturbations for image recognition and computational biology datasets.",
          "link": "http://arxiv.org/abs/2106.10324",
          "publishedOn": "2021-06-22T01:57:10.792Z",
          "wordCount": 607,
          "title": "Group-Structured Adversarial Training. (arXiv:2106.10324v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Triantafillou_E/0/1/0/all/0/1\">Eleni Triantafillou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1\">Hugo Larochelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1\">Vincent Dumoulin</a>",
          "description": "Few-shot dataset generalization is a challenging variant of the well-studied\nfew-shot classification problem where a diverse training set of several\ndatasets is given, for the purpose of training an adaptable model that can then\nlearn classes from new datasets using only a few examples. To this end, we\npropose to utilize the diverse training set to construct a universal template:\na partial model that can define a wide array of dataset-specialized models, by\nplugging in appropriate components. For each new few-shot classification\nproblem, our approach therefore only requires inferring a small number of\nparameters to insert into the universal template. We design a separate network\nthat produces an initialization of those parameters for each given task, and we\nthen fine-tune its proposed initialization via a few steps of gradient descent.\nOur approach is more parameter-efficient, scalable and adaptable compared to\nprevious methods, and achieves the state-of-the-art on the challenging\nMeta-Dataset benchmark.",
          "link": "http://arxiv.org/abs/2105.07029",
          "publishedOn": "2021-06-22T01:57:10.787Z",
          "wordCount": 618,
          "title": "Learning a Universal Template for Few-shot Dataset Generalization. (arXiv:2105.07029v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.09508",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Veerabadran_V/0/1/0/all/0/1\">Vijay Veerabadran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pourreza_R/0/1/0/all/0/1\">Reza Pourreza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Habibian_A/0/1/0/all/0/1\">Amirhossein Habibian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_T/0/1/0/all/0/1\">Taco Cohen</a>",
          "description": "In this paper, we present a novel adversarial lossy video compression model.\nAt extremely low bit-rates, standard video coding schemes suffer from\nunpleasant reconstruction artifacts such as blocking, ringing etc. Existing\nlearned neural approaches to video compression have achieved reasonable success\non reducing the bit-rate for efficient transmission and reduce the impact of\nartifacts to an extent. However, they still tend to produce blurred results\nunder extreme compression. In this paper, we present a deep adversarial learned\nvideo compression model that minimizes an auxiliary adversarial distortion\nobjective. We find this adversarial objective to correlate better with human\nperceptual quality judgement relative to traditional quality metrics such as\nMS-SSIM and PSNR. Our experiments using a state-of-the-art learned video\ncompression system demonstrate a reduction of perceptual artifacts and\nreconstruction of detail lost especially under extremely high compression.",
          "link": "http://arxiv.org/abs/2004.09508",
          "publishedOn": "2021-06-22T01:57:10.781Z",
          "wordCount": 614,
          "title": "Adversarial Distortion for Learned Video Compression. (arXiv:2004.09508v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01496",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxiao Wang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianhao Wang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lun Wang</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Luo_N/0/1/0/all/0/1\">Nanqing Luo</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a> (5) ((1) Tsinghua University, (2) Harvard University, (3) University of California, Berkeley, (4) Huazhong University of Science and Technology, (5) Virginia Tech)",
          "description": "Deep learning techniques have achieved remarkable performance in wide-ranging\ntasks. However, when trained on privacy-sensitive datasets, the model\nparameters may expose private information in training data. Prior attempts for\ndifferentially private training, although offering rigorous privacy guarantees,\nlead to much lower model performance than the non-private ones. Besides,\ndifferent runs of the same training algorithm produce models with large\nperformance variance. To address these issues, we propose DPlis--Differentially\nPrivate Learning wIth Smoothing. The core idea of DPlis is to construct a\nsmooth loss function that favors noise-resilient models lying in large flat\nregions of the loss landscape. We provide theoretical justification for the\nutility improvements of DPlis. Extensive experiments also demonstrate that\nDPlis can effectively boost model quality and training stability under a given\nprivacy budget.",
          "link": "http://arxiv.org/abs/2103.01496",
          "publishedOn": "2021-06-22T01:57:10.775Z",
          "wordCount": 646,
          "title": "DPlis: Boosting Utility of Differentially Private Deep Learning via Randomized Smoothing. (arXiv:2103.01496v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Attia_A/0/1/0/all/0/1\">Amit Attia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1\">Tomer Koren</a>",
          "description": "We study the algorithmic stability of Nesterov's accelerated gradient method.\nFor convex quadratic objectives, Chen et al. (2018) proved that the uniform\nstability of the method grows quadratically with the number of optimization\nsteps, and conjectured that the same is true for the general convex and smooth\ncase. We disprove this conjecture and show, for two notions of algorithmic\nstability (including uniform stability), that the stability of Nesterov's\naccelerated method in fact deteriorates exponentially fast with the number of\ngradient steps. This stands in sharp contrast to the bounds in the quadratic\ncase, but also to known results for non-accelerated gradient methods where\nstability typically grows linearly with the number of steps.",
          "link": "http://arxiv.org/abs/2102.02167",
          "publishedOn": "2021-06-22T01:57:10.757Z",
          "wordCount": 574,
          "title": "Algorithmic Instabilities of Accelerated Gradient Descent. (arXiv:2102.02167v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04628",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Molnar_C/0/1/0/all/0/1\">Christoph Molnar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Konig_G/0/1/0/all/0/1\">Gunnar K&#xf6;nig</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Casalicchio_G/0/1/0/all/0/1\">Giuseppe Casalicchio</a>",
          "description": "The interpretation of feature importance in machine learning models is\nchallenging when features are dependent. Permutation feature importance (PFI)\nignores such dependencies, which can cause misleading interpretations due to\nextrapolation. A possible remedy is more advanced conditional PFI approaches\nthat enable the assessment of feature importance conditional on all other\nfeatures. Due to this shift in perspective and in order to enable correct\ninterpretations, it is therefore important that the conditioning is transparent\nand humanly comprehensible. In this paper, we propose a new sampling mechanism\nfor the conditional distribution based on permutations in conditional\nsubgroups. As these subgroups are constructed using decision trees\n(transformation trees), the conditioning becomes inherently interpretable. This\nnot only provides a simple and effective estimator of conditional PFI, but also\nlocal PFI estimates within the subgroups. In addition, we apply the conditional\nsubgroups approach to partial dependence plots (PDP), a popular method for\ndescribing feature effects that can also suffer from extrapolation when\nfeatures are dependent and interactions are present in the model. We show that\nPFI and PDP based on conditional subgroups often outperform methods such as\nconditional PFI based on knockoffs, or accumulated local effect plots.\nFurthermore, our approach allows for a more fine-grained interpretation of\nfeature effects and importance within the conditional subgroups.",
          "link": "http://arxiv.org/abs/2006.04628",
          "publishedOn": "2021-06-22T01:57:10.752Z",
          "wordCount": 670,
          "title": "Model-agnostic Feature Importance and Effects with Dependent Features -- A Conditional Subgroup Approach. (arXiv:2006.04628v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atashgahi_Z/0/1/0/all/0/1\">Zahra Atashgahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Lu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kou_H/0/1/0/all/0/1\">Huanyu Kou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1\">Decebal Constantin Mocanu</a>",
          "description": "Works on lottery ticket hypothesis (LTH) and single-shot network pruning\n(SNIP) have raised a lot of attention currently on post-training pruning\n(iterative magnitude pruning), and before-training pruning (pruning at\ninitialization). The former method suffers from an extremely large computation\ncost and the latter category of methods usually struggles with insufficient\nperformance. In comparison, during-training pruning, a class of pruning methods\nthat simultaneously enjoys the training/inference efficiency and the comparable\nperformance, temporarily, has been less explored. To better understand\nduring-training pruning, we quantitatively study the effect of pruning\nthroughout training from the perspective of pruning plasticity (the ability of\nthe pruned networks to recover the original performance). Pruning plasticity\ncan help explain several other empirical observations about neural network\npruning in literature. We further find that pruning plasticity can be\nsubstantially improved by injecting a brain-inspired mechanism called\nneuroregeneration, i.e., to regenerate the same number of connections as\npruned. Based on the insights from pruning plasticity, we design a novel\ngradual magnitude pruning (GMP) method, named gradual pruning with zero-cost\nneuroregeneration (GraNet), and its dynamic sparse training (DST) variant\n(GraNet-ST). Both of them advance state of the art. Perhaps most impressively,\nthe latter for the first time boosts the sparse-to-sparse training performance\nover various dense-to-sparse methods by a large margin with ResNet-50 on\nImageNet. We will release all codes.",
          "link": "http://arxiv.org/abs/2106.10404",
          "publishedOn": "2021-06-22T01:57:10.741Z",
          "wordCount": 668,
          "title": "Sparse Training via Boosting Pruning Plasticity with Neuroregeneration. (arXiv:2106.10404v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Palma_I/0/1/0/all/0/1\">Ignacio Tampe Palma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendoza_M/0/1/0/all/0/1\">Marcelo Mendoza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milios_E/0/1/0/all/0/1\">Evangelos Milios</a>",
          "description": "Summarization has usually relied on gold standard summaries to train\nextractive or abstractive models. Social media brings a hurdle to summarization\ntechniques since it requires addressing a multi-document multi-author approach.\nWe address this challenging task by introducing a novel method that generates\nabstractive summaries of online news discussions. Our method extends a\nBERT-based architecture, including an attention encoding that fed comments'\nlikes during the training stage. To train our model, we define a task which\nconsists of reconstructing high impact comments based on popularity (likes).\nAccordingly, our model learns to summarize online discussions based on their\nmost relevant comments. Our novel approach provides a summary that represents\nthe most relevant aspects of a news item that users comment on, incorporating\nthe social context as a source of information to summarize texts in online\nsocial networks. Our model is evaluated using ROUGE scores between the\ngenerated summary and each comment on the thread. Our model, including the\nsocial attention encoding, significantly outperforms both extractive and\nabstractive summarization methods based on such evaluation.",
          "link": "http://arxiv.org/abs/2106.03953",
          "publishedOn": "2021-06-22T01:57:10.735Z",
          "wordCount": 623,
          "title": "Neural Abstractive Unsupervised Summarization of Online News Discussions. (arXiv:2106.03953v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "This paper makes two scientific contributions to the field of\nexoskeleton-based action and movement recognition. First, it presents a novel\nmachine learning and pattern recognition-based framework that can detect a wide\nrange of actions and movements - walking, walking upstairs, walking downstairs,\nsitting, standing, lying, stand to sit, sit to stand, sit to lie, lie to sit,\nstand to lie, and lie to stand, with an overall accuracy of 82.63%. Second, it\npresents a comprehensive comparative study of different learning approaches -\nRandom Forest, Artificial Neural Network, Decision Tree, Multiway Decision\nTree, Support Vector Machine, k-NN, Gradient Boosted Trees, Decision Stump,\nAuto MLP, Linear Regression, Vector Linear Regression, Random Tree, Na\\\"ive\nBayes, Na\\\"ive Bayes (Kernel), Linear Discriminant Analysis, Quadratic\nDiscriminant Analysis, and Deep Learning applied to this framework. The\nperformance of each of these learning approaches was boosted by using the\nAdaBoost algorithm, and the Cross Validation approach was used for training and\ntesting. The results show that in boosted form, the k- NN classifier\noutperforms all the other boosted learning approaches and is, therefore, the\noptimal learning method for this purpose. The results presented and discussed\nuphold the importance of this work to contribute towards augmenting the\nabilities of exoskeleton-based assisted and independent living of the elderly\nin the future of Internet of Things-based living environments, such as Smart\nHomes. As a specific use case, we also discuss how the findings of our work are\nrelevant for augmenting the capabilities of the Hybrid Assistive Limb\nexoskeleton, a highly functional lower limb exoskeleton.",
          "link": "http://arxiv.org/abs/2106.10331",
          "publishedOn": "2021-06-22T01:57:10.720Z",
          "wordCount": 719,
          "title": "Exoskeleton-Based Multimodal Action and Movement Recognition: Identifying and Developing the Optimal Boosted Learning Approach. (arXiv:2106.10331v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2007.16041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_U/0/1/0/all/0/1\">Usman Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Mahfuzur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedorov_A/0/1/0/all/0/1\">Alex Fedorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_N/0/1/0/all/0/1\">Noah Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zening Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calhoun_V/0/1/0/all/0/1\">Vince D. Calhoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plis_S/0/1/0/all/0/1\">Sergey M. Plis</a>",
          "description": "Behavioral changes are the earliest signs of a mental disorder, but arguably,\nthe dynamics of brain function gets affected even earlier. Subsequently,\nspatio-temporal structure of disorder-specific dynamics is crucial for early\ndiagnosis and understanding the disorder mechanism. A common way of learning\ndiscriminatory features relies on training a classifier and evaluating feature\nimportance. Classical classifiers, based on handcrafted features are quite\npowerful, but suffer the curse of dimensionality when applied to large input\ndimensions of spatio-temporal data. Deep learning algorithms could handle the\nproblem and a model introspection could highlight discriminatory\nspatio-temporal regions but need way more samples to train. In this paper we\npresent a novel self supervised training schema which reinforces whole sequence\nmutual information local to context (whole MILC). We pre-train the whole MILC\nmodel on unlabeled and unrelated healthy control data. We test our model on\nthree different disorders (i) Schizophrenia (ii) Autism and (iii) Alzheimers\nand four different studies. Our algorithm outperforms existing self-supervised\npre-training methods and provides competitive classification results to\nclassical machine learning algorithms. Importantly, whole MILC enables\nattribution of subject diagnosis to specific spatio-temporal regions in the\nfMRI signal.",
          "link": "http://arxiv.org/abs/2007.16041",
          "publishedOn": "2021-06-22T01:57:10.715Z",
          "wordCount": 681,
          "title": "Whole MILC: generalizing learned dynamics across tasks, datasets, and populations. (arXiv:2007.16041v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08102",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Corani_G/0/1/0/all/0/1\">Giorgio Corani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Benavoli_A/0/1/0/all/0/1\">Alessio Benavoli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zaffalon_M/0/1/0/all/0/1\">Marco Zaffalon</a>",
          "description": "Automatic forecasting is the task of receiving a time series and returning a\nforecast for the next time steps without any human intervention. Gaussian\nProcesses (GPs) are a powerful tool for modeling time series, but so far there\nare no competitive approaches for automatic forecasting based on GPs. We\npropose practical solutions to two problems: automatic selection of the optimal\nkernel and reliable estimation of the hyperparameters. We propose a fixed\ncomposition of kernels, which contains the components needed to model most time\nseries: linear trend, periodic patterns, and other flexible kernel for modeling\nthe non-linear trend. Not all components are necessary to model each time\nseries; during training the unnecessary components are automatically made\nirrelevant via automatic relevance determination (ARD). We moreover assign\npriors to the hyperparameters, in order to keep the inference within a\nplausible range; we design such priors through an empirical Bayes approach. We\npresent results on many time series of different types; our GP model is more\naccurate than state-of-the-art time series models. Thanks to the priors, a\nsingle restart is enough the estimate the hyperparameters; hence the model is\nalso fast to train.",
          "link": "http://arxiv.org/abs/2009.08102",
          "publishedOn": "2021-06-22T01:57:10.710Z",
          "wordCount": 638,
          "title": "Time series forecasting with Gaussian Processes needs priors. (arXiv:2009.08102v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.05719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raffin_A/0/1/0/all/0/1\">Antonin Raffin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kober_J/0/1/0/all/0/1\">Jens Kober</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stulp_F/0/1/0/all/0/1\">Freek Stulp</a>",
          "description": "Reinforcement learning (RL) enables robots to learn skills from interactions\nwith the real world. In practice, the unstructured step-based exploration used\nin Deep RL -- often very successful in simulation -- leads to jerky motion\npatterns on real robots. Consequences of the resulting shaky behavior are poor\nexploration, or even damage to the robot. We address these issues by adapting\nstate-dependent exploration (SDE) to current Deep RL algorithms. To enable this\nadaptation, we propose two extensions to the original SDE, using more general\nfeatures and re-sampling the noise periodically, which leads to a new\nexploration method generalized state-dependent exploration (gSDE). We evaluate\ngSDE both in simulation, on PyBullet continuous control tasks, and directly on\nthree different real robots: a tendon-driven elastic robot, a quadruped and an\nRC car. The noise sampling interval of gSDE permits to have a compromise\nbetween performance and smoothness, which allows training directly on the real\nrobots without loss of performance. The code is available at\nhttps://github.com/DLR-RM/stable-baselines3.",
          "link": "http://arxiv.org/abs/2005.05719",
          "publishedOn": "2021-06-22T01:57:10.704Z",
          "wordCount": 631,
          "title": "Smooth Exploration for Robotic Reinforcement Learning. (arXiv:2005.05719v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azad_A/0/1/0/all/0/1\">Abdus Salam Azad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Edward Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiancheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kimin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seshia_S/0/1/0/all/0/1\">Sanjit A. Seshia</a>",
          "description": "The capability of reinforcement learning (RL) agent directly depends on the\ndiversity of learning scenarios the environment generates and how closely it\ncaptures real-world situations. However, existing environments/simulators lack\nthe support to systematically model distributions over initial states and\ntransition dynamics. Furthermore, in complex domains such as soccer, the space\nof possible scenarios is infinite, which makes it impossible for one research\ngroup to provide a comprehensive set of scenarios to train, test, and benchmark\nRL algorithms. To address this issue, for the first time, we adopt an existing\nformal scenario specification language, SCENIC, to intuitively model and\ngenerate interactive scenarios. We interfaced SCENIC to Google Research Soccer\nenvironment to create a platform called SCENIC4RL. Using this platform, we\nprovide a dataset consisting of 36 scenario programs encoded in SCENIC and\ndemonstration data generated from a subset of them. We share our experimental\nresults to show the effectiveness of our dataset and the platform to train,\ntest, and benchmark RL algorithms. More importantly, we open-source our\nplatform to enable RL community to collectively contribute to constructing a\ncomprehensive set of scenarios.",
          "link": "http://arxiv.org/abs/2106.10365",
          "publishedOn": "2021-06-22T01:57:10.677Z",
          "wordCount": 633,
          "title": "Scenic4RL: Programmatic Modeling and Generation of Reinforcement Learning Environments. (arXiv:2106.10365v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10374",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1\">Pan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiapeng Zhang</a>",
          "description": "Motivated by applications in crowdsourced entity resolution in database,\nsigned edge prediction in social networks and correlation clustering, Mazumdar\nand Saha [NIPS 2017] proposed an elegant theoretical model for studying\nclustering with a faulty oracle. In this model, given a set of $n$ items which\nbelong to $k$ unknown groups (or clusters), our goal is to recover the clusters\nby asking pairwise queries to an oracle. This oracle can answer the query that\n``do items $u$ and $v$ belong to the same cluster?''. However, the answer to\neach pairwise query errs with probability $\\varepsilon$, for some\n$\\varepsilon\\in(0,\\frac12)$. Mazumdar and Saha provided two algorithms under\nthis model: one algorithm is query-optimal while time-inefficient (i.e.,\nrunning in quasi-polynomial time), the other is time efficient (i.e., in\npolynomial time) while query-suboptimal. Larsen, Mitzenmacher and Tsourakakis\n[WWW 2020] then gave a new time-efficient algorithm for the special case of $2$\nclusters, which is query-optimal if the bias $\\delta:=1-2\\varepsilon$ of the\nmodel is large. It was left as an open question whether one can obtain a\nquery-optimal, time-efficient algorithm for the general case of $k$ clusters\nand other regimes of $\\delta$.\n\nIn this paper, we make progress on the above question and provide a\ntime-efficient algorithm with nearly-optimal query complexity (up to a factor\nof $O(\\log^2 n)$) for all constant $k$ and any $\\delta$ in the regime when\ninformation-theoretic recovery is possible. Our algorithm is built on a\nconnection to the stochastic block model.",
          "link": "http://arxiv.org/abs/2106.10374",
          "publishedOn": "2021-06-22T01:57:10.651Z",
          "wordCount": 697,
          "title": "Towards a Query-Optimal and Time-Efficient Algorithm for Clustering with a Faulty Oracle. (arXiv:2106.10374v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10356",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ren_Y/0/1/0/all/0/1\">Yili Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>",
          "description": "The popularity of Internet-of-Things (IoT) has provided us with unprecedented\nopportunities to enable a variety of emerging services in a smart home\nenvironment. Among those services, sensing the liquid level in a container is\ncritical to building many smart home and mobile healthcare applications that\nimprove the quality of life. This paper presents LiquidSense, a liquid-level\nsensing system that is low-cost, high accuracy, widely applicable to different\ndaily liquids and containers, and can be easily integrated with existing smart\nhome networks. LiquidSense uses an existing home WiFi network and a low-cost\ntransducer that attached to the container to sense the resonance of the\ncontainer for liquid level detection. In particular, our system mounts a\nlow-cost transducer on the surface of the container and emits a well-designed\nchirp signal to make the container resonant, which introduces subtle changes to\nthe home WiFi signals. By analyzing the subtle phase changes of the WiFi\nsignals, LiquidSense extracts the resonance frequency as a feature for liquid\nlevel detection. Our system constructs prediction models for both continuous\nand discrete predictions using curve fitting and SVM respectively. We evaluate\nLiquidSense in home environments with containers of three different materials\nand six types of liquids. Results show that LiquidSense achieves an overall\naccuracy of 97% for continuous prediction and an overall F-score of 0.968 for\ndiscrete prediction. Results also show that our system has a large coverage in\na home environment and works well under non-line-of-sight (NLOS) scenarios.",
          "link": "http://arxiv.org/abs/2106.10356",
          "publishedOn": "2021-06-22T01:57:10.626Z",
          "wordCount": 668,
          "title": "Liquid Sensing Using WiFi Signals. (arXiv:2106.10356v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moreira_T/0/1/0/all/0/1\">T&#xfa;lio Marcondes Moreira</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Faria_J/0/1/0/all/0/1\">Jackson Geraldo de Faria Jr</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Melo_P/0/1/0/all/0/1\">Pedro O.S. Vaz de Melo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chaimowicz_L/0/1/0/all/0/1\">Luiz Chaimowicz</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Medeiros_Ribeiro_G/0/1/0/all/0/1\">Gilberto Medeiros-Ribeiro</a> (1) ((1) Universidade Federal de Minas Gerais, Belo Horizonte, Brazil)",
          "description": "Tidal range structures have been considered for large scale electricity\ngeneration for their potential ability to produce reasonable predictable energy\nwithout the emission of greenhouse gases. Once the main forcing components for\ndriving the tides have deterministic dynamics, the available energy in a given\ntidal power plant has been estimated, through analytical and numerical\noptimisation routines, as a mostly predictable event. This constraint imposes\nstate-of-art flexible operation methods to rely on tidal predictions\n(concurrent with measured data and up to a multiple of half-tidal cycles into\nthe future) to infer best operational strategies for tidal lagoons, with the\nadditional cost of requiring to run optimisation routines for every new tide.\nIn this paper, we propose a novel optimised operation of tidal lagoons with\nproximal policy optimisation through Unity ML-Agents. We compare this technique\nwith 6 different operation optimisation approaches (baselines) devised from the\nliterature, utilising the Swansea Bay Tidal Lagoon as a case study. We show\nthat our approach is successful in maximising energy generation through an\noptimised operational policy of turbines and sluices, yielding competitive\nresults with state-of-the-art methods of optimisation, regardless of test data\nused, requiring training once and performing real-time flexible control with\nmeasured ocean data only.",
          "link": "http://arxiv.org/abs/2106.10360",
          "publishedOn": "2021-06-22T01:57:10.620Z",
          "wordCount": 681,
          "title": "Prediction-Free, Real-Time Flexible Control of Tidal Lagoons through Proximal Policy Optimisation: A Case Study for the Swansea Lagoon. (arXiv:2106.10360v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drechsler_J/0/1/0/all/0/1\">Joerg Drechsler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globus_Harris_I/0/1/0/all/0/1\">Ira Globus-Harris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMillan_A/0/1/0/all/0/1\">Audra McMillan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarathy_J/0/1/0/all/0/1\">Jayshree Sarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1\">Adam Smith</a>",
          "description": "Differential privacy is a restriction on data processing algorithms that\nprovides strong confidentiality guarantees for individual records in the data.\nHowever, research on proper statistical inference, that is, research on\nproperly quantifying the uncertainty of the (noisy) sample estimate regarding\nthe true value in the population, is currently still limited. This paper\nproposes and evaluates several strategies to compute valid differentially\nprivate confidence intervals for the median. Instead of computing a\ndifferentially private point estimate and deriving its uncertainty, we directly\nestimate the interval bounds and discuss why this approach is superior if\nensuring privacy is important. We also illustrate that addressing both sources\nof uncertainty--the error from sampling and the error from protecting the\noutput--simultaneously should be preferred over simpler approaches that\nincorporate the uncertainty in a sequential fashion. We evaluate the\nperformance of the different algorithms under various parameter settings in\nextensive simulation studies and demonstrate how the findings could be applied\nin practical settings using data from the 1940 Decennial Census.",
          "link": "http://arxiv.org/abs/2106.10333",
          "publishedOn": "2021-06-22T01:57:10.614Z",
          "wordCount": 615,
          "title": "Non-parametric Differentially Private Confidence Intervals for the Median. (arXiv:2106.10333v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10277",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_L/0/1/0/all/0/1\">Lizhong Yao</a>",
          "description": "In this paper, we introduce a new acoustic leakage dataset of gas pipelines,\ncalled as GPLA-12, which has 12 categories over 684 training/testing acoustic\nsignals. Unlike massive image and voice datasets, there have relatively few\nacoustic signal datasets, especially for engineering fault detection. In order\nto enhance the development of fault diagnosis, we collect acoustic leakage\nsignals on the basis of an intact gas pipe system with external artificial\nleakages, and then preprocess the collected data with structured tailoring\nwhich are turned into GPLA-12. GPLA-12 dedicates to serve as a feature learning\ndataset for time-series tasks and classifications. To further understand the\ndataset, we train both shadow and deep learning algorithms to observe the\nperformance. The dataset as well as the pretrained models have been released at\nboth www.daip.club and github.com/Deep-AI-Application-DAIP",
          "link": "http://arxiv.org/abs/2106.10277",
          "publishedOn": "2021-06-22T01:57:10.587Z",
          "wordCount": 576,
          "title": "GPLA-12: An Acoustic Signal Dataset of Gas Pipeline Leakage. (arXiv:2106.10277v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10414",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yao_J/0/1/0/all/0/1\">Junwen Yao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mueller_J/0/1/0/all/0/1\">Jonas Mueller</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1\">Jane-Ling Wang</a>",
          "description": "Despite their widespread success, the application of deep neural networks to\nfunctional data remains scarce today. The infinite dimensionality of functional\ndata means standard learning algorithms can be applied only after appropriate\ndimension reduction, typically achieved via basis expansions. Currently, these\nbases are chosen a priori without the information for the task at hand and thus\nmay not be effective for the designated task. We instead propose to adaptively\nlearn these bases in an end-to-end fashion. We introduce neural networks that\nemploy a new Basis Layer whose hidden units are each basis functions themselves\nimplemented as a micro neural network. Our architecture learns to apply\nparsimonious dimension reduction to functional inputs that focuses only on\ninformation relevant to the target rather than irrelevant variation in the\ninput function. Across numerous classification/regression tasks with functional\ndata, our method empirically outperforms other types of neural networks, and we\nprove that our approach is statistically consistent with low generalization\nerror. Code is available at: \\url{https://github.com/jwyyy/AdaFNN}.",
          "link": "http://arxiv.org/abs/2106.10414",
          "publishedOn": "2021-06-22T01:57:10.550Z",
          "wordCount": 603,
          "title": "Deep Learning for Functional Data Analysis with Adaptive Basis Layers. (arXiv:2106.10414v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmetoglu_A/0/1/0/all/0/1\">Alper Ahmetoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ugur_E/0/1/0/all/0/1\">Emre Ugur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asada_M/0/1/0/all/0/1\">Minoru Asada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oztop_E/0/1/0/all/0/1\">Erhan Oztop</a>",
          "description": "Abstraction is an important aspect of intelligence which enables agents to\nconstruct robust representations for effective decision making. In the last\ndecade, deep networks are proven to be effective due to their ability to form\nincreasingly complex abstractions. However, these abstractions are distributed\nover many neurons, making the re-use of a learned skill costly. Previous work\neither enforced formation of abstractions creating a designer bias, or used a\nlarge number of neural units without investigating how to obtain high-level\nfeatures that may more effectively capture the source task. For avoiding\ndesigner bias and unsparing resource use, we propose to exploit neural response\ndynamics to form compact representations to use in skill transfer. For this, we\nconsider two competing methods based on (1) maximum information compression\nprinciple and (2) the notion that abstract events tend to generate slowly\nchanging signals, and apply them to the neural signals generated during task\nexecution. To be concrete, in our simulation experiments, we either apply\nprincipal component analysis (PCA) or slow feature analysis (SFA) on the\nsignals collected from the last hidden layer of a deep network while it\nperforms a source task, and use these features for skill transfer in a new\ntarget task. We compare the generalization performance of these alternatives\nwith the baselines of skill transfer with full layer output and no-transfer\nsettings. Our results show that SFA units are the most successful for skill\ntransfer. SFA as well as PCA, incur less resources compared to usual skill\ntransfer, whereby many units formed show a localized response reflecting\nend-effector-obstacle-goal relations. Finally, SFA units with lowest\neigenvalues resembles symbolic representations that highly correlate with\nhigh-level features such as joint angles which might be thought of precursors\nfor fully symbolic systems.",
          "link": "http://arxiv.org/abs/2106.10354",
          "publishedOn": "2021-06-22T01:57:10.533Z",
          "wordCount": 724,
          "title": "High-level Features for Resource Economy and Fast Learning in Skill Transfer. (arXiv:2106.10354v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2010.07904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zixin Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_W/0/1/0/all/0/1\">Wang Chi Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>",
          "description": "We consider a best arm identification (BAI) problem for stochastic bandits\nwith adversarial corruptions in the fixed-budget setting of T steps. We design\na novel randomized algorithm, Probabilistic Sequential Shrinking($u$)\n(PSS($u$)), which is agnostic to the amount of corruptions. When the amount of\ncorruptions per step (CPS) is below a threshold, PSS($u$) identifies the best\narm or item with probability tending to $1$ as $T\\rightarrow \\infty$.\nOtherwise, the optimality gap of the identified item degrades gracefully with\nthe CPS.We argue that such a bifurcation is necessary. In PSS($u$), the\nparameter $u$ serves to balance between the optimality gap and success\nprobability. The injection of randomization is shown to be essential to\nmitigate the impact of corruptions. To demonstrate this, we design two attack\nstrategies that are applicable to any algorithm. We apply one of them to a\ndeterministic analogue of PSS($u$) known as Successive Halving (SH) by Karnin\net al. (2013). The attack strategy results in a high failure probability for\nSH, but PSS($u$) remains robust. In the absence of corruptions, PSS($2$)'s\nperformance guarantee matches SH's. We show that when the CPS is sufficiently\nlarge, no algorithm can achieve a BAI probability tending to $1$ as\n$T\\rightarrow \\infty$. Numerical experiments corroborate our theoretical\nfindings.",
          "link": "http://arxiv.org/abs/2010.07904",
          "publishedOn": "2021-06-21T02:07:41.330Z",
          "wordCount": 705,
          "title": "Probabilistic Sequential Shrinking: A Best Arm Identification Algorithm for Stochastic Bandits with Corruptions. (arXiv:2010.07904v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.05050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yicheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filieri_A/0/1/0/all/0/1\">Antonio Filieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>",
          "description": "Probabilistic software analysis aims at quantifying the probability of a\ntarget event occurring during the execution of a program processing uncertain\nincoming data or written itself using probabilistic programming constructs.\nRecent techniques combine symbolic execution with model counting or solution\nspace quantification methods to obtain accurate estimates of the occurrence\nprobability of rare target events, such as failures in a mission-critical\nsystem. However, they face several scalability and applicability limitations\nwhen analyzing software processing with high-dimensional and correlated\nmultivariate input distributions. In this paper, we present SYMbolic Parallel\nAdaptive Importance Sampling (SYMPAIS), a new inference method tailored to\nanalyze path conditions generated from the symbolic execution of programs with\nhigh-dimensional, correlated input distributions. SYMPAIS combines results from\nimportance sampling and constraint solving to produce accurate estimates of the\nsatisfaction probability for a broad class of constraints that cannot be\nanalyzed by current solution space quantification methods. We demonstrate\nSYMPAIS's generality and performance compared with state-of-the-art\nalternatives on a set of problems from different application domains.",
          "link": "http://arxiv.org/abs/2010.05050",
          "publishedOn": "2021-06-21T02:07:41.323Z",
          "wordCount": 640,
          "title": "Symbolic Parallel Adaptive Importance Sampling for Probabilistic Program Analysis. (arXiv:2010.05050v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1\">Shufeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guevarra_D/0/1/0/all/0/1\">Dan Guevarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1\">Carla P. Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregoire_J/0/1/0/all/0/1\">John M. Gregoire</a>",
          "description": "The adoption of machine learning in materials science has rapidly transformed\nmaterials property prediction. Hurdles limiting full capitalization of recent\nadvancements in machine learning include the limited development of methods to\nlearn the underlying interactions of multiple elements, as well as the\nrelationships among multiple properties, to facilitate property prediction in\nnew composition spaces. To address these issues, we introduce the Hierarchical\nCorrelation Learning for Multi-property Prediction (H-CLMP) framework that\nseamlessly integrates (i) prediction using only a material's composition, (ii)\nlearning and exploitation of correlations among target properties in\nmulti-target regression, and (iii) leveraging training data from tangential\ndomains via generative transfer learning. The model is demonstrated for\nprediction of spectral optical absorption of complex metal oxides spanning 69\n3-cation metal oxide composition spaces. H-CLMP accurately predicts non-linear\ncomposition-property relationships in composition spaces for which no training\ndata is available, which broadens the purview of machine learning to the\ndiscovery of materials with exceptional properties. This achievement results\nfrom the principled integration of latent embedding learning, property\ncorrelation learning, generative transfer learning, and attention models. The\nbest performance is obtained using H-CLMP with Transfer learning (H-CLMP(T))\nwherein a generative adversarial network is trained on computational density of\nstates data and deployed in the target domain to augment prediction of optical\nabsorption from composition. H-CLMP(T) aggregates multiple knowledge sources\nwith a framework that is well-suited for multi-target regression across the\nphysical sciences.",
          "link": "http://arxiv.org/abs/2106.02225",
          "publishedOn": "2021-06-21T02:07:41.307Z",
          "wordCount": 708,
          "title": "Materials Representation and Transfer Learning for Multi-Property Prediction. (arXiv:2106.02225v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1\">Jean Kaddour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1\">Matt J. Kusner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1\">Ricardo Silva</a>",
          "description": "We address the estimation of conditional average treatment effects (CATEs)\nwhen treatments are graph-structured (e.g., molecular graphs of drugs). Given a\nweak condition on the effect, we propose a plug-in estimator that decomposes\nCATE estimation into separate, simpler optimization problems. Our estimator (a)\nisolates the causal estimands (reducing regularization bias), and (b) allows\none to plug in arbitrary models for learning. In experiments with small-world\nand molecular graphs, we show that our approach outperforms prior approaches\nand is robust to varying selection biases. Our implementation is online.",
          "link": "http://arxiv.org/abs/2106.01939",
          "publishedOn": "2021-06-21T02:07:41.301Z",
          "wordCount": 540,
          "title": "Graph Intervention Networks for Causal Effect Estimation. (arXiv:2106.01939v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_O/0/1/0/all/0/1\">Ou Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Weiyao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yingjun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haixiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qinghu Hou</a>",
          "description": "A common assumption in machine learning is that samples are independently and\nidentically distributed (i.i.d). However, the contributions of different\nsamples are not identical in training. Some samples are difficult to learn and\nsome samples are noisy. The unequal contributions of samples has a considerable\neffect on training performances. Studies focusing on unequal sample\ncontributions (e.g., easy, hard, noisy) in learning usually refer to these\ncontributions as robust machine learning (RML). Weighing and regularization are\ntwo common techniques in RML. Numerous learning algorithms have been proposed\nbut the strategies for dealing with easy/hard/noisy samples differ or even\ncontradict with different learning algorithms. For example, some strategies\ntake the hard samples first, whereas some strategies take easy first.\nConducting a clear comparison for existing RML algorithms in dealing with\ndifferent samples is difficult due to lack of a unified theoretical framework\nfor RML. This study attempts to construct a mathematical foundation for RML\nbased on the bias-variance trade-off theory. A series of definitions and\nproperties are presented and proved. Several classical learning algorithms are\nalso explained and compared. Improvements of existing methods are obtained\nbased on the comparison. A unified method that combines two classical learning\nstrategies is proposed.",
          "link": "http://arxiv.org/abs/2106.05522",
          "publishedOn": "2021-06-21T02:07:41.280Z",
          "wordCount": 663,
          "title": "A Mathematical Foundation for Robust Machine Learning based on Bias-Variance Trade-off. (arXiv:2106.05522v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yifan_W/0/1/0/all/0/1\">Wang Yifan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmann_L/0/1/0/all/0/1\">Lukas Rahmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorkine_Hornung_O/0/1/0/all/0/1\">Olga Sorkine-Hornung</a>",
          "description": "We present implicit displacement fields, a novel representation for detailed\n3D geometry. Inspired by a classic surface deformation technique, displacement\nmapping, our method represents a complex surface as a smooth base surface plus\na displacement along the base's normal directions, resulting in a\nfrequency-based shape decomposition, where the high frequency signal is\nconstrained geometrically by the low frequency signal. Importantly, this\ndisentanglement is unsupervised thanks to a tailored architectural design that\nhas an innate frequency hierarchy by construction. We explore implicit\ndisplacement field surface reconstruction and detail transfer and demonstrate\nsuperior representational power, training stability and generalizability.",
          "link": "http://arxiv.org/abs/2106.05187",
          "publishedOn": "2021-06-21T02:07:41.262Z",
          "wordCount": 567,
          "title": "Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields. (arXiv:2106.05187v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Backurs_A/0/1/0/all/0/1\">Arturs Backurs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Indyk_P/0/1/0/all/0/1\">Piotr Indyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1\">Cameron Musco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_T/0/1/0/all/0/1\">Tal Wagner</a>",
          "description": "We study fast algorithms for computing fundamental properties of a positive\nsemidefinite kernel matrix $K \\in \\mathbb{R}^{n \\times n}$ corresponding to $n$\npoints $x_1,\\ldots,x_n \\in \\mathbb{R}^d$. In particular, we consider estimating\nthe sum of kernel matrix entries, along with its top eigenvalue and\neigenvector.\n\nWe show that the sum of matrix entries can be estimated to $1+\\epsilon$\nrelative error in time $sublinear$ in $n$ and linear in $d$ for many popular\nkernels, including the Gaussian, exponential, and rational quadratic kernels.\nFor these kernels, we also show that the top eigenvalue (and an approximate\neigenvector) can be approximated to $1+\\epsilon$ relative error in time\n$subquadratic$ in $n$ and linear in $d$.\n\nOur algorithms represent significant advances in the best known runtimes for\nthese problems. They leverage the positive definiteness of the kernel matrix,\nalong with a recent line of work on efficient kernel density estimation.",
          "link": "http://arxiv.org/abs/2102.08341",
          "publishedOn": "2021-06-21T02:07:41.256Z",
          "wordCount": 614,
          "title": "Faster Kernel Matrix Algebra via Density Estimation. (arXiv:2102.08341v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1\">Vinicius L. S. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heaney_C/0/1/0/all/0/1\">Claire E. Heaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pain_C/0/1/0/all/0/1\">Christopher C. Pain</a>",
          "description": "We propose a new method in which a generative adversarial network (GAN) is\nused to quantify the uncertainty of forward simulations in the presence of\nobserved data. Previously, a method has been developed which enables GANs to\nmake time series predictions and data assimilation by training a GAN with\nunconditional simulations of a high-fidelity numerical model. After training,\nthe GAN can be used to predict the evolution of the spatial distribution of the\nsimulation states and observed data is assimilated. In this paper, we describe\nthe process required in order to quantify uncertainty, during which no\nadditional simulations of the high-fidelity numerical model are required. These\nmethods take advantage of the adjoint-like capabilities of generative models\nand the ability to simulate forwards and backwards in time. Set within a\nreduced-order model framework for efficiency, we apply these methods to a\ncompartmental model in epidemiology to predict the spread of COVID-19 in an\nidealised town. The results show that the proposed method can efficiently\nquantify uncertainty in the presence of measurements using only unconditional\nsimulations of the high-fidelity numerical model.",
          "link": "http://arxiv.org/abs/2105.13859",
          "publishedOn": "2021-06-21T02:07:41.248Z",
          "wordCount": 702,
          "title": "GAN for time series prediction, data assimilation and uncertainty quantification. (arXiv:2105.13859v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00075",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Moretti_A/0/1/0/all/0/1\">Antonio Khalil Moretti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_L/0/1/0/all/0/1\">Liyi Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Naesseth_C/0/1/0/all/0/1\">Christian A. Naesseth</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Venner_H/0/1/0/all/0/1\">Hadiah Venner</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1\">David Blei</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Peer_I/0/1/0/all/0/1\">Itsik Pe&#x27;er</a>",
          "description": "Bayesian phylogenetic inference is often conducted via local or sequential\nsearch over topologies and branch lengths using algorithms such as random-walk\nMarkov chain Monte Carlo (MCMC) or Combinatorial Sequential Monte Carlo (CSMC).\nHowever, when MCMC is used for evolutionary parameter learning, convergence\nrequires long runs with inefficient exploration of the state space. We\nintroduce Variational Combinatorial Sequential Monte Carlo (VCSMC), a powerful\nframework that establishes variational sequential search to learn distributions\nover intricate combinatorial structures. We then develop nested CSMC, an\nefficient proposal distribution for CSMC and prove that nested CSMC is an exact\napproximation to the (intractable) locally optimal proposal. We use nested CSMC\nto define a second objective, VNCSMC which yields tighter lower bounds than\nVCSMC. We show that VCSMC and VNCSMC are computationally efficient and explore\nhigher probability spaces than existing methods on a range of tasks.",
          "link": "http://arxiv.org/abs/2106.00075",
          "publishedOn": "2021-06-21T02:07:41.236Z",
          "wordCount": 608,
          "title": "Variational Combinatorial Sequential Monte Carlo Methods for Bayesian Phylogenetic Inference. (arXiv:2106.00075v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02532",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feldman_T/0/1/0/all/0/1\">Tal Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peake_A/0/1/0/all/0/1\">Ashley Peake</a>",
          "description": "Machine Learning models have been deployed across many different aspects of\nsociety, often in situations that affect social welfare. Although these models\noffer streamlined solutions to large problems, they may contain biases and\ntreat groups or individuals unfairly based on protected attributes such as\ngender. In this paper, we introduce several examples of machine learning gender\nbias in practice followed by formalizations of fairness. We provide a survey of\nfairness research by detailing influential pre-processing, in-processing, and\npost-processing bias mitigation algorithms. We then propose an\n\\textup{end-to-end bias mitigation} framework, which employs a fusion of pre-,\nin-, and post-processing methods to leverage the strengths of each individual\ntechnique. We test this method, along with the standard techniques we review,\non a deep neural network to analyze bias mitigation in a deep learning setting.\nWe find that our end-to-end bias mitigation framework outperforms the baselines\nwith respect to several fairness metrics, suggesting its promise as a method\nfor improving fairness. As society increasingly relies on artificial\nintelligence to help in decision-making, addressing gender biases present in\ndeep learning models is imperative. To provide readers with the tools to assess\nthe fairness of machine learning models and mitigate the biases present in\nthem, we discuss multiple open source packages for fairness in AI.",
          "link": "http://arxiv.org/abs/2104.02532",
          "publishedOn": "2021-06-21T02:07:41.229Z",
          "wordCount": 676,
          "title": "End-To-End Bias Mitigation: Removing Gender Bias in Deep Learning. (arXiv:2104.02532v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1\">Marcos Vendramini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1\">Hugo Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1\">Alexei Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>",
          "description": "Image classification methods are usually trained to perform predictions\ntaking into account a predefined group of known classes. Real-world problems,\nhowever, may not allow for a full knowledge of the input and label spaces,\nmaking failures in recognition a hazard to deep visual learning. Open set\nrecognition methods are characterized by the ability to correctly identifying\ninputs of known and unknown classes. In this context, we propose GeMOS: simple\nand plug-and-play open set recognition modules that can be attached to\npretrained Deep Neural Networks for visual recognition. The GeMOS framework\npairs pre-trained Convolutional Neural Networks with generative models for open\nset recognition to extract open set scores for each sample, allowing for\nfailure recognition in object recognition tasks. We conduct a thorough\nevaluation of the proposed method in comparison with state-of-the-art open set\nalgorithms, finding that GeMOS either outperforms or is statistically\nindistinguishable from more complex and costly models.",
          "link": "http://arxiv.org/abs/2105.10013",
          "publishedOn": "2021-06-21T02:07:41.212Z",
          "wordCount": 617,
          "title": "Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengrui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Guangchun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Ling Tian</a>",
          "description": "Finding a suitable data representation for a specific task has been shown to\nbe crucial in many applications. The success of subspace clustering depends on\nthe assumption that the data can be separated into different subspaces.\nHowever, this simple assumption does not always hold since the raw data might\nnot be separable into subspaces. To recover the ``clustering-friendly''\nrepresentation and facilitate the subsequent clustering, we propose a graph\nfiltering approach by which a smooth representation is achieved. Specifically,\nit injects graph similarity into data features by applying a low-pass filter to\nextract useful data representations for clustering. Extensive experiments on\nimage and document clustering datasets demonstrate that our method improves\nupon state-of-the-art subspace clustering techniques. Especially, its\ncomparable performance with deep learning methods emphasizes the effectiveness\nof the simple graph filtering scheme for many real-world applications. An\nablation study shows that graph filtering can remove noise, preserve structure\nin the image, and increase the separability of classes.",
          "link": "http://arxiv.org/abs/2106.09874",
          "publishedOn": "2021-06-21T02:07:41.205Z",
          "wordCount": 606,
          "title": "Towards Clustering-friendly Representations: Subspace Clustering via Graph Filtering. (arXiv:2106.09874v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yulin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yuni Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kaifa Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiapu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingquan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kai Zhou</a>",
          "description": "Graph-based Anomaly Detection (GAD) is becoming prevalent due to the powerful\nrepresentation abilities of graphs as well as recent advances in graph mining\ntechniques. These GAD tools, however, expose a new attacking surface,\nironically due to their unique advantage of being able to exploit the relations\namong data. That is, attackers now can manipulate those relations (i.e., the\nstructure of the graph) to allow some target nodes to evade detection. In this\npaper, we exploit this vulnerability by designing a new type of targeted\nstructural poisoning attacks to a representative regression-based GAD system\ntermed OddBall. Specially, we formulate the attack against OddBall as a\nbi-level optimization problem, where the key technical challenge is to\nefficiently solve the problem in a discrete domain. We propose a novel attack\nmethod termed BinarizedAttack based on gradient descent. Comparing to prior\narts, BinarizedAttack can better use the gradient information, making it\nparticularly suitable for solving combinatorial optimization problems.\nFurthermore, we investigate the attack transferability of BinarizedAttack by\nemploying it to attack other representation-learning-based GAD systems. Our\ncomprehensive experiments demonstrate that BinarizedAttack is very effective in\nenabling target nodes to evade graph-based anomaly detection tools with limited\nattackers' budget, and in the black-box transfer attack setting,\nBinarizedAttack is also tested effective and in particular, can significantly\nchange the node embeddings learned by the GAD systems. Our research thus opens\nthe door to studying a new type of attack against security analytic tools that\nrely on graph data.",
          "link": "http://arxiv.org/abs/2106.09989",
          "publishedOn": "2021-06-21T02:07:41.197Z",
          "wordCount": 682,
          "title": "BinarizedAttack: Structural Poisoning Attacks to Graph-based Anomaly Detection. (arXiv:2106.09989v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09474",
          "author": "<a href=\"http://arxiv.org/find/hep-ph/1/au:+Aylett_Bullock_J/0/1/0/all/0/1\">Joseph Aylett-Bullock</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Badger_S/0/1/0/all/0/1\">Simon Badger</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Moodie_R/0/1/0/all/0/1\">Ryan Moodie</a>",
          "description": "Machine learning technology has the potential to dramatically optimise event\ngeneration and simulations. We continue to investigate the use of neural\nnetworks to approximate matrix elements for high-multiplicity scattering\nprocesses. We focus on the case of loop-induced diphoton production through\ngluon fusion and develop a realistic simulation method that can be applied to\nhadron collider observables. Neural networks are trained using the one-loop\namplitudes implemented in the NJet C++ library and interfaced to the Sherpa\nMonte Carlo event generator where we perform a detailed study for $2\\to3$ and\n$2\\to4$ scattering problems. We also consider how the trained networks perform\nwhen varying the kinematic cuts effecting the phase space and the reliability\nof the neural network simulations.",
          "link": "http://arxiv.org/abs/2106.09474",
          "publishedOn": "2021-06-21T02:07:41.142Z",
          "wordCount": 571,
          "title": "Optimising simulations for diphoton production at hadron colliders using amplitude neural networks. (arXiv:2106.09474v1 [hep-ph] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1\">Tung Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Anup B. Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1\">Cameron Musco</a>",
          "description": "We give relative error coresets for training linear classifiers with a broad\nclass of loss functions, including the logistic loss and hinge loss. Our\nconstruction achieves $(1\\pm \\epsilon)$ relative error with $\\tilde O(d \\cdot\n\\mu_y(X)^2/\\epsilon^2)$ points, where $\\mu_y(X)$ is a natural complexity\nmeasure of the data matrix $X \\in \\mathbb{R}^{n \\times d}$ and label vector $y\n\\in \\{-1,1\\}^n$, introduced in by Munteanu et al. 2018. Our result is based on\nsubsampling data points with probabilities proportional to their $\\ell_1$\n$Lewis$ $weights$. It significantly improves on existing theoretical bounds and\nperforms well in practice, outperforming uniform subsampling along with other\nimportance sampling methods. Our sampling distribution does not depend on the\nlabels, so can be used for active learning. It also does not depend on the\nspecific loss function, so a single coreset can be used in multiple training\nscenarios.",
          "link": "http://arxiv.org/abs/2106.04254",
          "publishedOn": "2021-06-21T02:07:41.136Z",
          "wordCount": 589,
          "title": "Coresets for Classification -- Simplified and Strengthened. (arXiv:2106.04254v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fakoor_R/0/1/0/all/0/1\">Rasool Fakoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1\">Jonas Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadi_K/0/1/0/all/0/1\">Kavosh Asadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1\">Pratik Chaudhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>",
          "description": "Reliant on too many experiments to learn good actions, current Reinforcement\nLearning (RL) algorithms have limited applicability in real-world settings,\nwhich can be too expensive to allow exploration. We propose an algorithm for\nbatch RL, where effective policies are learned using only a fixed offline\ndataset instead of online interactions with the environment. The limited data\nin batch RL produces inherent uncertainty in value estimates of states/actions\nthat were insufficiently represented in the training data. This leads to\nparticularly severe extrapolation when our candidate policies diverge from one\nthat generated the data. We propose to mitigate this issue via two\nstraightforward penalties: a policy-constraint to reduce this divergence and a\nvalue-constraint that discourages overly optimistic estimates. Over a\ncomprehensive set of 32 continuous-action batch RL benchmarks, our approach\ncompares favorably to state-of-the-art methods, regardless of how the offline\ndata were collected.",
          "link": "http://arxiv.org/abs/2102.09225",
          "publishedOn": "2021-06-21T02:07:41.077Z",
          "wordCount": 614,
          "title": "Continuous Doubly Constrained Batch Reinforcement Learning. (arXiv:2102.09225v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jafarpour_S/0/1/0/all/0/1\">Saber Jafarpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davydov_A/0/1/0/all/0/1\">Alexander Davydov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proskurnikov_A/0/1/0/all/0/1\">Anton V. Proskurnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bullo_F/0/1/0/all/0/1\">Francesco Bullo</a>",
          "description": "Implicit neural networks, a.k.a., deep equilibrium networks, are a class of\nimplicit-depth learning models where function evaluation is performed by\nsolving a fixed point equation. They generalize classic feedforward models and\nare equivalent to infinite-depth weight-tied feedforward networks. While\nimplicit models show improved accuracy and significant reduction in memory\nconsumption, they can suffer from ill-posedness and convergence instability.\n\nThis paper provides a new framework to design well-posed and robust implicit\nneural networks based upon contraction theory for the non-Euclidean norm\n$\\ell_\\infty$. Our framework includes (i) a novel condition for well-posedness\nbased on one-sided Lipschitz constants, (ii) an average iteration for computing\nfixed-points, and (iii) explicit estimates on input-output Lipschitz constants.\nAdditionally, we design a training problem with the well-posedness condition\nand the average iteration as constraints and, to achieve robust models, with\nthe input-output Lipschitz constant as a regularizer. Our $\\ell_\\infty$\nwell-posedness condition leads to a larger polytopic training search space than\nexisting conditions and our average iteration enjoys accelerated convergence.\nFinally, we perform several numerical experiments for function estimation and\ndigit classification through the MNIST data set. Our numerical results\ndemonstrate improved accuracy and robustness of the implicit models with\nsmaller input-output Lipschitz bounds.",
          "link": "http://arxiv.org/abs/2106.03194",
          "publishedOn": "2021-06-21T02:07:41.070Z",
          "wordCount": 657,
          "title": "Robust Implicit Networks via Non-Euclidean Contractions. (arXiv:2106.03194v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Sumon Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1\">Hridesh Rajan</a>",
          "description": "In recent years, many incidents have been reported where machine learning\nmodels exhibited discrimination among people based on race, sex, age, etc.\nResearch has been conducted to measure and mitigate unfairness in machine\nlearning models. For a machine learning task, it is a common practice to build\na pipeline that includes an ordered set of data preprocessing stages followed\nby a classifier. However, most of the research on fairness has considered a\nsingle classifier based prediction task. What are the fairness impacts of the\npreprocessing stages in machine learning pipeline? Furthermore, studies showed\nthat often the root cause of unfairness is ingrained in the data itself, rather\nthan the model. But no research has been conducted to measure the unfairness\ncaused by a specific transformation made in the data preprocessing stage. In\nthis paper, we introduced the causal method of fairness to reason about the\nfairness impact of data preprocessing stages in ML pipeline. We leveraged\nexisting metrics to define the fairness measures of the stages. Then we\nconducted a detailed fairness evaluation of the preprocessing stages in 37\npipelines collected from three different sources. Our results show that certain\ndata transformers are causing the model to exhibit unfairness. We identified a\nnumber of fairness patterns in several categories of data transformers.\nFinally, we showed how the local fairness of a preprocessing stage composes in\nthe global fairness of the pipeline. We used the fairness composition to choose\nappropriate downstream transformer that mitigates unfairness in the machine\nlearning pipeline.",
          "link": "http://arxiv.org/abs/2106.06054",
          "publishedOn": "2021-06-21T02:07:41.062Z",
          "wordCount": 746,
          "title": "Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline. (arXiv:2106.06054v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11784",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Zwolak_J/0/1/0/all/0/1\">Justyna P. Zwolak</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+McJunkin_T/0/1/0/all/0/1\">Thomas McJunkin</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kalantre_S/0/1/0/all/0/1\">Sandesh S. Kalantre</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Neyens_S/0/1/0/all/0/1\">Samuel F. Neyens</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+MacQuarrie_E/0/1/0/all/0/1\">E. R. MacQuarrie</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Eriksson_M/0/1/0/all/0/1\">Mark A. Eriksson</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Taylor_J/0/1/0/all/0/1\">Jacob M. Taylor</a>",
          "description": "Quantum dots (QDs) defined with electrostatic gates are a leading platform\nfor a scalable quantum computing implementation. However, with increasing\nnumbers of qubits, the complexity of the control parameter space also grows.\nTraditional measurement techniques, relying on complete or near-complete\nexploration via two-parameter scans (images) of the device response, quickly\nbecome impractical with increasing numbers of gates. Here we propose to\ncircumvent this challenge by introducing a measurement technique relying on\none-dimensional projections of the device response in the multidimensional\nparameter space. Dubbed the ``ray-based classification (RBC) framework,'' we\nuse this machine learning approach to implement a classifier for QD states,\nenabling automated recognition of qubit-relevant parameter regimes. We show\nthat RBC surpasses the 82 % accuracy benchmark from the experimental\nimplementation of image-based classification techniques from prior work while\nreducing the number of measurement points needed by up to 70 %. The reduction\nin measurement cost is a significant gain for time-intensive QD measurements\nand is a step forward toward the scalability of these devices. We also discuss\nhow the RBC-based optimizer, which tunes the device to a multiqubit regime,\nperforms when tuning in the two-dimensional and three-dimensional parameter\nspaces defined by plunger and barrier gates that control the QDs.This work\nprovides experimental validation of both efficient state identification and\noptimization with machine learning techniques for non-traditional measurements\nin quantum systems with high-dimensional parameter spaces and time-intensive\nmeasurements.",
          "link": "http://arxiv.org/abs/2102.11784",
          "publishedOn": "2021-06-21T02:07:41.054Z",
          "wordCount": 708,
          "title": "Ray-based framework for state identification in quantum dot devices. (arXiv:2102.11784v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13727",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wood_K/0/1/0/all/0/1\">Kieran Wood</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zohren_S/0/1/0/all/0/1\">Stefan Zohren</a>",
          "description": "Momentum strategies are an important part of alternative investments and are\nat the heart of commodity trading advisors (CTAs). These strategies have\nhowever been found to have difficulties adjusting to rapid changes in market\nconditions, such as during the 2020 market crash. In particular, immediately\nafter momentum turning points, where a trend reverses from an uptrend\n(downtrend) to a downtrend (uptrend), time-series momentum (TSMOM) strategies\nare prone to making bad bets. To improve the response to regime change, we\nintroduce a novel approach, where we insert an online change-point detection\n(CPD) module into a Deep Momentum Network (DMN) [1904.04912] pipeline, which\nuses an LSTM deep-learning architecture to simultaneously learn both trend\nestimation and position sizing. Furthermore, our model is able to optimise the\nway in which it balances 1) a slow momentum strategy which exploits persisting\ntrends, but does not overreact to localised price moves, and 2) a fast\nmean-reversion strategy regime by quickly flipping its position, then swapping\nit back again to exploit localised price moves. Our CPD module outputs a\nchangepoint location and severity score, allowing our model to learn to respond\nto varying degrees of disequilibrium, or smaller and more localised\nchangepoints, in a data driven manner. Using a portfolio of 50, liquid,\ncontinuous futures contracts over the period 1990-2020, the addition of the CPD\nmodule leads to an improvement in Sharpe ratio of one-third. Even more notably,\nthis module is especially beneficial in periods of significant nonstationarity,\nand in particular, over the most recent years tested (2015-2020) the\nperformance boost is approximately two-thirds. This is especially interesting\nas traditional momentum strategies have been underperforming in this period.",
          "link": "http://arxiv.org/abs/2105.13727",
          "publishedOn": "2021-06-21T02:07:41.037Z",
          "wordCount": 752,
          "title": "Slow Momentum with Fast Reversion: A Trading Strategy Using Deep Learning and Changepoint Detection. (arXiv:2105.13727v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.15069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nowak_Vila_A/0/1/0/all/0/1\">Alex Nowak-Vila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>",
          "description": "The foundational concept of Max-Margin in machine learning is ill-posed for\noutput spaces with more than two labels such as in structured prediction. In\nthis paper, we show that the Max-Margin loss can only be consistent to the\nclassification task under highly restrictive assumptions on the discrete loss\nmeasuring the error between outputs. These conditions are satisfied by\ndistances defined in tree graphs, for which we prove consistency, thus being\nthe first losses shown to be consistent for Max-Margin beyond the binary\nsetting. We finally address these limitations by correcting the concept of\nMax-Margin and introducing the Restricted-Max-Margin, where the maximization of\nthe loss-augmented scores is maintained, but performed over a subset of the\noriginal domain. The resulting loss is also a generalization of the binary\nsupport vector machine and it is consistent under milder conditions on the\ndiscrete loss.",
          "link": "http://arxiv.org/abs/2105.15069",
          "publishedOn": "2021-06-21T02:07:41.028Z",
          "wordCount": 591,
          "title": "Max-Margin is Dead, Long Live Max-Margin!. (arXiv:2105.15069v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_Z/0/1/0/all/0/1\">Zeshan Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_R/0/1/0/all/0/1\">Rahul G. Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1\">David Sontag</a>",
          "description": "Modeling the time-series of high-dimensional, longitudinal data is important\nfor predicting patient disease progression. However, existing neural network\nbased approaches that learn representations of patient state, while very\nflexible, are susceptible to overfitting. We propose a deep generative model\nthat makes use of a novel attention-based neural architecture inspired by the\nphysics of how treatments affect disease state. The result is a scalable and\naccurate model of high-dimensional patient biomarkers as they vary over time.\nOur proposed model yields significant improvements in generalization and, on\nreal-world clinical data, provides interpretable insights into the dynamics of\ncancer progression.",
          "link": "http://arxiv.org/abs/2102.11218",
          "publishedOn": "2021-06-21T02:07:41.022Z",
          "wordCount": 570,
          "title": "Neural Pharmacodynamic State Space Modeling. (arXiv:2102.11218v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xuefeng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "In adversarial training (AT), the main focus has been the objective and\noptimizer while the model has been less studied, so that the models being used\nare still those classic ones in standard training (ST). Classic network\narchitectures (NAs) are generally worse than searched NAs in ST, which should\nbe the same in AT. In this paper, we argue that NA and AT cannot be handled\nindependently, since given a dataset, the optimal NA in ST would be no longer\noptimal in AT. That being said, AT is time-consuming itself; if we directly\nsearch NAs in AT over large search spaces, the computation will be practically\ninfeasible. Thus, we propose a diverse-structured network (DS-Net), to\nsignificantly reduce the size of the search space: instead of low-level\noperations, we only consider predefined atomic blocks, where an atomic block is\na time-tested building block like the residual block. There are only a few\natomic blocks and thus we can weight all atomic blocks rather than find the\nbest one in a searched block of DS-Net, which is an essential trade-off between\nexploring diverse structures and exploiting the best structures. Empirical\nresults demonstrate the advantages of DS-Net, i.e., weighting the atomic\nblocks.",
          "link": "http://arxiv.org/abs/2102.01886",
          "publishedOn": "2021-06-21T02:07:41.014Z",
          "wordCount": 695,
          "title": "Learning Diverse-Structured Networks for Adversarial Robustness. (arXiv:2102.01886v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhaowei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1\">Charless Fowlkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We present a plug-in replacement for batch normalization (BN) called\nexponential moving average normalization (EMAN), which improves the performance\nof existing student-teacher based self- and semi-supervised learning\ntechniques. Unlike the standard BN, where the statistics are computed within\neach batch, EMAN, used in the teacher, updates its statistics by exponential\nmoving average from the BN statistics of the student. This design reduces the\nintrinsic cross-sample dependency of BN and enhances the generalization of the\nteacher. EMAN improves strong baselines for self-supervised learning by 4-6/1-2\npoints and semi-supervised learning by about 7/2 points, when 1%/10% supervised\nlabels are available on ImageNet. These improvements are consistent across\nmethods, network architectures, training duration, and datasets, demonstrating\nthe general effectiveness of this technique. The code is available at\nhttps://github.com/amazon-research/exponential-moving-average-normalization.",
          "link": "http://arxiv.org/abs/2101.08482",
          "publishedOn": "2021-06-21T02:07:41.007Z",
          "wordCount": 614,
          "title": "Exponential Moving Average Normalization for Self-supervised and Semi-supervised Learning. (arXiv:2101.08482v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07729",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1\">Vinicius L. S. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heaney_C/0/1/0/all/0/1\">Claire E. Heaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pain_C/0/1/0/all/0/1\">Christopher C. Pain</a>",
          "description": "We propose the novel use of a generative adversarial network (GAN) (i) to\nmake predictions in time (PredGAN) and (ii) to assimilate measurements\n(DA-PredGAN). In the latter case, we take advantage of the natural adjoint-like\nproperties of generative models and the ability to simulate forwards and\nbackwards in time. GANs have received much attention recently, after achieving\nexcellent results for their generation of realistic-looking images. We wish to\nexplore how this property translates to new applications in computational\nmodelling and to exploit the adjoint-like properties for efficient data\nassimilation. To predict the spread of COVID-19 in an idealised town, we apply\nthese methods to a compartmental model in epidemiology that is able to model\nspace and time variations. To do this, the GAN is set within a reduced-order\nmodel (ROM), which uses a low-dimensional space for the spatial distribution of\nthe simulation states. Then the GAN learns the evolution of the low-dimensional\nstates over time. The results show that the proposed methods can accurately\npredict the evolution of the high-fidelity numerical simulation, and can\nefficiently assimilate observed data and determine the corresponding model\nparameters.",
          "link": "http://arxiv.org/abs/2105.07729",
          "publishedOn": "2021-06-21T02:07:41.000Z",
          "wordCount": 705,
          "title": "Data Assimilation Predictive GAN (DA-PredGAN): applied to determine the spread of COVID-19. (arXiv:2105.07729v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1\">Sai Praneeth Karimireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1\">Sebastian U. Stich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>",
          "description": "Decentralized training of deep learning models is a key element for enabling\ndata privacy and on-device learning over networks. In realistic learning\nscenarios, the presence of heterogeneity across different clients' local\ndatasets poses an optimization challenge and may severely deteriorate the\ngeneralization performance. In this paper, we investigate and identify the\nlimitation of several decentralized optimization algorithms for different\ndegrees of data heterogeneity. We propose a novel momentum-based method to\nmitigate this decentralized training difficulty. We show in extensive empirical\nexperiments on various CV/NLP datasets (CIFAR-10, ImageNet, and AG News) and\nseveral network topologies (Ring and Social Network) that our method is much\nmore robust to the heterogeneity of clients' data than other existing methods,\nby a significant improvement in test performance ($1\\% \\!-\\! 20\\%$). Our code\nis publicly available.",
          "link": "http://arxiv.org/abs/2102.04761",
          "publishedOn": "2021-06-21T02:07:40.983Z",
          "wordCount": 594,
          "title": "Quasi-Global Momentum: Accelerating Decentralized Deep Learning on Heterogeneous Data. (arXiv:2102.04761v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1\">Son Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Than_K/0/1/0/all/0/1\">Khoat Than</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_H/0/1/0/all/0/1\">Hung Bui</a>",
          "description": "Approximate inference in deep Bayesian networks exhibits a dilemma of how to\nyield high fidelity posterior approximations while maintaining computational\nefficiency and scalability. We tackle this challenge by introducing a novel\nvariational structured approximation inspired by the Bayesian interpretation of\nDropout regularization. Concretely, we focus on the inflexibility of the\nfactorized structure in Dropout posterior and then propose an improved method\ncalled Variational Structured Dropout (VSD). VSD employs an orthogonal\ntransformation to learn a structured representation on the variational noise\nand consequently induces statistical dependencies in the approximate posterior.\nTheoretically, VSD successfully addresses the pathologies of previous\nVariational Dropout methods and thus offers a standard Bayesian justification.\nWe further show that VSD induces an adaptive regularization term with several\ndesirable properties which contribute to better generalization. Finally, we\nconduct extensive experiments on standard benchmarks to demonstrate the\neffectiveness of VSD over state-of-the-art variational methods on predictive\naccuracy, uncertainty estimation, and out-of-distribution detection.",
          "link": "http://arxiv.org/abs/2102.07927",
          "publishedOn": "2021-06-21T02:07:40.975Z",
          "wordCount": 627,
          "title": "Structured Dropout Variational Inference for Bayesian Neural Networks. (arXiv:2102.07927v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15761",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Stanziola_A/0/1/0/all/0/1\">Antonio Stanziola</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Arridge_S/0/1/0/all/0/1\">Simon R. Arridge</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Cox_B/0/1/0/all/0/1\">Ben T. Cox</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Treeby_B/0/1/0/all/0/1\">Bradley E. Treeby</a>",
          "description": "Transcranial ultrasound therapy is increasingly used for the non-invasive\ntreatment of brain disorders. However, conventional numerical wave solvers are\ncurrently too computationally expensive to be used online during treatments to\npredict the acoustic field passing through the skull (e.g., to account for\nsubject-specific dose and targeting variations). As a step towards real-time\npredictions, in the current work, a fast iterative solver for the heterogeneous\nHelmholtz equation in 2D is developed using a fully-learned optimizer. The\nlightweight network architecture is based on a modified UNet that includes a\nlearned hidden state. The network is trained using a physics-based loss\nfunction and a set of idealized sound speed distributions with fully\nunsupervised training (no knowledge of the true solution is required). The\nlearned optimizer shows excellent performance on the test set, and is capable\nof generalization well outside the training examples, including to much larger\ncomputational domains, and more complex source and sound speed distributions,\nfor example, those derived from x-ray computed tomography images of the skull.",
          "link": "http://arxiv.org/abs/2010.15761",
          "publishedOn": "2021-06-21T02:07:40.968Z",
          "wordCount": 652,
          "title": "A Helmholtz equation solver using unsupervised learning: Application to transcranial ultrasound. (arXiv:2010.15761v2 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pfaff_T/0/1/0/all/0/1\">Tobias Pfaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortunato_M/0/1/0/all/0/1\">Meire Fortunato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Gonzalez_A/0/1/0/all/0/1\">Alvaro Sanchez-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1\">Peter W. Battaglia</a>",
          "description": "Mesh-based simulations are central to modeling complex physical systems in\nmany disciplines across science and engineering. Mesh representations support\npowerful numerical integration methods and their resolution can be adapted to\nstrike favorable trade-offs between accuracy and efficiency. However,\nhigh-dimensional scientific simulations are very expensive to run, and solvers\nand parameters must often be tuned individually to each system studied. Here we\nintroduce MeshGraphNets, a framework for learning mesh-based simulations using\ngraph neural networks. Our model can be trained to pass messages on a mesh\ngraph and to adapt the mesh discretization during forward simulation. Our\nresults show it can accurately predict the dynamics of a wide range of physical\nsystems, including aerodynamics, structural mechanics, and cloth. The model's\nadaptivity supports learning resolution-independent dynamics and can scale to\nmore complex state spaces at test time. Our method is also highly efficient,\nrunning 1-2 orders of magnitude faster than the simulation on which it is\ntrained. Our approach broadens the range of problems on which neural network\nsimulators can operate and promises to improve the efficiency of complex,\nscientific modeling tasks.",
          "link": "http://arxiv.org/abs/2010.03409",
          "publishedOn": "2021-06-21T02:07:40.961Z",
          "wordCount": 670,
          "title": "Learning Mesh-Based Simulation with Graph Networks. (arXiv:2010.03409v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12696",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Giollo_M/0/1/0/all/0/1\">Manuel Giollo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gunceler_D/0/1/0/all/0/1\">Deniz Gunceler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yulan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Willett_D/0/1/0/all/0/1\">Daniel Willett</a>",
          "description": "Bootstrapping speech recognition on limited data resources has been an area\nof active research for long. The recent transition to all-neural models and\nend-to-end (E2E) training brought along particular challenges as these models\nare known to be data hungry, but also came with opportunities around\nlanguage-agnostic representations derived from multilingual data as well as\nshared word-piece output representations across languages that share script and\nroots. We investigate here the effectiveness of different strategies to\nbootstrap an RNN-Transducer (RNN-T) based automatic speech recognition (ASR)\nsystem in the low resource regime, while exploiting the abundant resources\navailable in other languages as well as the synthetic audio from a\ntext-to-speech (TTS) engine. Our experiments demonstrate that transfer learning\nfrom a multilingual model, using a post-ASR text-to-text mapping and synthetic\naudio deliver additive improvements, allowing us to bootstrap a model for a new\nlanguage with a fraction of the data that would otherwise be needed. The best\nsystem achieved a 46% relative word error rate (WER) reduction compared to the\nmonolingual baseline, among which 25% relative WER improvement is attributed to\nthe post-ASR text-to-text mappings and the TTS synthetic data.",
          "link": "http://arxiv.org/abs/2011.12696",
          "publishedOn": "2021-06-21T02:07:40.944Z",
          "wordCount": 659,
          "title": "Bootstrap an end-to-end ASR system by multilingual training, transfer learning, text-to-text mapping and synthetic audio. (arXiv:2011.12696v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Armandpour_M/0/1/0/all/0/1\">Mohammadreza Armandpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghian_A/0/1/0/all/0/1\">Ali Sadeghian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>",
          "description": "Despite the success of Generative Adversarial Networks (GANs), their training\nsuffers from several well-known problems, including mode collapse and\ndifficulties learning a disconnected set of manifolds. In this paper, we break\ndown the challenging task of learning complex high dimensional distributions,\nsupporting diverse data samples, to simpler sub-tasks. Our solution relies on\ndesigning a partitioner that breaks the space into smaller regions, each having\na simpler distribution, and training a different generator for each partition.\nThis is done in an unsupervised manner without requiring any labels.\n\nWe formulate two desired criteria for the space partitioner that aid the\ntraining of our mixture of generators: 1) to produce connected partitions and\n2) provide a proxy of distance between partitions and data samples, along with\na direction for reducing that distance. These criteria are developed to avoid\nproducing samples from places with non-existent data density, and also\nfacilitate training by providing additional direction to the generators. We\ndevelop theoretical constraints for a space partitioner to satisfy the above\ncriteria. Guided by our theoretical analysis, we design an effective neural\narchitecture for the space partitioner that empirically assures these\nconditions. Experimental results on various standard benchmarks show that the\nproposed unsupervised model outperforms several recent methods.",
          "link": "http://arxiv.org/abs/2104.00816",
          "publishedOn": "2021-06-21T02:07:40.937Z",
          "wordCount": 662,
          "title": "Partition-Guided GANs. (arXiv:2104.00816v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.01420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elhamod_M/0/1/0/all/0/1\">Mohannad Elhamod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1\">Jie Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1\">Christopher Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redell_M/0/1/0/all/0/1\">Matthew Redell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Abantika Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Podolskiy_V/0/1/0/all/0/1\">Viktor Podolskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wei-Cheng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpatne_A/0/1/0/all/0/1\">Anuj Karpatne</a>",
          "description": "Physics-guided Neural Networks (PGNNs) represent an emerging class of neural\nnetworks that are trained using physics-guided (PG) loss functions (capturing\nviolations in network outputs with known physics), along with the supervision\ncontained in data. Existing work in PGNNs have demonstrated the efficacy of\nadding single PG loss functions in the neural network objectives, using\nconstant trade-off parameters, to ensure better generalizability. However, in\nthe presence of multiple physics loss functions with competing gradient\ndirections, there is a need to adaptively tune the contribution of competing PG\nloss functions during the course of training to arrive at generalizable\nsolutions. We demonstrate the presence of competing PG losses in the generic\nneural network problem of solving for the lowest (or highest) eigenvector of a\nphysics-based eigenvalue equation, common to many scientific problems. We\npresent a novel approach to handle competing PG losses and demonstrate its\nefficacy in learning generalizable solutions in two motivating applications of\nquantum mechanics and electromagnetic propagation. All the code and data used\nin this work is available at https://github.com/jayroxis/Cophy-PGNN.",
          "link": "http://arxiv.org/abs/2007.01420",
          "publishedOn": "2021-06-21T02:07:40.929Z",
          "wordCount": 697,
          "title": "CoPhy-PGNN: Learning Physics-guided Neural Networks with Competing Loss Functions for Solving Eigenvalue Problems. (arXiv:2007.01420v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.03384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1\">Fenglei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>",
          "description": "Despite its best performance in image denoising, the supervised deep\ndenoising methods require paired noise-clean data, which are often unavailable.\nTo address this challenge, Noise2Noise was designed based on the fact that\npaired noise-clean images can be replaced by paired noise-noise images that are\neasier to collect. However, in many scenarios the collection of paired\nnoise-noise images is still impractical. To bypass labeled images, Noise2Void\nmethods predict masked pixels from their surroundings with single noisy images\nonly and give improved denoising results that still need improvements. An\nobservation on classic denoising methods is that non-local mean (NLM) outcomes\nare typically superior to locally denoised results. In contrast, Noise2Void and\nits variants do not utilize self-similarities in an image as the NLM-based\nmethods do. Here we propose Noise2Sim, an NLM-inspired self-learning method for\nimage denoising. Specifically, Noise2Sim leverages the self-similarity of image\npixels to train the denoising network, requiring single noisy images only. Our\ntheoretical analysis shows that Noise2Sim tends to be equivalent to Noise2Noise\nunder mild conditions. To efficiently manage the computational burden for\nglobally searching similar pixels, we design a two-step procedure to provide\ndata for Noise2Sim training. Extensive experiments demonstrate the superiority\nof Noise2Sim on common benchmark datasets.",
          "link": "http://arxiv.org/abs/2011.03384",
          "publishedOn": "2021-06-21T02:07:40.920Z",
          "wordCount": 687,
          "title": "Noise2Sim -- Similarity-based Self-Learning for Image Denoising. (arXiv:2011.03384v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiancan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fuli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1\">Jianxun Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>",
          "description": "Representation learning on user-item graph for recommendation has evolved\nfrom using single ID or interaction history to exploiting higher-order\nneighbors. This leads to the success of graph convolution networks (GCNs) for\nrecommendation such as PinSage and LightGCN. Despite effectiveness, we argue\nthat they suffer from two limitations: (1) high-degree nodes exert larger\nimpact on the representation learning, deteriorating the recommendations of\nlow-degree (long-tail) items; and (2) representations are vulnerable to noisy\ninteractions, as the neighborhood aggregation scheme further enlarges the\nimpact of observed edges.\n\nIn this work, we explore self-supervised learning on user-item graph, so as\nto improve the accuracy and robustness of GCNs for recommendation. The idea is\nto supplement the classical supervised task of recommendation with an auxiliary\nself-supervised task, which reinforces node representation learning via\nself-discrimination. Specifically, we generate multiple views of a node,\nmaximizing the agreement between different views of the same node compared to\nthat of other nodes. We devise three operators to generate the views -- node\ndropout, edge dropout, and random walk -- that change the graph structure in\ndifferent manners. We term this new learning paradigm as\n\\textit{Self-supervised Graph Learning} (SGL), implementing it on the\nstate-of-the-art model LightGCN. Through theoretical analyses, we find that SGL\nhas the ability of automatically mining hard negatives. Empirical studies on\nthree benchmark datasets demonstrate the effectiveness of SGL, which improves\nthe recommendation accuracy, especially on long-tail items, and the robustness\nagainst interaction noises. Our implementations are available at\n\\url{https://github.com/wujcan/SGL}.",
          "link": "http://arxiv.org/abs/2010.10783",
          "publishedOn": "2021-06-21T02:07:40.911Z",
          "wordCount": 740,
          "title": "Self-supervised Graph Learning for Recommendation. (arXiv:2010.10783v4 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02452",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guillot_A/0/1/0/all/0/1\">Antoine Guillot</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thorey_V/0/1/0/all/0/1\">Valentin Thorey</a>",
          "description": "Sleep disorder diagnosis relies on the analysis of polysomnography (PSG)\nrecords. As a preliminary step of this examination, sleep stages are\nsystematically determined. In practice, sleep stage classification relies on\nthe visual inspection of 30-second epochs of polysomnography signals. Numerous\nautomatic approaches have been developed to replace this tedious and expensive\ntask. Although these methods demonstrated better performance than human sleep\nexperts on specific datasets, they remain largely unused in sleep clinics. The\nmain reason is that each sleep clinic uses a specific PSG montage that most\nautomatic approaches cannot handle out-of-the-box. Moreover, even when the PSG\nmontage is compatible, publications have shown that automatic approaches\nperform poorly on unseen data with different demographics. To address these\nissues, we introduce RobustSleepNet, a deep learning model for automatic sleep\nstage classification able to handle arbitrary PSG montages. We trained and\nevaluated this model in a leave-one-out-dataset fashion on a large corpus of 8\nheterogeneous sleep staging datasets to make it robust to demographic changes.\nWhen evaluated on an unseen dataset, RobustSleepNet reaches 97% of the F1 of a\nmodel explicitly trained on this dataset. Hence, RobustSleepNet unlocks the\npossibility to perform high-quality out-of-the-box automatic sleep staging with\nany clinical setup. We further show that finetuning RobustSleepNet, using a\npart of the unseen dataset, increases the F1 by 2% when compared to a model\ntrained specifically for this dataset. Therefore, finetuning might be used to\nreach a state-of-the-art level of performance on a specific population.",
          "link": "http://arxiv.org/abs/2101.02452",
          "publishedOn": "2021-06-21T02:07:40.904Z",
          "wordCount": 697,
          "title": "RobustSleepNet: Transfer learning for automated sleep staging at scale. (arXiv:2101.02452v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhiyan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_S/0/1/0/all/0/1\">Stephen Wright</a>",
          "description": "Finding parameters in a deep neural network (NN) that fit training data is a\nnonconvex optimization problem, but a basic first-order optimization method\n(gradient descent) finds a global solution with perfect fit in many practical\nsituations. We examine this phenomenon for the case of Residual Neural Networks\n(ResNet) with smooth activation functions in a limiting regime in which both\nthe number of layers (depth) and the number of neurons in each layer (width) go\nto infinity. First, we use a mean-field-limit argument to prove that the\ngradient descent for parameter training becomes a partial differential equation\n(PDE) that characterizes gradient flow for a probability distribution in the\nlarge-NN limit. Next, we show that the solution to the PDE converges in the\ntraining time to a zero-loss solution. Together, these results imply that\ntraining of the ResNet also gives a near-zero loss if the Resnet is large\nenough. We give estimates of the depth and width needed to reduce the loss\nbelow a given threshold, with high probability.",
          "link": "http://arxiv.org/abs/2105.14417",
          "publishedOn": "2021-06-21T02:07:40.897Z",
          "wordCount": 627,
          "title": "Overparameterization of deep ResNet: zero loss and mean-field analysis. (arXiv:2105.14417v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diao_C/0/1/0/all/0/1\">Cameron Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1\">Denis Kleyko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabaey_J/0/1/0/all/0/1\">Jan M. Rabaey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olshausen_B/0/1/0/all/0/1\">Bruno A. Olshausen</a>",
          "description": "Machine learning algorithms deployed on edge devices must meet certain\nresource constraints and efficiency requirements. Random Vector Functional Link\n(RVFL) networks are favored for such applications due to their simple design\nand training efficiency. We propose a modified RVFL network that avoids\ncomputationally expensive matrix operations during training, thus expanding the\nnetwork's range of potential applications. Our modification replaces the\nleast-squares classifier with the Generalized Learning Vector Quantization\n(GLVQ) classifier, which only employs simple vector and distance calculations.\nThe GLVQ classifier can also be considered an improvement upon certain\nclassification algorithms popularly used in the area of Hyperdimensional\nComputing. The proposed approach achieved state-of-the-art accuracy on a\ncollection of datasets from the UCI Machine Learning Repository - higher than\npreviously proposed RVFL networks. We further demonstrate that our approach\nstill achieves high accuracy while severely limited in training iterations\n(using on average only 21% of the least-squares classifier computational\ncosts).",
          "link": "http://arxiv.org/abs/2106.09821",
          "publishedOn": "2021-06-21T02:07:40.879Z",
          "wordCount": 597,
          "title": "Generalized Learning Vector Quantization for Classification in Randomized Neural Networks and Hyperdimensional Computing. (arXiv:2106.09821v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.09102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozfatura_E/0/1/0/all/0/1\">Emre Ozfatura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozfatura_K/0/1/0/all/0/1\">Kerem Ozfatura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>",
          "description": "Federated learning (FL) has become de facto framework for collaborative\nlearning among edge devices with privacy concern. The core of the FL strategy\nis the use of stochastic gradient descent (SGD) in a distributed manner. Large\nscale implementation of FL brings new challenges, such as the incorporation of\nacceleration techniques designed for SGD into the distributed setting, and\nmitigation of the drift problem due to non-homogeneous distribution of local\ndatasets. These two problems have been separately studied in the literature;\nwhereas, in this paper, we show that it is possible to address both problems\nusing a single strategy without any major alteration to the FL framework, or\nintroducing additional computation and communication load. To achieve this\ngoal, we propose FedADC, which is an accelerated FL algorithm with drift\ncontrol. We empirically illustrate the advantages of FedADC.",
          "link": "http://arxiv.org/abs/2012.09102",
          "publishedOn": "2021-06-21T02:07:40.850Z",
          "wordCount": 607,
          "title": "FedADC: Accelerated Federated Learning with Drift Control. (arXiv:2012.09102v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yoojin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Khamy_M/0/1/0/all/0/1\">Mostafa El-Khamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungwon Lee</a>",
          "description": "This paper proposes two novel knowledge transfer techniques for\nclass-incremental learning (CIL). First, we propose data-free generative replay\n(DF-GR) to mitigate catastrophic forgetting in CIL by using synthetic samples\nfrom a generative model. In the conventional generative replay, the generative\nmodel is pre-trained for old data and shared in extra memory for later\nincremental learning. In our proposed DF-GR, we train a generative model from\nscratch without using any training data, based on the pre-trained\nclassification model from the past, so we curtail the cost of sharing\npre-trained generative models. Second, we introduce dual-teacher information\ndistillation (DT-ID) for knowledge distillation from two teachers to one\nstudent. In CIL, we use DT-ID to learn new classes incrementally based on the\npre-trained model for old classes and another model (pre-)trained on the new\ndata for new classes. We implemented the proposed schemes on top of one of the\nstate-of-the-art CIL methods and showed the performance improvement on\nCIFAR-100 and ImageNet datasets.",
          "link": "http://arxiv.org/abs/2106.09835",
          "publishedOn": "2021-06-21T02:07:40.839Z",
          "wordCount": 612,
          "title": "Dual-Teacher Class-Incremental Learning With Data-Free Generative Replay. (arXiv:2106.09835v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.02373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Sin Kit Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qinghua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Liming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paik_H/0/1/0/all/0/1\">Hye-young Paik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>",
          "description": "Federated learning has received fast-growing interests from academia and\nindustry to tackle the challenges of data hungriness and privacy in machine\nlearning. A federated learning system can be viewed as a large-scale\ndistributed system with different components and stakeholders as numerous\nclient devices participate in federated learning. Designing a federated\nlearning system requires software system design thinking apart from machine\nlearning knowledge. Although much effort has been put into federated learning\nfrom the machine learning technique aspects, the software architecture design\nconcerns in building federated learning systems have been largely ignored.\nTherefore, in this paper, we present a collection of architectural patterns to\ndeal with the design challenges of federated learning systems. Architectural\npatterns present reusable solutions to a commonly occurring problem within a\ngiven context during software architecture design. The presented patterns are\nbased on the results of a systematic literature review and include three client\nmanagement patterns, four model management patterns, three model training\npatterns, and four model aggregation patterns. The patterns are associated to\nthe particular state transitions in a federated learning model lifecycle,\nserving as a guidance for effective use of the patterns in the design of\nfederated learning systems.",
          "link": "http://arxiv.org/abs/2101.02373",
          "publishedOn": "2021-06-21T02:07:40.830Z",
          "wordCount": 702,
          "title": "Architectural Patterns for the Design of Federated Learning Systems. (arXiv:2101.02373v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.05369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Willard_J/0/1/0/all/0/1\">Jared D. Willard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1\">Jordan S. Read</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Appling_A/0/1/0/all/0/1\">Alison P. Appling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliver_S/0/1/0/all/0/1\">Samantha K. Oliver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaowei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vipin Kumar</a>",
          "description": "Most environmental data come from a minority of well-monitored sites. An\nongoing challenge in the environmental sciences is transferring knowledge from\nmonitored sites to unmonitored sites. Here, we demonstrate a novel transfer\nlearning framework that accurately predicts depth-specific temperature in\nunmonitored lakes (targets) by borrowing models from well-monitored lakes\n(sources). This method, Meta Transfer Learning (MTL), builds a meta-learning\nmodel to predict transfer performance from candidate source models to targets\nusing lake attributes and candidates' past performance. We constructed source\nmodels at 145 well-monitored lakes using calibrated process-based modeling (PB)\nand a recently developed approach called process-guided deep learning (PGDL).\nWe applied MTL to either PB or PGDL source models (PB-MTL or PGDL-MTL,\nrespectively) to predict temperatures in 305 target lakes treated as\nunmonitored in the Upper Midwestern United States. We show significantly\nimproved performance relative to the uncalibrated process-based General Lake\nModel, where the median RMSE for the target lakes is $2.52^{\\circ}C$. PB-MTL\nyielded a median RMSE of $2.43^{\\circ}C$; PGDL-MTL yielded $2.16^{\\circ}C$; and\na PGDL-MTL ensemble of nine sources per target yielded $1.88^{\\circ}C$. For\nsparsely monitored target lakes, PGDL-MTL often outperformed PGDL models\ntrained on the target lakes themselves. Differences in maximum depth between\nthe source and target were consistently the most important predictors. Our\napproach readily scales to thousands of lakes in the Midwestern United States,\ndemonstrating that MTL with meaningful predictor variables and high-quality\nsource models is a promising approach for many kinds of unmonitored systems and\nenvironmental variables.",
          "link": "http://arxiv.org/abs/2011.05369",
          "publishedOn": "2021-06-21T02:07:40.818Z",
          "wordCount": 723,
          "title": "Predicting Water Temperature Dynamics of Unmonitored Lakes with Meta Transfer Learning. (arXiv:2011.05369v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06231",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kimura_M/0/1/0/all/0/1\">Masanari Kimura</a>",
          "description": "Machine learning techniques are used in a wide range of domains. However,\nmachine learning models often suffer from the problem of over-fitting. Many\ndata augmentation methods have been proposed to tackle such a problem, and one\nof them is called mixup. Mixup is a recently proposed regularization procedure,\nwhich linearly interpolates a random pair of training examples. This\nregularization method works very well experimentally, but its theoretical\nguarantee is not adequately discussed. In this study, we aim to discover why\nmixup works well from the aspect of the statistical learning theory.",
          "link": "http://arxiv.org/abs/2006.06231",
          "publishedOn": "2021-06-21T02:07:40.801Z",
          "wordCount": 550,
          "title": "Why Mixup Improves the Model Performance. (arXiv:2006.06231v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.14162",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gedon_D/0/1/0/all/0/1\">Daniel Gedon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wahlstrom_N/0/1/0/all/0/1\">Niklas Wahlstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schon_T/0/1/0/all/0/1\">Thomas B. Sch&#xf6;n</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ljung_L/0/1/0/all/0/1\">Lennart Ljung</a>",
          "description": "Deep state space models (SSMs) are an actively researched model class for\ntemporal models developed in the deep learning community which have a close\nconnection to classic SSMs. The use of deep SSMs as a black-box identification\nmodel can describe a wide range of dynamics due to the flexibility of deep\nneural networks. Additionally, the probabilistic nature of the model class\nallows the uncertainty of the system to be modelled. In this work a deep SSM\nclass and its parameter learning algorithm are explained in an effort to extend\nthe toolbox of nonlinear identification methods with a deep learning based\nmethod. Six recent deep SSMs are evaluated in a first unified implementation on\nnonlinear system identification benchmarks.",
          "link": "http://arxiv.org/abs/2003.14162",
          "publishedOn": "2021-06-21T02:07:40.774Z",
          "wordCount": 590,
          "title": "Deep State Space Models for Nonlinear System Identification. (arXiv:2003.14162v3 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.15714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojha_A/0/1/0/all/0/1\">Aditya Ojha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neider_D/0/1/0/all/0/1\">Daniel Neider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>",
          "description": "Despite the fact that deep reinforcement learning (RL) has surpassed\nhuman-level performances in various tasks, it still has several fundamental\nchallenges. First, most RL methods require intensive data from the exploration\nof the environment to achieve satisfactory performance. Second, the use of\nneural networks in RL renders it hard to interpret the internals of the system\nin a way that humans can understand. To address these two challenges, we\npropose a framework that enables an RL agent to reason over its exploration\nprocess and distill high-level knowledge for effectively guiding its future\nexplorations. Specifically, we propose a novel RL algorithm that learns\nhigh-level knowledge in the form of a finite reward automaton by using the L*\nlearning algorithm. We prove that in episodic RL, a finite reward automaton can\nexpress any non-Markovian bounded reward functions with finitely many reward\nvalues and approximate any non-Markovian bounded reward function (with\ninfinitely many reward values) with arbitrary precision. We also provide a\nlower bound for the episode length such that the proposed RL approach almost\nsurely converges to an optimal policy in the limit. We test this approach on\ntwo RL environments with non-Markovian reward functions, choosing a variety of\ntasks with increasing complexity for each environment. We compare our algorithm\nwith the state-of-the-art RL algorithms for non-Markovian reward functions,\nsuch as Joint Inference of Reward machines and Policies for RL (JIRP), Learning\nReward Machine (LRM), and Proximal Policy Optimization (PPO2). Our results show\nthat our algorithm converges to an optimal policy faster than other baseline\nmethods.",
          "link": "http://arxiv.org/abs/2006.15714",
          "publishedOn": "2021-06-21T02:07:40.766Z",
          "wordCount": 736,
          "title": "Active Finite Reward Automaton Inference and Reinforcement Learning Using Queries and Counterexamples. (arXiv:2006.15714v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.01496",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shuo Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fanglan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chang-Tien Lu</a>",
          "description": "Despite the great progress made by deep neural networks in the semantic\nsegmentation task, traditional neural-networkbased methods typically suffer\nfrom a shortage of large amounts of pixel-level annotations. Recent progress in\nfewshot semantic segmentation tackles the issue by only a few pixel-level\nannotated examples. However, these few-shot approaches cannot easily be applied\nto multi-way or weak annotation settings. In this paper, we advance the\nfew-shot segmentation paradigm towards a scenario where image-level annotations\nare available to help the training process of a few pixel-level annotations.\nOur key idea is to learn a better prototype representation of the class by\nfusing the knowledge from the image-level labeled data. Specifically, we\npropose a new framework, called PAIA, to learn the class prototype\nrepresentation in a metric space by integrating image-level annotations.\nFurthermore, by considering the uncertainty of pseudo-masks, a distilled soft\nmasked average pooling strategy is designed to handle distractions in\nimage-level annotations. Extensive empirical results on two datasets show\nsuperior performance of PAIA.",
          "link": "http://arxiv.org/abs/2007.01496",
          "publishedOn": "2021-06-21T02:07:40.759Z",
          "wordCount": 644,
          "title": "Few-Shot Semantic Segmentation Augmented with Image-Level Weak Annotations. (arXiv:2007.01496v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.14610",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Garnier_R/0/1/0/all/0/1\">R&#xe9;my Garnier</a>",
          "description": "Competition between times series often arises in sales prediction, when\nsimilar products are on sale on a marketplace. This article provides a model of\nthe presence of cannibalization between times series. This model creates a\n\"competitiveness\" function that depends on external features such as price and\nmargin. It also provides a theoretical guaranty on the error of the model under\nsome reasonable conditions, and implement this model using a neural network to\ncompute this competitiveness function. This implementation outperforms other\ntraditional time series methods and classical neural networks for market share\nprediction on a real-world data set.",
          "link": "http://arxiv.org/abs/2009.14610",
          "publishedOn": "2021-06-21T02:07:40.753Z",
          "wordCount": 551,
          "title": "Concurrent Neural Network : A model of competition between times series. (arXiv:2009.14610v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Papoudakis_G/0/1/0/all/0/1\">Georgios Papoudakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christianos_F/0/1/0/all/0/1\">Filippos Christianos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1\">Stefano V. Albrecht</a>",
          "description": "Modelling the behaviours of other agents is essential for understanding how\nagents interact and making effective decisions. Existing methods for agent\nmodelling commonly assume knowledge of the local observations and chosen\nactions of the modelled agents during execution. To eliminate this assumption,\nwe extract representations from the local information of the controlled agent\nusing encoder-decoder architectures. Using the observations and actions of the\nmodelled agents during training, our models learn to extract representations\nabout the modelled agents conditioned only on the local observations of the\ncontrolled agent. The representations are used to augment the controlled\nagent's decision policy which is trained via deep reinforcement learning; thus,\nduring execution, the policy does not require access to other agents'\ninformation. We provide a comprehensive evaluation and ablations studies in\ncooperative, competitive and mixed multi-agent environments, showing that our\nmethod achieves significantly higher returns than baseline methods which do not\nuse the learned representations.",
          "link": "http://arxiv.org/abs/2006.09447",
          "publishedOn": "2021-06-21T02:07:40.735Z",
          "wordCount": 625,
          "title": "Local Information Agent Modelling in Partially-Observable Environments. (arXiv:2006.09447v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ebert_Uphoff_I/0/1/0/all/0/1\">Imme Ebert-Uphoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lagerquist_R/0/1/0/all/0/1\">Ryan Lagerquist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilburn_K/0/1/0/all/0/1\">Kyle Hilburn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yoonjin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haynes_K/0/1/0/all/0/1\">Katherine Haynes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stock_J/0/1/0/all/0/1\">Jason Stock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumler_C/0/1/0/all/0/1\">Christina Kumler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stewart_J/0/1/0/all/0/1\">Jebb Q. Stewart</a>",
          "description": "Neural networks are increasingly used in environmental science applications.\nFurthermore, neural network models are trained by minimizing a loss function,\nand it is crucial to choose the loss function very carefully for environmental\nscience applications, as it determines what exactly is being optimized.\nStandard loss functions do not cover all the needs of the environmental\nsciences, which makes it important for scientists to be able to develop their\nown custom loss functions so that they can implement many of the classic\nperformance measures already developed in environmental science, including\nmeasures developed for spatial model verification. However, there are very few\nresources available that cover the basics of custom loss function development\ncomprehensively, and to the best of our knowledge none that focus on the needs\nof environmental scientists. This document seeks to fill this gap by providing\na guide on how to write custom loss functions targeted toward environmental\nscience applications. Topics include the basics of writing custom loss\nfunctions, common pitfalls, functions to use in loss functions, examples such\nas fractions skill score as loss function, how to incorporate physical\nconstraints, discrete and soft discretization, and concepts such as focal,\nrobust, and adaptive loss. While examples are currently provided in this guide\nfor Python with Keras and the TensorFlow backend, the basic concepts also apply\nto other environments, such as Python with PyTorch. Similarly, while the sample\nloss functions provided here are from meteorology, these are just examples of\nhow to create custom loss functions. Other fields in the environmental sciences\nhave very similar needs for custom loss functions, e.g., for evaluating spatial\nforecasts effectively, and the concepts discussed here can be applied there as\nwell. All code samples are provided in a GitHub repository.",
          "link": "http://arxiv.org/abs/2106.09757",
          "publishedOn": "2021-06-21T02:07:40.721Z",
          "wordCount": 752,
          "title": "CIRA Guide to Custom Loss Functions for Neural Networks in Environmental Sciences -- Version 1. (arXiv:2106.09757v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.10859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kodryan_M/0/1/0/all/0/1\">Maxim Kodryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kropotov_D/0/1/0/all/0/1\">Dmitry Kropotov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1\">Dmitry Vetrov</a>",
          "description": "Tensor decomposition methods are known to be efficient for compressing and\naccelerating neural networks. However, the problem of optimal decomposition\nstructure determination is still not well studied while being quite important.\nSpecifically, decomposition ranks present the crucial parameter controlling the\ncompression-accuracy trade-off. In this paper, we introduce MARS -- a new\nefficient method for the automatic selection of ranks in general tensor\ndecompositions. During training, the procedure learns binary masks over\ndecomposition cores that \"select\" the optimal tensor structure. The learning is\nperformed via relaxed maximum a posteriori (MAP) estimation in a specific\nBayesian model. The proposed method achieves better results compared to\nprevious works in various tasks.",
          "link": "http://arxiv.org/abs/2006.10859",
          "publishedOn": "2021-06-21T02:07:40.714Z",
          "wordCount": 569,
          "title": "MARS: Masked Automatic Ranks Selection in Tensor Decompositions. (arXiv:2006.10859v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.10847",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Chen_T/0/1/0/all/0/1\">Tianyi Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1\">Yuejiao Sun</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yin_W/0/1/0/all/0/1\">Wotao Yin</a>",
          "description": "Stochastic compositional optimization generalizes classic (non-compositional)\nstochastic optimization to the minimization of compositions of functions. Each\ncomposition may introduce an additional expectation. The series of expectations\nmay be nested. Stochastic compositional optimization is gaining popularity in\napplications such as reinforcement learning and meta learning. This paper\npresents a new Stochastically Corrected Stochastic Compositional gradient\nmethod (SCSC). SCSC runs in a single-time scale with a single loop, uses a\nfixed batch size, and guarantees to converge at the same rate as the stochastic\ngradient descent (SGD) method for non-compositional stochastic optimization.\nThis is achieved by making a careful improvement to a popular stochastic\ncompositional gradient method. It is easy to apply SGD-improvement techniques\nto accelerate SCSC. This helps SCSC achieve state-of-the-art performance for\nstochastic compositional optimization. In particular, we apply Adam to SCSC,\nand the exhibited rate of convergence matches that of the original Adam on\nnon-compositional stochastic optimization. We test SCSC using the portfolio\nmanagement and model-agnostic meta-learning tasks.",
          "link": "http://arxiv.org/abs/2008.10847",
          "publishedOn": "2021-06-21T02:07:40.707Z",
          "wordCount": 651,
          "title": "Solving Stochastic Compositional Optimization is Nearly as Easy as Solving Stochastic Optimization. (arXiv:2008.10847v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.06511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haiyun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiaosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>",
          "description": "This paper investigates a novel offline change-point detection problem from\nan information-theoretic perspective. In contrast to most related works, we\nassume that the knowledge of the underlying pre- and post-change distributions\nare not known and can only be learned from the training sequences which are\navailable. We further require the probability of the \\emph{estimation error} to\ndecay either exponentially or sub-exponentially fast (corresponding\nrespectively to the large and moderate deviations regimes in information theory\nparlance). Based on the training sequences as well as the test sequence\nconsisting of a single change-point, we design a change-point estimator and\nfurther show that this estimator is optimal by establishing matching (strong)\nconverses. This leads to a full characterization of the optimal confidence\nwidth (i.e., half the width of the confidence interval within which the true\nchange-point is located at with high probability) as a function of the\nundetected error, under both the large and moderate deviations regimes.",
          "link": "http://arxiv.org/abs/2003.06511",
          "publishedOn": "2021-06-21T02:07:40.688Z",
          "wordCount": 655,
          "title": "Optimal Change-Point Detection with Training Sequences in the Large and Moderate Deviations Regimes. (arXiv:2003.06511v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.09021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amir_T/0/1/0/all/0/1\">Tal Amir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basri_R/0/1/0/all/0/1\">Ronen Basri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadler_B/0/1/0/all/0/1\">Boaz Nadler</a>",
          "description": "We present a new approach to solve the sparse approximation or best subset\nselection problem, namely find a $k$-sparse vector ${\\bf x}\\in\\mathbb{R}^d$\nthat minimizes the $\\ell_2$ residual $\\lVert A{\\bf x}-{\\bf y} \\rVert_2$. We\nconsider a regularized approach, whereby this residual is penalized by the\nnon-convex $\\textit{trimmed lasso}$, defined as the $\\ell_1$-norm of ${\\bf x}$\nexcluding its $k$ largest-magnitude entries. We prove that the trimmed lasso\nhas several appealing theoretical properties, and in particular derive sparse\nrecovery guarantees assuming successful optimization of the penalized\nobjective. Next, we show empirically that directly optimizing this objective\ncan be quite challenging. Instead, we propose a surrogate for the trimmed\nlasso, called the $\\textit{generalized soft-min}$. This penalty smoothly\ninterpolates between the classical lasso and the trimmed lasso, while taking\ninto account all possible $k$-sparse patterns. The generalized soft-min penalty\ninvolves summation over $\\binom{d}{k}$ terms, yet we derive a polynomial-time\nalgorithm to compute it. This, in turn, yields a practical method for the\noriginal sparse approximation problem. Via simulations, we demonstrate its\ncompetitive performance compared to current state of the art.",
          "link": "http://arxiv.org/abs/2005.09021",
          "publishedOn": "2021-06-21T02:07:40.681Z",
          "wordCount": 683,
          "title": "The Trimmed Lasso: Sparse Recovery Guarantees and Practical Optimization by the Generalized Soft-Min Penalty. (arXiv:2005.09021v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07267",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wanrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1\">Shruti Tople</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohrimenko_O/0/1/0/all/0/1\">Olga Ohrimenko</a>",
          "description": "Secure multi-party machine learning allows several parties to build a model\non their pooled data to increase utility while not explicitly sharing data with\neach other. We show that such multi-party computation can cause leakage of\nglobal dataset properties between the parties even when parties obtain only\nblack-box access to the final model. In particular, a ``curious'' party can\ninfer the distribution of sensitive attributes in other parties' data with high\naccuracy. This raises concerns regarding the confidentiality of properties\npertaining to the whole dataset as opposed to individual data records. We show\nthat our attack can leak population-level properties in datasets of different\ntypes, including tabular, text, and graph data. To understand and measure the\nsource of leakage, we consider several models of correlation between a\nsensitive attribute and the rest of the data. Using multiple machine learning\nmodels, we show that leakage occurs even if the sensitive attribute is not\nincluded in the training data and has a low correlation with other attributes\nor the target variable.",
          "link": "http://arxiv.org/abs/2006.07267",
          "publishedOn": "2021-06-21T02:07:40.662Z",
          "wordCount": 651,
          "title": "Leakage of Dataset Properties in Multi-Party Machine Learning. (arXiv:2006.07267v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bykov_K/0/1/0/all/0/1\">Kirill Bykov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedstrom_A/0/1/0/all/0/1\">Anna Hedstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1\">Shinichi Nakajima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1\">Marina M.-C. H&#xf6;hne</a>",
          "description": "Attribution methods remain a practical instrument that is used in real-world\napplications to explain the decision-making process of complex learning\nmachines. It has been shown that a simple method called SmoothGrad can\neffectively reduce the visual diffusion of gradient-based attribution methods\nand has established itself among both researchers and practitioners. What\nremains unexplored in research, however, is how explanations can be improved by\nintroducing stochasticity to the model weights. In the light of this, we\nintroduce - NoiseGrad - a stochastic, method-agnostic explanation-enhancing\nmethod that adds noise to the weights instead of the input data. We investigate\nour proposed method through various experiments including different datasets,\nexplanation methods and network architectures and conclude that NoiseGrad (and\nits extension NoiseGrad++) with multiplicative Gaussian noise offers a clear\nadvantage compared to SmoothGrad on several evaluation criteria. We connect our\nproposed method to Bayesian Learning and provide the user with a heuristic for\nchoosing hyperparameters.",
          "link": "http://arxiv.org/abs/2106.10185",
          "publishedOn": "2021-06-21T02:07:40.252Z",
          "wordCount": 590,
          "title": "NoiseGrad: enhancing explanations by introducing stochasticity to model weights. (arXiv:2106.10185v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yi-Ling Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yu-Che Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng-Te Li</a>",
          "description": "Financial technology (FinTech) has drawn much attention among investors and\ncompanies. While conventional stock analysis in FinTech targets at predicting\nstock prices, less effort is made for profitable stock recommendation. Besides,\nin existing approaches on modeling time series of stock prices, the\nrelationships among stocks and sectors (i.e., categories of stocks) are either\nneglected or pre-defined. Ignoring stock relationships will miss the\ninformation shared between stocks while using pre-defined relationships cannot\ndepict the latent interactions or influence of stock prices between stocks. In\nthis work, we aim at recommending the top-K profitable stocks in terms of\nreturn ratio using time series of stock prices and sector information. We\npropose a novel deep learning-based model, Financial Graph Attention Networks\n(FinGAT), to tackle the task under the setting that no pre-defined\nrelationships between stocks are given. The idea of FinGAT is three-fold.\nFirst, we devise a hierarchical learning component to learn short-term and\nlong-term sequential patterns from stock time series. Second, a fully-connected\ngraph between stocks and a fully-connected graph between sectors are\nconstructed, along with graph attention networks, to learn the latent\ninteractions among stocks and sectors. Third, a multi-task objective is devised\nto jointly recommend the profitable stocks and predict the stock movement.\nExperiments conducted on Taiwan Stock, S&P 500, and NASDAQ datasets exhibit\nremarkable recommendation performance of our FinGAT, comparing to\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.10159",
          "publishedOn": "2021-06-21T02:07:40.210Z",
          "wordCount": 695,
          "title": "FinGAT: Financial Graph Attention Networks for Recommending Top-K Profitable Stocks. (arXiv:2106.10159v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09963",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jinhan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunzheng Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_R/0/1/0/all/0/1\">Ruchao Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chu_W/0/1/0/all/0/1\">Wei Chu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alwan_A/0/1/0/all/0/1\">Abeer Alwan</a>",
          "description": "This paper describes the SPAPL system for the INTERSPEECH 2021 Challenge:\nShared Task on Automatic Speech Recognition for Non-Native Children's Speech in\nGerman. ~ 5 hours of transcribed data and ~ 60 hours of untranscribed data are\nprovided to develop a German ASR system for children. For the training of the\ntranscribed data, we propose a non-speech state discriminative loss (NSDL) to\nmitigate the influence of long-duration non-speech segments within speech\nutterances. In order to explore the use of the untranscribed data, various\napproaches are implemented and combined together to incrementally improve the\nsystem performance. First, bidirectional autoregressive predictive coding\n(Bi-APC) is used to learn initial parameters for acoustic modelling using the\nprovided untranscribed data. Second, incremental semi-supervised learning is\nfurther used to iteratively generate pseudo-transcribed data. Third, different\ndata augmentation schemes are used at different training stages to increase the\nvariability and size of the training data. Finally, a recurrent neural network\nlanguage model (RNNLM) is used for rescoring. Our system achieves a word error\nrate (WER) of 39.68% on the evaluation data, an approximately 12% relative\nimprovement over the official baseline (45.21%).",
          "link": "http://arxiv.org/abs/2106.09963",
          "publishedOn": "2021-06-21T02:07:40.192Z",
          "wordCount": 653,
          "title": "Low Resource German ASR with Untranscribed Data Spoken by Non-native Children -- INTERSPEECH 2021 Shared Task SPAPL System. (arXiv:2106.09963v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Daya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1\">Alexey Svyatkovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1\">Marc Brockschmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allamanis_M/0/1/0/all/0/1\">Miltiadis Allamanis</a>",
          "description": "Traditional generative models are limited to predicting sequences of terminal\ntokens. However, ambiguities in the generation task may lead to incorrect\noutputs. Towards addressing this, we introduce Grammformers, transformer-based\ngrammar-guided models that learn (without explicit supervision) to generate\nsketches -- sequences of tokens with holes. Through reinforcement learning,\nGrammformers learn to introduce holes avoiding the generation of incorrect\ntokens where there is ambiguity in the target task.\n\nWe train Grammformers for statement-level source code completion, i.e., the\ngeneration of code snippets given an ambiguous user intent, such as a partial\ncode context. We evaluate Grammformers on code completion for C# and Python and\nshow that it generates 10-50% more accurate sketches compared to traditional\ngenerative models and 37-50% longer sketches compared to sketch-generating\nbaselines trained with similar techniques.",
          "link": "http://arxiv.org/abs/2106.10158",
          "publishedOn": "2021-06-21T02:07:40.185Z",
          "wordCount": 560,
          "title": "Learning to Generate Code Sketches. (arXiv:2106.10158v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ash_J/0/1/0/all/0/1\">Jordan T. Ash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1\">Surbhi Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1\">Akshay Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1\">Dipendra Misra</a>",
          "description": "Noise contrastive learning is a popular technique for unsupervised\nrepresentation learning. In this approach, a representation is obtained via\nreduction to supervised learning, where given a notion of semantic similarity,\nthe learner tries to distinguish a similar (positive) example from a collection\nof random (negative) examples. The success of modern contrastive learning\npipelines relies on many parameters such as the choice of data augmentation,\nthe number of negative examples, and the batch size; however, there is limited\nunderstanding as to how these parameters interact and affect downstream\nperformance. We focus on disambiguating the role of one of these parameters:\nthe number of negative examples. Theoretically, we show the existence of a\ncollision-coverage trade-off suggesting that the optimal number of negative\nexamples should scale with the number of underlying concepts in the data.\nEmpirically, we scrutinize the role of the number of negatives in both NLP and\nvision tasks. In the NLP task, we find that the results broadly agree with our\ntheory, while our vision experiments are murkier with performance sometimes\neven being insensitive to the number of negatives. We discuss plausible\nexplanations for this behavior and suggest future directions to better align\ntheory and practice.",
          "link": "http://arxiv.org/abs/2106.09943",
          "publishedOn": "2021-06-21T02:07:40.158Z",
          "wordCount": 639,
          "title": "Investigating the Role of Negatives in Contrastive Representation Learning. (arXiv:2106.09943v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Awiszus_M/0/1/0/all/0/1\">Maren Awiszus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_F/0/1/0/all/0/1\">Frederik Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>",
          "description": "This work introduces World-GAN, the first method to perform data-driven\nProcedural Content Generation via Machine Learning in Minecraft from a single\nexample. Based on a 3D Generative Adversarial Network (GAN) architecture, we\nare able to create arbitrarily sized world snippets from a given sample. We\nevaluate our approach on creations from the community as well as structures\ngenerated with the Minecraft World Generator. Our method is motivated by the\ndense representations used in Natural Language Processing (NLP) introduced with\nword2vec [1]. The proposed block2vec representations make World-GAN independent\nfrom the number of different blocks, which can vary a lot in Minecraft, and\nenable the generation of larger levels. Finally, we demonstrate that changing\nthis new representation space allows us to change the generated style of an\nalready trained generator. World-GAN enables its users to generate Minecraft\nworlds based on parts of their creations.",
          "link": "http://arxiv.org/abs/2106.10155",
          "publishedOn": "2021-06-21T02:07:40.152Z",
          "wordCount": 593,
          "title": "World-GAN: a Generative Model for Minecraft Worlds. (arXiv:2106.10155v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10052",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mathieu_E/0/1/0/all/0/1\">Emile Mathieu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Foster_A/0/1/0/all/0/1\">Adam Foster</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>",
          "description": "Learning representations of stochastic processes is an emerging problem in\nmachine learning with applications from meta-learning to physical object models\nto time series. Typical methods rely on exact reconstruction of observations,\nbut this approach breaks down as observations become high-dimensional or noise\ndistributions become complex. To address this, we propose a unifying framework\nfor learning contrastive representations of stochastic processes (CRESP) that\ndoes away with exact reconstruction. We dissect potential use cases for\nstochastic process representations, and propose methods that accommodate each.\nEmpirically, we show that our methods are effective for learning\nrepresentations of periodic functions, 3D objects and dynamical processes. Our\nmethods tolerate noisy high-dimensional observations better than traditional\napproaches, and the learned representations transfer to a range of downstream\ntasks.",
          "link": "http://arxiv.org/abs/2106.10052",
          "publishedOn": "2021-06-21T02:07:40.145Z",
          "wordCount": 551,
          "title": "On Contrastive Representations of Stochastic Processes. (arXiv:2106.10052v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09798",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Clerico_E/0/1/0/all/0/1\">Eugenio Clerico</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1\">George Deligiannidis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>",
          "description": "The limit of infinite width allows for substantial simplifications in the\nanalytical study of overparameterized neural networks. With a suitable random\ninitialization, an extremely large network is well approximated by a Gaussian\nprocess, both before and during training. In the present work, we establish a\nsimilar result for a simple stochastic architecture whose parameters are random\nvariables. The explicit evaluation of the output distribution allows for a\nPAC-Bayesian training procedure that directly optimizes the generalization\nbound. For a large but finite-width network, we show empirically on MNIST that\nthis training approach can outperform standard PAC-Bayesian methods.",
          "link": "http://arxiv.org/abs/2106.09798",
          "publishedOn": "2021-06-21T02:07:40.031Z",
          "wordCount": 532,
          "title": "Wide stochastic networks: Gaussian limit and PAC-Bayesian training. (arXiv:2106.09798v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10241",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pereira_M/0/1/0/all/0/1\">Mayana Pereira</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kshirsagar_M/0/1/0/all/0/1\">Meghana Kshirsagar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mukherjee_S/0/1/0/all/0/1\">Sumit Mukherjee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dodhia_R/0/1/0/all/0/1\">Rahul Dodhia</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ferres_J/0/1/0/all/0/1\">Juan Lavista Ferres</a>",
          "description": "Diferentially private (DP) synthetic datasets are a powerful approach for\ntraining machine learning models while respecting the privacy of individual\ndata providers. The effect of DP on the fairness of the resulting trained\nmodels is not yet well understood. In this contribution, we systematically\nstudy the effects of differentially private synthetic data generation on\nclassification. We analyze disparities in model utility and bias caused by the\nsynthetic dataset, measured through algorithmic fairness metrics. Our first set\nof results show that although there seems to be a clear negative correlation\nbetween privacy and utility (the more private, the less accurate) across all\ndata synthesizers we evaluated, more privacy does not necessarily imply more\nbias. Additionally, we assess the effects of utilizing synthetic datasets for\nmodel training and model evaluation. We show that results obtained on synthetic\ndata can misestimate the actual model performance when it is deployed on real\ndata. We hence advocate on the need for defining proper testing protocols in\nscenarios where differentially private synthetic datasets are utilized for\nmodel training and evaluation.",
          "link": "http://arxiv.org/abs/2106.10241",
          "publishedOn": "2021-06-21T02:07:40.024Z",
          "wordCount": 629,
          "title": "An Analysis of the Deployment of Models Trained on Private Tabular Synthetic Data: Unexpected Surprises. (arXiv:2106.10241v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2005.11115",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Steland_A/0/1/0/all/0/1\">Ansgar Steland</a>",
          "description": "Supervised learning by extreme learning machines resp. neural networks with\nrandom weights is studied under a non-stationary spatial-temporal sampling\ndesign which especially addresses settings where an autonomous object moving in\na non-stationary spatial environment collects and analyzes data. The stochastic\nmodel especially allows for spatial heterogeneity and weak dependence. As\nefficient and computationally cheap learning methods (unconstrained) least\nsquares, ridge regression and $\\ell_s$-penalized least squares (including the\nLASSO) are studied. Consistency and asymptotic normality of the least squares\nand ridge regression estimates as well as corresponding consistency results for\nthe $\\ell_s$-penalty are shown under weak conditions. The resuts also cover\nbounds for the sample squared predicition error.",
          "link": "http://arxiv.org/abs/2005.11115",
          "publishedOn": "2021-06-21T02:07:40.017Z",
          "wordCount": 581,
          "title": "Consistency of Extreme Learning Machines and Regression under Non-Stationarity and Dependence for ML-Enhanced Moving Objects. (arXiv:2005.11115v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.03180",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Bhattacharya_K/0/1/0/all/0/1\">Kaushik Bhattacharya</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hosseini_B/0/1/0/all/0/1\">Bamdad Hosseini</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kovachki_N/0/1/0/all/0/1\">Nikola B. Kovachki</a>, <a href=\"http://arxiv.org/find/math/1/au:+Stuart_A/0/1/0/all/0/1\">Andrew M. Stuart</a>",
          "description": "We develop a general framework for data-driven approximation of input-output\nmaps between infinite-dimensional spaces. The proposed approach is motivated by\nthe recent successes of neural networks and deep learning, in combination with\nideas from model reduction. This combination results in a neural network\napproximation which, in principle, is defined on infinite-dimensional spaces\nand, in practice, is robust to the dimension of finite-dimensional\napproximations of these spaces required for computation. For a class of\ninput-output maps, and suitably chosen probability measures on the inputs, we\nprove convergence of the proposed approximation methodology. We also include\nnumerical experiments which demonstrate the effectiveness of the method,\nshowing convergence and robustness of the approximation scheme with respect to\nthe size of the discretization, and compare it with existing algorithms from\nthe literature; our examples include the mapping from coefficient to solution\nin a divergence form elliptic partial differential equation (PDE) problem, and\nthe solution operator for viscous Burgers' equation.",
          "link": "http://arxiv.org/abs/2005.03180",
          "publishedOn": "2021-06-21T02:07:40.010Z",
          "wordCount": 626,
          "title": "Model Reduction and Neural Networks for Parametric PDEs. (arXiv:2005.03180v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aboutalebi_H/0/1/0/all/0/1\">Hossein Aboutalebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1\">Mohammad Javad Shafiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karg_M/0/1/0/all/0/1\">Michelle Karg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharfenberger_C/0/1/0/all/0/1\">Christian Scharfenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>",
          "description": "Despite the significant advances in deep learning over the past decade, a\nmajor challenge that limits the wide-spread adoption of deep learning has been\ntheir fragility to adversarial attacks. This sensitivity to making erroneous\npredictions in the presence of adversarially perturbed data makes deep neural\nnetworks difficult to adopt for certain real-world, mission-critical\napplications. While much of the research focus has revolved around adversarial\nexample creation and adversarial hardening, the area of performance measures\nfor assessing adversarial robustness is not well explored. Motivated by this,\nthis study presents the concept of residual error, a new performance measure\nfor not only assessing the adversarial robustness of a deep neural network at\nthe individual sample level, but also can be used to differentiate between\nadversarial and non-adversarial examples to facilitate for adversarial example\ndetection. Furthermore, we introduce a hybrid model for approximating the\nresidual error in a tractable manner. Experimental results using the case of\nimage classification demonstrates the effectiveness and efficacy of the\nproposed residual error metric for assessing several well-known deep neural\nnetwork architectures. These results thus illustrate that the proposed measure\ncould be a useful tool for not only assessing the robustness of deep neural\nnetworks used in mission-critical scenarios, but also in the design of\nadversarially robust models.",
          "link": "http://arxiv.org/abs/2106.10212",
          "publishedOn": "2021-06-21T02:07:40.004Z",
          "wordCount": 657,
          "title": "Residual Error: a New Performance Measure for Adversarial Robustness. (arXiv:2106.10212v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2001.09136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Byerly_A/0/1/0/all/0/1\">Adam Byerly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalganova_T/0/1/0/all/0/1\">Tatiana Kalganova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dear_I/0/1/0/all/0/1\">Ian Dear</a>",
          "description": "Most capsule network designs rely on traditional matrix multiplication\nbetween capsule layers and computationally expensive routing mechanisms to deal\nwith the capsule dimensional entanglement that the matrix multiplication\nintroduces. By using Homogeneous Vector Capsules (HVCs), which use element-wise\nmultiplication rather than matrix multiplication, the dimensions of the\ncapsules remain unentangled. In this work, we study HVCs as applied to the\nhighly structured MNIST dataset in order to produce a direct comparison to the\ncapsule research direction of Geoffrey Hinton, et al. In our study, we show\nthat a simple convolutional neural network using HVCs performs as well as the\nprior best performing capsule network on MNIST using 5.5x fewer parameters, 4x\nfewer training epochs, no reconstruction sub-network, and requiring no routing\nmechanism. The addition of multiple classification branches to the network\nestablishes a new state of the art for the MNIST dataset with an accuracy of\n99.87% for an ensemble of these models, as well as establishing a new state of\nthe art for a single model (99.83% accurate).",
          "link": "http://arxiv.org/abs/2001.09136",
          "publishedOn": "2021-06-21T02:07:39.984Z",
          "wordCount": 671,
          "title": "No Routing Needed Between Capsules. (arXiv:2001.09136v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sajadmanesh_S/0/1/0/all/0/1\">Sina Sajadmanesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatica_Perez_D/0/1/0/all/0/1\">Daniel Gatica-Perez</a>",
          "description": "Graph Neural Networks (GNNs) have demonstrated superior performance in\nlearning node representations for various graph inference tasks. However,\nlearning over graph data can raise privacy concerns when nodes represent people\nor human-related variables that involve sensitive or personal information.\nWhile numerous techniques have been proposed for privacy-preserving deep\nlearning over non-relational data, there is less work addressing the privacy\nissues pertained to applying deep learning algorithms on graphs. In this paper,\nwe study the problem of node data privacy, where graph nodes have potentially\nsensitive data that is kept private, but they could be beneficial for a central\nserver for training a GNN over the graph. To address this problem, we develop a\nprivacy-preserving, architecture-agnostic GNN learning algorithm with formal\nprivacy guarantees based on Local Differential Privacy (LDP). Specifically, we\npropose an LDP encoder and an unbiased rectifier, by which the server can\ncommunicate with the graph nodes to privately collect their data and\napproximate the GNN's first layer. To further reduce the effect of the injected\nnoise, we propose to prepend a simple graph convolution layer, called KProp,\nwhich is based on the multi-hop aggregation of the nodes' features acting as a\ndenoising mechanism. Finally, we propose a robust training framework, in which\nwe benefit from KProp's denoising capability to increase the accuracy of\ninference in the presence of noisy labels. Extensive experiments conducted over\nreal-world datasets demonstrate that our method can maintain a satisfying level\nof accuracy with low privacy loss.",
          "link": "http://arxiv.org/abs/2006.05535",
          "publishedOn": "2021-06-21T02:07:39.978Z",
          "wordCount": 759,
          "title": "Locally Private Graph Neural Networks. (arXiv:2006.05535v8 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tebbutt_W/0/1/0/all/0/1\">Will Tebbutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1\">Arno Solin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1\">Richard E. Turner</a>",
          "description": "Gaussian processes (GPs) are important probabilistic tools for inference and\nlearning in spatio-temporal modelling problems such as those in climate science\nand epidemiology. However, existing GP approximations do not simultaneously\nsupport large numbers of off-the-grid spatial data-points and long time-series\nwhich is a hallmark of many applications.\n\nPseudo-point approximations, one of the gold-standard methods for scaling GPs\nto large data sets, are well suited for handling off-the-grid spatial data.\nHowever, they cannot handle long temporal observation horizons effectively\nreverting to cubic computational scaling in the time dimension. State space GP\napproximations are well suited to handling temporal data, if the temporal GP\nprior admits a Markov form, leading to linear complexity in the number of\ntemporal observations, but have a cubic spatial cost and cannot handle\noff-the-grid spatial data.\n\nIn this work we show that there is a simple and elegant way to combine\npseudo-point methods with the state space GP approximation framework to get the\nbest of both worlds. The approach hinges on a surprising conditional\nindependence property which applies to space--time separable GPs. We\ndemonstrate empirically that the combined approach is more scalable and\napplicable to a greater range of spatio-temporal problems than either method on\nits own.",
          "link": "http://arxiv.org/abs/2106.10210",
          "publishedOn": "2021-06-21T02:07:39.970Z",
          "wordCount": 638,
          "title": "Combining Pseudo-Point and State Space Approximations for Sum-Separable Gaussian Processes. (arXiv:2106.10210v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mak_C/0/1/0/all/0/1\">Carol Mak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiser_F/0/1/0/all/0/1\">Fabian Zaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_L/0/1/0/all/0/1\">Luke Ong</a>",
          "description": "Probabilistic programming uses programs to express generative models whose\nposterior probability is then computed by built-in inference engines. A\nchallenging goal is to develop general purpose inference algorithms that work\nout-of-the-box for arbitrary programs in a universal probabilistic programming\nlanguage (PPL). The densities defined by such programs, which may use\nstochastic branching and recursion, are (in general) nonparametric, in the\nsense that they correspond to models on an infinite-dimensional parameter\nspace. However standard inference algorithms, such as the Hamiltonian Monte\nCarlo (HMC) algorithm, target distributions with a fixed number of parameters.\nThis paper introduces the Nonparametric Hamiltonian Monte Carlo (NP-HMC)\nalgorithm which generalises HMC to nonparametric models. Inputs to NP-HMC are a\nnew class of measurable functions called \"tree representable\", which serve as a\nlanguage-independent representation of the density functions of probabilistic\nprograms in a universal PPL. We provide a correctness proof of NP-HMC, and\nempirically demonstrate significant performance improvements over existing\napproaches on several nonparametric examples.",
          "link": "http://arxiv.org/abs/2106.10238",
          "publishedOn": "2021-06-21T02:07:39.963Z",
          "wordCount": 608,
          "title": "Nonparametric Hamiltonian Monte Carlo. (arXiv:2106.10238v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.02682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biau_G/0/1/0/all/0/1\">G&#xe9;rard Biau</a> (LPSM (UMR\\_8001)), <a href=\"http://arxiv.org/find/cs/1/au:+Sangnier_M/0/1/0/all/0/1\">Maxime Sangnier</a> (LPSM (UMR\\_8001)), <a href=\"http://arxiv.org/find/cs/1/au:+Tanielian_U/0/1/0/all/0/1\">Ugo Tanielian</a> (LPSM (UMR\\_8001))",
          "description": "Generative Adversarial Networks (GANs) have been successful in producing\noutstanding results in areas as diverse as image, video, and text generation.\nBuilding on these successes, a large number of empirical studies have validated\nthe benefits of the cousin approach called Wasserstein GANs (WGANs), which\nbrings stabilization in the training process. In the present paper, we add a\nnew stone to the edifice by proposing some theoretical advances in the\nproperties of WGANs. First, we properly define the architecture of WGANs in the\ncontext of integral probability metrics parameterized by neural networks and\nhighlight some of their basic mathematical features. We stress in particular\ninteresting optimization properties arising from the use of a parametric\n1-Lipschitz discriminator. Then, in a statistically-driven approach, we study\nthe convergence of empirical WGANs as the sample size tends to infinity, and\nclarify the adversarial effects of the generator and the discriminator by\nunderlining some trade-off properties. These features are finally illustrated\nwith experiments using both synthetic and real-world datasets.",
          "link": "http://arxiv.org/abs/2006.02682",
          "publishedOn": "2021-06-21T02:07:39.956Z",
          "wordCount": 639,
          "title": "Some Theoretical Insights into Wasserstein GANs. (arXiv:2006.02682v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.05006",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Miyazawa_S/0/1/0/all/0/1\">Sanzo Miyazawa</a>",
          "description": "The inverse Potts problem to infer a Boltzmann distribution for homologous\nprotein sequences from their single-site and pairwise amino acid frequencies\nrecently attracts a great deal of attention in the studies of protein structure\nand evolution. We study regularization and learning methods and how to tune\nregularization parameters to correctly infer interactions in Boltzmann machine\nlearning. Using $L_2$ regularization for fields, group $L_1$ for couplings is\nshown to be very effective for sparse couplings in comparison with $L_2$ and\n$L_1$. Two regularization parameters are tuned to yield equal values for both\nthe sample and ensemble averages of evolutionary energy. Both averages smoothly\nchange and converge, but their learning profiles are very different between\nlearning methods. The Adam method is modified to make stepsize proportional to\nthe gradient for sparse couplings and to use a soft-thresholding function for\ngroup $L_1$. It is shown by first inferring interactions from protein sequences\nand then from Monte Carlo samples that the fields and couplings can be well\nrecovered, but that recovering the pairwise correlations in the resolution of a\ntotal energy is harder for the natural proteins than for the protein-like\nsequences. Selective temperature for folding/structural constrains in protein\nevolution is also estimated.",
          "link": "http://arxiv.org/abs/1909.05006",
          "publishedOn": "2021-06-21T02:07:39.936Z",
          "wordCount": 754,
          "title": "Boltzmann machine learning and regularization methods for inferring evolutionary fields and couplings from a multiple sequence alignment. (arXiv:1909.05006v3 [q-bio.PE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10259",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tomanek_K/0/1/0/all/0/1\">Katrin Tomanek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cattiau_J/0/1/0/all/0/1\">Julie Cattiau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandorkar_A/0/1/0/all/0/1\">Angad Chandorkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>",
          "description": "While current state-of-the-art Automatic Speech Recognition (ASR) systems\nachieve high accuracy on typical speech, they suffer from significant\nperformance degradation on disordered speech and other atypical speech\npatterns. Personalization of ASR models, a commonly applied solution to this\nproblem, is usually performed in a server-based training environment posing\nproblems around data privacy, delayed model-update times, and communication\ncost for copying data and models between mobile device and server\ninfrastructure. In this paper, we present an approach to on-device based ASR\npersonalization with very small amounts of speaker-specific data. We test our\napproach on a diverse set of 100 speakers with disordered speech and find\nmedian relative word error rate improvement of 71% with only 50 short\nutterances required per speaker. When tested on a voice-controlled home\nautomation platform, on-device personalized models show a median task success\nrate of 81%, compared to only 40% of the unadapted models.",
          "link": "http://arxiv.org/abs/2106.10259",
          "publishedOn": "2021-06-21T02:07:39.929Z",
          "wordCount": 604,
          "title": "On-Device Personalization of Automatic Speech Recognition Models for Disordered Speech. (arXiv:2106.10259v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/1910.01210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prabhudesai_M/0/1/0/all/0/1\">Mihir Prabhudesai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_H/0/1/0/all/0/1\">Hsiao-Yu Fish Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1\">Syed Ashar Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sieb_M/0/1/0/all/0/1\">Maximilian Sieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1\">Adam W. Harley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>",
          "description": "We propose associating language utterances to 3D visual abstractions of the\nscene they describe. The 3D visual abstractions are encoded as 3-dimensional\nvisual feature maps. We infer these 3D visual scene feature maps from RGB\nimages of the scene via view prediction: when the generated 3D scene feature\nmap is neurally projected from a camera viewpoint, it should match the\ncorresponding RGB image. We present generative models that condition on the\ndependency tree of an utterance and generate a corresponding visual 3D feature\nmap as well as reason about its plausibility, and detector models that\ncondition on both the dependency tree of an utterance and a related image and\nlocalize the object referents in the 3D feature map inferred from the image.\nOur model outperforms models of language and vision that associate language\nwith 2D CNN activations or 2D images by a large margin in a variety of tasks,\nsuch as, classifying plausibility of utterances, detecting referential\nexpressions, and supplying rewards for trajectory optimization of object\nplacement policies from language instructions. We perform numerous ablations\nand show the improved performance of our detectors is due to its better\ngeneralization across camera viewpoints and lack of object interferences in the\ninferred 3D feature space, and the improved performance of our generators is\ndue to their ability to spatially reason about objects and their configurations\nin 3D when mapping from language to scenes.",
          "link": "http://arxiv.org/abs/1910.01210",
          "publishedOn": "2021-06-21T02:07:39.922Z",
          "wordCount": 735,
          "title": "Embodied Language Grounding with 3D Visual Feature Representations. (arXiv:1910.01210v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1\">Xinjie Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barner_K/0/1/0/all/0/1\">Kenneth Barner</a>",
          "description": "Recently, Mutual Information (MI) has attracted attention in bounding the\ngeneralization error of Deep Neural Networks (DNNs). However, it is intractable\nto accurately estimate the MI in DNNs, thus most previous works have to relax\nthe MI bound, which in turn weakens the information theoretic explanation for\ngeneralization. To address the limitation, this paper introduces a\nprobabilistic representation of DNNs for accurately estimating the MI.\nLeveraging the proposed MI estimator, we validate the information theoretic\nexplanation for generalization, and derive a tighter generalization bound than\nthe state-of-the-art relaxations.",
          "link": "http://arxiv.org/abs/2106.10262",
          "publishedOn": "2021-06-21T02:07:39.915Z",
          "wordCount": 540,
          "title": "A Probabilistic Representation of DNNs: Bridging Mutual Information and Generalization. (arXiv:2106.10262v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1908.03840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajapaksha_D/0/1/0/all/0/1\">Dilini Rajapaksha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergmeir_C/0/1/0/all/0/1\">Christoph Bergmeir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>",
          "description": "As we rely more and more on machine learning models for real-life\ndecision-making, being able to understand and trust the predictions becomes\never more important. Local explainer models have recently been introduced to\nexplain the predictions of complex machine learning models at the instance\nlevel. In this paper, we propose Local Rule-based Model Interpretability with\nk-optimal Associations (LoRMIkA), a novel model-agnostic approach that obtains\nk-optimal association rules from a neighbourhood of the instance to be\nexplained. Compared with other rule-based approaches in the literature, we\nargue that the most predictive rules are not necessarily the rules that provide\nthe best explanations. Consequently, the LoRMIkA framework provides a flexible\nway to obtain predictive and interesting rules. It uses an efficient search\nalgorithm guaranteed to find the k-optimal rules with respect to objectives\nsuch as confidence, lift, leverage, coverage, and support. It also provides\nmultiple rules which explain the decision and counterfactual rules, which give\nindications for potential changes to obtain different outputs for given\ninstances. We compare our approach to other state-of-the-art approaches in\nlocal model interpretability on three different datasets and achieve\ncompetitive results in terms of local accuracy and interpretability.",
          "link": "http://arxiv.org/abs/1908.03840",
          "publishedOn": "2021-06-21T02:07:39.909Z",
          "wordCount": 668,
          "title": "LoRMIkA: Local rule-based model interpretability with k-optimal associations. (arXiv:1908.03840v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beck_F/0/1/0/all/0/1\">Florian Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1\">Johannes F&#xfc;rnkranz</a>",
          "description": "Inductive rule learning is arguably among the most traditional paradigms in\nmachine learning. Although we have seen considerable progress over the years in\nlearning rule-based theories, all state-of-the-art learners still learn\ndescriptions that directly relate the input features to the target concept. In\nthe simplest case, concept learning, this is a disjunctive normal form (DNF)\ndescription of the positive class. While it is clear that this is sufficient\nfrom a logical point of view because every logical expression can be reduced to\nan equivalent DNF expression, it could nevertheless be the case that more\nstructured representations, which form deep theories by forming intermediate\nconcepts, could be easier to learn, in very much the same way as deep neural\nnetworks are able to outperform shallow networks, even though the latter are\nalso universal function approximators. In this paper, we empirically compare\ndeep and shallow rule learning with a uniform general algorithm, which relies\non greedy mini-batch based optimization. Our experiments on both artificial and\nreal-world benchmark data indicate that deep rule networks outperform shallow\nnetworks.",
          "link": "http://arxiv.org/abs/2106.10254",
          "publishedOn": "2021-06-21T02:07:39.890Z",
          "wordCount": 605,
          "title": "An Empirical Investigation into Deep and Shallow Rule Learning. (arXiv:2106.10254v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Samuel Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amos_B/0/1/0/all/0/1\">Brandon Amos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1\">Yaron Lipman</a>",
          "description": "Modeling distributions on Riemannian manifolds is a crucial component in\nunderstanding non-Euclidean data that arises, e.g., in physics and geology. The\nbudding approaches in this space are limited by representational and\ncomputational tradeoffs. We propose and study a class of flows that uses convex\npotentials from Riemannian optimal transport. These are universal and can model\ndistributions on any compact Riemannian manifold without requiring domain\nknowledge of the manifold to be integrated into the architecture. We\ndemonstrate that these flows can model standard distributions on spheres, and\ntori, on synthetic and geological data. Our source code is freely available\nonline at this http URL",
          "link": "http://arxiv.org/abs/2106.10272",
          "publishedOn": "2021-06-21T02:07:39.884Z",
          "wordCount": 530,
          "title": "Riemannian Convex Potential Maps. (arXiv:2106.10272v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10234",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhu_J/0/1/0/all/0/1\">Jinhua Zhu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xia_Y/0/1/0/all/0/1\">Yingce Xia</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Inspired by its success in natural language processing and computer vision,\npre-training has attracted substantial attention in cheminformatics and\nbioinformatics, especially for molecule based tasks. A molecule can be\nrepresented by either a graph (where atoms are connected by bonds) or a SMILES\nsequence (where depth-first-search is applied to the molecular graph with\nspecific rules). Existing works on molecule pre-training use either graph\nrepresentations only or SMILES representations only. In this work, we propose\nto leverage both the representations and design a new pre-training algorithm,\ndual-view molecule pre-training (briefly, DMP), that can effectively combine\nthe strengths of both types of molecule representations. The model of DMP\nconsists of two branches: a Transformer branch that takes the SMILES sequence\nof a molecule as input, and a GNN branch that takes a molecular graph as input.\nThe training of DMP contains three tasks: (1) predicting masked tokens in a\nSMILES sequence by the Transformer branch, (2) predicting masked atoms in a\nmolecular graph by the GNN branch, and (3) maximizing the consistency between\nthe two high-level representations output by the Transformer and GNN branches\nseparately. After pre-training, we can use either the Transformer branch (this\none is recommended according to empirical results), the GNN branch, or both for\ndownstream tasks. DMP is tested on nine molecular property prediction tasks and\nachieves state-of-the-art performances on seven of them. Furthermore, we test\nDMP on three retrosynthesis tasks and achieve state-of-the-result on the\nUSPTO-full dataset. Our code will be released soon.",
          "link": "http://arxiv.org/abs/2106.10234",
          "publishedOn": "2021-06-21T02:07:39.878Z",
          "wordCount": 678,
          "title": "Dual-view Molecule Pre-training. (arXiv:2106.10234v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10188",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Neklyudov_K/0/1/0/all/0/1\">Kirill Neklyudov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bondesan_R/0/1/0/all/0/1\">Roberto Bondesan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Welling_M/0/1/0/all/0/1\">Max Welling</a>",
          "description": "Deterministic dynamics is an essential part of many MCMC algorithms, e.g.\nHybrid Monte Carlo or samplers utilizing normalizing flows. This paper presents\na general construction of deterministic measure-preserving dynamics using\nautonomous ODEs and tools from differential geometry. We show how Hybrid Monte\nCarlo and other deterministic samplers follow as special cases of our theory.\nWe then demonstrate the utility of our approach by constructing a continuous\nnon-sequential version of Gibbs sampling in terms of an ODE flow and extending\nit to discrete state spaces. We find that our deterministic samplers are more\nsample efficient than stochastic counterparts, even if the latter generate\nindependent samples.",
          "link": "http://arxiv.org/abs/2106.10188",
          "publishedOn": "2021-06-21T02:07:39.870Z",
          "wordCount": 532,
          "title": "Deterministic Gibbs Sampling via Ordinary Differential Equations. (arXiv:2106.10188v1 [stat.CO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diskin_M/0/1/0/all/0/1\">Michael Diskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukhtiyarov_A/0/1/0/all/0/1\">Alexey Bukhtiyarov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryabinin_M/0/1/0/all/0/1\">Max Ryabinin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saulnier_L/0/1/0/all/0/1\">Lucile Saulnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lhoest_Q/0/1/0/all/0/1\">Quentin Lhoest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinitsin_A/0/1/0/all/0/1\">Anton Sinitsin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popov_D/0/1/0/all/0/1\">Dmitry Popov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyrkin_D/0/1/0/all/0/1\">Dmitry Pyrkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashirin_M/0/1/0/all/0/1\">Maxim Kashirin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borzunov_A/0/1/0/all/0/1\">Alexander Borzunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moral_A/0/1/0/all/0/1\">Albert Villanova del Moral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazur_D/0/1/0/all/0/1\">Denis Mazur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobelev_I/0/1/0/all/0/1\">Ilia Kobelev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1\">Thomas Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pekhimenko_G/0/1/0/all/0/1\">Gennady Pekhimenko</a>",
          "description": "Modern deep learning applications require increasingly more compute to train\nstate-of-the-art models. To address this demand, large corporations and\ninstitutions use dedicated High-Performance Computing clusters, whose\nconstruction and maintenance are both environmentally costly and well beyond\nthe budget of most organizations. As a result, some research directions become\nthe exclusive domain of a few large industrial and even fewer academic actors.\nTo alleviate this disparity, smaller groups may pool their computational\nresources and run collaborative experiments that benefit all participants. This\nparadigm, known as grid- or volunteer computing, has seen successful\napplications in numerous scientific areas. However, using this approach for\nmachine learning is difficult due to high latency, asymmetric bandwidth, and\nseveral challenges unique to volunteer computing. In this work, we carefully\nanalyze these constraints and propose a novel algorithmic framework designed\nspecifically for collaborative training. We demonstrate the effectiveness of\nour approach for SwAV and ALBERT pretraining in realistic conditions and\nachieve performance comparable to traditional setups at a fraction of the cost.\nFinally, we provide a detailed report of successful collaborative language\nmodel pretraining with 40 participants.",
          "link": "http://arxiv.org/abs/2106.10207",
          "publishedOn": "2021-06-21T02:07:39.863Z",
          "wordCount": 648,
          "title": "Distributed Deep Learning in Open Collaborations. (arXiv:2106.10207v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.05551",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rothfuss_J/0/1/0/all/0/1\">Jonas Rothfuss</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Josifoski_M/0/1/0/all/0/1\">Martin Josifoski</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1\">Andreas Krause</a>",
          "description": "Meta-learning can successfully acquire useful inductive biases from data.\nYet, its generalization properties to unseen learning tasks are poorly\nunderstood. Particularly if the number of meta-training tasks is small, this\nraises concerns about overfitting. We provide a theoretical analysis using the\nPAC-Bayesian framework and derive novel generalization bounds for\nmeta-learning. Using these bounds, we develop a class of PAC-optimal\nmeta-learning algorithms with performance guarantees and a principled\nmeta-level regularization. Unlike previous PAC-Bayesian meta-learners, our\nmethod results in a standard stochastic optimization problem which can be\nsolved efficiently and scales well. When instantiating our PAC-optimal\nhyper-posterior (PACOH) with Gaussian processes and Bayesian Neural Networks as\nbase learners, the resulting methods yield state-of-the-art performance, both\nin terms of predictive accuracy and the quality of uncertainty estimates.\nThanks to their principled treatment of uncertainty, our meta-learners can also\nbe successfully employed for sequential decision problems.",
          "link": "http://arxiv.org/abs/2002.05551",
          "publishedOn": "2021-06-21T02:07:39.845Z",
          "wordCount": 624,
          "title": "PACOH: Bayes-Optimal Meta-Learning with PAC-Guarantees. (arXiv:2002.05551v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10229",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Karanasou_P/0/1/0/all/0/1\">Penny Karanasou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karlapati_S/0/1/0/all/0/1\">Sri Karlapati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moinet_A/0/1/0/all/0/1\">Alexis Moinet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joly_A/0/1/0/all/0/1\">Arnaud Joly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abbas_A/0/1/0/all/0/1\">Ammar Abbas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Slangen_S/0/1/0/all/0/1\">Simon Slangen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trueba_J/0/1/0/all/0/1\">Jaime Lorenzo Trueba</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Drugman_T/0/1/0/all/0/1\">Thomas Drugman</a>",
          "description": "Many factors influence speech yielding different renditions of a given\nsentence. Generative models, such as variational autoencoders (VAEs), capture\nthis variability and allow multiple renditions of the same sentence via\nsampling. The degree of prosodic variability depends heavily on the prior that\nis used when sampling. In this paper, we propose a novel method to compute an\ninformative prior for the VAE latent space of a neural text-to-speech (TTS)\nsystem. By doing so, we aim to sample with more prosodic variability, while\ngaining controllability over the latent space's structure.\n\nBy using as prior the posterior distribution of a secondary VAE, which we\ncondition on a speaker vector, we can sample from the primary VAE taking\nexplicitly the conditioning into account and resulting in samples from a\nspecific region of the latent space for each condition (i.e. speaker). A formal\npreference test demonstrates significant preference of the proposed approach\nover standard Conditional VAE. We also provide visualisations of the latent\nspace where well-separated condition-specific clusters appear, as well as\nablation studies to better understand the behaviour of the system.",
          "link": "http://arxiv.org/abs/2106.10229",
          "publishedOn": "2021-06-21T02:07:39.838Z",
          "wordCount": 648,
          "title": "A learned conditional prior for the VAE acoustic space of a TTS system. (arXiv:2106.10229v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vodrahalli_K/0/1/0/all/0/1\">Kailas Vodrahalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>",
          "description": "Transfer learning aims to leverage models pre-trained on source data to\nefficiently adapt to target setting, where only limited data are available for\nmodel fine-tuning. Recent works empirically demonstrate that adversarial\ntraining in the source data can improve the ability of models to transfer to\nnew domains. However, why this happens is not known. In this paper, we provide\na theoretical model to rigorously analyze how adversarial training helps\ntransfer learning. We show that adversarial training in the source data\ngenerates provably better representations, so fine-tuning on top of this\nrepresentation leads to a more accurate predictor of the target data. We\nfurther demonstrate both theoretically and empirically that semi-supervised\nlearning in the source data can also improve transfer learning by similarly\nimproving the representation. Moreover, performing adversarial training on top\nof semi-supervised learning can further improve transferability, suggesting\nthat the two approaches have complementary benefits on representations. We\nsupport our theories with experiments on popular data sets and deep learning\narchitectures.",
          "link": "http://arxiv.org/abs/2106.10189",
          "publishedOn": "2021-06-21T02:07:39.830Z",
          "wordCount": 593,
          "title": "Adversarial Training Helps Transfer Learning via Better Representations. (arXiv:2106.10189v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10236",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Deo_A/0/1/0/all/0/1\">Anand Deo</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Murthy_K/0/1/0/all/0/1\">Karthyek Murthy</a>",
          "description": "This paper considers Importance Sampling (IS) for the estimation of tail\nrisks of a loss defined in terms of a sophisticated object such as a machine\nlearning feature map or a mixed integer linear optimisation formulation.\nAssuming only black-box access to the loss and the distribution of the\nunderlying random vector, the paper presents an efficient IS algorithm for\nestimating the Value at Risk and Conditional Value at Risk. The key challenge\nin any IS procedure, namely, identifying an appropriate change-of-measure, is\nautomated with a self-structuring IS transformation that learns and replicates\nthe concentration properties of the conditional excess from less rare samples.\nThe resulting estimators enjoy asymptotically optimal variance reduction when\nviewed in the logarithmic scale. Simulation experiments highlight the efficacy\nand practicality of the proposed scheme",
          "link": "http://arxiv.org/abs/2106.10236",
          "publishedOn": "2021-06-21T02:07:39.822Z",
          "wordCount": 572,
          "title": "Efficient Black-Box Importance Sampling for VaR and CVaR Estimation. (arXiv:2106.10236v1 [q-fin.RM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zaken_E/0/1/0/all/0/1\">Elad Ben Zaken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>",
          "description": "We show that with small-to-medium training data, fine-tuning only the bias\nterms (or a subset of the bias terms) of pre-trained BERT models is competitive\nwith (and sometimes better than) fine-tuning the entire model. For larger data,\nbias-only fine-tuning is competitive with other sparse fine-tuning methods.\nBesides their practical utility, these findings are relevant for the question\nof understanding the commonly-used process of finetuning: they support the\nhypothesis that finetuning is mainly about exposing knowledge induced by\nlanguage-modeling training, rather than learning new task-specific linguistic\nknowledge.",
          "link": "http://arxiv.org/abs/2106.10199",
          "publishedOn": "2021-06-21T02:07:39.815Z",
          "wordCount": 520,
          "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. (arXiv:2106.10199v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashidinejad_P/0/1/0/all/0/1\">Paria Rashidinejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jiantao Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1\">Stuart Russell</a>",
          "description": "In online reinforcement learning (RL), efficient exploration remains\nparticularly challenging in high-dimensional environments with sparse rewards.\nIn low-dimensional environments, where tabular parameterization is possible,\ncount-based upper confidence bound (UCB) exploration methods achieve minimax\nnear-optimal rates. However, it remains unclear how to efficiently implement\nUCB in realistic RL tasks that involve non-linear function approximation. To\naddress this, we propose a new exploration approach via \\textit{maximizing} the\ndeviation of the occupancy of the next policy from the explored regions. We add\nthis term as an adaptive regularizer to the standard RL objective to balance\nexploration vs. exploitation. We pair the new objective with a provably\nconvergent algorithm, giving rise to a new intrinsic reward that adjusts\nexisting bonuses. The proposed intrinsic reward is easy to implement and\ncombine with other existing RL algorithms to conduct exploration. As a proof of\nconcept, we evaluate the new intrinsic reward on tabular examples across a\nvariety of model-based and model-free algorithms, showing improvements over\ncount-only exploration strategies. When tested on navigation and locomotion\ntasks from MiniGrid and DeepMind Control Suite benchmarks, our approach\nsignificantly improves sample efficiency over state-of-the-art methods. Our\ncode is available at https://github.com/tianjunz/MADE.",
          "link": "http://arxiv.org/abs/2106.10268",
          "publishedOn": "2021-06-21T02:07:39.809Z",
          "wordCount": 641,
          "title": "MADE: Exploration via Maximizing Deviation from Explored Regions. (arXiv:2106.10268v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10169",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruirui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1\">Chelsea J.-T. Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1\">Hongda Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elibol_O/0/1/0/all/0/1\">Oguz Elibol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>",
          "description": "By implicitly recognizing a user based on his/her speech input, speaker\nidentification enables many downstream applications, such as personalized\nsystem behavior and expedited shopping checkouts. Based on whether the speech\ncontent is constrained or not, both text-dependent (TD) and text-independent\n(TI) speaker recognition models may be used. We wish to combine the advantages\nof both types of models through an ensemble system to make more reliable\npredictions. However, any such combined approach has to be robust to incomplete\ninputs, i.e., when either TD or TI input is missing. As a solution we propose a\nfusion of embeddings network foenet architecture, combining joint learning with\nneural attention. We compare foenet with four competitive baseline methods on a\ndataset of voice assistant inputs, and show that it achieves higher accuracy\nthan the baseline and score fusion methods, especially in the presence of\nincomplete inputs.",
          "link": "http://arxiv.org/abs/2106.10169",
          "publishedOn": "2021-06-21T02:07:39.790Z",
          "wordCount": 603,
          "title": "Fusion of Embeddings Networks for Robust Combination of Text Dependent and Independent Speaker Recognition. (arXiv:2106.10169v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shucheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fengyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Runchuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Sheng Zhong</a>",
          "description": "In recent years, phishing scams have become the crime type with the largest\nmoney involved on Ethereum, the second-largest blockchain platform. Meanwhile,\ngraph neural network (GNN) has shown promising performance in various node\nclassification tasks. However, for Ethereum transaction data, which could be\nnaturally abstracted to a real-world complex graph, the scarcity of labels and\nthe huge volume of transaction data make it difficult to take advantage of GNN\nmethods. Here in this paper, to address the two challenges, we propose a\nSelf-supervised Incremental deep Graph learning model (SIEGE), for the phishing\nscam detection problem on Ethereum. In our model, two pretext tasks designed\nfrom spatial and temporal perspectives help us effectively learn useful node\nembedding from the huge amount of unlabelled transaction data. And the\nincremental paradigm allows us to efficiently handle large-scale transaction\ndata and help the model maintain good performance when the data distribution is\ndrastically changing. We collect transaction records about half a year from\nEthereum and our extensive experiments show that our model consistently\noutperforms strong baselines in both transductive and inductive settings.",
          "link": "http://arxiv.org/abs/2106.10176",
          "publishedOn": "2021-06-21T02:07:39.783Z",
          "wordCount": 616,
          "title": "Self-supervised Incremental Deep Graph Learning for Ethereum Phishing Scam Detection. (arXiv:2106.10176v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozfatura_E/0/1/0/all/0/1\">Emre Ozfatura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hameed_M/0/1/0/all/0/1\">Muhammad Zaid Hameed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozfatura_K/0/1/0/all/0/1\">Kerem Ozfatura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>",
          "description": "A common observation regarding adversarial attacks is that they mostly give\nrise to false activation at the penultimate layer to fool the classifier.\nAssuming that these activation values correspond to certain features of the\ninput, the objective becomes choosing the features that are most useful for\nclassification. Hence, we propose a novel approach to identify the important\nfeatures by employing counter-adversarial attacks, which highlights the\nconsistency at the penultimate layer with respect to perturbations on input\nsamples. First, we empirically show that there exist a subset of features,\nclassification based in which bridge the gap between the clean and robust\naccuracy. Second, we propose a simple yet efficient mechanism to identify those\nfeatures by searching the neighborhood of input sample. We then select features\nby observing the consistency of the activation values at the penultimate layer.",
          "link": "http://arxiv.org/abs/2106.10252",
          "publishedOn": "2021-06-21T02:07:39.776Z",
          "wordCount": 585,
          "title": "Less is More: Feature Selection for Adversarial Robustness with Compressive Counter-Adversarial Attacks. (arXiv:2106.10252v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Minghai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zejiang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Kuang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>",
          "description": "Deep neural networks (DNNs) are effective in solving many real-world\nproblems. Larger DNN models usually exhibit better quality (e.g., accuracy) but\ntheir excessive computation results in long training and inference time. Model\nsparsification can reduce the computation and memory cost while maintaining\nmodel quality. Most existing sparsification algorithms unidirectionally remove\nweights, while others randomly or greedily explore a small subset of weights in\neach layer. The inefficiency of the algorithms reduces the achievable sparsity\nlevel. In addition, many algorithms still require pre-trained dense models and\nthus suffer from large memory footprint and long training time. In this paper,\nwe propose a novel scheduled grow-and-prune (GaP) methodology without\npre-training the dense models. It addresses the shortcomings of the previous\nworks by repeatedly growing a subset of layers to dense and then pruning back\nto sparse after some training. Experiments have shown that such models can\nmatch or beat the quality of highly optimized dense models at 80% sparsity on a\nvariety of tasks, such as image classification, objective detection, 3D object\npart segmentation, and translation. They also outperform other state-of-the-art\n(SOTA) pruning methods, including pruning from pre-trained dense models. As an\nexample, a 90% sparse ResNet-50 obtained via GaP achieves 77.9% top-1 accuracy\non ImageNet, improving the SOTA results by 1.5%.",
          "link": "http://arxiv.org/abs/2106.09857",
          "publishedOn": "2021-06-21T02:07:39.768Z",
          "wordCount": 669,
          "title": "Effective Model Sparsification by Scheduled Grow-and-Prune Methods. (arXiv:2106.09857v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Junyuan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haotao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiayu Zhou</a>",
          "description": "Federated learning (FL) emerges as a popular distributed learning schema that\nlearns a model from a set of participating users without requiring raw data to\nbe shared. One major challenge of FL comes from heterogeneity in users, which\nmay have distributionally different (or non-iid) data and varying computation\nresources. Just like in centralized learning, FL users also desire model\nrobustness against malicious attackers at test time. Whereas adversarial\ntraining (AT) provides a sound solution for centralized learning, extending its\nusage for FL users has imposed significant challenges, as many users may have\nvery limited training data as well as tight computational budgets, to afford\nthe data-hungry and costly AT. In this paper, we study a novel learning setting\nthat propagates adversarial robustness from high-resource users that can afford\nAT, to those low-resource users that cannot afford it, during the FL process.\nWe show that existing FL techniques cannot effectively propagate adversarial\nrobustness among non-iid users, and propose a simple yet effective propagation\napproach that transfers robustness through carefully designed\nbatch-normalization statistics. We demonstrate the rationality and\neffectiveness of our method through extensive experiments. Especially, the\nproposed method is shown to grant FL remarkable robustness even when only a\nsmall portion of users afford AT during learning. Codes will be published upon\nacceptance.",
          "link": "http://arxiv.org/abs/2106.10196",
          "publishedOn": "2021-06-21T02:07:39.730Z",
          "wordCount": 656,
          "title": "Federated Robustness Propagation: Sharing Adversarial Robustness in Federated Learning. (arXiv:2106.10196v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beck_F/0/1/0/all/0/1\">Florian Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1\">Johannes F&#xfc;rnkranz</a>",
          "description": "We investigate whether it is possible to learn rule sets efficiently in a\nnetwork structure with a single hidden layer using iterative refinements over\nmini-batches of examples. A first rudimentary version shows an acceptable\nperformance on all but one dataset, even though it does not yet reach the\nperformance levels of Ripper.",
          "link": "http://arxiv.org/abs/2106.10202",
          "publishedOn": "2021-06-21T02:07:39.723Z",
          "wordCount": 491,
          "title": "An Investigation into Mini-Batch Rule Learning. (arXiv:2106.10202v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10157",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heidrich_B/0/1/0/all/0/1\">Benedikt Heidrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartschat_A/0/1/0/all/0/1\">Andreas Bartschat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turowski_M/0/1/0/all/0/1\">Marian Turowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_O/0/1/0/all/0/1\">Oliver Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phipps_K/0/1/0/all/0/1\">Kaleb Phipps</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meisenbacher_S/0/1/0/all/0/1\">Stefan Meisenbacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmieder_K/0/1/0/all/0/1\">Kai Schmieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludwig_N/0/1/0/all/0/1\">Nicole Ludwig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikut_R/0/1/0/all/0/1\">Ralf Mikut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagenmeyer_V/0/1/0/all/0/1\">Veit Hagenmeyer</a>",
          "description": "Time series data are fundamental for a variety of applications, ranging from\nfinancial markets to energy systems. Due to their importance, the number and\ncomplexity of tools and methods used for time series analysis is constantly\nincreasing. However, due to unclear APIs and a lack of documentation,\nresearchers struggle to integrate them into their research projects and\nreplicate results. Additionally, in time series analysis there exist many\nrepetitive tasks, which are often re-implemented for each project,\nunnecessarily costing time. To solve these problems we present\n\\texttt{pyWATTS}, an open-source Python-based package that is a non-sequential\nworkflow automation tool for the analysis of time series data. pyWATTS includes\nmodules with clearly defined interfaces to enable seamless integration of new\nor existing methods, subpipelining to easily reproduce repetitive tasks, load\nand save functionality to simply replicate results, and native support for key\nPython machine learning libraries such as scikit-learn, PyTorch, and Keras.",
          "link": "http://arxiv.org/abs/2106.10157",
          "publishedOn": "2021-06-21T02:07:39.716Z",
          "wordCount": 590,
          "title": "pyWATTS: Python Workflow Automation Tool for Time Series. (arXiv:2106.10157v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Suyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wonho Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jana_S/0/1/0/all/0/1\">Suman Jana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1\">Meeyoung Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Sooel Son</a>",
          "description": "Trigger set-based watermarking schemes have gained emerging attention as they\nprovide a means to prove ownership for deep neural network model owners. In\nthis paper, we argue that state-of-the-art trigger set-based watermarking\nalgorithms do not achieve their designed goal of proving ownership. We posit\nthat this impaired capability stems from two common experimental flaws that the\nexisting research practice has committed when evaluating the robustness of\nwatermarking algorithms: (1) incomplete adversarial evaluation and (2)\noverlooked adaptive attacks.\n\nWe conduct a comprehensive adversarial evaluation of 10 representative\nwatermarking schemes against six of the existing attacks and demonstrate that\neach of these watermarking schemes lacks robustness against at least two\nattacks. We also propose novel adaptive attacks that harness the adversary's\nknowledge of the underlying watermarking algorithm of a target model. We\ndemonstrate that the proposed attacks effectively break all of the 10\nwatermarking schemes, consequently allowing adversaries to obscure the\nownership of any watermarked model. We encourage follow-up studies to consider\nour guidelines when evaluating the robustness of their watermarking schemes via\nconducting comprehensive adversarial evaluation that include our adaptive\nattacks to demonstrate a meaningful upper bound of watermark robustness.",
          "link": "http://arxiv.org/abs/2106.10147",
          "publishedOn": "2021-06-21T02:07:39.710Z",
          "wordCount": 635,
          "title": "Evaluating the Robustness of Trigger Set-Based Watermarks Embedded in Deep Neural Networks. (arXiv:2106.10147v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Konyushkova_K/0/1/0/all/0/1\">Ksenia Konyushkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yutian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paine_T/0/1/0/all/0/1\">Thomas Paine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulcehre_C/0/1/0/all/0/1\">Caglar Gulcehre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paduraru_C/0/1/0/all/0/1\">Cosmin Paduraru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mankowitz_D/0/1/0/all/0/1\">Daniel J Mankowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denil_M/0/1/0/all/0/1\">Misha Denil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_N/0/1/0/all/0/1\">Nando de Freitas</a>",
          "description": "This paper addresses the problem of policy selection in domains with abundant\nlogged data, but with a very restricted interaction budget. Solving this\nproblem would enable safe evaluation and deployment of offline reinforcement\nlearning policies in industry, robotics, and healthcare domain among others.\nSeveral off-policy evaluation (OPE) techniques have been proposed to assess the\nvalue of policies using only logged data. However, there is still a big gap\nbetween the evaluation by OPE and the full online evaluation in the real\nenvironment. To reduce this gap, we introduce a novel \\emph{active offline\npolicy selection} problem formulation, which combined logged data and limited\nonline interactions to identify the best policy. We rely on the advances in OPE\nto warm start the evaluation. We build upon Bayesian optimization to\niteratively decide which policies to evaluate in order to utilize the limited\nenvironment interactions wisely. Many candidate policies could be proposed,\nthus, we focus on making our approach scalable and introduce a kernel function\nto model similarity between policies. We use several benchmark environments to\nshow that the proposed approach improves upon state-of-the-art OPE estimates\nand fully online policy evaluation with limited budget. Additionally, we show\nthat each component of the proposed method is important, it works well with\nvarious number and quality of OPE estimates and even with a large number of\ncandidate policies.",
          "link": "http://arxiv.org/abs/2106.10251",
          "publishedOn": "2021-06-21T02:07:39.660Z",
          "wordCount": 662,
          "title": "Active Offline Policy Selection. (arXiv:2106.10251v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09832",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gaggion_N/0/1/0/all/0/1\">Nicol&#xe1;s Gaggion</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mansilla_L/0/1/0/all/0/1\">Lucas Mansilla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milone_D/0/1/0/all/0/1\">Diego Milone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferrante_E/0/1/0/all/0/1\">Enzo Ferrante</a>",
          "description": "In this work we address the problem of landmark-based segmentation for\nanatomical structures. We propose HybridGNet, an encoder-decoder neural\narchitecture which combines standard convolutions for image feature encoding,\nwith graph convolutional neural networks to decode plausible representations of\nanatomical structures. We benchmark the proposed architecture considering other\nstandard landmark and pixel-based models for anatomical segmentation in chest\nx-ray images, and found that HybridGNet is more robust to image occlusions. We\nalso show that it can be used to construct landmark-based segmentations from\npixel level annotations. Our experimental results suggest that HybridGNet\nproduces accurate and anatomically plausible landmark-based segmentations, by\nnaturally incorporating shape constraints within the decoding process via\nspectral convolutions.",
          "link": "http://arxiv.org/abs/2106.09832",
          "publishedOn": "2021-06-21T02:07:39.646Z",
          "wordCount": 567,
          "title": "Hybrid graph convolutional neural networks for landmark-based anatomical segmentation. (arXiv:2106.09832v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10065",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kristiadi_A/0/1/0/all/0/1\">Agustinus Kristiadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1\">Matthias Hein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1\">Philipp Hennig</a>",
          "description": "Despite their compelling theoretical properties, Bayesian neural networks\n(BNNs) tend to perform worse than frequentist methods in classification-based\nuncertainty quantification (UQ) tasks such as out-of-distribution (OOD)\ndetection and dataset-shift robustness. In this work, based on empirical\nfindings in prior works, we hypothesize that this issue is due to the avoidance\nof Bayesian methods in the so-called \"OOD training\" -- a family of techniques\nfor incorporating OOD data during training process, which has since been an\nintegral part of state-of-the-art frequentist UQ methods. To validate this, we\ntreat OOD data as a first-class citizen in BNN training by exploring four\ndifferent ways of incorporating OOD data in Bayesian inference. We show in\nextensive experiments that OOD-trained BNNs are competitive to, if not better\nthan recent frequentist baselines. This work thus provides strong baselines for\nfuture work in both Bayesian and frequentist UQ.",
          "link": "http://arxiv.org/abs/2106.10065",
          "publishedOn": "2021-06-21T02:07:39.628Z",
          "wordCount": 574,
          "title": "Being a Bit Frequentist Improves Bayesian Neural Networks. (arXiv:2106.10065v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10166",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Cheshire_J/0/1/0/all/0/1\">James Cheshire</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Menard_P/0/1/0/all/0/1\">Pierre M&#xe9;nard</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carpentier_A/0/1/0/all/0/1\">Alexandra Carpentier</a>",
          "description": "We investigate the problem dependent regime in the stochastic Thresholding\nBandit problem (TBP) under several shape constraints. In the TBP, the objective\nof the learner is to output, at the end of a sequential game, the set of arms\nwhose means are above a given threshold. The vanilla, unstructured, case is\nalready well studied in the literature. Taking $K$ as the number of arms, we\nconsider the case where (i) the sequence of arm's means $(\\mu_k)_{k=1}^K$ is\nmonotonically increasing (MTBP) and (ii) the case where $(\\mu_k)_{k=1}^K$ is\nconcave (CTBP). We consider both cases in the problem dependent regime and\nstudy the probability of error - i.e. the probability to mis-classify at least\none arm. In the fixed budget setting, we provide upper and lower bounds for the\nprobability of error in both the concave and monotone settings, as well as\nassociated algorithms. In both settings the bounds match in the problem\ndependent regime up to universal constants in the exponential.",
          "link": "http://arxiv.org/abs/2106.10166",
          "publishedOn": "2021-06-21T02:07:39.620Z",
          "wordCount": 601,
          "title": "Problem Dependent View on Structured Thresholding Bandit Problems. (arXiv:2106.10166v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Watson_D/0/1/0/all/0/1\">David S. Watson</a>",
          "description": "Explaining the predictions of opaque machine learning algorithms is an\nimportant and challenging task, especially as complex models are increasingly\nused to assist in high-stakes decisions such as those arising in healthcare and\nfinance. Most popular tools for post-hoc explainable artificial intelligence\n(XAI) are either insensitive to context (e.g., feature attributions) or\ndifficult to summarize (e.g., counterfactuals). In this paper, I introduce\n\\emph{rational Shapley values}, a novel XAI method that synthesizes and extends\nthese seemingly incompatible approaches in a rigorous, flexible manner. I\nleverage tools from decision theory and causal modeling to formalize and\nimplement a pragmatic approach that resolves a number of known challenges in\nXAI. By pairing the distribution of random variables with the appropriate\nreference class for a given explanation task, I illustrate through theory and\nexperiments how user goals and knowledge can inform and constrain the solution\nset in an iterative fashion. The method compares favorably to state of the art\nXAI tools in a range of quantitative and qualitative comparisons.",
          "link": "http://arxiv.org/abs/2106.10191",
          "publishedOn": "2021-06-21T02:07:39.612Z",
          "wordCount": 590,
          "title": "Rational Shapley Values. (arXiv:2106.10191v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roberts_D/0/1/0/all/0/1\">Daniel A. Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaida_S/0/1/0/all/0/1\">Sho Yaida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanin_B/0/1/0/all/0/1\">Boris Hanin</a>",
          "description": "This book develops an effective theory approach to understanding deep neural\nnetworks of practical relevance. Beginning from a first-principles\ncomponent-level picture of networks, we explain how to determine an accurate\ndescription of the output of trained networks by solving layer-to-layer\niteration equations and nonlinear learning dynamics. A main result is that the\npredictions of networks are described by nearly-Gaussian distributions, with\nthe depth-to-width aspect ratio of the network controlling the deviations from\nthe infinite-width Gaussian description. We explain how these effectively-deep\nnetworks learn nontrivial representations from training and more broadly\nanalyze the mechanism of representation learning for nonlinear models. From a\nnearly-kernel-methods perspective, we find that the dependence of such models'\npredictions on the underlying learning algorithm can be expressed in a simple\nand universal way. To obtain these results, we develop the notion of\nrepresentation group flow (RG flow) to characterize the propagation of signals\nthrough the network. By tuning networks to criticality, we give a practical\nsolution to the exploding and vanishing gradient problem. We further explain\nhow RG flow leads to near-universal behavior and lets us categorize networks\nbuilt from different activation functions into universality classes.\nAltogether, we show that the depth-to-width ratio governs the effective model\ncomplexity of the ensemble of trained networks. By using information-theoretic\ntechniques, we estimate the optimal aspect ratio at which we expect the network\nto be practically most useful and show how residual connections can be used to\npush this scale to arbitrary depths. With these tools, we can learn in detail\nabout the inductive bias of architectures, hyperparameters, and optimizers.",
          "link": "http://arxiv.org/abs/2106.10165",
          "publishedOn": "2021-06-21T02:07:39.603Z",
          "wordCount": 716,
          "title": "The Principles of Deep Learning Theory. (arXiv:2106.10165v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chenjun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Ilbin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1\">Csaba Szepesvari</a>",
          "description": "We study the fundamental question of the sample complexity of learning a good\npolicy in finite Markov decision processes (MDPs) when the data available for\nlearning is obtained by following a logging policy that must be chosen without\nknowledge of the underlying MDP. Our main results show that the sample\ncomplexity, the minimum number of transitions necessary and sufficient to\nobtain a good policy, is an exponential function of the relevant quantities\nwhen the planning horizon $H$ is finite. In particular, we prove that the\nsample complexity of obtaining $\\epsilon$-optimal policies is at least\n$\\Omega(\\mathrm{A}^{\\min(\\mathrm{S}-1, H+1)})$ for $\\gamma$-discounted\nproblems, where $\\mathrm{S}$ is the number of states, $\\mathrm{A}$ is the\nnumber of actions, and $H$ is the effective horizon defined as $H=\\lfloor\n\\tfrac{\\ln(1/\\epsilon)}{\\ln(1/\\gamma)} \\rfloor$; and it is at least\n$\\Omega(\\mathrm{A}^{\\min(\\mathrm{S}-1, H)}/\\varepsilon^2)$ for finite horizon\nproblems, where $H$ is the planning horizon of the problem. This lower bound is\nessentially matched by an upper bound. For the average-reward setting we show\nthat there is no algorithm finding $\\epsilon$-optimal policies with a finite\namount of data.",
          "link": "http://arxiv.org/abs/2106.09973",
          "publishedOn": "2021-06-21T02:07:39.596Z",
          "wordCount": 620,
          "title": "On the Sample Complexity of Batch Reinforcement Learning with Policy-Induced Data. (arXiv:2106.09973v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_I/0/1/0/all/0/1\">I&#xf1;igo Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viles_E/0/1/0/all/0/1\">Elisabeth Viles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabrejas_I/0/1/0/all/0/1\">I&#xf1;aki Cabrejas</a>",
          "description": "A failure detection system is the first step towards predictive maintenance\nstrategies. A popular data-driven method to detect incipient failures and\nanomalies is the training of normal behaviour models by applying a machine\nlearning technique like feed-forward neural networks (FFNN) or extreme learning\nmachines (ELM). However, the performance of any of these modelling techniques\ncan be deteriorated by the unexpected rise of non-stationarities in the dynamic\nenvironment in which industrial assets operate. This unpredictable statistical\nchange in the measured variable is known as concept drift. In this article a\nwind turbine maintenance case is presented, where non-stationarities of various\nkinds can happen unexpectedly. Such concept drift events are desired to be\ndetected by means of statistical detectors and window-based approaches.\nHowever, in real complex systems, concept drifts are not as clear and evident\nas in artificially generated datasets. In order to evaluate the effectiveness\nof current drift detectors and also to design an appropriate novel technique\nfor this specific industrial application, it is essential to dispose beforehand\nof a characterization of the existent drifts. Under the lack of information in\nthis regard, a methodology for labelling concept drift events in the lifetime\nof wind turbines is proposed. This methodology will facilitate the creation of\na drift database that will serve both as a training ground for concept drift\ndetectors and as a valuable information to enhance the knowledge about\nmaintenance of complex systems.",
          "link": "http://arxiv.org/abs/2106.09951",
          "publishedOn": "2021-06-21T02:07:39.589Z",
          "wordCount": 698,
          "title": "Labelling Drifts in a Fault Detection System for Wind Turbine Maintenance. (arXiv:2106.09951v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frigo_O/0/1/0/all/0/1\">Oriel Frigo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brossard_R/0/1/0/all/0/1\">R&#xe9;my Brossard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehaene_D/0/1/0/all/0/1\">David Dehaene</a>",
          "description": "We propose the Graph Context Encoder (GCE), a simple but efficient approach\nfor graph representation learning based on graph feature masking and\nreconstruction.\n\nGCE models are trained to efficiently reconstruct input graphs similarly to a\ngraph autoencoder where node and edge labels are masked. In particular, our\nmodel is also allowed to change graph structures by masking and reconstructing\ngraphs augmented by random pseudo-edges.\n\nWe show that GCE can be used for novel graph generation, with applications\nfor molecule generation. Used as a pretraining method, we also show that GCE\nimproves baseline performances in supervised classification tasks tested on\nmultiple standard benchmark graph datasets.",
          "link": "http://arxiv.org/abs/2106.10124",
          "publishedOn": "2021-06-21T02:07:39.575Z",
          "wordCount": 547,
          "title": "Graph Context Encoder: Graph Feature Inpainting for Graph Generation and Self-supervised Pretraining. (arXiv:2106.10124v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1\">Stefan Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janschek_M/0/1/0/all/0/1\">Michael Janschek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uelwer_T/0/1/0/all/0/1\">Tobias Uelwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harmeling_S/0/1/0/all/0/1\">Stefan Harmeling</a>",
          "description": "We propose a new approach to increase inference performance in environments\nthat require a specific sequence of actions in order to be solved. This is for\nexample the case for maze environments where ideally an optimal path is\ndetermined. Instead of learning a policy for a single step, we want to learn a\npolicy that can predict n actions in advance. Our proposed method called policy\nhorizon regression (PHR) uses knowledge of the environment sampled by A2C to\nlearn an n dimensional policy vector in a policy distillation setup which\nyields n sequential actions per observation. We test our method on the MiniGrid\nand Pong environments and show drastic speedup during inference time by\nsuccessfully predicting sequences of actions on a single observation.",
          "link": "http://arxiv.org/abs/2106.10075",
          "publishedOn": "2021-06-21T02:07:39.547Z",
          "wordCount": 576,
          "title": "Learning to Plan via a Multi-Step Policy Regression Method. (arXiv:2106.10075v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pintor_M/0/1/0/all/0/1\">Maura Pintor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demetrio_L/0/1/0/all/0/1\">Luca Demetrio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sotgiu_A/0/1/0/all/0/1\">Angelo Sotgiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manca_G/0/1/0/all/0/1\">Giovanni Manca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1\">Ambra Demontis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1\">Battista Biggio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1\">Fabio Roli</a>",
          "description": "Evaluating robustness of machine-learning models to adversarial examples is a\nchallenging problem. Many defenses have been shown to provide a false sense of\nsecurity by causing gradient-based attacks to fail, and they have been broken\nunder more rigorous evaluations. Although guidelines and best practices have\nbeen suggested to improve current adversarial robustness evaluations, the lack\nof automatic testing and debugging tools makes it difficult to apply these\nrecommendations in a systematic manner. In this work, we overcome these\nlimitations by (i) defining a set of quantitative indicators which unveil\ncommon failures in the optimization of gradient-based attacks, and (ii)\nproposing specific mitigation strategies within a systematic evaluation\nprotocol. Our extensive experimental analysis shows that the proposed\nindicators of failure can be used to visualize, debug and improve current\nadversarial robustness evaluations, providing a first concrete step towards\nautomatizing and systematizing current adversarial robustness evaluations. Our\nopen-source code is available at:\nhttps://github.com/pralab/IndicatorsOfAttackFailure.",
          "link": "http://arxiv.org/abs/2106.09947",
          "publishedOn": "2021-06-21T02:07:39.539Z",
          "wordCount": 609,
          "title": "Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples. (arXiv:2106.09947v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1\">Adi Shamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melamed_O/0/1/0/all/0/1\">Odelia Melamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+BenShmuel_O/0/1/0/all/0/1\">Oriel BenShmuel</a>",
          "description": "The extreme fragility of deep neural networks when presented with tiny\nperturbations in their inputs was independently discovered by several research\ngroups in 2013, but in spite of enormous effort these adversarial examples\nremained a baffling phenomenon with no clear explanation. In this paper we\nintroduce a new conceptual framework (which we call the Dimpled Manifold Model)\nwhich provides a simple explanation for why adversarial examples exist, why\ntheir perturbations have such tiny norms, why these perturbations look like\nrandom noise, and why a network which was adversarially trained with\nincorrectly labeled images can still correctly classify test images. In the\nlast part of the paper we describe the results of numerous experiments which\nstrongly support this new model, and in particular our assertion that\nadversarial perturbations are roughly perpendicular to the low dimensional\nmanifold which contains all the training examples.",
          "link": "http://arxiv.org/abs/2106.10151",
          "publishedOn": "2021-06-21T02:07:39.525Z",
          "wordCount": 583,
          "title": "The Dimpled Manifold Model of Adversarial Examples in Machine Learning. (arXiv:2106.10151v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wensheng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Ying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaoyong Yuan</a>",
          "description": "Vertical federated learning is a collaborative machine learning framework to\ntrain deep leaning models on vertically partitioned data with\nprivacy-preservation. It attracts much attention both from academia and\nindustry. Unfortunately, applying most existing vertical federated learning\nmethods in real-world applications still faces two daunting challenges. First,\nmost existing vertical federated learning methods have a strong assumption that\nat least one party holds the complete set of labels of all data samples, while\nthis assumption is not satisfied in many practical scenarios, where labels are\nhorizontally partitioned and the parties only hold partial labels. Existing\nvertical federated learning methods can only utilize partial labels, which may\nlead to inadequate model update in end-to-end backpropagation. Second,\ncomputational and communication resources vary in parties. Some parties with\nlimited computational and communication resources will become the stragglers\nand slow down the convergence of training. Such straggler problem will be\nexaggerated in the scenarios of horizontally partitioned labels in vertical\nfederated learning. To address these challenges, we propose a novel vertical\nfederated learning framework named Cascade Vertical Federated Learning (CVFL)\nto fully utilize all horizontally partitioned labels to train neural networks\nwith privacy-preservation. To mitigate the straggler problem, we design a novel\noptimization objective which can increase straggler's contribution to the\ntrained models. We conduct a series of qualitative experiments to rigorously\nverify the effectiveness of CVFL. It is demonstrated that CVFL can achieve\ncomparable performance (e.g., accuracy for classification tasks) with\ncentralized training. The new optimization objective can further mitigate the\nstraggler problem comparing with only using the asynchronous aggregation\nmechanism during training.",
          "link": "http://arxiv.org/abs/2106.10056",
          "publishedOn": "2021-06-21T02:07:39.510Z",
          "wordCount": 702,
          "title": "A Vertical Federated Learning Framework for Horizontally Partitioned Labels. (arXiv:2106.10056v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lowy_A/0/1/0/all/0/1\">Andrew Lowy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razaviyayn_M/0/1/0/all/0/1\">Meisam Razaviyayn</a>",
          "description": "Federated learning (FL) is a distributed learning paradigm in which many\nclients with heterogeneous, unbalanced, and often sensitive local data,\ncollaborate to learn a model. Local Differential Privacy (LDP) provides a\nstrong guarantee that each client's data cannot be leaked during and after\ntraining, without relying on a trusted third party. While LDP is often believed\nto be too stringent to allow for satisfactory utility, our paper challenges\nthis belief. We consider a general setup with unbalanced, heterogeneous data,\ndisparate privacy needs across clients, and unreliable communication, where a\nrandom number/subset of clients is available each round. We propose three LDP\nalgorithms for smooth (strongly) convex FL; each are noisy variations of\ndistributed minibatch SGD. One is accelerated and one involves novel\ntime-varying noise, which we use to obtain the first non-trivial LDP excess\nrisk bound for the fully general non-i.i.d. FL problem. Specializing to i.i.d.\nclients, our risk bounds interpolate between the best known and/or optimal\nbounds in the centralized setting and the cross-device setting, where each\nclient represents just one person's data. Furthermore, we show that in certain\nregimes, our convergence rate (nearly) matches the corresponding non-private\nlower bound or outperforms state of the art non-private algorithms (``privacy\nfor free''). Finally, we validate our theoretical results and illustrate the\npractical utility of our algorithm with numerical experiments.",
          "link": "http://arxiv.org/abs/2106.09779",
          "publishedOn": "2021-06-21T02:07:39.480Z",
          "wordCount": 666,
          "title": "Locally Differentially Private Federated Learning: Efficient Algorithms with Tight Risk Bounds. (arXiv:2106.09779v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jenner_E/0/1/0/all/0/1\">Erik Jenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiler_M/0/1/0/all/0/1\">Maurice Weiler</a>",
          "description": "Recent work in equivariant deep learning bears strong similarities to\nphysics. Fields over a base space are fundamental entities in both subjects, as\nare equivariant maps between these fields. In deep learning, however, these\nmaps are usually defined by convolutions with a kernel, whereas they are\npartial differential operators (PDOs) in physics. Developing the theory of\nequivariant PDOs in the context of deep learning could bring these subjects\neven closer together and lead to a stronger flow of ideas. In this work, we\nderive a $G$-steerability constraint that completely characterizes when a PDO\nbetween feature vector fields is equivariant, for arbitrary symmetry groups\n$G$. We then fully solve this constraint for several important groups. We use\nour solutions as equivariant drop-in replacements for convolutional layers and\nbenchmark them in that role. Finally, we develop a framework for equivariant\nmaps based on Schwartz distributions that unifies classical convolutions and\ndifferential operators and gives insight about the relation between the two.",
          "link": "http://arxiv.org/abs/2106.10163",
          "publishedOn": "2021-06-21T02:07:39.473Z",
          "wordCount": 603,
          "title": "Steerable Partial Differential Operators for Equivariant Neural Networks. (arXiv:2106.10163v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Piriyajitakonkij_M/0/1/0/all/0/1\">Maytus Piriyajitakonkij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itthipuripat_S/0/1/0/all/0/1\">Sirawaj Itthipuripat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1\">Theerawit Wilaiprasitporn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dilokthanakul_N/0/1/0/all/0/1\">Nat Dilokthanakul</a>",
          "description": "Supervised deep convolutional neural networks (DCNNs) are currently one of\nthe best computational models that can explain how the primate ventral visual\nstream solves object recognition. However, embodied cognition has not been\nconsidered in the existing visual processing models. From the ecological\nstandpoint, humans learn to recognize objects by interacting with them,\nallowing better classification, specialization, and generalization. Here, we\nask if computational models under the embodied learning framework can explain\nmechanisms underlying object recognition in the primate visual system better\nthan the existing supervised models? To address this question, we use\nreinforcement learning to train neural network models to play a 3D computer\ngame and we find that these reinforcement learning models achieve neural\nresponse prediction accuracy scores in the early visual areas (e.g., V1 and V2)\nin the levels that are comparable to those accomplished by the supervised\nneural network model. In contrast, the supervised neural network models yield\nbetter neural response predictions in the higher visual areas, compared to the\nreinforcement learning models. Our preliminary results suggest the future\ndirection of visual neuroscience in which deep reinforcement learning should be\nincluded to fill the missing embodiment concept.",
          "link": "http://arxiv.org/abs/2106.10112",
          "publishedOn": "2021-06-21T02:07:39.466Z",
          "wordCount": 635,
          "title": "Deep Reinforcement Learning Models Predict Visual Responses in the Brain: A Preliminary Result. (arXiv:2106.10112v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengrui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>",
          "description": "In recent years, multi-view subspace clustering has achieved impressive\nperformance due to the exploitation of complementary imformation across\nmultiple views. However, multi-view data can be very complicated and are not\neasy to cluster in real-world applications. Most existing methods operate on\nraw data and may not obtain the optimal solution. In this work, we propose a\nnovel multi-view clustering method named smoothed multi-view subspace\nclustering (SMVSC) by employing a novel technique, i.e., graph filtering, to\nobtain a smooth representation for each view, in which similar data points have\nsimilar feature values. Specifically, it retains the graph geometric features\nthrough applying a low-pass filter. Consequently, it produces a\n``clustering-friendly\" representation and greatly facilitates the downstream\nclustering task. Extensive experiments on benchmark datasets validate the\nsuperiority of our approach. Analysis shows that graph filtering increases the\nseparability of classes.",
          "link": "http://arxiv.org/abs/2106.09875",
          "publishedOn": "2021-06-21T02:07:39.460Z",
          "wordCount": 584,
          "title": "Smoothed Multi-View Subspace Clustering. (arXiv:2106.09875v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1\">Tijin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yufeng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuanqing Xia</a>",
          "description": "Multivariate time series prediction has attracted a lot of attention because\nof its wide applications such as intelligence transportation, AIOps. Generative\nmodels have achieved impressive results in time series modeling because they\ncan model data distribution and take noise into consideration. However, many\nexisting works can not be widely used because of the constraints of functional\nform of generative models or the sensitivity to hyperparameters. In this paper,\nwe propose ScoreGrad, a multivariate probabilistic time series forecasting\nframework based on continuous energy-based generative models. ScoreGrad is\ncomposed of time series feature extraction module and conditional stochastic\ndifferential equation based score matching module. The prediction can be\nachieved by iteratively solving reverse-time SDE. To the best of our knowledge,\nScoreGrad is the first continuous energy based generative model used for time\nseries forecasting. Furthermore, ScoreGrad achieves state-of-the-art results on\nsix real-world datasets. The impact of hyperparameters and sampler types on the\nperformance are also explored. Code is available at\nhttps://github.com/yantijin/ScoreGradPred.",
          "link": "http://arxiv.org/abs/2106.10121",
          "publishedOn": "2021-06-21T02:07:39.445Z",
          "wordCount": 609,
          "title": "ScoreGrad: Multivariate Probabilistic Time Series Forecasting with Continuous Energy-based Generative Models. (arXiv:2106.10121v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosato_A/0/1/0/all/0/1\">Antonello Rosato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panella_M/0/1/0/all/0/1\">Massimo Panella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osipov_E/0/1/0/all/0/1\">Evgeny Osipov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1\">Denis Kleyko</a>",
          "description": "A change of the prevalent supervised learning techniques is foreseeable in\nthe near future: from the complex, computational expensive algorithms to more\nflexible and elementary training ones. The strong revitalization of randomized\nalgorithms can be framed in this prospect steering. We recently proposed a\nmodel for distributed classification based on randomized neural networks and\nhyperdimensional computing, which takes into account cost of information\nexchange between agents using compression. The use of compression is important\nas it addresses the issues related to the communication bottleneck, however,\nthe original approach is rigid in the way the compression is used. Therefore,\nin this work, we propose a more flexible approach to compression and compare it\nto conventional compression algorithms, dimensionality reduction, and\nquantization techniques.",
          "link": "http://arxiv.org/abs/2106.09831",
          "publishedOn": "2021-06-21T02:07:39.332Z",
          "wordCount": 569,
          "title": "On Effects of Compression with Hyperdimensional Computing in Distributed Randomized Neural Networks. (arXiv:2106.09831v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12866",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wan_X/0/1/0/all/0/1\">Xiaoliang Wan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tang_K/0/1/0/all/0/1\">Kejun Tang</a>",
          "description": "In this work, we have proposed augmented KRnets including both discrete and\ncontinuous models. One difficulty in flow-based generative modeling is to\nmaintain the invertibility of the transport map, which is often a trade-off\nbetween effectiveness and robustness. The exact invertibility has been achieved\nin the real NVP using a specific pattern to exchange information between two\nseparated groups of dimensions. KRnet has been developed to enhance the\ninformation exchange among data dimensions by incorporating the\nKnothe-Rosenblatt rearrangement into the structure of the transport map. Due to\nthe maintenance of exact invertibility, a full nonlinear update of all data\ndimensions needs three iterations in KRnet. To alleviate this issue, we will\nadd augmented dimensions that act as a channel for communications among the\ndata dimensions. In the augmented KRnet, a fully nonlinear update is achieved\nin two iterations. We also show that the augmented KRnet can be reformulated as\nthe discretization of a neural ODE, where the exact invertibility is kept such\nthat the adjoint method can be formulated with respect to the discretized ODE\nto obtain the exact gradient. Numerical experiments have been implemented to\ndemonstrate the effectiveness of our models.",
          "link": "http://arxiv.org/abs/2105.12866",
          "publishedOn": "2021-06-21T02:07:39.325Z",
          "wordCount": 638,
          "title": "Augmented KRnet for density estimation and approximation. (arXiv:2105.12866v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06300",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Plassier_V/0/1/0/all/0/1\">Vincent Plassier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vono_M/0/1/0/all/0/1\">Maxime Vono</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1\">Alain Durmus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Moulines_E/0/1/0/all/0/1\">Eric Moulines</a>",
          "description": "Performing reliable Bayesian inference on a big data scale is becoming a\nkeystone in the modern era of machine learning. A workhorse class of methods to\nachieve this task are Markov chain Monte Carlo (MCMC) algorithms and their\ndesign to handle distributed datasets has been the subject of many works.\nHowever, existing methods are not completely either reliable or computationally\nefficient. In this paper, we propose to fill this gap in the case where the\ndataset is partitioned and stored on computing nodes within a cluster under a\nmaster/slaves architecture. We derive a user-friendly centralised distributed\nMCMC algorithm with provable scaling in high-dimensional settings. We\nillustrate the relevance of the proposed methodology on both synthetic and real\ndata experiments.",
          "link": "http://arxiv.org/abs/2106.06300",
          "publishedOn": "2021-06-21T02:07:39.319Z",
          "wordCount": 599,
          "title": "DG-LMC: A Turn-key and Scalable Synchronous Distributed MCMC Algorithm via Langevin Monte Carlo within Gibbs. (arXiv:2106.06300v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pawelczyk_M/0/1/0/all/0/1\">Martin Pawelczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Shalmali Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Sohini Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>",
          "description": "Counterfactual explanations and adversarial examples have emerged as critical\nresearch areas for addressing the explainability and robustness goals of\nmachine learning (ML). While counterfactual explanations were developed with\nthe goal of providing recourse to individuals adversely impacted by algorithmic\ndecisions, adversarial examples were designed to expose the vulnerabilities of\nML models. While prior research has hinted at the commonalities between these\nframeworks, there has been little to no work on systematically exploring the\nconnections between the literature on counterfactual explanations and\nadversarial examples. In this work, we make one of the first attempts at\nformalizing the connections between counterfactual explanations and adversarial\nexamples. More specifically, we theoretically analyze salient counterfactual\nexplanation and adversarial example generation methods, and highlight the\nconditions under which they behave similarly. Our analysis demonstrates that\nseveral popular counterfactual explanation and adversarial example generation\nmethods such as the ones proposed by Wachter et. al. and Carlini and Wagner\n(with mean squared error loss), and C-CHVAE and natural adversarial examples by\nZhao et. al. are equivalent. We also bound the distance between counterfactual\nexplanations and adversarial examples generated by Wachter et. al. and DeepFool\nmethods for linear models. Finally, we empirically validate our theoretical\nfindings using extensive experimentation with synthetic and real world\ndatasets.",
          "link": "http://arxiv.org/abs/2106.09992",
          "publishedOn": "2021-06-21T02:07:39.312Z",
          "wordCount": 643,
          "title": "On the Connections between Counterfactual Explanations and Adversarial Examples. (arXiv:2106.09992v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.08987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chatel_S/0/1/0/all/0/1\">Sylvain Chatel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyrgelis_A/0/1/0/all/0/1\">Apostolos Pyrgelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troncoso_Pastoriza_J/0/1/0/all/0/1\">Juan Ramon Troncoso-Pastoriza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubaux_J/0/1/0/all/0/1\">Jean-Pierre Hubaux</a>",
          "description": "Tree-based models are among the most efficient machine learning techniques\nfor data mining nowadays due to their accuracy, interpretability, and\nsimplicity. The recent orthogonal needs for more data and privacy protection\ncall for collaborative privacy-preserving solutions. In this work, we survey\nthe literature on distributed and privacy-preserving training of tree-based\nmodels and we systematize its knowledge based on four axes: the learning\nalgorithm, the collaborative model, the protection mechanism, and the threat\nmodel. We use this to identify the strengths and limitations of these works and\nprovide for the first time a framework analyzing the information leakage\noccurring in distributed tree-based model learning.",
          "link": "http://arxiv.org/abs/2103.08987",
          "publishedOn": "2021-06-21T02:07:39.305Z",
          "wordCount": 577,
          "title": "SoK: Privacy-Preserving Collaborative Tree-based Model Learning. (arXiv:2103.08987v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10064",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bellec_G/0/1/0/all/0/1\">Guillaume Bellec</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1\">Shuqi Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Modirshanechi_A/0/1/0/all/0/1\">Alireza Modirshanechi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Brea_J/0/1/0/all/0/1\">Johanni Brea</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gerstner_W/0/1/0/all/0/1\">Wulfram Gerstner</a>",
          "description": "Fitting network models to neural activity is becoming an important tool in\nneuroscience. A popular approach is to model a brain area with a probabilistic\nrecurrent spiking network whose parameters maximize the likelihood of the\nrecorded activity. Although this is widely used, we show that the resulting\nmodel does not produce realistic neural activity and wrongly estimates the\nconnectivity matrix when neurons that are not recorded have a substantial\nimpact on the recorded network. To correct for this, we suggest to augment the\nlog-likelihood with terms that measure the dissimilarity between simulated and\nrecorded activity. This dissimilarity is defined via summary statistics\ncommonly used in neuroscience, and the optimization is efficient because it\nrelies on back-propagation through the stochastically simulated spike trains.\nWe analyze this method theoretically and show empirically that it generates\nmore realistic activity statistics and recovers the connectivity matrix better\nthan other methods.",
          "link": "http://arxiv.org/abs/2106.10064",
          "publishedOn": "2021-06-21T02:07:39.287Z",
          "wordCount": 595,
          "title": "Fitting summary statistics of neural data with a differentiable spiking network simulator. (arXiv:2106.10064v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Runyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaolin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Na Li</a>",
          "description": "We study the performance of the gradient play algorithm for multi-agent\ntabular Markov decision processes (MDPs), which are also known as stochastic\ngames (SGs), where each agent tries to maximize its own total discounted reward\nby making decisions independently based on current state information which is\nshared between agents. Policies are directly parameterized by the probability\nof choosing a certain action at a given state. We show that Nash equilibria\n(NEs) and first order stationary policies are equivalent in this setting, and\ngive a non-asymptotic global convergence rate analysis to an $\\epsilon$-NE for\na subclass of multi-agent MDPs called Markov potential games, which includes\nthe cooperative setting with identical rewards among agents as an important\nspecial case. Our result shows that the number of iterations to reach an\n$\\epsilon$-NE scales linearly, instead of exponentially, with the number of\nagents. Local geometry and local stability are also considered. For Markov\npotential games, we prove that strict NEs are local maxima of the total\npotential function and fully-mixed NEs are saddle points. We also give a local\nconvergence rate around strict NEs for more general settings.",
          "link": "http://arxiv.org/abs/2106.00198",
          "publishedOn": "2021-06-21T02:07:39.281Z",
          "wordCount": 653,
          "title": "Gradient Play in Multi-Agent Markov Stochastic Games: Stationary Points and Convergence. (arXiv:2106.00198v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.05976",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Matsuda_T/0/1/0/all/0/1\">Takeru Matsuda</a>, <a href=\"http://arxiv.org/find/math/1/au:+Uehara_M/0/1/0/all/0/1\">Masatoshi Uehara</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hyvarinen_A/0/1/0/all/0/1\">Aapo Hyvarinen</a>",
          "description": "Many statistical models are given in the form of non-normalized densities\nwith an intractable normalization constant. Since maximum likelihood estimation\nis computationally intensive for these models, several estimation methods have\nbeen developed which do not require explicit computation of the normalization\nconstant, such as noise contrastive estimation (NCE) and score matching.\nHowever, model selection methods for general non-normalized models have not\nbeen proposed so far. In this study, we develop information criteria for\nnon-normalized models estimated by NCE or score matching. They are\napproximately unbiased estimators of discrepancy measures for non-normalized\nmodels. Simulation results and applications to real data demonstrate that the\nproposed criteria enable selection of the appropriate non-normalized model in a\ndata-driven manner.",
          "link": "http://arxiv.org/abs/1905.05976",
          "publishedOn": "2021-06-21T02:07:39.272Z",
          "wordCount": 583,
          "title": "Information criteria for non-normalized models. (arXiv:1905.05976v4 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Nanqing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maggioni_M/0/1/0/all/0/1\">Matteo Maggioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Pellitero_E/0/1/0/all/0/1\">Eduardo P&#xe9;rez-Pellitero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonagh_S/0/1/0/all/0/1\">Steven McDonagh</a>",
          "description": "The breakthrough of contrastive learning (CL) has fueled the recent success\nof self-supervised learning (SSL) in high-level vision tasks on RGB images.\nHowever, CL is still ill-defined for low-level vision tasks, such as joint\ndemosaicking and denoising (JDD), in the RAW domain. To bridge this\nmethodological gap, we present a novel CL approach on RAW images, residual\ncontrastive learning (RCL), which aims to learn meaningful representations for\nJDD. Our work is built on the assumption that noise contained in each RAW image\nis signal-dependent, thus two crops from the same RAW image should have more\nsimilar noise distribution than two crops from different RAW images. We use\nresiduals as a discriminative feature and the earth mover's distance to measure\nthe distribution divergence for the contrastive loss. To evaluate the proposed\nCL strategy, we simulate a series of unsupervised JDD experiments with\nlarge-scale data corrupted by synthetic signal-dependent noise, where we set a\nnew benchmark for unsupervised JDD tasks with unknown (random) noise variance.\nOur empirical study not only validates that CL can be applied on distributions\n(c.f. features), but also exposes the lack of robustness of previous non-ML and\nSSL JDD methods when the statistics of the noise are unknown, thus providing\nsome further insight into signal-dependent noise problems.",
          "link": "http://arxiv.org/abs/2106.10070",
          "publishedOn": "2021-06-21T02:07:39.266Z",
          "wordCount": 653,
          "title": "Residual Contrastive Learning for Joint Demosaicking and Denoising. (arXiv:2106.10070v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.04828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingjing Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koloskova_A/0/1/0/all/0/1\">Anastasia Koloskova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1\">Sebastian U. Stich</a>",
          "description": "Decentralized training of deep learning models enables on-device learning\nover networks, as well as efficient scaling to large compute clusters.\nExperiments in earlier works reveal that, even in a data-center setup,\ndecentralized training often suffers from the degradation in the quality of the\nmodel: the training and test performance of models trained in a decentralized\nfashion is in general worse than that of models trained in a centralized\nfashion, and this performance drop is impacted by parameters such as network\nsize, communication topology and data partitioning. We identify the changing\nconsensus distance between devices as a key parameter to explain the gap\nbetween centralized and decentralized training.\n\nWe show in theory that when the training consensus distance is lower than a\ncritical quantity, decentralized training converges as fast as the centralized\ncounterpart. We empirically validate that the relation between generalization\nperformance and consensus distance is consistent with this theoretical\nobservation. Our empirical insights allow the principled design of better\ndecentralized training schemes that mitigate the performance drop. To this end,\nwe provide practical training guidelines and exemplify its effectiveness on the\ndata-center setup as the important first step.",
          "link": "http://arxiv.org/abs/2102.04828",
          "publishedOn": "2021-06-21T02:07:39.259Z",
          "wordCount": 672,
          "title": "Consensus Control for Decentralized Deep Learning. (arXiv:2102.04828v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1\">John D. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modayil_J/0/1/0/all/0/1\">Joseph Modayil</a>",
          "description": "The performance of a reinforcement learning (RL) system depends on the\ncomputational architecture used to approximate a value function. Deep learning\nmethods provide both optimization techniques and architectures for\napproximating nonlinear functions from noisy, high-dimensional observations.\nHowever, prevailing optimization techniques are not designed for\nstrictly-incremental online updates. Nor are standard architectures designed\nfor observations with an a priori unknown structure: for example, light sensors\nrandomly dispersed in space. This paper proposes an online RL prediction\nalgorithm with an adaptive architecture that efficiently finds useful nonlinear\nfeatures. The algorithm is evaluated in a spatial domain with high-dimensional,\nstochastic observations. The algorithm outperforms non-adaptive baseline\narchitectures and approaches the performance of an architecture given\nside-channel information. These results are a step towards scalable RL\nalgorithms for more general problems, where the observation structure is not\navailable.",
          "link": "http://arxiv.org/abs/2106.09776",
          "publishedOn": "2021-06-21T02:07:39.240Z",
          "wordCount": 568,
          "title": "Adapting the Function Approximation Architecture in Online Reinforcement Learning. (arXiv:2106.09776v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.10351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lucas Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1\">Caleb Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1\">Bistra Dilkina</a>",
          "description": "Recent work has shown that deep learning models can be used to classify\nland-use data from geospatial satellite imagery. We show that when these deep\nlearning models are trained on data from specific continents/seasons, there is\na high degree of variability in model performance on out-of-sample\ncontinents/seasons. This suggests that just because a model accurately predicts\nland-use classes in one continent or season does not mean that the model will\naccurately predict land-use classes in a different continent or season. We then\nuse clustering techniques on satellite imagery from different continents to\nvisualize the differences in landscapes that make geospatial generalization\nparticularly difficult, and summarize our takeaways for future satellite\nimagery-related applications.",
          "link": "http://arxiv.org/abs/2008.10351",
          "publishedOn": "2021-06-21T02:07:39.234Z",
          "wordCount": 606,
          "title": "Model Generalization in Deep Learning Applications for Land Cover Mapping. (arXiv:2008.10351v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07030",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Araman_V/0/1/0/all/0/1\">Victor F. Araman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Caldentey_R/0/1/0/all/0/1\">Rene Caldentey</a>",
          "description": "We consider a decision maker who must choose an action in order to maximize a\nreward function that depends also on an unknown parameter {\\Theta}. The\ndecision maker can delay taking the action in order to experiment and gather\nadditional information on {\\Theta}. We model the decision maker's problem using\na Bayesian sequential experimentation framework and use dynamic programming and\ndiffusion-asymptotic analysis to solve it. For that, we scale our problem in a\nway that both the average number of experiments that is conducted per unit of\ntime is large and the informativeness of each individual experiment is low.\nUnder such regime, we derive a diffusion approximation for the sequential\nexperimentation problem, which provides a number of important insights about\nthe nature of the problem and its solution. Our solution method also shows that\nthe complexity of the problem grows only quadratically with the cardinality of\nthe set of actions from which the decision maker can choose. We illustrate our\nmethodology and results using a concrete application in the context of\nassortment selection and new product introduction. Specifically, we study the\nproblem of a seller who wants to select an optimal assortment of products to\nlaunch into the marketplace and is uncertain about consumers' preferences.\nMotivated by emerging practices in e-commerce, we assume that the seller is\nable to use a crowdvoting system to learn these preferences before a final\nassortment decision is made. In this context, we undertake an extensive\nnumerical analysis to assess the value of learning and demonstrate the\neffectiveness and robustness of the heuristics derived from the diffusion\napproximation.",
          "link": "http://arxiv.org/abs/2102.07030",
          "publishedOn": "2021-06-21T02:07:39.226Z",
          "wordCount": 713,
          "title": "Diffusion Approximations for a Class of Sequential Testing Problems. (arXiv:2102.07030v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.01607",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1\">Tian Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1\">Agisilaos Chartsias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>",
          "description": "Pseudo-healthy synthesis is the task of creating a subject-specific `healthy'\nimage from a pathological one. Such images can be helpful in tasks such as\nanomaly detection and understanding changes induced by pathology and disease.\nIn this paper, we present a model that is encouraged to disentangle the\ninformation of pathology from what seems to be healthy. We disentangle what\nappears to be healthy and where disease is as a segmentation map, which are\nthen recombined by a network to reconstruct the input disease image. We train\nour models adversarially using either paired or unpaired settings, where we\npair disease images and maps when available. We quantitatively and\nsubjectively, with a human study, evaluate the quality of pseudo-healthy images\nusing several criteria. We show in a series of experiments, performed on ISLES,\nBraTS and Cam-CAN datasets, that our method is better than several baselines\nand methods from the literature. We also show that due to better training\nprocesses we could recover deformations, on surrounding tissue, caused by\ndisease. Our implementation is publicly available at\nhttps://github.com/xiat0616/pseudo-healthy-synthesis. This paper has been\naccepted by Medical Image Analysis:\nhttps://doi.org/10.1016/j.media.2020.101719.",
          "link": "http://arxiv.org/abs/2005.01607",
          "publishedOn": "2021-06-21T02:07:39.216Z",
          "wordCount": 674,
          "title": "Pseudo-healthy synthesis with pathology disentanglement and adversarial learning. (arXiv:2005.01607v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhar_S/0/1/0/all/0/1\">Sauptik Dhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heydari_J/0/1/0/all/0/1\">Javad Heydari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Samarth Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurup_U/0/1/0/all/0/1\">Unmesh Kurup</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mohak Shah</a>",
          "description": "Limited availability of labeled-data makes any supervised learning problem\nchallenging. Alternative learning settings like semi-supervised and universum\nlearning alleviate the dependency on labeled data, but still require a large\namount of unlabeled data, which may be unavailable or expensive to acquire.\nGAN-based synthetic data generation methods have recently shown promise by\ngenerating synthetic samples to improve task at hand. However, these samples\ncannot be used for other purposes. In this paper, we propose a GAN game which\nprovides improved discriminator accuracy under limited data settings, while\ngenerating realistic synthetic data. This provides the added advantage that now\nthe generated data can be used for other similar tasks. We provide the\ntheoretical guarantees and empirical results in support of our approach.",
          "link": "http://arxiv.org/abs/2106.09946",
          "publishedOn": "2021-06-21T02:07:39.208Z",
          "wordCount": 574,
          "title": "Evolving GANs: When Contradictions Turn into Compliance. (arXiv:2106.09946v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenrui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Junni Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hongkai Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>",
          "description": "Graph convolution networks, like message passing graph convolution networks\n(MPGCNs), have been a powerful tool in representation learning of networked\ndata. However, when data is heterogeneous, most architectures are limited as\nthey employ a single strategy to handle multi-channel graph signals and they\ntypically focus on low-frequency information. In this paper, we present a novel\ngraph convolution operator, termed BankGCN, which keeps benefits of message\npassing models, but extends their capabilities beyond `low-pass' features. It\ndecomposes multi-channel signals on graphs into subspaces and handles\nparticular information in each subspace with an adapted filter. The filters of\nall subspaces have different frequency responses and together form a filter\nbank. Furthermore, each filter in the spectral domain corresponds to a message\npassing scheme, and diverse schemes are implemented via the filter bank.\nImportantly, the filter bank and the signal decomposition are jointly learned\nto adapt to the spectral characteristics of data and to target applications.\nFurthermore, this is implemented almost without extra parameters in comparison\nwith most existing MPGCNs. Experimental results show that the proposed\nconvolution operator permits to achieve excellent performance in graph\nclassification on a collection of benchmark graph datasets.",
          "link": "http://arxiv.org/abs/2106.09910",
          "publishedOn": "2021-06-21T02:07:39.189Z",
          "wordCount": 642,
          "title": "Message Passing in Graph Convolution Networks via Adaptive Filter Banks. (arXiv:2106.09910v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.12824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lordeiro_I/0/1/0/all/0/1\">Igor Q. Lordeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddad_D/0/1/0/all/0/1\">Diego B. Haddad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_D/0/1/0/all/0/1\">Douglas O. Cardoso</a>",
          "description": "A popular computer puzzle, the game of Minesweeper requires its human players\nto have a mix of both luck and strategy to succeed. Analyzing these aspects\nmore formally, in our research we assessed the feasibility of a novel\nmethodology based on Reinforcement Learning as an adequate approach to tackle\nthe problem presented by this game. For this purpose we employed Multi-Armed\nBandit algorithms which were carefully adapted in order to enable their use to\ndefine autonomous computational players, targeting to make the best use of some\ngame peculiarities. After experimental evaluation, results showed that this\napproach was indeed successful, especially in smaller game boards, such as the\nstandard beginner level. Despite this fact the main contribution of this work\nis a detailed examination of Minesweeper from a learning perspective, which led\nto various original insights which are thoroughly discussed.",
          "link": "http://arxiv.org/abs/2007.12824",
          "publishedOn": "2021-06-21T02:07:39.182Z",
          "wordCount": 619,
          "title": "Multi-Armed Bandits for Minesweeper: Profiting from Exploration-Exploitation Synergy. (arXiv:2007.12824v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10258",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fornoni_M/0/1/0/all/0/1\">Marco Fornoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chaochao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Liangchen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilber_K/0/1/0/all/0/1\">Kimberly Wilber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stark_A/0/1/0/all/0/1\">Alex Stark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howard_A/0/1/0/all/0/1\">Andrew Howard</a>",
          "description": "When interacting with objects through cameras, or pictures, users often have\na specific intent. For example, they may want to perform a visual search.\nHowever, most object detection models ignore the user intent, relying on image\npixels as their only input. This often leads to incorrect results, such as lack\nof a high-confidence detection on the object of interest, or detection with a\nwrong class label. In this paper we investigate techniques to modulate standard\nobject detectors to explicitly account for the user intent, expressed as an\nembedding of a simple query. Compared to standard object detectors,\nquery-modulated detectors show superior performance at detecting objects for a\ngiven label of interest. Thanks to large-scale training data synthesized from\nstandard object detection annotations, query-modulated detectors can also\noutperform specialized referring expression recognition systems. Furthermore,\nthey can be simultaneously trained to solve for both query-modulated detection\nand standard object detection.",
          "link": "http://arxiv.org/abs/2106.10258",
          "publishedOn": "2021-06-21T02:07:39.175Z",
          "wordCount": 606,
          "title": "Bridging the Gap Between Object Detection and User Intent via Query-Modulation. (arXiv:2106.10258v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Luofeng Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jia Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolar_M/0/1/0/all/0/1\">Mladen Kolar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Large scale convex-concave minimax problems arise in numerous applications,\nincluding game theory, robust training, and training of generative adversarial\nnetworks. Despite their wide applicability, solving such problems efficiently\nand effectively is challenging in the presence of large amounts of data using\nexisting stochastic minimax methods. We study a class of stochastic minimax\nmethods and develop a communication-efficient distributed stochastic\nextragradient algorithm, LocalAdaSEG, with an adaptive learning rate suitable\nfor solving convex-concave minimax problem in the Parameter-Server model.\nLocalAdaSEG has three main features: (i) periodic communication strategy\nreduces the communication cost between workers and the server; (ii) an adaptive\nlearning rate that is computed locally and allows for tuning-free\nimplementation; and (iii) theoretically, a nearly linear speed-up with respect\nto the dominant variance term, arising from estimation of the stochastic\ngradient, is proven in both the smooth and nonsmooth convex-concave settings.\nLocalAdaSEG is used to solve a stochastic bilinear game, and train generative\nadversarial network. We compare LocalAdaSEG against several existing optimizers\nfor minimax problems and demonstrate its efficacy through several experiments\nin both the homogeneous and heterogeneous settings.",
          "link": "http://arxiv.org/abs/2106.10022",
          "publishedOn": "2021-06-21T02:07:39.168Z",
          "wordCount": 625,
          "title": "Local AdaGrad-Type Algorithm for Stochastic Convex-Concave Minimax Problems. (arXiv:2106.10022v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10086",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">An-phi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_M/0/1/0/all/0/1\">Maria Rodriguez Martinez</a>",
          "description": "Interpretability has become a necessary feature for machine learning models\ndeployed in critical scenarios, e.g. legal systems, healthcare. In these\nsituations, algorithmic decisions may have (potentially negative) long-lasting\neffects on the end-user affected by the decision. In many cases, the\nrepresentational power of deep learning models is not needed, therefore simple\nand interpretable models (e.g. linear models) should be preferred. However, in\nhigh-dimensional and/or complex domains (e.g. computer vision), the universal\napproximation capabilities of neural networks is required. Inspired by linear\nmodels and the Kolmogorov-Arnol representation theorem, we propose a novel\nclass of structurally-constrained neural networks, which we call FLANs\n(Feature-wise Latent Additive Networks). Crucially, FLANs process each input\nfeature separately, computing for each of them a representation in a common\nlatent space. These feature-wise latent representations are then simply summed,\nand the aggregated representation is used for prediction. These constraints\n(which are at the core of the interpretability of linear models) allow an user\nto estimate the effect of each individual feature independently from the\nothers, enhancing interpretability. In a set of experiments across different\ndomains, we show how without compromising excessively the test performance, the\nstructural constraints proposed in FLANs indeed increase the interpretability\nof deep learning models.",
          "link": "http://arxiv.org/abs/2106.10086",
          "publishedOn": "2021-06-21T02:07:39.162Z",
          "wordCount": 634,
          "title": "It's FLAN time! Summing feature-wise latent representations for interpretability. (arXiv:2106.10086v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1\">Andreas Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1\">Alexander Kolesnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wightman_R/0/1/0/all/0/1\">Ross Wightman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1\">Jakob Uszkoreit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>",
          "description": "Vision Transformers (ViT) have been shown to attain highly competitive\nperformance for a wide range of vision applications, such as image\nclassification, object detection and semantic image segmentation. In comparison\nto convolutional neural networks, the Vision Transformer's weaker inductive\nbias is generally found to cause an increased reliance on model regularization\nor data augmentation (``AugReg'' for short) when training on smaller training\ndatasets. We conduct a systematic empirical study in order to better understand\nthe interplay between the amount of training data, AugReg, model size and\ncompute budget. As one result of this study we find that the combination of\nincreased compute and AugReg can yield models with the same performance as\nmodels trained on an order of magnitude more training data: we train ViT models\nof various sizes on the public ImageNet-21k dataset which either match or\noutperform their counterparts trained on the larger, but not publicly available\nJFT-300M dataset.",
          "link": "http://arxiv.org/abs/2106.10270",
          "publishedOn": "2021-06-21T02:07:39.136Z",
          "wordCount": 649,
          "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers. (arXiv:2106.10270v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rego_R/0/1/0/all/0/1\">Rosana C. B. Rego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1\">Ver&#xf4;nica M. L. Silva</a>",
          "description": "Predicting gender by the name is not a simple task. In many applications,\nespecially in the natural language processing (NLP) field, this task may be\nnecessary, mainly when considering foreign names. Some machine learning\nalgorithms can satisfactorily perform the prediction. In this paper, we\nexamined and implemented feedforward and recurrent deep neural network models,\nsuch as MLP, RNN, GRU, CNN, and BiLSTM, to classify gender through the first\nname. A dataset of Brazilian names is used to train and evaluate the models. We\nanalyzed the accuracy, recall, precision, and confusion matrix to measure the\nmodels' performances. The results indicate that the gender prediction can be\nperformed from the feature extraction strategy looking at the names as a set of\nstrings. Some models accurately predict the gender in more than 90% of the\ncases. The recurrent models overcome the feedforward models in this binary\nclassification problem.",
          "link": "http://arxiv.org/abs/2106.10156",
          "publishedOn": "2021-06-21T02:07:39.129Z",
          "wordCount": 589,
          "title": "Predicting gender of Brazilian names using deep learning. (arXiv:2106.10156v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haiping Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1\">Robert Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_P/0/1/0/all/0/1\">Peizhen Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1\">Raivo E Koot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chasmai_M/0/1/0/all/0/1\">Mustafa Chasmai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schobs_L/0/1/0/all/0/1\">Lawrence Schobs</a>",
          "description": "Machine learning is a general-purpose technology holding promises for many\ninterdisciplinary research problems. However, significant barriers exist in\ncrossing disciplinary boundaries when most machine learning tools are developed\nin different areas separately. We present Pykale - a Python library for\nknowledge-aware machine learning on graphs, images, texts, and videos to enable\nand accelerate interdisciplinary research. We formulate new green machine\nlearning guidelines based on standard software engineering practices and\npropose a novel pipeline-based application programming interface (API). PyKale\nfocuses on leveraging knowledge from multiple sources for accurate and\ninterpretable prediction, thus supporting multimodal learning and transfer\nlearning (particularly domain adaptation) with latest deep learning and\ndimensionality reduction models. We build PyKale on PyTorch and leverage the\nrich PyTorch ecosystem. Our pipeline-based API design enforces standardization\nand minimalism, embracing green machine learning concepts via reducing\nrepetitions and redundancy, reusing existing resources, and recycling learning\nmodels across areas. We demonstrate its interdisciplinary nature via examples\nin bioinformatics, knowledge graph, image/video recognition, and medical\nimaging.",
          "link": "http://arxiv.org/abs/2106.09756",
          "publishedOn": "2021-06-21T02:07:39.121Z",
          "wordCount": 628,
          "title": "PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python. (arXiv:2106.09756v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boucher_N/0/1/0/all/0/1\">Nicholas Boucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1\">Ilia Shumailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_R/0/1/0/all/0/1\">Ross Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1\">Nicolas Papernot</a>",
          "description": "Several years of research have shown that machine-learning systems are\nvulnerable to adversarial examples, both in theory and in practice. Until now,\nsuch attacks have primarily targeted visual models, exploiting the gap between\nhuman and machine perception. Although text-based models have also been\nattacked with adversarial examples, such attacks struggled to preserve semantic\nmeaning and indistinguishability. In this paper, we explore a large class of\nadversarial examples that can be used to attack text-based models in a\nblack-box setting without making any human-perceptible visual modification to\ninputs. We use encoding-specific perturbations that are imperceptible to the\nhuman eye to manipulate the outputs of a wide range of Natural Language\nProcessing (NLP) systems from neural machine-translation pipelines to web\nsearch engines. We find that with a single imperceptible encoding injection --\nrepresenting one invisible character, homoglyph, reordering, or deletion -- an\nattacker can significantly reduce the performance of vulnerable models, and\nwith three injections most models can be functionally broken. Our attacks work\nagainst currently-deployed commercial systems, including those produced by\nMicrosoft and Google, in addition to open source models published by Facebook\nand IBM. This novel series of attacks presents a significant threat to many\nlanguage processing systems: an attacker can affect systems in a targeted\nmanner without any assumptions about the underlying model. We conclude that\ntext-based NLP systems require careful input sanitization, just like\nconventional applications, and that given such systems are now being deployed\nrapidly at scale, the urgent attention of architects and operators is required.",
          "link": "http://arxiv.org/abs/2106.09898",
          "publishedOn": "2021-06-21T02:07:39.113Z",
          "wordCount": 683,
          "title": "Bad Characters: Imperceptible NLP Attacks. (arXiv:2106.09898v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gudur_G/0/1/0/all/0/1\">Gautham Krishna Gudur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perepu_S/0/1/0/all/0/1\">Satheesh K. Perepu</a>",
          "description": "Federated learning is an effective way of extracting insights from different\nuser devices while preserving the privacy of users. However, new classes with\ncompletely unseen data distributions can stream across any device in a\nfederated learning setting, whose data cannot be accessed by the global server\nor other users. To this end, we propose a unified zero-shot framework to handle\nthese aforementioned challenges during federated learning. We simulate two\nscenarios here -- 1) when the new class labels are not reported by the user,\nthe traditional FL setting is used; 2) when new class labels are reported by\nthe user, we synthesize Anonymized Data Impressions by calculating class\nsimilarity matrices corresponding to each device's new classes followed by\nunsupervised clustering to distinguish between new classes across different\nusers. Moreover, our proposed framework can also handle statistical\nheterogeneities in both labels and models across the participating users. We\nempirically evaluate our framework on-device across different communication\nrounds (FL iterations) with new classes in both local and global updates, along\nwith heterogeneous labels and models, on two widely used audio classification\napplications -- keyword spotting and urban sound classification, and observe an\naverage deterministic accuracy increase of ~4.041% and ~4.258% respectively.",
          "link": "http://arxiv.org/abs/2106.10019",
          "publishedOn": "2021-06-21T02:07:39.095Z",
          "wordCount": 664,
          "title": "Zero-Shot Federated Learning with New Classes for Audio Classification. (arXiv:2106.10019v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dongqi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doya_K/0/1/0/all/0/1\">Kenji Doya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tani_J/0/1/0/all/0/1\">Jun Tani</a>",
          "description": "What is the difference between goal-directed and habitual behavior? We\npropose a novel computational framework of decision making with Bayesian\ninference, in which everything is integrated as an entire neural network model.\nThe model learns to predict environmental state transitions by self-exploration\nand generating motor actions by sampling stochastic internal states $z$.\nHabitual behavior, which is obtained from the prior distribution of $z$, is\nacquired by reinforcement learning. Goal-directed behavior is determined from\nthe posterior distribution of $z$ by planning, using active inference, to\nminimize the free energy for goal observation. We demonstrate the effectiveness\nof the proposed framework by experiments in a sensorimotor navigation task with\ncamera observations and continuous motor actions.",
          "link": "http://arxiv.org/abs/2106.09938",
          "publishedOn": "2021-06-21T02:07:39.088Z",
          "wordCount": 552,
          "title": "Goal-Directed Planning by Reinforcement Learning and Active Inference. (arXiv:2106.09938v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09762",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Detommaso_G/0/1/0/all/0/1\">Gianluca Detommaso</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bruckner_M/0/1/0/all/0/1\">Michael Br&#xfc;ckner</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schulz_P/0/1/0/all/0/1\">Philip Schulz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chernozhukov_V/0/1/0/all/0/1\">Victor Chernozhukov</a>",
          "description": "In this work we develop a novel characterization of marginal causal effect\nand causal bias in the continuous treatment setting. We show they can be\nexpressed as an expectation with respect to a conditional probability\ndistribution, which can be estimated via standard statistical and probabilistic\nmethods. All terms in the expectations can be computed via automatic\ndifferentiation, also for highly non-linear models. We further develop a new\ncomplete criterion for identifiability of causal effects via covariate\nadjustment, showing the bias equals zero if the criterion is met. We study the\neffectiveness of our framework in three different scenarios: linear models\nunder confounding, overcontrol and endogenous selection bias; a non-linear\nmodel where full identifiability cannot be achieved because of missing data; a\nsimulated medical study of statins and atherosclerotic cardiovascular disease.",
          "link": "http://arxiv.org/abs/2106.09762",
          "publishedOn": "2021-06-21T02:07:39.077Z",
          "wordCount": 565,
          "title": "Causal Bias Quantification for Continuous Treatment. (arXiv:2106.09762v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09815",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Davis_D/0/1/0/all/0/1\">Damek Davis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Diaz_M/0/1/0/all/0/1\">Mateo D&#xed;az</a>, <a href=\"http://arxiv.org/find/math/1/au:+Drusvyatskiy_D/0/1/0/all/0/1\">Dmitriy Drusvyatskiy</a>",
          "description": "Recent work has shown that stochastically perturbed gradient methods can\nefficiently escape strict saddle points of smooth functions. We extend this\nbody of work to nonsmooth optimization, by analyzing an inexact analogue of a\nstochastically perturbed gradient method applied to the Moreau envelope. The\nmain conclusion is that a variety of algorithms for nonsmooth optimization can\nescape strict saddle points of the Moreau envelope at a controlled rate. The\nmain technical insight is that typical algorithms applied to the proximal\nsubproblem yield directions that approximate the gradient of the Moreau\nenvelope in relative terms.",
          "link": "http://arxiv.org/abs/2106.09815",
          "publishedOn": "2021-06-21T02:07:39.068Z",
          "wordCount": 551,
          "title": "Escaping strict saddle points of the Moreau envelope in nonsmooth optimization. (arXiv:2106.09815v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Avellaneda_F/0/1/0/all/0/1\">Florent Avellaneda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villemaire_R/0/1/0/all/0/1\">Roger Villemaire</a>",
          "description": "The Boolean matrix factorization problem consists in approximating a matrix\nby the Boolean product of two smaller Boolean matrices. To obtain optimal\nsolutions when the matrices to be factorized are small, we propose SAT and\nMaxSAT encoding; however, when the matrices to be factorized are large, we\npropose a heuristic based on the search for maximal biclique edge cover. We\nexperimentally demonstrate that our approaches allow a better factorization\nthan existing approaches while keeping reasonable computation times. Our\nmethods also allow the handling of incomplete matrices with missing entries.",
          "link": "http://arxiv.org/abs/2106.10105",
          "publishedOn": "2021-06-21T02:07:39.056Z",
          "wordCount": 512,
          "title": "Boolean Matrix Factorization with SAT and MaxSAT. (arXiv:2106.10105v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "Collecting training data from untrusted sources exposes machine learning\nservices to poisoning adversaries, who maliciously manipulate training data to\ndegrade the model accuracy. When trained on offline datasets, poisoning\nadversaries have to inject the poisoned data in advance before training, and\nthe order of feeding these poisoned batches into the model is stochastic. In\ncontrast, practical systems are more usually trained/fine-tuned on sequentially\ncaptured real-time data, in which case poisoning adversaries could dynamically\npoison each data batch according to the current model state. In this paper, we\nfocus on the real-time settings and propose a new attacking strategy, which\naffiliates an accumulative phase with poisoning attacks to secretly (i.e.,\nwithout affecting accuracy) magnify the destructive effect of a (poisoned)\ntrigger batch. By mimicking online learning and federated learning on CIFAR-10,\nwe show that the model accuracy will significantly drop by a single update step\non the trigger batch after the accumulative phase. Our work validates that a\nwell-designed but straightforward attacking strategy can dramatically amplify\nthe poisoning effects, with no need to explore complex techniques.",
          "link": "http://arxiv.org/abs/2106.09993",
          "publishedOn": "2021-06-21T02:07:39.049Z",
          "wordCount": 615,
          "title": "Accumulative Poisoning Attacks on Real-time Data. (arXiv:2106.09993v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Guang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Fei Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_V/0/1/0/all/0/1\">Vincent CS Lee</a>",
          "description": "Detecting anomalies for dynamic graphs has drawn increasing attention due to\ntheir wide applications in social networks, e-commerce, and cybersecurity. The\nrecent deep learning-based approaches have shown promising results over shallow\nmethods. However, they fail to address two core challenges of anomaly detection\nin dynamic graphs: the lack of informative encoding for unattributed nodes and\nthe difficulty of learning discriminate knowledge from coupled spatial-temporal\ndynamic graphs. To overcome these challenges, in this paper, we present a novel\nTransformer-based Anomaly Detection framework for DYnamic graph (TADDY). Our\nframework constructs a comprehensive node encoding strategy to better represent\neach node's structural and temporal roles in an evolving graphs stream.\nMeanwhile, TADDY captures informative representation from dynamic graphs with\ncoupled spatial-temporal patterns via a dynamic graph transformer model. The\nextensive experimental results demonstrate that our proposed TADDY framework\noutperforms the state-of-the-art methods by a large margin on four real-world\ndatasets.",
          "link": "http://arxiv.org/abs/2106.09876",
          "publishedOn": "2021-06-21T02:07:39.031Z",
          "wordCount": 585,
          "title": "Anomaly Detection in Dynamic Graphs via Transformer. (arXiv:2106.09876v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nock_R/0/1/0/all/0/1\">Richard Nock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sypherd_T/0/1/0/all/0/1\">Tyler Sypherd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_L/0/1/0/all/0/1\">Lalitha Sankar</a>",
          "description": "In today's ML, data can be twisted (changed) in various ways, either for bad\nor good intent. Such twisted data challenges the founding theory of properness\nfor supervised losses which form the basis for many popular losses for class\nprobability estimation. Unfortunately, at its core, properness ensures that the\noptimal models also learn the twist. In this paper, we analyse such class\nprobability-based losses when they are stripped off the mandatory properness;\nwe define twist-proper losses as losses formally able to retrieve the optimum\n(untwisted) estimate off the twists, and show that a natural extension of a\nhalf-century old loss introduced by S. Arimoto is twist proper. We then turn to\na theory that has provided some of the best off-the-shelf algorithms for proper\nlosses, boosting. Boosting can require access to the derivative of the convex\nconjugate of a loss to compute examples weights. Such a function can be hard to\nget, for computational or mathematical reasons; this turns out to be the case\nfor Arimoto's loss. We bypass this difficulty by inverting the problem as\nfollows: suppose a blueprint boosting algorithm is implemented with a general\nweight update function. What are the losses for which boosting-compliant\nminimisation happens? Our answer comes as a general boosting algorithm which\nmeets the optimal boosting dependence on the number of calls to the weak\nlearner; when applied to Arimoto's loss, it leads to a simple optimisation\nalgorithm whose performances are showcased on several domains and twists.",
          "link": "http://arxiv.org/abs/2106.09920",
          "publishedOn": "2021-06-21T02:07:39.025Z",
          "wordCount": 674,
          "title": "Being Properly Improper. (arXiv:2106.09920v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1\">Shuyue Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loew_M/0/1/0/all/0/1\">Murray Loew</a>",
          "description": "To evaluate clustering results is a significant part of cluster analysis.\nSince there are no true class labels for clustering in typical unsupervised\nlearning, many internal cluster validity indices (CVIs), which use predicted\nlabels and data, have been created. Without true labels, to design an effective\nCVI is as difficult as to create a clustering method. And it is crucial to have\nmore CVIs because there are no universal CVIs that can be used to measure all\ndatasets and no specific methods of selecting a proper CVI for clusters without\ntrue labels. Therefore, to apply a variety of CVIs to evaluate clustering\nresults is necessary. In this paper, we propose a novel internal CVI -- the\nDistance-based Separability Index (DSI), based on a data separability measure.\nWe compared the DSI with eight internal CVIs including studies from early Dunn\n(1974) to most recent CVDD (2019) and an external CVI as ground truth, by using\nclustering results of five clustering algorithms on 12 real and 97 synthetic\ndatasets. Results show DSI is an effective, unique, and competitive CVI to\nother compared CVIs. We also summarized the general process to evaluate CVIs\nand created the rank-difference metric for comparison of CVIs' results.",
          "link": "http://arxiv.org/abs/2106.09794",
          "publishedOn": "2021-06-21T02:07:39.009Z",
          "wordCount": 647,
          "title": "A Distance-based Separability Measure for Internal Cluster Validation. (arXiv:2106.09794v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>",
          "description": "This paper investigates two techniques for developing efficient\nself-supervised vision transformers (EsViT) for visual representation learning.\nFirst, we show through a comprehensive empirical study that multi-stage\narchitectures with sparse self-attentions can significantly reduce modeling\ncomplexity but with a cost of losing the ability to capture fine-grained\ncorrespondences between image regions. Second, we propose a new pre-training\ntask of region matching which allows the model to capture fine-grained region\ndependencies and as a result significantly improves the quality of the learned\nvision representations. Our results show that combining the two techniques,\nEsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation,\noutperforming prior arts with around an order magnitude of higher throughput.\nWhen transferring to downstream linear classification tasks, EsViT outperforms\nits supervised counterpart on 17 out of 18 datasets. The code and models will\nbe publicly available.",
          "link": "http://arxiv.org/abs/2106.09785",
          "publishedOn": "2021-06-21T02:07:39.002Z",
          "wordCount": 595,
          "title": "Efficient Self-supervised Vision Transformers for Representation Learning. (arXiv:2106.09785v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheu_A/0/1/0/all/0/1\">Albert Cheu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_M/0/1/0/all/0/1\">Matthew Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jieming Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Binghui Peng</a>",
          "description": "In shuffle privacy, each user sends a collection of randomized messages to a\ntrusted shuffler, the shuffler randomly permutes these messages, and the\nresulting shuffled collection of messages must satisfy differential privacy.\nPrior work in this model has largely focused on protocols that use a single\nround of communication to compute algorithmic primitives like means,\nhistograms, and counts. In this work, we present interactive shuffle protocols\nfor stochastic convex optimization. Our optimization protocols rely on a new\nnoninteractive protocol for summing vectors of bounded $\\ell_2$ norm. By\ncombining this sum subroutine with techniques including mini-batch stochastic\ngradient descent, accelerated gradient descent, and Nesterov's smoothing\nmethod, we obtain loss guarantees for a variety of convex loss functions that\nsignificantly improve on those of the local model and sometimes match those of\nthe central model.",
          "link": "http://arxiv.org/abs/2106.09805",
          "publishedOn": "2021-06-21T02:07:38.982Z",
          "wordCount": 574,
          "title": "Shuffle Private Stochastic Convex Optimization. (arXiv:2106.09805v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eugene Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_D/0/1/0/all/0/1\">David D. Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieder_O/0/1/0/all/0/1\">Ophir Frieder</a>",
          "description": "Technology-assisted review (TAR) refers to human-in-the-loop active learning\nworkflows for finding relevant documents in large collections. These workflows\noften must meet a target for the proportion of relevant documents found (i.e.\nrecall) while also holding down costs. A variety of heuristic stopping rules\nhave been suggested for striking this tradeoff in particular settings, but none\nhave been tested against a range of recall targets and tasks. We propose two\nnew heuristic stopping rules, Quant and QuantCI based on model-based estimation\ntechniques from survey research. We compare them against a range of proposed\nheuristics and find they are accurate at hitting a range of recall targets\nwhile substantially reducing review costs.",
          "link": "http://arxiv.org/abs/2106.09871",
          "publishedOn": "2021-06-21T02:07:38.976Z",
          "wordCount": 549,
          "title": "Heuristic Stopping Rules For Technology-Assisted Review. (arXiv:2106.09871v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09834",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Weiwen Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebrahimian_S/0/1/0/all/0/1\">Shadi Ebrahimian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1\">Hengyong Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalra_M/0/1/0/all/0/1\">Mannu Kalra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>",
          "description": "By the ALARA (As Low As Reasonably Achievable) principle, ultra-low-dose CT\nreconstruction is a holy grail to minimize cancer risks and genetic damages,\nespecially for children. With the development of medical CT technologies, the\niterative algorithms are widely used to reconstruct decent CT images from a\nlow-dose scan. Recently, artificial intelligence (AI) techniques have shown a\ngreat promise in further reducing CT radiation dose to the next level. In this\npaper, we demonstrate that AI-powered CT reconstruction offers diagnostic image\nquality at an ultra-low-dose level comparable to that of radiography.\nSpecifically, here we develop a Split Unrolled Grid-like Alternative\nReconstruction (SUGAR) network, in which deep learning, physical modeling and\nimage prior are integrated. The reconstruction results from clinical datasets\nshow that excellent images can be reconstructed using SUGAR from 36\nprojections. This approach has a potential to change future healthcare.",
          "link": "http://arxiv.org/abs/2106.09834",
          "publishedOn": "2021-06-21T02:07:38.970Z",
          "wordCount": 597,
          "title": "AI-Enabled Ultra-Low-Dose CT Reconstruction. (arXiv:2106.09834v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khezeli_K/0/1/0/all/0/1\">Kia Khezeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blaas_A/0/1/0/all/0/1\">Arno Blaas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soboczenski_F/0/1/0/all/0/1\">Frank Soboczenski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chia_N/0/1/0/all/0/1\">Nicholas Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalantari_J/0/1/0/all/0/1\">John Kalantari</a>",
          "description": "The Invariant Risk Minimization (IRM) principle was first proposed by\nArjovsky et al. [2019] to address the domain generalization problem by\nleveraging data heterogeneity from differing experimental conditions.\nSpecifically, IRM seeks to find a data representation under which an optimal\nclassifier remains invariant across all domains. Despite the conceptual appeal\nof IRM, the effectiveness of the originally proposed invariance penalty has\nrecently been brought into question. In particular, there exists\ncounterexamples for which that invariance penalty can be arbitrarily small for\nnon-invariant data representations. We propose an alternative invariance\npenalty by revisiting the Gramian matrix of the data representation. We discuss\nthe role of its eigenvalues in the relationship between the risk and the\ninvariance penalty, and demonstrate that it is ill-conditioned for said\ncounterexamples. The proposed approach is guaranteed to recover an invariant\nrepresentation for linear settings under mild non-degeneracy conditions. Its\neffectiveness is substantiated by experiments on DomainBed and\nInvarianceUnitTest, two extensive test beds for domain generalization.",
          "link": "http://arxiv.org/abs/2106.09777",
          "publishedOn": "2021-06-21T02:07:38.963Z",
          "wordCount": 591,
          "title": "On Invariance Penalties for Risk Minimization. (arXiv:2106.09777v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schaefermeier_B/0/1/0/all/0/1\">Bastian Schaefermeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stumme_G/0/1/0/all/0/1\">Gerd Stumme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanika_T/0/1/0/all/0/1\">Tom Hanika</a>",
          "description": "The ubiquitous presence of WiFi access points and mobile devices capable of\nmeasuring WiFi signal strengths allow for real-world applications in indoor\nlocalization and mapping. In particular, no additional infrastructure is\nrequired. Previous approaches in this field were, however, often hindered by\nproblems such as effortful map-building processes, changing environments and\nhardware differences. We tackle these problems focussing on topological maps.\nThese represent discrete locations, such as rooms, and their relations, e.g.,\ndistances and transition frequencies. In our unsupervised method, we employ\nWiFi signal strength distributions, dimension reduction and clustering. It can\nbe used in settings where users carry mobile devices and follow their normal\nroutine. We aim for applications in short-lived indoor events such as\nconferences.",
          "link": "http://arxiv.org/abs/2106.09789",
          "publishedOn": "2021-06-21T02:07:38.953Z",
          "wordCount": 557,
          "title": "Topological Indoor Mapping through WiFi Signals. (arXiv:2106.09789v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muzellec_B/0/1/0/all/0/1\">Boris Muzellec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>",
          "description": "Kernel mean embeddings are a popular tool that consists in representing\nprobability measures by their infinite-dimensional mean embeddings in a\nreproducing kernel Hilbert space. When the kernel is characteristic, mean\nembeddings can be used to define a distance between probability measures, known\nas the maximum mean discrepancy (MMD). A well-known advantage of mean\nembeddings and MMD is their low computational cost and low sample complexity.\nHowever, kernel mean embeddings have had limited applications to problems that\nconsist in optimizing distributions, due to the difficulty of characterizing\nwhich Hilbert space vectors correspond to a probability distribution. In this\nnote, we propose to leverage the kernel sums-of-squares parameterization of\npositive functions of Marteau-Ferey et al. [2020] to fit distributions in the\nMMD geometry. First, we show that when the kernel is characteristic,\ndistributions with a kernel sum-of-squares density are dense. Then, we provide\nalgorithms to optimize such distributions in the finite-sample setting, which\nwe illustrate in a density fitting numerical experiment.",
          "link": "http://arxiv.org/abs/2106.09994",
          "publishedOn": "2021-06-21T02:07:38.946Z",
          "wordCount": 593,
          "title": "A Note on Optimizing Distributions using Kernel Mean Embeddings. (arXiv:2106.09994v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1\">Tomoki Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1\">Paolo Favaro</a>",
          "description": "We propose a novel GAN training scheme that can handle any level of labeling\nin a unified manner. Our scheme introduces a form of artificial labeling that\ncan incorporate manually defined labels, when available, and induce an\nalignment between them. To define the artificial labels, we exploit the\nassumption that neural network generators can be trained more easily to map\nnearby latent vectors to data with semantic similarities, than across separate\ncategories. We use generated data samples and their corresponding artificial\nconditioning labels to train a classifier. The classifier is then used to\nself-label real data. To boost the accuracy of the self-labeling, we also use\nthe exponential moving average of the classifier. However, because the\nclassifier might still make mistakes, especially at the beginning of the\ntraining, we also refine the labels through self-attention, by using the\nlabeling of real data samples only when the classifier outputs a high\nclassification probability score. We evaluate our approach on CIFAR-10, STL-10\nand SVHN, and show that both self-labeling and self-attention consistently\nimprove the quality of generated data. More surprisingly, we find that the\nproposed scheme can even outperform class-conditional GANs.",
          "link": "http://arxiv.org/abs/2106.09914",
          "publishedOn": "2021-06-21T02:07:38.927Z",
          "wordCount": 626,
          "title": "A Unified Generative Adversarial Network Training via Self-Labeling and Self-Attention. (arXiv:2106.09914v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yabin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>",
          "description": "Domain adaptation becomes more challenging with increasing gaps between\nsource and target domains. Motivated from an empirical analysis on the\nreliability of labeled source data for the use of distancing target domains, we\npropose self-training of auxiliary models (AuxSelfTrain) that learns models for\nintermediate domains and gradually combats the distancing shifts across\ndomains. We introduce evolving intermediate domains as combinations of\ndecreasing proportion of source data and increasing proportion of target data,\nwhich are sampled to minimize the domain distance between consecutive domains.\nThen the source model could be gradually adapted for the use in the target\ndomain by self-training of auxiliary models on evolving intermediate domains.\nWe also introduce an enhanced indicator for sample selection via implicit\nensemble and extend the proposed method to semi-supervised domain adaptation.\nExperiments on benchmark datasets of unsupervised and semi-supervised domain\nadaptation verify its efficacy.",
          "link": "http://arxiv.org/abs/2106.09890",
          "publishedOn": "2021-06-21T02:07:38.921Z",
          "wordCount": 579,
          "title": "Gradual Domain Adaptation via Self-Training of Auxiliary Models. (arXiv:2106.09890v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Keyang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_P/0/1/0/all/0/1\">Prashant Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1\">Bikramjit Banerjee</a>",
          "description": "Recent renewed interest in multi-agent reinforcement learning (MARL) has\ngenerated an impressive array of techniques that leverage deep reinforcement\nlearning, primarily actor-critic architectures, and can be applied to a limited\nrange of settings in terms of observability and communication. However, a\ncontinuing limitation of much of this work is the curse of dimensionality when\nit comes to representations based on joint actions, which grow exponentially\nwith the number of agents. In this paper, we squarely focus on this challenge\nof scalability. We apply the key insight of action anonymity, which leads to\npermutation invariance of joint actions, to two recently presented deep MARL\nalgorithms, MADDPG and IA2C, and compare these instantiations to another recent\ntechnique that leverages action anonymity, viz., mean-field MARL. We show that\nour instantiations can learn the optimal behavior in a broader class of agent\nnetworks than the mean-field method, using a recently introduced pragmatic\ndomain.",
          "link": "http://arxiv.org/abs/2106.09825",
          "publishedOn": "2021-06-21T02:07:38.914Z",
          "wordCount": 583,
          "title": "Many Agent Reinforcement Learning Under Partial Observability. (arXiv:2106.09825v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09759",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zunair_H/0/1/0/all/0/1\">Hasib Zunair</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamza_A/0/1/0/all/0/1\">A. Ben Hamza</a>",
          "description": "We introduce a new dataset called Synthetic COVID-19 Chest X-ray Dataset for\ntraining machine learning models. The dataset consists of 21,295 synthetic\nCOVID-19 chest X-ray images to be used for computer-aided diagnosis. These\nimages, generated via an unsupervised domain adaptation approach, are of high\nquality. We find that the synthetic images not only improve performance of\nvarious deep learning architectures when used as additional training data under\nheavy imbalance conditions, but also detect the target class with high\nconfidence. We also find that comparable performance can also be achieved when\ntrained only on synthetic images. Further, salient features of the synthetic\nCOVID-19 images indicate that the distribution is significantly different from\nNon-COVID-19 classes, enabling a proper decision boundary. We hope the\navailability of such high fidelity chest X-ray images of COVID-19 will\nencourage advances in the development of diagnostic and/or management tools.",
          "link": "http://arxiv.org/abs/2106.09759",
          "publishedOn": "2021-06-21T02:07:38.900Z",
          "wordCount": 633,
          "title": "Synthetic COVID-19 Chest X-ray Dataset for Computer-Aided Diagnosis. (arXiv:2106.09759v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shibo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirby_R/0/1/0/all/0/1\">Robert M. Kirby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_S/0/1/0/all/0/1\">Shandian Zhe</a>",
          "description": "Bayesian optimization (BO) is a powerful approach for optimizing black-box,\nexpensive-to-evaluate functions. To enable a flexible trade-off between the\ncost and accuracy, many applications allow the function to be evaluated at\ndifferent fidelities. In order to reduce the optimization cost while maximizing\nthe benefit-cost ratio, in this paper, we propose Batch Multi-fidelity Bayesian\nOptimization with Deep Auto-Regressive Networks (BMBO-DARN). We use a set of\nBayesian neural networks to construct a fully auto-regressive model, which is\nexpressive enough to capture strong yet complex relationships across all the\nfidelities, so as to improve the surrogate learning and optimization\nperformance. Furthermore, to enhance the quality and diversity of queries, we\ndevelop a simple yet efficient batch querying method, without any combinatorial\nsearch over the fidelities. We propose a batch acquisition function based on\nMax-value Entropy Search (MES) principle, which penalizes highly correlated\nqueries and encourages diversity. We use posterior samples and moment matching\nto fulfill efficient computation of the acquisition function and conduct\nalternating optimization over every fidelity-input pair, which guarantees an\nimprovement at each step. We demonstrate the advantage of our approach on four\nreal-world hyperparameter optimization applications.",
          "link": "http://arxiv.org/abs/2106.09884",
          "publishedOn": "2021-06-21T02:07:38.875Z",
          "wordCount": 620,
          "title": "Batch Multi-Fidelity Bayesian Optimization with Deep Auto-Regressive Networks. (arXiv:2106.09884v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongmin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiucai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imakura_A/0/1/0/all/0/1\">Akira Imakura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakurai_T/0/1/0/all/0/1\">Tetsuya Sakurai</a>",
          "description": "Ensemble clustering is a fundamental problem in the machine learning field,\ncombining multiple base clusterings into a better clustering result. However,\nmost of the existing methods are unsuitable for large-scale ensemble clustering\ntasks due to the efficiency bottleneck. In this paper, we propose a large-scale\nspectral ensemble clustering (LSEC) method to strike a good balance between\nefficiency and effectiveness. In LSEC, a large-scale spectral clustering based\nefficient ensemble generation framework is designed to generate various base\nclusterings within a low computational complexity. Then all based clustering\nare combined through a bipartite graph partition based consensus function into\na better consensus clustering result. The LSEC method achieves a lower\ncomputational complexity than most existing ensemble clustering methods.\nExperiments conducted on ten large-scale datasets show the efficiency and\neffectiveness of the LSEC method. The MATLAB code of the proposed method and\nexperimental datasets are available at https://github.com/Li-\nHongmin/MyPaperWithCode.",
          "link": "http://arxiv.org/abs/2106.09852",
          "publishedOn": "2021-06-21T02:07:38.868Z",
          "wordCount": 574,
          "title": "LSEC: Large-scale spectral ensemble clustering. (arXiv:2106.09852v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dominguez_Caballero_J/0/1/0/all/0/1\">Javier Dominguez-Caballero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ward_R/0/1/0/all/0/1\">Rob Ward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayvar_Soberanis_S/0/1/0/all/0/1\">Sabino Ayvar-Soberanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curtis_D/0/1/0/all/0/1\">David Curtis</a>",
          "description": "Accurate prediction of machining cycle times is important in the\nmanufacturing industry. Usually, Computer Aided Manufacturing (CAM) software\nestimates the machining times using the commanded feedrate from the toolpath\nfile using basic kinematic settings. Typically, the methods do not account for\ntoolpath geometry or toolpath tolerance and therefore under estimate the\nmachining cycle times considerably. Removing the need for machine specific\nknowledge, this paper presents a data-driven feedrate and machining cycle time\nprediction method by building a neural network model for each machine tool\naxis. In this study, datasets composed of the commanded feedrate, nominal\nacceleration, toolpath geometry and the measured feedrate were used to train a\nneural network model. Validation trials using a representative industrial thin\nwall structure component on a commercial machining centre showed that this\nmethod estimated the machining time with more than 90% accuracy. This method\nshowed that neural network models have the capability to learn the behavior of\na complex machine tool system and predict cycle times. Further integration of\nthe methods will be critical in the implantation of digital twins in Industry\n4.0.",
          "link": "http://arxiv.org/abs/2106.09719",
          "publishedOn": "2021-06-21T02:07:38.862Z",
          "wordCount": 629,
          "title": "Machining Cycle Time Prediction: Data-driven Modelling of Machine Tool Feedrate Behavior with Neural Networks. (arXiv:2106.09719v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sangdon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobriban_E/0/1/0/all/0/1\">Edgar Dobriban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Insup Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1\">Osbert Bastani</a>",
          "description": "An important challenge facing modern machine learning is how to rigorously\nquantify the uncertainty of model predictions. Conveying uncertainty is\nespecially important when there are changes to the underlying data distribution\nthat might invalidate the predictive model. Yet, most existing uncertainty\nquantification algorithms break down in the presence of such shifts. We propose\na novel approach that addresses this challenge by constructing \\emph{probably\napproximately correct (PAC)} prediction sets in the presence of covariate\nshift. Our approach focuses on the setting where there is a covariate shift\nfrom the source distribution (where we have labeled training examples) to the\ntarget distribution (for which we want to quantify uncertainty). Our algorithm\nassumes given importance weights that encode how the probabilities of the\ntraining examples change under the covariate shift. In practice, importance\nweights typically need to be estimated; thus, we extend our algorithm to the\nsetting where we are given confidence intervals for the importance weights\nrather than their true value. We demonstrate the effectiveness of our approach\non various covariate shifts designed based on the DomainNet and ImageNet\ndatasets.",
          "link": "http://arxiv.org/abs/2106.09848",
          "publishedOn": "2021-06-21T02:07:38.855Z",
          "wordCount": 609,
          "title": "PAC Prediction Sets Under Covariate Shift. (arXiv:2106.09848v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09780",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Huhn_F/0/1/0/all/0/1\">Francisco Huhn</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Magri_L/0/1/0/all/0/1\">Luca Magri</a>",
          "description": "We develop a versatile optimization method, which finds the design parameters\nthat minimize time-averaged acoustic cost functionals. The method is\ngradient-free, model-informed, and data-driven with reservoir computing based\non echo state networks. First, we analyse the predictive capabilities of echo\nstate networks both in the short- and long-time prediction of the dynamics. We\nfind that both fully data-driven and model-informed architectures learn the\nchaotic acoustic dynamics, both time-accurately and statistically. Informing\nthe training with a physical reduced-order model with one acoustic mode\nmarkedly improves the accuracy and robustness of the echo state networks,\nwhilst keeping the computational cost low. Echo state networks offer accurate\npredictions of the long-time dynamics, which would be otherwise expensive by\nintegrating the governing equations to evaluate the time-averaged quantity to\noptimize. Second, we couple echo state networks with a Bayesian technique to\nexplore the design thermoacoustic parameter space. The computational method is\nminimally intrusive. Third, we find the set of flame parameters that minimize\nthe time-averaged acoustic energy of chaotic oscillations, which are caused by\nthe positive feedback with a heat source, such as a flame in gas turbines or\nrocket motors. These oscillations are known as thermoacoustic oscillations. The\noptimal set of flame parameters is found with the same accuracy as brute-force\ngrid search, but with a convergence rate that is more than one order of\nmagnitude faster. This work opens up new possibilities for non-intrusive\n(``hands-off'') optimization of chaotic systems, in which the cost of\ngenerating data, for example from high-fidelity simulations and experiments, is\nhigh.",
          "link": "http://arxiv.org/abs/2106.09780",
          "publishedOn": "2021-06-21T02:07:38.849Z",
          "wordCount": 693,
          "title": "Gradient-free optimization of chaotic acoustics with reservoir computing. (arXiv:2106.09780v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cranmer_M/0/1/0/all/0/1\">Miles Cranmer</a> (Princeton), <a href=\"http://arxiv.org/find/cs/1/au:+Melchior_P/0/1/0/all/0/1\">Peter Melchior</a> (Princeton), <a href=\"http://arxiv.org/find/cs/1/au:+Nord_B/0/1/0/all/0/1\">Brian Nord</a> (Fermilab)",
          "description": "We present an approach for maximizing a global utility function by learning\nhow to allocate resources in an unsupervised way. We expect interactions\nbetween allocation targets to be important and therefore propose to learn the\nreward structure for near-optimal allocation policies with a GNN. By relaxing\nthe resource constraint, we can employ gradient-based optimization in contrast\nto more standard evolutionary algorithms. Our algorithm is motivated by a\nproblem in modern astronomy, where one needs to select-based on limited initial\ninformation-among $10^9$ galaxies those whose detailed measurement will lead to\noptimal inference of the composition of the universe. Our technique presents a\nway of flexibly learning an allocation strategy by only requiring forward\nsimulators for the physics of interest and the measurement process. We\nanticipate that our technique will also find applications in a range of\nresource allocation problems.",
          "link": "http://arxiv.org/abs/2106.09761",
          "publishedOn": "2021-06-21T02:07:38.831Z",
          "wordCount": 602,
          "title": "Unsupervised Resource Allocation with Graph Neural Networks. (arXiv:2106.09761v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yining Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenfeld_E/0/1/0/all/0/1\">Elan Rosenfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellke_M/0/1/0/all/0/1\">Mark Sellke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1\">Andrej Risteski</a>",
          "description": "Domain generalization aims at performing well on unseen test environments\nwith data from a limited number of training environments. Despite a\nproliferation of proposal algorithms for this task, assessing their\nperformance, both theoretically and empirically is still very challenging.\nMoreover, recent approaches such as Invariant Risk Minimization (IRM) require a\nprohibitively large number of training environments - linear in the dimension\nof the spurious feature space $d_s$ - even on simple data models like the one\nproposed by [Rosenfeld et al., 2021]. Under a variant of this model, we show\nthat both ERM and IRM cannot generalize with $o(d_s)$ environments. We then\npresent a new algorithm based on performing iterative feature matching that is\nguaranteed with high probability to yield a predictor that generalizes after\nseeing only $O(\\log{d_s})$ environments.",
          "link": "http://arxiv.org/abs/2106.09913",
          "publishedOn": "2021-06-21T02:07:38.824Z",
          "wordCount": 570,
          "title": "Iterative Feature Matching: Toward Provable Domain Generalization with Logarithmic Environments. (arXiv:2106.09913v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kapishnikov_A/0/1/0/all/0/1\">Andrei Kapishnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1\">Subhashini Venugopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avci_B/0/1/0/all/0/1\">Besim Avci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wedin_B/0/1/0/all/0/1\">Ben Wedin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terry_M/0/1/0/all/0/1\">Michael Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolukbasi_T/0/1/0/all/0/1\">Tolga Bolukbasi</a>",
          "description": "Integrated Gradients (IG) is a commonly used feature attribution method for\ndeep neural networks. While IG has many desirable properties, the method often\nproduces spurious/noisy pixel attributions in regions that are not related to\nthe predicted class when applied to visual models. While this has been\npreviously noted, most existing solutions are aimed at addressing the symptoms\nby explicitly reducing the noise in the resulting attributions. In this work,\nwe show that one of the causes of the problem is the accumulation of noise\nalong the IG path. To minimize the effect of this source of noise, we propose\nadapting the attribution path itself -- conditioning the path not just on the\nimage but also on the model being explained. We introduce Adaptive Path Methods\n(APMs) as a generalization of path methods, and Guided IG as a specific\ninstance of an APM. Empirically, Guided IG creates saliency maps better aligned\nwith the model's prediction and the input image that is being explained. We\nshow through qualitative and quantitative experiments that Guided IG\noutperforms other, related methods in nearly every experiment.",
          "link": "http://arxiv.org/abs/2106.09788",
          "publishedOn": "2021-06-21T02:07:38.816Z",
          "wordCount": 657,
          "title": "Guided Integrated Gradients: An Adaptive Path Method for Removing Noise. (arXiv:2106.09788v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mauritz_R/0/1/0/all/0/1\">R.R. Mauritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nijweide_F/0/1/0/all/0/1\">F.P.J. Nijweide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goseling_J/0/1/0/all/0/1\">J. Goseling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keulen_M/0/1/0/all/0/1\">M. van Keulen</a>",
          "description": "In the field of data integration, data quality problems are often encountered\nwhen extracting, combining, and merging data. The probabilistic data\nintegration approach represents information about such problems as\nuncertainties in a probabilistic database. In this paper, we propose a\ndata-cleaning autoencoder capable of near-automatic data quality improvement.\nIt learns the structure and dependencies in the data to identify and correct\ndoubtful values. A theoretical framework is provided, and experiments show that\nit can remove significant amounts of noise from categorical and numeric\nprobabilistic data. Our method does not require clean data. We do, however,\nshow that manually cleaning a small fraction of the data significantly improves\nperformance.",
          "link": "http://arxiv.org/abs/2106.09764",
          "publishedOn": "2021-06-21T02:07:38.763Z",
          "wordCount": 561,
          "title": "Autoencoder-based cleaning in probabilistic databases. (arXiv:2106.09764v1 [cs.DB])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}