{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.01198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1\">Rajaswa Patil</a>",
          "description": "In this work, we present to the NLP community, and to the wider research\ncommunity as a whole, an application for the diachronic analysis of research\ncorpora. We open source an easy-to-use tool coined: DRIFT, which allows\nresearchers to track research trends and development over the years. The\nanalysis methods are collated from well-cited research works, with a few of our\nown methods added for good measure. Succinctly put, some of the analysis\nmethods are: keyword extraction, word clouds, predicting\ndeclining/stagnant/growing trends using Productivity, tracking bi-grams using\nAcceleration plots, finding the Semantic Drift of words, tracking trends using\nsimilarity, etc. To demonstrate the utility and efficacy of our tool, we\nperform a case study on the cs.CL corpus of the arXiv repository and draw\ninferences from the analysis methods. The toolkit and the associated code are\navailable here: https://github.com/rajaswa/DRIFT.",
          "link": "http://arxiv.org/abs/2107.01198",
          "publishedOn": "2021-07-20T02:04:41.716Z",
          "wordCount": 602,
          "title": "DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature. (arXiv:2107.01198v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Paniagua_V/0/1/0/all/0/1\">V&#xed;ctor Su&#xe1;rez-Paniagua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteley_W/0/1/0/all/0/1\">William Whiteley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>",
          "description": "Diagnostic or procedural coding of clinical notes aims to derive a coded\nsummary of disease-related information about patients. Such coding is usually\ndone manually in hospitals but could potentially be automated to improve the\nefficiency and accuracy of medical coding. Recent studies on deep learning for\nautomated medical coding achieved promising performances. However, the\nexplainability of these models is usually poor, preventing them to be used\nconfidently in supporting clinical practice. Another limitation is that these\nmodels mostly assume independence among labels, ignoring the complex\ncorrelation among medical codes which can potentially be exploited to improve\nthe performance. We propose a Hierarchical Label-wise Attention Network (HLAN),\nwhich aimed to interpret the model by quantifying importance (as attention\nweights) of words and sentences related to each of the labels. Secondly, we\npropose to enhance the major deep learning models with a label embedding (LE)\ninitialisation approach, which learns a dense, continuous vector representation\nand then injects the representation into the final layers and the label-wise\nattention layers in the models. We evaluated the methods using three settings\non the MIMIC-III discharge summaries: full codes, top-50 codes, and the UK NHS\nCOVID-19 shielding codes. Experiments were conducted to compare HLAN and LE\ninitialisation to the state-of-the-art neural network based methods. HLAN\nachieved the best Micro-level AUC and $F_1$ on the top-50 code prediction and\ncomparable results on the NHS COVID-19 shielding code prediction to other\nmodels. By highlighting the most salient words and sentences for each label,\nHLAN showed more meaningful and comprehensive model interpretation compared to\nits downgraded baselines and the CNN-based models. LE initialisation\nconsistently boosted most deep learning models for automated medical coding.",
          "link": "http://arxiv.org/abs/2010.15728",
          "publishedOn": "2021-07-20T02:04:41.578Z",
          "wordCount": 846,
          "title": "Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation. (arXiv:2010.15728v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1\">Gail Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yahav_E/0/1/0/all/0/1\">Eran Yahav</a>",
          "description": "What is the computational model behind a Transformer? Where recurrent neural\nnetworks have direct parallels in finite state machines, allowing clear\ndiscussion and thought around architecture variants or trained models,\nTransformers have no such familiar parallel. In this paper we aim to change\nthat, proposing a computational model for the transformer-encoder in the form\nof a programming language. We map the basic components of a transformer-encoder\n-- attention and feed-forward computation -- into simple primitives, around\nwhich we form a programming language: the Restricted Access Sequence Processing\nLanguage (RASP). We show how RASP can be used to program solutions to tasks\nthat could conceivably be learned by a Transformer, and how a Transformer can\nbe trained to mimic a RASP solution. In particular, we provide RASP programs\nfor histograms, sorting, and Dyck-languages. We further use our model to relate\ntheir difficulty in terms of the number of required layers and attention heads:\nanalyzing a RASP program implies a maximum number of heads and layers necessary\nto encode a task in a transformer. Finally, we see how insights gained from our\nabstraction might be used to explain phenomena seen in recent works.",
          "link": "http://arxiv.org/abs/2106.06981",
          "publishedOn": "2021-07-20T02:04:41.119Z",
          "wordCount": 651,
          "title": "Thinking Like Transformers. (arXiv:2106.06981v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Potdar_N/0/1/0/all/0/1\">Nihal Potdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1\">Anderson R. Avila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chao Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yiran Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>",
          "description": "End-to-end spoken language understanding (SLU) has recently attracted\nincreasing interest. Compared to the conventional tandem-based approach that\ncombines speech recognition and language understanding as separate modules, the\nnew approach extracts users' intentions directly from the speech signals,\nresulting in joint optimization and low latency. Such an approach, however, is\ntypically designed to process one intention at a time, which leads users to\ntake multiple rounds to fulfill their requirements while interacting with a\ndialogue system. In this paper, we propose a streaming end-to-end framework\nthat can process multiple intentions in an online and incremental way. The\nbackbone of our framework is a unidirectional RNN trained with the\nconnectionist temporal classification (CTC) criterion. By this design, an\nintention can be identified when sufficient evidence has been accumulated, and\nmultiple intentions can be identified sequentially. We evaluate our solution on\nthe Fluent Speech Commands (FSC) dataset and the intent detection accuracy is\nabout 97 % on all multi-intent settings. This result is comparable to the\nperformance of the state-of-the-art non-streaming models, but is achieved in an\nonline and incremental way. We also employ our model to a keyword spotting task\nusing the Google Speech Commands dataset and the results are also highly\npromising.",
          "link": "http://arxiv.org/abs/2105.10042",
          "publishedOn": "2021-07-20T02:04:41.090Z",
          "wordCount": 701,
          "title": "A Streaming End-to-End Framework For Spoken Language Understanding. (arXiv:2105.10042v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_B/0/1/0/all/0/1\">Bhumika Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1\">Anuj Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anjum/0/1/0/all/0/1\">Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katarya_R/0/1/0/all/0/1\">Rahul Katarya</a>",
          "description": "Cyberbullying is of extreme prevalence today. Online-hate comments, toxicity,\ncyberbullying amongst children and other vulnerable groups are only growing\nover online classes, and increased access to social platforms, especially post\nCOVID-19. It is paramount to detect and ensure minors' safety across social\nplatforms so that any violence or hate-crime is automatically detected and\nstrict action is taken against it. In our work, we explore binary\nclassification by using a combination of datasets from various social media\nplatforms that cover a wide range of cyberbullying such as sexism, racism,\nabusive, and hate-speech. We experiment through multiple models such as\nBi-LSTM, GloVe, state-of-the-art models like BERT, and apply a unique\npreprocessing technique by introducing a slang-abusive corpus, achieving a\nhigher precision in comparison to models without slang preprocessing.",
          "link": "http://arxiv.org/abs/2107.08902",
          "publishedOn": "2021-07-20T02:04:41.071Z",
          "wordCount": 626,
          "title": "Analysing Cyberbullying using Natural Language Processing by Understanding Jargon in Social Media. (arXiv:2107.08902v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.06110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "This paper models unsupervised learning of an identity-based pattern (or\ncopying) in speech called reduplication from raw continuous data with deep\nconvolutional neural networks. We use the ciwGAN architecture Begu\\v{s} (2021a;\narXiv:2006.02951) in which learning of meaningful representations in speech\nemerges from a requirement that the CNNs generate informative data. We propose\na technique to wug-test CNNs trained on speech and, based on four generative\ntests, argue that the network learns to represent an identity-based pattern in\nits latent space. By manipulating only two categorical variables in the latent\nspace, we can actively turn an unreduplicated form into a reduplicated form\nwith no other substantial changes to the output in the majority of cases. We\nalso argue that the network extends the identity-based pattern to unobserved\ndata. Exploration of how meaningful representations of identity-based patterns\nemerge in CNNs and how the latent space variables outside of the training range\ncorrelate with identity-based patterns in the output has general implications\nfor neural network interpretability.",
          "link": "http://arxiv.org/abs/2009.06110",
          "publishedOn": "2021-07-20T02:04:41.052Z",
          "wordCount": 628,
          "title": "Identity-Based Patterns in Deep Convolutional Networks: Generative Adversarial Phonology and Reduplication. (arXiv:2009.06110v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.12804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jon_J/0/1/0/all/0/1\">Josef Jon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>",
          "description": "This work demonstrates that, contrary to a common belief, using the objective\nwith independence assumption for modelling the span probability $P(a_s,a_e) =\nP(a_s)P(a_e)$ of span starting at position $a_s$ and ending at position $a_e$\nhas adverse effects. Therefore we propose multiple approaches to modelling\njoint probability $P(a_s,a_e)$ directly. Among those, we propose a compound\nobjective, composed from the joint probability while still keeping the\nobjective with independence assumption as an auxiliary objective. We find that\nthe compound objective is consistently superior or equal to other assumptions\nin exact match. Additionally, we identified common errors caused by the\nassumption of independence and manually checked the counterpart predictions,\ndemonstrating the impact of the compound objective on the real examples. Our\nfindings are supported via experiments with three extractive QA models (BIDAF,\nBERT, ALBERT) over six datasets and our code, individual results and manual\nanalysis are available online.",
          "link": "http://arxiv.org/abs/2008.12804",
          "publishedOn": "2021-07-20T02:04:41.021Z",
          "wordCount": 635,
          "title": "Rethinking the Objectives of Extractive Question Answering. (arXiv:2008.12804v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.09152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Ye Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Quan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>",
          "description": "Knowledge distillation has been proven to be effective in model acceleration\nand compression. It allows a small network to learn to generalize in the same\nway as a large network. Recent successes in pre-training suggest the\neffectiveness of transferring model parameters. Inspired by this, we\ninvestigate methods of model acceleration and compression in another line of\nresearch. We propose Weight Distillation to transfer the knowledge in the large\nnetwork parameters through a parameter generator. Our experiments on WMT16\nEn-Ro, NIST12 Zh-En, and WMT14 En-De machine translation tasks show that weight\ndistillation can train a small network that is 1.88~2.94x faster than the large\nnetwork but with competitive performance. With the same sized small network,\nweight distillation can outperform knowledge distillation by 0.51~1.82 BLEU\npoints.",
          "link": "http://arxiv.org/abs/2009.09152",
          "publishedOn": "2021-07-20T02:04:40.957Z",
          "wordCount": 606,
          "title": "Weight Distillation: Transferring the Knowledge in Neural Network Parameters. (arXiv:2009.09152v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Ye Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>",
          "description": "The large attention-based encoder-decoder network (Transformer) has become\nprevailing recently due to its effectiveness. But the high computation\ncomplexity of its decoder raises the inefficiency issue. By examining the\nmathematic formulation of the decoder, we show that under some mild conditions,\nthe architecture could be simplified by compressing its sub-layers, the basic\nbuilding block of Transformer, and achieves a higher parallelism. We thereby\npropose Compressed Attention Network, whose decoder layer consists of only one\nsub-layer instead of three. Extensive experiments on 14 WMT machine translation\ntasks show that our model is 1.42x faster with performance on par with a strong\nbaseline. This strong baseline is already 2x faster than the widely used\nstandard baseline without loss in performance.",
          "link": "http://arxiv.org/abs/2101.00542",
          "publishedOn": "2021-07-20T02:04:40.927Z",
          "wordCount": 580,
          "title": "An Efficient Transformer Decoder with Compressed Sub-layers. (arXiv:2101.00542v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.09132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jieh-Sheng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiang_J/0/1/0/all/0/1\">Jieh Hsiang</a>",
          "description": "Generative models, such as GPT-2, have demonstrated impressive results\nrecently. A fundamental question we'd like to address is: where did the\ngenerated text come from? This work is our initial effort toward answering the\nquestion by using prior art search. The purpose of the prior art search is to\nfind the most similar prior text in the training data of GPT-2. We take a\nreranking approach and apply it to the patent domain. Specifically, we\npre-train GPT-2 models from scratch by using the patent data from the USPTO.\nThe input for the prior art search is the patent text generated by the GPT-2\nmodel. We also pre-trained BERT models from scratch for converting patent text\nto embeddings. The steps of reranking are: (1) search the most similar text in\nthe training data of GPT-2 by taking a bag-of-word ranking approach (BM25), (2)\nconvert the search results in text format to BERT embeddings, and (3) provide\nthe final result by ranking the BERT embeddings based on their similarities\nwith the patent text generated by GPT-2. The experiments in this work show that\nsuch reranking is better than ranking with embeddings alone. However, our mixed\nresults also indicate that calculating the semantic similarities among long\ntext spans is still challenging. To our knowledge, this work is the first to\nimplement a reranking system to identify retrospectively the most similar\ninputs to a GPT model based on its output.",
          "link": "http://arxiv.org/abs/2009.09132",
          "publishedOn": "2021-07-20T02:04:40.905Z",
          "wordCount": 728,
          "title": "Prior Art Search and Reranking for Generated Patent Text. (arXiv:2009.09132v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00858",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_J/0/1/0/all/0/1\">Jian Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_W/0/1/0/all/0/1\">Wenning Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>",
          "description": "In this paper, several works are proposed to address practical challenges for\ndeploying RNN Transducer (RNN-T) based speech recognition system. These\nchallenges are adapting a well-trained RNN-T model to a new domain without\ncollecting the audio data, obtaining time stamps and confidence scores at word\nlevel. The first challenge is solved with a splicing data method which\nconcatenates the speech segments extracted from the source domain data. To get\nthe time stamp, a phone prediction branch is added to the RNN-T model by\nsharing the encoder for the purpose of force alignment. Finally, we obtain\nword-level confidence scores by utilizing several types of features calculated\nduring decoding and from confusion network. Evaluated with Microsoft production\ndata, the splicing data adaptation method improves the baseline and adaptation\nwith the text to speech method by 58.03% and 15.25% relative word error rate\nreduction, respectively. The proposed time stamping method can get less than\n50ms word timing difference from the ground truth alignment on average while\nmaintaining the recognition accuracy of the RNN-T model. We also obtain high\nconfidence annotation performance with limited computation cost.",
          "link": "http://arxiv.org/abs/2105.00858",
          "publishedOn": "2021-07-20T02:04:40.886Z",
          "wordCount": 657,
          "title": "On Addressing Practical Challenges for RNN-Transducer. (arXiv:2105.00858v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.00908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chesi_C/0/1/0/all/0/1\">Cristiano Chesi</a>",
          "description": "A cognitively plausible parsing algorithm should perform like the human\nparser in critical contexts. Here I propose an adaptation of Earley's parsing\nalgorithm, suitable for Phase-based Minimalist Grammars (PMG, Chesi 2012), that\nis able to predict complexity effects in performance. Focusing on self-paced\nreading experiments of object clefts sentences (Warren & Gibson 2005) I will\nassociate to parsing a complexity metric based on cued features to be retrieved\nat the verb segment (Feature Retrieval & Encoding Cost, FREC). FREC is\ncrucially based on the usage of memory predicted by the discussed parsing\nalgorithm and it correctly fits with the reading time revealed.",
          "link": "http://arxiv.org/abs/1906.00908",
          "publishedOn": "2021-07-20T02:04:40.864Z",
          "wordCount": 580,
          "title": "Phase-based Minimalist Parsing and complexity in non-local dependencies. (arXiv:1906.00908v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thanh-Dung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noumeir_R/0/1/0/all/0/1\">Rita Noumeir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambaud_J/0/1/0/all/0/1\">Jerome Rambaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sans_G/0/1/0/all/0/1\">Guillaume Sans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jouvet_P/0/1/0/all/0/1\">Philippe Jouvet</a>",
          "description": "The rapid progress in clinical data management systems and artificial\nintelligence approaches enable the era of personalized medicine. Intensive care\nunits (ICUs) are the ideal clinical research environment for such development\nbecause they collect many clinical data and are highly computerized\nenvironments. We designed a retrospective clinical study on a prospective ICU\ndatabase using clinical natural language to help in the early diagnosis of\nheart failure in critically ill children. The methodology consisted of\nempirical experiments of a learning algorithm to learn the hidden\ninterpretation and presentation of the French clinical note data. This study\nincluded 1386 patients' clinical notes with 5444 single lines of notes. There\nwere 1941 positive cases (36 % of total) and 3503 negative cases classified by\ntwo independent physicians using a standardized approach. The multilayer\nperceptron neural network outperforms other discriminative and generative\nclassifiers. Consequently, the proposed framework yields an overall\nclassification performance with 89 % accuracy, 88 % recall, and 89 % precision.\nFurthermore, a generative autoencoder learning algorithm was proposed to\nleverage the sparsity reduction that achieved 91% accuracy, 91% recall, and 91%\nprecision. This study successfully applied learning representation and machine\nlearning algorithms to detect heart failure from clinical natural language in a\nsingle French institution. Further work is needed to use the same methodology\nin other institutions and other languages.",
          "link": "http://arxiv.org/abs/2104.03969",
          "publishedOn": "2021-07-20T02:04:40.806Z",
          "wordCount": 716,
          "title": "Detecting of a Patient's Condition From Clinical Narratives Using Natural Language Representation. (arXiv:2104.03969v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1\">Aishwarya Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan Tur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>",
          "description": "Inspired by recent work in meta-learning and generative teaching networks, we\npropose a framework called Generative Conversational Networks, in which\nconversational agents learn to generate their own labelled training data (given\nsome seed data) and then train themselves from that data to perform a given\ntask. We use reinforcement learning to optimize the data generation process\nwhere the reward signal is the agent's performance on the task. The task can be\nany language-related task, from intent detection to full task-oriented\nconversations. In this work, we show that our approach is able to generalise\nfrom seed data and performs well in limited data and limited computation\nsettings, with significant gains for intent detection and slot tagging across\nmultiple datasets: ATIS, TOD, SNIPS, and Restaurants8k. We show an average\nimprovement of 35% in intent detection and 21% in slot tagging over a baseline\nmodel trained from the seed data. We also conduct an analysis of the novelty of\nthe generated data and provide generated examples for intent detection, slot\ntagging, and non-goal oriented conversations.",
          "link": "http://arxiv.org/abs/2106.08484",
          "publishedOn": "2021-07-20T02:04:40.786Z",
          "wordCount": 636,
          "title": "Generative Conversational Networks. (arXiv:2106.08484v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.09410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Deokgun Park</a>",
          "description": "Despite recent advances in many application-specific domains, we do not know\nhow to build a human-level artificial intelligence (HLAI). We conjecture that\nlearning from others' experience with the language is the essential\ncharacteristic that distinguishes human intelligence from the rest. Humans can\nupdate the action-value function with the verbal description as if they\nexperience states, actions, and corresponding rewards sequences firsthand. In\nthis paper, we present a classification of intelligence according to how\nindividual agents learn and propose a definition and a test for HLAI. The main\nidea is that language acquisition without explicit rewards can be a sufficient\ntest for HLAI.",
          "link": "http://arxiv.org/abs/2011.09410",
          "publishedOn": "2021-07-20T02:04:40.762Z",
          "wordCount": 575,
          "title": "A Definition and a Test for Human-Level Artificial Intelligence. (arXiv:2011.09410v4 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karakanta_A/0/1/0/all/0/1\">Alina Karakanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>",
          "description": "With the increased audiovisualisation of communication, the need for live\nsubtitles in multilingual events is more relevant than ever. In an attempt to\nautomatise the process, we aim at exploring the feasibility of simultaneous\nspeech translation (SimulST) for live subtitling. However, the word-for-word\nrate of generation of SimulST systems is not optimal for displaying the\nsubtitles in a comprehensible and readable way. In this work, we adapt SimulST\nsystems to predict subtitle breaks along with the translation. We then propose\na display mode that exploits the predicted break structure by presenting the\nsubtitles in scrolling lines. We compare our proposed mode with a display 1)\nword-for-word and 2) in blocks, in terms of reading speed and delay.\nExperiments on three language pairs (en$\\rightarrow$it, de, fr) show that\nscrolling lines is the only mode achieving an acceptable reading speed while\nkeeping delay close to a 4-second threshold. We argue that simultaneous\ntranslation for readable live subtitles still faces challenges, the main one\nbeing poor translation quality, and propose directions for steering future\nresearch.",
          "link": "http://arxiv.org/abs/2107.08807",
          "publishedOn": "2021-07-20T02:04:40.743Z",
          "wordCount": 622,
          "title": "Simultaneous Speech Translation for Live Subtitling: from Delay to Display. (arXiv:2107.08807v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.",
          "link": "http://arxiv.org/abs/2107.08264",
          "publishedOn": "2021-07-20T02:04:40.723Z",
          "wordCount": 685,
          "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jie_C/0/1/0/all/0/1\">Cheng Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Da Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zigeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>",
          "description": "With the increasing scale of search engine marketing, designing an efficient\nbidding system is becoming paramount for the success of e-commerce companies.\nThe critical challenges faced by a modern industrial-level bidding system\ninclude: 1. the catalog is enormous, and the relevant bidding features are of\nhigh sparsity; 2. the large volume of bidding requests induces significant\ncomputation burden to both the offline and online serving. Leveraging\nextraneous user-item information proves essential to mitigate the sparsity\nissue, for which we exploit the natural language signals from the users' query\nand the contextual knowledge from the products. In particular, we extract the\nvector representations of ads via the Transformer model and leverage their\ngeometric relation to building collaborative bidding predictions via\nclustering. The two-step procedure also significantly reduces the computation\nstress of bid evaluation and optimization. In this paper, we introduce the\nend-to-end structure of the bidding system for search engine marketing for\nWalmart e-commerce, which successfully handles tens of millions of bids each\nday. We analyze the online and offline performances of our approach and discuss\nhow we find it as a production-efficient solution.",
          "link": "http://arxiv.org/abs/2106.12700",
          "publishedOn": "2021-07-20T02:04:40.640Z",
          "wordCount": 657,
          "title": "An Efficient Group-based Search Engine Marketing System for E-Commerce. (arXiv:2106.12700v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anderson_M/0/1/0/all/0/1\">Mark Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>",
          "description": "We present the system submission from the FASTPARSE team for the EUD Shared\nTask at IWPT 2021. We engaged in the task last year by focusing on efficiency.\nThis year we have focused on experimenting with new ideas on a limited time\nbudget. Our system is based on splitting the EUD graph into several trees,\nbased on linguistic criteria. We predict these trees using a sequence-labelling\nparser and combine them into an EUD graph. The results were relatively poor,\nalthough not a total disaster and could probably be improved with some\npolishing of the system's rough edges.",
          "link": "http://arxiv.org/abs/2106.13155",
          "publishedOn": "2021-07-20T02:04:40.621Z",
          "wordCount": 575,
          "title": "Splitting EUD graphs into trees: A quick and clatty approach. (arXiv:2106.13155v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1\">Dana Ruiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1\">Josef van Genabith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espana_Bonet_C/0/1/0/all/0/1\">Cristina Espa&#xf1;a-Bonet</a>",
          "description": "For most language combinations, parallel data is either scarce or simply\nunavailable. To address this, unsupervised machine translation (UMT) exploits\nlarge amounts of monolingual data by using synthetic data generation techniques\nsuch as back-translation and noising, while self-supervised NMT (SSNMT)\nidentifies parallel sentences in smaller comparable data and trains on them. To\ndate, the inclusion of UMT data generation techniques in SSNMT has not been\ninvestigated. We show that including UMT techniques into SSNMT significantly\noutperforms SSNMT and UMT on all tested language pairs, with improvements of up\nto +4.3 BLEU, +50.8 BLEU, +51.5 over SSNMT, statistical UMT and hybrid UMT,\nrespectively, on Afrikaans to English. We further show that the combination of\nmultilingual denoising autoencoding, SSNMT with backtranslation and bilingual\nfinetuning enables us to learn machine translation even for distant language\npairs for which only small amounts of monolingual data are available, e.g.\nyielding BLEU scores of 11.6 (English to Swahili).",
          "link": "http://arxiv.org/abs/2107.08772",
          "publishedOn": "2021-07-20T02:04:40.442Z",
          "wordCount": 604,
          "title": "Integrating Unsupervised Data Generation into Self-Supervised Neural Machine Translation for Low-Resource Languages. (arXiv:2107.08772v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.01670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertie Vidgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>",
          "description": "Data-driven analysis and detection of abusive online content covers many\ndifferent tasks, phenomena, contexts, and methodologies. This paper\nsystematically reviews abusive language dataset creation and content in\nconjunction with an open website for cataloguing abusive language data. This\ncollection of knowledge leads to a synthesis providing evidence-based\nrecommendations for practitioners working with this complex and highly diverse\ndata.",
          "link": "http://arxiv.org/abs/2004.01670",
          "publishedOn": "2021-07-20T02:04:40.423Z",
          "wordCount": 533,
          "title": "Directions in Abusive Language Training Data: Garbage In, Garbage Out. (arXiv:2004.01670v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slavnov_S/0/1/0/all/0/1\">Sergey Slavnov</a>",
          "description": "We propose a concrete surface representation of abstract categorial grammars\nin the category of word cobordisms or cowordisms for short, which are certain\nbipartite graphs decorated with words in a given alphabet, generalizing linear\nlogic proof-nets. We also introduce and study linear logic grammars, directly\nbased on cobordisms and using classical multiplicative linear logic as a typing\nsystem.",
          "link": "http://arxiv.org/abs/2107.08728",
          "publishedOn": "2021-07-20T02:04:40.362Z",
          "wordCount": 519,
          "title": "Cobordisms and commutative categorial grammars. (arXiv:2107.08728v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2007.12988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1\">Thayer Alshaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_J/0/1/0/all/0/1\">Jane L. Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1\">Michael V. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minot_J/0/1/0/all/0/1\">Joshua R. Minot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dewhurst_D/0/1/0/all/0/1\">David R. Dewhurst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reagan_A/0/1/0/all/0/1\">Andrew J. Reagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1\">Christopher M. Danforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">Peter Sheridan Dodds</a>",
          "description": "In real-time, social media data strongly imprints world events, popular\nculture, and day-to-day conversations by millions of ordinary people at a scale\nthat is scarcely conventionalized and recorded. Vitally, and absent from many\nstandard corpora such as books and news archives, sharing and commenting\nmechanisms are native to social media platforms, enabling us to quantify social\namplification (i.e., popularity) of trending storylines and contemporary\ncultural phenomena. Here, we describe Storywrangler, a natural language\nprocessing instrument designed to carry out an ongoing, day-scale curation of\nover 100 billion tweets containing roughly 1 trillion 1-grams from 2008 to\n2021. For each day, we break tweets into unigrams, bigrams, and trigrams\nspanning over 100 languages. We track n-gram usage frequencies, and generate\nZipf distributions, for words, hashtags, handles, numerals, symbols, and\nemojis. We make the data set available through an interactive time series\nviewer, and as downloadable time series and daily distributions. Although\nStorywrangler leverages Twitter data, our method of extracting and tracking\ndynamic changes of n-grams can be extended to any similar social media\nplatform. We showcase a few examples of the many possible avenues of study we\naim to enable including how social amplification can be visualized through\n'contagiograms'. We also present some example case studies that bridge n-gram\ntime series with disparate data sources to explore sociotechnical dynamics of\nfamous individuals, box office success, and social unrest.",
          "link": "http://arxiv.org/abs/2007.12988",
          "publishedOn": "2021-07-20T02:04:40.344Z",
          "wordCount": 786,
          "title": "Storywrangler: A massive exploratorium for sociolinguistic, cultural, socioeconomic, and political timelines using Twitter. (arXiv:2007.12988v5 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Nianlong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ash_E/0/1/0/all/0/1\">Elliott Ash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahnloser_R/0/1/0/all/0/1\">Richard H.R. Hahnloser</a>",
          "description": "We introduce MemSum (Multi-step Episodic Markov decision process extractive\nSUMmarizer), a reinforcement-learning-based extractive summarizer enriched at\nany given time step with information on the current extraction history. Similar\nto previous models in this vein, MemSum iteratively selects sentences into the\nsummary. Our innovation is in considering a broader information set when\nsummarizing that would intuitively also be used by humans in this task: 1) the\ntext content of the sentence, 2) the global text context of the rest of the\ndocument, and 3) the extraction history consisting of the set of sentences that\nhave already been extracted. With a lightweight architecture, MemSum\nnonetheless obtains state-of-the-art test-set performance (ROUGE score) on long\ndocument datasets (PubMed, arXiv, and GovReport). Supporting analysis\ndemonstrates that the added awareness of extraction history gives MemSum\nrobustness against redundancy in the source document.",
          "link": "http://arxiv.org/abs/2107.08929",
          "publishedOn": "2021-07-20T02:04:40.322Z",
          "wordCount": 575,
          "title": "MemSum: Extractive Summarization of Long Documents using Multi-step Episodic Markov Decision Processes. (arXiv:2107.08929v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08721",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Chen_Q/0/1/0/all/0/1\">Qinkai Chen</a>",
          "description": "News events can greatly influence equity markets. In this paper, we are\ninterested in predicting the short-term movement of stock prices after\nfinancial news events using only the headlines of the news. To achieve this\ngoal, we introduce a new text mining method called Fine-Tuned\nContextualized-Embedding Recurrent Neural Network (FT-CE-RNN). Compared with\nprevious approaches which use static vector representations of the news (static\nembedding), our model uses contextualized vector representations of the\nheadlines (contextualized embeddings) generated from Bidirectional Encoder\nRepresentations from Transformers (BERT). Our model obtains the\nstate-of-the-art result on this stock movement prediction task. It shows\nsignificant improvement compared with other baseline models, in both accuracy\nand trading simulations. Through various trading simulations based on millions\nof headlines from Bloomberg News, we demonstrate the ability of this model in\nreal scenarios.",
          "link": "http://arxiv.org/abs/2107.08721",
          "publishedOn": "2021-07-20T02:04:40.303Z",
          "wordCount": 590,
          "title": "Stock Movement Prediction with Financial News using Contextualized Embedding from BERT. (arXiv:2107.08721v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fabiano_F/0/1/0/all/0/1\">Francesco Fabiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1\">Biplav Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenchner_J/0/1/0/all/0/1\">Jonathan Lenchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horesh_L/0/1/0/all/0/1\">Lior Horesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1\">Francesca Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganapini_M/0/1/0/all/0/1\">Marianna Bergamaschi Ganapini</a>",
          "description": "Epistemic Planning (EP) refers to an automated planning setting where the\nagent reasons in the space of knowledge states and tries to find a plan to\nreach a desirable state from the current state. Its general form, the\nMulti-agent Epistemic Planning (MEP) problem involves multiple agents who need\nto reason about both the state of the world and the information flow between\nagents. In a MEP problem, multiple approaches have been developed recently with\nvarying restrictions, such as considering only the concept of knowledge while\nnot allowing the idea of belief, or not allowing for ``complex\" modal operators\nsuch as those needed to handle dynamic common knowledge. While the diversity of\napproaches has led to a deeper understanding of the problem space, the lack of\na standardized way to specify MEP problems independently of solution approaches\nhas created difficulties in comparing performance of planners, identifying\npromising techniques, exploring new strategies like ensemble methods, and\nmaking it easy for new researchers to contribute to this research area. To\naddress the situation, we propose a unified way of specifying EP problems - the\nEpistemic Planning Domain Definition Language, E-PDDL. We show that E-PPDL can\nbe supported by leading MEP planners and provide corresponding parser code that\ntranslates EP problems specified in E-PDDL into (M)EP problems that can be\nhandled by several planners. This work is also useful in building more general\nepistemic planning environments where we envision a meta-cognitive module that\ntakes a planning problem in E-PDDL, identifies and assesses some of its\nfeatures, and autonomously decides which planner is the best one to solve it.",
          "link": "http://arxiv.org/abs/2107.08739",
          "publishedOn": "2021-07-20T02:04:40.282Z",
          "wordCount": 718,
          "title": "E-PDDL: A Standardized Way of Defining Epistemic Planning Problems. (arXiv:2107.08739v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_P/0/1/0/all/0/1\">Pan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>",
          "description": "A proactive dialogue system has the ability to proactively lead the\nconversation. Different from the general chatbots which only react to the user,\nproactive dialogue systems can be used to achieve some goals, e.g., to\nrecommend some items to the user. Background knowledge is essential to enable\nsmooth and natural transitions in dialogue. In this paper, we propose a new\nmulti-task learning framework for retrieval-based knowledge-grounded proactive\ndialogue. To determine the relevant knowledge to be used, we frame knowledge\nprediction as a complementary task and use explicit signals to supervise its\nlearning. The final response is selected according to the predicted knowledge,\nthe goal to achieve, and the context. Experimental results show that explicit\nmodeling of knowledge prediction and goal selection can greatly improve the\nfinal response selection. Our code is available at\nhttps://github.com/DaoD/KPN/.",
          "link": "http://arxiv.org/abs/2107.08329",
          "publishedOn": "2021-07-20T02:04:40.260Z",
          "wordCount": 581,
          "title": "Proactive Retrieval-based Chatbots based on Relevant Knowledge and Goals. (arXiv:2107.08329v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hegel_A/0/1/0/all/0/1\">Allison Hegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Marina Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peaslee_G/0/1/0/all/0/1\">Genevieve Peaslee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roof_B/0/1/0/all/0/1\">Brendan Roof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elwany_E/0/1/0/all/0/1\">Emad Elwany</a>",
          "description": "Large, pre-trained transformer models like BERT have achieved\nstate-of-the-art results on document understanding tasks, but most\nimplementations can only consider 512 tokens at a time. For many real-world\napplications, documents can be much longer, and the segmentation strategies\ntypically used on longer documents miss out on document structure and\ncontextual information, hurting their results on downstream tasks. In our work\non legal agreements, we find that visual cues such as layout, style, and\nplacement of text in a document are strong features that are crucial to\nachieving an acceptable level of accuracy on long documents. We measure the\nimpact of incorporating such visual cues, obtained via computer vision methods,\non the accuracy of document understanding tasks including document\nsegmentation, entity extraction, and attribute classification. Our method of\nsegmenting documents based on structural metadata out-performs existing methods\non four long-document understanding tasks as measured on the Contract\nUnderstanding Atticus Dataset.",
          "link": "http://arxiv.org/abs/2107.08128",
          "publishedOn": "2021-07-20T02:04:40.191Z",
          "wordCount": 602,
          "title": "The Law of Large Documents: Understanding the Structure of Legal Contracts Using Visual Cues. (arXiv:2107.08128v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Braun_R/0/1/0/all/0/1\">Rudolf A. Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madikeri_S/0/1/0/all/0/1\">Srikanth Madikeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>",
          "description": "A common problem for automatic speech recognition systems is how to recognize\nwords that they did not see during training. Currently there is no established\nmethod of evaluating different techniques for tackling this problem. We propose\nusing the CommonVoice dataset to create test sets for multiple languages which\nhave a high out-of-vocabulary (OOV) ratio relative to a training set and\nrelease a new tool for calculating relevant performance metrics. We then\nevaluate, within the context of a hybrid ASR system, how much better subword\nmodels are at recognizing OOVs, and how much benefit one can get from\nincorporating OOV-word information into an existing system by modifying WFSTs.\nAdditionally, we propose a new method for modifying a subword-based language\nmodel so as to better recognize OOV-words. We showcase very large improvements\nin OOV-word recognition and make both the data and code available.",
          "link": "http://arxiv.org/abs/2107.08091",
          "publishedOn": "2021-07-20T02:04:40.151Z",
          "wordCount": 592,
          "title": "A Comparison of Methods for OOV-word Recognition on a New Public Dataset. (arXiv:2107.08091v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.05357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazumder_S/0/1/0/all/0/1\">Sahisnu Mazumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>",
          "description": "Classifying and resolving coreferences of objects (e.g., product names) and\nattributes (e.g., product aspects) in opinionated reviews is crucial for\nimproving the opinion mining performance. However, the task is challenging as\none often needs to consider domain-specific knowledge (e.g., iPad is a tablet\nand has aspect resolution) to identify coreferences in opinionated reviews.\nAlso, compiling a handcrafted and curated domain-specific knowledge base for\neach domain is very time consuming and arduous. This paper proposes an approach\nto automatically mine and leverage domain-specific knowledge for classifying\nobjects and attribute coreferences. The approach extracts domain-specific\nknowledge from unlabeled review data and trains a knowledgeaware neural\ncoreference classification model to leverage (useful) domain knowledge together\nwith general commonsense knowledge for the task. Experimental evaluation on\nrealworld datasets involving five domains (product types) shows the\neffectiveness of the approach.",
          "link": "http://arxiv.org/abs/2010.05357",
          "publishedOn": "2021-07-20T02:04:40.129Z",
          "wordCount": 615,
          "title": "A Knowledge-Driven Approach to Classifying Object and Attribute Coreferences in Opinion Mining. (arXiv:2010.05357v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fanton_M/0/1/0/all/0/1\">Margherita Fanton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonaldi_H/0/1/0/all/0/1\">Helena Bonaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekiroglu_S/0/1/0/all/0/1\">Serra Sinem Tekiroglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerini_M/0/1/0/all/0/1\">Marco Guerini</a>",
          "description": "Undermining the impact of hateful content with informed and non-aggressive\nresponses, called counter narratives, has emerged as a possible solution for\nhaving healthier online communities. Thus, some NLP studies have started\naddressing the task of counter narrative generation. Although such studies have\nmade an effort to build hate speech / counter narrative (HS/CN) datasets for\nneural generation, they fall short in reaching either high-quality and/or\nhigh-quantity. In this paper, we propose a novel human-in-the-loop data\ncollection methodology in which a generative language model is refined\niteratively by using its own data from the previous loops to generate new\ntraining samples that experts review and/or post-edit. Our experiments\ncomprised several loops including dynamic variations. Results show that the\nmethodology is scalable and facilitates diverse, novel, and cost-effective data\ncollection. To our knowledge, the resulting dataset is the only expert-based\nmulti-target HS/CN dataset available to the community.",
          "link": "http://arxiv.org/abs/2107.08720",
          "publishedOn": "2021-07-20T02:04:40.103Z",
          "wordCount": 603,
          "title": "Human-in-the-Loop for Data Collection: a Multi-Target Counter Narrative Dataset to Fight Online Hate Speech. (arXiv:2107.08720v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1\">Roi Pomerantz</a>",
          "description": "We present Translatotron 2, a neural direct speech-to-speech translation\nmodel that can be trained end-to-end. Translatotron 2 consists of a speech\nencoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention\nmodule that connects all the previous three components. Experimental results\nsuggest that Translatotron 2 outperforms the original Translatotron by a large\nmargin in terms of translation quality and predicted speech naturalness, and\ndrastically improves the robustness of the predicted speech by mitigating\nover-generation, such as babbling or long pause. We also propose a new method\nfor retaining the source speaker's voice in the translated speech. The trained\nmodel is restricted to retain the source speaker's voice, and unlike the\noriginal Translatotron, it is not able to generate speech in a different\nspeaker's voice, making the model more robust for production deployment, by\nmitigating potential misuse for creating spoofing audio artifacts. When the new\nmethod is used together with a simple concatenation-based data augmentation,\nthe trained Translatotron 2 model is able to retain each speaker's voice for\ninput with speaker turns.",
          "link": "http://arxiv.org/abs/2107.08661",
          "publishedOn": "2021-07-20T02:04:40.071Z",
          "wordCount": 614,
          "title": "Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nyoungwoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Suwon Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Ho-Jin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myaeng_S/0/1/0/all/0/1\">Sung-Hyun Myaeng</a>",
          "description": "In multi-modal dialogue systems, it is important to allow the use of images\nas part of a multi-turn conversation. Training such dialogue systems generally\nrequires a large-scale dataset consisting of multi-turn dialogues that involve\nimages, but such datasets rarely exist. In response, this paper proposes a 45k\nmulti-modal dialogue dataset created with minimal human intervention. Our\nmethod to create such a dataset consists of (1) preparing and pre-processing\ntext dialogue datasets, (2) creating image-mixed dialogues by using a\ntext-to-image replacement technique, and (3) employing a\ncontextual-similarity-based filtering step to ensure the contextual coherence\nof the dataset. To evaluate the validity of our dataset, we devise a simple\nretrieval model for dialogue sentence prediction tasks. Automatic metrics and\nhuman evaluation results on such tasks show that our dataset can be effectively\nused as training data for multi-modal dialogue systems which require an\nunderstanding of images and text in a context-aware manner. Our dataset and\ngeneration code is available at\nhttps://github.com/shh1574/multi-modal-dialogue-dataset.",
          "link": "http://arxiv.org/abs/2107.08685",
          "publishedOn": "2021-07-20T02:04:40.007Z",
          "wordCount": 606,
          "title": "Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images. (arXiv:2107.08685v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1\">Ishika Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gargi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1\">Ashutosh Modi</a>",
          "description": "Recently, text world games have been proposed to enable artificial agents to\nunderstand and reason about real-world scenarios. These text-based games are\nchallenging for artificial agents, as it requires understanding and interaction\nusing natural language in a partially observable environment. In this paper, we\nimprove the semantic understanding of the agent by proposing a simple RL with\nLM framework where we use transformer-based language models with Deep RL\nmodels. We perform a detailed study of our framework to demonstrate how our\nmodel outperforms all existing agents on the popular game, Zork1, to achieve a\nscore of 44.7, which is 1.6 higher than the state-of-the-art model. Our\nproposed approach also performs comparably to the state-of-the-art models on\nthe other set of text games.",
          "link": "http://arxiv.org/abs/2107.08408",
          "publishedOn": "2021-07-20T02:04:39.987Z",
          "wordCount": 581,
          "title": "Pre-trained Language Models as Prior Knowledge for Playing Text-based Games. (arXiv:2107.08408v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gantt_W/0/1/0/all/0/1\">William Gantt</a>",
          "description": "Semantic role labeling (SRL) -- identifying the semantic relationships\nbetween a predicate and other constituents in the same sentence -- is a\nwell-studied task in natural language understanding (NLU). However, many of\nthese relationships are evident only at the level of the document, as a role\nfor a predicate in one sentence may often be filled by an argument in a\ndifferent one. This more general task, known as implicit semantic role labeling\nor argument linking, has received increased attention in recent years, as\nresearchers have recognized its centrality to information extraction and NLU.\nThis paper surveys the literature on argument linking and identifies several\nnotable shortcomings of existing approaches that indicate the paths along which\nfuture research effort could most profitably be spent.",
          "link": "http://arxiv.org/abs/2107.08523",
          "publishedOn": "2021-07-20T02:04:39.968Z",
          "wordCount": 549,
          "title": "Argument Linking: A Survey and Forecast. (arXiv:2107.08523v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geng_B/0/1/0/all/0/1\">Binzong Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1\">Fajie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiancheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>",
          "description": "This ability to learn consecutive tasks without forgetting how to perform\npreviously trained problems is essential for developing an online dialogue\nsystem. This paper proposes an effective continual learning for the\ntask-oriented dialogue system with iterative network pruning, expanding and\nmasking (TPEM), which preserves performance on previously encountered tasks\nwhile accelerating learning progress on subsequent tasks. Specifically, TPEM\n(i) leverages network pruning to keep the knowledge for old tasks, (ii) adopts\nnetwork expanding to create free weights for new tasks, and (iii) introduces\ntask-specific network masking to alleviate the negative impact of fixed weights\nof old tasks on new tasks. We conduct extensive experiments on seven different\ntasks from three benchmark datasets and show empirically that TPEM leads to\nsignificantly improved results over the strong competitors. For\nreproducibility, we submit the code and data at:\nhttps://github.com/siat-nlp/TPEM",
          "link": "http://arxiv.org/abs/2107.08173",
          "publishedOn": "2021-07-20T02:04:39.940Z",
          "wordCount": 598,
          "title": "Continual Learning for Task-oriented Dialogue System with Iterative Network Pruning, Expanding and Masking. (arXiv:2107.08173v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bian_N/0/1/0/all/0/1\">Ning Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Ben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>",
          "description": "Despite recent success in machine reading comprehension (MRC), learning\nhigh-quality MRC models still requires large-scale labeled training data, even\nusing strong pre-trained language models (PLMs). The pre-training tasks for\nPLMs are not question-answering or MRC-based tasks, making existing PLMs unable\nto be directly used for unsupervised MRC. Specifically, MRC aims to spot an\naccurate answer span from the given document, but PLMs focus on token filling\nin sentences. In this paper, we propose a new framework for unsupervised MRC.\nFirstly, we propose to learn to spot answer spans in documents via\nself-supervised learning, by designing a self-supervision pretext task for MRC\n- Spotting-MLM. Solving this task requires capturing deep interactions between\nsentences in documents. Secondly, we apply a simple sentence rewriting strategy\nin the inference stage to alleviate the expression mismatch between questions\nand documents. Experiments show that our method achieves a new state-of-the-art\nperformance for unsupervised MRC.",
          "link": "http://arxiv.org/abs/2107.08582",
          "publishedOn": "2021-07-20T02:04:39.921Z",
          "wordCount": 593,
          "title": "Bridging the Gap between Language Model and Reading Comprehension: Unsupervised MRC via Self-Supervision. (arXiv:2107.08582v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jack Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1\">Raphael Lenain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1\">Udeepa Meepegama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1\">Emil Fristed</a>",
          "description": "We introduce ParaBLEU, a paraphrase representation learning model and\nevaluation metric for text generation. Unlike previous approaches, ParaBLEU\nlearns to understand paraphrasis using generative conditioning as a pretraining\nobjective. ParaBLEU correlates more strongly with human judgements than\nexisting metrics, obtaining new state-of-the-art results on the 2017 WMT\nMetrics Shared Task. We show that our model is robust to data scarcity,\nexceeding previous state-of-the-art performance using only $50\\%$ of the\navailable training data and surpassing BLEU, ROUGE and METEOR with only $40$\nlabelled examples. Finally, we demonstrate that ParaBLEU can be used to\nconditionally generate novel paraphrases from a single demonstration, which we\nuse to confirm our hypothesis that it learns abstract, generalized paraphrase\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.08251",
          "publishedOn": "2021-07-20T02:04:39.649Z",
          "wordCount": 546,
          "title": "Generative Pretraining for Paraphrase Evaluation. (arXiv:2107.08251v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yao Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Haipeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Despite being a critical communication skill, grasping humor is challenging\n-- a successful use of humor requires a mixture of both engaging content\nbuild-up and an appropriate vocal delivery (e.g., pause). Prior studies on\ncomputational humor emphasize the textual and audio features immediately next\nto the punchline, yet overlooking longer-term context setup. Moreover, the\ntheories are usually too abstract for understanding each concrete humor\nsnippet. To fill in the gap, we develop DeHumor, a visual analytical system for\nanalyzing humorous behaviors in public speaking. To intuitively reveal the\nbuilding blocks of each concrete example, DeHumor decomposes each humorous\nvideo into multimodal features and provides inline annotations of them on the\nvideo script. In particular, to better capture the build-ups, we introduce\ncontent repetition as a complement to features introduced in theories of\ncomputational humor and visualize them in a context linking graph. To help\nusers locate the punchlines that have the desired features to learn, we\nsummarize the content (with keywords) and humor feature statistics on an\naugmented time matrix. With case studies on stand-up comedy shows and TED\ntalks, we show that DeHumor is able to highlight various building blocks of\nhumor examples. In addition, expert interviews with communication coaches and\nhumor researchers demonstrate the effectiveness of DeHumor for multimodal humor\nanalysis of speech content and vocal delivery.",
          "link": "http://arxiv.org/abs/2107.08356",
          "publishedOn": "2021-07-20T02:04:39.589Z",
          "wordCount": 689,
          "title": "DeHumor: Visual Analytics for Decomposing Humor. (arXiv:2107.08356v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arruda_H/0/1/0/all/0/1\">Henrique F. de Arruda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reia_S/0/1/0/all/0/1\">Sandro M. Reia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_F/0/1/0/all/0/1\">Filipi N. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amancio_D/0/1/0/all/0/1\">Diego R. Amancio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_L/0/1/0/all/0/1\">Luciano da F. Costa</a>",
          "description": "Poetry and prose are written artistic expressions that help us to appreciate\nthe reality we live. Each of these styles has its own set of subjective\nproperties, such as rhyme and rhythm, which are easily caught by a human\nreader's eye and ear. With the recent advances in artificial intelligence, the\ngap between humans and machines may have decreased, and today we observe\nalgorithms mastering tasks that were once exclusively performed by humans. In\nthis paper, we propose an automated method to distinguish between poetry and\nprose based solely on aural and rhythmic properties. In other to compare prose\nand poetry rhythms, we represent the rhymes and phones as temporal sequences\nand thus we propose a procedure for extracting rhythmic features from these\nsequences. The classification of the considered texts using the set of features\nextracted resulted in a best accuracy of 0.78, obtained with a neural network.\nInterestingly, by using an approach based on complex networks to visualize the\nsimilarities between the different texts considered, we found that the patterns\nof poetry vary much more than prose. Consequently, a much richer and complex\nset of rhythmic possibilities tends to be found in that modality.",
          "link": "http://arxiv.org/abs/2107.08512",
          "publishedOn": "2021-07-20T02:04:39.557Z",
          "wordCount": 646,
          "title": "A pattern recognition approach for distinguishing between prose and poetry. (arXiv:2107.08512v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Kishky_A/0/1/0/all/0/1\">Ahmed El-Kishky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1\">Benjamin I. P. Rubinstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>",
          "description": "Mistranslated numbers have the potential to cause serious effects, such as\nfinancial loss or medical misinformation. In this work we develop comprehensive\nassessments of the robustness of neural machine translation systems to\nnumerical text via behavioural testing. We explore a variety of numerical\ntranslation capabilities a system is expected to exhibit and design effective\ntest examples to expose system underperformance. We find that numerical\nmistranslation is a general issue: major commercial systems and\nstate-of-the-art research models fail on many of our test examples, for high-\nand low-resource languages. Our tests reveal novel errors that have not\npreviously been reported in NMT systems, to the best of our knowledge. Lastly,\nwe discuss strategies to mitigate numerical mistranslation.",
          "link": "http://arxiv.org/abs/2107.08357",
          "publishedOn": "2021-07-20T02:04:39.369Z",
          "wordCount": 577,
          "title": "As Easy as 1, 2, 3: Behavioural Testing of NMT Systems for Numerical Translation. (arXiv:2107.08357v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parry_H/0/1/0/all/0/1\">Hishan Parry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xun_L/0/1/0/all/0/1\">Lei Xun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_A/0/1/0/all/0/1\">Amin Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jia Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrett_G/0/1/0/all/0/1\">Geoff V. Merrett</a>",
          "description": "The Transformer architecture is widely used for machine translation tasks.\nHowever, its resource-intensive nature makes it challenging to implement on\nconstrained embedded devices, particularly where available hardware resources\ncan vary at run-time. We propose a dynamic machine translation model that\nscales the Transformer architecture based on the available resources at any\nparticular time. The proposed approach, 'Dynamic-HAT', uses a HAT\nSuperTransformer as the backbone to search for SubTransformers with different\naccuracy-latency trade-offs at design time. The optimal SubTransformers are\nsampled from the SuperTransformer at run-time, depending on latency\nconstraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses\ninherited SubTransformers sampled directly from the SuperTransformer with a\nswitching time of <1s. Using inherited SubTransformers results in a BLEU score\nloss of <1.5% because the SubTransformer configuration is not retrained from\nscratch after sampling. However, to recover this loss in performance, the\ndimensions of the design space can be reduced to tailor it to a family of\ntarget hardware. The new reduced design space results in a BLEU score increase\nof approximately 1% for sub-optimal models from the original design space, with\na wide range for performance scaling between 0.356s - 1.526s for the GPU and\n2.9s - 7.31s for the CPU.",
          "link": "http://arxiv.org/abs/2107.08199",
          "publishedOn": "2021-07-20T02:04:39.289Z",
          "wordCount": 651,
          "title": "Dynamic Transformer for Efficient Machine Translation on Embedded Devices. (arXiv:2107.08199v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chingacham_A/0/1/0/all/0/1\">Anupama Chingacham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1\">Vera Demberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>",
          "description": "Listening in noisy environments can be difficult even for individuals with a\nnormal hearing thresholds. The speech signal can be masked by noise, which may\nlead to word misperceptions on the side of the listener, and overall difficulty\nto understand the message. To mitigate hearing difficulties on listeners, a\nco-operative speaker utilizes voice modulation strategies like Lombard speech\nto generate noise-robust utterances, and similar solutions have been developed\nfor speech synthesis systems. In this work, we propose an alternate solution of\nchoosing noise-robust lexical paraphrases to represent an intended meaning. Our\nresults show that lexical paraphrases differ in their intelligibility in noise.\nWe evaluate the intelligibility of synonyms in context and find that choosing a\nlexical unit that is less risky to be misheard than its synonym introduced an\naverage gain in comprehension of 37% at SNR -5 dB and 21% at SNR 0 dB for\nbabble noise.",
          "link": "http://arxiv.org/abs/2107.08337",
          "publishedOn": "2021-07-20T02:04:39.266Z",
          "wordCount": 598,
          "title": "Exploring the Potential of Lexical Paraphrases for Mitigating Noise-Induced Comprehension Errors. (arXiv:2107.08337v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>",
          "description": "Previous studies have shown that initializing neural machine translation\n(NMT) models with the pre-trained language models (LM) can speed up the model\ntraining and boost the model performance. In this work, we identify a critical\nside-effect of pre-training for NMT, which is due to the discrepancy between\nthe training objectives of LM-based pre-training and NMT. Since the LM\nobjective learns to reconstruct a few source tokens and copy most of them, the\npre-training initialization would affect the copying behaviors of NMT models.\nWe provide a quantitative analysis of copying behaviors by introducing a metric\ncalled copying ratio, which empirically shows that pre-training based NMT\nmodels have a larger copying ratio than the standard one. In response to this\nproblem, we propose a simple and effective method named copying penalty to\ncontrol the copying behaviors in decoding. Extensive experiments on both\nin-domain and out-of-domain benchmarks show that the copying penalty method\nconsistently improves translation performance by controlling copying behaviors\nfor pre-training based NMT models. Source code is freely available at\nhttps://github.com/SunbowLiu/CopyingPenalty.",
          "link": "http://arxiv.org/abs/2107.08212",
          "publishedOn": "2021-07-20T02:04:39.238Z",
          "wordCount": 626,
          "title": "On the Copying Behaviors of Pre-Training for Neural Machine Translation. (arXiv:2107.08212v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jack Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1\">Raphael Lenain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1\">Udeepa Meepegama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1\">Emil Fristed</a>",
          "description": "We propose a method for learning de-identified prosody representations from\nraw audio using a contrastive self-supervised signal. Whereas prior work has\nrelied on conditioning models on bottlenecks, we introduce a set of inductive\nbiases that exploit the natural structure of prosody to minimize timbral\ninformation and decouple prosody from speaker representations. Despite\naggressive downsampling of the input and having no access to linguistic\ninformation, our model performs comparably to state-of-the-art speech\nrepresentations on DAMMP, a new benchmark we introduce for spoken language\nunderstanding. We use minimum description length probing to show that our\nrepresentations have selectively learned the subcomponents of non-timbral\nprosody, and that the product quantizer naturally disentangles them without\nusing bottlenecks. We derive an information-theoretic definition of speech\nde-identifiability and use it to demonstrate that our prosody representations\nare less identifiable than other speech representations.",
          "link": "http://arxiv.org/abs/2107.08248",
          "publishedOn": "2021-07-20T02:04:39.186Z",
          "wordCount": 611,
          "title": "Learning De-identified Representations of Prosody from Raw Audio. (arXiv:2107.08248v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pei_X/0/1/0/all/0/1\">Xin Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_D/0/1/0/all/0/1\">Deval Mehta</a>",
          "description": "Transcending the binary categorization of racist and xenophobic texts, this\nresearch takes cues from social science theories to develop a four dimensional\ncategory for racism and xenophobia detection, namely stigmatization,\noffensiveness, blame, and exclusion. With the aid of deep learning techniques,\nthis categorical detection enables insights into the nuances of emergent topics\nreflected in racist and xenophobic expression on Twitter. Moreover, a stage\nwise analysis is applied to capture the dynamic changes of the topics across\nthe stages of early development of Covid-19 from a domestic epidemic to an\ninternational public health emergency, and later to a global pandemic. The main\ncontributions of this research include, first the methodological advancement.\nBy bridging the state-of-the-art computational methods with social science\nperspective, this research provides a meaningful approach for future research\nto gain insight into the underlying subtlety of racist and xenophobic\ndiscussion on digital platforms. Second, by enabling a more accurate\ncomprehension and even prediction of public opinions and actions, this research\npaves the way for the enactment of effective intervention policies to combat\nracist crimes and social exclusion under Covid-19.",
          "link": "http://arxiv.org/abs/2107.08347",
          "publishedOn": "2021-07-20T02:04:39.148Z",
          "wordCount": 688,
          "title": "Beyond a binary of (non)racist tweets: A four-dimensional categorical detection and analysis of racist and xenophobic opinions on Twitter in early Covid-19. (arXiv:2107.08347v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wadden_D/0/1/0/all/0/1\">David Wadden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>",
          "description": "We present an overview of the SciVer shared task, presented at the 2nd\nScholarly Document Processing (SDP) workshop at NAACL 2021. In this shared\ntask, systems were provided a scientific claim and a corpus of research\nabstracts, and asked to identify which articles SUPPORT or REFUTE the claim as\nwell as provide evidentiary sentences justifying those labels. 11 teams made a\ntotal of 14 submissions to the shared task leaderboard, leading to an\nimprovement of more than +23 F1 on the primary task evaluation metric. In\naddition to surveying the participating systems, we provide several insights\ninto modeling approaches to support continued progress and future research on\nthe important and challenging task of scientific claim verification.",
          "link": "http://arxiv.org/abs/2107.08188",
          "publishedOn": "2021-07-20T02:04:39.083Z",
          "wordCount": 567,
          "title": "Overview and Insights from the SciVer Shared Task on Scientific Claim Verification. (arXiv:2107.08188v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08146",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1\">Peter Jansen</a>",
          "description": "Tamarian, a fictional language introduced in the Star Trek episode Darmok,\ncommunicates meaning through utterances of metaphorical references, such as\n\"Darmok and Jalad at Tanagra\" instead of \"We should work together.\" This work\nassembles a Tamarian-English dictionary of utterances from the original episode\nand several follow-on novels, and uses this to construct a parallel corpus of\n456 English-Tamarian utterances. A machine translation system based on a large\nlanguage model (T5) is trained using this parallel corpus, and is shown to\nproduce an accuracy of 76% when translating from English to Tamarian on known\nutterances.",
          "link": "http://arxiv.org/abs/2107.08146",
          "publishedOn": "2021-07-20T02:04:39.061Z",
          "wordCount": 527,
          "title": "Darmok and Jalad at Tanagra: A Dataset and Model for English-to-Tamarian Translation. (arXiv:2107.08146v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wysocki_O/0/1/0/all/0/1\">Oskar Wysocki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florea_M/0/1/0/all/0/1\">Malina Florea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1\">Donal Landers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>",
          "description": "This paper proposes a novel statistical corpus analysis framework targeted\ntowards the interpretation of Natural Language Processing (NLP) architectural\npatterns at scale. The proposed approach combines saturation-based lexicon\nconstruction, statistical corpus analysis methods and graph collocations to\ninduce a synthesis representation of NLP architectural patterns from corpora.\nThe framework is validated in the full corpus of Semeval tasks and demonstrated\ncoherent architectural patterns which can be used to answer architectural\nquestions on a data-driven fashion, providing a systematic mechanism to\ninterpret a largely dynamic and exponentially growing field.",
          "link": "http://arxiv.org/abs/2107.08124",
          "publishedOn": "2021-07-20T02:04:39.034Z",
          "wordCount": 546,
          "title": "Architectures of Meaning, A Systematic Corpus Analysis of NLP Systems. (arXiv:2107.08124v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Montero_I/0/1/0/all/0/1\">Ivan Montero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1\">Ni Lao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Andrew J. Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuBois_C/0/1/0/all/0/1\">Christopher DuBois</a>",
          "description": "Existing methods for open-retrieval question answering in lower resource\nlanguages (LRLs) lag significantly behind English. They not only suffer from\nthe shortcomings of non-English document retrieval, but are reliant on\nlanguage-specific supervision for either the task or translation. We formulate\na task setup more realistic to available resources, that circumvents document\nretrieval to reliably transfer knowledge from English to lower resource\nlanguages. Assuming a strong English question answering model or database, we\ncompare and analyze methods that pivot through English: to map foreign queries\nto English and then English answers back to target language answers. Within\nthis task setup we propose Reranked Multilingual Maximal Inner Product Search\n(RM-MIPS), akin to semantic similarity retrieval over the English training set\nwith reranking, which outperforms the strongest baselines by 2.7% on XQuAD and\n6.2% on MKQA. Analysis demonstrates the particular efficacy of this strategy\nover state-of-the-art alternatives in challenging settings: low-resource\nlanguages, with extensive distractor data and query distribution misalignment.\nCircumventing retrieval, our analysis shows this approach offers rapid answer\ngeneration to almost any language off-the-shelf, without the need for any\nadditional training data in the target language.",
          "link": "http://arxiv.org/abs/2012.14094",
          "publishedOn": "2021-07-19T00:49:05.908Z",
          "wordCount": 651,
          "title": "Pivot Through English: Reliably Answering Multilingual Questions without Document Retrieval. (arXiv:2012.14094v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yiran Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakemeyer_G/0/1/0/all/0/1\">Gerhard Lakemeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yunpu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>",
          "description": "We present Knowledge Enhanced Multimodal BART (KM-BART), which is a\nTransformer-based sequence-to-sequence model capable of reasoning about\ncommonsense knowledge from multimodal inputs of images and texts. We adapt the\ngenerative BART architecture to a multimodal model with visual and textual\ninputs. We further develop novel pretraining tasks to improve the model\nperformance on the Visual Commonsense Generation (VCG) task. In particular, our\npretraining task of Knowledge-based Commonsense Generation (KCG) boosts model\nperformance on the VCG task by leveraging commonsense knowledge from a large\nlanguage model pretrained on external commonsense knowledge graphs. To the best\nof our knowledge, we are the first to propose a dedicated task for improving\nmodel performance on the VCG task. Experimental results show that our model\nreaches state-of-the-art performance on the VCG task by applying these novel\npretraining tasks.",
          "link": "http://arxiv.org/abs/2101.00419",
          "publishedOn": "2021-07-19T00:49:05.844Z",
          "wordCount": 612,
          "title": "KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation. (arXiv:2101.00419v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1\">Jacob Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>",
          "description": "Recent results in end-to-end automatic speech recognition have demonstrated\nthe efficacy of pseudo-labeling for semi-supervised models trained both with\nConnectionist Temporal Classification (CTC) and Sequence-to-Sequence (seq2seq)\nlosses. Iterative Pseudo-Labeling (IPL), which continuously trains a single\nmodel using pseudo-labels iteratively re-generated as the model learns, has\nbeen shown to further improve performance in ASR. We improve upon the IPL\nalgorithm: as the model learns, we propose to iteratively re-generate\ntranscriptions with hard labels (the most probable tokens), that is, without a\nlanguage model. We call this approach Language-Model-Free IPL (slimIPL) and\ngive a resultant training setup for low-resource settings with CTC-based\nmodels. slimIPL features a dynamic cache for pseudo-labels which reduces\nsensitivity to changes in relabeling hyperparameters and results in improves\ntraining stability. slimIPL is also highly-efficient and requires 3.5-4x fewer\ncomputational resources to converge than other state-of-the-art\nsemi/self-supervised approaches. With only 10 hours of labeled audio, slimIPL\nis competitive with self-supervised approaches, and is state-of-the-art with\n100 hours of labeled audio without the use of a language model both at test\ntime and during pseudo-label generation.",
          "link": "http://arxiv.org/abs/2010.11524",
          "publishedOn": "2021-07-19T00:49:05.837Z",
          "wordCount": 654,
          "title": "SlimIPL: Language-Model-Free Iterative Pseudo-Labeling. (arXiv:2010.11524v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blodgett_S/0/1/0/all/0/1\">Su Lin Blodgett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waseem_Z/0/1/0/all/0/1\">Zeerak Waseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>",
          "description": "Despite inextricable ties between race and language, little work has\nconsidered race in NLP research and development. In this work, we survey 79\npapers from the ACL anthology that mention race. These papers reveal various\ntypes of race-related bias in all stages of NLP model development, highlighting\nthe need for proactive consideration of how NLP systems can uphold racial\nhierarchies. However, persistent gaps in research on race and NLP remain: race\nhas been siloed as a niche topic and remains ignored in many NLP tasks; most\nwork operationalizes race as a fixed single-dimensional variable with a\nground-truth label, which risks reinforcing differences produced by historical\nracism; and the voices of historically marginalized people are nearly absent in\nNLP literature. By identifying where and how NLP literature has and has not\nconsidered race, especially in comparison to related fields, our work calls for\ninclusion and racial justice in NLP research practices.",
          "link": "http://arxiv.org/abs/2106.11410",
          "publishedOn": "2021-07-19T00:49:05.830Z",
          "wordCount": 620,
          "title": "A Survey of Race, Racism, and Anti-Racism in NLP. (arXiv:2106.11410v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "Learning effective language representations from crowdsourced labels is\ncrucial for many real-world machine learning tasks. A challenging aspect of\nthis problem is that the quality of crowdsourced labels suffer high intra- and\ninter-observer variability. Since the high-capacity deep neural networks can\neasily memorize all disagreements among crowdsourced labels, directly applying\nexisting supervised language representation learning algorithms may yield\nsuboptimal solutions. In this paper, we propose \\emph{TACMA}, a\n\\underline{t}emporal-\\underline{a}ware language representation learning\nheuristic for \\underline{c}rowdsourced labels with \\underline{m}ultiple\n\\underline{a}nnotators. The proposed approach (1) explicitly models the\nintra-observer variability with attention mechanism; (2) computes and\naggregates per-sample confidence scores from multiple workers to address the\ninter-observer disagreements. The proposed heuristic is extremely easy to\nimplement in around 5 lines of code. The proposed heuristic is evaluated on\nfour synthetic and four real-world data sets. The results show that our\napproach outperforms a wide range of state-of-the-art baselines in terms of\nprediction accuracy and AUC. To encourage the reproducible results, we make our\ncode publicly available at \\url{https://github.com/CrowdsourcingMining/TACMA}.",
          "link": "http://arxiv.org/abs/2107.07958",
          "publishedOn": "2021-07-19T00:49:05.801Z",
          "wordCount": 621,
          "title": "Temporal-aware Language Representation Learning From Crowdsourced Labels. (arXiv:2107.07958v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heeringa_W/0/1/0/all/0/1\">Wilbert Heeringa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouma_G/0/1/0/all/0/1\">Gosse Bouma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofman_M/0/1/0/all/0/1\">Martha Hofman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drenth_E/0/1/0/all/0/1\">Eduard Drenth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijffels_J/0/1/0/all/0/1\">Jan Wijffels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velde_H/0/1/0/all/0/1\">Hans Van de Velde</a>",
          "description": "We present a lemmatizer/POS-tagger/dependency parser for West Frisian using a\ncorpus of 44,714 words in 3,126 sentences that were annotated according to the\nguidelines of Universal Dependency version 2. POS tags were assigned to words\nby using a Dutch POS tagger that was applied to a literal word-by-word\ntranslation, or to sentences of a Dutch parallel text. Best results were\nobtained when using literal translations that were created by using the Frisian\ntranslation program Oersetter. Morphologic and syntactic annotations were\ngenerated on the basis of a literal Dutch translation as well. The performance\nof the lemmatizer/tagger/annotator when it was trained using default parameters\nwas compared to the performance that was obtained when using the parameter\nvalues that were used for training the LassySmall UD 2.5 corpus. A significant\nimprovement was found for `lemma'. The Frisian lemmatizer/PoS tagger/dependency\nparser is released as a web app and as a web service.",
          "link": "http://arxiv.org/abs/2107.07974",
          "publishedOn": "2021-07-19T00:49:05.794Z",
          "wordCount": 606,
          "title": "POS tagging, lemmatization and dependency parsing of West Frisian. (arXiv:2107.07974v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07771",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yajing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Luxi Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiangpeng Wei</a>",
          "description": "End-to-End intelligent neural dialogue systems suffer from the problems of\ngenerating inconsistent and repetitive responses. Existing dialogue models pay\nattention to unilaterally incorporating personal knowledge into the dialog\nwhile ignoring the fact that incorporating the personality-related conversation\ninformation into personal knowledge taken as the bilateral information flow\nboosts the quality of the subsequent conversation. Besides, it is indispensable\nto control personal knowledge utilization over the conversation level. In this\npaper, we propose a conversation-adaption multi-view persona aware response\ngeneration model that aims at enhancing conversation consistency and\nalleviating the repetition from two folds. First, we consider conversation\nconsistency from multiple views. From the view of the persona profile, we\ndesign a novel interaction module that not only iteratively incorporates\npersonalized knowledge into each turn conversation but also captures the\npersonality-related information from conversation to enhance personalized\nknowledge semantic representation. From the view of speaking style, we\nintroduce the speaking style vector and feed it into the decoder to keep the\nspeaking style consistency. To avoid conversation repetition, we devise a\ncoverage mechanism to keep track of the activation of personal knowledge\nutilization. Experiments on both automatic and human evaluation verify the\nsuperiority of our model over previous models.",
          "link": "http://arxiv.org/abs/2107.07771",
          "publishedOn": "2021-07-19T00:49:05.774Z",
          "wordCount": 639,
          "title": "Know Deeper: Knowledge-Conversation Cyclic Utilization Mechanism for Open-domain Dialogue Generation. (arXiv:2107.07771v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koenders_C/0/1/0/all/0/1\">Camille Koenders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filla_J/0/1/0/all/0/1\">Johannes Filla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nicolai Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woloszyn_V/0/1/0/all/0/1\">Vinicius Woloszyn</a>",
          "description": "As the spread of false information on the internet has increased dramatically\nin recent years, more and more attention is being paid to automated fake news\ndetection. Some fake news detection methods are already quite successful.\nNevertheless, there are still many vulnerabilities in the detection algorithms.\nThe reason for this is that fake news publishers can structure and formulate\ntheir texts in such a way that a detection algorithm does not expose this text\nas fake news. This paper shows that it is possible to automatically attack\nstate-of-the-art models that have been trained to detect Fake News, making\nthese vulnerable. For this purpose, corresponding models were first trained\nbased on a dataset. Then, using Text-Attack, an attempt was made to manipulate\nthe trained models in such a way that previously correctly identified fake news\nwas classified as true news. The results show that it is possible to\nautomatically bypass Fake News detection mechanisms, leading to implications\nconcerning existing policy initiatives.",
          "link": "http://arxiv.org/abs/2107.07970",
          "publishedOn": "2021-07-19T00:49:05.715Z",
          "wordCount": 604,
          "title": "How Vulnerable Are Automatic Fake News Detection Methods to Adversarial Attacks?. (arXiv:2107.07970v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengju Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yonghui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Muhua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>",
          "description": "Recent studies on Knowledge Base Question Answering (KBQA) have shown great\nprogress on this task via better question understanding. Previous works for\nencoding questions mainly focus on the word sequences, but seldom consider the\ninformation from syntactic trees.In this paper, we propose an approach to learn\nsyntax-based representations for KBQA. First, we encode path-based syntax by\nconsidering the shortest dependency paths between keywords. Then, we propose\ntwo encoding strategies to mode the information of whole syntactic trees to\nobtain tree-based syntax. Finally, we combine both path-based and tree-based\nsyntax representations for KBQA. We conduct extensive experiments on a widely\nused benchmark dataset and the experimental results show that our syntax-aware\nsystems can make full use of syntax information in different settings and\nachieve state-of-the-art performance of KBQA.",
          "link": "http://arxiv.org/abs/2107.07940",
          "publishedOn": "2021-07-19T00:49:05.695Z",
          "wordCount": 565,
          "title": "Exploiting Rich Syntax for Better Knowledge Base Question Answering. (arXiv:2107.07940v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Q/0/1/0/all/0/1\">Qin Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>",
          "description": "Leveraging unlabelled data through weak or distant supervision is a\ncompelling approach to developing more effective text classification models.\nThis paper proposes a simple but effective data augmentation method, which\nleverages the idea of pseudo-labelling to select samples from noisy distant\nsupervision annotation datasets. The result shows that the proposed method\nimproves the accuracy of biased news detection models.",
          "link": "http://arxiv.org/abs/2107.07705",
          "publishedOn": "2021-07-19T00:49:05.680Z",
          "wordCount": 491,
          "title": "Pseudo-labelling Enhanced Media Bias Detection. (arXiv:2107.07705v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey A. Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>",
          "description": "The world of empirical machine learning (ML) strongly relies on benchmarks in\norder to determine the relative effectiveness of different algorithms and\nmethods. This paper proposes the notion of \"a benchmark lottery\" that describes\nthe overall fragility of the ML benchmarking process. The benchmark lottery\npostulates that many factors, other than fundamental algorithmic superiority,\nmay lead to a method being perceived as superior. On multiple benchmark setups\nthat are prevalent in the ML community, we show that the relative performance\nof algorithms may be altered significantly simply by choosing different\nbenchmark tasks, highlighting the fragility of the current paradigms and\npotential fallacious interpretation derived from benchmarking ML methods. Given\nthat every benchmark makes a statement about what it perceives to be important,\nwe argue that this might lead to biased progress in the community. We discuss\nthe implications of the observed phenomena and provide recommendations on\nmitigating them using multiple machine learning domains and communities as use\ncases, including natural language processing, computer vision, information\nretrieval, recommender systems, and reinforcement learning.",
          "link": "http://arxiv.org/abs/2107.07002",
          "publishedOn": "2021-07-19T00:49:05.656Z",
          "wordCount": 620,
          "title": "The Benchmark Lottery. (arXiv:2107.07002v1 [cs.LG] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Penagos_C/0/1/0/all/0/1\">Carlos Rodriguez-Penagos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonet_O/0/1/0/all/0/1\">Ona de Gibert Bonet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armentano_Oller_C/0/1/0/all/0/1\">Carme Armentano-Oller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melero_M/0/1/0/all/0/1\">Maite Melero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>",
          "description": "Multilingual language models have been a crucial breakthrough as they\nconsiderably reduce the need of data for under-resourced languages.\nNevertheless, the superiority of language-specific models has already been\nproven for languages having access to large amounts of data. In this work, we\nfocus on Catalan with the aim to explore to what extent a medium-sized\nmonolingual language model is competitive with state-of-the-art large\nmultilingual models. For this, we: (1) build a clean, high-quality textual\nCatalan corpus (CaText), the largest to date (but only a fraction of the usual\nsize of the previous work in monolingual language models), (2) train a\nTransformer-based language model for Catalan (BERTa), and (3) devise a thorough\nevaluation in a diversity of settings, comprising a complete array of\ndownstream tasks, namely, Part of Speech Tagging, Named Entity Recognition and\nClassification, Text Classification, Question Answering, and Semantic Textual\nSimilarity, with most of the corresponding datasets being created ex novo. The\nresult is a new benchmark, the Catalan Language Understanding Benchmark (CLUB),\nwhich we publish as an open resource, together with the clean textual corpus,\nthe language model, and the cleaning pipeline. Using state-of-the-art\nmultilingual models and a monolingual model trained only on Wikipedia as\nbaselines, we consistently observe the superiority of our model across tasks\nand settings.",
          "link": "http://arxiv.org/abs/2107.07903",
          "publishedOn": "2021-07-19T00:49:05.605Z",
          "wordCount": 674,
          "title": "Are Multilingual Models the Best Choice for Moderately Under-resourced Languages? A Comprehensive Assessment for Catalan. (arXiv:2107.07903v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2005.02721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gelderloos_L/0/1/0/all/0/1\">Lieke Gelderloos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrupala_G/0/1/0/all/0/1\">Grzegorz Chrupa&#x142;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alishahi_A/0/1/0/all/0/1\">Afra Alishahi</a>",
          "description": "Speech directed to children differs from adult-directed speech in linguistic\naspects such as repetition, word choice, and sentence length, as well as in\naspects of the speech signal itself, such as prosodic and phonemic variation.\nHuman language acquisition research indicates that child-directed speech helps\nlanguage learners. This study explores the effect of child-directed speech when\nlearning to extract semantic information from speech directly. We compare the\ntask performance of models trained on adult-directed speech (ADS) and\nchild-directed speech (CDS). We find indications that CDS helps in the initial\nstages of learning, but eventually, models trained on ADS reach comparable task\nperformance, and generalize better. The results suggest that this is at least\npartially due to linguistic rather than acoustic properties of the two\nregisters, as we see the same pattern when looking at models trained on\nacoustically comparable synthetic speech.",
          "link": "http://arxiv.org/abs/2005.02721",
          "publishedOn": "2021-07-19T00:49:05.581Z",
          "wordCount": 671,
          "title": "Learning to Understand Child-directed and Adult-directed Speech. (arXiv:2005.02721v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "The quality of vocal delivery is one of the key indicators for evaluating\nteacher enthusiasm, which has been widely accepted to be connected to the\noverall course qualities. However, existing evaluation for vocal delivery is\nmainly conducted with manual ratings, which faces two core challenges:\nsubjectivity and time-consuming. In this paper, we present a novel machine\nlearning approach that utilizes pairwise comparisons and a multimodal\northogonal fusing algorithm to generate large-scale objective evaluation\nresults of the teacher vocal delivery in terms of fluency and passion. We\ncollect two datasets from real-world education scenarios and the experiment\nresults demonstrate the effectiveness of our algorithm. To encourage\nreproducible results, we make our code public available at\n\\url{https://github.com/tal-ai/ML4VocalDelivery.git}.",
          "link": "http://arxiv.org/abs/2107.07956",
          "publishedOn": "2021-07-19T00:49:05.566Z",
          "wordCount": 577,
          "title": "A Multimodal Machine Learning Framework for Teacher Vocal Delivery Evaluation. (arXiv:2107.07956v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shiting Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guowei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_P/0/1/0/all/0/1\">Peilei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "Task requirements (TRs) writing is an important question type in Key English\nTest and Preliminary English Test. A TR writing question may include multiple\nrequirements and a high-quality essay must respond to each requirement\nthoroughly and accurately. However, the limited teacher resources prevent\nstudents from getting detailed grading instantly. The majority of existing\nautomatic essay scoring systems focus on giving a holistic score but rarely\nprovide reasons to support it. In this paper, we proposed an end-to-end\nframework based on machine reading comprehension (MRC) to address this problem\nto some extent. The framework not only detects whether an essay responds to a\nrequirement question, but clearly marks where the essay answers the question.\nOur framework consists of three modules: question normalization module, ELECTRA\nbased MRC module and response locating module. We extensively explore\nstate-of-the-art MRC methods. Our approach achieves 0.93 accuracy score and\n0.85 F1 score on a real-world educational dataset. To encourage reproducible\nresults, we make our code publicly available at\n\\url{https://github.com/aied2021TRMRC/AIED_2021_TRMRC_code}.",
          "link": "http://arxiv.org/abs/2107.07957",
          "publishedOn": "2021-07-19T00:49:05.545Z",
          "wordCount": 619,
          "title": "Automatic Task Requirements Writing Evaluation via Machine Reading Comprehension. (arXiv:2107.07957v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yukun Jiang</a>",
          "description": "Active learning is a state-of-art machine learning approach to deal with an\nabundance of unlabeled data. In the field of Natural Language Processing,\ntypically it is costly and time-consuming to have all the data annotated. This\ninefficiency inspires out our application of active learning in text\nclassification. Traditional unsupervised k-means clustering is first modified\ninto a semi-supervised version in this research. Then, a novel attempt is\napplied to further extend the algorithm into active learning scenario with\nPenalized Min-Max-selection, so as to make limited queries that yield more\nstable initial centroids. This method utilizes both the interactive query\nresults from users and the underlying distance representation. After tested on\na Chinese news dataset, it shows a consistent increase in accuracy while\nlowering the cost in training.",
          "link": "http://arxiv.org/abs/2107.07682",
          "publishedOn": "2021-07-19T00:49:05.525Z",
          "wordCount": 600,
          "title": "The Application of Active Query K-Means in Text Classification. (arXiv:2107.07682v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Magee_L/0/1/0/all/0/1\">Liam Magee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghahremanlou_L/0/1/0/all/0/1\">Lida Ghahremanlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldatic_K/0/1/0/all/0/1\">Karen Soldatic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_S/0/1/0/all/0/1\">Shanthi Robertson</a>",
          "description": "To examine whether intersectional bias can be observed in language\ngeneration, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size\nfrom 124 million to ~2.7 billion parameters. We conduct an experiment combining\nup to three social categories - gender, religion and disability - into\nunconditional or zero-shot prompts used to generate sentences that are then\nanalysed for sentiment. Our results confirm earlier tests conducted with\nauto-regressive causal models, including the \\emph{GPT} family of models. We\nalso illustrate why bias may be resistant to techniques that target single\ncategories (e.g. gender, religion and race), as it can also manifest, in often\nsubtle ways, in texts prompted by concatenated social categories. To address\nthese difficulties, we suggest technical and community-based approaches need to\ncombine to acknowledge and address complex and intersectional language model\nbias.",
          "link": "http://arxiv.org/abs/2107.07691",
          "publishedOn": "2021-07-19T00:49:05.500Z",
          "wordCount": 568,
          "title": "Intersectional Bias in Causal Language Models. (arXiv:2107.07691v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07634",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_T/0/1/0/all/0/1\">Takuya Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1\">Anmol Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dhir_C/0/1/0/all/0/1\">Chandra Dhir</a>",
          "description": "Keyword spotting (KWS) is an important technique for speech applications,\nwhich enables users to activate devices by speaking a keyword phrase. Although\na phoneme classifier can be used for KWS, exploiting a large amount of\ntranscribed data for automatic speech recognition (ASR), there is a mismatch\nbetween the training criterion (phoneme recognition) and the target task (KWS).\nRecently, multi-task learning has been applied to KWS to exploit both ASR and\nKWS training data. In this approach, an output of an acoustic model is split\ninto two branches for the two tasks, one for phoneme transcription trained with\nthe ASR data and one for keyword classification trained with the KWS data. In\nthis paper, we introduce a cross attention decoder in the multi-task learning\nframework. Unlike the conventional multi-task learning approach with the simple\nsplit of the output layer, the cross attention decoder summarizes information\nfrom a phonetic encoder by performing cross attention between the encoder\noutputs and a trainable query sequence to predict a confidence score for the\nKWS task. Experimental results on KWS tasks show that the proposed approach\noutperformed the conventional multi-task learning with split branches and a\nbi-directional long short-team memory decoder by 12% on average.",
          "link": "http://arxiv.org/abs/2107.07634",
          "publishedOn": "2021-07-19T00:49:05.441Z",
          "wordCount": 652,
          "title": "Multi-task Learning with Cross Attention for Keyword Spotting. (arXiv:2107.07634v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>",
          "description": "Despite recent improvements in open-domain dialogue models, state of the art\nmodels are trained and evaluated on short conversations with little context. In\ncontrast, the long-term conversation setting has hardly been studied. In this\nwork we collect and release a human-human dataset consisting of multiple chat\nsessions whereby the speaking partners learn about each other's interests and\ndiscuss the things they have learnt from past sessions. We show how existing\nmodels trained on existing datasets perform poorly in this long-term\nconversation setting in both automatic and human evaluations, and we study\nlong-context models that can perform much better. In particular, we find\nretrieval-augmented methods and methods with an ability to summarize and recall\nprevious conversations outperform the standard encoder-decoder architectures\ncurrently considered state of the art.",
          "link": "http://arxiv.org/abs/2107.07567",
          "publishedOn": "2021-07-19T00:49:05.430Z",
          "wordCount": 556,
          "title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation. (arXiv:2107.07567v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yihan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>",
          "description": "This paper improves the robustness of the pretrained language model BERT\nagainst word substitution-based adversarial attacks by leveraging\nself-supervised contrastive learning with adversarial perturbations. One\nadvantage of our method compared to previous works is that it is capable of\nimproving model robustness without using any labels. Additionally, we also\ncreate an adversarial attack for word-level adversarial training on BERT. The\nattack is efficient, allowing adversarial training for BERT on adversarial\nexamples generated on the fly during training. Experimental results on four\ndatasets show that our method improves the robustness of BERT against four\ndifferent word substitution-based adversarial attacks. Furthermore, to\nunderstand why our method can improve the model robustness against adversarial\nattacks, we study vector representations of clean examples and their\ncorresponding adversarial examples before and after applying our method. As our\nmethod improves model robustness with unlabeled raw data, it opens up the\npossibility of using large text datasets to train robust language models.",
          "link": "http://arxiv.org/abs/2107.07610",
          "publishedOn": "2021-07-19T00:49:05.402Z",
          "wordCount": 596,
          "title": "Self-Supervised Contrastive Learning with Adversarial Perturbations for Robust Pretrained Language Models. (arXiv:2107.07610v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-guang Lou</a>",
          "description": "Recent years pre-trained language models hit a success on modeling natural\nlanguage sentences and (semi-)structured tables. However, existing table\npre-training techniques always suffer from low data quality and low\npre-training efficiency. In this paper, we show that table pre-training can be\nrealized by learning a neural SQL executor over a synthetic corpus, which is\nobtained by automatically synthesizing executable SQL queries. By pre-training\non the synthetic corpus, our approach TAPEX dramatically improves the\nperformance on downstream tasks, boosting existing language models by at most\n19.5%. Meanwhile, TAPEX has remarkably high pre-training efficiency and yields\nstrong results when using a small pre-trained corpus. Experimental results\ndemonstrate that TAPEX outperforms previous table pre-training approaches by a\nlarge margin, and our model achieves new state-of-the-art results on four\nwell-known datasets, including improving the WikiSQL denotation accuracy to\n89.6% (+4.9%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the\nSQA denotation accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.6%\n(+3.6%). Our work opens the way to reason over structured data by pre-training\non synthetic executable programs.",
          "link": "http://arxiv.org/abs/2107.07653",
          "publishedOn": "2021-07-19T00:49:05.328Z",
          "wordCount": 624,
          "title": "TAPEX: Table Pre-training via Learning a Neural SQL Executor. (arXiv:2107.07653v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Komeili_M/0/1/0/all/0/1\">Mojtaba Komeili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>",
          "description": "The largest store of continually updating knowledge on our planet can be\naccessed via internet search. In this work we study giving access to this\ninformation to conversational agents. Large language models, even though they\nstore an impressive amount of knowledge within their weights, are known to\nhallucinate facts when generating dialogue (Shuster et al., 2021); moreover,\nthose facts are frozen in time at the point of model training. In contrast, we\npropose an approach that learns to generate an internet search query based on\nthe context, and then conditions on the search results to finally generate a\nresponse, a method that can employ up-to-the-minute relevant information. We\ntrain and evaluate such models on a newly collected dataset of human-human\nconversations whereby one of the speakers is given access to internet search\nduring knowledgedriven discussions in order to ground their responses. We find\nthat search-query based access of the internet in conversation provides\nsuperior performance compared to existing approaches that either use no\naugmentation or FAISS-based retrieval (Lewis et al., 2020).",
          "link": "http://arxiv.org/abs/2107.07566",
          "publishedOn": "2021-07-19T00:49:05.279Z",
          "wordCount": 594,
          "title": "Internet-Augmented Dialogue Generation. (arXiv:2107.07566v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07509",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>",
          "description": "In this work, we propose novel decoding algorithms to enable streaming\nautomatic speech recognition (ASR) on unsegmented long-form recordings without\nvoice activity detection (VAD), based on monotonic chunkwise attention (MoChA)\nwith an auxiliary connectionist temporal classification (CTC) objective. We\npropose a block-synchronous beam search decoding to take advantage of efficient\nbatched output-synchronous and low-latency input-synchronous searches. We also\npropose a VAD-free inference algorithm that leverages CTC probabilities to\ndetermine a suitable timing to reset the model states to tackle the\nvulnerability to long-form data. Experimental evaluations demonstrate that the\nblock-synchronous decoding achieves comparable accuracy to the\nlabel-synchronous one. Moreover, the VAD-free inference can recognize long-form\nspeech robustly for up to a few hours.",
          "link": "http://arxiv.org/abs/2107.07509",
          "publishedOn": "2021-07-16T00:48:23.310Z",
          "wordCount": 560,
          "title": "VAD-free Streaming Hybrid CTC/Attention ASR for Unsegmented Recording. (arXiv:2107.07509v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00635",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>",
          "description": "While attention-based encoder-decoder (AED) models have been successfully\nextended to the online variants for streaming automatic speech recognition\n(ASR), such as monotonic chunkwise attention (MoChA), the models still have a\nlarge label emission latency because of the unconstrained end-to-end training\nobjective. Previous works tackled this problem by leveraging alignment\ninformation to control the timing to emit tokens during training. In this work,\nwe propose a simple alignment-free regularization method, StableEmit, to\nencourage MoChA to emit tokens earlier. StableEmit discounts the selection\nprobabilities in hard monotonic attention for token boundary detection by a\nconstant factor and regularizes them to recover the total attention mass during\ntraining. As a result, the scale of the selection probabilities is increased,\nand the values can reach a threshold for token emission earlier, leading to a\nreduction of emission latency and deletion errors. Moreover, StableEmit can be\ncombined with methods that constraint alignments to further improve the\naccuracy and latency. Experimental evaluations with LSTM and Conformer encoders\ndemonstrate that StableEmit significantly reduces the recognition errors and\nthe emission latency simultaneously. We also show that the use of alignment\ninformation is complementary in both metrics.",
          "link": "http://arxiv.org/abs/2107.00635",
          "publishedOn": "2021-07-16T00:48:23.271Z",
          "wordCount": 662,
          "title": "StableEmit: Selection Probability Discount for Reducing Emission Latency of Streaming Monotonic Attention ASR. (arXiv:2107.00635v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1\">Efthymios Tzinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casebeer_J/0/1/0/all/0/1\">Jonah Casebeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhepei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1\">Paris Smaragdis</a>",
          "description": "We propose FEDENHANCE, an unsupervised federated learning (FL) approach for\nspeech enhancement and separation with non-IID distributed data across multiple\nclients. We simulate a real-world scenario where each client only has access to\na few noisy recordings from a limited and disjoint number of speakers (hence\nnon-IID). Each client trains their model in isolation using mixture invariant\ntraining while periodically providing updates to a central server. Our\nexperiments show that our approach achieves competitive enhancement performance\ncompared to IID training on a single device and that we can further facilitate\nthe convergence speed and the overall performance using transfer learning on\nthe server-side. Moreover, we show that we can effectively combine updates from\nclients trained locally with supervised and unsupervised losses. We also\nrelease a new dataset LibriFSD50K and its creation recipe in order to\nfacilitate FL research for source separation problems.",
          "link": "http://arxiv.org/abs/2105.04727",
          "publishedOn": "2021-07-16T00:48:23.256Z",
          "wordCount": 627,
          "title": "Separate but Together: Unsupervised Federated Learning for Speech Enhancement from Non-IID Data. (arXiv:2105.04727v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zetian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jason Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Leslie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peter Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Michelle A. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>",
          "description": "Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. It is a challenging yet crucial area\nwith numerous real-world applications in multimedia, affective computing,\nrobotics, finance, human-computer interaction, and healthcare. Unfortunately,\nmultimodal research has seen limited resources to study (1) generalization\nacross domains and modalities, (2) complexity during training and inference,\nand (3) robustness to noisy and missing modalities. In order to accelerate\nprogress towards understudied modalities and tasks while ensuring real-world\nrobustness, we release MultiBench, a systematic and unified large-scale\nbenchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6\nresearch areas. MultiBench provides an automated end-to-end machine learning\npipeline that simplifies and standardizes data loading, experimental setup, and\nmodel evaluation. To enable holistic evaluation, MultiBench offers a\ncomprehensive methodology to assess (1) generalization, (2) time and space\ncomplexity, and (3) modality robustness. MultiBench introduces impactful\nchallenges for future research, including scalability to large-scale multimodal\ndatasets and robustness to realistic imperfections. To accompany this\nbenchmark, we also provide a standardized implementation of 20 core approaches\nin multimodal learning. Simply applying methods proposed in different research\nareas can improve the state-of-the-art performance on 9/15 datasets. Therefore,\nMultiBench presents a milestone in unifying disjoint efforts in multimodal\nresearch and paves the way towards a better understanding of the capabilities\nand limitations of multimodal models, all the while ensuring ease of use,\naccessibility, and reproducibility. MultiBench, our standardized code, and\nleaderboards are publicly available, will be regularly updated, and welcomes\ninputs from the community.",
          "link": "http://arxiv.org/abs/2107.07502",
          "publishedOn": "2021-07-16T00:48:23.230Z",
          "wordCount": 722,
          "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baradaran_R/0/1/0/all/0/1\">Razieh Baradaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirkhani_H/0/1/0/all/0/1\">Hossein Amirkhani</a>",
          "description": "Machine Reading Comprehension (MRC) is an active field in natural language\nprocessing with many successful developed models in recent years. Despite their\nhigh in-distribution accuracy, these models suffer from two issues: high\ntraining cost and low out-of-distribution accuracy. Even though some approaches\nhave been presented to tackle the generalization problem, they have high,\nintolerable training costs. In this paper, we investigate the effect of\nensemble learning approach to improve generalization of MRC systems without\nretraining a big model. After separately training the base models with\ndifferent structures on different datasets, they are ensembled using weighting\nand stacking approaches in probabilistic and non-probabilistic settings. Three\nconfigurations are investigated including heterogeneous, homogeneous, and\nhybrid on eight datasets and six state-of-the-art models. We identify the\nimportant factors in the effectiveness of ensemble methods. Also, we compare\nthe robustness of ensemble and fine-tuned models against data distribution\nshifts. The experimental results show the effectiveness and robustness of the\nensemble approach in improving the out-of-distribution accuracy of MRC systems,\nespecially when the base models are similar in accuracies.",
          "link": "http://arxiv.org/abs/2107.00368",
          "publishedOn": "2021-07-16T00:48:23.222Z",
          "wordCount": 625,
          "title": "Ensemble Learning-Based Approach for Improving Generalization Capability of Machine Reading Comprehension Systems. (arXiv:2107.00368v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaojing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chenyang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huilin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Guoao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hai Hu</a>",
          "description": "Pretrained Language Models (PLMs) have achieved tremendous success in natural\nlanguage understanding tasks. While different learning schemes -- fine-tuning,\nzero-shot and few-shot learning -- have been widely explored and compared for\nlanguages such as English, there is comparatively little work in Chinese to\nfairly and comprehensively evaluate and compare these methods. This work first\nintroduces Chinese Few-shot Learning Evaluation Benchmark (FewCLUE), the first\ncomprehensive small sample evaluation benchmark in Chinese. It includes nine\ntasks, ranging from single-sentence and sentence-pair classification tasks to\nmachine reading comprehension tasks. Given the high variance of the few-shot\nlearning performance, we provide multiple training/validation sets to\nfacilitate a more accurate and stable evaluation of few-shot modeling. An\nunlabeled training set with up to 20,000 additional samples per task is\nprovided, allowing researchers to explore better ways of using unlabeled\nsamples. Next, we implement a set of state-of-the-art (SOTA) few-shot learning\nmethods (including PET, ADAPET, LM-BFF, P-tuning and EFL), and compare their\nperformance with fine-tuning and zero-shot learning schemes on the newly\nconstructed FewCLUE benchmark.Our results show that: 1) all five few-shot\nlearning methods exhibit better performance than fine-tuning or zero-shot\nlearning; 2) among the five methods, PET is the best performing few-shot\nmethod; 3) few-shot learning performance is highly dependent on the specific\ntask. Our benchmark and code are available at\nhttps://github.com/CLUEbenchmark/FewCLUE",
          "link": "http://arxiv.org/abs/2107.07498",
          "publishedOn": "2021-07-16T00:48:23.183Z",
          "wordCount": 671,
          "title": "FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark. (arXiv:2107.07498v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Abul Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levene_M/0/1/0/all/0/1\">Mark Levene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_D/0/1/0/all/0/1\">David Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fromson_R/0/1/0/all/0/1\">Renate Fromson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koslover_N/0/1/0/all/0/1\">Nicolas Koslover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levene_T/0/1/0/all/0/1\">Tamara Levene</a>",
          "description": "Objective: This study aims to develop an end-to-end natural language\nprocessing pipeline for triage and diagnosis of COVID-19 from patient-authored\nsocial media posts, in order to provide researchers and other interested\nparties with additional information on the symptoms, severity and prevalence of\nthe disease. Materials and Methods: The text processing pipeline first extracts\nCOVID-19 symptoms and related concepts such as severity, duration, negations,\nand body parts from patients posts using conditional random fields. An\nunsupervised rule-based algorithm is then applied to establish relations\nbetween concepts in the next step of the pipeline. The extracted concepts and\nrelations are subsequently used to construct two different vector\nrepresentations of each post. These vectors are applied separately to build\nsupport vector machine learning models to triage patients into three categories\nand diagnose them for COVID-19. Results: We report that macro- and\nmicro-averaged F1 scores in the range of 71-96% and 61-87%, respectively, for\nthe triage and diagnosis of COVID-19, when the models are trained on human\nlabelled data. Our experimental results indicate that similar performance can\nbe achieved when the models are trained using predicted labels from concept\nextraction and rule-based classifiers, thus yielding end-to-end machine\nlearning. Also, we highlight important features uncovered by our diagnostic\nmachine learning models and compare them with the most frequent symptoms\nrevealed in another COVID-19 dataset. In particular, we found that the most\nimportant features are not always the most frequent ones.",
          "link": "http://arxiv.org/abs/2103.11850",
          "publishedOn": "2021-07-16T00:48:23.138Z",
          "wordCount": 757,
          "title": "Triage and diagnosis of COVID-19 from medical social media. (arXiv:2103.11850v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1\">Efthymios Tzinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhepei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xilin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1\">Paris Smaragdis</a>",
          "description": "Recent progress in audio source separation lead by deep learning has enabled\nmany neural network models to provide robust solutions to this fundamental\nestimation problem. In this study, we provide a family of efficient neural\nnetwork architectures for general purpose audio source separation while\nfocusing on multiple computational aspects that hinder the application of\nneural networks in real-world scenarios. The backbone structure of this\nconvolutional network is the SUccessive DOwnsampling and Resampling of\nMulti-Resolution Features (SuDoRM-RF) as well as their aggregation which is\nperformed through simple one-dimensional convolutions. This mechanism enables\nour models to obtain high fidelity signal separation in a wide variety of\nsettings where variable number of sources are present and with limited\ncomputational resources (e.g. floating point operations, memory footprint,\nnumber of parameters and latency). Our experiments show that SuDoRM-RF models\nperform comparably and even surpass several state-of-the-art benchmarks with\nsignificantly higher computational resource requirements. The causal variation\nof SuDoRM-RF is able to obtain competitive performance in real-time speech\nseparation of around 10dB scale-invariant signal-to-distortion ratio\nimprovement (SI-SDRi) while remaining up to 20 times faster than real-time on a\nlaptop device.",
          "link": "http://arxiv.org/abs/2103.02644",
          "publishedOn": "2021-07-16T00:48:23.079Z",
          "wordCount": 674,
          "title": "Compute and memory efficient universal sound source separation. (arXiv:2103.02644v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilijevic_M/0/1/0/all/0/1\">Momcilo Vasilijevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "Video transcript summarization is a fundamental task for video understanding.\nConventional approaches for transcript summarization are usually built upon the\nsummarization data for written language such as news articles, while the domain\ndiscrepancy may degrade the model performance on spoken text. In this paper, we\npresent VT-SSum, a benchmark dataset with spoken language for video transcript\nsegmentation and summarization, which includes 125K transcript-summary pairs\nfrom 9,616 videos. VT-SSum takes advantage of the videos from VideoLectures.NET\nby leveraging the slides content as the weak supervision to generate the\nextractive summary for video transcripts. Experiments with a state-of-the-art\ndeep learning approach show that the model trained with VT-SSum brings a\nsignificant improvement on the AMI spoken text summarization benchmark. VT-SSum\nis publicly available at https://github.com/Dod-o/VT-SSum to support the future\nresearch of video transcript segmentation and summarization tasks.",
          "link": "http://arxiv.org/abs/2106.05606",
          "publishedOn": "2021-07-16T00:48:23.072Z",
          "wordCount": 605,
          "title": "VT-SSum: A Benchmark Dataset for Video Transcript Segmentation and Summarization. (arXiv:2106.05606v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1\">Ori Yoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talmor_A/0/1/0/all/0/1\">Alon Talmor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>",
          "description": "Models pre-trained with a language modeling objective possess ample world\nknowledge and language skills, but are known to struggle in tasks that require\nreasoning. In this work, we propose to leverage semi-structured tables, and\nautomatically generate at scale question-paragraph pairs, where answering the\nquestion requires reasoning over multiple facts in the paragraph. We add a\npre-training step over this synthetic data, which includes examples that\nrequire 16 different reasoning skills such as number comparison, conjunction,\nand fact composition. To improve data efficiency, we propose sampling\nstrategies that focus training on reasoning skills the model is currently\nlacking. We evaluate our approach on three reading comprehension datasets that\nare focused on reasoning, and show that our model, PReasM, substantially\noutperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling\nexamples based on current model errors leads to faster training and higher\noverall performance.",
          "link": "http://arxiv.org/abs/2107.07261",
          "publishedOn": "2021-07-16T00:48:22.991Z",
          "wordCount": 587,
          "title": "Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills. (arXiv:2107.07261v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Alexis Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>",
          "description": "Making controlled perturbations is essential for various tasks (e.g., data\naugmentation), but building task-specific generators can be expensive. We\nintroduce Tailor, a task-agnostic generation system that perturbs text in a\nsemantically-controlled way. With unlikelihood training, we design Tailor's\ngenerator to follow a series of control codes derived from semantic roles.\nThrough modifications of these control codes, Tailor can produce fine-grained\nperturbations. We implement a set of operations on control codes that can be\ncomposed into complex perturbation strategies, and demonstrate their\neffectiveness in three distinct applications: First, Tailor facilitates the\nconstruction of high-quality contrast sets that are lexically diverse, and less\nbiased than original task test data. Second, paired with automated labeling\nheuristics, Tailor helps improve model generalization through data\naugmentation: We obtain an average gain of 1.73 on an NLI challenge set by\nperturbing just 5% of training data. Third, without any finetuning overhead,\nTailor's perturbations effectively improve compositionality in fine-grained\nstyle transfer, outperforming fine-tuned baselines on 6 transfers.",
          "link": "http://arxiv.org/abs/2107.07150",
          "publishedOn": "2021-07-16T00:48:22.906Z",
          "wordCount": 594,
          "title": "Tailor: Generating and Perturbing Text with Semantic Controls. (arXiv:2107.07150v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1\">Jonathan Bragg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>",
          "description": "Few-shot NLP research is highly active, yet conducted in disjoint research\nthreads with evaluation suites that lack challenging-yet-realistic testing\nsetups and fail to employ careful experimental design. Consequently, the\ncommunity does not know which techniques perform best or even if they\noutperform simple baselines. We formulate desiderata for an ideal few-shot NLP\nbenchmark and present FLEX, the first benchmark, public leaderboard, and\nframework that provides unified, comprehensive measurement for few-shot NLP\ntechniques. FLEX incorporates and introduces new best practices for few-shot\nevaluation, including measurement of four transfer settings, textual labels for\nzero-shot evaluation, and a principled approach to benchmark design that\noptimizes statistical accuracy while keeping evaluation costs accessible to\nresearchers without large compute resources. In addition, we present UniFew, a\nsimple yet strong prompt-based model for few-shot learning which unifies the\npretraining and finetuning prompt formats, eschewing complex machinery of\nrecent prompt-based approaches in adapting downstream task formats to language\nmodel pretraining objectives. We demonstrate that despite simplicity UniFew\nachieves results competitive with both popular meta-learning and prompt-based\napproaches.",
          "link": "http://arxiv.org/abs/2107.07170",
          "publishedOn": "2021-07-16T00:48:22.899Z",
          "wordCount": 619,
          "title": "FLEX: Unifying Evaluation for Few-Shot NLP. (arXiv:2107.07170v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guowei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1\">Weiping Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "Many real-world applications involve the use of Optical Character Recognition\n(OCR) engines to transform handwritten images into transcripts on which\ndownstream Natural Language Processing (NLP) models are applied. In this\nprocess, OCR engines may introduce errors and inputs to downstream NLP models\nbecome noisy. Despite that pre-trained models achieve state-of-the-art\nperformance in many NLP benchmarks, we prove that they are not robust to noisy\ntexts generated by real OCR engines. This greatly limits the application of NLP\nmodels in real-world scenarios. In order to improve model performance on noisy\nOCR transcripts, it is natural to train the NLP model on labelled noisy texts.\nHowever, in most cases there are only labelled clean texts. Since there is no\nhandwritten pictures corresponding to the text, it is impossible to directly\nuse the recognition model to obtain noisy labelled data. Human resources can be\nemployed to copy texts and take pictures, but it is extremely expensive\nconsidering the size of data for model training. Consequently, we are\ninterested in making NLP models intrinsically robust to OCR errors in a low\nresource manner. We propose a novel robust training framework which 1) employs\nsimple but effective methods to directly simulate natural OCR noises from clean\ntexts and 2) iteratively mines the hard examples from a large number of\nsimulated samples for optimal performance. 3) To make our model learn\nnoise-invariant representations, a stability loss is employed. Experiments on\nthree real-world datasets show that the proposed framework boosts the\nrobustness of pre-trained models by a large margin. We believe that this work\ncan greatly promote the application of NLP models in actual scenarios, although\nthe algorithm we use is simple and straightforward. We make our codes and three\ndatasets publicly\navailable\\footnote{https://github.com/tal-ai/Robust-learning-MSSHEM}.",
          "link": "http://arxiv.org/abs/2107.07113",
          "publishedOn": "2021-07-16T00:48:22.816Z",
          "wordCount": 754,
          "title": "Robust Learning for Text Classification with Multi-source Noise Simulation and Hard Example Mining. (arXiv:2107.07113v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rashkin_H/0/1/0/all/0/1\">Hannah Rashkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reitter_D/0/1/0/all/0/1\">David Reitter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomar_G/0/1/0/all/0/1\">Gaurav Singh Tomar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipanjan Das</a>",
          "description": "Knowledge-grounded dialogue systems are intended to convey information that\nis based on evidence provided in a given source text. We discuss the challenges\nof training a generative neural dialogue model for such systems that is\ncontrolled to stay faithful to the evidence. Existing datasets contain a mix of\nconversational responses that are faithful to selected evidence as well as more\nsubjective or chit-chat style responses. We propose different evaluation\nmeasures to disentangle these different styles of responses by quantifying the\ninformativeness and objectivity. At training time, additional inputs based on\nthese evaluation measures are given to the dialogue model. At generation time,\nthese additional inputs act as stylistic controls that encourage the model to\ngenerate responses that are faithful to the provided evidence. We also\ninvestigate the usage of additional controls at decoding time using resampling\ntechniques. In addition to automatic metrics, we perform a human evaluation\nstudy where raters judge the output of these controlled generation models to be\ngenerally more objective and faithful to the evidence compared to baseline\ndialogue systems.",
          "link": "http://arxiv.org/abs/2107.06963",
          "publishedOn": "2021-07-16T00:48:22.800Z",
          "wordCount": 609,
          "title": "Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features. (arXiv:2107.06963v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiahui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+shi_H/0/1/0/all/0/1\">Han shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip L.H. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>",
          "description": "Transformer-based pre-trained language models like BERT and its variants have\nrecently achieved promising performance in various natural language processing\n(NLP) tasks. However, the conventional paradigm constructs the backbone by\npurely stacking the manually designed global self-attention layers, introducing\ninductive bias and thus leading to sub-optimal. In this work, we propose an\nOperation-Priority Neural Architecture Search (OP-NAS) algorithm to\nautomatically search for promising hybrid backbone architectures. Our\nwell-designed search space (i) contains primitive math operations in the\nintra-layer level to explore novel attention structures, and (ii) leverages\nconvolution blocks to be the supplementary for attention structure in the\ninter-layer level to better learn local dependency. We optimize both the search\nalgorithm and evaluation of candidate models to boost the efficiency of our\nproposed OP-NAS. Specifically, we propose Operation-Priority (OP) evolution\nstrategy to facilitate model search via balancing exploration and exploitation.\nFurthermore, we design a Bi-branch Weight-Sharing (BIWS) training strategy for\nfast model evaluation. Extensive experiments show that the searched\narchitecture (named AutoBERT-Zero) significantly outperforms BERT and its\nvariants of different model capacities in various downstream tasks, proving the\narchitecture's transfer and generalization abilities. Remarkably,\nAutoBERT-Zero-base outperforms RoBERTa-base (using much more data) and\nBERT-large (with much larger model size) by 2.4 and 1.4 higher score on GLUE\ntest set. Code and pre-trained models will be made publicly available.",
          "link": "http://arxiv.org/abs/2107.07445",
          "publishedOn": "2021-07-16T00:48:22.784Z",
          "wordCount": 660,
          "title": "AutoBERT-Zero: Evolving BERT Backbone from Scratch. (arXiv:2107.07445v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chimmwal_N/0/1/0/all/0/1\">Neeraj Chimmwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1\">Rishabh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>",
          "description": "We present a CLSRIL-23, a self supervised learning based audio pre-trained\nmodel which learns cross lingual speech representations from raw audio across\n23 Indic languages. It is built on top of wav2vec 2.0 which is solved by\ntraining a contrastive task over masked latent speech representations and\njointly learns the quantization of latents shared across all languages. We\ncompare the language wise loss during pretraining to compare effects of\nmonolingual and multilingual pretraining. Performance on some downstream\nfine-tuning tasks for speech recognition is also compared and our experiments\nshow that multilingual pretraining outperforms monolingual training, in terms\nof learning speech representations which encodes phonetic similarity of\nlanguages and also in terms of performance on down stream tasks. A decrease of\n5% is observed in WER and 9.5% in CER when a multilingual pretrained model is\nused for finetuning in Hindi. All the code models are also open sourced.\nCLSRIL-23 is a model trained on $23$ languages and almost 10,000 hours of audio\ndata to facilitate research in speech recognition for Indic languages. We hope\nthat new state of the art systems will be created using the self supervised\napproach, especially for low resources Indic languages.",
          "link": "http://arxiv.org/abs/2107.07402",
          "publishedOn": "2021-07-16T00:48:22.760Z",
          "wordCount": 652,
          "title": "CLSRIL-23: Cross Lingual Speech Representations for Indic Languages. (arXiv:2107.07402v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pamies_M/0/1/0/all/0/1\">Marc P&#xe0;mies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llop_Palao_J/0/1/0/all/0/1\">Joan Llop-Palao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silveira_Ocampo_J/0/1/0/all/0/1\">Joaqu&#xed;n Silveira-Ocampo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armentano_Oller_C/0/1/0/all/0/1\">Carme Armentano-Oller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Penagos_C/0/1/0/all/0/1\">Carlos Rodriguez-Penagos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>",
          "description": "This paper presents the Spanish RoBERTa-base and RoBERTa-large models, as\nwell as the corresponding performance evaluations. Both models were pre-trained\nusing the largest Spanish corpus known to date, with a total of 570GB of clean\nand deduplicated text processed for this work, compiled from the web crawlings\nperformed by the National Library of Spain from 2009 to 2019.",
          "link": "http://arxiv.org/abs/2107.07253",
          "publishedOn": "2021-07-16T00:48:22.751Z",
          "wordCount": 497,
          "title": "Spanish Language Models. (arXiv:2107.07253v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afrin_T/0/1/0/all/0/1\">Tazin Afrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1\">Elaine Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumura_L/0/1/0/all/0/1\">Lindsay C. Matsumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correnti_R/0/1/0/all/0/1\">Richard Correnti</a>",
          "description": "Automated writing evaluation systems can improve students' writing insofar as\nstudents attend to the feedback provided and revise their essay drafts in ways\naligned with such feedback. Existing research on revision of argumentative\nwriting in such systems, however, has focused on the types of revisions\nstudents make (e.g., surface vs. content) rather than the extent to which\nrevisions actually respond to the feedback provided and improve the essay. We\nintroduce an annotation scheme to capture the nature of sentence-level\nrevisions of evidence use and reasoning (the `RER' scheme) and apply it to 5th-\nand 6th-grade students' argumentative essays. We show that reliable manual\nannotation can be achieved and that revision annotations correlate with a\nholistic assessment of essay improvement in line with the feedback provided.\nFurthermore, we explore the feasibility of automatically classifying revisions\naccording to our scheme.",
          "link": "http://arxiv.org/abs/2107.06990",
          "publishedOn": "2021-07-16T00:48:22.730Z",
          "wordCount": 605,
          "title": "Annotation and Classification of Evidence and Reasoning Revisions in Argumentative Writing. (arXiv:2107.06990v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luckin_R/0/1/0/all/0/1\">Rose Luckin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "In this work, we study computational approaches to detect online dialogic\ninstructions, which are widely used to help students understand learning\nmaterials, and build effective study habits. This task is rather challenging\ndue to the widely-varying quality and pedagogical styles of dialogic\ninstructions. To address these challenges, we utilize pre-trained language\nmodels, and propose a multi-task paradigm which enhances the ability to\ndistinguish instances of different classes by enlarging the margin between\ncategories via contrastive loss. Furthermore, we design a strategy to fully\nexploit the misclassified examples during the training stage. Extensive\nexperiments on a real-world online educational data set demonstrate that our\napproach achieves superior performance compared to representative baselines. To\nencourage reproducible results, we make our implementation online available at\n\\url{https://github.com/AIED2021/multitask-dialogic-instruction}.",
          "link": "http://arxiv.org/abs/2107.07119",
          "publishedOn": "2021-07-16T00:48:22.722Z",
          "wordCount": 586,
          "title": "Multi-Task Learning based Online Dialogic Instruction Detection with Pre-trained Language Models. (arXiv:2107.07119v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1\">Holger Schwenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>",
          "description": "In this paper, we describe our end-to-end multilingual speech translation\nsystem submitted to the IWSLT 2021 evaluation campaign on the Multilingual\nSpeech Translation shared task. Our system is built by leveraging transfer\nlearning across modalities, tasks and languages. First, we leverage\ngeneral-purpose multilingual modules pretrained with large amounts of\nunlabelled and labelled data. We further enable knowledge transfer from the\ntext task to the speech task by training two tasks jointly. Finally, our\nmultilingual model is finetuned on speech translation task-specific data to\nachieve the best translation results. Experimental results show our system\noutperforms the reported systems, including both end-to-end and cascaded based\napproaches, by a large margin.\n\nIn some translation directions, our speech translation results evaluated on\nthe public Multilingual TEDx test set are even comparable with the ones from a\nstrong text-to-text translation system, which uses the oracle speech\ntranscripts as input.",
          "link": "http://arxiv.org/abs/2107.06959",
          "publishedOn": "2021-07-16T00:48:22.698Z",
          "wordCount": 610,
          "title": "FST: the FAIR Speech Translation System for the IWSLT21 Multilingual Shared Task. (arXiv:2107.06959v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Coenen_A/0/1/0/all/0/1\">Andy Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Luke Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1\">Emily Reif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1\">Ann Yuan</a>",
          "description": "As neural language models grow in effectiveness, they are increasingly being\napplied in real-world settings. However these applications tend to be limited\nin the modes of interaction they support. In this extended abstract, we propose\nWordcraft, an AI-assisted editor for story writing in which a writer and a\ndialog system collaborate to write a story. Our novel interface uses few-shot\nlearning and the natural affordances of conversation to support a variety of\ninteractions. Our editor provides a sandbox for writers to probe the boundaries\nof transformer-based language models and paves the way for future\nhuman-in-the-loop training pipelines and novel evaluation methods.",
          "link": "http://arxiv.org/abs/2107.07430",
          "publishedOn": "2021-07-16T00:48:22.690Z",
          "wordCount": 550,
          "title": "Wordcraft: a Human-AI Collaborative Editor for Story Writing. (arXiv:2107.07430v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mawalim_C/0/1/0/all/0/1\">Candy Olivia Mawalim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unoki_M/0/1/0/all/0/1\">Masashi Unoki</a>",
          "description": "Speaker anonymization aims to suppress speaker individuality to protect\nprivacy in speech while preserving the other aspects, such as speech content.\nOne effective solution for anonymization is to modify the McAdams coefficient.\nIn this work, we propose a method to improve the security for speaker\nanonymization based on the McAdams coefficient by using a speech watermarking\napproach. The proposed method consists of two main processes: one for embedding\nand one for detection. In embedding process, two different McAdams coefficients\nrepresent binary bits ``0\" and ``1\". The watermarked speech is then obtained by\nframe-by-frame bit inverse switching. Subsequently, the detection process is\ncarried out by a power spectrum comparison. We conducted objective evaluations\nwith reference to the VoicePrivacy 2020 Challenge (VP2020) and of the speech\nwatermarking with reference to the Information Hiding Challenge (IHC) and found\nthat our method could satisfy the blind detection, inaudibility, and robustness\nrequirements in watermarking. It also significantly improved the anonymization\nperformance in comparison to the secondary baseline system in VP2020.",
          "link": "http://arxiv.org/abs/2107.07223",
          "publishedOn": "2021-07-16T00:48:22.664Z",
          "wordCount": 603,
          "title": "Improving Security in McAdams Coefficient-Based Speaker Anonymization by Watermarking Method. (arXiv:2107.07223v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "We introduce HTLM, a hyper-text language model trained on a large-scale web\ncrawl. Modeling hyper-text has a number of advantages: (1) it is easily\ngathered at scale, (2) it provides rich document-level and end-task-adjacent\nsupervision (e.g. class and id attributes often encode document category\ninformation), and (3) it allows for new structured prompting that follows the\nestablished semantics of HTML (e.g. to do zero-shot summarization by infilling\ntitle tags for a webpage that contains the input text). We show that\npretraining with a BART-style denoising loss directly on simplified HTML\nprovides highly effective transfer for a wide range of end tasks and\nsupervision levels. HTLM matches or exceeds the performance of comparably sized\ntext-only LMs for zero-shot prompting and fine-tuning for classification\nbenchmarks, while also setting new state-of-the-art performance levels for\nzero-shot summarization. We also find that hyper-text prompts provide more\nvalue to HTLM, in terms of data efficiency, than plain text prompts do for\nexisting LMs, and that HTLM is highly effective at auto-prompting itself, by\nsimply generating the most likely hyper-text formatting for any available\ntraining data. We will release all code and models to support future HTLM\nresearch.",
          "link": "http://arxiv.org/abs/2107.06955",
          "publishedOn": "2021-07-16T00:48:22.621Z",
          "wordCount": 633,
          "title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models. (arXiv:2107.06955v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiongqiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiafu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Feng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "Sentence completion (SC) questions present a sentence with one or more blanks\nthat need to be filled in, three to five possible words or phrases as options.\nSC questions are widely used for students learning English as a Second Language\n(ESL) and building computational approaches to automatically solve such\nquestions is beneficial to language learners. In this work, we propose a neural\nframework to solve SC questions in English examinations by utilizing\npre-trained language models. We conduct extensive experiments on a real-world\nK-12 ESL SC question dataset and the results demonstrate the superiority of our\nmodel in terms of prediction accuracy. Furthermore, we run precision-recall\ntrade-off analysis to discuss the practical issues when deploying it in\nreal-life scenarios. To encourage reproducible results, we make our code\npublicly available at \\url{https://github.com/AIED2021/ESL-SentenceCompletion}.",
          "link": "http://arxiv.org/abs/2107.07122",
          "publishedOn": "2021-07-16T00:48:22.592Z",
          "wordCount": 595,
          "title": "Solving ESL Sentence Completion Questions via Pre-trained Neural Language Models. (arXiv:2107.07122v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1\">Matteo Stefanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1\">Silvia Cascianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Connecting Vision and Language plays an essential role in Generative\nIntelligence. For this reason, in the last few years, a large research effort\nhas been devoted to image captioning, i.e. the task of describing images with\nsyntactically and semantically meaningful sentences. Starting from 2015 the\ntask has generally been addressed with pipelines composed of a visual encoding\nstep and a language model for text generation. During these years, both\ncomponents have evolved considerably through the exploitation of object\nregions, attributes, and relationships and the introduction of multi-modal\nconnections, fully-attentive approaches, and BERT-like early-fusion strategies.\nHowever, regardless of the impressive results obtained, research in image\ncaptioning has not reached a conclusive answer yet. This work aims at providing\na comprehensive overview and categorization of image captioning approaches,\nfrom visual encoding and text generation to training strategies, used datasets,\nand evaluation metrics. In this respect, we quantitatively compare many\nrelevant state-of-the-art approaches to identify the most impactful technical\ninnovations in image captioning architectures and training strategies.\nMoreover, many variants of the problem and its open challenges are analyzed and\ndiscussed. The final goal of this work is to serve as a tool for understanding\nthe existing state-of-the-art and highlighting the future directions for an\narea of research where Computer Vision and Natural Language Processing can find\nan optimal synergy.",
          "link": "http://arxiv.org/abs/2107.06912",
          "publishedOn": "2021-07-16T00:48:22.510Z",
          "wordCount": 664,
          "title": "From Show to Tell: A Survey on Image Captioning. (arXiv:2107.06912v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Tianze Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lillian Lee</a>",
          "description": "We propose a transition-based bubble parser to perform coordination structure\nidentification and dependency-based syntactic analysis simultaneously. Bubble\nrepresentations were proposed in the formal linguistics literature decades ago;\nthey enhance dependency trees by encoding coordination boundaries and internal\nrelationships within coordination structures explicitly. In this paper, we\nintroduce a transition system and neural models for parsing these\nbubble-enhanced structures. Experimental results on the English Penn Treebank\nand the English GENIA corpus show that our parsers beat previous\nstate-of-the-art approaches on the task of coordination structure prediction,\nespecially for the subset of sentences with complex coordination structures.",
          "link": "http://arxiv.org/abs/2107.06905",
          "publishedOn": "2021-07-16T00:48:22.501Z",
          "wordCount": 536,
          "title": "Transition-based Bubble Parsing: Improvements on Coordination Structure Prediction. (arXiv:2107.06905v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Tianze Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lillian Lee</a>",
          "description": "We present our contribution to the IWPT 2021 shared task on parsing into\nenhanced Universal Dependencies. Our main system component is a hybrid\ntree-graph parser that integrates (a) predictions of spanning trees for the\nenhanced graphs with (b) additional graph edges not present in the spanning\ntrees. We also adopt a finetuning strategy where we first train a\nlanguage-generic parser on the concatenation of data from all available\nlanguages, and then, in a second step, finetune on each individual language\nseparately. Additionally, we develop our own complete set of pre-processing\nmodules relevant to the shared task, including tokenization, sentence\nsegmentation, and multiword token expansion, based on pre-trained XLM-R models\nand our own pre-training of character-level language models. Our submission\nreaches a macro-average ELAS of 89.24 on the test set. It ranks top among all\nteams, with a margin of more than 2 absolute ELAS over the next best-performing\nsubmission, and best score on 16 out of 17 languages.",
          "link": "http://arxiv.org/abs/2107.06907",
          "publishedOn": "2021-07-16T00:48:22.340Z",
          "wordCount": 610,
          "title": "TGIF: Tree-Graph Integrated-Format Parser for Enhanced UD with Two-Stage Generic- to Individual-Language Finetuning. (arXiv:2107.06907v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenyao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1\">Shengnan An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "Neural sequence models exhibit limited compositional generalization ability\nin semantic parsing tasks. Compositional generalization requires algebraic\nrecombination, i.e., dynamically recombining structured expressions in a\nrecursive manner. However, most previous studies mainly concentrate on\nrecombining lexical units, which is an important but not sufficient part of\nalgebraic recombination. In this paper, we propose LeAR, an end-to-end neural\nmodel to learn algebraic recombination for compositional generalization. The\nkey insight is to model the semantic parsing task as a homomorphism between a\nlatent syntactic algebra and a semantic algebra, thus encouraging algebraic\nrecombination. Specifically, we learn two modules jointly: a Composer for\nproducing latent syntax, and an Interpreter for assigning semantic operations.\nExperiments on two realistic and comprehensive compositional generalization\nbenchmarks demonstrate the effectiveness of our model. The source code is\npublicly available at https://github.com/microsoft/ContextualSP.",
          "link": "http://arxiv.org/abs/2107.06516",
          "publishedOn": "2021-07-15T01:59:02.545Z",
          "wordCount": 580,
          "title": "Learning Algebraic Recombination for Compositional Generalization. (arXiv:2107.06516v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>",
          "description": "Multi-choice Machine Reading Comprehension (MRC) as a challenge requires\nmodel to select the most appropriate answer from a set of candidates given\npassage and question. Most of the existing researches focus on the modeling of\nthe task datasets without explicitly referring to external fine-grained\nknowledge sources, which is supposed to greatly make up the deficiency of the\ngiven passage. Thus we propose a novel reference-based knowledge enhancement\nmodel called Reference Knowledgeable Network (RekNet), which refines critical\ninformation from the passage and quote explicit knowledge in necessity. In\ndetail, RekNet refines fine-grained critical information and defines it as\nReference Span, then quotes explicit knowledge quadruples by the co-occurrence\ninformation of Reference Span and candidates. The proposed RekNet is evaluated\non three multi-choice MRC benchmarks: RACE, DREAM and Cosmos QA, which shows\nconsistent and remarkable performance improvement with observable statistical\nsignificance level over strong baselines.",
          "link": "http://arxiv.org/abs/2012.03709",
          "publishedOn": "2021-07-15T01:59:02.527Z",
          "wordCount": 602,
          "title": "Reference Knowledgeable Network for Machine Reading Comprehension. (arXiv:2012.03709v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>",
          "description": "Emotion recognition in conversation (ERC) is a crucial component in affective\ndialogue systems, which helps the system understand users' emotions and\ngenerate empathetic responses. However, most works focus on modeling speaker\nand contextual information primarily on the textual modality or simply\nleveraging multimodal information through feature concatenation. In order to\nexplore a more effective way of utilizing both multimodal and long-distance\ncontextual information, we propose a new model based on multimodal fused graph\nconvolutional network, MMGCN, in this work. MMGCN can not only make use of\nmultimodal dependencies effectively, but also leverage speaker information to\nmodel inter-speaker and intra-speaker dependency. We evaluate our proposed\nmodel on two public benchmark datasets, IEMOCAP and MELD, and the results prove\nthe effectiveness of MMGCN, which outperforms other SOTA methods by a\nsignificant margin under the multimodal conversation setting.",
          "link": "http://arxiv.org/abs/2107.06779",
          "publishedOn": "2021-07-15T01:59:02.521Z",
          "wordCount": 587,
          "title": "MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation. (arXiv:2107.06779v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Awalina_A/0/1/0/all/0/1\">Aisyah Awalina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fawaid_J/0/1/0/all/0/1\">Jibran Fawaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krisnabayu_R/0/1/0/all/0/1\">Rifky Yunus Krisnabayu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1\">Novanto Yudistira</a>",
          "description": "Fake news is a problem faced by society in this era. It is not rare for fake\nnews to cause provocation and problem for the people. Indonesia, as a country\nwith the 4th largest population, has a problem in dealing with fake news. More\nthan 30% of rural and urban population are deceived by this fake news problem.\nAs we have been studying, there is only few literatures on preventing the\nspread of fake news in Bahasa Indonesia. So, this research is conducted to\nprevent these problems. The dataset used in this research was obtained from a\nnews portal that identifies fake news, turnbackhoax.id. Using Web Scrapping on\nthis page, we got 1116 data consisting of valid news and fake news. The dataset\ncan be accessed at https://github.com/JibranFawaid/turnbackhoax-dataset. This\ndataset will be combined with other available datasets. The methods used are\nCNN, BiLSTM, Hybrid CNN-BiLSTM, and BERT with Transformer Network. This\nresearch shows that the BERT method with Transformer Network has the best\nresults with an accuracy of up to 90%.",
          "link": "http://arxiv.org/abs/2107.06796",
          "publishedOn": "2021-07-15T01:59:02.515Z",
          "wordCount": 608,
          "title": "Indonesia's Fake News Detection using Transformer Network. (arXiv:2107.06796v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Imani_A/0/1/0/all/0/1\">Ayyoob Imani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_M/0/1/0/all/0/1\">Masoud Jalili Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufter_P/0/1/0/all/0/1\">Philipp Dufter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cysouw_M/0/1/0/all/0/1\">Michael Cysouw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>",
          "description": "With more than 7000 languages worldwide, multilingual natural language\nprocessing (NLP) is essential both from an academic and commercial perspective.\nResearching typological properties of languages is fundamental for progress in\nmultilingual NLP. Examples include assessing language similarity for effective\ntransfer learning, injecting inductive biases into machine learning models or\ncreating resources such as dictionaries and inflection tables. We provide\nParCourE, an online tool that allows to browse a word-aligned parallel corpus,\ncovering 1334 languages. We give evidence that this is useful for typological\nresearch. ParCourE can be set up for any parallel corpus and can thus be used\nfor typological research on other corpora as well as for exploring their\nquality and properties.",
          "link": "http://arxiv.org/abs/2107.06632",
          "publishedOn": "2021-07-15T01:59:02.507Z",
          "wordCount": 576,
          "title": "ParCourE: A Parallel Corpus Explorer fora Massively Multilingual Corpus. (arXiv:2107.06632v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nystrom_A/0/1/0/all/0/1\">Andrew Nystrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eck_D/0/1/0/all/0/1\">Douglas Eck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>",
          "description": "We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.",
          "link": "http://arxiv.org/abs/2107.06499",
          "publishedOn": "2021-07-15T01:59:02.453Z",
          "wordCount": 571,
          "title": "Deduplicating Training Data Makes Language Models Better. (arXiv:2107.06499v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tarunesh_I/0/1/0/all/0/1\">Ishan Tarunesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Syamantak Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>",
          "description": "Generating code-switched text is a problem of growing interest, especially\ngiven the scarcity of corpora containing large volumes of real code-switched\ntext. In this work, we adapt a state-of-the-art neural machine translation\nmodel to generate Hindi-English code-switched sentences starting from\nmonolingual Hindi sentences. We outline a carefully designed curriculum of\npretraining steps, including the use of synthetic code-switched text, that\nenable the model to generate high-quality code-switched text. Using text\ngenerated from our model as data augmentation, we show significant reductions\nin perplexity on a language modeling task, compared to using text from other\ngenerative models of CS text. We also show improvements using our text for a\ndownstream code-switched natural language inference task. Our generated text is\nfurther subjected to a rigorous evaluation using a human evaluation study and a\nrange of objective metrics, where we show performance comparable (and sometimes\neven superior) to code-switched text obtained via crowd workers who are native\nHindi speakers.",
          "link": "http://arxiv.org/abs/2107.06483",
          "publishedOn": "2021-07-15T01:59:02.435Z",
          "wordCount": 617,
          "title": "From Machine Translation to Code-Switching: Generating High-Quality Code-Switched Text. (arXiv:2107.06483v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wanying Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuhao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>",
          "description": "Multilingual neural machine translation with a single model has drawn much\nattention due to its capability to deal with multiple languages. However, the\ncurrent multilingual translation paradigm often makes the model tend to\npreserve the general knowledge, but ignore the language-specific knowledge.\nSome previous works try to solve this problem by adding various kinds of\nlanguage-specific modules to the model, but they suffer from the parameter\nexplosion problem and require specialized manual design. To solve these\nproblems, we propose to divide the model neurons into general and\nlanguage-specific parts based on their importance across languages. The general\npart is responsible for preserving the general knowledge and participating in\nthe translation of all the languages, while the language-specific part is\nresponsible for preserving the language-specific knowledge and participating in\nthe translation of some specific languages. Experimental results on several\nlanguage pairs, covering IWSLT and Europarl corpus datasets, demonstrate the\neffectiveness and universality of the proposed method.",
          "link": "http://arxiv.org/abs/2107.06569",
          "publishedOn": "2021-07-15T01:59:02.429Z",
          "wordCount": 589,
          "title": "Importance-based Neuron Allocation for Multilingual Neural Machine Translation. (arXiv:2107.06569v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1\">Kuncahyo Setyo Nugroho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukmadewa_A/0/1/0/all/0/1\">Anantha Yullian Sukmadewa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DW_H/0/1/0/all/0/1\">Haftittah Wuswilahaken DW</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachtiar_F/0/1/0/all/0/1\">Fitra Abdurrachman Bachtiar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1\">Novanto Yudistira</a>",
          "description": "User reviews have an essential role in the success of the developed mobile\napps. User reviews in the textual form are unstructured data, creating a very\nhigh complexity when processed for sentiment analysis. Previous approaches that\nhave been used often ignore the context of reviews. In addition, the relatively\nsmall data makes the model overfitting. A new approach, BERT, has been\nintroduced as a transfer learning model with a pre-trained model that has\npreviously been trained to have a better context representation. This study\nexamines the effectiveness of fine-tuning BERT for sentiment analysis using two\ndifferent pre-trained models. Besides the multilingual pre-trained model, we\nuse the pre-trained model that only has been trained in Indonesian. The dataset\nused is Indonesian user reviews of the ten best apps in 2020 in Google Play\nsites. We also perform hyper-parameter tuning to find the optimum trained\nmodel. Two training data labeling approaches were also tested to determine the\neffectiveness of the model, which is score-based and lexicon-based. The\nexperimental results show that pre-trained models trained in Indonesian have\nbetter average accuracy on lexicon-based data. The pre-trained Indonesian model\nhighest accuracy is 84%, with 25 epochs and a training time of 24 minutes.\nThese results are better than all of the machine learning and multilingual\npre-trained models.",
          "link": "http://arxiv.org/abs/2107.06802",
          "publishedOn": "2021-07-15T01:59:02.403Z",
          "wordCount": 657,
          "title": "BERT Fine-Tuning for Sentiment Analysis on Indonesian Mobile Apps Reviews. (arXiv:2107.06802v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_R/0/1/0/all/0/1\">Razin A. Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_L/0/1/0/all/0/1\">Lia Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodatz_B/0/1/0/all/0/1\">Benjamin Rodatz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coecke_B/0/1/0/all/0/1\">Bob Coecke</a>",
          "description": "Negation in natural language does not follow Boolean logic and is therefore\ninherently difficult to model. In particular, it takes into account the broader\nunderstanding of what is being negated. In previous work, we proposed a\nframework for negation of words that accounts for `worldly context'. In this\npaper, we extend that proposal now accounting for the compositional structure\ninherent in language, within the DisCoCirc framework. We compose the negations\nof single words to capture the negation of sentences. We also describe how to\nmodel the negation of words whose meanings evolve in the text.",
          "link": "http://arxiv.org/abs/2107.06820",
          "publishedOn": "2021-07-15T01:59:02.397Z",
          "wordCount": 532,
          "title": "Composing Conversational Negation. (arXiv:2107.06820v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1\">Kuncahyo Setyo Nugroho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1\">Kuncahyo Setyo Nugroho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1\">Novanto Yudistira</a>",
          "description": "The rise of big data analytics on top of NLP increases the computational\nburden for text processing at scale. The problems faced in NLP are very high\ndimensional text, so it takes a high computation resource. The MapReduce allows\nparallelization of large computations and can improve the efficiency of text\nprocessing. This research aims to study the effect of big data processing on\nNLP tasks based on a deep learning approach. We classify a big text of news\ntopics with fine-tuning BERT used pre-trained models. Five pre-trained models\nwith a different number of parameters were used in this study. To measure the\nefficiency of this method, we compared the performance of the BERT with the\npipelines from Spark NLP. The result shows that BERT without Spark NLP gives\nhigher accuracy compared to BERT with Spark NLP. The accuracy average and\ntraining time of all models using BERT is 0.9187 and 35 minutes while using\nBERT with Spark NLP pipeline is 0.8444 and 9 minutes. The bigger model will\ntake more computation resources and need a longer time to complete the tasks.\nHowever, the accuracy of BERT with Spark NLP only decreased by an average of\n5.7%, while the training time was reduced significantly by 62.9% compared to\nBERT without Spark NLP.",
          "link": "http://arxiv.org/abs/2107.06785",
          "publishedOn": "2021-07-15T01:59:02.381Z",
          "wordCount": 653,
          "title": "Large-Scale News Classification using BERT Language Model: Spark NLP Approach. (arXiv:2107.06785v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06776",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Coecke_B/0/1/0/all/0/1\">Bob Coecke</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Felice_G/0/1/0/all/0/1\">Giovanni de Felice</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Meichanetzidis_K/0/1/0/all/0/1\">Konstantinos Meichanetzidis</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Toumi_A/0/1/0/all/0/1\">Alexis Toumi</a>",
          "description": "This is a story about making quantum computers speak, and doing so in a\nquantum-native, compositional and meaning-aware manner. Recently we did\nquestion-answering with an actual quantum computer. We explain what we did,\nstress that this was all done in terms of pictures, and provide many pointers\nto the related literature. In fact, besides natural language, many other things\ncan be implemented in a quantum-native, compositional and meaning-aware manner,\nand we provide the reader with some indications of that broader pictorial\nlandscape, including our account on the notion of compositionality. We also\nprovide some guidance for the actual execution, so that the reader can give it\na go as well.",
          "link": "http://arxiv.org/abs/2107.06776",
          "publishedOn": "2021-07-15T01:59:02.375Z",
          "wordCount": 550,
          "title": "How to make qubits speak. (arXiv:2107.06776v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cabanac_G/0/1/0/all/0/1\">Guillaume Cabanac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labbe_C/0/1/0/all/0/1\">Cyril Labb&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magazinov_A/0/1/0/all/0/1\">Alexander Magazinov</a>",
          "description": "Probabilistic text generators have been used to produce fake scientific\npapers for more than a decade. Such nonsensical papers are easily detected by\nboth human and machine. Now more complex AI-powered generation techniques\nproduce texts indistinguishable from that of humans and the generation of\nscientific texts from a few keywords has been documented. Our study introduces\nthe concept of tortured phrases: unexpected weird phrases in lieu of\nestablished ones, such as 'counterfeit consciousness' instead of 'artificial\nintelligence.' We combed the literature for tortured phrases and study one\nreputable journal where these concentrated en masse. Hypothesising the use of\nadvanced language models we ran a detector on the abstracts of recent articles\nof this journal and on several control sets. The pairwise comparisons reveal a\nconcentration of abstracts flagged as 'synthetic' in the journal. We also\nhighlight irregularities in its operation, such as abrupt changes in editorial\ntimelines. We substantiate our call for investigation by analysing several\nindividual dubious articles, stressing questionable features: tortured writing\nstyle, citation of non-existent literature, and unacknowledged image reuse.\nSurprisingly, some websites offer to rewrite texts for free, generating\ngobbledegook full of tortured phrases. We believe some authors used rewritten\ntexts to pad their manuscripts. We wish to raise the awareness on publications\ncontaining such questionable AI-generated or rewritten texts that passed (poor)\npeer review. Deception with synthetic texts threatens the integrity of the\nscientific literature.",
          "link": "http://arxiv.org/abs/2107.06751",
          "publishedOn": "2021-07-15T01:59:02.369Z",
          "wordCount": 688,
          "title": "Tortured phrases: A dubious writing style emerging in science. Evidence of critical issues affecting established journals. (arXiv:2107.06751v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06546",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alishahia_A/0/1/0/all/0/1\">Afra Alishahia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrupala_G/0/1/0/all/0/1\">Grzegorz Chrupa&#x142;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristia_A/0/1/0/all/0/1\">Alejandrina Cristia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higy_B/0/1/0/all/0/1\">Bertrand Higy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavechin_M/0/1/0/all/0/1\">Marvin Lavechin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chen Yu</a>",
          "description": "We present the visually-grounded language modelling track that was introduced\nin the Zero-Resource Speech challenge, 2021 edition, 2nd round. We motivate the\nnew track and discuss participation rules in detail. We also present the two\nbaseline systems that were developed for this track.",
          "link": "http://arxiv.org/abs/2107.06546",
          "publishedOn": "2021-07-15T01:59:02.362Z",
          "wordCount": 501,
          "title": "ZR-2021VG: Zero-Resource Speech Challenge, Visually-Grounded Language Modelling track, 2021 edition. (arXiv:2107.06546v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nagaraja_V/0/1/0/all/0/1\">Varun Nagaraja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yangyang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1\">Ganesh Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1\">Michael L. Seltzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1\">Vikas Chandra</a>",
          "description": "On-device speech recognition requires training models of different sizes for\ndeploying on devices with various computational budgets. When building such\ndifferent models, we can benefit from training them jointly to take advantage\nof the knowledge shared between them. Joint training is also efficient since it\nreduces the redundancy in the training procedure's data handling operations. We\npropose a method for collaboratively training acoustic encoders of different\nsizes for speech recognition. We use a sequence transducer setup where\ndifferent acoustic encoders share a common predictor and joiner modules. The\nacoustic encoders are also trained using co-distillation through an auxiliary\ntask for frame level chenone prediction, along with the transducer loss. We\nperform experiments using the LibriSpeech corpus and demonstrate that the\ncollaboratively trained acoustic encoders can provide up to a 11% relative\nimprovement in the word error rate on both the test partitions.",
          "link": "http://arxiv.org/abs/2106.08960",
          "publishedOn": "2021-07-15T01:59:02.356Z",
          "wordCount": 622,
          "title": "Collaborative Training of Acoustic Encoders for Speech Recognition. (arXiv:2106.08960v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>",
          "description": "Transformer is important for text modeling. However, it has difficulty in\nhandling long documents due to the quadratic complexity with input text length.\nIn order to handle this problem, we propose a hierarchical interactive\nTransformer (Hi-Transformer) for efficient and effective long document\nmodeling. Hi-Transformer models documents in a hierarchical way, i.e., first\nlearns sentence representations and then learns document representations. It\ncan effectively reduce the complexity and meanwhile capture global document\ncontext in the modeling of each sentence. More specifically, we first use a\nsentence Transformer to learn the representations of each sentence. Then we use\na document Transformer to model the global document context from these sentence\nrepresentations. Next, we use another sentence Transformer to enhance sentence\nmodeling using the global document context. Finally, we use hierarchical\npooling method to obtain document embedding. Extensive experiments on three\nbenchmark datasets validate the efficiency and effectiveness of Hi-Transformer\nin long document modeling.",
          "link": "http://arxiv.org/abs/2106.01040",
          "publishedOn": "2021-07-15T01:59:02.349Z",
          "wordCount": 619,
          "title": "Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling. (arXiv:2106.01040v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bayer_M/0/1/0/all/0/1\">Markus Bayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufhold_M/0/1/0/all/0/1\">Marc-Andr&#xe9; Kaufhold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_C/0/1/0/all/0/1\">Christian Reuter</a>",
          "description": "Data augmentation, the artificial creation of training data for machine\nlearning by transformations, is a widely studied research field across machine\nlearning disciplines. While it is useful for increasing the generalization\ncapabilities of a model, it can also address many other challenges and\nproblems, from overcoming a limited amount of training data over regularizing\nthe objective to limiting the amount data used to protect privacy. Based on a\nprecise description of the goals and applications of data augmentation (C1) and\na taxonomy for existing works (C2), this survey is concerned with data\naugmentation methods for textual classification and aims to achieve a concise\nand comprehensive overview for researchers and practitioners (C3). Derived from\nthe taxonomy, we divided more than 100 methods into 12 different groupings and\nprovide state-of-the-art references expounding which methods are highly\npromising (C4). Finally, research perspectives that may constitute a building\nblock for future work are given (C5).",
          "link": "http://arxiv.org/abs/2107.03158",
          "publishedOn": "2021-07-15T01:59:02.329Z",
          "wordCount": 608,
          "title": "A Survey on Data Augmentation for Text Classification. (arXiv:2107.03158v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1\">Ahmad Beirami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1\">Ankita De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1\">Alborz Geramifard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1\">Chinnadhurai Sankar</a>",
          "description": "MultiWOZ is one of the most popular multi-domain task-oriented dialog\ndatasets, containing 10K+ annotated dialogs covering eight domains. It has been\nwidely accepted as a benchmark for various dialog tasks, e.g., dialog state\ntracking (DST), natural language generation (NLG), and end-to-end (E2E) dialog\nmodeling. In this work, we identify an overlooked issue with dialog state\nannotation inconsistencies in the dataset, where a slot type is tagged\ninconsistently across similar dialogs leading to confusion for DST modeling. We\npropose an automated correction for this issue, which is present in a whopping\n70% of the dialogs. Additionally, we notice that there is significant entity\nbias in the dataset (e.g., \"cambridge\" appears in 50% of the destination cities\nin the train domain). The entity bias can potentially lead to named entity\nmemorization in generative models, which may go unnoticed as the test set\nsuffers from a similar entity bias as well. We release a new test set with all\nentities replaced with unseen entities. Finally, we benchmark joint goal\naccuracy (JGA) of the state-of-the-art DST baselines on these modified versions\nof the data. Our experiments show that the annotation inconsistency corrections\nlead to 7-10% improvement in JGA. On the other hand, we observe a 29% drop in\nJGA when models are evaluated on the new test set with unseen entities.",
          "link": "http://arxiv.org/abs/2105.14150",
          "publishedOn": "2021-07-15T01:59:02.318Z",
          "wordCount": 689,
          "title": "Annotation Inconsistency and Entity Bias in MultiWOZ. (arXiv:2105.14150v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00120",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lohrenz_T/0/1/0/all/0/1\">Timo Lohrenz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhengyang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fingscheidt_T/0/1/0/all/0/1\">Tim Fingscheidt</a>",
          "description": "Stream fusion, also known as system combination, is a common technique in\nautomatic speech recognition for traditional hybrid hidden Markov model\napproaches, yet mostly unexplored for modern deep neural network end-to-end\nmodel architectures. Here, we investigate various fusion techniques for the\nall-attention-based encoder-decoder architecture known as the transformer,\nstriving to achieve optimal fusion by investigating different fusion levels in\nan example single-microphone setting with fusion of standard magnitude and\nphase features. We introduce a novel multi-encoder learning method that\nperforms a weighted combination of two encoder-decoder multi-head attention\noutputs only during training. Employing then only the magnitude feature encoder\nin inference, we are able to show consistent improvement on Wall Street Journal\n(WSJ) with language model and on Librispeech, without increase in runtime or\nparameters. Combining two such multi-encoder trained models by a simple late\nfusion in inference, we achieve state-of-the-art performance for\ntransformer-based models on WSJ with a significant WER reduction of 19%\nrelative compared to the current benchmark approach.",
          "link": "http://arxiv.org/abs/2104.00120",
          "publishedOn": "2021-07-15T01:59:02.309Z",
          "wordCount": 638,
          "title": "Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition. (arXiv:2104.00120v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Camargo_A/0/1/0/all/0/1\">Augusto Camargo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_W/0/1/0/all/0/1\">Wesley Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peressim_F/0/1/0/all/0/1\">Felipe Peressim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_A/0/1/0/all/0/1\">Alan Barzilay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finger_M/0/1/0/all/0/1\">Marcelo Finger</a>",
          "description": "In this paper, we studied whether models based on BiLSTM and BERT can predict\nhashtags in Brazilian Portuguese for Ecommerce websites. Hashtags have a\nsizable financial impact on Ecommerce. We processed a corpus of Ecommerce\nreviews as inputs, and predicted hashtags as outputs. We evaluated the results\nusing four quantitative metrics: NIST, BLEU, METEOR and a crowdsourced score. A\nword cloud was used as a qualitative metric. While all computer-generated\nmetrics (NIST, BLEU and METEOR) indicated bad results, the crowdsourced results\nproduced amazing scores. We concluded that the texts predicted by the neural\nnetworks are very promising for use as hashtags for products on Ecommerce\nwebsites. The code for this work is available at\nhttps://github.com/augustocamargo/text-to-hashtag.",
          "link": "http://arxiv.org/abs/2102.00904",
          "publishedOn": "2021-07-15T01:59:02.301Z",
          "wordCount": 573,
          "title": "Text-to-hashtag Generation using Seq2seq Learning. (arXiv:2102.00904v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klicpera_J/0/1/0/all/0/1\">Johannes Klicpera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lienen_M/0/1/0/all/0/1\">Marten Lienen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "The current best practice for computing optimal transport (OT) is via entropy\nregularization and Sinkhorn iterations. This algorithm runs in quadratic time\nas it requires the full pairwise cost matrix, which is prohibitively expensive\nfor large sets of objects. In this work we propose two effective log-linear\ntime approximations of the cost matrix: First, a sparse approximation based on\nlocality-sensitive hashing (LSH) and, second, a Nystr\\\"om approximation with\nLSH-based sparse corrections, which we call locally corrected Nystr\\\"om (LCN).\nThese approximations enable general log-linear time algorithms for\nentropy-regularized OT that perform well even for the complex, high-dimensional\nspaces common in deep learning. We analyse these approximations theoretically\nand evaluate them experimentally both directly and end-to-end as a component\nfor real-world applications. Using our approximations for unsupervised word\nembedding alignment enables us to speed up a state-of-the-art method by a\nfactor of 3 while also improving the accuracy by 3.1 percentage points without\nany additional model changes. For graph distance regression we propose the\ngraph transport network (GTN), which combines graph neural networks (GNNs) with\nenhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales\nlog-linearly in the number of nodes.",
          "link": "http://arxiv.org/abs/2107.06876",
          "publishedOn": "2021-07-15T01:59:02.294Z",
          "wordCount": 658,
          "title": "Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More. (arXiv:2107.06876v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sujeong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamrakar_A/0/1/0/all/0/1\">Amir Tamrakar</a>",
          "description": "We propose a method to generate optimal natural language for block placement\ndirectives generated by a machine's planner during human-agent interactions in\nthe blocks world. A non user-friendly machine directive, e.g., move(ObjId,\ntoPos), is transformed into visually and contextually grounded referring\nexpressions that are much easier for the user to comprehend. We describe an\nalgorithm that progressively and generatively transforms the machine's\ndirective in ECI (Elementary Composable Ideas)-space, generating many\nalternative versions of the directive. We then define a cost function to\nevaluate the ease of comprehension of these alternatives and select the best\noption. The parameters for this cost function were derived empirically from a\nuser study that measured utterance-to-action timings.",
          "link": "http://arxiv.org/abs/2107.06886",
          "publishedOn": "2021-07-15T01:59:02.276Z",
          "wordCount": 564,
          "title": "\"How to best say it?\" : Translating Directives in Machine Language into Natural Language in the Blocks World. (arXiv:2107.06886v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bei Yu</a>",
          "description": "Accurately linking news articles to scientific research works is a critical\ncomponent in a number of applications, such as measuring the social impact of a\nresearch work and detecting inaccuracies or distortions in science news.\nAlthough the lack of links between news and literature has been a challenge in\nthese applications, it is a relatively unexplored research problem. In this\npaper we designed and evaluated a new approach that consists of (1) augmenting\nlatest named-entity recognition techniques to extract various metadata, and (2)\ndesigning a new elastic search engine that can facilitate the use of enriched\nmetadata queries. To evaluate our approach, we constructed two datasets of\npaired news articles and research papers: one is used for training models to\nextract metadata, and the other for evaluation. Our experiments showed that the\nnew approach performed significantly better than a baseline approach used by\naltmetric.com (0.89 vs 0.32 in terms of top-1 accuracy). To further demonstrate\nthe effectiveness of the approach, we also conducted a study on 37,600\nhealth-related press releases published on EurekAlert!, which showed that our\napproach was able to identify the corresponding research papers with a top-1\naccuracy of at least 0.97.",
          "link": "http://arxiv.org/abs/2107.06472",
          "publishedOn": "2021-07-15T01:59:02.269Z",
          "wordCount": 632,
          "title": "Linking Health News to Research Literature. (arXiv:2107.06472v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "Most existing Vision-and-Language (V&L) models rely on pre-trained visual\nencoders, using a relatively small set of manually-annotated data (as compared\nto web-crawled data), to perceive the visual world. However, it has been\nobserved that large-scale pretraining usually can result in better\ngeneralization performance, e.g., CLIP (Contrastive Language-Image\nPre-training), trained on a massive amount of image-caption pairs, has shown a\nstrong zero-shot capability on various vision tasks. To further study the\nadvantage brought by CLIP, we propose to use CLIP as the visual encoder in\nvarious V&L models in two typical scenarios: 1) plugging CLIP into\ntask-specific fine-tuning; 2) combining CLIP with V&L pre-training and\ntransferring to downstream tasks. We show that CLIP significantly outperforms\nwidely-used visual encoders trained with in-domain annotated data, such as\nBottomUp-TopDown. We achieve competitive or better results on diverse V&L\ntasks, while establishing new state-of-the-art results on Visual Question\nAnswering, Visual Entailment, and V&L Navigation tasks. We release our code at\nhttps://github.com/clip-vil/CLIP-ViL.",
          "link": "http://arxiv.org/abs/2107.06383",
          "publishedOn": "2021-07-15T01:59:02.234Z",
          "wordCount": 618,
          "title": "How Much Can CLIP Benefit Vision-and-Language Tasks?. (arXiv:2107.06383v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Galeano_S/0/1/0/all/0/1\">Sergio Rojas-Galeano</a>",
          "description": "One of the stratagems used to deceive spam filters is to substitute vocables\nwith synonyms or similar words that turn the message unrecognisable by the\ndetection algorithms. In this paper we investigate whether the recent\ndevelopment of language models sensitive to the semantics and context of words,\nsuch as Google's BERT, may be useful to overcome this adversarial attack\n(called \"Mad-lib\" as per the word substitution game). Using a dataset of 5572\nSMS spam messages, we first established a baseline of detection performance\nusing widely known document representation models (BoW and TFIDF) and the novel\nBERT model, coupled with a variety of classification algorithms (Decision Tree,\nkNN, SVM, Logistic Regression, Naive Bayes, Multilayer Perceptron). Then, we\nbuilt a thesaurus of the vocabulary contained in these messages, and set up a\nMad-lib attack experiment in which we modified each message of a held out\nsubset of data (not used in the baseline experiment) with different rates of\nsubstitution of original words with synonyms from the thesaurus. Lastly, we\nevaluated the detection performance of the three representation models (BoW,\nTFIDF and BERT) coupled with the best classifier from the baseline experiment\n(SVM). We found that the classic models achieved a 94% Balanced Accuracy (BA)\nin the original dataset, whereas the BERT model obtained 96%. On the other\nhand, the Mad-lib attack experiment showed that BERT encodings manage to\nmaintain a similar BA performance of 96% with an average substitution rate of\n1.82 words per message, and 95% with 3.34 words substituted per message. In\ncontrast, the BA performance of the BoW and TFIDF encoders dropped to chance.\nThese results hint at the potential advantage of BERT models to combat these\ntype of ingenious attacks, offsetting to some extent for the inappropriate use\nof semantic relationships in language.",
          "link": "http://arxiv.org/abs/2107.06400",
          "publishedOn": "2021-07-15T01:59:02.123Z",
          "wordCount": 732,
          "title": "Using BERT Encoding to Tackle the Mad-lib Attack in SMS Spam Detection. (arXiv:2107.06400v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nath_A/0/1/0/all/0/1\">Apurba Nath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubba_A/0/1/0/all/0/1\">Aayush Kubba</a>",
          "description": "Can we discover dialog structure by dividing utterances into labelled\nclusters. Can these labels be generated from the data. Typically for dialogs we\nneed an ontology and use that to discover structure, however by using\nunsupervised classification and self-labelling we are able to intuit this\nstructure without any labels or ontology. In this paper we apply SCAN (Semantic\nClustering using Nearest Neighbors) to dialog data. We used BERT for pretext\ntask and an adaptation of SCAN for clustering and self labeling. These clusters\nare used to identify transition probabilities and create the dialog structure.\nThe self-labelling method used for SCAN makes these structures interpretable as\nevery cluster has a label. As the approach is unsupervised, evaluation metrics\nis a challenge, we use statistical measures as proxies for structure quality",
          "link": "http://arxiv.org/abs/2107.06426",
          "publishedOn": "2021-07-15T01:59:02.099Z",
          "wordCount": 561,
          "title": "TSCAN : Dialog Structure discovery using SCAN. (arXiv:2107.06426v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zining Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>",
          "description": "As the numbers of submissions to conferences grow quickly, the task of\nassessing the quality of academic papers automatically, convincingly, and with\nhigh accuracy attracts increasing attention. We argue that studying\ninterpretable dimensions of these submissions could lead to scalable solutions.\nWe extract a collection of writing features, and construct a suite of\nprediction tasks to assess the usefulness of these features in predicting\ncitation counts and the publication of AI-related papers. Depending on the\nvenues, the writing features can predict the conference vs. workshop appearance\nwith F1 scores up to 60-90, sometimes even outperforming the content-based\ntf-idf features and RoBERTa. We show that the features describe writing style\nmore than content. To further understand the results, we estimate the causal\nimpact of the most indicative features. Our analysis on writing features\nprovides a perspective to assessing and refining the writing of academic\narticles at scale.",
          "link": "http://arxiv.org/abs/2107.06310",
          "publishedOn": "2021-07-15T01:59:02.084Z",
          "wordCount": 585,
          "title": "What do writing features tell us about AI papers?. (arXiv:2107.06310v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2104.11783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanci Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1\">Tianming Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yujie Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donohue_L/0/1/0/all/0/1\">Lawrence Donohue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_R/0/1/0/all/0/1\">Rui Dai</a>",
          "description": "The quarterly financial statement, or Form 10-Q, is one of the most\nfrequently required filings for US public companies to disclose financial and\nother important business information. Due to the massive volume of 10-Q filings\nand the enormous variations in the reporting format, it has been a\nlong-standing challenge to retrieve item-specific information from 10-Q filings\nthat lack machine-readable hierarchy. This paper presents a solution for\nitemizing 10-Q files by complementing a rule-based algorithm with a\nConvolutional Neural Network (CNN) image classifier. This solution demonstrates\na pipeline that can be generalized to a rapid data retrieval solution among a\nlarge volume of textual data using only typographic items. The extracted\ntextual data can be used as unlabeled content-specific data to train\ntransformer models (e.g., BERT) or fit into various field-focus natural\nlanguage processing (NLP) applications.",
          "link": "http://arxiv.org/abs/2104.11783",
          "publishedOn": "2021-07-20T02:04:39.008Z",
          "wordCount": 601,
          "title": "Form 10-Q Itemization. (arXiv:2104.11783v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_N/0/1/0/all/0/1\">Niloofar Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouvelas_N/0/1/0/all/0/1\">Nikolaos Kouvelas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_R/0/1/0/all/0/1\">R Venkatesha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucani_D/0/1/0/all/0/1\">Daniel E. Lucani</a>",
          "description": "High frame-corruption is widely observed in Long Range Wide Area Networks\n(LoRaWAN) due to the coexistence with other networks in ISM bands and an\nAloha-like MAC layer. LoRa's Forward Error Correction (FEC) mechanism is often\ninsufficient to retrieve corrupted data. In fact, real-life measurements show\nthat at least one-fourth of received transmissions are corrupted. When more\nframes are dropped, LoRa nodes usually switch over to higher spreading factors\n(SF), thus increasing transmission times and increasing the required energy.\nThis paper introduces ReDCoS, a novel coding technique at the application layer\nthat improves recovery of corrupted LoRa frames, thus reducing the overall\ntransmission time and energy invested by LoRa nodes by several-fold. ReDCoS\nutilizes lightweight coding techniques to pre-encode the transmitted data.\nTherefore, the inbuilt Cyclic Redundancy Check (CRC) that follows is computed\nbased on an already encoded data. At the receiver, we use both the CRC and the\ncoded data to recover data from a corrupted frame beyond the built-in Error\nCorrecting Code (ECC). We compare the performance of ReDCoS to (I) the standard\nFEC of vanilla-LoRaWAN, and to (ii) RS coding applied as ECC to the data of\nLoRaWAN. The results indicated a 54x and 13.5x improvement of decoding ratio,\nrespectively, when 20 data symbols were sent. Furthermore, we evaluated ReDCoS\non-field using LoRa SX1261 transceivers showing that it outperformed RS-coding\nby factor of at least 2x (and up to 6x) in terms of the decoding ratio while\nconsuming 38.5% less energy per correctly received transmission.",
          "link": "http://arxiv.org/abs/2107.08868",
          "publishedOn": "2021-07-20T02:04:38.982Z",
          "wordCount": 688,
          "title": "Energy Efficient Data Recovery from Corrupted LoRa Frames. (arXiv:2107.08868v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mathew_M/0/1/0/all/0/1\">Minesh Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1\">Ruben Tito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1\">R. Manmatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C.V. Jawahar</a>",
          "description": "This paper presents results of Document Visual Question Answering Challenge\norganized as part of \"Text and Documents in the Deep Learning Era\" workshop, in\nCVPR 2020. The challenge introduces a new problem - Visual Question Answering\non document images. The challenge comprised two tasks. The first task concerns\nwith asking questions on a single document image. On the other hand, the second\ntask is set as a retrieval task where the question is posed over a collection\nof images. For the task 1 a new dataset is introduced comprising 50,000\nquestions-answer(s) pairs defined over 12,767 document images. For task 2\nanother dataset has been created comprising 20 questions over 14,362 document\nimages which share the same document template.",
          "link": "http://arxiv.org/abs/2008.08899",
          "publishedOn": "2021-07-20T02:04:38.396Z",
          "wordCount": 597,
          "title": "Document Visual Question Answering Challenge 2020. (arXiv:2008.08899v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pourkamali_F/0/1/0/all/0/1\">Farzad Pourkamali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macris_N/0/1/0/all/0/1\">Nicolas Macris</a>",
          "description": "We consider the estimation of an n-dimensional vector s from the noisy\nelement-wise measurements of $\\mathbf{s}\\mathbf{s}^T$, a generic problem that\narises in statistics and machine learning. We study a mismatched Bayesian\ninference setting, where some of the parameters are not known to the\nstatistician. We derive the full exact analytic expression of the asymptotic\nmean squared error (MSE) in the large system size limit for the particular case\nof Gaussian priors and additive noise. From our formulas, we see that\nestimation is still possible in the mismatched case; and also that the minimum\nMSE (MMSE) can be achieved if the statistician chooses suitable parameters. Our\ntechnique relies on the asymptotics of the spherical integrals and can be\napplied as long as the statistician chooses a rotationally invariant prior.",
          "link": "http://arxiv.org/abs/2107.08927",
          "publishedOn": "2021-07-20T02:04:38.368Z",
          "wordCount": 562,
          "title": "Mismatched Estimation of rank-one symmetric matrices under Gaussian noise. (arXiv:2107.08927v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Feiyang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yanrong Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_A/0/1/0/all/0/1\">Ao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "The exploration/exploitation (E&E) dilemma lies at the core of interactive\nsystems such as online advertising, for which contextual bandit algorithms have\nbeen proposed. Bayesian approaches provide guided exploration with principled\nuncertainty estimation, but the applicability is often limited due to\nover-simplified assumptions. Non-Bayesian bootstrap methods, on the other hand,\ncan apply to complex problems by using deep reward models, but lacks clear\nguidance to the exploration behavior. It still remains largely unsolved to\ndevelop a practical method for complex deep contextual bandits.\n\nIn this paper, we introduce Guided Bootstrap (GuideBoot for short), combining\nthe best of both worlds. GuideBoot provides explicit guidance to the\nexploration behavior by training multiple models over both real samples and\nnoisy samples with fake labels, where the noise is added according to the\npredictive uncertainty. The proposed method is efficient as it can make\ndecisions on-the-fly by utilizing only one randomly chosen model, but is also\neffective as we show that it can be viewed as a non-Bayesian approximation of\nThompson sampling. Moreover, we extend it to an online version that can learn\nsolely from streaming data, which is favored in real applications. Extensive\nexperiments on both synthetic task and large-scale advertising environments\nshow that GuideBoot achieves significant improvements against previous\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.08383",
          "publishedOn": "2021-07-20T02:04:38.289Z",
          "wordCount": 649,
          "title": "GuideBoot: Guided Bootstrap for Deep Contextual Bandits. (arXiv:2107.08383v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_D/0/1/0/all/0/1\">Divya Shanmugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabanian_S/0/1/0/all/0/1\">Samira Shabanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finck_M/0/1/0/all/0/1\">Mich&#xe8;le Finck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biega_A/0/1/0/all/0/1\">Asia Biega</a>",
          "description": "Data minimization is a legal obligation defined in the European Union's\nGeneral Data Protection Regulation (GDPR) as the responsibility to process an\nadequate, relevant, and limited amount of personal data in relation to a\nprocessing purpose. However, unlike fairness or transparency, the principle has\nnot seen wide adoption for machine learning systems due to a lack of\ncomputational interpretation. In this paper, we build on literature in machine\nlearning and law to propose the first learning framework for limiting data\ncollection based on an interpretation that ties the data collection purpose to\nsystem performance. We formalize a data minimization criterion based on\nperformance curve derivatives and provide an effective and interpretable\npiecewise power law technique that models distinct stages of an algorithm's\nperformance throughout data collection. Results from our empirical\ninvestigation offer deeper insights into the relevant considerations when\ndesigning a data minimization framework, including the choice of feature\nacquisition algorithm, initialization conditions, as well as impacts on\nindividuals that hint at tensions between data minimization and fairness.",
          "link": "http://arxiv.org/abs/2107.08096",
          "publishedOn": "2021-07-20T02:04:38.147Z",
          "wordCount": 618,
          "title": "Learning to Limit Data Collection via Scaling Laws: Data Minimization Compliance in Practice. (arXiv:2107.08096v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_L/0/1/0/all/0/1\">Lakshya Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Sagnik Sarkar</a>",
          "description": "Typical e-commerce platforms contain millions of products in the catalog.\nUsers visit these platforms and enter search queries to retrieve their desired\nproducts. Therefore, showing the relevant products at the top is essential for\nthe success of e-commerce platforms. We approach this problem by learning low\ndimension representations for queries and product descriptions by leveraging\nuser click-stream data as our main source of signal for product relevance.\nStarting from GRU-based architectures as our baseline model, we move towards a\nmore advanced transformer-based architecture. This helps the model to learn\ncontextual representations of queries and products to serve better search\nresults and understand the user intent in an efficient manner. We perform\nexperiments related to pre-training of the Transformer based RoBERTa model\nusing a fashion corpus and fine-tuning it over the triplet loss. Our\nexperiments on the product ranking task show that the RoBERTa model is able to\ngive an improvement of 7.8% in Mean Reciprocal Rank(MRR), 15.8% in Mean Average\nPrecision(MAP) and 8.8% in Normalized Discounted Cumulative Gain(NDCG), thus\noutperforming our GRU based baselines. For the product retrieval task, RoBERTa\nmodel is able to outperform other two models with an improvement of 164.7% in\nPrecision@50 and 145.3% in Recall@50. In order to highlight the importance of\npre-training RoBERTa for fashion domain, we qualitatively compare already\npre-trained RoBERTa on standard datasets with our custom pre-trained RoBERTa\nover a fashion corpus for the query token prediction task. Finally, we also\nshow a qualitative comparison between GRU and RoBERTa results for product\nretrieval task for some test queries.",
          "link": "http://arxiv.org/abs/2107.08291",
          "publishedOn": "2021-07-20T02:04:38.103Z",
          "wordCount": 692,
          "title": "Neural Search: Learning Query and Product Representations in Fashion E-commerce. (arXiv:2107.08291v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yinqiong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yixing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Similar question retrieval is a core task in community-based question\nanswering (CQA) services. To balance the effectiveness and efficiency, the\nquestion retrieval system is typically implemented as multi-stage rankers: The\nfirst-stage ranker aims to recall potentially relevant questions from a large\nrepository, and the latter stages attempt to re-rank the retrieved results.\nMost existing works on question retrieval mainly focused on the re-ranking\nstages, leaving the first-stage ranker to some traditional term-based methods.\nHowever, term-based methods often suffer from the vocabulary mismatch problem,\nespecially on short texts, which may block the re-rankers from relevant\nquestions at the very beginning. An alternative is to employ embedding-based\nmethods for the first-stage ranker, which compress texts into dense vectors to\nenhance the semantic matching. However, these methods often lose the\ndiscriminative power as term-based methods, thus introduce noise during\nretrieval and hurt the recall performance. In this work, we aim to tackle the\ndilemma of the first-stage ranker, and propose a discriminative semantic\nranker, namely DenseTrans, for high-recall retrieval. Specifically, DenseTrans\nis a densely connected Transformer, which learns semantic embeddings for texts\nbased on Transformer layers. Meanwhile, DenseTrans promotes low-level features\nthrough dense connections to keep the discriminative power of the learned\nrepresentations. DenseTrans is inspired by DenseNet in computer vision (CV),\nbut poses a new way to use the dense connectivity which is totally different\nfrom its original design purpose. Experimental results over two question\nretrieval benchmark datasets show that our model can obtain significant gain on\nrecall against strong term-based methods as well as state-of-the-art\nembedding-based methods.",
          "link": "http://arxiv.org/abs/2107.08345",
          "publishedOn": "2021-07-20T02:04:38.062Z",
          "wordCount": 691,
          "title": "A Discriminative Semantic Ranker for Question Retrieval. (arXiv:2107.08345v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey A. Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>",
          "description": "The world of empirical machine learning (ML) strongly relies on benchmarks in\norder to determine the relative effectiveness of different algorithms and\nmethods. This paper proposes the notion of \"a benchmark lottery\" that describes\nthe overall fragility of the ML benchmarking process. The benchmark lottery\npostulates that many factors, other than fundamental algorithmic superiority,\nmay lead to a method being perceived as superior. On multiple benchmark setups\nthat are prevalent in the ML community, we show that the relative performance\nof algorithms may be altered significantly simply by choosing different\nbenchmark tasks, highlighting the fragility of the current paradigms and\npotential fallacious interpretation derived from benchmarking ML methods. Given\nthat every benchmark makes a statement about what it perceives to be important,\nwe argue that this might lead to biased progress in the community. We discuss\nthe implications of the observed phenomena and provide recommendations on\nmitigating them using multiple machine learning domains and communities as use\ncases, including natural language processing, computer vision, information\nretrieval, recommender systems, and reinforcement learning.",
          "link": "http://arxiv.org/abs/2107.07002",
          "publishedOn": "2021-07-19T00:49:05.316Z",
          "wordCount": 620,
          "title": "The Benchmark Lottery. (arXiv:2107.07002v1 [cs.LG] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1\">Jacopo Tagliabue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greco_C/0/1/0/all/0/1\">Ciro Greco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_J/0/1/0/all/0/1\">Jean-Francis Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bingqing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chia_P/0/1/0/all/0/1\">Patrick John Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassani_G/0/1/0/all/0/1\">Giovanni Cassani</a>",
          "description": "The 2021 SIGIR workshop on eCommerce is hosting the Coveo Data Challenge for\n\"In-session prediction for purchase intent and recommendations\". The challenge\naddresses the growing need for reliable predictions within the boundaries of a\nshopping session, as customer intentions can be different depending on the\noccasion. The need for efficient procedures for personalization is even clearer\nif we consider the e-commerce landscape more broadly: outside of giant digital\nretailers, the constraints of the problem are stricter, due to smaller user\nbases and the realization that most users are not frequently returning\ncustomers. We release a new session-based dataset including more than 30M\nfine-grained browsing events (product detail, add, purchase), enriched by\nlinguistic behavior (queries made by shoppers, with items clicked and items not\nclicked after the query) and catalog meta-data (images, text, pricing\ninformation). On this dataset, we ask participants to showcase innovative\nsolutions for two open problems: a recommendation task (where a model is shown\nsome events at the start of a session, and it is asked to predict future\nproduct interactions); an intent prediction task, where a model is shown a\nsession containing an add-to-cart event, and it is asked to predict whether the\nitem will be bought before the end of the session.",
          "link": "http://arxiv.org/abs/2104.09423",
          "publishedOn": "2021-07-19T00:49:05.195Z",
          "wordCount": 695,
          "title": "SIGIR 2021 E-Commerce Workshop Data Challenge. (arXiv:2104.09423v4 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1\">Arpita Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samanta_D/0/1/0/all/0/1\">Debasis Samanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarma_M/0/1/0/all/0/1\">Monalisa Sarma</a>",
          "description": "User intention which often changes dynamically is considered to be an\nimportant factor for modeling users in the design of recommendation systems.\nRecent studies are starting to focus on predicting user intention (what users\nwant) beyond user preference (what users like). In this work, a user intention\nmodel is proposed based on deep sequential topic analysis. The model predicts a\nuser's intention in terms of the topic of interest. The Hybrid Topic Model\n(HTM) comprising Latent Dirichlet Allocation (LDA) and Word2Vec is proposed to\nderive the topic of interest of users and the history of preferences. HTM finds\nthe true topics of papers estimating word-topic distribution which includes\nsyntactic and semantic correlations among words. Next, to model user intention,\na Long Short Term Memory (LSTM) based sequential deep learning model is\nproposed. This model takes into account temporal context, namely the time\ndifference between clicks of two consecutive papers seen by a user. Extensive\nexperiments with the real-world research paper dataset indicate that the\nproposed approach significantly outperforms the state-of-the-art methods.\nFurther, the proposed approach introduces a new road map to model a user\nactivity suitable for the design of a research paper recommendation system.",
          "link": "http://arxiv.org/abs/2107.07831",
          "publishedOn": "2021-07-19T00:49:05.156Z",
          "wordCount": 630,
          "title": "Modeling User Behaviour in Research Paper Recommendation System. (arXiv:2107.07831v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1\">Shivani Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luthra_T/0/1/0/all/0/1\">Tarun Luthra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Ashima Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rajat Singh</a>",
          "description": "Knowledge Graph embedding provides a versatile technique for representing\nknowledge. These techniques can be used in a variety of applications such as\ncompletion of knowledge graph to predict missing information, recommender\nsystems, question answering, query expansion, etc. The information embedded in\nKnowledge graph though being structured is challenging to consume in a\nreal-world application. Knowledge graph embedding enables the real-world\napplication to consume information to improve performance. Knowledge graph\nembedding is an active research area. Most of the embedding methods focus on\nstructure-based information. Recent research has extended the boundary to\ninclude text-based information and image-based information in entity embedding.\nEfforts have been made to enhance the representation with context information.\nThis paper introduces growth in the field of KG embedding from simple\ntranslation-based models to enrichment-based models. This paper includes the\nutility of the Knowledge graph in real-world applications.",
          "link": "http://arxiv.org/abs/2107.07842",
          "publishedOn": "2021-07-19T00:49:05.141Z",
          "wordCount": 580,
          "title": "A Survey of Knowledge Graph Embedding and Their Applications. (arXiv:2107.07842v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shatri_E/0/1/0/all/0/1\">Elona Shatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>",
          "description": "The main challenges of Optical Music Recognition (OMR) come from the nature\nof written music, its complexity and the difficulty of finding an appropriate\ndata representation. This paper provides a first look at DoReMi, an OMR dataset\nthat addresses these challenges, and a baseline object detection model to\nassess its utility. Researchers often approach OMR following a set of small\nstages, given that existing data often do not satisfy broader research. We\nexamine the possibility of changing this tendency by presenting more metadata.\nOur approach complements existing research; hence DoReMi allows harmonisation\nwith two existing datasets, DeepScores and MUSCIMA++. DoReMi was generated\nusing a music notation software and includes over 6400 printed sheet music\nimages with accompanying metadata useful in OMR research. Our dataset provides\nOMR metadata, MIDI, MEI, MusicXML and PNG files, each aiding a different stage\nof OMR. We obtain 64% mean average precision (mAP) in object detection using\nhalf of the data. Further work includes re-iterating through the creation\nprocess to satisfy custom OMR models. While we do not assume to have solved the\nmain challenges in OMR, this dataset opens a new course of discussions that\nwould ultimately aid that goal.",
          "link": "http://arxiv.org/abs/2107.07786",
          "publishedOn": "2021-07-19T00:49:05.131Z",
          "wordCount": 651,
          "title": "DoReMi: First glance at a universal OMR dataset. (arXiv:2107.07786v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Q/0/1/0/all/0/1\">Qin Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>",
          "description": "Leveraging unlabelled data through weak or distant supervision is a\ncompelling approach to developing more effective text classification models.\nThis paper proposes a simple but effective data augmentation method, which\nleverages the idea of pseudo-labelling to select samples from noisy distant\nsupervision annotation datasets. The result shows that the proposed method\nimproves the accuracy of biased news detection models.",
          "link": "http://arxiv.org/abs/2107.07705",
          "publishedOn": "2021-07-19T00:49:04.905Z",
          "wordCount": 491,
          "title": "Pseudo-labelling Enhanced Media Bias Detection. (arXiv:2107.07705v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yizhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>",
          "description": "Dense retrieval conducts text retrieval in the embedding space and has shown\nmany advantages compared to sparse retrieval. Existing dense retrievers\noptimize representations of queries and documents with contrastive training and\nmap them to the embedding space. The embedding space is optimized by aligning\nthe matched query-document pairs and pushing the negative documents away from\nthe query. However, in such training paradigm, the queries are only optimized\nto align to the documents and are coarsely positioned, leading to an\nanisotropic query embedding space. In this paper, we analyze the embedding\nspace distributions and propose an effective training paradigm, Contrastive\nDual Learning for Approximate Nearest Neighbor (DANCE) to learn fine-grained\nquery representations for dense retrieval. DANCE incorporates an additional\ndual training object of query retrieval, inspired by the classic information\nretrieval training axiom, query likelihood. With contrastive learning, the dual\ntraining object of DANCE learns more tailored representations for queries and\ndocuments to keep the embedding space smooth and uniform, thriving on the\nranking performance of DANCE on the MS MARCO document retrieval task. Different\nfrom ANCE that only optimized with the document retrieval task, DANCE\nconcentrates the query embeddings closer to document representations while\nmaking the document distribution more discriminative. Such concentrated query\nembedding distribution assigns more uniform negative sampling probabilities to\nqueries and helps to sufficiently optimize query representations in the query\nretrieval task. Our codes are released at https://github.com/thunlp/DANCE.",
          "link": "http://arxiv.org/abs/2107.07773",
          "publishedOn": "2021-07-19T00:49:04.885Z",
          "wordCount": 669,
          "title": "More Robust Dense Retrieval with Contrastive Dual Learning. (arXiv:2107.07773v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jing Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaochen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiayi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>",
          "description": "In this paper, we propose a cross-modal variational auto-encoder (CMVAE) for\ncontent-based micro-video background music recommendation. CMVAE is a\nhierarchical Bayesian generative model that matches relevant background music\nto a micro-video by projecting these two multimodal inputs into a shared\nlow-dimensional latent space, where the alignment of two corresponding\nembeddings of a matched video-music pair is achieved by cross-generation.\nMoreover, the multimodal information is fused by the product-of-experts (PoE)\nprinciple, where the semantic information in visual and textual modalities of\nthe micro-video are weighted according to their variance estimations such that\nthe modality with a lower noise level is given more weights. Therefore, the\nmicro-video latent variables contain less irrelevant information that results\nin a more robust model generalization. Furthermore, we establish a large-scale\ncontent-based micro-video background music recommendation dataset, TT-150k,\ncomposed of approximately 3,000 different background music clips associated to\n150,000 micro-videos from different users. Extensive experiments on the\nestablished TT-150k dataset demonstrate the effectiveness of the proposed\nmethod. A qualitative assessment of CMVAE by visualizing some recommendation\nresults is also included.",
          "link": "http://arxiv.org/abs/2107.07268",
          "publishedOn": "2021-07-16T00:48:22.212Z",
          "wordCount": 606,
          "title": "Cross-modal Variational Auto-encoder for Content-based Micro-video Background Music Recommendation. (arXiv:2107.07268v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sudhanshu/0/1/0/all/0/1\">Sudhanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "With the advancement in the technology sector spanning over every field, a\nhuge influx of information is inevitable. Among all the opportunities that the\nadvancements in the technology have brought, one of them is to propose\nefficient solutions for data retrieval. This means that from an enormous pile\nof data, the retrieval methods should allow the users to fetch the relevant and\nrecent data over time. In the field of entertainment and e-commerce,\nrecommender systems have been functioning to provide the aforementioned.\nEmploying the same systems in the medical domain could definitely prove to be\nuseful in variety of ways. Following this context, the goal of this paper is to\npropose collaborative filtering based recommender system in the healthcare\nsector to recommend remedies based on the symptoms experienced by the patients.\nFurthermore, a new dataset is developed consisting of remedies concerning\nvarious diseases to address the limited availability of the data. The proposed\nrecommender system accepts the prognostic markers of a patient as the input and\ngenerates the best remedy course. With several experimental trials, the\nproposed model achieved promising results in recommending the possible remedy\nfor given prognostic markers.",
          "link": "http://arxiv.org/abs/2107.07500",
          "publishedOn": "2021-07-16T00:48:22.196Z",
          "wordCount": 640,
          "title": "Recommending best course of treatment based on similarities of prognostic markers\\thanks{All authors contributed equally. (arXiv:2107.07500v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_C/0/1/0/all/0/1\">Chintoo Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdary_C/0/1/0/all/0/1\">C. Ravindranath Chowdary</a>",
          "description": "In general, recommender systems are designed to provide personalized items to\na user. But in few cases, items are recommended for a group, and the challenge\nis to aggregate the individual user preferences to infer the recommendation to\na group. It is also important to consider the similarity of characteristics\namong the members of a group to generate a better recommendation. Members of an\nautomatically identified group will have similar characteristics, and reaching\na consensus with a decision-making process is preferable in this case. It\nrequires users-items and their rating interactions over a utility matrix to\nauto-detect the groups in group recommendations. We may not overlook other\nintrinsic information to form a group. The textual information also plays a\npivotal role in user clustering. In this paper, we auto-detect the groups based\non the textual similarity of the metadata (review texts). We consider the order\nin user preferences in our models. We have conducted extensive experiments over\ntwo real-world datasets to check the efficacy of the proposed models. We have\nalso conducted a competitive comparison with a baseline model to show the\nimprovements in the quality of recommendations.",
          "link": "http://arxiv.org/abs/2107.07284",
          "publishedOn": "2021-07-16T00:48:22.160Z",
          "wordCount": 618,
          "title": "Auto-detecting groups based on textual similarity for group recommendations. (arXiv:2107.07284v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wenzhuo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shoujin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengsheng Wang</a>",
          "description": "The changing preferences of users towards items trigger the emergence of\nsession-based recommender systems (SBRSs), which aim to model the dynamic\npreferences of users for next-item recommendations. However, most of the\nexisting studies on SBRSs are based on long sessions only for recommendations,\nignoring short sessions, though short sessions, in fact, account for a large\nproportion in most of the real-world datasets. As a result, the applicability\nof existing SBRSs solutions is greatly reduced. In a short session, quite\nlimited contextual information is available, making the next-item\nrecommendation very challenging. To this end, in this paper, inspired by the\nsuccess of few-shot learning (FSL) in effectively learning a model with limited\ninstances, we formulate the next-item recommendation as an FSL problem.\nAccordingly, following the basic idea of a representative approach for FSL,\ni.e., meta-learning, we devise an effective SBRS called INter-SEssion\ncollaborative Recommender netTwork (INSERT) for next-item recommendations in\nshort sessions. With the carefully devised local module and global module,\nINSERT is able to learn an optimal preference representation of the current\nuser in a given short session. In particular, in the global module, a similar\nsession retrieval network (SSRN) is designed to find out the sessions similar\nto the current short session from the historical sessions of both the current\nuser and other users, respectively. The obtained similar sessions are then\nutilized to complement and optimize the preference representation learned from\nthe current short session by the local module for more accurate next-item\nrecommendations in this short session. Extensive experiments conducted on two\nreal-world datasets demonstrate the superiority of our proposed INSERT over the\nstate-of-the-art SBRSs when making next-item recommendations in short sessions.",
          "link": "http://arxiv.org/abs/2107.07453",
          "publishedOn": "2021-07-16T00:48:22.148Z",
          "wordCount": 707,
          "title": "Next-item Recommendations in Short Sessions. (arXiv:2107.07453v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1\">Fajie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaxi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengming Li</a>",
          "description": "Sequential recommender systems (SRS) have become a research hotspot due to\nits power in modeling user dynamic interests and sequential behavioral\npatterns. To maximize model expressive ability, a default choice is to apply a\nlarger and deeper network architecture, which, however, often brings high\nnetwork latency when generating online recommendations. Naturally, we argue\nthat compressing the heavy recommendation models into middle- or light- weight\nneural networks is of great importance for practical production systems. To\nrealize such a goal, we propose AdaRec, a knowledge distillation (KD) framework\nwhich compresses knowledge of a teacher model into a student model adaptively\naccording to its recommendation scene by using differentiable Neural\nArchitecture Search (NAS). Specifically, we introduce a target-oriented\ndistillation loss to guide the structure search process for finding the student\nnetwork architecture, and a cost-sensitive loss as constraints for model size,\nwhich achieves a superior trade-off between recommendation effectiveness and\nefficiency. In addition, we leverage Earth Mover's Distance (EMD) to realize\nmany-to-many layer mapping during knowledge distillation, which enables each\nintermediate student layer to learn from other intermediate teacher layers\nadaptively. Extensive experiments on real-world recommendation datasets\ndemonstrate that our model achieves competitive or better accuracy with notable\ninference speedup comparing to strong counterparts, while discovering diverse\nneural architectures for sequential recommender models under different\nrecommendation scenes.",
          "link": "http://arxiv.org/abs/2107.07173",
          "publishedOn": "2021-07-16T00:48:22.123Z",
          "wordCount": 654,
          "title": "Scene-adaptive Knowledge Distillation for Sequential Recommendation via Differentiable Architecture Search. (arXiv:2107.07173v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hande Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1\">Xin Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guli Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Keping Yang</a>",
          "description": "Recommender systems rely on user behavior data like ratings and clicks to\nbuild personalization model. However, the collected data is observational\nrather than experimental, causing various biases in the data which\nsignificantly affect the learned model. Most existing work for recommendation\ndebiasing, such as the inverse propensity scoring and imputation approaches,\nfocuses on one or two specific biases, lacking the universal capacity that can\naccount for mixed or even unknown biases in the data. Towards this research\ngap, we first analyze the origin of biases from the perspective of \\textit{risk\ndiscrepancy} that represents the difference between the expectation empirical\nrisk and the true risk. Remarkably, we derive a general learning framework that\nwell summarizes most existing debiasing strategies by specifying some\nparameters of the general framework. This provides a valuable opportunity to\ndevelop a universal solution for debiasing, e.g., by learning the debiasing\nparameters from data. However, the training data lacks important signal of how\nthe data is biased and what the unbiased data looks like. To move this idea\nforward, we propose \\textit{AotoDebias} that leverages another (small) set of\nuniform data to optimize the debiasing parameters by solving the bi-level\noptimization problem with meta-learning. Through theoretical analyses, we\nderive the generalization bound for AutoDebias and prove its ability to acquire\nthe appropriate debiasing strategy. Extensive experiments on two real datasets\nand a simulated dataset demonstrated effectiveness of AutoDebias. The code is\navailable at \\url{https://github.com/DongHande/AutoDebias}.",
          "link": "http://arxiv.org/abs/2105.04170",
          "publishedOn": "2021-07-16T00:48:22.054Z",
          "wordCount": 722,
          "title": "AutoDebias: Learning to Debias for Recommendation. (arXiv:2105.04170v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Egg_A/0/1/0/all/0/1\">Alex Egg</a>",
          "description": "We propose a method to easily modify existing offline Recommender Systems to\nrun online using Transfer Learning. Online Learning for Recommender Systems has\ntwo main advantages: quality and scale. Like many Machine Learning algorithms\nin production if not regularly retrained will suffer from Concept Drift. A\npolicy that is updated frequently online can adapt to drift faster than a batch\nsystem. This is especially true for user-interaction systems like recommenders\nwhere the underlying distribution can shift drastically to follow user\nbehaviour. As a platform grows rapidly like Grubhub, the cost of running batch\ntraining jobs becomes material. A shift from stateless batch learning offline\nto stateful incremental learning online can recover, for example, at Grubhub,\nup to a 45x cost savings and a +20% metrics increase. There are a few\nchallenges to overcome with the transition to online stateful learning, namely\nconvergence, non-stationary embeddings and off-policy evaluation, which we\nexplore from our experiences running this system in production.",
          "link": "http://arxiv.org/abs/2107.07106",
          "publishedOn": "2021-07-16T00:48:21.994Z",
          "wordCount": 583,
          "title": "Online Learning for Recommendations at Grubhub. (arXiv:2107.07106v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luptak_D/0/1/0/all/0/1\">D&#xe1;vid Lupt&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1\">V&#xed;t Novotn&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>",
          "description": "Math informational retrieval (MIR) search engines are absent in the\nwide-spread production use, even though documents in the STEM fields contain\nmany mathematical formulae, which are sometimes more important than text for\nunderstanding. We have developed and open-sourced the WebMIaS MIR search engine\nthat has been successfully deployed in the European Digital Mathematics Library\n(EuDML). However, its deployment is difficult to automate due to the complexity\nof this task. Moreover, the solutions developed so far to tackle this challenge\nare imperfect in terms of speed, maintenance, and robustness. In this paper, we\nwill describe the virtualization of WebMIaS using Docker that solves all three\nproblems and allows anyone to deploy containerized WebMIaS in a single line of\ncode. The publicly available Docker image will also help the community push the\ndevelopment of math-aware search engines in the ARQMath workshop series.",
          "link": "http://arxiv.org/abs/2106.00411",
          "publishedOn": "2021-07-15T01:59:02.249Z",
          "wordCount": 655,
          "title": "WebMIaS on Docker: Deploying Math-Aware Search in a Single Line of Code. (arXiv:2106.00411v2 [cs.DL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">Sabuzima Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patgiri_R/0/1/0/all/0/1\">Ripon Patgiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waikhom_L/0/1/0/all/0/1\">Lilapati Waikhom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Arif Ahmed</a>",
          "description": "Edge technology aims to bring Cloud resources (specifically, the compute,\nstorage, and network) to the closed proximity of the Edge devices, i.e., smart\ndevices where the data are produced and consumed. Embedding computing and\napplication in Edge devices lead to emerging of two new concepts in Edge\ntechnology, namely, Edge computing and Edge analytics. Edge analytics uses some\ntechniques or algorithms to analyze the data generated by the Edge devices.\nWith the emerging of Edge analytics, the Edge devices have become a complete\nset. Currently, Edge analytics is unable to provide full support for the\nexecution of the analytic techniques. The Edge devices cannot execute advanced\nand sophisticated analytic algorithms following various constraints such as\nlimited power supply, small memory size, limited resources, etc. This article\naims to provide a detailed discussion on Edge analytics. A clear explanation to\ndistinguish between the three concepts of Edge technology, namely, Edge\ndevices, Edge computing, and Edge analytics, along with their issues.\nFurthermore, the article discusses the implementation of Edge analytics to\nsolve many problems in various areas such as retail, agriculture, industry, and\nhealthcare. In addition, the research papers of the state-of-the-art edge\nanalytics are rigorously reviewed in this article to explore the existing\nissues, emerging challenges, research opportunities and their directions, and\napplications.",
          "link": "http://arxiv.org/abs/2107.06835",
          "publishedOn": "2021-07-15T01:59:02.242Z",
          "wordCount": 685,
          "title": "A Review on Edge Analytics: Issues, Challenges, Opportunities, Promises, Future Directions, and Applications. (arXiv:2107.06835v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petersen_F/0/1/0/all/0/1\">Felix Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgelt_C/0/1/0/all/0/1\">Christian Borgelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deussen_O/0/1/0/all/0/1\">Oliver Deussen</a>",
          "description": "Sorting and ranking supervision is a method for training neural networks\nend-to-end based on ordering constraints. That is, the ground truth order of\nsets of samples is known, while their absolute values remain unsupervised. For\nthat, we propose differentiable sorting networks by relaxing their pairwise\nconditional swap operations. To address the problems of vanishing gradients and\nextensive blurring that arise with larger numbers of layers, we propose mapping\nactivations to regions with moderate gradients. We consider odd-even as well as\nbitonic sorting networks, which outperform existing relaxations of the sorting\noperation. We show that bitonic sorting networks can achieve stable training on\nlarge input sets of up to 1024 elements.",
          "link": "http://arxiv.org/abs/2105.04019",
          "publishedOn": "2021-07-15T01:59:02.195Z",
          "wordCount": 595,
          "title": "Differentiable Sorting Networks for Scalable Sorting and Ranking Supervision. (arXiv:2105.04019v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bei Yu</a>",
          "description": "Accurately linking news articles to scientific research works is a critical\ncomponent in a number of applications, such as measuring the social impact of a\nresearch work and detecting inaccuracies or distortions in science news.\nAlthough the lack of links between news and literature has been a challenge in\nthese applications, it is a relatively unexplored research problem. In this\npaper we designed and evaluated a new approach that consists of (1) augmenting\nlatest named-entity recognition techniques to extract various metadata, and (2)\ndesigning a new elastic search engine that can facilitate the use of enriched\nmetadata queries. To evaluate our approach, we constructed two datasets of\npaired news articles and research papers: one is used for training models to\nextract metadata, and the other for evaluation. Our experiments showed that the\nnew approach performed significantly better than a baseline approach used by\naltmetric.com (0.89 vs 0.32 in terms of top-1 accuracy). To further demonstrate\nthe effectiveness of the approach, we also conducted a study on 37,600\nhealth-related press releases published on EurekAlert!, which showed that our\napproach was able to identify the corresponding research papers with a top-1\naccuracy of at least 0.97.",
          "link": "http://arxiv.org/abs/2107.06472",
          "publishedOn": "2021-07-15T01:59:02.187Z",
          "wordCount": 632,
          "title": "Linking Health News to Research Literature. (arXiv:2107.06472v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1\">Kaize Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caverlee_J/0/1/0/all/0/1\">James Caverlee</a>",
          "description": "A fundamental challenge for sequential recommenders is to capture the\nsequential patterns of users toward modeling how users transit among items. In\nmany practical scenarios, however, there are a great number of cold-start users\nwith only minimal logged interactions. As a result, existing sequential\nrecommendation models will lose their predictive power due to the difficulties\nin learning sequential patterns over users with only limited interactions. In\nthis work, we aim to improve sequential recommendation for cold-start users\nwith a novel framework named MetaTL, which learns to model the transition\npatterns of users through meta-learning. Specifically, the proposed MetaTL: (i)\nformulates sequential recommendation for cold-start users as a few-shot\nlearning problem; (ii) extracts the dynamic transition patterns among users\nwith a translation-based architecture; and (iii) adopts meta transitional\nlearning to enable fast learning for cold-start users with only limited\ninteractions, leading to accurate inference of sequential interactions.",
          "link": "http://arxiv.org/abs/2107.06427",
          "publishedOn": "2021-07-15T01:59:02.178Z",
          "wordCount": 579,
          "title": "Sequential Recommendation for Cold-start Users with Meta Transitional Learning. (arXiv:2107.06427v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Galeano_S/0/1/0/all/0/1\">Sergio Rojas-Galeano</a>",
          "description": "One of the stratagems used to deceive spam filters is to substitute vocables\nwith synonyms or similar words that turn the message unrecognisable by the\ndetection algorithms. In this paper we investigate whether the recent\ndevelopment of language models sensitive to the semantics and context of words,\nsuch as Google's BERT, may be useful to overcome this adversarial attack\n(called \"Mad-lib\" as per the word substitution game). Using a dataset of 5572\nSMS spam messages, we first established a baseline of detection performance\nusing widely known document representation models (BoW and TFIDF) and the novel\nBERT model, coupled with a variety of classification algorithms (Decision Tree,\nkNN, SVM, Logistic Regression, Naive Bayes, Multilayer Perceptron). Then, we\nbuilt a thesaurus of the vocabulary contained in these messages, and set up a\nMad-lib attack experiment in which we modified each message of a held out\nsubset of data (not used in the baseline experiment) with different rates of\nsubstitution of original words with synonyms from the thesaurus. Lastly, we\nevaluated the detection performance of the three representation models (BoW,\nTFIDF and BERT) coupled with the best classifier from the baseline experiment\n(SVM). We found that the classic models achieved a 94% Balanced Accuracy (BA)\nin the original dataset, whereas the BERT model obtained 96%. On the other\nhand, the Mad-lib attack experiment showed that BERT encodings manage to\nmaintain a similar BA performance of 96% with an average substitution rate of\n1.82 words per message, and 95% with 3.34 words substituted per message. In\ncontrast, the BA performance of the BoW and TFIDF encoders dropped to chance.\nThese results hint at the potential advantage of BERT models to combat these\ntype of ingenious attacks, offsetting to some extent for the inappropriate use\nof semantic relationships in language.",
          "link": "http://arxiv.org/abs/2107.06400",
          "publishedOn": "2021-07-15T01:59:02.170Z",
          "wordCount": 732,
          "title": "Using BERT Encoding to Tackle the Mad-lib Attack in SMS Spam Detection. (arXiv:2107.06400v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Ashudeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kempe_D/0/1/0/all/0/1\">David Kempe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joachims_T/0/1/0/all/0/1\">Thorsten Joachims</a>",
          "description": "Fairness has emerged as an important consideration in algorithmic\ndecision-making. Unfairness occurs when an agent with higher merit obtains a\nworse outcome than an agent with lower merit. Our central point is that a\nprimary cause of unfairness is uncertainty. A principal or algorithm making\ndecisions never has access to the agents' true merit, and instead uses proxy\nfeatures that only imperfectly predict merit (e.g., GPA, star ratings,\nrecommendation letters). None of these ever fully capture an agent's merit; yet\nexisting approaches have mostly been defining fairness notions directly based\non observed features and outcomes.\n\nOur primary point is that it is more principled to acknowledge and model the\nuncertainty explicitly. The role of observed features is to give rise to a\nposterior distribution of the agents' merits. We use this viewpoint to define a\nnotion of approximate fairness in ranking. We call an algorithm $\\phi$-fair\n(for $\\phi \\in [0,1]$) if it has the following property for all agents $x$ and\nall $k$: if agent $x$ is among the top $k$ agents with respect to merit with\nprobability at least $\\rho$ (according to the posterior merit distribution),\nthen the algorithm places the agent among the top $k$ agents in its ranking\nwith probability at least $\\phi \\rho$.\n\nWe show how to compute rankings that optimally trade off approximate fairness\nagainst utility to the principal. In addition to the theoretical\ncharacterization, we present an empirical analysis of the potential impact of\nthe approach in simulation studies. For real-world validation, we applied the\napproach in the context of a paper recommendation system that we built and\nfielded at a large conference.",
          "link": "http://arxiv.org/abs/2107.06720",
          "publishedOn": "2021-07-15T01:59:02.151Z",
          "wordCount": 708,
          "title": "Fairness in Ranking under Uncertainty. (arXiv:2107.06720v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06416",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petrescu_D/0/1/0/all/0/1\">Diana Petrescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>",
          "description": "Recommendations with personalized explanations have been shown to increase\nuser trust and perceived quality and help users make better decisions.\nMoreover, such explanations allow users to provide feedback by critiquing them.\nSeveral algorithms for recommendation systems with multi-step critiquing have\ntherefore been developed. However, providing a user-friendly interface based on\npersonalized explanations and critiquing has not been addressed in the last\ndecade. In this paper, we introduce four different web interfaces (available\nunder https://lia.epfl.ch/critiquing/) helping users making decisions and\nfinding their ideal item. We have chosen the hotel recommendation domain as a\nuse case even though our approach is trivially adaptable for other domains.\nMoreover, our system is model-agnostic (for both recommender systems and\ncritiquing models) allowing a great flexibility and further extensions. Our\ninterfaces are above all a useful tool to help research in recommendation with\ncritiquing. They allow to test such systems on a real use case and also to\nhighlight some limitations of these approaches to find solutions to overcome\nthem.",
          "link": "http://arxiv.org/abs/2107.06416",
          "publishedOn": "2021-07-15T01:59:02.063Z",
          "wordCount": 592,
          "title": "Multi-Step Critiquing User Interface for Recommender Systems. (arXiv:2107.06416v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+AlGhamdi_K/0/1/0/all/0/1\">Kholoud AlGhamdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Miaojing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simperl_E/0/1/0/all/0/1\">Elena Simperl</a>",
          "description": "Wikidata is an open knowledge graph built by a global community of\nvolunteers. As it advances in scale, it faces substantial challenges around\neditor engagement. These challenges are in terms of both attracting new editors\nto keep up with the sheer amount of work and retaining existing editors.\nExperience from other online communities and peer-production systems, including\nWikipedia, suggests that personalised recommendations could help, especially\nnewcomers, who are sometimes unsure about how to contribute best to an ongoing\neffort. For this reason, we propose a recommender system WikidataRec for\nWikidata items. The system uses a hybrid of content-based and collaborative\nfiltering techniques to rank items for editors relying on both item features\nand item-editor previous interaction. A neural network, named a neural mixture\nof representations, is designed to learn fine weights for the combination of\nitem-based representations and optimize them with editor-based representation\nby item-editor interaction. To facilitate further research in this space, we\nalso create two benchmark datasets, a general-purpose one with 220,000 editors\nresponsible for 14 million interactions with 4 million items and a second one\nfocusing on the contributions of more than 8,000 more active editors. We\nperform an offline evaluation of the system on both datasets with promising\nresults. Our code and datasets are available at\nhttps://github.com/WikidataRec-developer/Wikidata_Recommender.",
          "link": "http://arxiv.org/abs/2107.06423",
          "publishedOn": "2021-07-15T01:59:02.033Z",
          "wordCount": 639,
          "title": "Learning to Recommend Items to Wikidata Editors. (arXiv:2107.06423v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cabanac_G/0/1/0/all/0/1\">Guillaume Cabanac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labbe_C/0/1/0/all/0/1\">Cyril Labb&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magazinov_A/0/1/0/all/0/1\">Alexander Magazinov</a>",
          "description": "Probabilistic text generators have been used to produce fake scientific\npapers for more than a decade. Such nonsensical papers are easily detected by\nboth human and machine. Now more complex AI-powered generation techniques\nproduce texts indistinguishable from that of humans and the generation of\nscientific texts from a few keywords has been documented. Our study introduces\nthe concept of tortured phrases: unexpected weird phrases in lieu of\nestablished ones, such as 'counterfeit consciousness' instead of 'artificial\nintelligence.' We combed the literature for tortured phrases and study one\nreputable journal where these concentrated en masse. Hypothesising the use of\nadvanced language models we ran a detector on the abstracts of recent articles\nof this journal and on several control sets. The pairwise comparisons reveal a\nconcentration of abstracts flagged as 'synthetic' in the journal. We also\nhighlight irregularities in its operation, such as abrupt changes in editorial\ntimelines. We substantiate our call for investigation by analysing several\nindividual dubious articles, stressing questionable features: tortured writing\nstyle, citation of non-existent literature, and unacknowledged image reuse.\nSurprisingly, some websites offer to rewrite texts for free, generating\ngobbledegook full of tortured phrases. We believe some authors used rewritten\ntexts to pad their manuscripts. We wish to raise the awareness on publications\ncontaining such questionable AI-generated or rewritten texts that passed (poor)\npeer review. Deception with synthetic texts threatens the integrity of the\nscientific literature.",
          "link": "http://arxiv.org/abs/2107.06751",
          "publishedOn": "2021-07-15T01:59:02.007Z",
          "wordCount": 688,
          "title": "Tortured phrases: A dubious writing style emerging in science. Evidence of critical issues affecting established journals. (arXiv:2107.06751v1 [cs.DL])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.08688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yinzhe Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanzhou Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>",
          "description": "In order to protect the intellectual property (IP) of deep neural networks\n(DNNs), many existing DNN watermarking techniques either embed watermarks\ndirectly into the DNN parameters or insert backdoor watermarks by fine-tuning\nthe DNN parameters, which, however, cannot resist against various attack\nmethods that remove watermarks by altering DNN parameters. In this paper, we\nbypass such attacks by introducing a structural watermarking scheme that\nutilizes channel pruning to embed the watermark into the host DNN architecture\ninstead of crafting the DNN parameters. To be specific, during watermark\nembedding, we prune the internal channels of the host DNN with the channel\npruning rates controlled by the watermark. During watermark extraction, the\nwatermark is retrieved by identifying the channel pruning rates from the\narchitecture of the target DNN model. Due to the superiority of pruning\nmechanism, the performance of the DNN model on its original task is reserved\nduring watermark embedding. Experimental results have shown that, the proposed\nwork enables the embedded watermark to be reliably recovered and provides a\nhigh watermark capacity, without sacrificing the usability of the DNN model. It\nis also demonstrated that the work is robust against common transforms and\nattacks designed for conventional watermarking approaches.",
          "link": "http://arxiv.org/abs/2107.08688",
          "publishedOn": "2021-07-20T02:04:38.180Z",
          "wordCount": 640,
          "title": "Structural Watermarking to Deep Neural Networks via Network Channel Pruning. (arXiv:2107.08688v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>",
          "description": "In this paper, we develop face.evoLVe -- a comprehensive library that\ncollects and implements a wide range of popular deep learning-based methods for\nface recognition. First of all, face.evoLVe is composed of key components that\ncover the full process of face analytics, including face alignment, data\nprocessing, various backbones, losses, and alternatives with bags of tricks for\nimproving performance. Later, face.evoLVe supports multi-GPU training on top of\ndifferent deep learning platforms, such as PyTorch and PaddlePaddle, which\nfacilitates researchers to work on both large-scale datasets with millions of\nimages and low-shot counterparts with limited well-annotated data. More\nimportantly, along with face.evoLVe, images before & after alignment in the\ncommon benchmark datasets are released with source codes and trained models\nprovided. All these efforts lower the technical burdens in reproducing the\nexisting methods for comparison, while users of our library could focus on\ndeveloping advanced approaches more efficiently. Last but not least,\nface.evoLVe is well designed and vibrantly evolving, so that new face\nrecognition approaches can be easily plugged into our framework. Note that we\nhave used face.evoLVe to participate in a number of face recognition\ncompetitions and secured the first place. The version that supports PyTorch is\npublicly available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch and the\nPaddlePaddle version is available at\nhttps://github.com/ZhaoJ9014/face.evoLVe.PyTorch/tree/master/paddle.\nFace.evoLVe has been widely used for face analytics, receiving 2.4K stars and\n622 forks.",
          "link": "http://arxiv.org/abs/2107.08621",
          "publishedOn": "2021-07-20T02:04:37.964Z",
          "wordCount": 668,
          "title": "Face.evoLVe: A High-Performance Face Recognition Library. (arXiv:2107.08621v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.",
          "link": "http://arxiv.org/abs/2107.08264",
          "publishedOn": "2021-07-20T02:04:37.888Z",
          "wordCount": 685,
          "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yao Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Haipeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Despite being a critical communication skill, grasping humor is challenging\n-- a successful use of humor requires a mixture of both engaging content\nbuild-up and an appropriate vocal delivery (e.g., pause). Prior studies on\ncomputational humor emphasize the textual and audio features immediately next\nto the punchline, yet overlooking longer-term context setup. Moreover, the\ntheories are usually too abstract for understanding each concrete humor\nsnippet. To fill in the gap, we develop DeHumor, a visual analytical system for\nanalyzing humorous behaviors in public speaking. To intuitively reveal the\nbuilding blocks of each concrete example, DeHumor decomposes each humorous\nvideo into multimodal features and provides inline annotations of them on the\nvideo script. In particular, to better capture the build-ups, we introduce\ncontent repetition as a complement to features introduced in theories of\ncomputational humor and visualize them in a context linking graph. To help\nusers locate the punchlines that have the desired features to learn, we\nsummarize the content (with keywords) and humor feature statistics on an\naugmented time matrix. With case studies on stand-up comedy shows and TED\ntalks, we show that DeHumor is able to highlight various building blocks of\nhumor examples. In addition, expert interviews with communication coaches and\nhumor researchers demonstrate the effectiveness of DeHumor for multimodal humor\nanalysis of speech content and vocal delivery.",
          "link": "http://arxiv.org/abs/2107.08356",
          "publishedOn": "2021-07-20T02:04:37.706Z",
          "wordCount": 689,
          "title": "DeHumor: Visual Analytics for Decomposing Humor. (arXiv:2107.08356v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07907",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1\">Kanglin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_G/0/1/0/all/0/1\">Gaofeng Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_J/0/1/0/all/0/1\">Jiang Duan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>",
          "description": "Single-image HDR reconstruction or inverse tone mapping (iTM) is a\nchallenging task. In particular, recovering information in over-exposed regions\nis extremely difficult because details in such regions are almost completely\nlost. In this paper, we present a deep learning based iTM method that takes\nadvantage of the feature extraction and mapping power of deep convolutional\nneural networks (CNNs) and uses a lightness prior to modulate the CNN to better\nexploit observations in the surrounding areas of the over-exposed regions to\nenhance the quality of HDR image reconstruction. Specifically, we introduce a\nHierarchical Synthesis Network (HiSN) for inferring a HDR image from a LDR\ninput and a Lightness Adpative Modulation Network (LAMN) to incorporate the the\nlightness prior knowledge in the inferring process. The HiSN hierarchically\nsynthesizes the high-brightness component and the low-brightness component of\nthe HDR image whilst the LAMN uses a lightness adaptive mask that separates\ndetail-less saturated bright pixels from well-exposed lower light pixels to\nenable HiSN to better infer the missing information, particularly in the\ndifficult over-exposed detail-less areas. We present experimental results to\ndemonstrate the effectiveness of the new technique based on quantitative\nmeasures and visual comparisons. In addition, we present ablation studies of\nHiSN and visualization of the activation maps inside LAMN to help gain a deeper\nunderstanding of the internal working of the new iTM algorithm and explain why\nit can achieve much improved performance over state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2107.07907",
          "publishedOn": "2021-07-19T00:49:04.940Z",
          "wordCount": 683,
          "title": "Lightness Modulated Deep Inverse Tone Mapping. (arXiv:2107.07907v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shatri_E/0/1/0/all/0/1\">Elona Shatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>",
          "description": "The main challenges of Optical Music Recognition (OMR) come from the nature\nof written music, its complexity and the difficulty of finding an appropriate\ndata representation. This paper provides a first look at DoReMi, an OMR dataset\nthat addresses these challenges, and a baseline object detection model to\nassess its utility. Researchers often approach OMR following a set of small\nstages, given that existing data often do not satisfy broader research. We\nexamine the possibility of changing this tendency by presenting more metadata.\nOur approach complements existing research; hence DoReMi allows harmonisation\nwith two existing datasets, DeepScores and MUSCIMA++. DoReMi was generated\nusing a music notation software and includes over 6400 printed sheet music\nimages with accompanying metadata useful in OMR research. Our dataset provides\nOMR metadata, MIDI, MEI, MusicXML and PNG files, each aiding a different stage\nof OMR. We obtain 64% mean average precision (mAP) in object detection using\nhalf of the data. Further work includes re-iterating through the creation\nprocess to satisfy custom OMR models. While we do not assume to have solved the\nmain challenges in OMR, this dataset opens a new course of discussions that\nwould ultimately aid that goal.",
          "link": "http://arxiv.org/abs/2107.07786",
          "publishedOn": "2021-07-19T00:49:04.774Z",
          "wordCount": 651,
          "title": "DoReMi: First glance at a universal OMR dataset. (arXiv:2107.07786v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zetian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jason Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Leslie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peter Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Michelle A. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>",
          "description": "Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. It is a challenging yet crucial area\nwith numerous real-world applications in multimedia, affective computing,\nrobotics, finance, human-computer interaction, and healthcare. Unfortunately,\nmultimodal research has seen limited resources to study (1) generalization\nacross domains and modalities, (2) complexity during training and inference,\nand (3) robustness to noisy and missing modalities. In order to accelerate\nprogress towards understudied modalities and tasks while ensuring real-world\nrobustness, we release MultiBench, a systematic and unified large-scale\nbenchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6\nresearch areas. MultiBench provides an automated end-to-end machine learning\npipeline that simplifies and standardizes data loading, experimental setup, and\nmodel evaluation. To enable holistic evaluation, MultiBench offers a\ncomprehensive methodology to assess (1) generalization, (2) time and space\ncomplexity, and (3) modality robustness. MultiBench introduces impactful\nchallenges for future research, including scalability to large-scale multimodal\ndatasets and robustness to realistic imperfections. To accompany this\nbenchmark, we also provide a standardized implementation of 20 core approaches\nin multimodal learning. Simply applying methods proposed in different research\nareas can improve the state-of-the-art performance on 9/15 datasets. Therefore,\nMultiBench presents a milestone in unifying disjoint efforts in multimodal\nresearch and paves the way towards a better understanding of the capabilities\nand limitations of multimodal models, all the while ensuring ease of use,\naccessibility, and reproducibility. MultiBench, our standardized code, and\nleaderboards are publicly available, will be regularly updated, and welcomes\ninputs from the community.",
          "link": "http://arxiv.org/abs/2107.07502",
          "publishedOn": "2021-07-16T00:48:22.536Z",
          "wordCount": 722,
          "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+HajiAkhondi_Meybodi_Z/0/1/0/all/0/1\">Zohreh HajiAkhondi-Meybodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abouei_J/0/1/0/all/0/1\">Jamshid Abouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_M/0/1/0/all/0/1\">Ming Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>",
          "description": "Recently, as a consequence of the COVID-19 pandemic, dependence on\ntelecommunication for remote working and telemedicine has significantly\nincreased. In cellular networks, incorporation of Unmanned Aerial Vehicles\n(UAVs) can result in enhanced connectivity for outdoor users due to the high\nprobability of establishing Line of Sight (LoS) links. The UAV's limited\nbattery life and its signal attenuation in indoor areas, however, make it\ninefficient to manage users' requests in indoor environments. Referred to as\nthe Cluster centric and Coded UAV-aided Femtocaching (CCUF) framework, the\nnetwork's coverage in both indoor and outdoor environments increases via a\ntwo-phase clustering for FAPs' formation and UAVs' deployment. First objective\nis to increase the content diversity. In this context, we propose a coded\ncontent placement in a cluster-centric cellular network, which is integrated\nwith the Coordinated Multi-Point (CoMP) to mitigate the inter-cell interference\nin edge areas. Then, we compute, experimentally, the number of coded contents\nto be stored in each caching node to increase the cache-hit ratio,\nSignal-to-Interference-plus-Noise Ratio (SINR), and cache diversity and\ndecrease the users' access delay and cache redundancy for different content\npopularity profiles. Capitalizing on clustering, our second objective is to\nassign the best caching node to indoor/outdoor users for managing their\nrequests. In this regard, we define the movement speed of ground users as the\ndecision metric of the transmission scheme for serving outdoor users' requests\nto avoid frequent handovers between FAPs and increase the battery life of UAVs.\nSimulation results illustrate that the proposed CCUF implementation increases\nthe cache hit-ratio, SINR, and cache diversity and decrease the users' access\ndelay, cache redundancy and UAVs' energy consumption.",
          "link": "http://arxiv.org/abs/2101.11787",
          "publishedOn": "2021-07-16T00:48:22.525Z",
          "wordCount": 790,
          "title": "Joint Transmission Scheme and Coded Content Placement in Cluster-centric UAV-aided Cellular Networks. (arXiv:2101.11787v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jing Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaochen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiayi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>",
          "description": "In this paper, we propose a cross-modal variational auto-encoder (CMVAE) for\ncontent-based micro-video background music recommendation. CMVAE is a\nhierarchical Bayesian generative model that matches relevant background music\nto a micro-video by projecting these two multimodal inputs into a shared\nlow-dimensional latent space, where the alignment of two corresponding\nembeddings of a matched video-music pair is achieved by cross-generation.\nMoreover, the multimodal information is fused by the product-of-experts (PoE)\nprinciple, where the semantic information in visual and textual modalities of\nthe micro-video are weighted according to their variance estimations such that\nthe modality with a lower noise level is given more weights. Therefore, the\nmicro-video latent variables contain less irrelevant information that results\nin a more robust model generalization. Furthermore, we establish a large-scale\ncontent-based micro-video background music recommendation dataset, TT-150k,\ncomposed of approximately 3,000 different background music clips associated to\n150,000 micro-videos from different users. Extensive experiments on the\nestablished TT-150k dataset demonstrate the effectiveness of the proposed\nmethod. A qualitative assessment of CMVAE by visualizing some recommendation\nresults is also included.",
          "link": "http://arxiv.org/abs/2107.07268",
          "publishedOn": "2021-07-16T00:48:22.242Z",
          "wordCount": 606,
          "title": "Cross-modal Variational Auto-encoder for Content-based Micro-video Background Music Recommendation. (arXiv:2107.07268v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyoungjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Myungchul Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_L/0/1/0/all/0/1\">Laihyuk Park</a>",
          "description": "Video streaming services strive to support high-quality videos at higher\nresolutions and frame rates to improve the quality of experience (QoE).\nHowever, high-quality videos consume considerable amounts of energy on mobile\ndevices. This paper proposes NeuSaver, which reduces the power consumption of\nmobile devices when streaming videos by applying an adaptive frame rate to each\nvideo chunk without compromising user experience. NeuSaver generates an optimal\npolicy that determines the appropriate frame rate for each video chunk using\nreinforcement learning (RL). The RL model automatically learns the policy that\nmaximizes the QoE goals based on previous observations. NeuSaver also uses an\nasynchronous advantage actor-critic algorithm to reinforce the RL model quickly\nand robustly. Streaming servers that support NeuSaver preprocesses videos into\nsegments with various frame rates, which is similar to the process of creating\nvideos with multiple bit rates in dynamic adaptive streaming over HTTP.\nNeuSaver utilizes the commonly used H.264 video codec. We evaluated NeuSaver in\nvarious experiments and a user study through four video categories along with\nthe state-of-the-art model. Our experiments showed that NeuSaver effectively\nreduces the power consumption of mobile devices when streaming video by an\naverage of 16.14% and up to 23.12% while achieving high QoE.",
          "link": "http://arxiv.org/abs/2107.07127",
          "publishedOn": "2021-07-16T00:48:22.137Z",
          "wordCount": 672,
          "title": "NeuSaver: Neural Adaptive Power Consumption Optimization for Mobile Video Streaming. (arXiv:2107.07127v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lobbers_S/0/1/0/all/0/1\">Sebastian L&#xf6;bbers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barthet_M/0/1/0/all/0/1\">Mathieu Barthet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>",
          "description": "Sound synthesiser controls typically correspond to technical parameters of\nsignal processing algorithms rather than intuitive sound descriptors that\nrelate to human perception of sound. This makes it difficult to realise sound\nideas in a straightforward way. Cross-modal mappings, for example between\ngestures and sound, have been suggested as a more intuitive control mechanism.\nA large body of research shows consistency in human associations between sounds\nand shapes. However, the use of drawings to drive sound synthesis has not been\nexplored to its full extent. This paper presents an exploratory study that\nasked participants to sketch visual imagery of sounds with a monochromatic\ndigital drawing interface, with the aim to identify different representational\napproaches and determine whether timbral sound characteristics can be\ncommunicated reliably through visual sketches. Results imply that the\ndevelopment of a synthesiser exploiting sound-shape associations is feasible,\nbut a larger and more focused dataset is needed in followup studies.",
          "link": "http://arxiv.org/abs/2107.07360",
          "publishedOn": "2021-07-16T00:48:22.090Z",
          "wordCount": 599,
          "title": "Sketching sounds: an exploratory study on sound-shape associations. (arXiv:2107.07360v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhiying Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>",
          "description": "As a key component of talking face generation, lip movements generation\ndetermines the naturalness and coherence of the generated talking face video.\nPrior literature mainly focuses on speech-to-lip generation while there is a\npaucity in text-to-lip (T2L) generation. T2L is a challenging task and existing\nend-to-end works depend on the attention mechanism and autoregressive (AR)\ndecoding manner. However, the AR decoding manner generates current lip frame\nconditioned on frames generated previously, which inherently hinders the\ninference speed, and also has a detrimental effect on the quality of generated\nlip frames due to error propagation. This encourages the research of parallel\nT2L generation. In this work, we propose a novel parallel decoding model for\nhigh-speed and high-quality text-to-lip generation (HH-T2L). Specifically, we\npredict the duration of the encoded linguistic features and model the target\nlip frames conditioned on the encoded linguistic features with their duration\nin a non-autoregressive manner. Furthermore, we incorporate the structural\nsimilarity index loss and adversarial learning to improve perceptual quality of\ngenerated lip frames and alleviate the blurry prediction problem. Extensive\nexperiments conducted on GRID and TCD-TIMIT datasets show that 1) HH-T2L\ngenerates lip movements with competitive quality compared with the\nstate-of-the-art AR T2L model DualLip and exceeds the baseline AR model\nTransformerT2L by a notable margin benefiting from the mitigation of the error\npropagation problem; and 2) exhibits distinct superiority in inference speed\n(an average speedup of 19$\\times$ than DualLip on TCD-TIMIT).",
          "link": "http://arxiv.org/abs/2107.06831",
          "publishedOn": "2021-07-15T01:59:02.205Z",
          "wordCount": 668,
          "title": "High-Speed and High-Quality Text-to-Lip Generation. (arXiv:2107.06831v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Trinh Man Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinjia Zhou</a>",
          "description": "COVID-19 leads to the high demand for remote interactive systems ever seen.\nOne of the key elements of these systems is video streaming, which requires a\nvery high network bandwidth due to its specific real-time demand, especially\nwith high-resolution video. Existing video compression methods are struggling\nin the trade-off between video quality and the speed requirement. Addressed\nthat the background information rarely changes in most remote meeting cases, we\nintroduce a Region-Of-Interests (ROI) based video compression framework (named\nRCLC) that leverages the cutting-edge learning-based and conventional\ntechnologies. In RCLC, each coming frame is marked as a background-updating\n(BU) or ROI-updating (RU) frame. By applying the conventional video codec, the\nBU frame is compressed with low-quality and high-compression, while the ROI\nfrom RU-frame is compressed with high-quality and low-compression. The\nlearning-based methods are applied to detect the ROI, blend background-ROI, and\nenhance video quality. The experimental results show that our RCLC can reduce\nup to 32.55\\% BD-rate for the ROI region compared to H.265 video codec under a\nsimilar compression time with 1080p resolution.",
          "link": "http://arxiv.org/abs/2107.06492",
          "publishedOn": "2021-07-15T01:59:02.136Z",
          "wordCount": 663,
          "title": "RCLC: ROI-based joint conventional and learning video compression. (arXiv:2107.06492v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06538",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinda Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>",
          "description": "Fine-grained image recognition is challenging because discriminative clues\nare usually fragmented, whether from a single image or multiple images. Despite\ntheir significant improvements, most existing methods still focus on the most\ndiscriminative parts from a single image, ignoring informative details in other\nregions and lacking consideration of clues from other associated images. In\nthis paper, we analyze the difficulties of fine-grained image recognition from\na new perspective and propose a transformer architecture with the peak\nsuppression module and knowledge guidance module, which respects the\ndiversification of discriminative features in a single image and the\naggregation of discriminative clues among multiple images. Specifically, the\npeak suppression module first utilizes a linear projection to convert the input\nimage into sequential tokens. It then blocks the token based on the attention\nresponse generated by the transformer encoder. This module penalizes the\nattention to the most discriminative parts in the feature learning process,\ntherefore, enhancing the information exploitation of the neglected regions. The\nknowledge guidance module compares the image-based representation generated\nfrom the peak suppression module with the learnable knowledge embedding set to\nobtain the knowledge response coefficients. Afterwards, it formalizes the\nknowledge learning as a classification problem using response coefficients as\nthe classification scores. Knowledge embeddings and image-based representations\nare updated during training so that the knowledge embedding includes\ndiscriminative clues for different images. Finally, we incorporate the acquired\nknowledge embeddings into the image-based representations as comprehensive\nrepresentations, leading to significantly higher performance. Extensive\nevaluations on the six popular datasets demonstrate the advantage of the\nproposed method.",
          "link": "http://arxiv.org/abs/2107.06538",
          "publishedOn": "2021-07-15T01:59:02.048Z",
          "wordCount": 693,
          "title": "Transformer with Peak Suppression and Knowledge Guidance for Fine-grained Image Recognition. (arXiv:2107.06538v1 [cs.MM])"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.08211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Soumyadeep Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjay Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_J/0/1/0/all/0/1\">Janu Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Awanish Kumar</a>",
          "description": "In order to train robust deep learning models, large amounts of labelled data\nis required. However, in the absence of such large repositories of labelled\ndata, unlabeled data can be exploited for the same. Semi-Supervised learning\naims to utilize such unlabeled data for training classification models. Recent\nprogress of self-training based approaches have shown promise in this area,\nwhich leads to this study where we utilize an ensemble approach for the same. A\nby-product of any semi-supervised approach may be loss of calibration of the\ntrained model especially in scenarios where unlabeled data may contain\nout-of-distribution samples, which leads to this investigation on how to adapt\nto such effects. Our proposed algorithm carefully avoids common pitfalls in\nutilizing unlabeled data and leads to a more accurate and calibrated supervised\nmodel compared to vanilla self-training based student-teacher algorithms. We\nperform several experiments on the popular STL-10 database followed by an\nextensive analysis of our approach and study its effects on model accuracy and\ncalibration.",
          "link": "http://arxiv.org/abs/2107.08211",
          "publishedOn": "2021-07-20T02:04:48.562Z",
          "wordCount": 607,
          "title": "Self Training with Ensemble of Teacher Models. (arXiv:2107.08211v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianshu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaomin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiali Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "In this paper, we propose a novel training strategy for convolutional neural\nnetwork(CNN) named Feature Mining, that aims to strengthen the network's\nlearning of the local feature. Through experiments, we find that semantic\ncontained in different parts of the feature is different, while the network\nwill inevitably lose the local information during feedforward propagation. In\norder to enhance the learning of local feature, Feature Mining divides the\ncomplete feature into two complementary parts and reuse these divided feature\nto make the network learn more local information, we call the two steps as\nfeature segmentation and feature reusing. Feature Mining is a parameter-free\nmethod and has plug-and-play nature, and can be applied to any CNN models.\nExtensive experiments demonstrate the wide applicability, versatility, and\ncompatibility of our method.",
          "link": "http://arxiv.org/abs/2107.08421",
          "publishedOn": "2021-07-20T02:04:48.428Z",
          "wordCount": 571,
          "title": "Feature Mining: A Novel Training Strategy for Convolutional Neural Network. (arXiv:2107.08421v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1\">Xing Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shuowen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Recent advances in deep convolutional neural networks (DCNNs) have shown\nimpressive performance improvements on thermal to visible face synthesis and\nmatching problems. However, current DCNN-based synthesis models do not perform\nwell on thermal faces with large pose variations. In order to deal with this\nproblem, heterogeneous face frontalization methods are needed in which a model\ntakes a thermal profile face image and generates a frontal visible face. This\nis an extremely difficult problem due to the large domain as well as large pose\ndiscrepancies between the two modalities. Despite its applications in\nbiometrics and surveillance, this problem is relatively unexplored in the\nliterature. We propose a domain agnostic learning-based generative adversarial\nnetwork (DAL-GAN) which can synthesize frontal views in the visible domain from\nthermal faces with pose variations. DAL-GAN consists of a generator with an\nauxiliary classifier and two discriminators which capture both local and global\ntexture discriminations for better synthesis. A contrastive constraint is\nenforced in the latent space of the generator with the help of a dual-path\ntraining strategy, which improves the feature vector discrimination. Finally, a\nmulti-purpose loss function is utilized to guide the network in synthesizing\nidentity preserving cross-domain frontalization. Extensive experimental results\ndemonstrate that DAL-GAN can generate better quality frontal views compared to\nthe other baseline methods.",
          "link": "http://arxiv.org/abs/2107.08311",
          "publishedOn": "2021-07-20T02:04:48.375Z",
          "wordCount": 661,
          "title": "Heterogeneous Face Frontalization via Domain Agnostic Learning. (arXiv:2107.08311v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mathew_M/0/1/0/all/0/1\">Minesh Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1\">Ruben Tito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1\">R. Manmatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C.V. Jawahar</a>",
          "description": "This paper presents results of Document Visual Question Answering Challenge\norganized as part of \"Text and Documents in the Deep Learning Era\" workshop, in\nCVPR 2020. The challenge introduces a new problem - Visual Question Answering\non document images. The challenge comprised two tasks. The first task concerns\nwith asking questions on a single document image. On the other hand, the second\ntask is set as a retrieval task where the question is posed over a collection\nof images. For the task 1 a new dataset is introduced comprising 50,000\nquestions-answer(s) pairs defined over 12,767 document images. For task 2\nanother dataset has been created comprising 20 questions over 14,362 document\nimages which share the same document template.",
          "link": "http://arxiv.org/abs/2008.08899",
          "publishedOn": "2021-07-20T02:04:47.750Z",
          "wordCount": 597,
          "title": "Document Visual Question Answering Challenge 2020. (arXiv:2008.08899v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dawei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Longyin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pengfei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Heng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junwen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Ali_A/0/1/0/all/0/1\">Ali Al-Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Amr Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imene_B/0/1/0/all/0/1\">Bakour Imene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nesma_B/0/1/0/all/0/1\">Bouchali Hadia Nesma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenzhen Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castiello_C/0/1/0/all/0/1\">Ciro Castiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mencar_C/0/1/0/all/0/1\">Corrado Mencar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Dingkang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruger_F/0/1/0/all/0/1\">Florian Kr&#xfc;ger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vessio_G/0/1/0/all/0/1\">Gennaro Vessio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castellano_G/0/1/0/all/0/1\">Giovanna Castellano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jieru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abualsaud_K/0/1/0/all/0/1\">Khalid Abualsaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Laihui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cianciotta_M/0/1/0/all/0/1\">Marco Cianciotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saqib_M/0/1/0/all/0/1\">Muhammad Saqib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almaadeed_N/0/1/0/all/0/1\">Noor Almaadeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elharrouss_O/0/1/0/all/0/1\">Omar Elharrouss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_P/0/1/0/all/0/1\">Pei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shidong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shuang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Siyang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Maadeed_S/0/1/0/all/0/1\">Somaya Al-Maadeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Sultan Daud Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattab_T/0/1/0/all/0/1\">Tamer Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golda_T/0/1/0/all/0/1\">Thomas Golda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaoqing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Ye Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingnan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongchao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuehan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhijian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhiwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhiyuan Zhao</a>",
          "description": "Crowd counting on the drone platform is an interesting topic in computer\nvision, which brings new challenges such as small object inference, background\nclutter and wide viewpoint. However, there are few algorithms focusing on crowd\ncounting on the drone-captured data due to the lack of comprehensive datasets.\nTo this end, we collect a large-scale dataset and organize the Vision Meets\nDrone Crowd Counting Challenge (VisDrone-CC2020) in conjunction with the 16th\nEuropean Conference on Computer Vision (ECCV 2020) to promote the developments\nin the related fields. The collected dataset is formed by $3,360$ images,\nincluding $2,460$ images for training, and $900$ images for testing.\nSpecifically, we manually annotate persons with points in each video frame.\nThere are $14$ algorithms from $15$ institutes submitted to the VisDrone-CC2020\nChallenge. We provide a detailed analysis of the evaluation results and\nconclude the challenge. More information can be found at the website:\n\\url{this http URL}.",
          "link": "http://arxiv.org/abs/2107.08766",
          "publishedOn": "2021-07-20T02:04:47.020Z",
          "wordCount": null,
          "title": "VisDrone-CC2020: The Vision Meets Drone Crowd Counting Challenge Results. (arXiv:2107.08766v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gun-Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Han-Bin Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Deep learning has played a major role in the interpretation of dermoscopic\nimages for detecting skin defects and abnormalities. However, current deep\nlearning solutions for dermatological lesion analysis are typically limited in\nproviding probabilistic predictions which highlights the importance of\nconcerning uncertainties. This concept of uncertainty can provide a confidence\nlevel for each feature which prevents overconfident predictions with poor\ngeneralization on unseen data. In this paper, we propose an overall framework\nthat jointly considers dermatological classification and uncertainty estimation\ntogether. The estimated confidence of each feature to avoid uncertain feature\nand undesirable shift, which are caused by environmental difference of input\nimage, in the latent space is pooled from confidence network. Our qualitative\nresults show that modeling uncertainties not only helps to quantify model\nconfidence for each prediction but also helps classification layers to focus on\nconfident features, therefore, improving the accuracy for dermatological lesion\nclassification. We demonstrate the potential of the proposed approach in two\nstate-of-the-art dermoscopic datasets (ISIC 2018 and ISIC 2019).",
          "link": "http://arxiv.org/abs/2107.08770",
          "publishedOn": "2021-07-20T02:04:47.020Z",
          "wordCount": null,
          "title": "Joint Dermatological Lesion Classification and Confidence Modeling with Uncertainty Estimation. (arXiv:2107.08770v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thuy_H/0/1/0/all/0/1\">Hang Duong Thi Thuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minh_T/0/1/0/all/0/1\">Tuan Nguyen Minh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Van_P/0/1/0/all/0/1\">Phi Nguyen Van</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quoc_L/0/1/0/all/0/1\">Long Tran Quoc</a>",
          "description": "Nowadays, cardiac diagnosis largely depends on left ventricular function\nassessment. With the help of the segmentation deep learning model, the\nassessment of the left ventricle becomes more accessible and accurate. However,\ndeep learning technique still faces two main obstacles: the difficulty in\nacquiring sufficient training data and time-consuming in developing quality\nmodels. In the ordinary data acquisition process, the dataset was selected\nrandomly from a large pool of unlabeled images for labeling, leading to massive\nlabor time to annotate those images. Besides that, hand-designed model\ndevelopment is laborious and also costly. This paper introduces a pipeline that\nrelies on Active Learning to ease the labeling work and utilizes Neural\nArchitecture Search's idea to design the adequate deep learning model\nautomatically. We called this Fully automated machine learning pipeline for\nechocardiogram segmentation. The experiment results show that our method\nobtained the same IOU accuracy with only two-fifths of the original training\ndataset, and the searched model got the same accuracy as the hand-designed\nmodel given the same training dataset.",
          "link": "http://arxiv.org/abs/2107.08440",
          "publishedOn": "2021-07-20T02:04:46.794Z",
          "wordCount": null,
          "title": "Fully Automated Machine Learning Pipeline for Echocardiogram Segmentation. (arXiv:2107.08440v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Liangqiong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandar_N/0/1/0/all/0/1\">Niranjan Balachandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel L Rubin</a>",
          "description": "Federated learning enables multiple institutions to collaboratively train\nmachine learning models on their local data in a privacy-preserving way.\nHowever, its distributed nature often leads to significant heterogeneity in\ndata distributions across institutions. In this paper, we investigate the\ndeleterious impact of a taxonomy of data heterogeneity regimes on federated\nlearning methods, including quantity skew, label distribution skew, and imaging\nacquisition skew. We show that the performance degrades with the increasing\ndegrees of data heterogeneity. We present several mitigation strategies to\novercome performance drops from data heterogeneity, including weighted average\nfor data quantity skew, weighted loss and batch normalization averaging for\nlabel distribution skew. The proposed optimizations to federated learning\nmethods improve their capability of handling heterogeneity across institutions,\nwhich provides valuable guidance for the deployment of federated learning in\nreal clinical applications.",
          "link": "http://arxiv.org/abs/2107.08371",
          "publishedOn": "2021-07-20T02:04:46.772Z",
          "wordCount": null,
          "title": "An Experimental Study of Data Heterogeneity in Federated Learning Methods for Medical Imaging. (arXiv:2107.08371v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11237",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jinlong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengkai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yueyang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>",
          "description": "Recently, most siamese network based trackers locate targets via object\nclassification and bounding-box regression. Generally, they select the\nbounding-box with maximum classification confidence as the final prediction.\nThis strategy may miss the right result due to the accuracy misalignment\nbetween classification and regression. In this paper, we propose a novel\nsiamese tracking algorithm called SiamRCR, addressing this problem with a\nsimple, light and effective solution. It builds reciprocal links between\nclassification and regression branches, which can dynamically re-weight their\nlosses for each positive sample. In addition, we add a localization branch to\npredict the localization accuracy, so that it can work as the replacement of\nthe regression assistance link during inference. This branch makes the training\nand inference more consistent. Extensive experimental results demonstrate the\neffectiveness of SiamRCR and its superiority over the state-of-the-art\ncompetitors on GOT-10k, LaSOT, TrackingNet, OTB-2015, VOT-2018 and VOT-2019.\nMoreover, our SiamRCR runs at 65 FPS, far above the real-time requirement.",
          "link": "http://arxiv.org/abs/2105.11237",
          "publishedOn": "2021-07-20T02:04:46.771Z",
          "wordCount": null,
          "title": "SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking. (arXiv:2105.11237v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radoi_E/0/1/0/all/0/1\">Emilian Radoi</a>",
          "description": "The use of gait for person identification has important advantages such as\nbeing non-invasive, unobtrusive, not requiring cooperation and being less\nlikely to be obscured compared to other biometrics. Existing methods for gait\nrecognition require cooperative gait scenarios, in which a single person is\nwalking multiple times in a straight line in front of a camera. We aim to\naddress the hard challenges of real-world scenarios in which camera feeds\ncapture multiple people, who in most cases pass in front of the camera only\nonce. We address privacy concerns by using only the motion information of\nwalking individuals, with no identifiable appearance-based information. As\nsuch, we propose a novel weakly supervised learning framework, WildGait, which\nconsists of training a Spatio-Temporal Graph Convolutional Network on a large\nnumber of automatically annotated skeleton sequences obtained from raw,\nreal-world, surveillance streams to learn useful gait signatures. Our results\nshow that, with fine-tuning, we surpass in terms of recognition accuracy the\ncurrent state-of-the-art pose-based gait recognition solutions. Our proposed\nmethod is reliable in training gait recognition methods in unconstrained\nenvironments, especially in settings with scarce amounts of annotated data.",
          "link": "http://arxiv.org/abs/2105.05528",
          "publishedOn": "2021-07-20T02:04:46.705Z",
          "wordCount": null,
          "title": "WildGait: Learning Gait Representations from Raw Surveillance Streams. (arXiv:2105.05528v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1809.00970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lejeune_L/0/1/0/all/0/1\">Laurent Lejeune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grossrieder_J/0/1/0/all/0/1\">Jan Grossrieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1\">Raphael Sznitman</a>",
          "description": "Recent machine learning strategies for segmentation tasks have shown great\nability when trained on large pixel-wise annotated image datasets. It remains a\nmajor challenge however to aggregate such datasets, as the time and monetary\ncost associated with collecting extensive annotations is extremely high. This\nis particularly the case for generating precise pixel-wise annotations in video\nand volumetric image data. To this end, this work presents a novel framework to\nproduce pixel-wise segmentations using minimal supervision. Our method relies\non 2D point supervision, whereby a single 2D location within an object of\ninterest is provided on each image of the data. Our method then estimates the\nobject appearance in a semi-supervised fashion by learning\nobject-image-specific features and by using these in a semi-supervised learning\nframework. Our object model is then used in a graph-based optimization problem\nthat takes into account all provided locations and the image data in order to\ninfer the complete pixel-wise segmentation. In practice, we solve this\noptimally as a tracking problem using a K-shortest path approach. Both the\nobject model and segmentation are then refined iteratively to further improve\nthe final segmentation. We show that by collecting 2D locations using a gaze\ntracker, our approach can provide state-of-the-art segmentations on a range of\nobjects and image modalities (video and 3D volumes), and that these can then be\nused to train supervised machine learning classifiers.",
          "link": "http://arxiv.org/abs/1809.00970",
          "publishedOn": "2021-07-20T02:04:46.694Z",
          "wordCount": null,
          "title": "Iterative multi-path tracking for video and volume segmentation with sparse point supervision. (arXiv:1809.00970v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all the previously\nmentioned drawbacks). The entropic out-of-distribution detection solution\ncomprises the IsoMax loss for training and the entropic score for\nout-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in\nreplacement because swapping the SoftMax loss with the IsoMax loss requires no\nchanges in the model's architecture or training procedures/hyperparameters. In\nthis paper, we propose to perform what we call an isometrization of the\ndistances used in the IsoMax loss. Additionally, we propose to replace the\nentropic score with the minimum distance score. Our experiments showed that\nthese simple modifications increase out-of-distribution detection performance\nwhile keeping the solution seamless. Code available at\n$\\href{https://github.com/dlmacedo/entropic-out-of-distribution-detection}{\\text{entropic\nout-of-distribution detection}}$.",
          "link": "http://arxiv.org/abs/2105.14399",
          "publishedOn": "2021-07-20T02:04:46.693Z",
          "wordCount": null,
          "title": "Improving Entropic Out-of-Distribution Detection using Isometric Distances and the Minimum Distance Score. (arXiv:2105.14399v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xueying Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "Automated surgical gesture recognition is of great importance in\nrobot-assisted minimally invasive surgery. However, existing methods assume\nthat training and testing data are from the same domain, which suffers from\nsevere performance degradation when a domain gap exists, such as the simulator\nand real robot. In this paper, we propose a novel unsupervised domain\nadaptation framework which can simultaneously transfer multi-modality\nknowledge, i.e., both kinematic and visual data, from simulator to real robot.\nIt remedies the domain gap with enhanced transferable features by using\ntemporal cues in videos, and inherent correlations in multi-modal towards\nrecognizing gesture. Specifically, we first propose an MDO-K to align\nkinematics, which exploits temporal continuity to transfer motion directions\nwith smaller gap rather than position values, relieving the adaptation burden.\nMoreover, we propose a KV-Relation-ATT to transfer the co-occurrence signals of\nkinematics and vision. Such features attended by correlation similarity are\nmore informative for enhancing domain-invariance of the model. Two feature\nalignment strategies benefit the model mutually during the end-to-end learning\nprocess. We extensively evaluate our method for gesture recognition using DESK\ndataset with peg transfer procedure. Results show that our approach recovers\nthe performance with great improvement gains, up to 12.91% in ACC and 20.16% in\nF1score without using any annotations in real robot.",
          "link": "http://arxiv.org/abs/2103.04075",
          "publishedOn": "2021-07-20T02:04:46.690Z",
          "wordCount": null,
          "title": "Domain Adaptive Robotic Gesture Recognition with Unsupervised Kinematic-Visual Data Alignment. (arXiv:2103.04075v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuehua Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>",
          "description": "With the identity information in face data more closely related to personal\ncredit and property security, people pay increasing attention to the protection\nof face data privacy. In different tasks, people have various requirements for\nface de-identification (De-ID), so we propose a systematical solution\ncompatible for these De-ID operations. Firstly, an attribute disentanglement\nand generative network is constructed to encode two parts of the face, which\nare the identity (facial features like mouth, nose and eyes) and expression\n(including expression, pose and illumination). Through face swapping, we can\nremove the original ID completely. Secondly, we add an adversarial vector\nmapping network to perturb the latent code of the face image, different from\nprevious traditional adversarial methods. Through this, we can construct\nunrestricted adversarial image to decrease ID similarity recognized by model.\nOur method can flexibly de-identify the face data in various ways and the\nprocessed images have high image quality.",
          "link": "http://arxiv.org/abs/2107.08581",
          "publishedOn": "2021-07-20T02:04:46.688Z",
          "wordCount": null,
          "title": "A Systematical Solution for Face De-identification. (arXiv:2107.08581v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.12434",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jiahang Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xinzhe Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ouyang_C/0/1/0/all/0/1\">Cheng Ouyang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campello_V/0/1/0/all/0/1\">Victor M. Campello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vesal_S/0/1/0/all/0/1\">Sulaiman Vesal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+RaviKumar_N/0/1/0/all/0/1\">Nishant RaviKumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yashu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_G/0/1/0/all/0/1\">Gongning Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jingkun Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ly_B/0/1/0/all/0/1\">Buntheng Ly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sermesant_M/0/1/0/all/0/1\">Maxime Sermesant</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roth_H/0/1/0/all/0/1\">Holger Roth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiexiang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_X/0/1/0/all/0/1\">Xinghao Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xinyue Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Accurate computing, analysis and modeling of the ventricles and myocardium\nfrom medical images are important, especially in the diagnosis and treatment\nmanagement for patients suffering from myocardial infarction (MI). Late\ngadolinium enhancement (LGE) cardiac magnetic resonance (CMR) provides an\nimportant protocol to visualize MI. However, automated segmentation of LGE CMR\nis still challenging, due to the indistinguishable boundaries, heterogeneous\nintensity distribution and complex enhancement patterns of pathological\nmyocardium from LGE CMR. Furthermore, compared with the other sequences LGE CMR\nimages with gold standard labels are particularly limited, which represents\nanother obstacle for developing novel algorithms for automatic segmentation of\nLGE CMR. This paper presents the selective results from the Multi-Sequence\nCardiac MR (MS-CMR) Segmentation challenge, in conjunction with MICCAI 2019.\nThe challenge offered a data set of paired MS-CMR images, including auxiliary\nCMR sequences as well as LGE CMR, from 45 patients who underwent\ncardiomyopathy. It was aimed to develop new algorithms, as well as benchmark\nexisting ones for LGE CMR segmentation and compare them objectively. In\naddition, the paired MS-CMR images could enable algorithms to combine the\ncomplementary information from the other sequences for the segmentation of LGE\nCMR. Nine representative works were selected for evaluation and comparisons,\namong which three methods are unsupervised methods and the other six are\nsupervised. The results showed that the average performance of the nine methods\nwas comparable to the inter-observer variations. The success of these methods\nwas mainly attributed to the inclusion of the auxiliary sequences from the\nMS-CMR images, which provide important label information for the training of\ndeep neural networks.",
          "link": "http://arxiv.org/abs/2006.12434",
          "publishedOn": "2021-07-20T02:04:46.616Z",
          "wordCount": null,
          "title": "Cardiac Segmentation on Late Gadolinium Enhancement MRI: A Benchmark Study from Multi-Sequence Cardiac MR Segmentation Challenge. (arXiv:2006.12434v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nishimura_K/0/1/0/all/0/1\">Kazuya Nishimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyeonwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bise_R/0/1/0/all/0/1\">Ryoma Bise</a>",
          "description": "Cell detection is the task of detecting the approximate positions of cell\ncentroids from microscopy images. Recently, convolutional neural network-based\napproaches have achieved promising performance. However, these methods require\na certain amount of annotation for each imaging condition. This annotation is a\ntime-consuming and labor-intensive task. To overcome this problem, we propose a\nsemi-supervised cell-detection method that effectively uses a time-lapse\nsequence with one labeled image and the other images unlabeled. First, we train\na cell-detection network with a one-labeled image and estimate the unlabeled\nimages with the trained network. We then select high-confidence positions from\nthe estimations by tracking the detected cells from the labeled frame to those\nfar from it. Next, we generate pseudo-labels from the tracking results and\ntrain the network by using pseudo-labels. We evaluated our method for seven\nconditions of public datasets, and we achieved the best results relative to\nother semi-supervised methods. Our code is available at\nhttps://github.com/naivete5656/SCDTC",
          "link": "http://arxiv.org/abs/2107.08639",
          "publishedOn": "2021-07-20T02:04:46.615Z",
          "wordCount": null,
          "title": "Semi-supervised Cell Detection in Time-lapse Images Using Temporal Consistency. (arXiv:2107.08639v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kunlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shinan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jun Hou</a>",
          "description": "Video crowd localization is a crucial yet challenging task, which aims to\nestimate exact locations of human heads in the given crowded videos. To model\nspatial-temporal dependencies of human mobility, we propose a multi-focus\nGaussian neighbor attention (GNA), which can effectively exploit long-range\ncorrespondences while maintaining the spatial topological structure of the\ninput videos. In particular, our GNA can also capture the scale variation of\nhuman heads well using the equipped multi-focus mechanism. Based on the\nmulti-focus GNA, we develop a unified neural network called GNANet to\naccurately locate head centers in video clips by fully aggregating\nspatial-temporal information via a scene modeling module and a context\ncross-attention module. Moreover, to facilitate future researches in this\nfield, we introduce a large-scale crowded video benchmark named SenseCrowd,\nwhich consists of 60K+ frames captured in various surveillance scenarios and\n2M+ head annotations. Finally, we conduct extensive experiments on three\ndatasets including our SenseCrowd, and the experiment results show that the\nproposed method is capable to achieve state-of-the-art performance for both\nvideo crowd localization and counting. The code and the dataset will be\nreleased.",
          "link": "http://arxiv.org/abs/2107.08645",
          "publishedOn": "2021-07-20T02:04:46.614Z",
          "wordCount": null,
          "title": "Video Crowd Localization with Multi-focus Gaussian Neighbor Attention and a Large-Scale Benchmark. (arXiv:2107.08645v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2004.10141",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Ari_R/0/1/0/all/0/1\">Rami Ben-Ari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shpigel_M/0/1/0/all/0/1\">Mor Shpigel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azulai_O/0/1/0/all/0/1\">Ophir Azulai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzelay_U/0/1/0/all/0/1\">Udi Barzelay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rotman_D/0/1/0/all/0/1\">Daniel Rotman</a>",
          "description": "Classification of new class entities requires collecting and annotating\nhundreds or thousands of samples that is often prohibitively costly. Few-shot\nlearning suggests learning to classify new classes using just a few examples.\nOnly a small number of studies address the challenge of few-shot learning on\nspatio-temporal patterns such as videos. In this paper, we present the Temporal\nAware Embedding Network (TAEN) for few-shot action recognition, that learns to\nrepresent actions, in a metric space as a trajectory, conveying both short term\nsemantics and longer term connectivity between action parts. We demonstrate the\neffectiveness of TAEN on two few shot tasks, video classification and temporal\naction detection and evaluate our method on the Kinetics-400 and on ActivityNet\n1.2 few-shot benchmarks. With training of just a few fully connected layers we\nreach comparable results to prior art on both few shot video classification and\ntemporal detection tasks, while reaching state-of-the-art in certain scenarios.",
          "link": "http://arxiv.org/abs/2004.10141",
          "publishedOn": "2021-07-20T02:04:46.614Z",
          "wordCount": null,
          "title": "TAEN: Temporal Aware Embedding Network for Few-Shot Action Recognition. (arXiv:2004.10141v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Congram_B/0/1/0/all/0/1\">Benjamin Congram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barfoot_T/0/1/0/all/0/1\">Timothy D. Barfoot</a>",
          "description": "Visual Teach and Repeat has shown relative navigation is a robust and\nefficient solution for autonomous vision-based path following in difficult\nenvironments. Adding additional absolute sensors such as Global Navigation\nSatellite Systems (GNSS) has the potential to expand the domain of Visual Teach\nand Repeat to environments where the ability to visually localize is not\nguaranteed. Our method of lazy mapping and delaying estimation until a\npath-tracking error is needed avoids the need to estimate absolute states. As a\nresult, map optimization is not required and paths can be driven immediately\nafter being taught. We validate our approach on a real robot through an\nexperiment in a joint indoor-outdoor environment comprising 3.5km of autonomous\nroute repeating across a variety of lighting conditions. We achieve smooth\nerror signals throughout the runs despite large sections of dropout for each\nsensor.",
          "link": "http://arxiv.org/abs/2101.05107",
          "publishedOn": "2021-07-20T02:04:46.613Z",
          "wordCount": null,
          "title": "Relatively Lazy: Indoor-Outdoor Navigation Using Vision and GNSS. (arXiv:2101.05107v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shunyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenxi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Libo Wang</a>",
          "description": "Semantic segmentation using fine-resolution remotely sensed images plays a\ncritical role in many practical applications, such as urban planning,\nenvironmental protection, natural and anthropogenic landscape monitoring, etc.\nHowever, the automation of semantic segmentation, i.e., automatic\ncategorization/labeling and segmentation is still a challenging task,\nparticularly for fine-resolution images with huge spatial and spectral\ncomplexity. Addressing such a problem represents an exciting research field,\nwhich paves the way for scene-level landscape pattern analysis and decision\nmaking. In this paper, we propose an approach for automatic land segmentation\nbased on the Feature Pyramid Network (FPN). As a classic architecture, FPN can\nbuild a feature pyramid with high-level semantics throughout. However,\nintrinsic defects in feature extraction and fusion hinder FPN from further\naggregating more discriminative features. Hence, we propose an Attention\nAggregation Module (AAM) to enhance multi-scale feature learning through\nattention-guided feature aggregation. Based on FPN and AAM, a novel framework\nnamed Attention Aggregation Feature Pyramid Network (A2-FPN) is developed for\nsemantic segmentation of fine-resolution remotely sensed images. Extensive\nexperiments conducted on three datasets demonstrate the effectiveness of our A2\n-FPN in segmentation accuracy. Code is available at\nhttps://github.com/lironui/A2-FPN.",
          "link": "http://arxiv.org/abs/2102.07997",
          "publishedOn": "2021-07-20T02:04:46.613Z",
          "wordCount": null,
          "title": "A2-FPN for Semantic Segmentation of Fine-Resolution Remotely Sensed Images. (arXiv:2102.07997v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11860",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Garg_A/0/1/0/all/0/1\">Aksh Garg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salehi_S/0/1/0/all/0/1\">Sana Salehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rocca_M/0/1/0/all/0/1\">Marianna La Rocca</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garner_R/0/1/0/all/0/1\">Rachael Garner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duncan_D/0/1/0/all/0/1\">Dominique Duncan</a>",
          "description": "With COVID-19 cases rising rapidly, deep learning has emerged as a promising\ndiagnosis technique. However, identifying the most accurate models to\ncharacterize COVID-19 patients is challenging because comparing results\nobtained with different types of data and acquisition processes is non-trivial.\nIn this paper we designed, evaluated, and compared the performance of 20\nconvolutional neutral networks in classifying patients as COVID-19 positive,\nhealthy, or suffering from other pulmonary lung infections based on Chest CT\nscans, serving as the first to consider the EfficientNet family for COVID-19\ndiagnosis and employ intermediate activation maps for visualizing model\nperformance. All models are trained and evaluated in Python using 4173 Chest CT\nimages from the dataset entitled \"A COVID multiclass dataset of CT scans,\" with\n2168, 758, and 1247 images of patients that are COVID-19 positive, healthy, or\nsuffering from other pulmonary infections, respectively. EfficientNet-B5 was\nidentified as the best model with an F1 score of 0.9769+/-0.0046, accuracy of\n0.9759+/-0.0048, sensitivity of 0.9788+/-0.0055, specificity of\n0.9730+/-0.0057, and precision of 0.9751 +/- 0.0051. On an alternate 2-class\ndataset, EfficientNetB5 obtained an accuracy of 0.9845+/-0.0109, F1 score of\n0.9599+/-0.0251, sensitivity of 0.9682+/-0.0099, specificity of\n0.9883+/-0.0150, and precision of 0.9526 +/- 0.0523. Intermediate activation\nmaps and Gradient-weighted Class Activation Mappings offered\nhuman-interpretable evidence of the model's perception of ground-class\nopacities and consolidations, hinting towards a promising use-case of\nartificial intelligence-assisted radiology tools. With a prediction speed of\nunder 0.1 seconds on GPUs and 0.5 seconds on CPUs, our proposed model offers a\nrapid, scalable, and accurate diagnostic for COVID-19.",
          "link": "http://arxiv.org/abs/2012.11860",
          "publishedOn": "2021-07-20T02:04:46.612Z",
          "wordCount": null,
          "title": "Efficient and Visualizable Convolutional Neural Networks for COVID-19 Classification Using Chest CT. (arXiv:2012.11860v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plested_J/0/1/0/all/0/1\">Jo Plested</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuyang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "The current standard for a variety of computer vision tasks using smaller\nnumbers of labelled training examples is to fine-tune from weights pre-trained\non a large image classification dataset such as ImageNet. The application of\ntransfer learning and transfer learning methods tends to be rigidly binary. A\nmodel is either pre-trained or not pre-trained. Pre-training a model either\nincreases performance or decreases it, the latter being defined as negative\ntransfer. Application of L2-SP regularisation that decays the weights towards\ntheir pre-trained values is either applied or all weights are decayed towards\n0. This paper re-examines these assumptions. Our recommendations are based on\nextensive empirical evaluation that demonstrate the application of a non-binary\napproach to achieve optimal results. (1) Achieving best performance on each\nindividual dataset requires careful adjustment of various transfer learning\nhyperparameters not usually considered, including number of layers to transfer,\ndifferent learning rates for different layers and different combinations of\nL2SP and L2 regularization. (2) Best practice can be achieved using a number of\nmeasures of how well the pre-trained weights fit the target dataset to guide\noptimal hyperparameters. We present methods for non-binary transfer learning\nincluding combining L2SP and L2 regularization and performing non-traditional\nfine-tuning hyperparameter searches. Finally we suggest heuristics for\ndetermining the optimal transfer learning hyperparameters. The benefits of\nusing a non-binary approach are supported by final results that come close to\nor exceed state of the art performance on a variety of tasks that have\ntraditionally been more difficult for transfer learning.",
          "link": "http://arxiv.org/abs/2107.08585",
          "publishedOn": "2021-07-20T02:04:46.611Z",
          "wordCount": null,
          "title": "Non-binary deep transfer learning for imageclassification. (arXiv:2107.08585v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lonkar_S/0/1/0/all/0/1\">Subodh Lonkar</a>",
          "description": "Over the centuries, humans have developed and acquired a number of ways to\ncommunicate. But hardly any of them can be as natural and instinctive as facial\nexpressions. On the other hand, neural networks have taken the world by storm.\nAnd no surprises, that the area of Computer Vision and the problem of facial\nexpressions recognitions hasn't remained untouched. Although a wide range of\ntechniques have been applied, achieving extremely high accuracies and preparing\nhighly robust FER systems still remains a challenge due to heterogeneous\ndetails in human faces. In this paper, we will be deep diving into implementing\na system for recognition of facial expressions (FER) by leveraging neural\nnetworks, and more specifically, Convolutional Neural Networks (CNNs). We adopt\nthe fundamental concepts of deep learning and computer vision with various\narchitectures, fine-tune it's hyperparameters and experiment with various\noptimization methods and demonstrate a state-of-the-art single-network-accuracy\nof 70.10% on the FER2013 dataset without using any additional training data.",
          "link": "http://arxiv.org/abs/2107.08640",
          "publishedOn": "2021-07-20T02:04:46.611Z",
          "wordCount": null,
          "title": "Facial Expressions Recognition with Convolutional Neural Networks. (arXiv:2107.08640v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.02638",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kanglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Gaofeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bozhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiang Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>",
          "description": "Facial attributes in StyleGAN generated images are entangled in the latent\nspace which makes it very difficult to independently control a specific\nattribute without affecting the others. Supervised attribute editing requires\nannotated training data which is difficult to obtain and limits the editable\nattributes to those with labels. Therefore, unsupervised attribute editing in\nan disentangled latent space is key to performing neat and versatile semantic\nface editing. In this paper, we present a new technique termed\nStructure-Texture Independent Architecture with Weight Decomposition and\nOrthogonal Regularization (STIA-WO) to disentangle the latent space for\nunsupervised semantic face editing. By applying STIA-WO to GAN, we have\ndeveloped a StyleGAN termed STGAN-WO which performs weight decomposition\nthrough utilizing the style vector to construct a fully controllable weight\nmatrix to regulate image synthesis, and employs orthogonal regularization to\nensure each entry of the style vector only controls one independent feature\nmatrix. To further disentangle the facial attributes, STGAN-WO introduces a\nstructure-texture independent architecture which utilizes two independently and\nidentically distributed (i.i.d.) latent vectors to control the synthesis of the\ntexture and structure components in a disentangled way. Unsupervised semantic\nediting is achieved by moving the latent code in the coarse layers along its\northogonal directions to change texture related attributes or changing the\nlatent code in the fine layers to manipulate structure related ones. We present\nexperimental results which show that our new STGAN-WO can achieve better\nattribute editing than state of the art methods.",
          "link": "http://arxiv.org/abs/2011.02638",
          "publishedOn": "2021-07-20T02:04:46.610Z",
          "wordCount": null,
          "title": "Towards Disentangling Latent Space for Unsupervised Semantic Face Editing. (arXiv:2011.02638v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>",
          "description": "In this paper, we develop face.evoLVe -- a comprehensive library that\ncollects and implements a wide range of popular deep learning-based methods for\nface recognition. First of all, face.evoLVe is composed of key components that\ncover the full process of face analytics, including face alignment, data\nprocessing, various backbones, losses, and alternatives with bags of tricks for\nimproving performance. Later, face.evoLVe supports multi-GPU training on top of\ndifferent deep learning platforms, such as PyTorch and PaddlePaddle, which\nfacilitates researchers to work on both large-scale datasets with millions of\nimages and low-shot counterparts with limited well-annotated data. More\nimportantly, along with face.evoLVe, images before & after alignment in the\ncommon benchmark datasets are released with source codes and trained models\nprovided. All these efforts lower the technical burdens in reproducing the\nexisting methods for comparison, while users of our library could focus on\ndeveloping advanced approaches more efficiently. Last but not least,\nface.evoLVe is well designed and vibrantly evolving, so that new face\nrecognition approaches can be easily plugged into our framework. Note that we\nhave used face.evoLVe to participate in a number of face recognition\ncompetitions and secured the first place. The version that supports PyTorch is\npublicly available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch and the\nPaddlePaddle version is available at\nhttps://github.com/ZhaoJ9014/face.evoLVe.PyTorch/tree/master/paddle.\nFace.evoLVe has been widely used for face analytics, receiving 2.4K stars and\n622 forks.",
          "link": "http://arxiv.org/abs/2107.08621",
          "publishedOn": "2021-07-20T02:04:46.609Z",
          "wordCount": null,
          "title": "Face.evoLVe: A High-Performance Face Recognition Library. (arXiv:2107.08621v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.09465",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_J/0/1/0/all/0/1\">Jue Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yu Chi Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tyagi_N/0/1/0/all/0/1\">Neelam Tyagi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rimner_A/0/1/0/all/0/1\">Andreas Rimner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_N/0/1/0/all/0/1\">Nancy Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deasy_J/0/1/0/all/0/1\">Joseph O. Deasy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Berry_S/0/1/0/all/0/1\">Sean Berry</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veeraraghavan_H/0/1/0/all/0/1\">Harini Veeraraghavan</a>",
          "description": "We developed a new joint probabilistic segmentation and image distribution\nmatching generative adversarial network (PSIGAN) for unsupervised domain\nadaptation (UDA) and multi-organ segmentation from magnetic resonance (MRI)\nimages. Our UDA approach models the co-dependency between images and their\nsegmentation as a joint probability distribution using a new structure\ndiscriminator. The structure discriminator computes structure of interest\nfocused adversarial loss by combining the generated pseudo MRI with\nprobabilistic segmentations produced by a simultaneously trained segmentation\nsub-network. The segmentation sub-network is trained using the pseudo MRI\nproduced by the generator sub-network. This leads to a cyclical optimization of\nboth the generator and segmentation sub-networks that are jointly trained as\npart of an end-to-end network. Extensive experiments and comparisons against\nmultiple state-of-the-art methods were done on four different MRI sequences\ntotalling 257 scans for generating multi-organ and tumor segmentation. The\nexperiments included, (a) 20 T1-weighted (T1w) in-phase mdixon and (b) 20\nT2-weighted (T2w) abdominal MRI for segmenting liver, spleen, left and right\nkidneys, (c) 162 T2-weighted fat suppressed head and neck MRI (T2wFS) for\nparotid gland segmentation, and (d) 75 T2w MRI for lung tumor segmentation. Our\nmethod achieved an overall average DSC of 0.87 on T1w and 0.90 on T2w for the\nabdominal organs, 0.82 on T2wFS for the parotid glands, and 0.77 on T2w MRI for\nlung tumors.",
          "link": "http://arxiv.org/abs/2007.09465",
          "publishedOn": "2021-07-20T02:04:46.608Z",
          "wordCount": null,
          "title": "PSIGAN: Joint probabilistic segmentation and image distribution matching for unpaired cross-modality adaptation based MRI segmentation. (arXiv:2007.09465v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">He Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1\">Richard P. Wildes</a>",
          "description": "Video predictive understanding encompasses a wide range of efforts that are\nconcerned with the anticipation of the unobserved future from the current as\nwell as historical video observations. Action prediction is a major sub-area of\nvideo predictive understanding and is the focus of this review. This sub-area\nhas two major subdivisions: early action recognition and future action\nprediction. Early action recognition is concerned with recognizing an ongoing\naction as soon as possible. Future action prediction is concerned with the\nanticipation of actions that follow those previously observed. In either case,\nthe \\textbf{\\textit{causal}} relationship between the past, current, and\npotential future information is the main focus. Various mathematical tools such\nas Markov Chains, Gaussian Processes, Auto-Regressive modeling, and Bayesian\nrecursive filtering are widely adopted jointly with computer vision techniques\nfor these two tasks. However, these approaches face challenges such as the\ncurse of dimensionality, poor generalization, and constraints from\ndomain-specific knowledge. Recently, structures that rely on deep convolutional\nneural networks and recurrent neural networks have been extensively proposed\nfor improving the performance of existing vision tasks, in general, and action\nprediction tasks, in particular. However, they have their own shortcomings, \\eg\nreliance on massive training data and lack of strong theoretical underpinnings.\nIn this survey, we start by introducing the major sub-areas of the broad area\nof video predictive understanding, which recently have received intensive\nattention and proven to have practical value. Next, a thorough review of\nvarious early action recognition and future action prediction algorithms are\nprovided with suitably organized divisions. Finally, we conclude our discussion\nwith future research directions.",
          "link": "http://arxiv.org/abs/2107.05140",
          "publishedOn": "2021-07-20T02:04:46.587Z",
          "wordCount": null,
          "title": "Review of Video Predictive Understanding: Early Action Recognition and Future Action Prediction. (arXiv:2107.05140v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yifan Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tianjun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1\">David Wipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We propose a hierarchical graph neural network (GNN) model that learns how to\ncluster a set of images into an unknown number of identities using a training\nset of images annotated with labels belonging to a disjoint set of identities.\nOur hierarchical GNN uses a novel approach to merge connected components\npredicted at each level of the hierarchy to form a new graph at the next level.\nUnlike fully unsupervised hierarchical clustering, the choice of grouping and\ncomplexity criteria stems naturally from supervision in the training set. The\nresulting method, Hi-LANDER, achieves an average of 54% improvement in F-score\nand 8% increase in Normalized Mutual Information (NMI) relative to current\nGNN-based clustering algorithms. Additionally, state-of-the-art GNN-based\nmethods rely on separate models to predict linkage probabilities and node\ndensities as intermediate steps of the clustering process. In contrast, our\nunified framework achieves a seven-fold decrease in computational cost. We\nrelease our training and inference code at\nhttps://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander.",
          "link": "http://arxiv.org/abs/2107.01319",
          "publishedOn": "2021-07-20T02:04:46.585Z",
          "wordCount": null,
          "title": "Learning Hierarchical Graph Neural Networks for Image Clustering. (arXiv:2107.01319v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06419",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jinke Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lv_P/0/1/0/all/0/1\">Peiqing Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haiying Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1\">Changfa Shi</a>",
          "description": "Background and objective: In this paper, a modified U-Net based framework is\npresented, which leverages techniques from Squeeze-and-Excitation (SE) block,\nAtrous Spatial Pyramid Pooling (ASPP) and residual learning for accurate and\nrobust liver CT segmentation, and the effectiveness of the proposed method was\ntested on two public datasets LiTS17 and SLiver07.\n\nMethods: A new network architecture called SAR-U-Net was designed. Firstly,\nthe SE block is introduced to adaptively extract image features after each\nconvolution in the U-Net encoder, while suppressing irrelevant regions, and\nhighlighting features of specific segmentation task; Secondly, ASPP was\nemployed to replace the transition layer and the output layer, and acquire\nmulti-scale image information via different receptive fields. Thirdly, to\nalleviate the degradation problem, the traditional convolution block was\nreplaced with the residual block and thus prompt the network to gain accuracy\nfrom considerably increased depth.\n\nResults: In the LiTS17 experiment, the mean values of Dice, VOE, RVD, ASD and\nMSD were 95.71, 9.52, -0.84, 1.54 and 29.14, respectively. Compared with other\nclosely related 2D-based models, the proposed method achieved the highest\naccuracy. In the experiment of the SLiver07, the mean values of Dice, VOE, RVD,\nASD and MSD were 97.31, 5.37, -1.08, 1.85 and 27.45, respectively. Compared\nwith other closely related models, the proposed method achieved the highest\nsegmentation accuracy except for the RVD.\n\nConclusion: The proposed model enables a great improvement on the accuracy\ncompared to 2D-based models, and its robustness in circumvent challenging\nproblems, such as small liver regions, discontinuous liver regions, and fuzzy\nliver boundaries, is also well demonstrated and validated.",
          "link": "http://arxiv.org/abs/2103.06419",
          "publishedOn": "2021-07-20T02:04:46.582Z",
          "wordCount": null,
          "title": "SAR-U-Net: squeeze-and-excitation block and atrous spatial pyramid pooling based residual U-Net for automatic liver segmentation in Computed Tomography. (arXiv:2103.06419v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weimin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bo Yan</a>",
          "description": "Affective Analysis is not a single task, and the valence-arousal value,\nexpression class, and action unit can be predicted at the same time. Previous\nresearches did not pay enough attention to the entanglement and hierarchical\nrelation of these three facial attributes. We propose a novel model named\nfeature pyramid networks for multi-task affect analysis. The hierarchical\nfeatures are extracted to predict three labels and we apply a teacher-student\ntraining strategy to learn from pretrained single-task models. Extensive\nexperiment results demonstrate the proposed model outperforms other models.\nThis is a submission to The 2nd Workshop and Competition on Affective Behavior\nAnalysis in the wild (ABAW). The code and model are available for research\npurposes at https://github.com/ryanhe312/ABAW2-FPNMAA.",
          "link": "http://arxiv.org/abs/2107.03670",
          "publishedOn": "2021-07-20T02:04:46.580Z",
          "wordCount": null,
          "title": "Feature Pyramid Network for Multi-task Affective Analysis. (arXiv:2107.03670v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_E/0/1/0/all/0/1\">Erin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1\">Meher Anand Kasam</a>",
          "description": "Data imbalance is a ubiquitous problem in machine learning. In large scale\ncollected and annotated datasets, data imbalance is either mitigated manually\nby undersampling frequent classes and oversampling rare classes, or planned for\nwith imputation and augmentation techniques. In both cases balancing data\nrequires labels. In other words, only annotated data can be balanced.\nCollecting fully annotated datasets is challenging, especially for large scale\nsatellite systems such as the unlabeled NASA's 35 PB Earth Imagery dataset.\nAlthough the NASA Earth Imagery dataset is unlabeled, there are implicit\nproperties of the data source that we can rely on to hypothesize about its\nimbalance, such as distribution of land and water in the case of the Earth's\nimagery. We present a new iterative method to balance unlabeled data. Our\nmethod utilizes image embeddings as a proxy for image labels that can be used\nto balance data, and ultimately when trained increases overall accuracy.",
          "link": "http://arxiv.org/abs/2107.03227",
          "publishedOn": "2021-07-20T02:04:46.580Z",
          "wordCount": null,
          "title": "Scalable Data Balancing for Unlabeled Satellite Imagery. (arXiv:2107.03227v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kotseruba_I/0/1/0/all/0/1\">Iuliia Kotseruba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papagelis_M/0/1/0/all/0/1\">Manos Papagelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsotsos_J/0/1/0/all/0/1\">John K. Tsotsos</a>",
          "description": "This work aims to study the dynamic between research in the industry and\nacademia in computer vision. The results are demonstrated on a set of top-5\nvision conferences that are representative of the field. Since data for such\nanalysis was not readily available, significant effort was spent on gathering\nand processing meta-data from the original publications. First, this study\nquantifies the share of industry-sponsored research. Specifically, it shows\nthat the proportion of papers published by industry-affiliated researchers is\nincreasing and that more academics join companies or collaborate with them.\nNext, the possible impact of industry presence is further explored, namely in\nthe distribution of research topics and citation patterns. The results indicate\nthat the distribution of the research topics is similar in industry and\nacademic papers. However, there is a strong preference towards citing industry\npapers. Finally, possible reasons for citation bias, such as code availability\nand influence, are investigated.",
          "link": "http://arxiv.org/abs/2107.04902",
          "publishedOn": "2021-07-20T02:04:46.579Z",
          "wordCount": null,
          "title": "Industry and Academic Research in Computer Vision. (arXiv:2107.04902v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>",
          "description": "Generalization to out-of-distribution (OOD) data is a capability natural to\nhumans yet challenging for machines to reproduce. This is because most learning\nalgorithms strongly rely on the i.i.d.~assumption on source/target data, which\nis often violated in practice due to domain shift. Domain generalization (DG)\naims to achieve OOD generalization by using only source data for model\nlearning. Since first introduced in 2011, research in DG has made great\nprogresses. In particular, intensive research in this topic has led to a broad\nspectrum of methodologies, e.g., those based on domain alignment,\nmeta-learning, data augmentation, or ensemble learning, just to name a few; and\nhas covered various vision applications such as object recognition,\nsegmentation, action recognition, and person re-identification. In this paper,\nfor the first time a comprehensive literature review is provided to summarize\nthe developments in DG for computer vision over the past decade. Specifically,\nwe first cover the background by formally defining DG and relating it to other\nresearch fields like domain adaptation and transfer learning. Second, we\nconduct a thorough review into existing methods and present a categorization\nbased on their methodologies and motivations. Finally, we conclude this survey\nwith insights and discussions on future research directions.",
          "link": "http://arxiv.org/abs/2103.02503",
          "publishedOn": "2021-07-20T02:04:46.578Z",
          "wordCount": null,
          "title": "Domain Generalization in Vision: A Survey. (arXiv:2103.02503v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvirul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1\">Muhammad Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1\">Ferda Ofli</a>",
          "description": "Images shared on social media help crisis managers gain situational awareness\nand assess incurred damages, among other response tasks. As the volume and\nvelocity of such content are typically high, real-time image classification has\nbecome an urgent need for a faster disaster response. Recent advances in\ncomputer vision and deep neural networks have enabled the development of models\nfor real-time image classification for a number of tasks, including detecting\ncrisis incidents, filtering irrelevant images, classifying images into specific\nhumanitarian categories, and assessing the severity of the damage. To develop\nrobust real-time models, it is necessary to understand the capability of the\npublicly available pre-trained models for these tasks, which remains to be\nunder-explored in the crisis informatics literature. In this study, we address\nsuch limitations by investigating ten different network architectures for four\ndifferent tasks using the largest publicly available datasets for these tasks.\nWe also explore various data augmentation strategies, semi-supervised\ntechniques, and a multitask learning setup. In our extensive experiments, we\nachieve promising results.",
          "link": "http://arxiv.org/abs/2104.04184",
          "publishedOn": "2021-07-20T02:04:46.578Z",
          "wordCount": null,
          "title": "Robust Training of Social Media Image Classification Models for Rapid Disaster Response. (arXiv:2104.04184v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02739",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Han_S/0/1/0/all/0/1\">Sukjin Han</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Schulman_E/0/1/0/all/0/1\">Eric H. Schulman</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Ramakrishnan_S/0/1/0/all/0/1\">Santhosh Ramakrishnan</a>",
          "description": "Many differentiated products have key attributes that are unstructured and\nthus high-dimensional (e.g., design, text). Instead of treating unstructured\nattributes as unobservables in economic models, quantifying them can be\nimportant to answer interesting economic questions. To propose an analytical\nframework for this type of products, this paper considers one of the simplest\ndesign products -- fonts -- and investigates merger and product differentiation\nusing an original dataset from the world's largest online marketplace for\nfonts. We quantify font shapes by constructing embeddings from a deep\nconvolutional neural network. Each embedding maps a font's shape onto a\nlow-dimensional vector. In the resulting product space, designers are assumed\nto engage in Hotelling-type spatial competition. From the image embeddings, we\nconstruct two alternative measures that capture the degree of design\ndifferentiation. We then study the causal effects of a merger on the merging\nfirm's creative decisions using the constructed measures in a synthetic control\nmethod. We find that the merger causes the merging firm to increase the visual\nvariety of font design. Notably, such effects are not captured when using\ntraditional measures for product offerings (e.g., specifications and the number\nof products) constructed from structured data.",
          "link": "http://arxiv.org/abs/2107.02739",
          "publishedOn": "2021-07-20T02:04:46.577Z",
          "wordCount": null,
          "title": "Shapes as Product Differentiation: Neural Network Embedding in the Analysis of Markets for Fonts. (arXiv:2107.02739v1 [econ.EM] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Linqing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "In this paper, we propose a similarity-aware fusion network (SAFNet) to\nadaptively fuse 2D images and 3D point clouds for 3D semantic segmentation.\nExisting fusion-based methods achieve remarkable performances by integrating\ninformation from multiple modalities. However, they heavily rely on the\ncorrespondence between 2D pixels and 3D points by projection and can only\nperform the information fusion in a fixed manner, and thus their performances\ncannot be easily migrated to a more realistic scenario where the collected data\noften lack strict pair-wise features for prediction. To address this, we employ\na late fusion strategy where we first learn the geometric and contextual\nsimilarities between the input and back-projected (from 2D pixels) point clouds\nand utilize them to guide the fusion of two modalities to further exploit\ncomplementary information. Specifically, we employ a geometric similarity\nmodule (GSM) to directly compare the spatial coordinate distributions of\npair-wise 3D neighborhoods, and a contextual similarity module (CSM) to\naggregate and compare spatial contextual information of corresponding central\npoints. The two proposed modules can effectively measure how much image\nfeatures can help predictions, enabling the network to adaptively adjust the\ncontributions of two modalities to the final prediction of each point.\nExperimental results on the ScanNetV2 benchmark demonstrate that SAFNet\nsignificantly outperforms existing state-of-the-art fusion-based approaches\nacross various data integrity.",
          "link": "http://arxiv.org/abs/2107.01579",
          "publishedOn": "2021-07-20T02:04:46.576Z",
          "wordCount": null,
          "title": "Similarity-Aware Fusion Network for 3D Semantic Segmentation. (arXiv:2107.01579v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mihai_D/0/1/0/all/0/1\">Daniela Mihai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>",
          "description": "We present a bottom-up differentiable relaxation of the process of drawing\npoints, lines and curves into a pixel raster. Our approach arises from the\nobservation that rasterising a pixel in an image given parameters of a\nprimitive can be reformulated in terms of the primitive's distance transform,\nand then relaxed to allow the primitive's parameters to be learned. This\nrelaxation allows end-to-end differentiable programs and deep networks to be\nlearned and optimised and provides several building blocks that allow control\nover how a compositional drawing process is modelled. We emphasise the\nbottom-up nature of our proposed approach, which allows for drawing operations\nto be composed in ways that can mimic the physical reality of drawing rather\nthan being tied to, for example, approaches in modern computer graphics. With\nthe proposed approach we demonstrate how sketches can be generated by directly\noptimising against photographs and how auto-encoders can be built to transform\nrasterised handwritten digits into vectors without supervision. Extensive\nexperimental results highlight the power of this approach under different\nmodelling assumptions for drawing tasks.",
          "link": "http://arxiv.org/abs/2103.16194",
          "publishedOn": "2021-07-20T02:04:46.572Z",
          "wordCount": null,
          "title": "Differentiable Drawing and Sketching. (arXiv:2103.16194v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12056",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1\">Changfa Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1\">Min Xian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiancheng Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haotian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_H/0/1/0/all/0/1\">Heng-Da Cheng</a>",
          "description": "Liver segmentation from abdominal CT images is an essential step for liver\ncancer computer-aided diagnosis and surgical planning. However, both the\naccuracy and robustness of existing liver segmentation methods cannot meet the\nrequirements of clinical applications. In particular, for the common clinical\ncases where the liver tissue contains major pathology, current segmentation\nmethods show poor performance. In this paper, we propose a novel low-rank\ntensor decomposition (LRTD) based multi-atlas segmentation (MAS) framework that\nachieves accurate and robust pathological liver segmentation of CT images.\nFirstly, we propose a multi-slice LRTD scheme to recover the underlying\nlow-rank structure embedded in 3D medical images. It performs the LRTD on small\nimage segments consisting of multiple consecutive image slices. Then, we\npresent an LRTD-based atlas construction method to generate tumor-free liver\natlases that mitigates the performance degradation of liver segmentation due to\nthe presence of tumors. Finally, we introduce an LRTD-based MAS algorithm to\nderive patient-specific liver atlases for each test image, and to achieve\naccurate pairwise image registration and label propagation. Extensive\nexperiments on three public databases of pathological liver cases validate the\neffectiveness of the proposed method. Both qualitative and quantitative results\ndemonstrate that, in the presence of major pathology, the proposed method is\nmore accurate and robust than state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2102.12056",
          "publishedOn": "2021-07-20T02:04:46.561Z",
          "wordCount": null,
          "title": "Multi-Slice Low-Rank Tensor Decomposition Based Multi-Atlas Segmentation: Application to Automatic Pathological Liver CT Segmentation. (arXiv:2102.12056v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chopin_B/0/1/0/all/0/1\">Baptiste Chopin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otberdout_N/0/1/0/all/0/1\">Naima Otberdout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daoudi_M/0/1/0/all/0/1\">Mohamed Daoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartolo_A/0/1/0/all/0/1\">Angela Bartolo</a>",
          "description": "Human motion prediction aims to forecast future human poses given a prior\npose sequence. The discontinuity of the predicted motion and the performance\ndeterioration in long-term horizons are still the main challenges encountered\nin current literature. In this work, we tackle these issues by using a compact\nmanifold-valued representation of human motion. Specifically, we model the\ntemporal evolution of the 3D human poses as trajectory, what allows us to map\nhuman motions to single points on a sphere manifold. To learn these\nnon-Euclidean representations, we build a manifold-aware Wasserstein generative\nadversarial model that captures the temporal and spatial dependencies of human\nmotion through different losses. Extensive experiments show that our approach\noutperforms the state-of-the-art on CMU MoCap and Human 3.6M datasets. Our\nqualitative results show the smoothness of the predicted motions.",
          "link": "http://arxiv.org/abs/2105.08715",
          "publishedOn": "2021-07-20T02:04:46.557Z",
          "wordCount": null,
          "title": "Human Motion Prediction Using Manifold-Aware Wasserstein GAN. (arXiv:2105.08715v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyeonwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishimura_K/0/1/0/all/0/1\">Kazuya Nishimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_K/0/1/0/all/0/1\">Kazuhide Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bise_R/0/1/0/all/0/1\">Ryoma Bise</a>",
          "description": "The domain shift problem is an important issue in automatic cell detection. A\ndetection network trained with training data under a specific condition (source\ndomain) may not work well in data under other conditions (target domain). We\npropose an unsupervised domain adaptation method for cell detection using the\npseudo-cell-position heatmap, where a cell centroid becomes a peak with a\nGaussian distribution in the map. In the prediction result for the target\ndomain, even if a peak location is correct, the signal distribution around the\npeak often has anon-Gaussian shape. The pseudo-cell-position heatmap is\nre-generated using the peak positions in the predicted heatmap to have a clear\nGaussian shape. Our method selects confident pseudo-cell-position heatmaps\nusing a Bayesian network and adds them to the training data in the next\niteration. The method can incrementally extend the domain from the source\ndomain to the target domain in a semi-supervised manner. In the experiments\nusing 8 combinations of domains, the proposed method outperformed the existing\ndomain adaptation methods.",
          "link": "http://arxiv.org/abs/2107.08653",
          "publishedOn": "2021-07-20T02:04:46.488Z",
          "wordCount": null,
          "title": "Cell Detection in Domain Shift Problem Using Pseudo-Cell-Position Heatmap. (arXiv:2107.08653v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2005.04401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bui_K/0/1/0/all/0/1\">Kevin Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1\">Fredrick Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yifei Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jack Xin</a>",
          "description": "In a class of piecewise-constant image segmentation models, we propose to\nincorporate a weighted difference of anisotropic and isotropic total variation\n(AITV) to regularize the partition boundaries in an image. In particular, we\nreplace the total variation regularization in the Chan-Vese segmentation model\nand a fuzzy region competition model by the proposed AITV. To deal with the\nnonconvex nature of AITV, we apply the difference-of-convex algorithm (DCA), in\nwhich the subproblems can be minimized by the primal-dual hybrid gradient\nmethod with linesearch. The convergence of the DCA scheme is analyzed. In\naddition, a generalization to color image segmentation is discussed. In the\nnumerical experiments, we compare the proposed models with the classic convex\napproaches and the two-stage segmentation methods (smoothing and then\nthresholding) on various images, showing that our models are effective in image\nsegmentation and robust with respect to impulsive noises.",
          "link": "http://arxiv.org/abs/2005.04401",
          "publishedOn": "2021-07-20T02:04:46.483Z",
          "wordCount": null,
          "title": "A Weighted Difference of Anisotropic and Isotropic Total Variation for Relaxed Mumford-Shah Color and Multiphase Image Segmentation. (arXiv:2005.04401v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.07335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md. Mushfiqur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abedin_T/0/1/0/all/0/1\">Thasin Abedin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prottoy_K/0/1/0/all/0/1\">Khondokar S. S. Prottoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moshruba_A/0/1/0/all/0/1\">Ayana Moshruba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_F/0/1/0/all/0/1\">Fazlul Hasan Siddiqui</a>",
          "description": "Video captioning, i.e. the task of generating captions from video sequences\ncreates a bridge between the Natural Language Processing and Computer Vision\ndomains of computer science. The task of generating a semantically accurate\ndescription of a video is quite complex. Considering the complexity, of the\nproblem, the results obtained in recent research works are praiseworthy.\nHowever, there is plenty of scope for further investigation. This paper\naddresses this scope and proposes a novel solution. Most video captioning\nmodels comprise two sequential/recurrent layers - one as a video-to-context\nencoder and the other as a context-to-caption decoder. This paper proposes a\nnovel architecture, namely Semantically Sensible Video Captioning (SSVC) which\nmodifies the context generation mechanism by using two novel approaches -\n\"stacked attention\" and \"spatial hard pull\". As there are no exclusive metrics\nfor evaluating video captioning models, we emphasize both quantitative and\nqualitative analysis of our model. Hence, we have used the BLEU scoring metric\nfor quantitative analysis and have proposed a human evaluation metric for\nqualitative analysis, namely the Semantic Sensibility (SS) scoring metric. SS\nScore overcomes the shortcomings of common automated scoring metrics. This\npaper reports that the use of the aforementioned novelties improves the\nperformance of state-of-the-art architectures.",
          "link": "http://arxiv.org/abs/2009.07335",
          "publishedOn": "2021-07-20T02:04:46.482Z",
          "wordCount": null,
          "title": "Video captioning with stacked attention and semantic hard pull. (arXiv:2009.07335v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Di Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaohui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dantcheva_A/0/1/0/all/0/1\">Antitza Dantcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garattoni_L/0/1/0/all/0/1\">Lorenzo Garattoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francesca_G/0/1/0/all/0/1\">Gianpiero Francesca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Francois Bremond</a>",
          "description": "Action recognition based on skeleton data has recently witnessed increasing\nattention and progress. State-of-the-art approaches adopting Graph\nConvolutional networks (GCNs) can effectively extract features on human\nskeletons relying on the pre-defined human topology. Despite associated\nprogress, GCN-based methods have difficulties to generalize across domains,\nespecially with different human topological structures. In this context, we\nintroduce UNIK, a novel skeleton-based action recognition method that is not\nonly effective to learn spatio-temporal features on human skeleton sequences\nbut also able to generalize across datasets. This is achieved by learning an\noptimal dependency matrix from the uniform distribution based on a multi-head\nattention mechanism. Subsequently, to study the cross-domain generalizability\nof skeleton-based action recognition in real-world videos, we re-evaluate\nstate-of-the-art approaches as well as the proposed UNIK in light of a novel\nPosetics dataset. This dataset is created from Kinetics-400 videos by\nestimating, refining and filtering poses. We provide an analysis on how much\nperformance improves on smaller benchmark datasets after pre-training on\nPosetics for the action classification task. Experimental results show that the\nproposed UNIK, with pre-training on Posetics, generalizes well and outperforms\nstate-of-the-art when transferred onto four target action classification\ndatasets: Toyota Smarthome, Penn Action, NTU-RGB+D 60 and NTU-RGB+D 120.",
          "link": "http://arxiv.org/abs/2107.08580",
          "publishedOn": "2021-07-20T02:04:46.481Z",
          "wordCount": null,
          "title": "UNIK: A Unified Framework for Real-world Skeleton-based Action Recognition. (arXiv:2107.08580v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.10934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yiyuan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuecheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weijie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yunxiang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Rong Xiong</a>",
          "description": "Place recognition is indispensable for a drift-free localization system. Due\nto the variations of the environment, place recognition using single-modality\nhas limitations. In this paper, we propose a bi-modal place recognition method,\nwhich can extract a compound global descriptor from the two modalities, vision\nand LiDAR. Specifically, we first build the elevation image generated from 3D\npoints as a structural representation. Then, we derive the correspondences\nbetween 3D points and image pixels that are further used in merging the\npixel-wise visual features into the elevation map grids. In this way, we fuse\nthe structural features and visual features in the consistent bird-eye view\nframe, yielding a semantic representation, namely CORAL. And the whole network\nis called CORAL-VLAD. Comparisons on the Oxford RobotCar show that CORAL-VLAD\nhas superior performance against other state-of-the-art methods. We also\ndemonstrate that our network can be generalized to other scenes and sensor\nconfigurations on cross-city datasets.",
          "link": "http://arxiv.org/abs/2011.10934",
          "publishedOn": "2021-07-20T02:04:46.481Z",
          "wordCount": null,
          "title": "CORAL: Colored structural representation for bi-modal place recognition. (arXiv:2011.10934v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guoping Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xingrong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xinwei He</a>",
          "description": "Medical image segmentation plays an essential role in developing\ncomputer-assisted diagnosis and therapy systems, yet still faces many\nchallenges. In the past few years, the popular encoder-decoder architectures\nbased on CNNs (e.g., U-Net) have been successfully applied in the task of\nmedical image segmentation. However, due to the locality of convolution\noperations, they demonstrate limitations in learning global context and\nlong-range spatial relations. Recently, several researchers try to introduce\ntransformers to both the encoder and decoder components with promising results,\nbut the efficiency requires further improvement due to the high computational\ncomplexity of transformers. In this paper, we propose LeViT-UNet, which\nintegrates a LeViT Transformer module into the U-Net architecture, for fast and\naccurate medical image segmentation. Specifically, we use LeViT as the encoder\nof the LeViT-UNet, which better trades off the accuracy and efficiency of the\nTransformer block. Moreover, multi-scale feature maps from transformer blocks\nand convolutional blocks of LeViT are passed into the decoder via\nskip-connection, which can effectively reuse the spatial information of the\nfeature maps. Our experiments indicate that the proposed LeViT-UNet achieves\nbetter performance comparing to various competing methods on several\nchallenging medical image segmentation benchmarks including Synapse and ACDC.\nCode and models will be publicly available at\nhttps://github.com/apple1986/LeViT_UNet.",
          "link": "http://arxiv.org/abs/2107.08623",
          "publishedOn": "2021-07-20T02:04:46.480Z",
          "wordCount": null,
          "title": "LeViT-UNet: Make Faster Encoders with Transformer for Medical Image Segmentation. (arXiv:2107.08623v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Tianyuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1\">Chang Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1\">Ruining Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuanhan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiachen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Aadarsh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Shunxing Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fogo_A/0/1/0/all/0/1\">Agnes B. Fogo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A.Landman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Catie Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haichun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>",
          "description": "Unsupervised learning algorithms (e.g., self-supervised learning,\nauto-encoder, contrastive learning) allow deep learning models to learn\neffective image representations from large-scale unlabeled data. In medical\nimage analysis, even unannotated data can be difficult to obtain for individual\nlabs. Fortunately, national-level efforts have been made to provide efficient\naccess to obtain biomedical image data from previous scientific publications.\nFor instance, NIH has launched the Open-i search engine that provides a\nlarge-scale image database with free access. However, the images in scientific\npublications consist of a considerable amount of compound figures with\nsubplots. To extract and curate individual subplots, many different compound\nfigure separation approaches have been developed, especially with the recent\nadvances in deep learning. However, previous approaches typically required\nresource extensive bounding box annotation to train detection models. In this\npaper, we propose a simple compound figure separation (SimCFS) framework that\nuses weak classification annotations from individual images. Our technical\ncontribution is three-fold: (1) we introduce a new side loss that is designed\nfor compound figure separation; (2) we introduce an intra-class image\naugmentation method to simulate hard cases; (3) the proposed framework enables\nan efficient deployment to new classes of images, without requiring resource\nextensive bounding box annotations. From the results, the SimCFS achieved a new\nstate-of-the-art performance on the ImageCLEF 2016 Compound Figure Separation\nDatabase. The source code of SimCFS is made publicly available at\nhttps://github.com/hrlblab/ImageSeperation.",
          "link": "http://arxiv.org/abs/2107.08650",
          "publishedOn": "2021-07-20T02:04:46.480Z",
          "wordCount": null,
          "title": "Compound Figure Separation of Biomedical Images with Side Loss. (arXiv:2107.08650v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ng_Y/0/1/0/all/0/1\">Yan Bin Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1\">Basura Fernando</a>",
          "description": "We present a new architecture for human action forecasting from videos. A\ntemporal recurrent encoder captures temporal information of input videos while\na self-attention model is used to attend on relevant feature dimensions of the\ninput space. To handle temporal variations in observed video data, a feature\nmasking techniques is employed. We classify observed actions accurately using\nan auxiliary classifier which helps to understand what has happened so far.\nThen the decoder generates actions for the future based on the output of the\nrecurrent encoder and the self-attention model. Experimentally, we validate\neach component of our architecture where we see that the impact of\nself-attention to identify relevant feature dimensions, temporal masking, and\nobserved auxiliary classifier. We evaluate our method on two standard action\nforecasting benchmarks and obtain state-of-the-art results.",
          "link": "http://arxiv.org/abs/2107.08579",
          "publishedOn": "2021-07-20T02:04:46.479Z",
          "wordCount": null,
          "title": "Action Forecasting with Feature-wise Self-Attention. (arXiv:2107.08579v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08470",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ho_Y/0/1/0/all/0/1\">Yung-Han Ho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_C/0/1/0/all/0/1\">Chih-Chun Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_W/0/1/0/all/0/1\">Wen-Hsiao Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hang_H/0/1/0/all/0/1\">Hsueh-Ming Hang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Domanski_M/0/1/0/all/0/1\">Marek Domanski</a>",
          "description": "This paper introduces an end-to-end learned image compression system, termed\nANFIC, based on Augmented Normalizing Flows (ANF). ANF is a new type of flow\nmodel, which stacks multiple variational autoencoders (VAE) for greater model\nexpressiveness. The VAE-based image compression has gone mainstream, showing\npromising compression performance. Our work presents the first attempt to\nleverage VAE-based compression in a flow-based framework. ANFIC advances\nfurther compression efficiency by stacking and extending hierarchically\nmultiple VAE's. The invertibility of ANF, together with our training\nstrategies, enables ANFIC to support a wide range of quality levels without\nchanging the encoding and decoding networks. Extensive experimental results\nshow that in terms of PSNR-RGB, ANFIC performs comparably to or better than the\nstate-of-the-art learned image compression. Moreover, it performs close to VVC\nintra coding, from low-rate compression up to nearly-lossless compression. In\nparticular, ANFIC achieves the state-of-the-art performance, when extended with\nconditional convolution for variable rate compression with a single model.",
          "link": "http://arxiv.org/abs/2107.08470",
          "publishedOn": "2021-07-20T02:04:46.478Z",
          "wordCount": null,
          "title": "ANFIC: Image Compression Using Augmented Normalizing Flows. (arXiv:2107.08470v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.10939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivgi_M/0/1/0/all/0/1\">Maor Ivgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benny_Y/0/1/0/all/0/1\">Yaniv Benny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_David_A/0/1/0/all/0/1\">Avichai Ben-David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "Generating images from scene graphs is a challenging task that attracted\nsubstantial interest recently. Prior works have approached this task by\ngenerating an intermediate layout description of the target image. However, the\nrepresentation of each object in the layout was generated independently, which\nresulted in high overlap, low coverage, and an overall blurry layout. We\npropose a novel method that alleviates these issues by generating the entire\nlayout description gradually to improve inter-object dependency. We empirically\nshow on the COCO-STUFF dataset that our approach improves the quality of both\nthe intermediate layout and the final image. Our approach improves the layout\ncoverage by almost 20 points and drops object overlap to negligible amounts.",
          "link": "http://arxiv.org/abs/2009.10939",
          "publishedOn": "2021-07-20T02:04:46.127Z",
          "wordCount": null,
          "title": "Scene Graph to Image Generation with Contextualized Object Layout Refinement. (arXiv:2009.10939v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05318",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Skandarani_Y/0/1/0/all/0/1\">Youssef Skandarani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lalande_A/0/1/0/all/0/1\">Alain Lalande</a>",
          "description": "Generative Adversarial Networks (GANs) have become increasingly powerful,\ngenerating mind-blowing photorealistic images that mimic the content of\ndatasets they were trained to replicate. One recurrent theme in medical imaging\nis whether GANs can also be effective at generating workable medical data as\nthey are for generating realistic RGB images. In this paper, we perform a\nmulti-GAN and multi-application study to gauge the benefits of GANs in medical\nimaging. We tested various GAN architectures from basic DCGAN to more\nsophisticated style-based GANs on three medical imaging modalities and organs\nnamely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on\nwell-known and widely utilized datasets from which their FID score were\ncomputed to measure the visual acuity of their generated images. We further\ntested their usefulness by measuring the segmentation accuracy of a U-Net\ntrained on these generated images.\n\nResults reveal that GANs are far from being equal as some are ill-suited for\nmedical imaging applications while others are much better off. The\ntop-performing GANs are capable of generating realistic-looking medical images\nby FID standards that can fool trained experts in a visual Turing test and\ncomply to some metrics. However, segmentation results suggests that no GAN is\ncapable of reproducing the full richness of a medical datasets.",
          "link": "http://arxiv.org/abs/2105.05318",
          "publishedOn": "2021-07-20T02:04:46.096Z",
          "wordCount": null,
          "title": "GANs for Medical Image Synthesis: An Empirical Study. (arXiv:2105.05318v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Transformer recently has shown encouraging progresses in computer vision. In\nthis work, we present new baselines by improving the original Pyramid Vision\nTransformer (abbreviated as PVTv1) by adding three designs, including (1)\noverlapping patch embedding, (2) convolutional feed-forward networks, and (3)\nlinear complexity attention layers.\n\nWith these modifications, our PVTv2 significantly improves PVTv1 on three\ntasks e.g., classification, detection, and segmentation. Moreover, PVTv2\nachieves comparable or better performances than recent works such as Swin\nTransformer. We hope this work will facilitate state-of-the-art Transformer\nresearches in computer vision. Code is available at\nhttps://github.com/whai362/PVT .",
          "link": "http://arxiv.org/abs/2106.13797",
          "publishedOn": "2021-07-20T02:04:46.096Z",
          "wordCount": null,
          "title": "PVTv2: Improved Baselines with Pyramid Vision Transformer. (arXiv:2106.13797v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhengping Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>",
          "description": "In deep model compression, the recent finding \"Lottery Ticket Hypothesis\"\n(LTH) (Frankle & Carbin, 2018) pointed out that there could exist a winning\nticket (i.e., a properly pruned sub-network together with original weight\ninitialization) that can achieve competitive performance than the original\ndense network. However, it is not easy to observe such winning property in many\nscenarios, where for example, a relatively large learning rate is used even if\nit benefits training the original dense model. In this work, we investigate the\nunderlying condition and rationale behind the winning property, and find that\nthe underlying reason is largely attributed to the correlation between\ninitialized weights and final-trained weights when the learning rate is not\nsufficiently large. Thus, the existence of winning property is correlated with\nan insufficient DNN pretraining, and is unlikely to occur for a well-trained\nDNN. To overcome this limitation, we propose the \"pruning & fine-tuning\" method\nthat consistently outperforms lottery ticket sparse training under the same\npruning algorithm and the same total training epochs. Extensive experiments\nover multiple deep models (VGG, ResNet, MobileNet-v2) on different datasets\nhave been conducted to justify our proposals.",
          "link": "http://arxiv.org/abs/2102.11068",
          "publishedOn": "2021-07-20T02:04:46.094Z",
          "wordCount": null,
          "title": "Lottery Ticket Preserves Weight Correlation: Is It Desirable or Not?. (arXiv:2102.11068v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.06070",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laval_J/0/1/0/all/0/1\">Jorge Laval</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_A/0/1/0/all/0/1\">Anye Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Wenchao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qing_Z/0/1/0/all/0/1\">Zhu Qing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peeta_S/0/1/0/all/0/1\">Srinivas Peeta</a>",
          "description": "Self-driving technology companies and the research community are accelerating\ntheir pace to use machine learning longitudinal motion planning (mMP) for\nautonomous vehicles (AVs). This paper reviews the current state of the art in\nmMP, with an exclusive focus on its impact on traffic congestion. We identify\nthe availability of congestion scenarios in current datasets, and summarize the\nrequired features for training mMP. For learning methods, we survey the major\nmethods in both imitation learning and non-imitation learning. We also\nhighlight the emerging technologies adopted by some leading AV companies, e.g.\nTesla, Waymo, and Comma.ai. We find that: i) the AV industry has been mostly\nfocusing on the long tail problem related to safety and overlooked the impact\non traffic congestion, ii) the current public self-driving datasets have not\nincluded enough congestion scenarios, and mostly lack the necessary input\nfeatures/output labels to train mMP, and iii) albeit reinforcement learning\n(RL) approach can integrate congestion mitigation into the learning goal, the\nmajor mMP method adopted by industry is still behavior cloning (BC), whose\ncapability to learn a congestion-mitigating mMP remains to be seen. Based on\nthe review, the study identifies the research gaps in current mMP development.\nSome suggestions towards congestion mitigation for future mMP studies are\nproposed: i) enrich data collection to facilitate the congestion learning, ii)\nincorporate non-imitation learning methods to combine traffic efficiency into a\nsafety-oriented technical route, and iii) integrate domain knowledge from the\ntraditional car following (CF) theory to improve the string stability of mMP.",
          "link": "http://arxiv.org/abs/1910.06070",
          "publishedOn": "2021-07-20T02:04:46.003Z",
          "wordCount": null,
          "title": "Review of Learning-based Longitudinal Motion Planning for Autonomous Vehicles: Research Gaps between Self-driving and Traffic Congestion. (arXiv:1910.06070v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.10784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belmonte_R/0/1/0/all/0/1\">Romain Belmonte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allaert_B/0/1/0/all/0/1\">Benjamin Allaert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tirilly_P/0/1/0/all/0/1\">Pierre Tirilly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilasco_I/0/1/0/all/0/1\">Ioan Marius Bilasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djeraba_C/0/1/0/all/0/1\">Chaabane Djeraba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>",
          "description": "Although facial landmark localization (FLL) approaches are becoming\nincreasingly accurate for characterizing facial regions, one question remains\nunanswered: what is the impact of these approaches on subsequent related tasks?\nIn this paper, the focus is put on facial expression recognition (FER), where\nfacial landmarks are used for face registration, which is a common usage. Since\nthe most used datasets for facial landmark localization do not allow for a\nproper measurement of performance according to the different difficulties\n(e.g., pose, expression, illumination, occlusion, motion blur), we also\nquantify the performance of recent approaches in the presence of head pose\nvariations and facial expressions. Finally, a study of the impact of these\napproaches on FER is conducted. We show that the landmark accuracy achieved so\nfar optimizing the conventional Euclidean distance does not necessarily\nguarantee a gain in performance for FER. To deal with this issue, we propose a\nnew evaluation metric for FLL adapted to FER.",
          "link": "http://arxiv.org/abs/1905.10784",
          "publishedOn": "2021-07-20T02:04:46.002Z",
          "wordCount": null,
          "title": "Impact of facial landmark localization on facial expression recognition. (arXiv:1905.10784v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.02463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mounir_R/0/1/0/all/0/1\">Ramy Mounir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gula_R/0/1/0/all/0/1\">Roman Gula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theuerkauf_J/0/1/0/all/0/1\">J&#xf6;rn Theuerkauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Sudeep Sarkar</a>",
          "description": "Using offline training schemes, researchers have tackled the event\nsegmentation problem by providing full or weak-supervision through manually\nannotated labels or self-supervised epoch-based training. Most works consider\nvideos that are at most 10's of minutes long. We present a self-supervised\nperceptual prediction framework capable of temporal event segmentation by\nbuilding stable representations of objects over time and demonstrate it on long\nvideos, spanning several days. The approach is deceptively simple but quite\neffective. We rely on predictions of high-level features computed by a standard\ndeep learning backbone. For prediction, we use an LSTM, augmented with an\nattention mechanism, trained in a self-supervised manner using the prediction\nerror. The self-learned attention maps effectively localize and track the\nevent-related objects in each frame. The proposed approach does not require\nlabels. It requires only a single pass through the video, with no separate\ntraining set. Given the lack of datasets of very long videos, we demonstrate\nour method on video from 10 days (254 hours) of continuous wildlife monitoring\ndata that we had collected with required permissions. We find that the approach\nis robust to various environmental conditions such as day/night conditions,\nrain, sharp shadows, and windy conditions. For the task of temporally locating\nevents, we had an 80% recall rate at 20% false-positive rate for frame-level\nsegmentation. At the activity level, we had an 80% activity recall rate for one\nfalse activity detection every 50 minutes. We will make the dataset, which is\nthe first of its kind, and the code available to the research community.",
          "link": "http://arxiv.org/abs/2005.02463",
          "publishedOn": "2021-07-20T02:04:46.001Z",
          "wordCount": null,
          "title": "Spatio-Temporal Event Segmentation and Localization for Wildlife Extended Videos. (arXiv:2005.02463v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chin_H/0/1/0/all/0/1\">Hsu-Hsun Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsay_R/0/1/0/all/0/1\">Ren-Song Tsay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hsin-I Wu</a>",
          "description": "Recent convolutional neural network (CNN) development continues to advance\nthe state-of-the-art model accuracy for various applications. However, the\nenhanced accuracy comes at the cost of substantial memory bandwidth and storage\nrequirements and demanding computational resources. Although in the past the\nquantization methods have effectively reduced the deployment cost for edge\ndevices, it suffers from significant information loss when processing the\nbiased activations of contemporary CNNs. In this paper, we hence introduce an\nadaptive high-performance quantization method to resolve the issue of biased\nactivation by dynamically adjusting the scaling and shifting factors based on\nthe task loss. Our proposed method has been extensively evaluated on image\nclassification models (ResNet-18/34/50, MobileNet-V2, EfficientNet-B0) with\nImageNet dataset, object detection model (YOLO-V4) with COCO dataset, and\nlanguage models with PTB dataset. The results show that our 4-bit integer\n(INT4) quantization models achieve better accuracy than the state-of-the-art\n4-bit models, and in some cases, even surpass the golden full-precision models.\nThe final designs have been successfully deployed onto extremely\nresource-constrained edge devices for many practical applications.",
          "link": "http://arxiv.org/abs/2107.08382",
          "publishedOn": "2021-07-20T02:04:45.991Z",
          "wordCount": null,
          "title": "A High-Performance Adaptive Quantization Approach for Edge CNN Applications. (arXiv:2107.08382v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1912.08393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinming Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Changqun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingcan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "By the aid of attention mechanisms to weight the image features adaptively,\nrecent advanced deep learning-based models encourage the predicted results to\napproximate the ground-truth masks with as large predictable areas as possible,\nthus achieving the state-of-the-art performance. However, these methods do not\npay enough attention to small areas prone to misprediction. In this way, it is\nstill tough to accurately locate salient objects due to the existence of\nregions with indistinguishable foreground and background and regions with\ncomplex or fine structures. To address these problems, we propose a novel\nconvolutional neural network with purificatory mechanism and structural\nsimilarity loss. Specifically, in order to better locate preliminary salient\nobjects, we first introduce the promotion attention, which is based on spatial\nand channel attention mechanisms to promote attention to salient regions.\nSubsequently, for the purpose of restoring the indistinguishable regions that\ncan be regarded as error-prone regions of one model, we propose the\nrectification attention, which is learned from the areas of wrong prediction\nand guide the network to focus on error-prone regions thus rectifying errors.\nThrough these two attentions, we use the Purificatory Mechanism to impose\nstrict weights with different regions of the whole salient objects and purify\nresults from hard-to-distinguish regions, thus accurately predicting the\nlocations and details of salient objects. In addition to paying different\nattention to these hard-to-distinguish regions, we also consider the structural\nconstraints on complex regions and propose the Structural Similarity Loss. In\nexperiments, the proposed approach outperforms 19 state-of-the-art methods on\nsix datasets with a notable margin at over 27FPS on a single NVIDIA 1080Ti GPU.",
          "link": "http://arxiv.org/abs/1912.08393",
          "publishedOn": "2021-07-20T02:04:45.988Z",
          "wordCount": null,
          "title": "Salient Object Detection with Purificatory Mechanism and Structural Similarity Loss. (arXiv:1912.08393v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yingchao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_W/0/1/0/all/0/1\">Wenhui Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>",
          "description": "The balance between high accuracy and high speed has always been a\nchallenging task in semantic image segmentation. Compact segmentation networks\nare more widely used in the case of limited resources, while their performances\nare constrained. In this paper, motivated by the residual learning and global\naggregation, we propose a simple yet general and effective knowledge\ndistillation framework called double similarity distillation (DSD) to improve\nthe classification accuracy of all existing compact networks by capturing the\nsimilarity knowledge in pixel and category dimensions, respectively.\nSpecifically, we propose a pixel-wise similarity distillation (PSD) module that\nutilizes residual attention maps to capture more detailed spatial dependencies\nacross multiple layers. Compared with exiting methods, the PSD module greatly\nreduces the amount of calculation and is easy to expand. Furthermore,\nconsidering the differences in characteristics between semantic segmentation\ntask and other computer vision tasks, we propose a category-wise similarity\ndistillation (CSD) module, which can help the compact segmentation network\nstrengthen the global category correlation by constructing the correlation\nmatrix. Combining these two modules, DSD framework has no extra parameters and\nonly a minimal increase in FLOPs. Extensive experiments on four challenging\ndatasets, including Cityscapes, CamVid, ADE20K, and Pascal VOC 2012, show that\nDSD outperforms current state-of-the-art methods, proving its effectiveness and\ngenerality. The code and models will be publicly available.",
          "link": "http://arxiv.org/abs/2107.08591",
          "publishedOn": "2021-07-20T02:04:45.987Z",
          "wordCount": null,
          "title": "Double Similarity Distillation for Semantic Image Segmentation. (arXiv:2107.08591v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.02443",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pomponi_J/0/1/0/all/0/1\">Jary Pomponi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scardapane_S/0/1/0/all/0/1\">Simone Scardapane</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Uncini_A/0/1/0/all/0/1\">Aurelio Uncini</a>",
          "description": "Catastrophic forgetting (CF) happens whenever a neural network overwrites\npast knowledge while being trained on new tasks. Common techniques to handle CF\ninclude regularization of the weights (using, e.g., their importance on past\ntasks), and rehearsal strategies, where the network is constantly re-trained on\npast data. Generative models have also been applied for the latter, in order to\nhave endless sources of data. In this paper, we propose a novel method that\ncombines the strengths of regularization and generative-based rehearsal\napproaches. Our generative model consists of a normalizing flow (NF), a\nprobabilistic and invertible neural network, trained on the internal embeddings\nof the network. By keeping a single NF conditioned on the task, we show that\nour memory overhead remains constant. In addition, exploiting the invertibility\nof the NF, we propose a simple approach to regularize the network's embeddings\nwith respect to past tasks. We show that our method performs favorably with\nrespect to state-of-the-art approaches in the literature, with bounded\ncomputational power and memory overheads.",
          "link": "http://arxiv.org/abs/2007.02443",
          "publishedOn": "2021-07-20T02:04:45.985Z",
          "wordCount": null,
          "title": "Pseudo-Rehearsal for Continual Learning with Normalizing Flows. (arXiv:2007.02443v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1\">Wei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jie Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>",
          "description": "Few-shot class-incremental learning is to recognize the new classes given few\nsamples and not forget the old classes. It is a challenging task since\nrepresentation optimization and prototype reorganization can only be achieved\nunder little supervision. To address this problem, we propose a novel\nincremental prototype learning scheme. Our scheme consists of a random episode\nselection strategy that adapts the feature representation to various generated\nincremental episodes to enhance the corresponding extensibility, and a\nself-promoted prototype refinement mechanism which strengthens the expression\nability of the new classes by explicitly considering the dependencies among\ndifferent classes. Particularly, a dynamic relation projection module is\nproposed to calculate the relation matrix in a shared embedding space and\nleverage it as the factor for bootstrapping the update of prototypes. Extensive\nexperiments on three benchmark datasets demonstrate the above-par incremental\nperformance, outperforming state-of-the-art methods by a margin of 13%, 17% and\n11%, respectively.",
          "link": "http://arxiv.org/abs/2107.08918",
          "publishedOn": "2021-07-20T02:04:45.984Z",
          "wordCount": null,
          "title": "Self-Promoted Prototype Refinement for Few-Shot Class-Incremental Learning. (arXiv:2107.08918v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sayak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>",
          "description": "Floods wreak havoc throughout the world, causing billions of dollars in\ndamages, and uprooting communities, ecosystems and economies. Accurate and\nrobust flood detection including delineating open water flood areas and\nidentifying flood levels can aid in disaster response and mitigation. However,\nestimating flood levels remotely is of essence as physical access to flooded\nareas is limited and the ability to deploy instruments in potential flood zones\ncan be dangerous. Aligning flood extent mapping with local topography can\nprovide a plan-of-action that the disaster response team can consider. Thus,\nremote flood level estimation via satellites like Sentinel-1 can prove to be\nremedial. The Emerging Techniques in Computational Intelligence (ETCI)\ncompetition on Flood Detection tasked participants with predicting flooded\npixels after training with synthetic aperture radar (SAR) images in a\nsupervised setting. We use a cyclical approach involving two stages (1)\ntraining an ensemble model of multiple UNet architectures with available high\nand low confidence labeled data and, (2) generating pseudo labels or low\nconfidence labels on the unlabeled test dataset, and then, combining the\ngenerated labels with the previously available high confidence labeled dataset.\nThis assimilated dataset is used for the next round of training ensemble\nmodels. This cyclical process is repeated until the performance improvement\nplateaus. Additionally, we post process our results with Conditional Random\nFields. Our approach sets a high score on the public leaderboard for the ETCI\ncompetition with 0.7654 IoU. Our method, which we release with all the code\nincluding trained models, can also be used as an open science benchmark for the\nSentinel-1 released dataset on GitHub.",
          "link": "http://arxiv.org/abs/2107.08369",
          "publishedOn": "2021-07-20T02:04:45.661Z",
          "wordCount": null,
          "title": "Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08543",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saparov_T/0/1/0/all/0/1\">Talgat Saparov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurmukov_A/0/1/0/all/0/1\">Anvar Kurmukov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shirokih_B/0/1/0/all/0/1\">Boris Shirokih</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Belyaev_M/0/1/0/all/0/1\">Mikhail Belyaev</a>",
          "description": "Domain shift is one of the most salient challenges in medical computer\nvision. Due to immense variability in scanners' parameters and imaging\nprotocols, even images obtained from the same person and the same scanner could\ndiffer significantly. We address variability in computed tomography (CT) images\ncaused by different convolution kernels used in the reconstruction process, the\ncritical domain shift factor in CT. The choice of a convolution kernel affects\npixels' granularity, image smoothness, and noise level. We analyze a dataset of\npaired CT images, where smooth and sharp images were reconstructed from the\nsame sinograms with different kernels, thus providing identical anatomy but\ndifferent style. Though identical predictions are desired, we show that the\nconsistency, measured as the average Dice between predictions on pairs, is just\n0.54. We propose Filtered Back-Projection Augmentation (FBPAug), a simple and\nsurprisingly efficient approach to augment CT images in sinogram space\nemulating reconstruction with different kernels. We apply the proposed method\nin a zero-shot domain adaptation setup and show that the consistency boosts\nfrom 0.54 to 0.92 outperforming other augmentation approaches. Neither specific\npreparation of source domain data nor target domain data is required, so our\npublicly released FBPAug can be used as a plug-and-play module for zero-shot\ndomain adaptation in any CT-based task.",
          "link": "http://arxiv.org/abs/2107.08543",
          "publishedOn": "2021-07-20T02:04:45.229Z",
          "wordCount": 664,
          "title": "Zero-Shot Domain Adaptation in CT Segmentation by Filtered Back Projection Augmentation. (arXiv:2107.08543v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shutai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yinhao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunhua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>",
          "description": "Small target detection is known to be a challenging problem. Inspired by the\nstructural characteristics and physiological mechanism of eagle-eye, a\nminiature vision system is designed for small target detection in this paper.\nFirst, a hardware platform is established, which consists of a pan-tilt, a\nshort-focus camera and a long-focus camera. Then, based on the visual attention\nmechanism of eagle-eye, the cameras with different focal lengths are controlled\ncooperatively to achieve small target detection. Experimental results show that\nthe designed biological eagle-eye vision system can accurately detect small\ntargets, which has a strong adaptive ability.",
          "link": "http://arxiv.org/abs/2107.08406",
          "publishedOn": "2021-07-20T02:04:45.201Z",
          "wordCount": 552,
          "title": "A Miniature Biological Eagle-Eye Vision System for Small Target Detection. (arXiv:2107.08406v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08471",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dengshan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chengjun Xie</a>",
          "description": "Artificial neural networks that simulate human achieves great successes. From\nthe perspective of simulating human memory method, we propose a stepped sampler\nbased on the \"repeated input\". We repeatedly inputted data to the LSTM model\nstepwise in a batch. The stepped sampler is used to strengthen the ability of\nfusing the temporal information in LSTM. We tested the stepped sampler on the\nLSTM built-in in PyTorch. Compared with the traditional sampler of PyTorch,\nsuch as sequential sampler, batch sampler, the training loss of the proposed\nstepped sampler converges faster in the training of the model, and the training\nloss after convergence is more stable. Meanwhile, it can maintain a higher test\naccuracy. We quantified the algorithm of the stepped sampler.",
          "link": "http://arxiv.org/abs/2107.08471",
          "publishedOn": "2021-07-20T02:04:45.170Z",
          "wordCount": 556,
          "title": "A stepped sampling method for video detection using LSTM. (arXiv:2107.08471v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Dongze Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenghua Gao</a>",
          "description": "An Axial Shifted MLP architecture (AS-MLP) is proposed in this paper.\nDifferent from MLP-Mixer, where the global spatial feature is encoded for the\ninformation flow through matrix transposition and one token-mixing MLP, we pay\nmore attention to the local features communication. By axially shifting\nchannels of the feature map, AS-MLP is able to obtain the information flow from\ndifferent axial directions, which captures the local dependencies. Such an\noperation enables us to utilize a pure MLP architecture to achieve the same\nlocal receptive field as CNN-like architecture. We can also design the\nreceptive field size and dilation of blocks of AS-MLP, etc, just like designing\nthose of convolution kernels. With the proposed AS-MLP architecture, our model\nobtains 83.3% Top-1 accuracy with 88M parameters and 15.2 GFLOPs on the\nImageNet-1K dataset. Such a simple yet effective architecture outperforms all\nMLP-based architectures and achieves competitive performance compared to the\ntransformer-based architectures (e.g., Swin Transformer) even with slightly\nlower FLOPs. In addition, AS-MLP is also the first MLP-based architecture to be\napplied to the downstream tasks (e.g., object detection and semantic\nsegmentation). The experimental results are also impressive. Our proposed\nAS-MLP obtains 51.5 mAP on the COCO validation set and 49.5 MS mIoU on the\nADE20K dataset, which is competitive compared to the transformer-based\narchitectures. Code is available at https://github.com/svip-lab/AS-MLP.",
          "link": "http://arxiv.org/abs/2107.08391",
          "publishedOn": "2021-07-20T02:04:45.154Z",
          "wordCount": 658,
          "title": "AS-MLP: An Axial Shifted MLP Architecture for Vision. (arXiv:2107.08391v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_I/0/1/0/all/0/1\">Iker Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skalski_P/0/1/0/all/0/1\">Piotr Skalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barns_Graham_A/0/1/0/all/0/1\">Alec Barns-Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Jason Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_D/0/1/0/all/0/1\">David Sutton</a>",
          "description": "Enabling interpretations of model uncertainties is of key importance in\nBayesian machine learning applications. Often, this requires to meaningfully\nattribute predictive uncertainties to source features in an image, text or\ncategorical array. However, popular attribution methods are particularly\ndesigned for classification and regression scores. In order to explain\nuncertainties, state of the art alternatives commonly procure counterfactual\nfeature vectors, and proceed by making direct comparisons. In this paper, we\nleverage path integrals to attribute uncertainties in Bayesian differentiable\nmodels. We present a novel algorithm that relies on in-distribution curves\nconnecting a feature vector to some counterfactual counterpart, and we retain\ndesirable properties of interpretability methods. We validate our approach on\nbenchmark image data sets with varying resolution, and show that it\nsignificantly simplifies interpretability over the existing alternatives.",
          "link": "http://arxiv.org/abs/2107.08756",
          "publishedOn": "2021-07-20T02:04:42.595Z",
          "wordCount": 572,
          "title": "Path Integrals for the Attribution of Model Uncertainties. (arXiv:2107.08756v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiahuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yansong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1\">Bing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Wu</a>",
          "description": "Existing popular unsupervised embedding learning methods focus on enhancing\nthe instance-level local discrimination of the given unlabeled images by\nexploring various negative data. However, the existed sample outliers which\nexhibit large intra-class divergences or small inter-class variations severely\nlimit their learning performance. We justify that the performance limitation is\ncaused by the gradient vanishing on these sample outliers. Moreover, the\nshortage of positive data and disregard for global discrimination consideration\nalso pose critical issues for unsupervised learning but are always ignored by\nexisting methods. To handle these issues, we propose a novel solution to\nexplicitly model and directly explore the uncertainty of the given unlabeled\nlearning samples. Instead of learning a deterministic feature point for each\nsample in the embedding space, we propose to represent a sample by a stochastic\nGaussian with the mean vector depicting its space localization and covariance\nvector representing the sample uncertainty. We leverage such uncertainty\nmodeling as momentum to the learning which is helpful to tackle the outliers.\nFurthermore, abundant positive candidates can be readily drawn from the learned\ninstance-specific distributions which are further adopted to mitigate the\naforementioned issues. Thorough rationale analyses and extensive experiments\nare presented to verify our superiority.",
          "link": "http://arxiv.org/abs/2107.08892",
          "publishedOn": "2021-07-20T02:04:42.575Z",
          "wordCount": 635,
          "title": "Unsupervised Embedding Learning from Uncertainty Momentum Modeling. (arXiv:2107.08892v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08751",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Memmel_M/0/1/0/all/0/1\">Marius Memmel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonzalez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>",
          "description": "Deep learning for medical imaging suffers from temporal and privacy-related\nrestrictions on data availability. To still obtain viable models, continual\nlearning aims to train in sequential order, as and when data is available. The\nmain challenge that continual learning methods face is to prevent catastrophic\nforgetting, i.e., a decrease in performance on the data encountered earlier.\nThis issue makes continuous training of segmentation models for medical\napplications extremely difficult. Yet, often, data from at least two different\ndomains is available which we can exploit to train the model in a way that it\ndisregards domain-specific information. We propose an architecture that\nleverages the simultaneous availability of two or more datasets to learn a\ndisentanglement between the content and domain in an adversarial fashion. The\ndomain-invariant content representation then lays the base for continual\nsemantic segmentation. Our approach takes inspiration from domain adaptation\nand combines it with continual learning for hippocampal segmentation in brain\nMRI. We showcase that our method reduces catastrophic forgetting and\noutperforms state-of-the-art continual learning methods.",
          "link": "http://arxiv.org/abs/2107.08751",
          "publishedOn": "2021-07-20T02:04:42.557Z",
          "wordCount": 612,
          "title": "Adversarial Continual Learning for Multi-Domain Hippocampal Segmentation. (arXiv:2107.08751v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08850",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ganz_J/0/1/0/all/0/1\">Jonathan Ganz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kirsch_T/0/1/0/all/0/1\">Tobias Kirsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_L/0/1/0/all/0/1\">Lucas Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bertram_C/0/1/0/all/0/1\">Christof A. Bertram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_C/0/1/0/all/0/1\">Christoph Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blumcke_I/0/1/0/all/0/1\">Ingmar Bl&#xfc;mcke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jabari_S/0/1/0/all/0/1\">Samir Jabari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aubreville_M/0/1/0/all/0/1\">Marc Aubreville</a>",
          "description": "Meningioma is one of the most prevalent brain tumors in adults. To determine\nits malignancy, it is graded by a pathologist into three grades according to\nWHO standards. This grade plays a decisive role in treatment, and yet may be\nsubject to inter-rater discordance. In this work, we present and compare three\napproaches towards fully automatic meningioma grading from histology whole\nslide images. All approaches are following a two-stage paradigm, where we first\nidentify a region of interest based on the detection of mitotic figures in the\nslide using a state-of-the-art object detection deep learning network. This\nregion of highest mitotic rate is considered characteristic for biological\ntumor behavior. In the second stage, we calculate a score corresponding to\ntumor malignancy based on information contained in this region using three\ndifferent settings. In a first approach, image patches are sampled from this\nregion and regression is based on morphological features encoded by a\nResNet-based network. We compare this to learning a logistic regression from\nthe determined mitotic count, an approach which is easily traceable and\nexplainable. Lastly, we combine both approaches in a single network. We trained\nthe pipeline on 951 slides from 341 patients and evaluated them on a separate\nset of 141 slides from 43 patients. All approaches yield a high correlation to\nthe WHO grade. The logistic regression and the combined approach had the best\nresults in our experiments, yielding correct predictions in 32 and 33 of all\ncases, respectively, with the image-based approach only predicting 25 cases\ncorrectly. Spearman's correlation was 0.716, 0.792 and 0.790 respectively. It\nmay seem counterintuitive at first that morphological features provided by\nimage patches do not improve model performance. Yet, this mirrors the criteria\nof the grading scheme, where mitotic count is the only unequivocal parameter.",
          "link": "http://arxiv.org/abs/2107.08850",
          "publishedOn": "2021-07-20T02:04:42.480Z",
          "wordCount": 767,
          "title": "Automatic and explainable grading of meningiomas from histopathology images. (arXiv:2107.08850v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08767",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nam_W/0/1/0/all/0/1\">Woo-Jeoung Nam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "As interpretability has been pointed out as the obstacle to the adoption of\nDeep Neural Networks (DNNs), there is an increasing interest in solving a\ntransparency issue to guarantee the impressive performance. In this paper, we\ndemonstrate the efficiency of recent attribution techniques to explain the\ndiagnostic decision by visualizing the significant factors in the input image.\nBy utilizing the characteristics of objectness that DNNs have learned, fully\ndecomposing the network prediction visualizes clear localization of target\nlesion. To verify our work, we conduct our experiments on Chest X-ray diagnosis\nwith publicly accessible datasets. As an intuitive assessment metric for\nexplanations, we report the performance of intersection of Union between visual\nexplanation and bounding box of lesions. Experiment results show that recently\nproposed attribution methods visualize the more accurate localization for the\ndiagnostic decision compared to the traditionally used CAM. Furthermore, we\nanalyze the inconsistency of intentions between humans and DNNs, which is\neasily obscured by high performance. By visualizing the relevant factors, it is\npossible to confirm that the criterion for decision is in line with the\nlearning strategy. Our analysis of unmasking machine intelligence represents\nthe necessity of explainability in the medical diagnostic decision.",
          "link": "http://arxiv.org/abs/2107.08767",
          "publishedOn": "2021-07-20T02:04:42.431Z",
          "wordCount": 654,
          "title": "Improving Interpretability of Deep Neural Networks in Medical Diagnosis by Investigating the Individual Units. (arXiv:2107.08767v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1\">Myeong-Seok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong-Ju Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Aerial image registration or matching is a geometric process of aligning two\naerial images captured in different environments. Estimating the precise\ntransformation parameters is hindered by various environments such as time,\nweather, and viewpoints. The characteristics of the aerial images are mainly\ncomposed of a straight line owing to building and road. Therefore, the straight\nlines are distorted when estimating homography parameters directly between two\nimages. In this paper, we propose a deep homography alignment network to\nprecisely match two aerial images by progressively estimating the various\ntransformation parameters. The proposed network is possible to train the\nmatching network with a higher degree of freedom by progressively analyzing the\ntransformation parameters. The precision matching performances have been\nincreased by applying homography transformation. In addition, we introduce a\nmethod that can effectively learn the difficult-to-learn homography estimation\nnetwork. Since there is no published learning data for aerial image\nregistration, in this paper, a pair of images to which random homography\ntransformation is applied within a certain range is used for learning. Hence,\nwe could confirm that the deep homography alignment network shows high\nprecision matching performance compared with conventional works.",
          "link": "http://arxiv.org/abs/2107.08768",
          "publishedOn": "2021-07-20T02:04:42.402Z",
          "wordCount": 624,
          "title": "Precise Aerial Image Matching based on Deep Homography Estimation. (arXiv:2107.08768v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenkai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jizheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Man Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yuanzhu Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_X/0/1/0/all/0/1\">Xinchao Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Muqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jing Song</a>",
          "description": "The 3D visual perception for vehicles with the surround-view fisheye camera\nsystem is a critical and challenging task for low-cost urban autonomous\ndriving. While existing monocular 3D object detection methods perform not well\nenough on the fisheye images for mass production, partly due to the lack of 3D\ndatasets of such images. In this paper, we manage to overcome and avoid the\ndifficulty of acquiring the large scale of accurate 3D labeled truth data, by\nbreaking down the 3D object detection task into some sub-tasks, such as\nvehicle's contact point detection, type classification, re-identification and\nunit assembling, etc. Particularly, we propose the concept of Multidimensional\nVector to include the utilizable information generated in different dimensions\nand stages, instead of the descriptive approach for the bird's eye view (BEV)\nor a cube of eight points. The experiments of real fisheye images demonstrate\nthat our solution achieves state-of-the-art accuracy while being real-time in\npractice.",
          "link": "http://arxiv.org/abs/2107.08862",
          "publishedOn": "2021-07-20T02:04:42.276Z",
          "wordCount": 617,
          "title": "Disentangling and Vectorization: A 3D Visual Perception Approach for Autonomous Driving Based on Surround-View Fisheye Cameras. (arXiv:2107.08862v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08673",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Massalimova_A/0/1/0/all/0/1\">Aidana Massalimova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varol_H/0/1/0/all/0/1\">Huseyin Atakan Varol</a>",
          "description": "Alzheimer's disease (AD) is a progressive brain disorder that causes memory\nand functional impairments. The advances in machine learning and publicly\navailable medical datasets initiated multiple studies in AD diagnosis. In this\nwork, we utilize a multi-modal deep learning approach in classifying normal\ncognition, mild cognitive impairment and AD classes on the basis of structural\nMRI and diffusion tensor imaging (DTI) scans from the OASIS-3 dataset. In\naddition to a conventional multi-modal network, we also present an input\nagnostic architecture that allows diagnosis with either sMRI or DTI scan, which\ndistinguishes our method from previous multi-modal machine learning-based\nmethods. The results show that the input agnostic model achieves 0.96 accuracy\nwhen both structural MRI and DTI scans are provided as inputs.",
          "link": "http://arxiv.org/abs/2107.08673",
          "publishedOn": "2021-07-20T02:04:42.239Z",
          "wordCount": 581,
          "title": "Input Agnostic Deep Learning for Alzheimer's Disease Classification Using Multimodal MRI Images. (arXiv:2107.08673v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Pengfei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>",
          "description": "By considering the spatial correspondence, dense self-supervised\nrepresentation learning has achieved superior performance on various dense\nprediction tasks. However, the pixel-level correspondence tends to be noisy\nbecause of many similar misleading pixels, e.g., backgrounds. To address this\nissue, in this paper, we propose to explore \\textbf{set} \\textbf{sim}ilarity\n(SetSim) for dense self-supervised representation learning. We generalize\npixel-wise similarity learning to set-wise one to improve the robustness\nbecause sets contain more semantic and structure information. Specifically, by\nresorting to attentional features of views, we establish corresponding sets,\nthus filtering out noisy backgrounds that may cause incorrect correspondences.\nMeanwhile, these attentional features can keep the coherence of the same image\nacross different views to alleviate semantic inconsistency. We further search\nthe cross-view nearest neighbours of sets and employ the structured\nneighbourhood information to enhance the robustness. Empirical evaluations\ndemonstrate that SetSim is superior to state-of-the-art methods on object\ndetection, keypoint detection, instance segmentation, and semantic\nsegmentation.",
          "link": "http://arxiv.org/abs/2107.08712",
          "publishedOn": "2021-07-20T02:04:42.135Z",
          "wordCount": 603,
          "title": "Exploring Set Similarity for Dense Self-supervised Representation Learning. (arXiv:2107.08712v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shilei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xianli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_B/0/1/0/all/0/1\">Buyue Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Universal lesion detection in computed tomography (CT) images is an important\nyet challenging task due to the large variations in lesion type, size, shape,\nand appearance. Considering that data in clinical routine (such as the\nDeepLesion dataset) are usually annotated with a long and a short diameter\naccording to the standard of Response Evaluation Criteria in Solid Tumors\n(RECIST) diameters, we propose RECIST-Net, a new approach to lesion detection\nin which the four extreme points and center point of the RECIST diameters are\ndetected. By detecting a lesion as keypoints, we provide a more conceptually\nstraightforward formulation for detection, and overcome several drawbacks\n(e.g., requiring extensive effort in designing data-appropriate anchors and\nlosing shape information) of existing bounding-box-based methods while\nexploring a single-task, one-stage approach compared to other RECIST-based\napproaches. Experiments show that RECIST-Net achieves a sensitivity of 92.49%\nat four false positives per image, outperforming other recent methods including\nthose using multi-task learning.",
          "link": "http://arxiv.org/abs/2107.08715",
          "publishedOn": "2021-07-20T02:04:42.115Z",
          "wordCount": 609,
          "title": "RECIST-Net: Lesion detection via grouping keypoints on RECIST-based annotation. (arXiv:2107.08715v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young J. Kim</a>",
          "description": "We propose a 3D face generative model with local weights to increase the\nmodel's variations and expressiveness. The proposed model allows partial\nmanipulation of the face while still learning the whole face mesh. For this\npurpose, we address an effective way to extract local facial features from the\nentire data and explore a way to manipulate them during a holistic generation.\nFirst, we factorize the latent space of the whole face to the subspace\nindicating different parts of the face. In addition, local weights generated by\nnon-negative matrix factorization are applied to the factorized latent space so\nthat the decomposed part space is semantically meaningful. We experiment with\nour model and observe that effective facial part manipulation is possible and\nthat the model's expressiveness is improved.",
          "link": "http://arxiv.org/abs/2107.08737",
          "publishedOn": "2021-07-20T02:04:42.096Z",
          "wordCount": 584,
          "title": "Synthesizing Human Faces using Latent Space Factorization and Local Weights (Extended Version). (arXiv:2107.08737v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenpeng Chen</a>",
          "description": "Among 2D convolutional networks on point clouds, point-based approaches\nconsume point clouds of fixed size directly. By analysis of PointNet, a pioneer\nin introducing deep learning into point sets, we reveal that current\npoint-based methods are essentially spatial relationship processing networks.\nIn this paper, we take a different approach. Our architecture, named PE-Net,\nlearns the representation of point clouds in high-dimensional space, and\nencodes the unordered input points to feature vectors, which standard 2D CNNs\ncan be applied to. The recommended network can adapt to changes in the number\nof input points which is the limit of current methods. Experiments show that in\nthe tasks of classification and part segmentation, PE-Net achieves the\nstate-of-the-art performance in multiple challenging datasets, such as ModelNet\nand ShapeNetPart.",
          "link": "http://arxiv.org/abs/2107.08565",
          "publishedOn": "2021-07-20T02:04:41.858Z",
          "wordCount": 550,
          "title": "Learning point embedding for 3D data processing. (arXiv:2107.08565v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songtao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>",
          "description": "In this report, we present some experienced improvements to YOLO series,\nforming a new high-performance detector -- YOLOX. We switch the YOLO detector\nto an anchor-free manner and conduct other advanced detection techniques, i.e.,\na decoupled head and the leading label assignment strategy SimOTA to achieve\nstate-of-the-art results across a large scale range of models: For YOLO-Nano\nwith only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing\nNanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in\nindustry, we boost it to 47.3% AP on COCO, outperforming the current best\npractice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as\nYOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on\nTesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on\nStreaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021)\nusing a single YOLOX-L model. We hope this report can provide useful experience\nfor developers and researchers in practical scenes, and we also provide deploy\nversions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at\nhttps://github.com/Megvii-BaseDetection/YOLOX.",
          "link": "http://arxiv.org/abs/2107.08430",
          "publishedOn": "2021-07-20T02:04:41.791Z",
          "wordCount": 625,
          "title": "YOLOX: Exceeding YOLO Series in 2021. (arXiv:2107.08430v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>",
          "description": "We propose an approach to instance segmentation from 3D point clouds based on\ndynamic convolution. This enables it to adapt, at inference, to varying feature\nand object scales. Doing so avoids some pitfalls of bottom up approaches,\nincluding a dependence on hyper-parameter tuning and heuristic post-processing\npipelines to compensate for the inevitable variability in object sizes, even\nwithin a single scene. The representation capability of the network is greatly\nimproved by gathering homogeneous points that have identical semantic\ncategories and close votes for the geometric centroids. Instances are then\ndecoded via several simple convolution layers, where the parameters are\ngenerated conditioned on the input. The proposed approach is proposal-free, and\ninstead exploits a convolution process that adapts to the spatial and semantic\ncharacteristics of each instance. A light-weight transformer, built on the\nbottleneck layer, allows the model to capture long-range dependencies, with\nlimited computational overhead. The result is a simple, efficient, and robust\napproach that yields strong performance on various datasets: ScanNetV2, S3DIS,\nand PartNet. The consistent improvements on both voxel- and point-based\narchitectures imply the effectiveness of the proposed method. Code is available\nat: https://git.io/DyCo3D",
          "link": "http://arxiv.org/abs/2107.08392",
          "publishedOn": "2021-07-20T02:04:41.770Z",
          "wordCount": 628,
          "title": "Dynamic Convolution for 3D Point Cloud Instance Segmentation. (arXiv:2107.08392v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yongxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiaolin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuncong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lu Li</a>",
          "description": "As one of the prevalent components, Feature Pyramid Network (FPN) is widely\nused in the current object detection models to improve the performance of\nmulti-scale detection. However, its interaction is still in a local and lossy\nmanner, thus limiting the representation power. In this paper, to simulate a\nglobal view of human vision in object detection and address the inherent\ndefects of interaction mode in FPN, we construct a novel architecture termed\nContent-Augmented Feature Pyramid Network (CA-FPN). Unlike the vanilla FPN,\nwhich fuses features within a local receptive field, CA-FPN can adaptively\naggregate similar features from a global view. It is equipped with a global\ncontent extraction module and light linear spatial transformers. The former\nallows to extract multi-scale context information and the latter can deeply\ncombine the global content extraction module with the vanilla FPN using the\nlinearized attention function, which is designed to reduce model complexity.\nFurthermore, CA-FPN can be readily plugged into existing FPN-based models.\nExtensive experiments on the challenging COCO and PASCAL VOC object detection\ndatasets demonstrated that our CA-FPN significantly outperforms competitive\nFPN-based detectors without bells and whistles. When plugging CA-FPN into\nCascade R-CNN framework built upon a standard ResNet-50 backbone, our method\ncan achieve 44.8 AP on COCO mini-val. Its performance surpasses the previous\nstate-of-the-art by 1.5 AP, demonstrating the potentiality of application.",
          "link": "http://arxiv.org/abs/2105.09464",
          "publishedOn": "2021-07-20T02:04:41.740Z",
          "wordCount": 703,
          "title": "Content-Augmented Feature Pyramid Network with Light Linear Spatial Transformers for Object Detection. (arXiv:2105.09464v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiaxiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaokang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Gang Zeng</a>",
          "description": "Guided depth super-resolution is a practical task where a low-resolution and\nnoisy input depth map is restored to a high-resolution version, with the help\nof a high-resolution RGB guide image. Existing methods usually view this task\nas a generalized guided filtering problem that relies on designing explicit\nfilters and objective functions, or a dense regression problem that directly\npredicts the target image via deep neural networks. These methods suffer from\neither model capability or interpretability. Inspired by the recent progress in\nimplicit neural representation, we propose to formulate the guided\nsuper-resolution as a neural implicit image interpolation problem, where we\ntake the form of a general image interpolation but use a novel Joint Implicit\nImage Function (JIIF) representation to learn both the interpolation weights\nand values. JIIF represents the target image domain with spatially distributed\nlocal latent codes extracted from the input image and the guide image, and uses\na graph attention mechanism to learn the interpolation weights at the same time\nin one unified deep implicit function. We demonstrate the effectiveness of our\nJIIF representation on guided depth super-resolution task, significantly\noutperforming state-of-the-art methods on three public benchmarks. Code can be\nfound at \\url{https://git.io/JC2sU}.",
          "link": "http://arxiv.org/abs/2107.08717",
          "publishedOn": "2021-07-20T02:04:41.695Z",
          "wordCount": 639,
          "title": "Joint Implicit Image Function for Guided Depth Super-Resolution. (arXiv:2107.08717v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1\">Prashant Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_M/0/1/0/all/0/1\">Mrigank Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varambally_S/0/1/0/all/0/1\">Sumanth Varambally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1\">Prathosh AP</a>",
          "description": "Generalization of machine learning models trained on a set of source domains\non unseen target domains with different statistics, is a challenging problem.\nWhile many approaches have been proposed to solve this problem, they only\nutilize source data during training but do not take advantage of the fact that\na single target example is available at the time of inference. Motivated by\nthis, we propose a method that effectively uses the target sample during\ninference beyond mere classification. Our method has three components - (i) A\nlabel-preserving feature or metric transformation on source data such that the\nsource samples are clustered in accordance with their class irrespective of\ntheir domain (ii) A generative model trained on the these features (iii) A\nlabel-preserving projection of the target point on the source-feature manifold\nduring inference via solving an optimization problem on the input space of the\ngenerative model using the learned metric. Finally, the projected target is\nused in the classifier. Since the projected target feature comes from the\nsource manifold and has the same label as the real target by design, the\nclassifier is expected to perform better on it than the true target. We\ndemonstrate that our method outperforms the state-of-the-art Domain\nGeneralization methods on multiple datasets and tasks.",
          "link": "http://arxiv.org/abs/2103.01134",
          "publishedOn": "2021-07-20T02:04:41.676Z",
          "wordCount": 678,
          "title": "Domain Generalization via Inference-time Label-Preserving Target Projections. (arXiv:2103.01134v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08111",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Roth_H/0/1/0/all/0/1\">Holger R. Roth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wenqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Myronenko_A/0/1/0/all/0/1\">Andriy Myronenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyue Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>",
          "description": "Building robust deep learning-based models requires diverse training data,\nideally from several sources. However, these datasets cannot be combined easily\nbecause of patient privacy concerns or regulatory hurdles, especially if\nmedical data is involved. Federated learning (FL) is a way to train machine\nlearning models without the need for centralized datasets. Each FL client\ntrains on their local data while only sharing model parameters with a global\nserver that aggregates the parameters from all clients. At the same time, each\nclient's data can exhibit differences and inconsistencies due to the local\nvariation in the patient population, imaging equipment, and acquisition\nprotocols. Hence, the federated learned models should be able to adapt to the\nlocal particularities of a client's data. In this work, we combine FL with an\nAutoML technique based on local neural architecture search by training a\n\"supernet\". Furthermore, we propose an adaptation scheme to allow for\npersonalized model architectures at each FL client's site. The proposed method\nis evaluated on four different datasets from 3D prostate MRI and shown to\nimprove the local models' performance after adaptation through selecting an\noptimal path through the AutoML supernet.",
          "link": "http://arxiv.org/abs/2107.08111",
          "publishedOn": "2021-07-20T02:04:41.619Z",
          "wordCount": 650,
          "title": "Federated Whole Prostate Segmentation in MRI with Personalized Neural Architectures. (arXiv:2107.08111v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ashesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1\">Luca Del Pero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1\">Hugo Grimmett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1\">Peter Ondruska</a>",
          "description": "Despite the numerous successes of machine learning over the past decade\n(image recognition, decision-making, NLP, image synthesis), self-driving\ntechnology has not yet followed the same trend. In this paper, we study the\nhistory, composition, and development bottlenecks of the modern self-driving\nstack. We argue that the slow progress is caused by approaches that require too\nmuch hand-engineering, an over-reliance on road testing, and high fleet\ndeployment costs. We observe that the classical stack has several bottlenecks\nthat preclude the necessary scale needed to capture the long tail of rare\nevents. To resolve these problems, we outline the principles of Autonomy 2.0,\nan ML-first approach to self-driving, as a viable alternative to the currently\nadopted state-of-the-art. This approach is based on (i) a fully differentiable\nAV stack trainable from human demonstrations, (ii) closed-loop data-driven\nreactive simulation, and (iii) large-scale, low-cost data collections as\ncritical solutions towards scalability issues. We outline the general\narchitecture, survey promising works in this direction and propose key\nchallenges to be addressed by the community in the future.",
          "link": "http://arxiv.org/abs/2107.08142",
          "publishedOn": "2021-07-20T02:04:41.599Z",
          "wordCount": 616,
          "title": "Autonomy 2.0: Why is self-driving always 5 years away?. (arXiv:2107.08142v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08330",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Konwer_A/0/1/0/all/0/1\">Aishik Konwer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_J/0/1/0/all/0/1\">Joseph Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_G/0/1/0/all/0/1\">Gagandeep Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gattu_R/0/1/0/all/0/1\">Rishabh Gattu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Syed Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Green_J/0/1/0/all/0/1\">Jeremy Green</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phatak_T/0/1/0/all/0/1\">Tej Phatak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>",
          "description": "COVID-19 image analysis has mostly focused on diagnostic tasks using single\ntimepoint scans acquired upon disease presentation or admission. We present a\ndeep learning-based approach to predict lung infiltrate progression from serial\nchest radiographs (CXRs) of COVID-19 patients. Our method first utilizes\nconvolutional neural networks (CNNs) for feature extraction from patches within\nthe concerned lung zone, and also from neighboring and remote boundary regions.\nThe framework further incorporates a multi-scale Gated Recurrent Unit (GRU)\nwith a correlation module for effective predictions. The GRU accepts CNN\nfeature vectors from three different areas as input and generates a fused\nrepresentation. The correlation module attempts to minimize the correlation\nloss between hidden representations of concerned and neighboring area feature\nvectors, while maximizing the loss between the same from concerned and remote\nregions. Further, we employ an attention module over the output hidden states\nof each encoder timepoint to generate a context vector. This vector is used as\nan input to a decoder module to predict patch severity grades at a future\ntimepoint. Finally, we ensemble the patch classification scores to calculate\npatient-wise grades. Specifically, our framework predicts zone-wise disease\nseverity for a patient on a given day by learning representations from the\nprevious temporal CXRs. Our novel multi-institutional dataset comprises\nsequential CXR scans from N=93 patients. Our approach outperforms transfer\nlearning and radiomic feature-based baseline approaches on this dataset.",
          "link": "http://arxiv.org/abs/2107.08330",
          "publishedOn": "2021-07-20T02:04:41.508Z",
          "wordCount": 742,
          "title": "Attention-based Multi-scale Gated Recurrent Encoder with Novel Correlation Loss for COVID-19 Progression Prediction. (arXiv:2107.08330v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08355",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Liupeng Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1\">Huanfeng Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_L/0/1/0/all/0/1\">Lingli Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_Q/0/1/0/all/0/1\">Qiangqiang Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xinghua Li</a>",
          "description": "The data fusion technology aims to aggregate the characteristics of different\ndata and obtain products with multiple data advantages. To solves the problem\nof reduced resolution of PolSAR images due to system limitations, we propose a\nfully polarimetric synthetic aperture radar (PolSAR) images and\nsingle-polarization synthetic aperture radar SAR (SinSAR) images fusion network\nto generate high-resolution PolSAR (HR-PolSAR) images. To take advantage of the\npolarimetric information of the low-resolution PolSAR (LR-PolSAR) image and the\nspatial information of the high-resolution single-polarization SAR (HR-SinSAR)\nimage, we propose a fusion framework for joint LR-PolSAR image and HR-SinSAR\nimage and design a cross-attention mechanism to extract features from the joint\ninput data. Besides, based on the physical imaging mechanism, we designed the\nPolSAR polarimetric loss function for constrained network training. The\nexperimental results confirm the superiority of fusion network over traditional\nalgorithms. The average PSNR is increased by more than 3.6db, and the average\nMAE is reduced to less than 0.07. Experiments on polarimetric decomposition and\npolarimetric signature show that it maintains polarimetric information well.",
          "link": "http://arxiv.org/abs/2107.08355",
          "publishedOn": "2021-07-20T02:04:41.490Z",
          "wordCount": 621,
          "title": "Fully Polarimetric SAR and Single-Polarization SAR Image Fusion Network. (arXiv:2107.08355v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yijin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pujin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1\">Junyan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>",
          "description": "Manually annotating medical images is extremely expensive, especially for\nlarge-scale datasets. Self-supervised contrastive learning has been explored to\nlearn feature representations from unlabeled images. However, unlike natural\nimages, the application of contrastive learning to medical images is relatively\nlimited. In this work, we propose a self-supervised framework, namely\nlesion-based contrastive learning for automated diabetic retinopathy (DR)\ngrading. Instead of taking entire images as the input in the common contrastive\nlearning scheme, lesion patches are employed to encourage the feature extractor\nto learn representations that are highly discriminative for DR grading. We also\ninvestigate different data augmentation operations in defining our contrastive\nprediction task. Extensive experiments are conducted on the publicly-accessible\ndataset EyePACS, demonstrating that our proposed framework performs\noutstandingly on DR grading in terms of both linear evaluation and transfer\ncapacity evaluation.",
          "link": "http://arxiv.org/abs/2107.08274",
          "publishedOn": "2021-07-20T02:04:41.472Z",
          "wordCount": 588,
          "title": "Lesion-based Contrastive Learning for Diabetic Retinopathy Grading from Fundus Images. (arXiv:2107.08274v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vo_Ho_V/0/1/0/all/0/1\">Viet-Khoa Vo-Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamazaki_K/0/1/0/all/0/1\">Kashu Yamazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_A/0/1/0/all/0/1\">Akihiro Sugimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>",
          "description": "Temporal action proposal generation is an essential and challenging task that\naims at localizing temporal intervals containing human actions in untrimmed\nvideos. Most of existing approaches are unable to follow the human cognitive\nprocess of understanding the video context due to lack of attention mechanism\nto express the concept of an action or an agent who performs the action or the\ninteraction between the agent and the environment. Based on the action\ndefinition that a human, known as an agent, interacts with the environment and\nperforms an action that affects the environment, we propose a contextual\nAgent-Environment Network. Our proposed contextual AEN involves (i) agent\npathway, operating at a local level to tell about which humans/agents are\nacting and (ii) environment pathway operating at a global level to tell about\nhow the agents interact with the environment. Comprehensive evaluations on\n20-action THUMOS-14 and 200-action ActivityNet-1.3 datasets with different\nbackbone networks, i.e C3D and SlowFast, show that our method robustly exhibits\noutperformance against state-of-the-art methods regardless of the employed\nbackbone network.",
          "link": "http://arxiv.org/abs/2107.08323",
          "publishedOn": "2021-07-20T02:04:41.454Z",
          "wordCount": 613,
          "title": "Agent-Environment Network for Temporal Action Proposal Generation. (arXiv:2107.08323v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zare_S/0/1/0/all/0/1\">Samira Zare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hien Van Nguyen</a>",
          "description": "Set-input deep networks have recently drawn much interest in computer vision\nand machine learning. This is in part due to the increasing number of important\ntasks such as meta-learning, clustering, and anomaly detection that are defined\non set inputs. These networks must take an arbitrary number of input samples\nand produce the output invariant to the input set permutation. Several\nalgorithms have been recently developed to address this urgent need. Our paper\nanalyzes these algorithms using both synthetic and real-world datasets, and\nshows that they are not effective in dealing with common data variations such\nas image translation or viewpoint change. To address this limitation, we\npropose a permutation-invariant cascaded attentional set operator (PICASO). The\ngist of PICASO is a cascade of multihead attention blocks with dynamic\ntemplates. The proposed operator is a stand-alone module that can be adapted\nand extended to serve different machine learning tasks. We demonstrate the\nutilities of PICASO in four diverse scenarios: (i) clustering, (ii) image\nclassification under novel viewpoints, (iii) image anomaly detection, and (iv)\nstate prediction. PICASO increases the SmallNORB image classification accuracy\nwith novel viewpoints by about 10% points. For set anomaly detection on CelebA\ndataset, our model improves the areas under ROC and PR curves dataset by about\n22% and 10%, respectively. For the state prediction on CLEVR dataset, it\nimproves the AP by about 40%.",
          "link": "http://arxiv.org/abs/2107.08305",
          "publishedOn": "2021-07-20T02:04:41.436Z",
          "wordCount": 654,
          "title": "PICASO: Permutation-Invariant Cascaded Attentional Set Operator. (arXiv:2107.08305v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_S/0/1/0/all/0/1\">Saravanabalagi Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1\">Ganesh Sistu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1\">John McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>",
          "description": "We present the WoodScape fisheye semantic segmentation challenge for\nautonomous driving which was held as part of the CVPR 2021 Workshop on\nOmnidirectional Computer Vision (OmniCV). This challenge is one of the first\nopportunities for the research community to evaluate the semantic segmentation\ntechniques targeted for fisheye camera perception. Due to strong radial\ndistortion standard models don't generalize well to fisheye images and hence\nthe deformations in the visual appearance of objects and entities needs to be\nencoded implicitly or as explicit knowledge. This challenge served as a medium\nto investigate the challenges and new methodologies to handle the complexities\nwith perception on fisheye images. The challenge was hosted on CodaLab and used\nthe recently released WoodScape dataset comprising of 10k samples. In this\npaper, we provide a summary of the competition which attracted the\nparticipation of 71 global teams and a total of 395 submissions. The top teams\nrecorded significantly improved mean IoU and accuracy scores over the baseline\nPSPNet with ResNet-50 backbone. We summarize the methods of winning algorithms\nand analyze the failure cases. We conclude by providing future directions for\nthe research.",
          "link": "http://arxiv.org/abs/2107.08246",
          "publishedOn": "2021-07-20T02:04:41.417Z",
          "wordCount": 659,
          "title": "Woodscape Fisheye Semantic Segmentation for Autonomous Driving -- CVPR 2021 OmniCV Workshop Challenge. (arXiv:2107.08246v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schott_L/0/1/0/all/0/1\">Lukas Schott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1\">Frederik Tr&#xe4;uble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1\">Peter Gehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1\">Chris Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>",
          "description": "An important component for generalization in machine learning is to uncover\nunderlying latent factors of variation as well as the mechanism through which\neach factor acts in the world. In this paper, we test whether 17 unsupervised,\nweakly supervised, and fully supervised representation learning approaches\ncorrectly infer the generative factors of variation in simple datasets\n(dSprites, Shapes3D, MPI3D). In contrast to prior robustness work that\nintroduces novel factors of variation during test time, such as blur or other\n(un)structured noise, we here recompose, interpolate, or extrapolate only\nexisting factors of variation from the training data set (e.g., small and\nmedium-sized objects during training and large objects during testing). Models\nthat learn the correct mechanism should be able to generalize to this\nbenchmark. In total, we train and test 2000+ models and observe that all of\nthem struggle to learn the underlying mechanism regardless of supervision\nsignal and architectural bias. Moreover, the generalization capabilities of all\ntested models drop significantly as we move from artificial datasets towards\nmore realistic real-world datasets. Despite their inability to identify the\ncorrect mechanism, the models are quite modular as their ability to infer other\nin-distribution factors remains fairly stable, providing only a single factor\nis out-of-distribution. These results point to an important yet understudied\nproblem of learning mechanistic models of observations that can facilitate\ngeneralization.",
          "link": "http://arxiv.org/abs/2107.08221",
          "publishedOn": "2021-07-20T02:04:41.399Z",
          "wordCount": 673,
          "title": "Visual Representation Learning Does Not Generalize Strongly Within the Same Domain. (arXiv:2107.08221v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Lisha Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_L/0/1/0/all/0/1\">Lap-Pui Chau</a>",
          "description": "Vehicle re-identification (Re-ID) is to retrieve images of the same vehicle\nacross different cameras. Two key challenges lie in the subtle inter-instance\ndiscrepancy caused by near-duplicate identities and the large intra-instance\nvariance caused by different views. Since the holistic appearance suffers from\nviewpoint variation and distortion, part-level feature learning has been\nintroduced to enhance vehicle description. However, existing approaches to\nlocalize and amplify significant parts often fail to handle spatial\nmisalignment as well as occlusion and require expensive annotations. In this\npaper, we propose a weakly supervised Part-Mentored Attention Network (PMANet)\ncomposed of a Part Attention Network (PANet) for vehicle part localization with\nself-attention and a Part-Mentored Network (PMNet) for mentoring the global and\nlocal feature aggregation. Firstly, PANet is introduced to predict a foreground\nmask and pinpoint $K$ prominent vehicle parts only with weak identity\nsupervision. Secondly, we propose a PMNet to learn global and part-level\nfeatures with multi-scale attention and aggregate them in $K$ main-partial\ntasks via part transfer. Like humans who first differentiate objects with\ngeneral information and then observe salient parts for more detailed clues,\nPANet and PMNet construct a two-stage attention structure to perform a\ncoarse-to-fine search among identities. Finally, we address this Re-ID issue as\na multi-task problem, including global feature learning, identity\nclassification, and part transfer. We adopt Homoscedastic Uncertainty to learn\nthe optimal weighing of different losses. Comprehensive experiments are\nconducted on two benchmark datasets. Our approach outperforms recent\nstate-of-the-art methods by averagely 2.63% in CMC@1 on VehicleID and 2.2% in\nmAP on VeRi776. Results on occluded test sets also demonstrate the\ngeneralization ability of PMANet.",
          "link": "http://arxiv.org/abs/2107.08228",
          "publishedOn": "2021-07-20T02:04:41.369Z",
          "wordCount": 733,
          "title": "Looking Twice for Partial Clues: Weakly-supervised Part-Mentored Attention Network for Vehicle Re-Identification. (arXiv:2107.08228v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yunqing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Haiwen Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>",
          "description": "In fine-grained image recognition (FGIR), the localization and amplification\nof region attention is an important factor, which has been explored a lot by\nconvolutional neural networks (CNNs) based approaches. The recently developed\nvision transformer (ViT) has achieved promising results on computer vision\ntasks. Compared with CNNs, Image sequentialization is a brand new manner.\nHowever, ViT is limited in its receptive field size and thus lacks local\nattention like CNNs due to the fixed size of its patches, and is unable to\ngenerate multi-scale features to learn discriminative region attention. To\nfacilitate the learning of discriminative region attention without box/part\nannotations, we use the strength of the attention weights to measure the\nimportance of the patch tokens corresponding to the raw images. We propose the\nrecurrent attention multi-scale transformer (RAMS-Trans), which uses the\ntransformer's self-attention to recursively learn discriminative region\nattention in a multi-scale manner. Specifically, at the core of our approach\nlies the dynamic patch proposal module (DPPM) guided region amplification to\ncomplete the integration of multi-scale image patches. The DPPM starts with the\nfull-size image patches and iteratively scales up the region attention to\ngenerate new patches from global to local by the intensity of the attention\nweights generated at each scale as an indicator. Our approach requires only the\nattention weights that come with ViT itself and can be easily trained\nend-to-end. Extensive experiments demonstrate that RAMS-Trans performs better\nthan concurrent works, in addition to efficient CNN models, achieving\nstate-of-the-art results on three benchmark datasets.",
          "link": "http://arxiv.org/abs/2107.08192",
          "publishedOn": "2021-07-20T02:04:41.304Z",
          "wordCount": 689,
          "title": "RAMS-Trans: Recurrent Attention Multi-scale Transformer forFine-grained Image Recognition. (arXiv:2107.08192v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "Convolutional neural network (CNN)-based stereo matching approaches generally\nrequire a dense cost volume (DCV) for disparity estimation. However, generating\nsuch cost volumes is computationally-intensive and memory-consuming, hindering\nCNN training and inference efficiency. To address this problem, we propose\nSCV-Stereo, a novel CNN architecture, capable of learning dense stereo matching\nfrom sparse cost volume (SCV) representations. Our inspiration is derived from\nthe fact that DCV representations are somewhat redundant and can be replaced\nwith SCV representations. Benefiting from these SCV representations, our\nSCV-Stereo can update disparity estimations in an iterative fashion for\naccurate and efficient stereo matching. Extensive experiments carried out on\nthe KITTI Stereo benchmarks demonstrate that our SCV-Stereo can significantly\nminimize the trade-off between accuracy and efficiency for stereo matching. Our\nproject page is https://sites.google.com/view/scv-stereo.",
          "link": "http://arxiv.org/abs/2107.08187",
          "publishedOn": "2021-07-20T02:04:41.252Z",
          "wordCount": 582,
          "title": "SCV-Stereo: Learning Stereo Matching from a Sparse Cost Volume. (arXiv:2107.08187v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "Stereo matching is a key component of autonomous driving perception. Recent\nunsupervised stereo matching approaches have received adequate attention due to\ntheir advantage of not requiring disparity ground truth. These approaches,\nhowever, perform poorly near occlusions. To overcome this drawback, in this\npaper, we propose CoT-Stereo, a novel unsupervised stereo matching approach.\nSpecifically, we adopt a co-teaching framework where two networks interactively\nteach each other about the occlusions in an unsupervised fashion, which greatly\nimproves the robustness of unsupervised stereo matching. Extensive experiments\non the KITTI Stereo benchmarks demonstrate the superior performance of\nCoT-Stereo over all other state-of-the-art unsupervised stereo matching\napproaches in terms of both accuracy and speed. Our project webpage is\nhttps://sites.google.com/view/cot-stereo.",
          "link": "http://arxiv.org/abs/2107.08186",
          "publishedOn": "2021-07-20T02:04:41.214Z",
          "wordCount": 566,
          "title": "Co-Teaching: An Ark to Unsupervised Stereo Matching. (arXiv:2107.08186v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08120",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yilin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yap_P/0/1/0/all/0/1\">Pew-Thian Yap</a>",
          "description": "Magnetic resonance Fingerprinting (MRF) is a relatively new multi-parametric\nquantitative imaging method that involves a two-step process: (i)\nreconstructing a series of time frames from highly-undersampled non-Cartesian\nspiral k-space data and (ii) pattern matching using the time frames to infer\ntissue properties (e.g., T1 and T2 relaxation times). In this paper, we\nintroduce a novel end-to-end deep learning framework to seamlessly map the\ntissue properties directly from spiral k-space MRF data, thereby avoiding\ntime-consuming processing such as the nonuniform fast Fourier transform (NUFFT)\nand the dictionary-based Fingerprint matching. Our method directly consumes the\nnon-Cartesian k- space data, performs adaptive density compensation, and\npredicts multiple tissue property maps in one forward pass. Experiments on both\n2D and 3D MRF data demonstrate that quantification accuracy comparable to\nstate-of-the-art methods can be accomplished within 0.5 second, which is 1100\nto 7700 times faster than the original MRF framework. The proposed method is\nthus promising for facilitating the adoption of MRF in clinical settings.",
          "link": "http://arxiv.org/abs/2107.08120",
          "publishedOn": "2021-07-20T02:04:40.658Z",
          "wordCount": 610,
          "title": "Real-Time Mapping of Tissue Properties for Magnetic Resonance Fingerprinting. (arXiv:2107.08120v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09022",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chung_H/0/1/0/all/0/1\">Hyungjin Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huh_J/0/1/0/all/0/1\">Jaeyoung Huh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_G/0/1/0/all/0/1\">Geon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1\">Yong Keun Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>",
          "description": "Optical diffraction tomography (ODT) produces three dimensional distribution\nof refractive index (RI) by measuring scattering fields at various angles.\nAlthough the distribution of RI index is highly informative, due to the missing\ncone problem stemming from the limited-angle acquisition of holograms,\nreconstructions have very poor resolution along axial direction compared to the\nhorizontal imaging plane. To solve this issue, here we present a novel\nunsupervised deep learning framework, which learns the probability distribution\nof missing projection views through optimal transport driven cycleGAN.\nExperimental results show that missing cone artifact in ODT can be\nsignificantly resolved by the proposed method.",
          "link": "http://arxiv.org/abs/2103.09022",
          "publishedOn": "2021-07-20T02:04:40.602Z",
          "wordCount": 589,
          "title": "Missing Cone Artifacts Removal in ODT using Unsupervised Deep Learning in Projection Domain. (arXiv:2103.09022v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Ruojin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1\">Bharath Hariharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1\">Hadar Averbuch-Elor</a>",
          "description": "We present a technique for estimating the relative 3D rotation of an RGB\nimage pair in an extreme setting, where the images have little or no overlap.\nWe observe that, even when images do not overlap, there may be rich hidden cues\nas to their geometric relationship, such as light source directions, vanishing\npoints, and symmetries present in the scene. We propose a network design that\ncan automatically learn such implicit cues by comparing all pairs of points\nbetween the two input images. Our method therefore constructs dense feature\ncorrelation volumes and processes these to predict relative 3D rotations. Our\npredictions are formed over a fine-grained discretization of rotations,\nbypassing difficulties associated with regressing 3D rotations. We demonstrate\nour approach on a large variety of extreme RGB image pairs, including indoor\nand outdoor images captured under different lighting conditions and geographic\nlocations. Our evaluation shows that our model can successfully estimate\nrelative rotations among non-overlapping images without compromising\nperformance over overlapping image pairs.",
          "link": "http://arxiv.org/abs/2104.13530",
          "publishedOn": "2021-07-20T02:04:40.583Z",
          "wordCount": 635,
          "title": "Extreme Rotation Estimation using Dense Correlation Volumes. (arXiv:2104.13530v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1\">Masahiro Kato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teshima_T/0/1/0/all/0/1\">Takeshi Teshima</a>",
          "description": "Density ratio estimation (DRE) is at the core of various machine learning\ntasks such as anomaly detection and domain adaptation. In existing studies on\nDRE, methods based on Bregman divergence (BD) minimization have been\nextensively studied. However, BD minimization when applied with highly flexible\nmodels, such as deep neural networks, tends to suffer from what we call\ntrain-loss hacking, which is a source of overfitting caused by a typical\ncharacteristic of empirical BD estimators. In this paper, to mitigate\ntrain-loss hacking, we propose a non-negative correction for empirical BD\nestimators. Theoretically, we confirm the soundness of the proposed method\nthrough a generalization error bound. Through our experiments, the proposed\nmethods show a favorable performance in inlier-based outlier detection.",
          "link": "http://arxiv.org/abs/2006.06979",
          "publishedOn": "2021-07-20T02:04:40.526Z",
          "wordCount": 596,
          "title": "Non-Negative Bregman Divergence Minimization for Deep Direct Density Ratio Estimation. (arXiv:2006.06979v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.14595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marzahl_C/0/1/0/all/0/1\">Christian Marzahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubreville_M/0/1/0/all/0/1\">Marc Aubreville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertram_C/0/1/0/all/0/1\">Christof A. Bertram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_J/0/1/0/all/0/1\">Jennifer Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergler_C/0/1/0/all/0/1\">Christian Bergler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kroger_C/0/1/0/all/0/1\">Christine Kr&#xf6;ger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voigt_J/0/1/0/all/0/1\">J&#xf6;rn Voigt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klopfleisch_R/0/1/0/all/0/1\">Robert Klopfleisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>",
          "description": "In many research areas, scientific progress is accelerated by\nmultidisciplinary access to image data and their interdisciplinary annotation.\nHowever, keeping track of these annotations to ensure a high-quality\nmulti-purpose data set is a challenging and labour intensive task. We developed\nthe open-source online platform EXACT (EXpert Algorithm Collaboration Tool)\nthat enables the collaborative interdisciplinary analysis of images from\ndifferent domains online and offline. EXACT supports multi-gigapixel medical\nwhole slide images as well as image series with thousands of images. The\nsoftware utilises a flexible plugin system that can be adapted to diverse\napplications such as counting mitotic figures with a screening mode, finding\nfalse annotations on a novel validation view, or using the latest deep learning\nimage analysis technologies. This is combined with a version control system\nwhich makes it possible to keep track of changes in the data sets and, for\nexample, to link the results of deep learning experiments to specific data set\nversions. EXACT is freely available and has already been successfully applied\nto a broad range of annotation tasks, including highly diverse applications\nlike deep learning supported cytology scoring, interdisciplinary multi-centre\nwhole slide image tumour annotation, and highly specialised whale sound\nspectroscopy clustering.",
          "link": "http://arxiv.org/abs/2004.14595",
          "publishedOn": "2021-07-20T02:04:40.502Z",
          "wordCount": 712,
          "title": "EXACT: A collaboration toolset for algorithm-aided annotation of images with annotation version control. (arXiv:2004.14595v3 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lejeune_L/0/1/0/all/0/1\">Laurent Lejeune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1\">Raphael Sznitman</a>",
          "description": "The ability to quickly annotate medical imaging data plays a critical role in\ntraining deep learning frameworks for segmentation. Doing so for image volumes\nor video sequences is even more pressing as annotating these is particularly\nburdensome. To alleviate this problem, this work proposes a new method to\nefficiently segment medical imaging volumes or videos using point-wise\nannotations only. This allows annotations to be collected extremely quickly and\nremains applicable to numerous segmentation tasks. Our approach trains a deep\nlearning model using an appropriate Positive/Unlabeled objective function using\nsparse point-wise annotations. While most methods of this kind assume that the\nproportion of positive samples in the data is known a-priori, we introduce a\nnovel self-supervised method to estimate this prior efficiently by combining a\nBayesian estimation framework and new stopping criteria. Our method iteratively\nestimates appropriate class priors and yields high segmentation quality for a\nvariety of object types and imaging modalities. In addition, by leveraging a\nspatio-temporal tracking framework, we regularize our predictions by leveraging\nthe complete data volume. We show experimentally that our approach outperforms\nstate-of-the-art methods tailored to the same problem.",
          "link": "http://arxiv.org/abs/2107.08394",
          "publishedOn": "2021-07-20T02:04:40.480Z",
          "wordCount": 623,
          "title": "A Positive/Unlabeled Approach for the Segmentation of Medical Sequences using Point-Wise Supervision. (arXiv:2107.08394v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingfeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>",
          "description": "Graphically-rich applications such as games are ubiquitous with attractive\nvisual effects of Graphical User Interface (GUI) that offers a bridge between\nsoftware applications and end-users. However, various types of graphical\nglitches may arise from such GUI complexity and have become one of the main\ncomponent of software compatibility issues. Our study on bug reports from game\ndevelopment teams in NetEase Inc. indicates that graphical glitches frequently\noccur during the GUI rendering and severely degrade the quality of\ngraphically-rich applications such as video games. Existing automated testing\ntechniques for such applications focus mainly on generating various GUI test\nsequences and check whether the test sequences can cause crashes. These\ntechniques require constant human attention to captures non-crashing bugs such\nas bugs causing graphical glitches. In this paper, we present the first step in\nautomating the test oracle for detecting non-crashing bugs in graphically-rich\napplications. Specifically, we propose \\texttt{GLIB} based on a code-based data\naugmentation technique to detect game GUI glitches. We perform an evaluation of\n\\texttt{GLIB} on 20 real-world game apps (with bug reports available) and the\nresult shows that \\texttt{GLIB} can achieve 100\\% precision and 99.5\\% recall\nin detecting non-crashing bugs such as game GUI glitches. Practical application\nof \\texttt{GLIB} on another 14 real-world games (without bug reports) further\ndemonstrates that \\texttt{GLIB} can effectively uncover GUI glitches, with 48\nof 53 bugs reported by \\texttt{GLIB} having been confirmed and fixed so far.",
          "link": "http://arxiv.org/abs/2106.10507",
          "publishedOn": "2021-07-19T01:59:51.012Z",
          "wordCount": 727,
          "title": "GLIB: Towards Automated Test Oracle for Graphically-Rich Applications. (arXiv:2106.10507v3 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1\">Jessica Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Um_I/0/1/0/all/0/1\">In Hwa Um</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1\">Ognjen Arandjelovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1\">David J Harrison</a>",
          "description": "Multiplex immunofluorescence and immunohistochemistry benefit patients by\nallowing cancer pathologists to identify several proteins expressed on the\nsurface of cells, enabling cell classification, better understanding of the\ntumour micro-environment, more accurate diagnoses, prognoses, and tailored\nimmunotherapy based on the immune status of individual patients. However, they\nare expensive and time consuming processes which require complex staining and\nimaging techniques by expert technicians. Hoechst staining is much cheaper and\neasier to perform, but is not typically used in this case as it binds to DNA\nrather than to the proteins targeted by immunofluorescent techniques, and it\nwas not previously thought possible to differentiate cells expressing these\nproteins based only on DNA morphology. In this work we show otherwise, training\na deep convolutional neural network to identify cells expressing three proteins\n(T lymphocyte markers CD3 and CD8, and the B lymphocyte marker CD20) with\ngreater than 90% precision and recall, from Hoechst 33342 stained tissue only.\nOur model learns previously unknown morphological features associated with\nexpression of these proteins which can be used to accurately differentiate\nlymphocyte subtypes for use in key prognostic metrics such as assessment of\nimmune cell infiltration,and thereby predict and improve patient outcomes\nwithout the need for costly multiplex immunofluorescence.",
          "link": "http://arxiv.org/abs/2107.04388",
          "publishedOn": "2021-07-19T01:59:50.979Z",
          "wordCount": 673,
          "title": "Hoechst Is All You Need: Lymphocyte Classification with Deep Learning. (arXiv:2107.04388v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changgong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1\">Tien-Tsin Wong</a>",
          "description": "Graph convolutional networks have significantly improved 3D human pose\nestimation by representing the human skeleton as an undirected graph. However,\nthis representation fails to reflect the articulated characteristic of human\nskeletons as the hierarchical orders among the joints are not explicitly\npresented. In this paper, we propose to represent the human skeleton as a\ndirected graph with the joints as nodes and bones as edges that are directed\nfrom parent joints to child joints. By so doing, the directions of edges can\nexplicitly reflect the hierarchical relationships among the nodes. Based on\nthis representation, we adopt the spatial-temporal directed graph convolution\n(ST-DGConv) to extract features from 2D poses represented in a temporal\nsequence of directed graphs. We further propose a spatial-temporal conditional\ndirected graph convolution (ST-CondDGConv) to leverage varying non-local\ndependence for different poses by conditioning the graph topology on input\nposes. Altogether, we form a U-shaped network with ST-DGConv and ST-CondDGConv\nlayers, named U-shaped Conditional Directed Graph Convolutional Network\n(U-CondDGCN), for 3D human pose estimation from monocular videos. To evaluate\nthe effectiveness of our U-CondDGCN, we conducted extensive experiments on two\nchallenging large-scale benchmarks: Human3.6M and MPI-INF-3DHP. Both\nquantitative and qualitative results show that our method achieves top\nperformance. Also, ablation studies show that directed graphs can better\nexploit the hierarchy of articulated human skeletons than undirected graphs,\nand the conditional connections can yield adaptive graph topologies for\ndifferent kinds of poses.",
          "link": "http://arxiv.org/abs/2107.07797",
          "publishedOn": "2021-07-19T00:49:08.141Z",
          "wordCount": 683,
          "title": "Conditional Directed Graph Convolution for 3D Human Pose Estimation. (arXiv:2107.07797v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garnot_V/0/1/0/all/0/1\">Vivien Sainte Fare Garnot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Loic Landrieu</a>",
          "description": "Unprecedented access to multi-temporal satellite imagery has opened new\nperspectives for a variety of Earth observation tasks. Among them,\npixel-precise panoptic segmentation of agricultural parcels has major economic\nand environmental implications. While researchers have explored this problem\nfor single images, we argue that the complex temporal patterns of crop\nphenology are better addressed with temporal sequences of images. In this\npaper, we present the first end-to-end, single-stage method for panoptic\nsegmentation of Satellite Image Time Series (SITS). This module can be combined\nwith our novel image sequence encoding network which relies on temporal\nself-attention to extract rich and adaptive multi-scale spatio-temporal\nfeatures. We also introduce PASTIS, the first open-access SITS dataset with\npanoptic annotations. We demonstrate the superiority of our encoder for\nsemantic segmentation against multiple competing architectures, and set up the\nfirst state-of-the-art of panoptic segmentation of SITS. Our implementation and\nPASTIS are publicly available.",
          "link": "http://arxiv.org/abs/2107.07933",
          "publishedOn": "2021-07-19T00:49:08.120Z",
          "wordCount": 605,
          "title": "Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks. (arXiv:2107.07933v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1\">Hyeon Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hyung-Kwon Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Jaemin Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngtaek Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jinwook Seo</a>",
          "description": "We propose Steadiness and Cohesiveness, two novel metrics to measure the\ninter-cluster reliability of multidimensional projection (MDP), specifically\nhow well the inter-cluster structures are preserved between the original\nhigh-dimensional space and the low-dimensional projection space. Measuring\ninter-cluster reliability is crucial as it directly affects how well\ninter-cluster tasks (e.g., identifying cluster relationships in the original\nspace from a projected view) can be conducted; however, despite the importance\nof inter-cluster tasks, we found that previous metrics, such as Trustworthiness\nand Continuity, fail to measure inter-cluster reliability. Our metrics consider\ntwo aspects of the inter-cluster reliability: Steadiness measures the extent to\nwhich clusters in the projected space form clusters in the original space, and\nCohesiveness measures the opposite. They extract random clusters with arbitrary\nshapes and positions in one space and evaluate how much the clusters are\nstretched or dispersed in the other space. Furthermore, our metrics can\nquantify pointwise distortions, allowing for the visualization of inter-cluster\nreliability in a projection, which we call a reliability map. Through\nquantitative experiments, we verify that our metrics precisely capture the\ndistortions that harm inter-cluster reliability while previous metrics have\ndifficulty capturing the distortions. A case study also demonstrates that our\nmetrics and the reliability map 1) support users in selecting the proper\nprojection techniques or hyperparameters and 2) prevent misinterpretation while\nperforming inter-cluster tasks, thus allow an adequate identification of\ninter-cluster structure.",
          "link": "http://arxiv.org/abs/2107.07859",
          "publishedOn": "2021-07-19T00:49:08.097Z",
          "wordCount": 686,
          "title": "Measuring and Explaining the Inter-Cluster Reliability of Multidimensional Projections. (arXiv:2107.07859v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shimizu_Y/0/1/0/all/0/1\">Yugo Shimizu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1\">Ryosuke Furuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_D/0/1/0/all/0/1\">Delong Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_Y/0/1/0/all/0/1\">Yukinobu Taniguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinami_R/0/1/0/all/0/1\">Ryota Hinami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishiwatari_S/0/1/0/all/0/1\">Shonosuke Ishiwatari</a>",
          "description": "Japanese comics (called manga) are traditionally created in monochrome\nformat. In recent years, in addition to monochrome comics, full color comics, a\nmore attractive medium, have appeared. Unfortunately, color comics require\nmanual colorization, which incurs high labor costs. Although automatic\ncolorization methods have been recently proposed, most of them are designed for\nillustrations, not for comics. Unlike illustrations, since comics are composed\nof many consecutive images, the painting style must be consistent. To realize\nconsistent colorization, we propose here a semi-automatic colorization method\nbased on generative adversarial networks (GAN); the method learns the painting\nstyle of a specific comic from small amount of training data. The proposed\nmethod takes a pair of a screen tone image and a flat colored image as input,\nand outputs a colorized image. Experiments show that the proposed method\nachieves better performance than the existing alternatives.",
          "link": "http://arxiv.org/abs/2107.07943",
          "publishedOn": "2021-07-19T00:49:08.092Z",
          "wordCount": 595,
          "title": "Painting Style-Aware Manga Colorization Based on Generative Adversarial Networks. (arXiv:2107.07943v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xinxin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Minglun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>",
          "description": "This paper presents a novel unsupervised approach to reconstruct human shape\nand pose from noisy point cloud. Traditional approaches search for\ncorrespondences and conduct model fitting iteratively where a good\ninitialization is critical. Relying on large amount of dataset with\nground-truth annotations, recent learning-based approaches predict\ncorrespondences for every vertice on the point cloud; Chamfer distance is\nusually used to minimize the distance between a deformed template model and the\ninput point cloud. However, Chamfer distance is quite sensitive to noise and\noutliers, thus could be unreliable to assign correspondences. To address these\nissues, we model the probability distribution of the input point cloud as\ngenerated from a parametric human model under a Gaussian Mixture Model. Instead\nof explicitly aligning correspondences, we treat the process of correspondence\nsearch as an implicit probabilistic association by updating the posterior\nprobability of the template model given the input. A novel unsupervised loss is\nfurther derived that penalizes the discrepancy between the deformed template\nand the input point cloud conditioned on the posterior probability. Our\napproach is very flexible, which works with both complete point cloud and\nincomplete ones including even a single depth image as input. Our network is\ntrained from scratch with no need to warm-up the network with supervised data.\nCompared to previous unsupervised methods, our method shows the capability to\ndeal with substantial noise and outliers. Extensive experiments conducted on\nvarious public synthetic datasets as well as a very noisy real dataset (i.e.\nCMU Panoptic) demonstrate the superior performance of our approach over the\nstate-of-the-art methods. Code can be found\n\\url{https://github.com/wangsen1312/unsupervised3dhuman.git}",
          "link": "http://arxiv.org/abs/2107.07539",
          "publishedOn": "2021-07-19T00:49:07.772Z",
          "wordCount": 701,
          "title": "Unsupervised 3D Human Mesh Recovery from Noisy Point Clouds. (arXiv:2107.07539v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selvaraju_R/0/1/0/all/0/1\">Ramprasaath R. Selvaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotmare_A/0/1/0/all/0/1\">Akhilesh Deepak Gotmare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven Hoi</a>",
          "description": "Large-scale vision and language representation learning has shown promising\nimprovements on various vision-language tasks. Most existing methods employ a\ntransformer-based multimodal encoder to jointly model visual tokens\n(region-based image features) and word tokens. Because the visual tokens and\nword tokens are unaligned, it is challenging for the multimodal encoder to\nlearn image-text interactions. In this paper, we introduce a contrastive loss\nto ALign the image and text representations BEfore Fusing (ALBEF) them through\ncross-modal attention, which enables more grounded vision and language\nrepresentation learning. Unlike most existing methods, our method does not\nrequire bounding box annotations nor high-resolution images. In order to\nimprove learning from noisy web data, we propose momentum distillation, a\nself-training method which learns from pseudo-targets produced by a momentum\nmodel. We provide a theoretical analysis of ALBEF from a mutual information\nmaximization perspective, showing that different training tasks can be\ninterpreted as different ways to generate views for an image-text pair. ALBEF\nachieves state-of-the-art performance on multiple downstream vision-language\ntasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained\non orders of magnitude larger datasets. On VQA and NLVR$^2$, ALBEF achieves\nabsolute improvements of 2.37% and 3.84% compared to the state-of-the-art,\nwhile enjoying faster inference speed. Code and pre-trained models are\navailable at https://github.com/salesforce/ALBEF/.",
          "link": "http://arxiv.org/abs/2107.07651",
          "publishedOn": "2021-07-19T00:49:07.747Z",
          "wordCount": 662,
          "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation. (arXiv:2107.07651v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hechen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>",
          "description": "In recent years, deep learning has made brilliant achievements in image\nclassification. However, image classification of small datasets is still not\nobtained good research results. This article first briefly explains the\napplication and characteristics of convolutional neural networks and visual\ntransformers. Meanwhile, the influence of small data set on classification and\nthe solution are introduced. Then a series of experiments are carried out on\nthe small datasets by using various models, and the problems of some models in\nthe experiments are discussed. Through the comparison of experimental results,\nthe recommended deep learning model is given according to the model application\nenvironment. Finally, we give directions for future work.",
          "link": "http://arxiv.org/abs/2107.07699",
          "publishedOn": "2021-07-19T00:49:07.742Z",
          "wordCount": 571,
          "title": "A Comparison of Deep Learning Classification Methods on Small-scale Image Data set: from Converlutional Neural Networks to Visual Transformers. (arXiv:2107.07699v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10785",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Godaz_R/0/1/0/all/0/1\">Reza Godaz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Monsefi_R/0/1/0/all/0/1\">Reza Monsefi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Toutounian_F/0/1/0/all/0/1\">Faezeh Toutounian</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hosseini_R/0/1/0/all/0/1\">Reshad Hosseini</a>",
          "description": "In this paper we tackle two important challenges related to the accurate\npartial singular value decomposition (SVD) and numerical rank estimation of a\nhuge matrix to use in low-rank learning problems in a fast way. We use the\nconcepts of Krylov subspaces such as the Golub-Kahan bidiagonalization process\nas well as Ritz vectors to achieve these goals. Our experiments identify\nvarious advantages of the proposed methods compared to traditional and\nrandomized SVD (R-SVD) methods with respect to the accuracy of the singular\nvalues and corresponding singular vectors computed in a similar execution time.\nThe proposed methods are appropriate for applications involving huge matrices\nwhere accuracy in all spectrum of the desired singular values, and also all of\ncorresponding singular vectors is essential. We evaluate our method in the real\napplication of Riemannian similarity learning (RSL) between two various image\ndatasets of MNIST and USPS.",
          "link": "http://arxiv.org/abs/2104.10785",
          "publishedOn": "2021-07-19T00:49:07.530Z",
          "wordCount": 611,
          "title": "Accurate and fast matrix factorization for low-rank learning. (arXiv:2104.10785v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13044",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guangwei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guoan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_D/0/1/0/all/0/1\">Dong Yue</a>",
          "description": "In recent years, how to strike a good trade-off between accuracy and\ninference speed has become the core issue for real-time semantic segmentation\napplications, which plays a vital role in real-world scenarios such as\nautonomous driving systems and drones. In this study, we devise a novel\nlightweight network using a multi-scale context fusion (MSCFNet) scheme, which\nexplores an asymmetric encoder-decoder architecture to dispose this problem.\nMore specifically, the encoder adopts some developed efficient asymmetric\nresidual (EAR) modules, which are composed of factorization depth-wise\nconvolution and dilation convolution. Meanwhile, instead of complicated\ncomputation, simple deconvolution is applied in the decoder to further reduce\nthe amount of parameters while still maintaining high segmentation accuracy.\nAlso, MSCFNet has branches with efficient attention modules from different\nstages of the network to well capture multi-scale contextual information. Then\nwe combine them before the final classification to enhance the expression of\nthe features and improve the segmentation efficiency. Comprehensive experiments\non challenging datasets have demonstrated that the proposed MSCFNet, which\ncontains only 1.15M parameters, achieves 71.9\\% Mean IoU on the Cityscapes\ntesting dataset and can run at over 50 FPS on a single Titan XP GPU\nconfiguration.",
          "link": "http://arxiv.org/abs/2103.13044",
          "publishedOn": "2021-07-19T00:49:07.523Z",
          "wordCount": 679,
          "title": "MSCFNet: A Lightweight Network With Multi-Scale Context Fusion for Real-Time Semantic Segmentation. (arXiv:2103.13044v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_W/0/1/0/all/0/1\">William Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_G/0/1/0/all/0/1\">Glen Kelly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leer_R/0/1/0/all/0/1\">Robert Leer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricardo_F/0/1/0/all/0/1\">Frederick Ricardo</a>",
          "description": "Generative Adversarial Networks (GANs) have been extremely successful in\nvarious application domains. Adversarial image synthesis has drawn increasing\nattention and made tremendous progress in recent years because of its wide\nrange of applications in many computer vision and image processing problems.\nAmong the many applications of GAN, image synthesis is the most well-studied\none, and research in this area has already demonstrated the great potential of\nusing GAN in image synthesis. In this paper, we provide a taxonomy of methods\nused in image synthesis, review different models for text-to-image synthesis\nand image-to-image translation, and discuss some evaluation metrics as well as\npossible future research directions in image synthesis with GAN.",
          "link": "http://arxiv.org/abs/2106.16056",
          "publishedOn": "2021-07-19T00:49:07.510Z",
          "wordCount": 581,
          "title": "A Survey on Adversarial Image Synthesis. (arXiv:2106.16056v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Tyler L. Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>",
          "description": "Humans are incredibly good at transferring knowledge from one domain to\nanother, enabling rapid learning of new tasks. Likewise, transfer learning has\nenabled enormous success in many computer vision problems using pretraining.\nHowever, the benefits of transfer in multi-domain learning, where a network\nlearns multiple tasks defined by different datasets, has not been adequately\nstudied. Learning multiple domains could be beneficial or these domains could\ninterfere with each other given limited network capacity. In this work, we\ndecipher the conditions where interference and knowledge transfer occur in\nmulti-domain learning. We propose new metrics disentangling interference and\ntransfer and set up experimental protocols. We further examine the roles of\nnetwork capacity, task grouping, and dynamic loss weighting in reducing\ninterference and facilitating transfer. We demonstrate our findings on the\nCIFAR-100, MiniPlaces, and Tiny-ImageNet datasets.",
          "link": "http://arxiv.org/abs/2107.05445",
          "publishedOn": "2021-07-19T00:49:07.433Z",
          "wordCount": 591,
          "title": "Disentangling Transfer and Interference in Multi-Domain Learning. (arXiv:2107.05445v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Davis_B/0/1/0/all/0/1\">Brian Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morse%7F_B/0/1/0/all/0/1\">Bryan Morse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price%7F_B/0/1/0/all/0/1\">Brian Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tensmeyer%7F_C/0/1/0/all/0/1\">Chris Tensmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiginton_C/0/1/0/all/0/1\">Curtis Wiginton</a>",
          "description": "We address the problem of form understanding: finding text entities and the\nrelationships/links between them in form images. The proposed FUDGE model\nformulates this problem on a graph of text elements (the vertices) and uses a\nGraph Convolutional Network to predict changes to the graph. The initial\nvertices are detected text lines and do not necessarily correspond to the final\ntext entities, which can span multiple lines. Also, initial edges contain many\nfalse-positive relationships. FUDGE edits the graph structure by combining text\nsegments (graph vertices) and pruning edges in an iterative fashion to obtain\nthe final text entities and relationships. While recent work in this area has\nfocused on leveraging large-scale pre-trained Language Models (LM), FUDGE\nachieves almost the same level of entity linking performance on the FUNSD\ndataset by learning only visual features from the (small) provided training\nset. FUDGE can be applied on forms where text recognition is difficult (e.g.\ndegraded or historical forms) and on forms in resource-poor languages where\npre-training such LMs is challenging. FUDGE is state-of-the-art on the\nhistorical NAF dataset.",
          "link": "http://arxiv.org/abs/2105.08194",
          "publishedOn": "2021-07-19T00:49:06.730Z",
          "wordCount": 652,
          "title": "Visual FUDGE: Form Understanding via Dynamic Graph Editing. (arXiv:2105.08194v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Broome_S/0/1/0/all/0/1\">Sofia Broom&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ask_K/0/1/0/all/0/1\">Katrina Ask</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1\">Maheen Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andersen_P/0/1/0/all/0/1\">Pia Haubro Andersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>",
          "description": "Orthopedic disorders are a common cause for euthanasia among horses, which\noften could have been avoided with earlier detection. These conditions often\ncreate varying degrees of subtle but long-term pain. It is challenging to train\na visual pain recognition method with video data depicting such pain, since the\nresulting pain behavior also is subtle, sparsely appearing, and varying, making\nit challenging for even an expert human labeler to provide accurate\nground-truth for the data. We show that transferring features from a dataset of\nhorses with acute nociceptive pain (where labeling is less ambiguous) can aid\nthe learning to recognize more complex orthopedic pain. Moreover, we present a\nhuman expert baseline for the problem, as well as an extensive empirical study\nof various domain transfer methods and of what is detected by the pain\nrecognition method trained on acute pain in the orthopedic dataset. Finally,\nthis is accompanied with a discussion around the challenges posed by real-world\nanimal behavior datasets and how best practices can be established for similar\nfine-grained action recognition tasks. Our code is available at\nhttps://github.com/sofiabroome/painface-recognition.",
          "link": "http://arxiv.org/abs/2105.10313",
          "publishedOn": "2021-07-19T00:49:06.704Z",
          "wordCount": 665,
          "title": "Sharing Pain: Using Domain Transfer Between Pain Types for Recognition of Sparse Pain Expressions in Horses. (arXiv:2105.10313v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey A. Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>",
          "description": "The world of empirical machine learning (ML) strongly relies on benchmarks in\norder to determine the relative effectiveness of different algorithms and\nmethods. This paper proposes the notion of \"a benchmark lottery\" that describes\nthe overall fragility of the ML benchmarking process. The benchmark lottery\npostulates that many factors, other than fundamental algorithmic superiority,\nmay lead to a method being perceived as superior. On multiple benchmark setups\nthat are prevalent in the ML community, we show that the relative performance\nof algorithms may be altered significantly simply by choosing different\nbenchmark tasks, highlighting the fragility of the current paradigms and\npotential fallacious interpretation derived from benchmarking ML methods. Given\nthat every benchmark makes a statement about what it perceives to be important,\nwe argue that this might lead to biased progress in the community. We discuss\nthe implications of the observed phenomena and provide recommendations on\nmitigating them using multiple machine learning domains and communities as use\ncases, including natural language processing, computer vision, information\nretrieval, recommender systems, and reinforcement learning.",
          "link": "http://arxiv.org/abs/2107.07002",
          "publishedOn": "2021-07-19T00:49:06.626Z",
          "wordCount": 620,
          "title": "The Benchmark Lottery. (arXiv:2107.07002v1 [cs.LG] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03686",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thanthrige_U/0/1/0/all/0/1\">Udaya S.K.P. Miriya Thanthrige</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_P/0/1/0/all/0/1\">Peter Jung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sezgin_A/0/1/0/all/0/1\">Aydin Sezgin</a>",
          "description": "We address the detection of material defects, which are inside a layered\nmaterial structure using compressive sensing based multiple-input and\nmultiple-output (MIMO) wireless radar. Here, the strong clutter due to the\nreflection of the layered structure's surface often makes the detection of the\ndefects challenging. Thus, sophisticated signal separation methods are required\nfor improved defect detection. In many scenarios, the number of defects that we\nare interested in is limited and the signaling response of the layered\nstructure can be modeled as a low-rank structure. Therefore, we propose joint\nrank and sparsity minimization for defect detection. In particular, we propose\na non-convex approach based on the iteratively reweighted nuclear and\n$\\ell_1-$norm (a double-reweighted approach) to obtain a higher accuracy\ncompared to the conventional nuclear norm and $\\ell_1-$norm minimization. To\nthis end, an iterative algorithm is designed to estimate the low-rank and\nsparse contributions. Further, we propose deep learning to learn the parameters\nof the algorithm (i.e., algorithm unfolding) to improve the accuracy and the\nspeed of convergence of the algorithm. Our numerical results show that the\nproposed approach outperforms the conventional approaches in terms of mean\nsquare errors of the recovered low-rank and sparse components and the speed of\nconvergence.",
          "link": "http://arxiv.org/abs/2106.03686",
          "publishedOn": "2021-07-19T00:49:06.597Z",
          "wordCount": 681,
          "title": "Deep Unfolding of Iteratively Reweighted ADMM for Wireless RF Sensing. (arXiv:2106.03686v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>",
          "description": "Despite remarkable progress achieved, most neural architecture search (NAS)\nmethods focus on searching for one single accurate and robust architecture. To\nfurther build models with better generalization capability and performance,\nmodel ensemble is usually adopted and performs better than stand-alone models.\nInspired by the merits of model ensemble, we propose to search for multiple\ndiverse models simultaneously as an alternative way to find powerful models.\nSearching for ensembles is non-trivial and has two key challenges: enlarged\nsearch space and potentially more complexity for the searched model. In this\npaper, we propose a one-shot neural ensemble architecture search (NEAS)\nsolution that addresses the two challenges. For the first challenge, we\nintroduce a novel diversity-based metric to guide search space shrinking,\nconsidering both the potentiality and diversity of candidate operators. For the\nsecond challenge, we enable a new search dimension to learn layer sharing among\ndifferent models for efficiency purposes. The experiments on ImageNet clearly\ndemonstrate that our solution can improve the supernet's capacity of ranking\nensemble architectures, and further lead to better search results. The\ndiscovered architectures achieve superior performance compared with\nstate-of-the-arts such as MobileNetV3 and EfficientNet families under aligned\nsettings. Moreover, we evaluate the generalization ability and robustness of\nour searched architecture on the COCO detection benchmark and achieve a 3.1%\nimprovement on AP compared with MobileNetV3. Codes and models are available at\nhttps://github.com/researchmm/NEAS.",
          "link": "http://arxiv.org/abs/2104.00597",
          "publishedOn": "2021-07-19T00:49:06.565Z",
          "wordCount": 700,
          "title": "One-Shot Neural Ensemble Architecture Search by Diversity-Guided Search Space Shrinking. (arXiv:2104.00597v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ishii_Y/0/1/0/all/0/1\">Yasunori Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_T/0/1/0/all/0/1\">Takayoshi Yamashita</a>",
          "description": "It is difficult to collect data on a large scale in a monocular depth\nestimation because the task requires the simultaneous acquisition of RGB images\nand depths. Data augmentation is thus important to this task. However, there\nhas been little research on data augmentation for tasks such as monocular depth\nestimation, where the transformation is performed pixel by pixel. In this\npaper, we propose a data augmentation method, called CutDepth. In CutDepth,\npart of the depth is pasted onto an input image during training. The method\nextends variations data without destroying edge features. Experiments\nobjectively and subjectively show that the proposed method outperforms\nconventional methods of data augmentation. The estimation accuracy is improved\nwith CutDepth even though there are few training data at long distances.",
          "link": "http://arxiv.org/abs/2107.07684",
          "publishedOn": "2021-07-19T00:49:06.547Z",
          "wordCount": 561,
          "title": "CutDepth:Edge-aware Data Augmentation in Depth Estimation. (arXiv:2107.07684v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.10670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Junwei Liang</a>",
          "description": "With the advancement in computer vision deep learning, systems now are able\nto analyze an unprecedented amount of rich visual information from videos to\nenable applications such as autonomous driving, socially-aware robot assistant\nand public safety monitoring. Deciphering human behaviors to predict their\nfuture paths/trajectories and what they would do from videos is important in\nthese applications. However, human trajectory prediction still remains a\nchallenging task, as scene semantics and human intent are difficult to model.\nMany systems do not provide high-level semantic attributes to reason about\npedestrian future. This design hinders prediction performance in video data\nfrom diverse domains and unseen scenarios. To enable optimal future human\nbehavioral forecasting, it is crucial for the system to be able to detect and\nanalyze human activities as well as scene semantics, passing informative\nfeatures to the subsequent prediction module for context understanding.",
          "link": "http://arxiv.org/abs/2011.10670",
          "publishedOn": "2021-07-19T00:49:06.497Z",
          "wordCount": 626,
          "title": "From Recognition to Prediction: Analysis of Human Action and Trajectory Prediction in Video. (arXiv:2011.10670v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.04785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Neural architecture search (NAS) with an accuracy predictor that predicts the\naccuracy of candidate architectures has drawn increasing attention due to its\nsimplicity and effectiveness. Previous works usually employ neural\nnetwork-based predictors which require more delicate design and are easy to\noverfit. Considering that most architectures are represented as sequences of\ndiscrete symbols which are more like tabular data and preferred by non-neural\npredictors, in this paper, we study an alternative approach which uses\nnon-neural model for accuracy prediction. Specifically, as decision tree based\nmodels can better handle tabular data, we leverage gradient boosting decision\ntree (GBDT) as the predictor for NAS. We demonstrate that the GBDT predictor\ncan achieve comparable (if not better) prediction accuracy than neural network\nbased predictors. Moreover, considering that a compact search space can ease\nthe search process, we propose to prune the search space gradually according to\nimportant features derived from GBDT. In this way, NAS can be performed by\nfirst pruning the search space and then searching a neural architecture, which\nis more efficient and effective. Experiments on NASBench-101 and ImageNet\ndemonstrate the effectiveness of using GBDT as predictor for NAS: (1) On\nNASBench-101, it is 22x, 8x, and 6x more sample efficient than random search,\nregularized evolution, and Monte Carlo Tree Search (MCTS) in finding the global\noptimum; (2) It achieves 24.2% top-1 error rate on ImageNet, and further\nachieves 23.4% top-1 error rate on ImageNet when enhanced with search space\npruning. Code is provided in the supplementary materials.",
          "link": "http://arxiv.org/abs/2007.04785",
          "publishedOn": "2021-07-19T00:49:06.477Z",
          "wordCount": 738,
          "title": "Accuracy Prediction with Non-neural Model for Neural Architecture Search. (arXiv:2007.04785v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.11150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jihun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Eunji Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Siwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "In this work, we attempt to explain the prediction of any black-box\nclassifier from an information-theoretic perspective. For each input feature,\nwe compare the classifier outputs with and without that feature using two\ninformation-theoretic metrics. Accordingly, we obtain two attribution maps--an\ninformation gain (IG) map and a point-wise mutual information (PMI) map. IG map\nprovides a class-independent answer to \"How informative is each pixel?\", and\nPMI map offers a class-specific explanation of \"How much does each pixel\nsupport a specific class?\" Compared to existing methods, our method improves\nthe correctness of the attribution maps in terms of a quantitative metric. We\nalso provide a detailed analysis of an ImageNet classifier using the proposed\nmethod, and the code is available online.",
          "link": "http://arxiv.org/abs/2009.11150",
          "publishedOn": "2021-07-19T00:49:06.470Z",
          "wordCount": 580,
          "title": "Information-Theoretic Visual Explanation for Black-Box Classifiers. (arXiv:2009.11150v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cardenas_B/0/1/0/all/0/1\">Bryan G. Cardenas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arya_D/0/1/0/all/0/1\">Devanshu Arya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak K. Gupta</a>",
          "description": "Recent developments related to generative models have made it possible to\ngenerate diverse high-fidelity images. In particular, layout-to-image\ngeneration models have gained significant attention due to their capability to\ngenerate realistic complex images containing distinct objects. These models are\ngenerally conditioned on either semantic layouts or textual descriptions.\nHowever, unlike natural images, providing auxiliary information can be\nextremely hard in domains such as biomedical imaging and remote sensing. In\nthis work, we propose a multi-object generation framework that can synthesize\nimages with multiple objects without explicitly requiring their contextual\ninformation during the generation process. Based on a vector-quantized\nvariational autoencoder (VQ-VAE) backbone, our model learns to preserve spatial\ncoherency within an image as well as semantic coherency between the objects and\nthe background through two powerful autoregressive priors: PixelSNAIL and\nLayoutPixelSNAIL. While the PixelSNAIL learns the distribution of the latent\nencodings of the VQ-VAE, the LayoutPixelSNAIL is used to specifically learn the\nsemantic distribution of the objects. An implicit advantage of our approach is\nthat the generated samples are accompanied by object-level annotations. We\ndemonstrate how coherency and fidelity are preserved with our method through\nexperiments on the Multi-MNIST and CLEVR datasets; thereby outperforming\nstate-of-the-art multi-object generative methods. The efficacy of our approach\nis demonstrated through application on medical imaging datasets, where we show\nthat augmenting the training set with generated samples using our approach\nimproves the performance of existing models.",
          "link": "http://arxiv.org/abs/2006.12150",
          "publishedOn": "2021-07-19T00:49:06.464Z",
          "wordCount": 711,
          "title": "Generating Annotated High-Fidelity Images Containing Multiple Coherent Objects. (arXiv:2006.12150v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhizhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1\">Charless Fowlkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1\">Marzia Polito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhotika_R/0/1/0/all/0/1\">Rahul Bhotika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "Traditionally, distillation has been used to train a student model to emulate\nthe input/output functionality of a teacher. A more useful goal than emulation,\nyet under-explored, is for the student to learn feature representations that\ntransfer well to future tasks. However, we observe that standard distillation\nof task-specific teachers actually *reduces* the transferability of student\nrepresentations to downstream tasks. We show that a multi-head, multi-task\ndistillation method using an unlabeled proxy dataset and a generalist teacher\nis sufficient to consolidate representations from task-specific teacher(s) and\nimprove downstream performance, outperforming the teacher(s) and the strong\nbaseline of ImageNet pretrained features. Our method can also combine the\nrepresentational knowledge of multiple teachers trained on one or multiple\ndomains into a single model, whose representation is improved on all teachers'\ndomain(s).",
          "link": "http://arxiv.org/abs/2107.08039",
          "publishedOn": "2021-07-19T00:49:06.449Z",
          "wordCount": 568,
          "title": "Representation Consolidation for Training Expert Students. (arXiv:2107.08039v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.14509",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1\">Joseph P. Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1\">Zaid Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1\">Ming Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>",
          "description": "Kinship is a soft biometric detectable in media with an abundance of\npractical applications. Despite the difficulty of detecting kinship, annual\ndata challenges using still-images have consistently improved performances and\nattracted new researchers. Now, systems reach performance levels unforeseeable\na decade ago, closing in on performances acceptable to deploy in practice.\nSimilar to other biometric tasks, we expect systems can benefit from additional\nmodalities. We hypothesize that adding modalities to FIW, which contains only\nstill-images, will improve performance. Thus, to narrow the gap between\nresearch and reality and enhance the power of kinship recognition systems, we\nextend FIW with multimedia (MM) data (i.e., video, audio, and text captions).\nSpecifically, we introduce the first publicly available multi-task MM kinship\ndataset. To build FIW MM, we developed machinery to automatically collect,\nannotate, and prepare the data, requiring minimal human input and no financial\ncost. The proposed MM corpus allows the problem statements to be more realistic\ntemplate-based protocols. We show significant improvements in all benchmarks\nwith the added modalities. The results highlight edge cases to inspire future\nresearch with different areas of improvement. FIW MM provides the data required\nto increase the potential of automated systems to detect kinship in MM. It also\nallows experts from diverse fields to collaborate in novel ways.",
          "link": "http://arxiv.org/abs/2007.14509",
          "publishedOn": "2021-07-19T00:49:06.444Z",
          "wordCount": 706,
          "title": "Families In Wild Multimedia (FIW MM): A Multi-Modal Database for Recognizing Kinship. (arXiv:2007.14509v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xudong Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "Generative Adversarial Networks (GANs) have been widely-used in image\ntranslation, but their high computational and storage costs impede the\ndeployment on mobile devices. Prevalent methods for CNN compression cannot be\ndirectly applied to GANs due to the complicated generator architecture and the\nunstable adversarial training. To solve these, in this paper, we introduce a\nnovel GAN compression method, termed DMAD, by proposing a Differentiable Mask\nand a co-Attention Distillation. The former searches for a light-weight\ngenerator architecture in a training-adaptive manner. To overcome channel\ninconsistency when pruning the residual connections, an adaptive cross-block\ngroup sparsity is further incorporated. The latter simultaneously distills\ninformative attention maps from both the generator and discriminator of a\npre-trained model to the searched generator, effectively stabilizing the\nadversarial training of our light-weight model. Experiments show that DMAD can\nreduce the Multiply Accumulate Operations (MACs) of CycleGAN by 13$\\times$ and\nthat of Pix2Pix by 4$\\times$ while retaining a comparable performance against\nthe full model. Our code can be available at https://github.com/SJLeo/DMAD.",
          "link": "http://arxiv.org/abs/2011.08382",
          "publishedOn": "2021-07-19T00:49:06.438Z",
          "wordCount": 663,
          "title": "Learning Efficient GANs for Image Translation via Differentiable Masks and co-Attention Distillation. (arXiv:2011.08382v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Achaji_L/0/1/0/all/0/1\">Lina Achaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_J/0/1/0/all/0/1\">Julien Moreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fouqueray_T/0/1/0/all/0/1\">Thibault Fouqueray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aioun_F/0/1/0/all/0/1\">Francois Aioun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charpillet_F/0/1/0/all/0/1\">Francois Charpillet</a>",
          "description": "The human driver is no longer the only one concerned with the complexity of\nthe driving scenarios. Autonomous vehicles (AV) are similarly becoming involved\nin the process. Nowadays, the development of AV in urban places underpins\nessential safety concerns for vulnerable road users (VRUs) such as pedestrians.\nTherefore, to make the roads safer, it is critical to classify and predict\ntheir future behavior. In this paper, we present a framework based on multiple\nvariations of the Transformer models to reason attentively about the dynamic\nevolution of the pedestrians' past trajectory and predict its future actions of\ncrossing or not crossing the street. We proved that using only bounding boxes\nas input to our model can outperform the previous state-of-the-art models and\nreach a prediction accuracy of 91 % and an F1-score of 0.83 on the PIE dataset\nup to two seconds ahead in the future. In addition, we introduced a large-size\nsimulated dataset (CP2A) using CARLA for action prediction. Our model has\nsimilarly reached high accuracy (91 %) and F1-score (0.91) on this dataset.\nInterestingly, we showed that pre-training our Transformer model on the\nsimulated dataset and then fine-tuning it on the real dataset can be very\neffective for the action prediction task.",
          "link": "http://arxiv.org/abs/2107.08031",
          "publishedOn": "2021-07-19T00:49:06.431Z",
          "wordCount": 656,
          "title": "Is attention to bounding boxes all you need for pedestrian action prediction?. (arXiv:2107.08031v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lulan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guikang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>",
          "description": "Multiple studies in the past have shown that there is a strong correlation\nbetween human vocal characteristics and facial features. However, existing\napproaches generate faces simply from voice, without exploring the set of\nfeatures that contribute to these observed correlations. A computational\nmethodology to explore this can be devised by rephrasing the question to: \"how\nmuch would a target face have to change in order to be perceived as the\noriginator of a source voice?\" With this in perspective, we propose a framework\nto morph a target face in response to a given voice in a way that facial\nfeatures are implicitly guided by learned voice-face correlation in this paper.\nOur framework includes a guided autoencoder that converts one face to another,\ncontrolled by a unique model-conditioning component called a gating controller\nwhich modifies the reconstructed face based on input voice recordings. We\nevaluate the framework on VoxCelab and VGGFace datasets through human subjects\nand face retrieval. Various experiments demonstrate the effectiveness of our\nproposed model.",
          "link": "http://arxiv.org/abs/2107.07988",
          "publishedOn": "2021-07-19T00:49:06.425Z",
          "wordCount": 621,
          "title": "Controlled AutoEncoders to Generate Faces from Voices. (arXiv:2107.07988v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moing_G/0/1/0/all/0/1\">Guillaume Le Moing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1\">Jean Ponce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>",
          "description": "This presentation introduces a self-supervised learning approach to the\nsynthesis of new video clips from old ones, with several new key elements for\nimproved spatial resolution and realism: It conditions the synthesis process on\ncontextual information for temporal continuity and ancillary information for\nfine control. The prediction model is doubly autoregressive, in the latent\nspace of an autoencoder for forecasting, and in image space for updating\ncontextual information, which is also used to enforce spatio-temporal\nconsistency through a learnable optical flow module. Adversarial training of\nthe autoencoder in the appearance and temporal domains is used to further\nimprove the realism of its output. A quantizer inserted between the encoder and\nthe transformer in charge of forecasting future frames in latent space (and its\ninverse inserted between the transformer and the decoder) adds even more\nflexibility by affording simple mechanisms for handling multimodal ancillary\ninformation for controlling the synthesis process (eg, a few sample frames, an\naudio track, a trajectory in image space) and taking into account the\nintrinsically uncertain nature of the future by allowing multiple predictions.\nExperiments with an implementation of the proposed approach give very good\nqualitative and quantitative results on multiple tasks and standard benchmarks.",
          "link": "http://arxiv.org/abs/2107.08037",
          "publishedOn": "2021-07-19T00:49:06.418Z",
          "wordCount": 628,
          "title": "CCVS: Context-aware Controllable Video Synthesis. (arXiv:2107.08037v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1905.10170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thanh-Dat Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1\">Chi Nhan Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>",
          "description": "Flow-based generative models have recently become one of the most efficient\napproaches to model data generation. Indeed, they are constructed with a\nsequence of invertible and tractable transformations. Glow first introduced a\nsimple type of generative flow using an invertible $1 \\times 1$ convolution.\nHowever, the $1 \\times 1$ convolution suffers from limited flexibility compared\nto the standard convolutions. In this paper, we propose a novel invertible $n\n\\times n$ convolution approach that overcomes the limitations of the invertible\n$1 \\times 1$ convolution. In addition, our proposed network is not only\ntractable and invertible but also uses fewer parameters than standard\nconvolutions. The experiments on CIFAR-10, ImageNet and Celeb-HQ datasets, have\nshown that our invertible $n \\times n$ convolution helps to improve the\nperformance of generative models significantly.",
          "link": "http://arxiv.org/abs/1905.10170",
          "publishedOn": "2021-07-19T00:49:06.400Z",
          "wordCount": 594,
          "title": "Generative Flow via Invertible nxn Convolution. (arXiv:1905.10170v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chull Hwan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hye Joo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1\">Yannis Avrithis</a>",
          "description": "We address representation learning for large-scale instance-level image\nretrieval. Apart from backbone, training pipelines and loss functions, popular\napproaches have focused on different spatial pooling and attention mechanisms,\nwhich are at the core of learning a powerful global image representation. There\nare different forms of attention according to the interaction of elements of\nthe feature tensor (local and global) and the dimensions where it is applied\n(spatial and channel). Unfortunately, each study addresses only one or two\nforms of attention and applies it to different problems like classification,\ndetection or retrieval.\n\nWe present global-local attention module (GLAM), which is attached at the end\nof a backbone network and incorporates all four forms of attention: local and\nglobal, spatial and channel. We obtain a new feature tensor and, by spatial\npooling, we learn a powerful embedding for image retrieval. Focusing on global\ndescriptors, we provide empirical evidence of the interaction of all forms of\nattention and improve the state of the art on standard benchmarks.",
          "link": "http://arxiv.org/abs/2107.08000",
          "publishedOn": "2021-07-19T00:49:06.393Z",
          "wordCount": 606,
          "title": "All the attention you need: Global-local, spatial-channel attention for image retrieval. (arXiv:2107.08000v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.03787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wagstaff_B/0/1/0/all/0/1\">Brandon Wagstaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1\">Jonathan Kelly</a>",
          "description": "The self-supervised loss formulation for jointly training depth and egomotion\nneural networks with monocular images is well studied and has demonstrated\nstate-of-the-art accuracy. One of the main limitations of this approach,\nhowever, is that the depth and egomotion estimates are only determined up to an\nunknown scale. In this paper, we present a novel scale recovery loss that\nenforces consistency between a known camera height and the estimated camera\nheight, generating metric (scaled) depth and egomotion predictions. We show\nthat our proposed method is competitive with other scale recovery techniques\nthat require more information. Further, we demonstrate that our method\nfacilitates network retraining within new environments, whereas other\nscale-resolving approaches are incapable of doing so. Notably, our egomotion\nnetwork is able to produce more accurate estimates than a similar method which\nrecovers scale at test time only.",
          "link": "http://arxiv.org/abs/2009.03787",
          "publishedOn": "2021-07-19T00:49:06.379Z",
          "wordCount": 630,
          "title": "Self-Supervised Scale Recovery for Monocular Depth and Egomotion Estimation. (arXiv:2009.03787v4 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07907",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1\">Kanglin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_G/0/1/0/all/0/1\">Gaofeng Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_J/0/1/0/all/0/1\">Jiang Duan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>",
          "description": "Single-image HDR reconstruction or inverse tone mapping (iTM) is a\nchallenging task. In particular, recovering information in over-exposed regions\nis extremely difficult because details in such regions are almost completely\nlost. In this paper, we present a deep learning based iTM method that takes\nadvantage of the feature extraction and mapping power of deep convolutional\nneural networks (CNNs) and uses a lightness prior to modulate the CNN to better\nexploit observations in the surrounding areas of the over-exposed regions to\nenhance the quality of HDR image reconstruction. Specifically, we introduce a\nHierarchical Synthesis Network (HiSN) for inferring a HDR image from a LDR\ninput and a Lightness Adpative Modulation Network (LAMN) to incorporate the the\nlightness prior knowledge in the inferring process. The HiSN hierarchically\nsynthesizes the high-brightness component and the low-brightness component of\nthe HDR image whilst the LAMN uses a lightness adaptive mask that separates\ndetail-less saturated bright pixels from well-exposed lower light pixels to\nenable HiSN to better infer the missing information, particularly in the\ndifficult over-exposed detail-less areas. We present experimental results to\ndemonstrate the effectiveness of the new technique based on quantitative\nmeasures and visual comparisons. In addition, we present ablation studies of\nHiSN and visualization of the activation maps inside LAMN to help gain a deeper\nunderstanding of the internal working of the new iTM algorithm and explain why\nit can achieve much improved performance over state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2107.07907",
          "publishedOn": "2021-07-19T00:49:06.350Z",
          "wordCount": 683,
          "title": "Lightness Modulated Deep Inverse Tone Mapping. (arXiv:2107.07907v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Josse_E/0/1/0/all/0/1\">Elias Josse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nerborg_A/0/1/0/all/0/1\">Amanda Nerborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Diaz_K/0/1/0/all/0/1\">Kevin Hernandez-Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1\">Fernando Alonso-Fernandez</a>",
          "description": "The world is expecting an aging population and shortage of healthcare\nprofessionals. This poses the problem of providing a safe and dignified life\nfor the elderly. Technological solutions involving cameras can contribute to\nsafety, comfort and efficient emergency responses, but they are invasive of\nprivacy. We use 'Griddy', a prototype with a Panasonic Grid-EYE, a\nlow-resolution infrared thermopile array sensor, which offers more privacy.\nMounted over a bed, it can determine if the user is on the bed or not without\nhuman interaction. For this purpose, two datasets were captured, one (480\nimages) under constant conditions, and a second one (200 images) under\ndifferent variations such as use of a duvet, sleeping with a pet, or increased\nroom temperature. We test three machine learning algorithms: Support Vector\nMachines (SVM), k-Nearest Neighbors (k-NN) and Neural Network (NN). With\n10-fold cross validation, the highest accuracy in the main dataset is for both\nSVM and k-NN (99%). The results with variable data show a lower reliability\nunder certain circumstances, highlighting the need of extra work to meet the\nchallenge of variations in the environment.",
          "link": "http://arxiv.org/abs/2107.07986",
          "publishedOn": "2021-07-19T00:49:06.343Z",
          "wordCount": 628,
          "title": "In-Bed Person Monitoring Using Thermal Infrared Sensors. (arXiv:2107.07986v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weizhi Lu</a>",
          "description": "Recently, it has been observed that {0,1,-1}-ternary codes which are simply\ngenerated from deep features by hard thresholding, tend to outperform\n{-1,1}-binary codes in image retrieval. To obtain better ternary codes, we for\nthe first time propose to jointly learn the features with the codes by\nappending a smoothed function to the networks. During training, the function\ncould evolve into a non-smoothed ternary function by a continuation method. The\nmethod circumvents the difficulty of directly training discrete functions and\nreduces the quantization errors of ternary codes. Experiments show that the\ngenerated codes indeed could achieve higher retrieval accuracy.",
          "link": "http://arxiv.org/abs/2107.07987",
          "publishedOn": "2021-07-19T00:49:06.308Z",
          "wordCount": 532,
          "title": "Deep Learning to Ternary Hash Codes by Continuation. (arXiv:2107.07987v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fabbrizzi_S/0/1/0/all/0/1\">Simone Fabbrizzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Symeon Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1\">Eirini Ntoutsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1\">Ioannis Kompatsiaris</a>",
          "description": "Computer Vision (CV) has achieved remarkable results, outperforming humans in\nseveral tasks. Nonetheless, it may result in major discrimination if not dealt\nwith proper care. CV systems highly depend on the data they are fed with and\ncan learn and amplify biases within such data. Thus, both the problems of\nunderstanding and discovering biases are of utmost importance. Yet, to date\nthere is no comprehensive survey on bias in visual datasets. To this end, this\nwork aims to: i) describe the biases that can affect visual datasets; ii)\nreview the literature on methods for bias discovery and quantification in\nvisual datasets; iii) discuss existing attempts to collect bias-aware visual\ndatasets. A key conclusion of our study is that the problem of bias discovery\nand quantification in visual datasets is still open and there is room for\nimprovement in terms of both methods and the range of biases that can be\naddressed; moreover, there is no such thing as a bias-free dataset, so\nscientists and practitioners must become aware of the biases in their datasets\nand make them explicit. To this end, we propose a checklist that can be used to\nspot different types of bias during visual dataset collection.",
          "link": "http://arxiv.org/abs/2107.07919",
          "publishedOn": "2021-07-19T00:49:06.301Z",
          "wordCount": 632,
          "title": "A Survey on Bias in Visual Datasets. (arXiv:2107.07919v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong-Xing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>",
          "description": "We study the problem of inferring an object-centric scene representation from\na single image, aiming to derive a representation that explains the image\nformation process, captures the scene's 3D nature, and is learned without\nsupervision. Most existing methods on scene decomposition lack one or more of\nthese characteristics, due to the fundamental challenge in integrating the\ncomplex 3D-to-2D image formation process into powerful inference schemes like\ndeep networks. In this paper, we propose unsupervised discovery of Object\nRadiance Fields (uORF), integrating recent progresses in neural 3D scene\nrepresentations and rendering with deep inference networks for unsupervised 3D\nscene decomposition. Trained on multi-view RGB images without annotations, uORF\nlearns to decompose complex scenes with diverse, textured background from a\nsingle image. We show that uORF performs well on unsupervised 3D scene\nsegmentation, novel view synthesis, and scene editing on three datasets.",
          "link": "http://arxiv.org/abs/2107.07905",
          "publishedOn": "2021-07-19T00:49:06.288Z",
          "wordCount": 580,
          "title": "Unsupervised Discovery of Object Radiance Fields. (arXiv:2107.07905v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Engel_N/0/1/0/all/0/1\">Nico Engel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1\">Klaus Dietmayer</a>",
          "description": "We present a vehicle self-localization method using point-based deep neural\nnetworks. Our approach processes measurements and point features, i.e.\nlandmarks, from a high-definition digital map to infer the vehicle's pose. To\nlearn the best association and incorporate local information between the point\nsets, we propose an attention mechanism that matches the measurements to the\ncorresponding landmarks. Finally, we use this representation for the\npoint-cloud registration and the subsequent pose regression task. Furthermore,\nwe introduce a training simulation framework that artificially generates\nmeasurements and landmarks to facilitate the deployment process and reduce the\ncost of creating extensive datasets from real-world data. We evaluate our\nmethod on our dataset, as well as an adapted version of the Kitti odometry\ndataset, where we achieve superior performance compared to related approaches;\nand additionally show dominant generalization capabilities.",
          "link": "http://arxiv.org/abs/2107.07787",
          "publishedOn": "2021-07-19T00:49:06.279Z",
          "wordCount": 583,
          "title": "Attention-based Vehicle Self-Localization with HD Feature Maps. (arXiv:2107.07787v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pont_M/0/1/0/all/0/1\">Mathieu Pont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_J/0/1/0/all/0/1\">Jules Vidal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delon_J/0/1/0/all/0/1\">Julie Delon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tierny_J/0/1/0/all/0/1\">Julien Tierny</a>",
          "description": "This paper presents a unified computational framework for the estimation of\ndistances, geodesics and barycenters of merge trees. We extend recent work on\nthe edit distance [106] and introduce a new metric, called the Wasserstein\ndistance between merge trees, which is purposely designed to enable efficient\ncomputations of geodesics and barycenters. Specifically, our new distance is\nstrictly equivalent to the L2-Wasserstein distance between extremum persistence\ndiagrams, but it is restricted to a smaller solution space, namely, the space\nof rooted partial isomorphisms between branch decomposition trees. This enables\na simple extension of existing optimization frameworks [112] for geodesics and\nbarycenters from persistence diagrams to merge trees. We introduce a task-based\nalgorithm which can be generically applied to distance, geodesic, barycenter or\ncluster computation. The task-based nature of our approach enables further\naccelerations with shared-memory parallelism. Extensive experiments on public\nensembles and SciVis contest benchmarks demonstrate the efficiency of our\napproach -- with barycenter computations in the orders of minutes for the\nlargest examples -- as well as its qualitative ability to generate\nrepresentative barycenter merge trees, visually summarizing the features of\ninterest found in the ensemble. We show the utility of our contributions with\ndedicated visualization applications: feature tracking, temporal reduction and\nensemble clustering. We provide a lightweight C++ implementation that can be\nused to reproduce our results.",
          "link": "http://arxiv.org/abs/2107.07789",
          "publishedOn": "2021-07-19T00:49:06.272Z",
          "wordCount": 664,
          "title": "Wasserstein Distances, Geodesics and Barycenters of Merge Trees. (arXiv:2107.07789v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blekos_K/0/1/0/all/0/1\">Kostas Blekos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nousias_S/0/1/0/all/0/1\">Stavros Nousias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalos_A/0/1/0/all/0/1\">Aris S Lalos</a>",
          "description": "Delineation approaches provide significant benefits to various domains,\nincluding agriculture, environmental and natural disasters monitoring. Most of\nthe work in the literature utilize traditional segmentation methods that\nrequire a large amount of computational and storage resources. Deep learning\nhas transformed computer vision and dramatically improved machine translation,\nthough it requires massive dataset for training and significant resources for\ninference. More importantly, energy-efficient embedded vision hardware\ndelivering real-time and robust performance is crucial in the aforementioned\napplication. In this work, we propose a U-Net based tree delineation method,\nwhich is effectively trained using multi-spectral imagery but can then\ndelineate single-spectrum images. The deep architecture that also performs\nlocalization, i.e., a class label corresponds to each pixel, has been\nsuccessfully used to allow training with a small set of segmented images. The\nground truth data were generated using traditional image denoising and\nsegmentation approaches. To be able to execute the proposed DNN efficiently in\nembedded platforms designed for deep learning approaches, we employ traditional\nmodel compression and acceleration methods. Extensive evaluation studies using\ndata collected from UAVs equipped with multi-spectral cameras demonstrate the\neffectiveness of the proposed methods in terms of delineation accuracy and\nexecution efficiency.",
          "link": "http://arxiv.org/abs/2107.07826",
          "publishedOn": "2021-07-19T00:49:06.255Z",
          "wordCount": 657,
          "title": "Efficient automated U-Net based tree crown delineation using UAV multi-spectral imagery on embedded devices. (arXiv:2107.07826v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Runde Li</a>",
          "description": "To solve the issue of video dehazing, there are two main tasks to attain: how\nto align adjacent frames to the reference frame; how to restore the reference\nframe. Some papers adopt explicit approaches (e.g., the Markov random field,\noptical flow, deformable convolution, 3D convolution) to align neighboring\nframes with the reference frame in feature space or image space, they then use\nvarious restoration methods to achieve the final dehazing results. In this\npaper, we propose a progressive alignment and restoration method for video\ndehazing. The alignment process aligns consecutive neighboring frames stage by\nstage without using the optical flow estimation. The restoration process is not\nonly implemented under the alignment process but also uses a refinement network\nto improve the dehazing performance of the whole network. The proposed networks\ninclude four fusion networks and one refinement network. To decrease the\nparameters of networks, three fusion networks in the first fusion stage share\nthe same parameters. Extensive experiments demonstrate that the proposed video\ndehazing method achieves outstanding performance against the-state-of-art\nmethods.",
          "link": "http://arxiv.org/abs/2107.07837",
          "publishedOn": "2021-07-19T00:49:06.241Z",
          "wordCount": 600,
          "title": "Progressive Deep Video Dehazing without Explicit Alignment Estimation. (arXiv:2107.07837v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07975",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Savioli_N/0/1/0/all/0/1\">Nicolo Savioli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marvao_A/0/1/0/all/0/1\">Antonio de Marvao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cook_S/0/1/0/all/0/1\">Stuart A. Cook</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chin_C/0/1/0/all/0/1\">Calvin W.L. Chin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ORegan_D/0/1/0/all/0/1\">Declan P. O&#x27;Regan</a>",
          "description": "Optimising the analysis of cardiac structure and function requires accurate\n3D representations of shape and motion. However, techniques such as cardiac\nmagnetic resonance imaging are conventionally limited to acquiring contiguous\ncross-sectional slices with low through-plane resolution and potential\ninter-slice spatial misalignment. Super-resolution in medical imaging aims to\nincrease the resolution of images but is conventionally trained on features\nfrom low resolution datasets and does not super-resolve corresponding\nsegmentations. Here we propose a semi-supervised multi-task generative\nadversarial network (Gemini-GAN) that performs joint super-resolution of the\nimages and their labels using a ground truth of high resolution 3D cines and\nsegmentations, while an unsupervised variational adversarial mixture\nautoencoder (V-AMA) is used for continuous domain adaptation. Our proposed\napproach is extensively evaluated on two transnational multi-ethnic populations\nof 1,331 and 205 adults respectively, delivering an improvement on state of the\nart methods in terms of Dice index, peak signal to noise ratio, and structural\nsimilarity index measure. This framework also exceeds the performance of state\nof the art generative domain adaptation models on external validation (Dice\nindex 0.81 vs 0.74 for the left ventricle). This demonstrates how joint\nsuper-resolution and segmentation, trained on 3D ground-truth data with\ncross-domain generalization, enables robust precision phenotyping in diverse\npopulations.",
          "link": "http://arxiv.org/abs/2107.07975",
          "publishedOn": "2021-07-19T00:49:06.224Z",
          "wordCount": 667,
          "title": "Joint Semi-supervised 3D Super-Resolution and Segmentation with Mixed Adversarial Gaussian Domain Adaptation. (arXiv:2107.07975v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kannan_N/0/1/0/all/0/1\">Nagajothi Kannan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danda_S/0/1/0/all/0/1\">Sravan Danda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Challa_A/0/1/0/all/0/1\">Aditya Challa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_D/0/1/0/all/0/1\">Daya Sagar B S</a>",
          "description": "The study of water bodies such as rivers is an important problem in the\nremote sensing community. A meaningful set of quantitative features reflecting\nthe geophysical properties help us better understand the formation and\nevolution of rivers. Typically, river sub-basins are analysed using Cartosat\nDigital Elevation Models (DEMs), obtained at regular time epochs. One of the\nuseful geophysical features of a river sub-basin is that of a roughness measure\non DEMs. However, to the best of our knowledge, there is not much literature\navailable on theoretical analysis of roughness measures. In this article, we\nrevisit the roughness measure on DEM data adapted from multiscale\ngranulometries in mathematical morphology, namely multiscale directional\ngranulometric index (MDGI). This measure was classically used to obtain\nshape-size analysis in greyscale images. In earlier works, MDGIs were\nintroduced to capture the characteristic surficial roughness of a river\nsub-basin along specific directions. Also, MDGIs can be efficiently computed\nand are known to be useful features for classification of river sub-basins. In\nthis article, we provide a theoretical analysis of a MDGI. In particular, we\ncharacterize non-trivial sufficient conditions on the structure of DEMs under\nwhich MDGIs are invariant. These properties are illustrated with some\nfictitious DEMs. We also provide connections to a discrete derivative of volume\nof a DEM. Based on these connections, we provide intuition as to why a MDGI is\nconsidered a roughness measure. Further, we experimentally illustrate on\nLower-Indus, Wardha, and Barmer river sub-basins that the proposed features\ncapture the characteristics of the river sub-basin.",
          "link": "http://arxiv.org/abs/2107.07827",
          "publishedOn": "2021-07-19T00:49:06.208Z",
          "wordCount": 708,
          "title": "A Theoretical Analysis of Granulometry-based Roughness Measures on Cartosat DEMs. (arXiv:2107.07827v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07985",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_J/0/1/0/all/0/1\">Jue Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rimner_A/0/1/0/all/0/1\">Andreas Rimner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deasy_J/0/1/0/all/0/1\">Joseph O. Deasy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veeraraghavan_H/0/1/0/all/0/1\">Harini Veeraraghavan</a>",
          "description": "Accurate and robust segmentation of lung cancers from CTs is needed to more\naccurately plan and deliver radiotherapy and to measure treatment response.\nThis is particularly difficult for tumors located close to mediastium, due to\nlow soft-tissue contrast. Therefore, we developed a new cross-modality educed\ndistillation (CMEDL) approach, using unpaired CT and MRI scans, whereby a\nteacher MRI network guides a student CT network to extract features that signal\nthe difference between foreground and background. Our contribution eliminates\ntwo requirements of distillation methods: (i) paired image sets by using an\nimage to image (I2I) translation and (ii) pre-training of the teacher network\nwith a large training set by using concurrent training of all networks. Our\nframework uses an end-to-end trained unpaired I2I translation, teacher, and\nstudent segmentation networks. Our framework can be combined with any I2I and\nsegmentation network. We demonstrate our framework's feasibility using 3\nsegmentation and 2 I2I methods. All networks were trained with 377 CT and 82\nT2w MRI from different sets of patients. Ablation tests and different\nstrategies for incorporating MRI information into CT were performed. Accuracy\nwas measured using Dice similarity (DSC), surface Dice (sDSC), and Hausdorff\ndistance at the 95$^{th}$ percentile (HD95). The CMEDL approach was\nsignificantly (p $<$ 0.001) more accurate than non-CMEDL methods,\nquantitatively and visually. It produced the highest segmentation accuracy\n(sDSC of 0.83 $\\pm$ 0.16 and HD95 of 5.20 $\\pm$ 6.86mm). CMEDL was also more\naccurate than using either pMRI's or the combination of CT's with pMRI's for\nsegmentation.",
          "link": "http://arxiv.org/abs/2107.07985",
          "publishedOn": "2021-07-19T00:49:06.202Z",
          "wordCount": 715,
          "title": "Unpaired cross-modality educed distillation (CMEDL) applied to CT lung tumor segmentation. (arXiv:2107.07985v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shatri_E/0/1/0/all/0/1\">Elona Shatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>",
          "description": "The main challenges of Optical Music Recognition (OMR) come from the nature\nof written music, its complexity and the difficulty of finding an appropriate\ndata representation. This paper provides a first look at DoReMi, an OMR dataset\nthat addresses these challenges, and a baseline object detection model to\nassess its utility. Researchers often approach OMR following a set of small\nstages, given that existing data often do not satisfy broader research. We\nexamine the possibility of changing this tendency by presenting more metadata.\nOur approach complements existing research; hence DoReMi allows harmonisation\nwith two existing datasets, DeepScores and MUSCIMA++. DoReMi was generated\nusing a music notation software and includes over 6400 printed sheet music\nimages with accompanying metadata useful in OMR research. Our dataset provides\nOMR metadata, MIDI, MEI, MusicXML and PNG files, each aiding a different stage\nof OMR. We obtain 64% mean average precision (mAP) in object detection using\nhalf of the data. Further work includes re-iterating through the creation\nprocess to satisfy custom OMR models. While we do not assume to have solved the\nmain challenges in OMR, this dataset opens a new course of discussions that\nwould ultimately aid that goal.",
          "link": "http://arxiv.org/abs/2107.07786",
          "publishedOn": "2021-07-19T00:49:06.177Z",
          "wordCount": 651,
          "title": "DoReMi: First glance at a universal OMR dataset. (arXiv:2107.07786v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muzammul_M/0/1/0/all/0/1\">Muhammed Muzammul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>",
          "description": "This survey paper specially analyzed computer vision-based object detection\nchallenges and solutions by different techniques. We mainly highlighted object\ndetection by three different trending strategies, i.e., 1) domain adaptive deep\nlearning-based approaches (discrepancy-based, Adversarial-based,\nReconstruction-based, Hybrid). We examined general as well as tiny object\ndetection-related challenges and offered solutions by historical and\ncomparative analysis. In part 2) we mainly focused on tiny object detection\ntechniques (multi-scale feature learning, Data augmentation, Training strategy\n(TS), Context-based detection, GAN-based detection). In part 3), To obtain\nknowledge-able findings, we discussed different object detection methods, i.e.,\nconvolutions and convolutional neural networks (CNN), pooling operations with\ntrending types. Furthermore, we explained results with the help of some object\ndetection algorithms, i.e., R-CNN, Fast R-CNN, Faster R-CNN, YOLO, and SSD,\nwhich are generally considered the base bone of CV, CNN, and OD. We performed\ncomparative analysis on different datasets such as MS-COCO, PASCAL VOC07,12,\nand ImageNet to analyze results and present findings. At the end, we showed\nfuture directions with existing challenges of the field. In the future, OD\nmethods and models can be analyzed for real-time object detection, tracking\nstrategies.",
          "link": "http://arxiv.org/abs/2107.07927",
          "publishedOn": "2021-07-19T00:49:06.171Z",
          "wordCount": 627,
          "title": "A Survey on Deep Domain Adaptation and Tiny Object Detection Challenges, Techniques and Datasets. (arXiv:2107.07927v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07746",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Longhui Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Liangjian Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinrong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Few-Shot image classification aims to utilize pretrained knowledge learned\nfrom a large-scale dataset to tackle a series of downstream classification\ntasks. Typically, each task involves only few training examples from brand-new\ncategories. This requires the pretraining models to focus on well-generalizable\nknowledge, but ignore domain-specific information. In this paper, we observe\nthat image background serves as a source of domain-specific knowledge, which is\na shortcut for models to learn in the source dataset, but is harmful when\nadapting to brand-new classes. To prevent the model from learning this shortcut\nknowledge, we propose COSOC, a novel Few-Shot Learning framework, to\nautomatically figure out foreground objects at both pretraining and evaluation\nstage. COSOC is a two-stage algorithm motivated by the observation that\nforeground objects from different images within the same class share more\nsimilar patterns than backgrounds. At the pretraining stage, for each class, we\ncluster contrastive-pretrained features of randomly cropped image patches, such\nthat crops containing only foreground objects can be identified by a single\ncluster. We then force the pretraining model to focus on found foreground\nobjects by a fusion sampling strategy; at the evaluation stage, among images in\neach training class of any few-shot task, we seek for shared contents and\nfilter out background. The recognized foreground objects of each class are used\nto match foreground of testing images. Extensive experiments tailored to\ninductive FSL tasks on two benchmarks demonstrate the state-of-the-art\nperformance of our method.",
          "link": "http://arxiv.org/abs/2107.07746",
          "publishedOn": "2021-07-19T00:49:06.158Z",
          "wordCount": 694,
          "title": "Rectifying the Shortcut Learning of Background: Shared Object Concentration for Few-Shot Image Recognition. (arXiv:2107.07746v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hubner_P/0/1/0/all/0/1\">Patrick H&#xfc;bner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinmann_M/0/1/0/all/0/1\">Martin Weinmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wursthorn_S/0/1/0/all/0/1\">Sven Wursthorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinz_S/0/1/0/all/0/1\">Stefan Hinz</a>",
          "description": "In this paper, we present a novel pose normalization method for indoor\nmapping point clouds and triangle meshes that is robust against large fractions\nof the indoor mapping geometries deviating from an ideal Manhattan World\nstructure. In the case of building structures that contain multiple Manhattan\nWorld systems, the dominant Manhattan World structure supported by the largest\nfraction of geometries is determined and used for alignment. In a first step, a\nvertical alignment orienting a chosen axis to be orthogonal to horizontal floor\nand ceiling surfaces is conducted. Subsequently, a rotation around the\nresulting vertical axis is determined that aligns the dataset horizontally with\nthe coordinate axes. The proposed method is evaluated quantitatively against\nseveral publicly available indoor mapping datasets. Our implementation of the\nproposed procedure along with code for reproducing the evaluation will be made\navailable to the public upon acceptance for publication.",
          "link": "http://arxiv.org/abs/2107.07778",
          "publishedOn": "2021-07-19T00:49:06.151Z",
          "wordCount": 590,
          "title": "Pose Normalization of Indoor Mapping Datasets Partially Compliant to the Manhattan World Assumption. (arXiv:2107.07778v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haan_P/0/1/0/all/0/1\">Puck de Haan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1\">Sindy L&#xf6;we</a>",
          "description": "Reliable detection of anomalies is crucial when deploying machine learning\nmodels in practice, but remains challenging due to the lack of labeled data. To\ntackle this challenge, contrastive learning approaches are becoming\nincreasingly popular, given the impressive results they have achieved in\nself-supervised representation learning settings. However, while most existing\ncontrastive anomaly detection and segmentation approaches have been applied to\nimages, none of them can use the contrastive losses directly for both anomaly\ndetection and segmentation. In this paper, we close this gap by making use of\nthe Contrastive Predictive Coding model (arXiv:1807.03748). We show that its\npatch-wise contrastive loss can directly be interpreted as an anomaly score,\nand how this allows for the creation of anomaly segmentation masks. The\nresulting model achieves promising results for both anomaly detection and\nsegmentation on the challenging MVTec-AD dataset.",
          "link": "http://arxiv.org/abs/2107.07820",
          "publishedOn": "2021-07-19T00:49:06.129Z",
          "wordCount": 583,
          "title": "Contrastive Predictive Coding for Anomaly Detection. (arXiv:2107.07820v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qaiser_T/0/1/0/all/0/1\">Talha Qaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winzeck_S/0/1/0/all/0/1\">Stefan Winzeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barfoot_T/0/1/0/all/0/1\">Theodore Barfoot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barwick_T/0/1/0/all/0/1\">Tara Barwick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doran_S/0/1/0/all/0/1\">Simon J. Doran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaiser_M/0/1/0/all/0/1\">Martin F. Kaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wedlake_L/0/1/0/all/0/1\">Linda Wedlake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunariu_N/0/1/0/all/0/1\">Nina Tunariu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_D/0/1/0/all/0/1\">Dow-Mu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messiou_C/0/1/0/all/0/1\">Christina Messiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rockall_A/0/1/0/all/0/1\">Andrea Rockall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>",
          "description": "Whole body magnetic resonance imaging (WB-MRI) is the recommended modality\nfor diagnosis of multiple myeloma (MM). WB-MRI is used to detect sites of\ndisease across the entire skeletal system, but it requires significant\nexpertise and is time-consuming to report due to the great number of images. To\naid radiological reading, we propose an auxiliary task-based multiple instance\nlearning approach (ATMIL) for MM classification with the ability to localize\nsites of disease. This approach is appealing as it only requires patient-level\nannotations where an attention mechanism is used to identify local regions with\nactive disease. We borrow ideas from multi-task learning and define an\nauxiliary task with adaptive reweighting to support and improve learning\nefficiency in the presence of data scarcity. We validate our approach on both\nsynthetic and real multi-center clinical data. We show that the MIL attention\nmodule provides a mechanism to localize bone regions while the adaptive\nreweighting of the auxiliary task considerably improves the performance.",
          "link": "http://arxiv.org/abs/2107.07805",
          "publishedOn": "2021-07-19T00:49:06.122Z",
          "wordCount": 623,
          "title": "Multiple Instance Learning with Auxiliary Task Weighting for Multiple Myeloma Classification. (arXiv:2107.07805v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07752",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cognolato_F/0/1/0/all/0/1\">Francesco Cognolato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+OBrien_K/0/1/0/all/0/1\">Kieran O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_J/0/1/0/all/0/1\">Jin Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robinson_S/0/1/0/all/0/1\">Simon Robinson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laun_F/0/1/0/all/0/1\">Frederik B. Laun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barth_M/0/1/0/all/0/1\">Markus Barth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bollmann_S/0/1/0/all/0/1\">Steffen Bollmann</a>",
          "description": "Deep learning based Quantitative Susceptibility Mapping (QSM) has shown great\npotential in recent years, outperforming traditional non-learning approaches in\nspeed and accuracy. However, many of the current deep learning approaches are\nnot data consistent, require in vivo training data or do not solve all steps of\nthe QSM processing pipeline. Here we aim to overcome these limitations and\ndeveloped a framework to solve the QSM processing steps jointly. We developed a\nnew hybrid training data generation method that enables the end-to-end training\nfor solving background field correction and dipole inversion in a\ndata-consistent fashion using a variational network that combines the QSM model\nterm and a learned regularizer. We demonstrate that NeXtQSM overcomes the\nlimitations of previous model-agnostic deep learning methods and show that\nNeXtQSM offers a complete deep learning based pipeline for computing robust,\nfast and accurate quantitative susceptibility maps.",
          "link": "http://arxiv.org/abs/2107.07752",
          "publishedOn": "2021-07-19T00:49:06.107Z",
          "wordCount": 613,
          "title": "NeXtQSM -- A complete deep learning pipeline for data-consistent quantitative susceptibility mapping trained with hybrid data. (arXiv:2107.07752v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07761",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mascolini_A/0/1/0/all/0/1\">Alessio Mascolini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardamone_D/0/1/0/all/0/1\">Dario Cardamone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ponzio_F/0/1/0/all/0/1\">Francesco Ponzio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cataldo_S/0/1/0/all/0/1\">Santa Di Cataldo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ficarra_E/0/1/0/all/0/1\">Elisa Ficarra</a>",
          "description": "Computer-aided analysis of biological images typically requires extensive\ntraining on large-scale annotated datasets, which is not viable in many\nsituations. In this paper we present GAN-DL, a Discriminator Learner based on\nthe StyleGAN2 architecture, which we employ for self-supervised image\nrepresentation learning in the case of fluorescent biological images. We show\nthat Wasserstein Generative Adversarial Networks combined with linear Support\nVector Machines enable high-throughput compound screening based on raw images.\nWe demonstrate this by classifying active and inactive compounds tested for the\ninhibition of SARS-CoV-2 infection in VERO and HRCE cell lines. In contrast to\nprevious methods, our deep learning based approach does not require any\nannotation besides the one that is normally collected during the sample\npreparation process. We test our technique on the RxRx19a Sars-CoV-2 image\ncollection. The dataset consists of fluorescent images that were generated to\nassess the ability of regulatory-approved or in late-stage clinical trials\ncompound to modulate the in vitro infection from SARS-CoV-2 in both VERO and\nHRCE cell lines. We show that our technique can be exploited not only for\nclassification tasks, but also to effectively derive a dose response curve for\nthe tested treatments, in a self-supervised manner. Lastly, we demonstrate its\ngeneralization capabilities by successfully addressing a zero-shot learning\ntask, consisting in the categorization of four different cell types of the\nRxRx1 fluorescent images collection.",
          "link": "http://arxiv.org/abs/2107.07761",
          "publishedOn": "2021-07-19T00:49:06.096Z",
          "wordCount": 733,
          "title": "Exploiting generative self-supervised learning for the assessment of biological images with lack of annotations: a COVID-19 case-study. (arXiv:2107.07761v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07714",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Lavrik_E/0/1/0/all/0/1\">E. Lavrik</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Shiroya_M/0/1/0/all/0/1\">M. Shiroya</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schmidt_H/0/1/0/all/0/1\">H.R. Schmidt</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Toia_A/0/1/0/all/0/1\">A. Toia</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Heuser_J/0/1/0/all/0/1\">J.M. Heuser</a>",
          "description": "Optical inspection of 1191 silicon micro-strip sensors was performed using a\ncustom made optical inspection setup, employing a machine-learning based\napproach for the defect analysis and subsequent quality assurance. Furthermore,\nmetrological control of the sensor's surface was performed. In this manuscript,\nwe present the analysis of various sensor surface defects. Among these are\nimplant breaks, p-stop breaks, aluminium strip opens, aluminium strip shorts,\nsurface scratches, double metallization layer defects, passivation layer\ndefects, bias resistor defects as well as dust particle identification. The\ndefect detection was done using the application of Convolutional Deep Neural\nNetworks (CDNNs). From this, defective strips and defect clusters were\nidentified, as well as a 2D map of the defects using their geometrical\npositions on the sensor was performed. Based on the total number of defects\nfound on the sensor's surface, a method for the estimation of sensor's overall\nquality grade and quality score was proposed.",
          "link": "http://arxiv.org/abs/2107.07714",
          "publishedOn": "2021-07-19T00:49:06.076Z",
          "wordCount": 610,
          "title": "Optical Inspection of the Silicon Micro-strip Sensors for the CBM Experiment employing Artificial Intelligence. (arXiv:2107.07714v1 [physics.ins-det])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1\">Zahra Gharaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1\">Shreyas Kowshik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1\">Oliver Stromann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>",
          "description": "We present a novel learning-based approach to graph representations of road\nnetworks employing state-of-the-art graph convolutional neural networks. Our\napproach is applied to realistic road networks of 17 cities from Open Street\nMap. While edge features are crucial to generate descriptive graph\nrepresentations of road networks, graph convolutional networks usually rely on\nnode features only. We show that the highly representative edge features can\nstill be integrated into such networks by applying a line graph transformation.\nWe also propose a method for neighborhood sampling based on a topological\nneighborhood composed of both local and global neighbors. We compare the\nperformance of learning representations using different types of neighborhood\naggregation functions in transductive and inductive tasks and in supervised and\nunsupervised learning. Furthermore, we propose a novel aggregation approach,\nGraph Attention Isomorphism Network, GAIN. Our results show that GAIN\noutperforms state-of-the-art methods on the road type classification problem.",
          "link": "http://arxiv.org/abs/2107.07791",
          "publishedOn": "2021-07-19T00:49:06.068Z",
          "wordCount": 587,
          "title": "Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_E/0/1/0/all/0/1\">Euijoon Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinman Kim</a>",
          "description": "Recent supervised deep learning methods have shown that heart rate can be\nmeasured remotely using facial videos. However, the performance of these\nsupervised method are dependent on the availability of large-scale labelled\ndata and they have been limited to 2D deep learning architectures that do not\nfully exploit the 3D spatiotemporal information. To solve this problem, we\npresent a novel 3D self-supervised spatiotemporal learning framework for remote\nHR estimation on facial videos. Concretely, we propose a landmark-based spatial\naugmentation which splits the face into several informative parts based on the\nShafer's dichromatic reflection model and a novel sparsity-based temporal\naugmentation exploiting Nyquist-Shannon sampling theorem to enhance the signal\nmodelling ability. We evaluated our method on 3 public datasets and\noutperformed other self-supervised methods and achieved competitive accuracy\nwith the state-of-the-art supervised methods.",
          "link": "http://arxiv.org/abs/2107.07695",
          "publishedOn": "2021-07-19T00:49:06.044Z",
          "wordCount": 572,
          "title": "Self-Supervised Learning Framework for Remote Heart Rate Estimation Using Spatiotemporal Augmentation. (arXiv:2107.07695v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Ming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_T/0/1/0/all/0/1\">Tobias Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunderhauf_N/0/1/0/all/0/1\">Niko S&#xfc;nderhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>",
          "description": "Probabilistic state-estimation approaches offer a principled foundation for\ndesigning localization systems, because they naturally integrate sequences of\nimperfect motion and exteroceptive sensor data. Recently, probabilistic\nlocalization systems utilizing appearance-invariant visual place recognition\n(VPR) methods as the primary exteroceptive sensor have demonstrated\nstate-of-the-art performance in the presence of substantial appearance change.\nHowever, existing systems 1) do not fully utilize odometry data within the\nmotion models, and 2) are unable to handle route deviations, due to the\nassumption that query traverses exactly repeat the mapping traverse. To address\nthese shortcomings, we present a new probabilistic topometric localization\nsystem which incorporates full 3-dof odometry into the motion model and\nfurthermore, adds an \"off-map\" state within the state-estimation framework,\nallowing query traverses which feature significant route detours from the\nreference map to be successfully localized. We perform extensive evaluation on\nmultiple query traverses from the Oxford RobotCar dataset exhibiting both\nsignificant appearance change and deviations from routes previously traversed.\nIn particular, we evaluate performance on two practically relevant localization\ntasks: loop closure detection and global localization. Our approach achieves\nmajor performance improvements over both existing and improved state-of-the-art\nsystems.",
          "link": "http://arxiv.org/abs/2107.07707",
          "publishedOn": "2021-07-19T00:49:06.038Z",
          "wordCount": 634,
          "title": "Probabilistic Appearance-Invariant Topometric Localization with New Place Awareness. (arXiv:2107.07707v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaojian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wuyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuchen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yonggan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingyan Lin</a>",
          "description": "Semantic segmentation for scene understanding is nowadays widely demanded,\nraising significant challenges for the algorithm efficiency, especially its\napplications on resource-limited platforms. Current segmentation models are\ntrained and evaluated on massive high-resolution scene images (\"data level\")\nand suffer from the expensive computation arising from the required multi-scale\naggregation(\"network level\"). In both folds, the computational and energy costs\nin training and inference are notable due to the often desired large input\nresolutions and heavy computational burden of segmentation models. To this end,\nwe propose DANCE, general automated DAta-Network Co-optimization for Efficient\nsegmentation model training and inference. Distinct from existing efficient\nsegmentation approaches that focus merely on light-weight network design, DANCE\ndistinguishes itself as an automated simultaneous data-network co-optimization\nvia both input data manipulation and network architecture slimming.\nSpecifically, DANCE integrates automated data slimming which adaptively\ndownsamples/drops input images and controls their corresponding contribution to\nthe training loss guided by the images' spatial complexity. Such a downsampling\noperation, in addition to slimming down the cost associated with the input size\ndirectly, also shrinks the dynamic range of input object and context scales,\ntherefore motivating us to also adaptively slim the network to match the\ndownsampled data. Extensive experiments and ablating studies (on four SOTA\nsegmentation models with three popular segmentation datasets under two training\nsettings) demonstrate that DANCE can achieve \"all-win\" towards efficient\nsegmentation(reduced training cost, less expensive inference, and better mean\nIntersection-over-Union (mIoU)).",
          "link": "http://arxiv.org/abs/2107.07706",
          "publishedOn": "2021-07-19T00:49:06.031Z",
          "wordCount": 687,
          "title": "DANCE: DAta-Network Co-optimization for Efficient Segmentation Model Training and Inference. (arXiv:2107.07706v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_M/0/1/0/all/0/1\">Mann Patel</a>",
          "description": "Violence rates however have been brought down about 57% during the span of\nthe past 4 decades yet it doesn't change the way that the demonstration of\nviolence actually happens, unseen by the law. Violence can be mass controlled\nsometimes by higher authorities, however, to hold everything in line one must\n\"Microgovern\" over each movement occurring in every road of each square. To\naddress the butterfly effects impact in our setting, I made a unique model and\na theorized system to handle the issue utilizing deep learning. The model takes\nthe input of the CCTV video feeds and after drawing inference, recognizes if a\nviolent movement is going on. And hypothesized architecture aims towards\nprobability-driven computation of video feeds and reduces overhead from naively\ncomputing for every CCTV video feeds.",
          "link": "http://arxiv.org/abs/2107.07578",
          "publishedOn": "2021-07-19T00:49:06.004Z",
          "wordCount": 561,
          "title": "Real-Time Violence Detection Using CNN-LSTM. (arXiv:2107.07578v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07596",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lo_C/0/1/0/all/0/1\">Chen-Chou Lo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vandewalle_P/0/1/0/all/0/1\">Patrick Vandewalle</a>",
          "description": "We integrate sparse radar data into a monocular depth estimation model and\nintroduce a novel preprocessing method for reducing the sparseness and limited\nfield of view provided by radar. We explore the intrinsic error of different\nradar modalities and show our proposed method results in more data points with\nreduced error. We further propose a novel method for estimating dense depth\nmaps from monocular 2D images and sparse radar measurements using deep learning\nbased on the deep ordinal regression network by Fu et al. Radar data are\nintegrated by first converting the sparse 2D points to a height-extended 3D\nmeasurement and then including it into the network using a late fusion\napproach. Experiments are conducted on the nuScenes dataset. Our experiments\ndemonstrate state-of-the-art performance in both day and night scenes.",
          "link": "http://arxiv.org/abs/2107.07596",
          "publishedOn": "2021-07-19T00:49:05.988Z",
          "wordCount": 593,
          "title": "Depth Estimation from Monocular Images and Sparse radar using Deep Ordinal Regression Network. (arXiv:2107.07596v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_S/0/1/0/all/0/1\">Saravanabalagi Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1\">John McDonald</a>",
          "description": "OdoViz is a reactive web-based tool for 3D visualization and processing of\nautonomous vehicle datasets designed to support common tasks in visual place\nrecognition research. The system includes functionality for loading,\ninspecting, visualizing, and processing GPS/INS poses, point clouds and camera\nimages. It supports a number of commonly used driving datasets and can be\nadapted to load custom datasets with minimal effort. OdoViz's design consists\nof a slim server to serve the datasets coupled with a rich client frontend.\nThis design supports multiple deployment configurations including single user\nstand-alone installations, research group installations serving datasets\ninternally across a lab, or publicly accessible web-frontends for providing\nonline interfaces for exploring and interacting with datasets. The tool allows\nviewing complete vehicle trajectories traversed at multiple different time\nperiods simultaneously, facilitating tasks such as sub-sampling, comparing and\nfinding pose correspondences both across and within sequences. This\nsignificantly reduces the effort required in creating subsets of data from\nexisting datasets for machine learning tasks. Further to the above, the system\nalso supports adding custom extensions and plugins to extend the capabilities\nof the software for other potential data management, visualization and\nprocessing tasks. The platform has been open-sourced to promote its use and\nencourage further contributions from the research community.",
          "link": "http://arxiv.org/abs/2107.07557",
          "publishedOn": "2021-07-19T00:49:05.971Z",
          "wordCount": 644,
          "title": "OdoViz: A 3D Odometry Visualization and Processing Tool. (arXiv:2107.07557v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zida Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>",
          "description": "3D hand-object pose estimation is an important issue to understand the\ninteraction between human and environment. Current hand-object pose estimation\nmethods require detailed 3D labels, which are expensive and labor-intensive. To\ntackle the problem of data collection, we propose a semi-supervised 3D\nhand-object pose estimation method with two key techniques: pose dictionary\nlearning and an object-oriented coordinate system. The proposed pose dictionary\nlearning module can distinguish infeasible poses by reconstruction error,\nenabling unlabeled data to provide supervision signals. The proposed\nobject-oriented coordinate system can make 3D estimations equivariant to the\ncamera perspective. Experiments are conducted on FPHA and HO-3D datasets. Our\nmethod reduces estimation error by 19.5% / 24.9% for hands/objects compared to\nstraightforward use of labeled data on FPHA and outperforms several baseline\nmethods. Extensive experiments also validate the robustness of the proposed\nmethod.",
          "link": "http://arxiv.org/abs/2107.07676",
          "publishedOn": "2021-07-19T00:49:05.963Z",
          "wordCount": 571,
          "title": "Semi-supervised 3D Hand-Object Pose Estimation via Pose Dictionary Learning. (arXiv:2107.07676v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colbert_I/0/1/0/all/0/1\">Ian Colbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutz_Delgado_K/0/1/0/all/0/1\">Ken Kreutz-Delgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srinjoy Das</a>",
          "description": "A novel energy-efficient edge computing paradigm is proposed for real-time\ndeep learning-based image upsampling applications. State-of-the-art deep\nlearning solutions for image upsampling are currently trained using either\nresize or sub-pixel convolution to learn kernels that generate high fidelity\nimages with minimal artifacts. However, performing inference with these learned\nconvolution kernels requires memory-intensive feature map transformations that\ndominate time and energy costs in real-time applications. To alleviate this\npressure on memory bandwidth, we confine the use of resize or sub-pixel\nconvolution to training in the cloud by transforming learned convolution\nkernels to deconvolution kernels before deploying them for inference as a\nfunctionally equivalent deconvolution. These kernel transformations, intended\nas a one-time cost when shifting from training to inference, enable a systems\ndesigner to use each algorithm in their optimal context by preserving the image\nfidelity learned when training in the cloud while minimizing data transfer\npenalties during inference at the edge. We also explore existing variants of\ndeconvolution inference algorithms and introduce a novel variant for\nconsideration. We analyze and compare the inference properties of\nconvolution-based upsampling algorithms using a quantitative model of incurred\ntime and energy costs and show that using deconvolution for inference at the\nedge improves both system latency and energy efficiency when compared to their\nsub-pixel or resize convolution counterparts.",
          "link": "http://arxiv.org/abs/2107.07647",
          "publishedOn": "2021-07-19T00:49:05.921Z",
          "wordCount": 664,
          "title": "An Energy-Efficient Edge Computing Paradigm for Convolution-based Image Upsampling. (arXiv:2107.07647v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferdous_R/0/1/0/all/0/1\">Refat E Ferdous</a>",
          "description": "During the COVID-19 pandemic, most of the human-to-human interactions have\nbeen stopped. To mitigate the spread of deadly coronavirus, many offices took\nthe initiative so that the employees can work from home. But, tracking the\nemployees and finding out if they are really performing what they were supposed\nto turn out to be a serious challenge for all the companies and organizations\nwho are facilitating \"Work From Home\". To deal with the challenge effectively,\nwe came up with a solution to track the employees with face recognition. We\nhave been testing this system experimentally for our office. To train the face\nrecognition module, we used FaceNet with KNN using the Labeled Faces in the\nWild (LFW) dataset and achieved 97.8% accuracy. We integrated the trained model\ninto our central system, where the employees log their time. In this paper, we\ndiscuss in brief the system we have been experimenting with and the pros and\ncons of the system.",
          "link": "http://arxiv.org/abs/2107.07576",
          "publishedOn": "2021-07-19T00:49:05.915Z",
          "wordCount": 666,
          "title": "Real-Time Face Recognition System for Remote Employee Tracking. (arXiv:2107.07576v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07608",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>",
          "description": "Contrastive learning is a discriminative approach that aims at grouping\nsimilar samples closer and diverse samples far from each other. It it an\nefficient technique to train an encoder generating distinguishable and\ninformative representations, and it may even increase the encoder's\ntransferability. Most current applications of contrastive learning benefit only\na single representation from the last layer of an encoder.In this paper, we\npropose a multi-level contrasitive learning approach which applies contrastive\nlosses at different layers of an encoder to learn multiple representations from\nthe encoder. Afterward, an ensemble can be constructed to take advantage of the\nmultiple representations for the downstream tasks. We evaluated the proposed\nmethod on few-shot learning problems and conducted experiments using the\nmini-ImageNet and the tiered-ImageNet datasets. Our model achieved the new\nstate-of-the-art results for both datasets, comparing to previous regular,\nensemble, and contrastive learing (single-level) based approaches.",
          "link": "http://arxiv.org/abs/2107.07608",
          "publishedOn": "2021-07-19T00:49:05.614Z",
          "wordCount": 572,
          "title": "Multi-Level Contrastive Learning for Few-Shot Problems. (arXiv:2107.07608v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.08177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lara_J/0/1/0/all/0/1\">Juan S. Lara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1\">Fabio A. Gonz&#xe1;lez</a>",
          "description": "The dissimilarity mixture autoencoder (DMAE) is a neural network model for\nfeature-based clustering that incorporates a flexible dissimilarity function\nand can be integrated into any kind of deep learning architecture. It\ninternally represents a dissimilarity mixture model (DMM) that extends\nclassical methods like K-Means, Gaussian mixture models, or Bregman clustering\nto any convex and differentiable dissimilarity function through the\nreinterpretation of probabilities as neural network representations. DMAE can\nbe integrated with deep learning architectures into end-to-end models, allowing\nthe simultaneous estimation of the clustering and neural network's parameters.\nExperimental evaluation was performed on image and text clustering benchmark\ndatasets showing that DMAE is competitive in terms of unsupervised\nclassification accuracy and normalized mutual information. The source code with\nthe implementation of DMAE is publicly available at:\nhttps://github.com/juselara1/dmae",
          "link": "http://arxiv.org/abs/2006.08177",
          "publishedOn": "2021-07-16T00:48:26.226Z",
          "wordCount": 622,
          "title": "Dissimilarity Mixture Autoencoder for Deep Clustering. (arXiv:2006.08177v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yinong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haonan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jingwen Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ru_L/0/1/0/all/0/1\">Lixiang Ru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongruixuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>",
          "description": "In order to mitigate the spread of COVID-19, Wuhan was the first city to\nimplement strict lockdown policy in 2020. Even though numerous researches have\ndiscussed the travel restriction between cities and provinces, few studies\nfocus on the effect of transportation control inside the city due to the lack\nof the measurement and available data in Wuhan. Since the public transports\nhave been shut down in the beginning of city lockdown, the change of traffic\ndensity is a good indicator to reflect the intracity population flow.\nTherefore, in this paper, we collected time-series high-resolution remote\nsensing images with the resolution of 1m acquired before, during and after\nWuhan lockdown by GF-2 satellite. Vehicles on the road were extracted and\ncounted for the statistics of traffic density to reflect the changes of human\ntransmissions in the whole period of Wuhan lockdown. Open Street Map was used\nto obtain observation road surfaces, and a vehicle detection method combing\nmorphology filter and deep learning was utilized to extract vehicles with the\naccuracy of 62.56%. According to the experimental results, the traffic density\nof Wuhan dropped with the percentage higher than 80%, and even higher than 90%\non main roads during city lockdown; after lockdown lift, the traffic density\nrecovered to the normal rate. Traffic density distributions also show the\nobvious reduction and increase throughout the whole study area. The significant\nreduction and recovery of traffic density indicates that the lockdown policy in\nWuhan show effectiveness in controlling human transmission inside the city, and\nthe city returned to normal after lockdown lift.",
          "link": "http://arxiv.org/abs/2006.16098",
          "publishedOn": "2021-07-16T00:48:25.974Z",
          "wordCount": 813,
          "title": "An Investigation of Traffic Density Changes inside Wuhan during the COVID-19 Epidemic with GF-2 Time-Series Images. (arXiv:2006.16098v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.02068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Rakesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengcheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_P/0/1/0/all/0/1\">Peter Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meghanath_G/0/1/0/all/0/1\">Ganga Meghanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaterji_S/0/1/0/all/0/1\">Somali Chaterji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1\">Subrata Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1\">Saurabh Bagchi</a>",
          "description": "Videos take a lot of time to transport over the network, hence running\nanalytics on the live video on embedded or mobile devices has become an\nimportant system driver. Considering that such devices, e.g., surveillance\ncameras or AR/VR gadgets, are resource constrained, creating lightweight deep\nneural networks (DNNs) for embedded devices is crucial. None of the current\napproximation techniques for object classification DNNs can adapt to changing\nruntime conditions, e.g., changes in resource availability on the device, the\ncontent characteristics, or requirements from the user. In this paper, we\nintroduce ApproxNet, a video object classification system for embedded or\nmobile clients. It enables novel dynamic approximation techniques to achieve\ndesired inference latency and accuracy trade-off under changing runtime\nconditions. It achieves this by enabling two approximation knobs within a\nsingle DNN model, rather than creating and maintaining an ensemble of models\n(e.g., MCDNN [MobiSys-16]. We show that ApproxNet can adapt seamlessly at\nruntime to these changes, provides low and stable latency for the image and\nvideo frame classification problems, and show the improvement in accuracy and\nlatency over ResNet [CVPR-16], MCDNN [MobiSys-16], MobileNets [Google-17],\nNestDNN [MobiCom-18], and MSDNet [ICLR-18].",
          "link": "http://arxiv.org/abs/1909.02068",
          "publishedOn": "2021-07-16T00:48:25.719Z",
          "wordCount": 717,
          "title": "ApproxNet: Content and Contention-Aware Video Analytics System for Embedded Clients. (arXiv:1909.02068v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aouayeb_M/0/1/0/all/0/1\">Mouath Aouayeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soladie_C/0/1/0/all/0/1\">Catherine Soladie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kpalma_K/0/1/0/all/0/1\">Kidiyo Kpalma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seguier_R/0/1/0/all/0/1\">Renaud Seguier</a>",
          "description": "As various databases of facial expressions have been made accessible over the\nlast few decades, the Facial Expression Recognition (FER) task has gotten a lot\nof interest. The multiple sources of the available databases raised several\nchallenges for facial recognition task. These challenges are usually addressed\nby Convolution Neural Network (CNN) architectures. Different from CNN models, a\nTransformer model based on attention mechanism has been presented recently to\naddress vision tasks. One of the major issue with Transformers is the need of a\nlarge data for training, while most FER databases are limited compared to other\nvision applications. Therefore, we propose in this paper to learn a vision\nTransformer jointly with a Squeeze and Excitation (SE) block for FER task. The\nproposed method is evaluated on different publicly available FER databases\nincluding CK+, JAFFE,RAF-DB and SFEW. Experiments demonstrate that our model\noutperforms state-of-the-art methods on CK+ and SFEW and achieves competitive\nresults on JAFFE and RAF-DB.",
          "link": "http://arxiv.org/abs/2107.03107",
          "publishedOn": "2021-07-16T00:48:25.308Z",
          "wordCount": 625,
          "title": "Learning Vision Transformer with Squeeze and Excitation for Facial Expression Recognition. (arXiv:2107.03107v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mier_J/0/1/0/all/0/1\">Juan Carlos Mier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_E/0/1/0/all/0/1\">Eddie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talebi_H/0/1/0/all/0/1\">Hossein Talebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>",
          "description": "Lossy Image compression is necessary for efficient storage and transfer of\ndata. Typically the trade-off between bit-rate and quality determines the\noptimal compression level. This makes the image quality metric an integral part\nof any imaging system. While the existing full-reference metrics such as PSNR\nand SSIM may be less sensitive to perceptual quality, the recently introduced\nlearning methods may fail to generalize to unseen data. In this paper we\npropose the largest image compression quality dataset to date with human\nperceptual preferences, enabling the use of deep learning, and we develop a\nfull reference perceptual quality assessment metric for lossy image compression\nthat outperforms the existing state-of-the-art methods. We show that the\nproposed model can effectively learn from thousands of examples available in\nthe new dataset, and consequently it generalizes better to other unseen\ndatasets of human perceptual preference.",
          "link": "http://arxiv.org/abs/2103.01114",
          "publishedOn": "2021-07-16T00:48:24.289Z",
          "wordCount": 611,
          "title": "Deep Perceptual Image Quality Assessment for Compression. (arXiv:2103.01114v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1\">Anqi Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>",
          "description": "Markerless motion capture and understanding of professional non-daily human\nmovements is an important yet unsolved task, which suffers from complex motion\npatterns and severe self-occlusion, especially for the monocular setting. In\nthis paper, we propose SportsCap -- the first approach for simultaneously\ncapturing 3D human motions and understanding fine-grained actions from\nmonocular challenging sports video input. Our approach utilizes the semantic\nand temporally structured sub-motion prior in the embedding space for motion\ncapture and understanding in a data-driven multi-task manner. To enable robust\ncapture under complex motion patterns, we propose an effective motion embedding\nmodule to recover both the implicit motion embedding and explicit 3D motion\ndetails via a corresponding mapping function as well as a sub-motion\nclassifier. Based on such hybrid motion information, we introduce a\nmulti-stream spatial-temporal Graph Convolutional Network(ST-GCN) to predict\nthe fine-grained semantic action attributes, and adopt a semantic attribute\nmapping block to assemble various correlated action attributes into a\nhigh-level action label for the overall detailed understanding of the whole\nsequence, so as to enable various applications like action assessment or motion\nscoring. Comprehensive experiments on both public and our proposed datasets\nshow that with a challenging monocular sports video input, our novel approach\nnot only significantly improves the accuracy of 3D human motion capture, but\nalso recovers accurate fine-grained semantic action attributes.",
          "link": "http://arxiv.org/abs/2104.11452",
          "publishedOn": "2021-07-16T00:48:24.247Z",
          "wordCount": 713,
          "title": "SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos. (arXiv:2104.11452v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Donghuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jiangpeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagadeesan_J/0/1/0/all/0/1\">Jayender Jagadeesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wells_W/0/1/0/all/0/1\">William Wells III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frisken_S/0/1/0/all/0/1\">Sarah Frisken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_R/0/1/0/all/0/1\">Raymond Kai-yu Tong</a>",
          "description": "In order to tackle the difficulty associated with the ill-posed nature of the\nimage registration problem, researchers use regularization to constrain the\nsolution space. For most learning-based registration approaches, the\nregularization usually has a fixed weight and only constrains the spatial\ntransformation. Such convention has two limitations: (1) The regularization\nstrength of a specific image pair should be associated with the content of the\nimages, thus the ``one value fits all'' scheme is not ideal; (2) Only spatially\nregularizing the transformation (but overlooking the temporal consistency of\ndifferent estimations) may not be the best strategy to cope with the\nill-posedness. In this study, we propose a mean-teacher based registration\nframework. This framework incorporates an additional \\textit{temporal\nregularization} term by encouraging the teacher model's temporal ensemble\nprediction to be consistent with that of the student model. At each training\nstep, it also automatically adjusts the weights of the \\textit{spatial\nregularization} and the \\textit{temporal regularization} by taking account of\nthe transformation uncertainty and appearance uncertainty derived from the\nperturbed teacher model. We perform experiments on multi- and uni-modal\nregistration tasks, and the results show that our strategy outperforms the\ntraditional and learning-based benchmark methods.",
          "link": "http://arxiv.org/abs/2107.02433",
          "publishedOn": "2021-07-16T00:48:24.188Z",
          "wordCount": 675,
          "title": "Double-Uncertainty Assisted Spatial and Temporal Regularization Weighting for Learning-based Registration. (arXiv:2107.02433v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1\">Tero Karras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1\">Miika Aittala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1\">Samuli Laine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1\">Erik H&#xe4;rk&#xf6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1\">Janne Hellsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1\">Jaakko Lehtinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1\">Timo Aila</a>",
          "description": "We observe that despite their hierarchical convolutional nature, the\nsynthesis process of typical generative adversarial networks depends on\nabsolute pixel coordinates in an unhealthy manner. This manifests itself as,\ne.g., detail appearing to be glued to image coordinates instead of the surfaces\nof depicted objects. We trace the root cause to careless signal processing that\ncauses aliasing in the generator network. Interpreting all signals in the\nnetwork as continuous, we derive generally applicable, small architectural\nchanges that guarantee that unwanted information cannot leak into the\nhierarchical synthesis process. The resulting networks match the FID of\nStyleGAN2 but differ dramatically in their internal representations, and they\nare fully equivariant to translation and rotation even at subpixel scales. Our\nresults pave the way for generative models better suited for video and\nanimation.",
          "link": "http://arxiv.org/abs/2106.12423",
          "publishedOn": "2021-07-16T00:48:24.167Z",
          "wordCount": 614,
          "title": "Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>",
          "description": "We present CSWin Transformer, an efficient and effective Transformer-based\nbackbone for general-purpose vision tasks. A challenging issue in Transformer\ndesign is that global self-attention is very expensive to compute whereas local\nself-attention often limits the field of interactions of each token. To address\nthis issue, we develop the Cross-Shaped Window self-attention mechanism for\ncomputing self-attention in the horizontal and vertical stripes in parallel\nthat form a cross-shaped window, with each stripe obtained by splitting the\ninput feature into stripes of equal width. We provide a detailed mathematical\nanalysis of the effect of the stripe width and vary the stripe width for\ndifferent layers of the Transformer network which achieves strong modeling\ncapability while limiting the computation cost. We also introduce\nLocally-enhanced Positional Encoding (LePE), which handles the local positional\ninformation better than existing encoding schemes. LePE naturally supports\narbitrary input resolutions, and is thus especially effective and friendly for\ndownstream tasks. Incorporated with these designs and a hierarchical structure,\nCSWin Transformer demonstrates competitive performance on common vision tasks.\nSpecifically, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra\ntraining data or label, 53.9 box AP and 46.4 mask AP on the COCO detection\ntask, and 51.7 mIOU on the ADE20K semantic segmentation task, surpassing\nprevious state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and\n+2.0 respectively under the similar FLOPs setting. By further pretraining on\nthe larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K\nand state-of-the-art segmentation performance on ADE20K with 55.7 mIoU. The\ncode and models will be available at\nhttps://github.com/microsoft/CSWin-Transformer.",
          "link": "http://arxiv.org/abs/2107.00652",
          "publishedOn": "2021-07-16T00:48:24.161Z",
          "wordCount": 744,
          "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08757",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Wenjian Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_Y/0/1/0/all/0/1\">Yiran Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Price_S/0/1/0/all/0/1\">Stephen J. Price</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>",
          "description": "We present an Expectation-Maximization (EM) Regularized Deep Learning\n(EMReDL) model for weakly supervised tumor segmentation. The proposed framework\nis tailored to glioblastoma, a type of malignant tumor characterized by its\ndiffuse infiltration into the surrounding brain tissue, which poses significant\nchallenge to treatment target and tumor burden estimation using conventional\nstructural MRI. Although physiological MRI provides more specific information\nregarding tumor infiltration, the relatively low resolution hinders a precise\nfull annotation. This has motivated us to develop a weakly supervised deep\nlearning solution that exploits the partial labelled tumor regions.\n\nEMReDL contains two components: a physiological prior prediction model and\nEM-regularized segmentation model. The physiological prior prediction model\nexploits the physiological MRI by training a classifier to generate a\nphysiological prior map. This map is passed to the segmentation model for\nregularization using the EM algorithm. We evaluated the model on a glioblastoma\ndataset with the pre-operative multiparametric and recurrence MRI available.\nEMReDL showed to effectively segment the infiltrated tumor from the partially\nlabelled region of potential infiltration. The segmented core tumor and\ninfiltrated tumor demonstrated high consistency with the tumor burden labelled\nby experts. The performance comparisons showed that EMReDL achieved higher\naccuracy than published state-of-the-art models. On MR spectroscopy, the\nsegmented region displayed more aggressive features than other partial labelled\nregion. The proposed model can be generalized to other segmentation tasks that\nrely on partial labels, with the CNN architecture flexible in the framework.",
          "link": "http://arxiv.org/abs/2101.08757",
          "publishedOn": "2021-07-16T00:48:24.154Z",
          "wordCount": 731,
          "title": "Expectation-Maximization Regularized Deep Learning for Weakly Supervised Tumor Segmentation for Glioblastoma. (arXiv:2101.08757v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parihar_U/0/1/0/all/0/1\">Udit Singh Parihar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gujarathi_A/0/1/0/all/0/1\">Aniket Gujarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_K/0/1/0/all/0/1\">Kinal Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tourani_S/0/1/0/all/0/1\">Satyajit Tourani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">K. Madhava Krishna</a>",
          "description": "The use of local detectors and descriptors in typical computer vision\npipelines work well until variations in viewpoint and appearance change become\nextreme. Past research in this area has typically focused on one of two\napproaches to this challenge: the use of projections into spaces more suitable\nfor feature matching under extreme viewpoint changes, and attempting to learn\nfeatures that are inherently more robust to viewpoint change. In this paper, we\npresent a novel framework that combines learning of invariant descriptors\nthrough data augmentation and orthographic viewpoint projection. We propose\nrotation-robust local descriptors, learnt through training data augmentation\nbased on rotation homographies, and a correspondence ensemble technique that\ncombines vanilla feature correspondences with those obtained through\nrotation-robust features. Using a range of benchmark datasets as well as\ncontributing a new bespoke dataset for this research domain, we evaluate the\neffectiveness of the proposed approach on key tasks including pose estimation\nand visual place recognition. Our system outperforms a range of baseline and\nstate-of-the-art techniques, including enabling higher levels of place\nrecognition precision across opposing place viewpoints and achieves\npractically-useful performance levels even under extreme viewpoint changes.",
          "link": "http://arxiv.org/abs/2103.08573",
          "publishedOn": "2021-07-16T00:48:24.116Z",
          "wordCount": 677,
          "title": "RoRD: Rotation-Robust Descriptors and Orthographic Views for Local Feature Matching. (arXiv:2103.08573v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Anqi Joyce Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barsan_I/0/1/0/all/0/1\">Ioan Andrei B&#xe2;rsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1\">Raquel Urtasun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shenlong Wang</a>",
          "description": "Existing multi-camera SLAM systems assume synchronized shutters for all\ncameras, which is often not the case in practice. In this work, we propose a\ngeneralized multi-camera SLAM formulation which accounts for asynchronous\nsensor observations. Our framework integrates a continuous-time motion model to\nrelate information across asynchronous multi-frames during tracking, local\nmapping, and loop closing. For evaluation, we collected AMV-Bench, a\nchallenging new SLAM dataset covering 482 km of driving recorded using our\nasynchronous multi-camera robotic platform. AMV-Bench is over an order of\nmagnitude larger than previous multi-view HD outdoor SLAM datasets, and covers\ndiverse and challenging motions and environments. Our experiments emphasize the\nnecessity of asynchronous sensor modeling, and show that the use of multiple\ncameras is critical towards robust and accurate SLAM in challenging outdoor\nscenes. For additional information, please see the project website at:\nhttps://www.cs.toronto.edu/~ajyang/amv-slam",
          "link": "http://arxiv.org/abs/2101.06562",
          "publishedOn": "2021-07-16T00:48:24.104Z",
          "wordCount": 620,
          "title": "Asynchronous Multi-View SLAM. (arXiv:2101.06562v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.05722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yunlong_Z/0/1/0/all/0/1\">Zhang Yunlong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenxin_L/0/1/0/all/0/1\">Li Chenxin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_L/0/1/0/all/0/1\">Lin Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liyan_S/0/1/0/all/0/1\">Sun Liyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yihong_Z/0/1/0/all/0/1\">Zhuang Yihong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_H/0/1/0/all/0/1\">Huang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xinghao_D/0/1/0/all/0/1\">Ding Xinghao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiaoqing_L/0/1/0/all/0/1\">Liu Xiaoqing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yizhou_Y/0/1/0/all/0/1\">Yu Yizhou</a>",
          "description": "This paper investigates the problem of pseudo-healthy synthesis that is\ndefined as synthesizing a subject-specific pathology-free image from a\npathological one. Recent approaches based on Generative Adversarial Network\n(GAN) have been developed for this task. However, these methods will inevitably\nfall into the trade-off between preserving the subject-specific identity and\ngenerating healthy-like appearances. To overcome this challenge, we propose a\nnovel adversarial training regime, Generator versus Segmentor (GVS), to\nalleviate this trade-off by a divide-and-conquer strategy. We further consider\nthe deteriorating generalization performance of the segmentor throughout the\ntraining and develop a pixel-wise weighted loss by muting the well-transformed\npixels to promote it. Moreover, we propose a new metric to measure how healthy\nthe synthetic images look. The qualitative and quantitative experiments on the\npublic dataset BraTS demonstrate that the proposed method outperforms the\nexisting methods. Besides, we also certify the effectiveness of our method on\ndatasets LiTS. Our implementation and pre-trained networks are publicly\navailable at https://github.com/Au3C2/Generator-Versus-Segmentor.",
          "link": "http://arxiv.org/abs/2009.05722",
          "publishedOn": "2021-07-16T00:48:24.080Z",
          "wordCount": 644,
          "title": "Generator Versus Segmentor: Pseudo-healthy Synthesis. (arXiv:2009.05722v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.02766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arun_N/0/1/0/all/0/1\">Nishanth Arun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaw_N/0/1/0/all/0/1\">Nathan Gaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Praveer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Ken Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1\">Mehak Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bryan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoebel_K/0/1/0/all/0/1\">Katharina Hoebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sharut Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_J/0/1/0/all/0/1\">Jay Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gidwani_M/0/1/0/all/0/1\">Mishka Gidwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adebayo_J/0/1/0/all/0/1\">Julius Adebayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Matthew D. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>",
          "description": "Saliency maps have become a widely used method to make deep learning models\nmore interpretable by providing post-hoc explanations of classifiers through\nidentification of the most pertinent areas of the input medical image. They are\nincreasingly being used in medical imaging to provide clinically plausible\nexplanations for the decisions the neural network makes. However, the utility\nand robustness of these visualization maps has not yet been rigorously examined\nin the context of medical imaging. We posit that trustworthiness in this\ncontext requires 1) localization utility, 2) sensitivity to model weight\nrandomization, 3) repeatability, and 4) reproducibility. Using the localization\ninformation available in two large public radiology datasets, we quantify the\nperformance of eight commonly used saliency map approaches for the above\ncriteria using area under the precision-recall curves (AUPRC) and structural\nsimilarity index (SSIM), comparing their performance to various baseline\nmeasures. Using our framework to quantify the trustworthiness of saliency maps,\nwe show that all eight saliency map techniques fail at least one of the\ncriteria and are, in most cases, less trustworthy when compared to the\nbaselines. We suggest that their usage in the high-risk domain of medical\nimaging warrants additional scrutiny and recommend that detection or\nsegmentation models be used if localization is the desired output of the\nnetwork. Additionally, to promote reproducibility of our findings, we provide\nthe code we used for all tests performed in this work at this link:\nhttps://github.com/QTIM-Lab/Assessing-Saliency-Maps.",
          "link": "http://arxiv.org/abs/2008.02766",
          "publishedOn": "2021-07-16T00:48:24.064Z",
          "wordCount": 735,
          "title": "Assessing the (Un)Trustworthiness of Saliency Maps for Localizing Abnormalities in Medical Imaging. (arXiv:2008.02766v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kafri_O/0/1/0/all/0/1\">Omer Kafri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patashnik_O/0/1/0/all/0/1\">Or Patashnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alaluf_Y/0/1/0/all/0/1\">Yuval Alaluf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>",
          "description": "We present StyleFusion, a new mapping architecture for StyleGAN, which takes\nas input a number of latent codes and fuses them into a single style code.\nInserting the resulting style code into a pre-trained StyleGAN generator\nresults in a single harmonized image in which each semantic region is\ncontrolled by one of the input latent codes. Effectively, StyleFusion yields a\ndisentangled representation of the image, providing fine-grained control over\neach region of the generated image. Moreover, to help facilitate global control\nover the generated image, a special input latent code is incorporated into the\nfused representation. StyleFusion operates in a hierarchical manner, where each\nlevel is tasked with learning to disentangle a pair of image regions (e.g., the\ncar body and wheels). The resulting learned disentanglement allows one to\nmodify both local, fine-grained semantics (e.g., facial features) as well as\nmore global features (e.g., pose and background), providing improved\nflexibility in the synthesis process. As a natural extension, StyleFusion\nenables one to perform semantically-aware cross-image mixing of regions that\nare not necessarily aligned. Finally, we demonstrate how StyleFusion can be\npaired with existing editing techniques to more faithfully constrain the edit\nto the user's region of interest.",
          "link": "http://arxiv.org/abs/2107.07437",
          "publishedOn": "2021-07-16T00:48:24.050Z",
          "wordCount": 641,
          "title": "StyleFusion: A Generative Model for Disentangling Spatial Segments. (arXiv:2107.07437v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.05383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Sheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe-Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-An Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Siang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Ya-Liang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>",
          "description": "We study the XAI (explainable AI) on the face recognition task, particularly\nthe face verification here. Face verification is a crucial task in recent days\nand it has been deployed to plenty of applications, such as access control,\nsurveillance, and automatic personal log-on for mobile devices. With the\nincreasing amount of data, deep convolutional neural networks can achieve very\nhigh accuracy for the face verification task. Beyond exceptional performances,\ndeep face verification models need more interpretability so that we can trust\nthe results they generate. In this paper, we propose a novel similarity metric,\ncalled explainable cosine ($xCos$), that comes with a learnable module that can\nbe plugged into most of the verification models to provide meaningful\nexplanations. With the help of $xCos$, we can see which parts of the two input\nfaces are similar, where the model pays its attention to, and how the local\nsimilarities are weighted to form the output $xCos$ score. We demonstrate the\neffectiveness of our proposed method on LFW and various competitive benchmarks,\nresulting in not only providing novel and desiring model interpretability for\nface verification but also ensuring the accuracy as plugging into existing face\nrecognition models.",
          "link": "http://arxiv.org/abs/2003.05383",
          "publishedOn": "2021-07-16T00:48:24.013Z",
          "wordCount": 699,
          "title": "xCos: An Explainable Cosine Metric for Face Verification Task. (arXiv:2003.05383v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wisth_D/0/1/0/all/0/1\">David Wisth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camurri_M/0/1/0/all/0/1\">Marco Camurri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallon_M/0/1/0/all/0/1\">Maurice Fallon</a>",
          "description": "We present VILENS (Visual Inertial Lidar Legged Navigation System), an\nodometry system for legged robots based on factor graphs. The key novelty is\nthe tight fusion of four different sensor modalities to achieve reliable\noperation when the individual sensors would otherwise produce degenerate\nestimation. To minimize leg odometry drift, we extend the robot's state with a\nlinear velocity bias term which is estimated online. This bias is only\nobservable because of the tight fusion of this preintegrated velocity factor\nwith vision, lidar, and IMU factors. Extensive experimental validation on the\nANYmal quadruped robots is presented, for a total duration of 2 h and 1.8 km\ntraveled. The experiments involved dynamic locomotion over loose rocks, slopes,\nand mud; these included perceptual challenges, such as dark and dusty\nunderground caverns or open, feature-deprived areas, as well as mobility\nchallenges such as slipping and terrain deformation. We show an average\nimprovement of 62% translational and 51% rotational errors compared to a\nstate-of-the-art loosely coupled approach. To demonstrate its robustness,\nVILENS was also integrated with a perceptive controller and a local path\nplanner.",
          "link": "http://arxiv.org/abs/2107.07243",
          "publishedOn": "2021-07-16T00:48:23.997Z",
          "wordCount": 623,
          "title": "VILENS: Visual, Inertial, Lidar, and Leg Odometry for All-Terrain Legged Robots. (arXiv:2107.07243v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2008.00394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H Ang Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>",
          "description": "In view of the difficulty in reconstructing object details in point cloud\ncompletion, we propose a shape prior learning method for object completion. The\nshape priors include geometric information in both complete and the partial\npoint clouds. We design a feature alignment strategy to learn the shape prior\nfrom complete points, and a coarse to fine strategy to incorporate partial\nprior in the fine stage. To learn the complete objects prior, we first train a\npoint cloud auto-encoder to extract the latent embeddings from complete points.\nThen we learn a mapping to transfer the point features from partial points to\nthat of the complete points by optimizing feature alignment losses. The feature\nalignment losses consist of a L2 distance and an adversarial loss obtained by\nMaximum Mean Discrepancy Generative Adversarial Network (MMD-GAN). The L2\ndistance optimizes the partial features towards the complete ones in the\nfeature space, and MMD-GAN decreases the statistical distance of two point\nfeatures in a Reproducing Kernel Hilbert Space. We achieve state-of-the-art\nperformances on the point cloud completion task. Our code is available at\nhttps://github.com/xiaogangw/point-cloud-completion-shape-prior.",
          "link": "http://arxiv.org/abs/2008.00394",
          "publishedOn": "2021-07-16T00:48:23.969Z",
          "wordCount": 661,
          "title": "Point Cloud Completion by Learning Shape Priors. (arXiv:2008.00394v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zetian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jason Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Leslie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peter Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Michelle A. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>",
          "description": "Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. It is a challenging yet crucial area\nwith numerous real-world applications in multimedia, affective computing,\nrobotics, finance, human-computer interaction, and healthcare. Unfortunately,\nmultimodal research has seen limited resources to study (1) generalization\nacross domains and modalities, (2) complexity during training and inference,\nand (3) robustness to noisy and missing modalities. In order to accelerate\nprogress towards understudied modalities and tasks while ensuring real-world\nrobustness, we release MultiBench, a systematic and unified large-scale\nbenchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6\nresearch areas. MultiBench provides an automated end-to-end machine learning\npipeline that simplifies and standardizes data loading, experimental setup, and\nmodel evaluation. To enable holistic evaluation, MultiBench offers a\ncomprehensive methodology to assess (1) generalization, (2) time and space\ncomplexity, and (3) modality robustness. MultiBench introduces impactful\nchallenges for future research, including scalability to large-scale multimodal\ndatasets and robustness to realistic imperfections. To accompany this\nbenchmark, we also provide a standardized implementation of 20 core approaches\nin multimodal learning. Simply applying methods proposed in different research\nareas can improve the state-of-the-art performance on 9/15 datasets. Therefore,\nMultiBench presents a milestone in unifying disjoint efforts in multimodal\nresearch and paves the way towards a better understanding of the capabilities\nand limitations of multimodal models, all the while ensuring ease of use,\naccessibility, and reproducibility. MultiBench, our standardized code, and\nleaderboards are publicly available, will be regularly updated, and welcomes\ninputs from the community.",
          "link": "http://arxiv.org/abs/2107.07502",
          "publishedOn": "2021-07-16T00:48:23.960Z",
          "wordCount": 722,
          "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharan_L/0/1/0/all/0/1\">Lalith Sharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romano_G/0/1/0/all/0/1\">Gabriele Romano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehler_S/0/1/0/all/0/1\">Sven Koehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelm_H/0/1/0/all/0/1\">Halvar Kelm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karck_M/0/1/0/all/0/1\">Matthias Karck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simone_R/0/1/0/all/0/1\">Raffaele De Simone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelhardt_S/0/1/0/all/0/1\">Sandy Engelhardt</a>",
          "description": "The CycleGAN framework allows for unsupervised image-to-image translation of\nunpaired data. In a scenario of surgical training on a physical surgical\nsimulator, this method can be used to transform endoscopic images of phantoms\ninto images which more closely resemble the intra-operative appearance of the\nsame surgical target structure. This can be viewed as a novel augmented reality\napproach, which we coined Hyperrealism in previous work. In this use case, it\nis of paramount importance to display objects like needles, sutures or\ninstruments consistent in both domains while altering the style to a more\ntissue-like appearance. Segmentation of these objects would allow for a direct\ntransfer, however, contouring of these, partly tiny and thin foreground objects\nis cumbersome and perhaps inaccurate. Instead, we propose to use landmark\ndetection on the points when sutures pass into the tissue. This objective is\ndirectly incorporated into a CycleGAN framework by treating the performance of\npre-trained detector models as an additional optimization goal. We show that a\ntask defined on these sparse landmark labels improves consistency of synthesis\nby the generator network in both domains. Comparing a baseline CycleGAN\narchitecture to our proposed extension (DetCycleGAN), mean precision (PPV)\nimproved by +61.32, mean sensitivity (TPR) by +37.91, and mean F1 score by\n+0.4743. Furthermore, it could be shown that by dataset fusion, generated\nintra-operative images can be leveraged as additional training data for the\ndetection network itself. The data is released within the scope of the AdaptOR\nMICCAI Challenge 2021 at https://adaptor2021.github.io/, and code at\nhttps://github.com/Cardio-AI/detcyclegan_pytorch.",
          "link": "http://arxiv.org/abs/2107.06941",
          "publishedOn": "2021-07-16T00:48:23.952Z",
          "wordCount": 720,
          "title": "Mutually improved endoscopic image synthesis and landmark detection in unpaired image-to-image translation. (arXiv:2107.06941v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.12368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1\">Theodoros Tsiligkaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1\">Jay Roberts</a>",
          "description": "Deep neural networks are easily fooled by small perturbations known as\nadversarial attacks. Adversarial Training (AT) is a technique that\napproximately solves a robust optimization problem to minimize the worst-case\nloss and is widely regarded as the most effective defense against such attacks.\nWe develop a theoretical framework for adversarial training with FW\noptimization (FW-AT) that reveals a geometric connection between the loss\nlandscape and the distortion of $\\ell_\\infty$ FW attacks (the attack's $\\ell_2$\nnorm). Specifically, we show that high distortion of FW attacks is equivalent\nto low variation along the attack path. It is then experimentally demonstrated\non various deep neural network architectures that $\\ell_\\infty$ attacks against\nrobust models achieve near maximal $\\ell_2$ distortion. This mathematical\ntransparency differentiates FW from the more popular Projected Gradient Descent\n(PGD) optimization. To demonstrate the utility of our theoretical framework we\ndevelop FW-Adapt, a novel adversarial training algorithm which uses simple\ndistortion measure to adaptively change number of attack steps during training.\nFW-Adapt provides strong robustness at lower training times in comparison to\nPGD-AT for a variety of white-box and black-box attacks.",
          "link": "http://arxiv.org/abs/2012.12368",
          "publishedOn": "2021-07-16T00:48:23.934Z",
          "wordCount": 649,
          "title": "Understanding Frank-Wolfe Adversarial Training. (arXiv:2012.12368v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sudhanshu/0/1/0/all/0/1\">Sudhanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "With the advancement in the technology sector spanning over every field, a\nhuge influx of information is inevitable. Among all the opportunities that the\nadvancements in the technology have brought, one of them is to propose\nefficient solutions for data retrieval. This means that from an enormous pile\nof data, the retrieval methods should allow the users to fetch the relevant and\nrecent data over time. In the field of entertainment and e-commerce,\nrecommender systems have been functioning to provide the aforementioned.\nEmploying the same systems in the medical domain could definitely prove to be\nuseful in variety of ways. Following this context, the goal of this paper is to\npropose collaborative filtering based recommender system in the healthcare\nsector to recommend remedies based on the symptoms experienced by the patients.\nFurthermore, a new dataset is developed consisting of remedies concerning\nvarious diseases to address the limited availability of the data. The proposed\nrecommender system accepts the prognostic markers of a patient as the input and\ngenerates the best remedy course. With several experimental trials, the\nproposed model achieved promising results in recommending the possible remedy\nfor given prognostic markers.",
          "link": "http://arxiv.org/abs/2107.07500",
          "publishedOn": "2021-07-16T00:48:23.918Z",
          "wordCount": 640,
          "title": "Recommending best course of treatment based on similarities of prognostic markers\\thanks{All authors contributed equally. (arXiv:2107.07500v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Migdal_P/0/1/0/all/0/1\">Piotr Migda&#x142;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olechno_B/0/1/0/all/0/1\">Bart&#x142;omiej Olechno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Podgorski_B/0/1/0/all/0/1\">B&#x142;a&#x17c;ej Podg&#xf3;rski</a>",
          "description": "We present practical approaches of using deep learning to create and enhance\nlevel maps and textures for video games -- desktop, mobile, and web. We aim to\npresent new possibilities for game developers and level artists. The task of\ndesigning levels and filling them with details is challenging. It is both\ntime-consuming and takes effort to make levels rich, complex, and with a\nfeeling of being natural. Fortunately, recent progress in deep learning\nprovides new tools to accompany level designers and visual artists. Moreover,\nthey offer a way to generate infinite worlds for game replayability and adjust\neducational games to players' needs. We present seven approaches to create\nlevel maps, each using statistical methods, machine learning, or deep learning.\nIn particular, we include:\n\n- Generative Adversarial Networks for creating new images from existing\nexamples (e.g. ProGAN).\n\n- Super-resolution techniques for upscaling images while preserving crisp\ndetail (e.g. ESRGAN).\n\n- Neural style transfer for changing visual themes.\n\n- Image translation - turning semantic maps into images (e.g. GauGAN).\n\n- Semantic segmentation for turning images into semantic masks (e.g. U-Net).\n\n- Unsupervised semantic segmentation for extracting semantic features (e.g.\nTile2Vec).\n\n- Texture synthesis - creating large patterns based on a smaller sample (e.g.\nInGAN).",
          "link": "http://arxiv.org/abs/2107.07397",
          "publishedOn": "2021-07-16T00:48:23.875Z",
          "wordCount": 666,
          "title": "Level generation and style enhancement -- deep learning for game development overview. (arXiv:2107.07397v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07271",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Moyes_A/0/1/0/all/0/1\">Andrew Moyes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gault_R/0/1/0/all/0/1\">Richard Gault</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ming_J/0/1/0/all/0/1\">Ji Ming</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crookes_D/0/1/0/all/0/1\">Danny Crookes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>",
          "description": "Domain shift is a problem commonly encountered when developing automated\nhistopathology pipelines. The performance of machine learning models such as\nconvolutional neural networks within automated histopathology pipelines is\noften diminished when applying them to novel data domains due to factors\narising from differing staining and scanning protocols. The Dual-Channel\nAuto-Encoder (DCAE) model was previously shown to produce feature\nrepresentations that are less sensitive to appearance variation introduced by\ndifferent digital slide scanners. In this work, the Multi-Channel Auto-Encoder\n(MCAE) model is presented as an extension to DCAE which learns from more than\ntwo domains of data. Additionally, a synthetic dataset is generated using\nCycleGANs that contains aligned tissue images that have had their appearance\nsynthetically modified. Experimental results show that the MCAE model produces\nfeature representations that are less sensitive to inter-domain variations than\nthe comparative StaNoSA method when tested on the novel synthetic data.\nAdditionally, the MCAE and StaNoSA models are tested on a novel tissue\nclassification task. The results of this experiment show the MCAE model out\nperforms the StaNoSA model by 5 percentage-points in the f1-score. These\nresults show that the MCAE model is able to generalise better to novel data and\ntasks than existing approaches by actively learning normalised feature\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.07271",
          "publishedOn": "2021-07-16T00:48:23.866Z",
          "wordCount": 669,
          "title": "Multi-Channel Auto-Encoders and a Novel Dataset for Learning Domain Invariant Representations of Histopathology Images. (arXiv:2107.07271v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miah_M/0/1/0/all/0/1\">Mehdi Miah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1\">Guillaume-Alexandre Bilodeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunier_N/0/1/0/all/0/1\">Nicolas Saunier</a>",
          "description": "We propose a method for multi-object tracking and segmentation (MOTS) that\ndoes not require fine-tuning or per benchmark hyperparameter selection. The\nproposed method addresses particularly the data association problem. Indeed,\nthe recently introduced HOTA metric, that has a better alignment with the human\nvisual assessment by evenly balancing detections and associations quality, has\nshown that improvements are still needed for data association. After creating\ntracklets using instance segmentation and optical flow, the proposed method\nrelies on a space-time memory network (STM) developed for one-shot video object\nsegmentation to improve the association of tracklets with temporal gaps. To the\nbest of our knowledge, our method, named MeNToS, is the first to use the STM\nnetwork to track object masks for MOTS. We took the 4th place in the RobMOTS\nchallenge. The project page is https://mehdimiah.com/mentos.html.",
          "link": "http://arxiv.org/abs/2107.07067",
          "publishedOn": "2021-07-16T00:48:23.860Z",
          "wordCount": 585,
          "title": "MeNToS: Tracklets Association with a Space-Time Memory Network. (arXiv:2107.07067v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lagunas_M/0/1/0/all/0/1\">Manuel Lagunas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jimei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_R/0/1/0/all/0/1\">Ruben Villegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1\">Zhixin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masia_B/0/1/0/all/0/1\">Belen Masia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_D/0/1/0/all/0/1\">Diego Gutierrez</a>",
          "description": "We present a single-image data-driven method to automatically relight images\nwith full-body humans in them. Our framework is based on a realistic scene\ndecomposition leveraging precomputed radiance transfer (PRT) and spherical\nharmonics (SH) lighting. In contrast to previous work, we lift the assumptions\non Lambertian materials and explicitly model diffuse and specular reflectance\nin our data. Moreover, we introduce an additional light-dependent residual term\nthat accounts for errors in the PRT-based image reconstruction. We propose a\nnew deep learning architecture, tailored to the decomposition performed in PRT,\nthat is trained using a combination of L1, logarithmic, and rendering losses.\nOur model outperforms the state of the art for full-body human relighting both\nwith synthetic images and photographs.",
          "link": "http://arxiv.org/abs/2107.07259",
          "publishedOn": "2021-07-16T00:48:23.853Z",
          "wordCount": 570,
          "title": "Single-image Full-body Human Relighting. (arXiv:2107.07259v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Najdenkoska_I/0/1/0/all/0/1\">Ivona Najdenkoska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1\">Marcel Worring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Automating report generation for medical imaging promises to reduce workload\nand assist diagnosis in clinical practice. Recent work has shown that deep\nlearning models can successfully caption natural images. However, learning from\nmedical data is challenging due to the diversity and uncertainty inherent in\nthe reports written by different radiologists with discrepant expertise and\nexperience. To tackle these challenges, we propose variational topic inference\nfor automatic report generation. Specifically, we introduce a set of topics as\nlatent variables to guide sentence generation by aligning image and language\nmodalities in a latent space. The topics are inferred in a conditional\nvariational inference framework, with each topic governing the generation of a\nsentence in the report. Further, we adopt a visual attention module that\nenables the model to attend to different locations in the image and generate\nmore informative descriptions. We conduct extensive experiments on two\nbenchmarks, namely Indiana U. Chest X-rays and MIMIC-CXR. The results\ndemonstrate that our proposed variational topic inference method can generate\nnovel reports rather than mere copies of reports used in training, while still\nachieving comparable performance to state-of-the-art methods in terms of\nstandard language generation criteria.",
          "link": "http://arxiv.org/abs/2107.07314",
          "publishedOn": "2021-07-16T00:48:23.836Z",
          "wordCount": 653,
          "title": "Variational Topic Inference for Chest X-Ray Report Generation. (arXiv:2107.07314v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sangmin Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1\">Junhyug Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kangil Kim</a>",
          "description": "Identifying relations between objects is central to understanding the scene.\nWhile several works have been proposed for relation modeling in the image\ndomain, there have been many constraints in the video domain due to challenging\ndynamics of spatio-temporal interactions (e.g., Between which objects are there\nan interaction? When do relations occur and end?). To date, two representative\nmethods have been proposed to tackle Video Visual Relation Detection (VidVRD):\nsegment-based and window-based. We first point out the limitations these two\nmethods have and propose Temporal Span Proposal Network (TSPN), a novel method\nwith two advantages in terms of efficiency and effectiveness. 1) TSPN tells\nwhat to look: it sparsifies relation search space by scoring relationness\n(i.e., confidence score for the existence of a relation between pair of\nobjects) of object pair. 2) TSPN tells when to look: it leverages the full\nvideo context to simultaneously predict the temporal span and categories of the\nentire relations. TSPN demonstrates its effectiveness by achieving new\nstate-of-the-art by a significant margin on two VidVRD benchmarks\n(ImageNet-VidVDR and VidOR) while also showing lower time complexity than\nexisting methods - in particular, twice as efficient as a popular segment-based\napproach.",
          "link": "http://arxiv.org/abs/2107.07154",
          "publishedOn": "2021-07-16T00:48:23.829Z",
          "wordCount": 669,
          "title": "What and When to Look?: Temporal Span Proposal Network for Video Visual Relation Detection. (arXiv:2107.07154v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1\">Dong An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuankai Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>",
          "description": "Vision and Language Navigation (VLN) requires an agent to navigate to a\ntarget location by following natural language instructions. Most of existing\nworks represent a navigation candidate by the feature of the corresponding\nsingle view where the candidate lies in. However, an instruction may mention\nlandmarks out of the single view as references, which might lead to failures of\ntextual-visual matching of existing methods. In this work, we propose a\nmulti-module Neighbor-View Enhanced Model (NvEM) to adaptively incorporate\nvisual contexts from neighbor views for better textual-visual matching.\nSpecifically, our NvEM utilizes a subject module and a reference module to\ncollect contexts from neighbor views. The subject module fuses neighbor views\nat a global level, and the reference module fuses neighbor objects at a local\nlevel. Subjects and references are adaptively determined via attention\nmechanisms. Our model also includes an action module to utilize the strong\norientation guidance (e.g., ``turn left'') in instructions. Each module\npredicts navigation action separately and their weighted sum is used for\npredicting the final action. Extensive experimental results demonstrate the\neffectiveness of the proposed method on the R2R and R4R benchmarks against\nseveral state-of-the-art navigators, and NvEM even beats some pre-training\nones. Our code is available at https://github.com/MarSaKi/NvEM.",
          "link": "http://arxiv.org/abs/2107.07201",
          "publishedOn": "2021-07-16T00:48:23.822Z",
          "wordCount": 643,
          "title": "Neighbor-view Enhanced Model for Vision and Language Navigation. (arXiv:2107.07201v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xunli Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>",
          "description": "Amodal segmentation is a new direction of instance segmentation while\nconsidering the segmentation of the visible and occluded parts of the instance.\nThe existing state-of-the-art method uses multi-task branches to predict the\namodal part and the visible part separately and subtract the visible part from\nthe amodal part to obtain the occluded part. However, the amodal part contains\nvisible information. Therefore, the separated prediction method will generate\nduplicate information. Different from this method, we propose a method of\namodal segmentation based on the idea of the jigsaw. The method uses multi-task\nbranches to predict the two naturally decoupled parts of visible and occluded,\nwhich is like getting two matching jigsaw pieces. Then put the two jigsaw\npieces together to get the amodal part. This makes each branch focus on the\nmodeling of the object. And we believe that there are certain rules in the\nocclusion relationship in the real world. This is a kind of occlusion context\ninformation. This jigsaw method can better model the occlusion relationship and\nuse the occlusion context information, which is important for amodal\nsegmentation. Experiments on two widely used amodally annotated datasets prove\nthat our method exceeds existing state-of-the-art methods. The source code of\nthis work will be made public soon.",
          "link": "http://arxiv.org/abs/2107.07464",
          "publishedOn": "2021-07-16T00:48:23.816Z",
          "wordCount": 636,
          "title": "Amodal segmentation just like doing a jigsaw. (arXiv:2107.07464v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Sourav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1\">Suresh Sundaram</a>",
          "description": "In this paper, a novel confidence conditioned knowledge distillation (CCKD)\nscheme for transferring the knowledge from a teacher model to a student model\nis proposed. Existing state-of-the-art methods employ fixed loss functions for\nthis purpose and ignore the different levels of information that need to be\ntransferred for different samples. In addition to that, these methods are also\ninefficient in terms of data usage. CCKD addresses these issues by leveraging\nthe confidence assigned by the teacher model to the correct class to devise\nsample-specific loss functions (CCKD-L formulation) and targets (CCKD-T\nformulation). Further, CCKD improves the data efficiency by employing\nself-regulation to stop those samples from participating in the distillation\nprocess on which the student model learns faster. Empirical evaluations on\nseveral benchmark datasets show that CCKD methods achieve at least as much\ngeneralization performance levels as other state-of-the-art methods while being\ndata efficient in the process. Student models trained through CCKD methods do\nnot retain most of the misclassifications commited by the teacher model on the\ntraining set. Distillation through CCKD methods improves the resilience of the\nstudent models against adversarial attacks compared to the conventional KD\nmethod. Experiments show at least 3% increase in performance against\nadversarial attacks for the MNIST and the Fashion MNIST datasets, and at least\n6% increase for the CIFAR10 dataset.",
          "link": "http://arxiv.org/abs/2107.06993",
          "publishedOn": "2021-07-16T00:48:23.808Z",
          "wordCount": 650,
          "title": "Confidence Conditioned Knowledge Distillation. (arXiv:2107.06993v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sobh_I/0/1/0/all/0/1\">Ibrahim Sobh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamed_A/0/1/0/all/0/1\">Ahmed Hamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Ravi Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>",
          "description": "Deep neural networks (DNNs) have accomplished impressive success in various\napplications, including autonomous driving perception tasks, in recent years.\nOn the other hand, current deep neural networks are easily fooled by\nadversarial attacks. This vulnerability raises significant concerns,\nparticularly in safety-critical applications. As a result, research into\nattacking and defending DNNs has gained much coverage. In this work, detailed\nadversarial attacks are applied on a diverse multi-task visual perception deep\nnetwork across distance estimation, semantic segmentation, motion detection,\nand object detection. The experiments consider both white and black box attacks\nfor targeted and un-targeted cases, while attacking a task and inspecting the\neffect on all the others, in addition to inspecting the effect of applying a\nsimple defense method. We conclude this paper by comparing and discussing the\nexperimental results, proposing insights and future work. The visualizations of\nthe attacks are available at https://youtu.be/R3JUV41aiPY.",
          "link": "http://arxiv.org/abs/2107.07449",
          "publishedOn": "2021-07-16T00:48:23.791Z",
          "wordCount": 585,
          "title": "Adversarial Attacks on Multi-task Visual Perception for Autonomous Driving. (arXiv:2107.07449v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mangla_P/0/1/0/all/0/1\">Puneet Mangla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandhok_S/0/1/0/all/0/1\">Shivam Chandhok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>",
          "description": "Recent progress towards designing models that can generalize to unseen\ndomains (i.e domain generalization) or unseen classes (i.e zero-shot learning)\nhas embarked interest towards building models that can tackle both domain-shift\nand semantic shift simultaneously (i.e zero-shot domain generalization). For\nmodels to generalize to unseen classes in unseen domains, it is crucial to\nlearn feature representation that preserves class-level (domain-invariant) as\nwell as domain-specific information. Motivated from the success of generative\nzero-shot approaches, we propose a feature generative framework integrated with\na COntext COnditional Adaptive (COCOA) Batch-Normalization to seamlessly\nintegrate class-level semantic and domain-specific information. The generated\nvisual features better capture the underlying data distribution enabling us to\ngeneralize to unseen classes and domains at test-time. We thoroughly evaluate\nand analyse our approach on established large-scale benchmark - DomainNet and\ndemonstrate promising performance over baselines and state-of-art methods.",
          "link": "http://arxiv.org/abs/2107.07497",
          "publishedOn": "2021-07-16T00:48:23.785Z",
          "wordCount": 577,
          "title": "Context-Conditional Adaptation for Recognizing Unseen Classes in Unseen Domains. (arXiv:2107.07497v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07468",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bertoldo_J/0/1/0/all/0/1\">Jo&#xe3;o P C Bertoldo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Decenciere_E/0/1/0/all/0/1\">Etienne Decenci&#xe8;re</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ryckelynck_D/0/1/0/all/0/1\">David Ryckelynck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Proudhon_H/0/1/0/all/0/1\">Henry Proudhon</a>",
          "description": "X-ray Computed Tomography (XCT) techniques have evolved to a point that\nhigh-resolution data can be acquired so fast that classic segmentation methods\nare prohibitively cumbersome, demanding automated data pipelines capable of\ndealing with non-trivial 3D images. Deep learning has demonstrated success in\nmany image processing tasks, including material science applications, showing a\npromising alternative for a humanfree segmentation pipeline. In this paper a\nmodular interpretation of UNet (Modular U-Net) is proposed and trained to\nsegment 3D tomography images of a three-phased glass fiber-reinforced Polyamide\n66. We compare 2D and 3D versions of our model, finding that the former is\nslightly better than the latter. We observe that human-comparable results can\nbe achievied even with only 10 annotated layers and using a shallow U-Net\nyields better results than a deeper one. As a consequence, Neural Network (NN)\nshow indeed a promising venue to automate XCT data processing pipelines needing\nno human, adhoc intervention.",
          "link": "http://arxiv.org/abs/2107.07468",
          "publishedOn": "2021-07-16T00:48:23.779Z",
          "wordCount": 630,
          "title": "A modular U-Net for automated segmentation of X-ray tomography images in composite materials. (arXiv:2107.07468v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1\">Taimur Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akcay_S/0/1/0/all/0/1\">Samet Akcay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1\">Naoufel Werghi</a>",
          "description": "Identifying potential threats concealed within the baggage is of prime\nconcern for the security staff. Many researchers have developed frameworks that\ncan detect baggage threats from X-ray scans. However, to the best of our\nknowledge, all of these frameworks require extensive training on large-scale\nand well-annotated datasets, which are hard to procure in the real world. This\npaper presents a novel unsupervised anomaly instance segmentation framework\nthat recognizes baggage threats, in X-ray scans, as anomalies without requiring\nany ground truth labels. Furthermore, thanks to its stylization capacity, the\nframework is trained only once, and at the inference stage, it detects and\nextracts contraband items regardless of their scanner specifications. Our\none-staged approach initially learns to reconstruct normal baggage content via\nan encoder-decoder network utilizing a proposed stylization loss function. The\nmodel subsequently identifies the abnormal regions by analyzing the disparities\nwithin the original and the reconstructed scans. The anomalous regions are then\nclustered and post-processed to fit a bounding box for their localization. In\naddition, an optional classifier can also be appended with the proposed\nframework to recognize the categories of these extracted anomalies. A thorough\nevaluation of the proposed system on four public baggage X-ray datasets,\nwithout any re-training, demonstrates that it achieves competitive performance\nas compared to the conventional fully supervised methods (i.e., the mean\naverage precision score of 0.7941 on SIXray, 0.8591 on GDXray, 0.7483 on\nOPIXray, and 0.5439 on COMPASS-XP dataset) while outperforming state-of-the-art\nsemi-supervised and unsupervised baggage threat detection frameworks by 67.37%,\n32.32%, 47.19%, and 45.81% in terms of F1 score across SIXray, GDXray, OPIXray,\nand COMPASS-XP datasets, respectively.",
          "link": "http://arxiv.org/abs/2107.07333",
          "publishedOn": "2021-07-16T00:48:23.766Z",
          "wordCount": 708,
          "title": "Unsupervised Anomaly Instance Segmentation for Baggage Threat Recognition. (arXiv:2107.07333v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07436",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jethani_N/0/1/0/all/0/1\">Neil Jethani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sudarshan_M/0/1/0/all/0/1\">Mukund Sudarshan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Covert_I/0/1/0/all/0/1\">Ian Covert</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1\">Su-In Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ranganath_R/0/1/0/all/0/1\">Rajesh Ranganath</a>",
          "description": "Shapley values are widely used to explain black-box models, but they are\ncostly to calculate because they require many model evaluations. We introduce\nFastSHAP, a method for estimating Shapley values in a single forward pass using\na learned explainer model. FastSHAP amortizes the cost of explaining many\ninputs via a learning approach inspired by the Shapley value's weighted least\nsquares characterization, and it can be trained using standard stochastic\ngradient optimization. We compare FastSHAP to existing estimation approaches,\nrevealing that it generates high-quality explanations with orders of magnitude\nspeedup.",
          "link": "http://arxiv.org/abs/2107.07436",
          "publishedOn": "2021-07-16T00:48:23.748Z",
          "wordCount": 533,
          "title": "FastSHAP: Real-Time Shapley Value Estimation. (arXiv:2107.07436v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">D. Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">J. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">J. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">K. Lee</a>",
          "description": "In the process of intelligently segmenting foods in images using deep neural\nnetworks for diet management, data collection and labeling for network training\nare very important but labor-intensive tasks. In order to solve the\ndifficulties of data collection and annotations, this paper proposes a food\nsegmentation method applicable to real-world through synthetic data. To perform\nfood segmentation on healthcare robot systems, such as meal assistance robot\narm, we generate synthetic data using the open-source 3D graphics software\nBlender placing multiple objects on meal plate and train Mask R-CNN for\ninstance segmentation. Also, we build a data collection system and verify our\nsegmentation model on real-world food data. As a result, on our real-world\ndataset, the model trained only synthetic data is available to segment food\ninstances that are not trained with 52.2% mask AP@all, and improve performance\nby +6.4%p after fine-tuning comparing to the model trained from scratch. In\naddition, we also confirm the possibility and performance improvement on the\npublic dataset for fair analysis. Our code and pre-trained weights are\navaliable online at: https://github.com/gist-ailab/Food-Instance-Segmentation",
          "link": "http://arxiv.org/abs/2107.07191",
          "publishedOn": "2021-07-16T00:48:23.735Z",
          "wordCount": 621,
          "title": "Deep Learning based Food Instance Segmentation using Synthetic Data. (arXiv:2107.07191v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yousefzadeh_A/0/1/0/all/0/1\">Amirreza Yousefzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifalakis_M/0/1/0/all/0/1\">Manolis Sifalakis</a>",
          "description": "Activation sparsity improves compute efficiency and resource utilization in\nsparsity-aware neural network accelerators. As the predominant operation in\nDNNs is multiply-accumulate (MAC) of activations with weights to compute inner\nproducts, skipping operations where (at least) one of the two operands is zero\ncan make inference more efficient in terms of latency and power. Spatial\nsparsification of activations is a popular topic in DNN literature and several\nmethods have already been established to bias a DNN for it. On the other hand,\ntemporal sparsity is an inherent feature of bio-inspired spiking neural\nnetworks (SNNs), which neuromorphic processing exploits for hardware\nefficiency. Introducing and exploiting spatio-temporal sparsity, is a topic\nmuch less explored in DNN literature, but in perfect resonance with the trend\nin DNN, to shift from static signal processing to more streaming signal\nprocessing. Towards this goal, in this paper we introduce a new DNN layer\n(called Delta Activation Layer), whose sole purpose is to promote temporal\nsparsity of activations during training. A Delta Activation Layer casts\ntemporal sparsity into spatial activation sparsity to be exploited when\nperforming sparse tensor multiplications in hardware. By employing delta\ninference and ``the usual'' spatial sparsification heuristics during training,\nthe resulting model learns to exploit not only spatial but also temporal\nactivation sparsity (for a given input data distribution). One may use the\nDelta Activation Layer either during vanilla training or during a refinement\nphase. We have implemented Delta Activation Layer as an extension of the\nstandard Tensoflow-Keras library, and applied it to train deep neural networks\non the Human Action Recognition (UCF101) dataset. We report an almost 3x\nimprovement of activation sparsity, with recoverable loss of model accuracy\nafter longer training.",
          "link": "http://arxiv.org/abs/2107.07305",
          "publishedOn": "2021-07-16T00:48:23.725Z",
          "wordCount": 728,
          "title": "Training for temporal sparsity in deep neural networks, application in video processing. (arXiv:2107.07305v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07224",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fox_G/0/1/0/all/0/1\">Gereon Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1\">Ayush Tewari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elgharib_M/0/1/0/all/0/1\">Mohamed Elgharib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>",
          "description": "Generative adversarial models (GANs) continue to produce advances in terms of\nthe visual quality of still images, as well as the learning of temporal\ncorrelations. However, few works manage to combine these two interesting\ncapabilities for the synthesis of video content: Most methods require an\nextensive training dataset in order to learn temporal correlations, while being\nrather limited in the resolution and visual quality of their output frames. In\nthis paper, we present a novel approach to the video synthesis problem that\nhelps to greatly improve visual quality and drastically reduce the amount of\ntraining data and resources necessary for generating video content. Our\nformulation separates the spatial domain, in which individual frames are\nsynthesized, from the temporal domain, in which motion is generated. For the\nspatial domain we make use of a pre-trained StyleGAN network, the latent space\nof which allows control over the appearance of the objects it was trained for.\nThe expressive power of this model allows us to embed our training videos in\nthe StyleGAN latent space. Our temporal architecture is then trained not on\nsequences of RGB frames, but on sequences of StyleGAN latent codes. The\nadvantageous properties of the StyleGAN space simplify the discovery of\ntemporal correlations. We demonstrate that it suffices to train our temporal\narchitecture on only 10 minutes of footage of 1 subject for about 6 hours.\nAfter training, our model can not only generate new portrait videos for the\ntraining subject, but also for any random subject which can be embedded in the\nStyleGAN space.",
          "link": "http://arxiv.org/abs/2107.07224",
          "publishedOn": "2021-07-16T00:48:23.715Z",
          "wordCount": 693,
          "title": "StyleVideoGAN: A Temporal Generative Model using a Pretrained StyleGAN. (arXiv:2107.07224v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1\">Yikun Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingrui He</a>",
          "description": "Online advertising is ubiquitous in web business. Image displaying is\nconsidered as one of the most commonly used formats to interact with customers.\nContextual multi-armed bandit has shown success in the application of\nadvertising to solve the exploration-exploitation dilemma existed in the\nrecommendation procedure. Inspired by the visual-aware advertising, in this\npaper, we propose a contextual bandit algorithm, where the convolutional neural\nnetwork (CNN) is utilized to learn the reward function along with an upper\nconfidence bound (UCB) for exploration. We also prove a near-optimal regret\nbound $\\tilde{\\mathcal{O}}(\\sqrt{T})$ when the network is over-parameterized\nand establish strong connections with convolutional neural tangent kernel\n(CNTK). Finally, we evaluate the empirical performance of the proposed\nalgorithm and show that it outperforms other state-of-the-art UCB-based bandit\nalgorithms on real-world image data sets.",
          "link": "http://arxiv.org/abs/2107.07438",
          "publishedOn": "2021-07-16T00:48:23.709Z",
          "wordCount": 565,
          "title": "Convolutional Neural Bandit: Provable Algorithm for Visual-aware Advertising. (arXiv:2107.07438v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deane_J/0/1/0/all/0/1\">Jake Deane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kearney_S/0/1/0/all/0/1\">Sinead Kearney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kwang In Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosker_D/0/1/0/all/0/1\">Darren Cosker</a>",
          "description": "Synthetic data is becoming increasingly common for training computer vision\nmodels for a variety of tasks. Notably, such data has been applied in tasks\nrelated to humans such as 3D pose estimation where data is either difficult to\ncreate or obtain in realistic settings. Comparatively, there has been less work\ninto synthetic animal data and it's uses for training models. Consequently, we\nintroduce a parametric canine model, DynaDog+T, for generating synthetic canine\nimages and data which we use for a common computer vision task, binary\nsegmentation, which would otherwise be difficult due to the lack of available\ndata.",
          "link": "http://arxiv.org/abs/2107.07330",
          "publishedOn": "2021-07-16T00:48:23.691Z",
          "wordCount": 544,
          "title": "DynaDog+T: A Parametric Animal Model for Synthetic Canine Image Generation. (arXiv:2107.07330v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lang_N/0/1/0/all/0/1\">Nico Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1\">Jan Dirk Wegner</a>",
          "description": "The increasing demand for commodities is leading to changes in land use\nworldwide. In the tropics, deforestation, which causes high carbon emissions\nand threatens biodiversity, is often linked to agricultural expansion. While\nthe need for deforestation-free global supply chains is widely recognized,\nmaking progress in practice remains a challenge. Here, we propose an automated\napproach that aims to support conservation and sustainable land use planning\ndecisions by mapping tropical landscapes at large scale and high spatial\nresolution following the High Carbon Stock (HCS) approach. A deep learning\napproach is developed that estimates canopy height for each 10 m Sentinel-2\npixel by learning from sparse GEDI LIDAR reference data, achieving an overall\nRMSE of 6.3 m. We show that these wall-to-wall maps of canopy top height are\npredictive for classifying HCS forests and degraded areas with an overall\naccuracy of 86 % and produce a first high carbon stock map for Indonesia,\nMalaysia, and the Philippines.",
          "link": "http://arxiv.org/abs/2107.07431",
          "publishedOn": "2021-07-16T00:48:23.675Z",
          "wordCount": 606,
          "title": "High carbon stock mapping at large scale with optical satellite imagery and spaceborne LIDAR. (arXiv:2107.07431v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "The increasing population of elderly people is associated with the need to\nmeet their increasing requirements and to provide solutions that can improve\ntheir quality of life in a smart home. In addition to fear and anxiety towards\ninterfacing with systems; cognitive disabilities, weakened memory, disorganized\nbehavior and even physical limitations are some of the problems that elderly\npeople tend to face with increasing age. The essence of providing\ntechnology-based solutions to address these needs of elderly people and to\ncreate smart and assisted living spaces for the elderly; lies in developing\nsystems that can adapt by addressing their diversity and can augment their\nperformances in the context of their day to day goals. Therefore, this work\nproposes a framework for development of a Personalized Intelligent Assistant to\nhelp elderly people perform Activities of Daily Living (ADLs) in a smart and\nconnected Internet of Things (IoT) based environment. This Personalized\nIntelligent Assistant can analyze different tasks performed by the user and\nrecommend activities by considering their daily routine, current affective\nstate and the underlining user experience. To uphold the efficacy of this\nproposed framework, it has been tested on a couple of datasets for modelling an\naverage user and a specific user respectively. The results presented show that\nthe model achieves a performance accuracy of 73.12% when modelling a specific\nuser, which is considerably higher than its performance while modelling an\naverage user, this upholds the relevance for development and implementation of\nthis proposed framework.",
          "link": "http://arxiv.org/abs/2107.07344",
          "publishedOn": "2021-07-16T00:48:23.649Z",
          "wordCount": 731,
          "title": "Framework for A Personalized Intelligent Assistant to Elderly People for Activities of Daily Living. (arXiv:2107.07344v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jizhizi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Automatic image matting (AIM) refers to estimating the soft foreground from\nan arbitrary natural image without any auxiliary input like trimap, which is\nuseful for image editing. Prior methods try to learn semantic features to aid\nthe matting process while being limited to images with salient opaque\nforegrounds such as humans and animals. In this paper, we investigate the\ndifficulties when extending them to natural images with salient\ntransparent/meticulous foregrounds or non-salient foregrounds. To address the\nproblem, a novel end-to-end matting network is proposed, which can predict a\ngeneralized trimap for any image of the above types as a unified semantic\nrepresentation. Simultaneously, the learned semantic features guide the matting\nnetwork to focus on the transition areas via an attention mechanism. We also\nconstruct a test set AIM-500 that contains 500 diverse natural images covering\nall types along with manually labeled alpha mattes, making it feasible to\nbenchmark the generalization ability of AIM models. Results of the experiments\ndemonstrate that our network trained on available composite matting datasets\noutperforms existing methods both objectively and subjectively. The source code\nand dataset are available at https://github.com/JizhiziLi/AIM.",
          "link": "http://arxiv.org/abs/2107.07235",
          "publishedOn": "2021-07-16T00:48:23.587Z",
          "wordCount": 630,
          "title": "Deep Automatic Natural Image Matting. (arXiv:2107.07235v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Corcoll_O/0/1/0/all/0/1\">Oriol Corcoll</a>",
          "description": "Automatic image cropping techniques are commonly used to enhance the\naesthetic quality of an image; they do it by detecting the most beautiful or\nthe most salient parts of the image and removing the unwanted content to have a\nsmaller image that is more visually pleasing. In this thesis, I introduce an\nadditional dimension to the problem of cropping, semantics. I argue that image\ncropping can also enhance the image's relevancy for a given entity by using the\nsemantic information contained in the image. I call this problem, Semantic\nImage Cropping. To support my argument, I provide a new dataset containing 100\nimages with at least two different entities per image and four ground truth\ncroppings collected using Amazon Mechanical Turk. I use this dataset to show\nthat state-of-the-art cropping algorithms that only take into account\naesthetics do not perform well in the problem of semantic image cropping.\nAdditionally, I provide a new deep learning system that takes not just\naesthetics but also semantics into account to generate image croppings, and I\nevaluate its performance using my new semantic cropping dataset, showing that\nusing the semantic information of an image can help to produce better\ncroppings.",
          "link": "http://arxiv.org/abs/2107.07153",
          "publishedOn": "2021-07-16T00:48:23.563Z",
          "wordCount": 615,
          "title": "Semantic Image Cropping. (arXiv:2107.07153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1\">Yakun Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_M/0/1/0/all/0/1\">Muwei Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shaoxiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>",
          "description": "The goal of photometric stereo is to measure the precise surface normal of a\n3D object from observations with various shading cues. However, non-Lambertian\nsurfaces influence the measurement accuracy due to irregular shading cues.\nDespite deep neural networks have been employed to simulate the performance of\nnon-Lambertian surfaces, the error in specularities, shadows, and crinkle\nregions is hard to be reduced. In order to address this challenge, we here\npropose a photometric stereo network that incorporates Lambertian priors to\nbetter measure the surface normal. In this paper, we use the initial normal\nunder the Lambertian assumption as the prior information to refine the normal\nmeasurement, instead of solely applying the observed shading cues to deriving\nthe surface normal. Our method utilizes the Lambertian information to\nreparameterize the network weights and the powerful fitting ability of deep\nneural networks to correct these errors caused by general reflectance\nproperties. Our explorations include: the Lambertian priors (1) reduce the\nlearning hypothesis space, making our method learn the mapping in the same\nsurface normal space and improving the accuracy of learning, and (2) provides\nthe differential features learning, improving the surfaces reconstruction of\ndetails. Extensive experiments verify the effectiveness of the proposed\nLambertian prior photometric stereo network in accurate surface normal\nmeasurement, on the challenging benchmark dataset.",
          "link": "http://arxiv.org/abs/2107.07192",
          "publishedOn": "2021-07-16T00:48:23.554Z",
          "wordCount": 653,
          "title": "Incorporating Lambertian Priors into Surface Normals Measurement. (arXiv:2107.07192v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruohua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1\">Kazuki Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1\">Katherine Driggs-Campbell</a>",
          "description": "Multi-pedestrian trajectory prediction is an indispensable safety element of\nautonomous systems that interact with crowds in unstructured environments. Many\nrecent efforts have developed trajectory prediction algorithms with focus on\nunderstanding social norms behind pedestrian motions. Yet we observe these\nworks usually hold two assumptions that prevent them from being smoothly\napplied to robot applications: positions of all pedestrians are consistently\ntracked; the target agent pays attention to all pedestrians in the scene. The\nfirst assumption leads to biased interaction modeling with incomplete\npedestrian data, and the second assumption introduces unnecessary disturbances\nand leads to the freezing robot problem. Thus, we propose Gumbel Social\nTransformer, in which an Edge Gumbel Selector samples a sparse interaction\ngraph of partially observed pedestrians at each time step. A Node Transformer\nEncoder and a Masked LSTM encode the pedestrian features with the sampled\nsparse graphs to predict trajectories. We demonstrate that our model overcomes\nthe potential problems caused by the assumptions, and our approach outperforms\nthe related works in benchmark evaluation.",
          "link": "http://arxiv.org/abs/2107.07056",
          "publishedOn": "2021-07-16T00:48:23.530Z",
          "wordCount": 612,
          "title": "Learning Sparse Interaction Graphs of Partially Observed Pedestrians for Trajectory Prediction. (arXiv:2107.07056v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiayun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yubei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_B/0/1/0/all/0/1\">Brian Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>",
          "description": "We present a generic method for recurrently using the same parameters for\nmany different convolution layers to build a deep network. Specifically, for a\nnetwork, we create a recurrent parameter generator (RPG), from which the\nparameters of each convolution layer are generated. Though using recurrent\nmodels to build a deep convolutional neural network (CNN) is not entirely new,\nour method achieves significant performance gain compared to the existing\nworks. We demonstrate how to build a one-layer neural network to achieve\nsimilar performance compared to other traditional CNN models on various\napplications and datasets. Such a method allows us to build an arbitrarily\ncomplex neural network with any amount of parameters. For example, we build a\nResNet34 with model parameters reduced by more than $400$ times, which still\nachieves $41.6\\%$ ImageNet top-1 accuracy. Furthermore, we demonstrate the RPG\ncan be applied at different scales, such as layers, blocks, or even\nsub-networks. Specifically, we use the RPG to build a ResNet18 network with the\nnumber of weights equivalent to one convolutional layer of a conventional\nResNet and show this model can achieve $67.2\\%$ ImageNet top-1 accuracy. The\nproposed method can be viewed as an inverse approach to model compression.\nRather than removing the unused parameters from a large model, it aims to\nsqueeze more information into a small number of parameters. Extensive\nexperiment results are provided to demonstrate the power of the proposed\nrecurrent parameter generator.",
          "link": "http://arxiv.org/abs/2107.07110",
          "publishedOn": "2021-07-16T00:48:23.523Z",
          "wordCount": 666,
          "title": "Recurrent Parameter Generators. (arXiv:2107.07110v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kilic_V/0/1/0/all/0/1\">Velat Kilic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_D/0/1/0/all/0/1\">Deepti Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindagi_V/0/1/0/all/0/1\">Vishwanath Sindagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1\">A. Brinton Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_M/0/1/0/all/0/1\">Mark A. Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Lidar-based object detectors are critical parts of the 3D perception pipeline\nin autonomous navigation systems such as self-driving cars. However, they are\nknown to be sensitive to adverse weather conditions such as rain, snow and fog\ndue to reduced signal-to-noise ratio (SNR) and signal-to-background ratio\n(SBR). As a result, lidar-based object detectors trained on data captured in\nnormal weather tend to perform poorly in such scenarios. However, collecting\nand labelling sufficient training data in a diverse range of adverse weather\nconditions is laborious and prohibitively expensive. To address this issue, we\npropose a physics-based approach to simulate lidar point clouds of scenes in\nadverse weather conditions. These augmented datasets can then be used to train\nlidar-based detectors to improve their all-weather reliability. Specifically,\nwe introduce a hybrid Monte-Carlo based approach that treats (i) the effects of\nlarge particles by placing them randomly and comparing their back reflected\npower against the target, and (ii) attenuation effects on average through\ncalculation of scattering efficiencies from the Mie theory and particle size\ndistributions. Retraining networks with this augmented data improves mean\naverage precision evaluated on real world rainy scenes and we observe greater\nimprovement in performance with our model relative to existing models from the\nliterature. Furthermore, we evaluate recent state-of-the-art detectors on the\nsimulated weather conditions and present an in-depth analysis of their\nperformance.",
          "link": "http://arxiv.org/abs/2107.07004",
          "publishedOn": "2021-07-16T00:48:23.517Z",
          "wordCount": 683,
          "title": "Lidar Light Scattering Augmentation (LISA): Physics-based Simulation of Adverse Weather Conditions for 3D Object Detection. (arXiv:2107.07004v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaomeng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziwei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leake_D/0/1/0/all/0/1\">David Leake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xizi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1\">David Crandall</a>",
          "description": "The case difference heuristic (CDH) approach is a knowledge-light method for\nlearning case adaptation knowledge from the case base of a case-based reasoning\nsystem. Given a pair of cases, the CDH approach attributes the difference in\ntheir solutions to the difference in the problems they solve, and generates\nadaptation rules to adjust solutions accordingly when a retrieved case and new\nquery have similar problem differences. As an alternative to learning\nadaptation rules, several researchers have applied neural networks to learn to\npredict solution differences from problem differences. Previous work on such\napproaches has assumed that the feature set describing problems is predefined.\nThis paper investigates a two-phase process combining deep learning for feature\nextraction and neural network based adaptation learning from extracted\nfeatures. Its performance is demonstrated in a regression task on an image\ndata: predicting age given the image of a face. Results show that the combined\nprocess can successfully learn adaptation knowledge applicable to nonsymbolic\ndifferences in cases. The CBR system achieves slightly lower performance\noverall than a baseline deep network regressor, but better performance than the\nbaseline on novel queries.",
          "link": "http://arxiv.org/abs/2107.07095",
          "publishedOn": "2021-07-16T00:48:23.509Z",
          "wordCount": 654,
          "title": "Applying the Case Difference Heuristic to Learn Adaptations from Deep Network Features. (arXiv:2107.07095v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_D/0/1/0/all/0/1\">Di You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jingfen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>",
          "description": "Recent deep network-based compressive sensing (CS) methods have achieved\ngreat success. However, most of them regard different sampling matrices as\ndifferent independent tasks and need to train a specific model for each target\nsampling matrix. Such practices give rise to inefficiency in computing and\nsuffer from poor generalization ability. In this paper, we propose a novel\nCOntrollable Arbitrary-Sampling neTwork, dubbed COAST, to solve CS problems of\narbitrary-sampling matrices (including unseen sampling matrices) with one\nsingle model. Under the optimization-inspired deep unfolding framework, our\nCOAST exhibits good interpretability. In COAST, a random projection\naugmentation (RPA) strategy is proposed to promote the training diversity in\nthe sampling space to enable arbitrary sampling, and a controllable proximal\nmapping module (CPMM) and a plug-and-play deblocking (PnP-D) strategy are\nfurther developed to dynamically modulate the network features and effectively\neliminate the blocking artifacts, respectively. Extensive experiments on widely\nused benchmark datasets demonstrate that our proposed COAST is not only able to\nhandle arbitrary sampling matrices with one single model but also to achieve\nstate-of-the-art performance with fast speed. The source code is available on\nhttps://github.com/jianzhangcs/COAST.",
          "link": "http://arxiv.org/abs/2107.07225",
          "publishedOn": "2021-07-16T00:48:23.418Z",
          "wordCount": 647,
          "title": "COAST: COntrollable Arbitrary-Sampling NeTwork for Compressive Sensing. (arXiv:2107.07225v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Feng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chonghan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tianyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muralidhar_S/0/1/0/all/0/1\">Shivran Muralidhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1\">Vijaykrishnan Narayanan</a>",
          "description": "The cognitive system for human action and behavior has evolved into a deep\nlearning regime, and especially the advent of Graph Convolution Networks has\ntransformed the field in recent years. However, previous works have mainly\nfocused on over-parameterized and complex models based on dense graph\nconvolution networks, resulting in low efficiency in training and inference.\nMeanwhile, the Transformer architecture-based model has not yet been well\nexplored for cognitive application in human action and behavior estimation.\nThis work proposes a novel skeleton-based human action recognition model with\nsparse attention on the spatial dimension and segmented linear attention on the\ntemporal dimension of data. Our model can also process the variable length of\nvideo clips grouped as a single batch. Experiments show that our model can\nachieve comparable performance while utilizing much less trainable parameters\nand achieve high speed in training and inference. Experiments show that our\nmodel achieves 4~18x speedup and 1/7~1/15 model size compared with the baseline\nmodels at competitive accuracy.",
          "link": "http://arxiv.org/abs/2107.07089",
          "publishedOn": "2021-07-16T00:48:23.377Z",
          "wordCount": 605,
          "title": "STAR: Sparse Transformer-based Action Recognition. (arXiv:2107.07089v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ferrero_V/0/1/0/all/0/1\">Vincenzo Ferrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassani_K/0/1/0/all/0/1\">Kaveh Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandi_D/0/1/0/all/0/1\">Daniele Grandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuPont_B/0/1/0/all/0/1\">Bryony DuPont</a>",
          "description": "Function is defined as the ensemble of tasks that enable the product to\ncomplete the designed purpose. Functional tools, such as functional modeling,\noffer decision guidance in the early phase of product design, where explicit\ndesign decisions are yet to be made. Function-based design data is often sparse\nand grounded in individual interpretation. As such, function-based design tools\ncan benefit from automatic function classification to increase data fidelity\nand provide function representation models that enable function-based\nintelligent design agents. Function-based design data is commonly stored in\nmanually generated design repositories. These design repositories are a\ncollection of expert knowledge and interpretations of function in product\ndesign bounded by function-flow and component taxonomies. In this work, we\nrepresent a structured taxonomy-based design repository as assembly-flow\ngraphs, then leverage a graph neural network (GNN) model to perform automatic\nfunction classification. We support automated function classification by\nlearning from repository data to establish the ground truth of component\nfunction assignment. Experimental results show that our GNN model achieves a\nmicro-average F${_1}$-score of 0.832 for tier 1 (broad), 0.756 for tier 2, and\n0.783 for tier 3 (specific) functions. Given the imbalance of data features,\nthe results are encouraging. Our efforts in this paper can be a starting point\nfor more sophisticated applications in knowledge-based CAD systems and\nDesign-for-X consideration in function-based design.",
          "link": "http://arxiv.org/abs/2107.07042",
          "publishedOn": "2021-07-16T00:48:23.370Z",
          "wordCount": 660,
          "title": "Classifying Component Function in Product Assemblies with Graph Neural Networks. (arXiv:2107.07042v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shi-Yao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chung-Yen Su</a>",
          "description": "Nowadays, due to the rapid population expansion, food shortage has become a\ncritical issue. In order to stabilizing the food source production, preventing\ncrops from being attacked by pests is very important. In generally, farmers use\npesticides to kill pests, however, improperly using pesticides will also kill\nsome insects which is beneficial to crops, such as bees. If the number of bees\nis too few, the supplement of food in the world will be in short. Besides,\nexcessive pesticides will seriously pollute the environment. Accordingly,\nfarmers need a machine which can automatically recognize the pests. Recently,\ndeep learning is popular because its effectiveness in the field of image\nclassification. In this paper, we propose a small and efficient model called\nExquisiteNet to complete the task of recognizing the pests and we expect to\napply our model on mobile devices. ExquisiteNet mainly consists of two blocks.\nOne is double fusion with squeeze-and-excitation-bottleneck block (DFSEB\nblock), and the other is max feature expansion block (ME block). ExquisiteNet\nonly has 0.98M parameters and its computing speed is very fast almost the same\nas SqueezeNet. In order to evaluate our model's performance, we test our model\non a benchmark pest dataset called IP102. Compared to many state-of-the-art\nmodels, such as ResNet101, ShuffleNetV2, MobileNetV3-large and EfficientNet\netc., our model achieves higher accuracy, that is, 52.32% on the test set of\nIP102 without any data augmentation.",
          "link": "http://arxiv.org/abs/2107.07167",
          "publishedOn": "2021-07-16T00:48:23.363Z",
          "wordCount": 675,
          "title": "An Efficient and Small Convolutional Neural Network for Pest Recognition -- ExquisiteNet. (arXiv:2107.07167v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ufer_N/0/1/0/all/0/1\">Nikolai Ufer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_S/0/1/0/all/0/1\">Sabine Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>",
          "description": "The search for specific objects or motifs is essential to art history as both\nassist in decoding the meaning of artworks. Digitization has produced large art\ncollections, but manual methods prove to be insufficient to analyze them. In\nthe following, we introduce an algorithm that allows users to search for image\nregions containing specific motifs or objects and find similar regions in an\nextensive dataset, helping art historians to analyze large digitized art\ncollections. Computer vision has presented efficient methods for visual\ninstance retrieval across photographs. However, applied to art collections,\nthey reveal severe deficiencies because of diverse motifs and massive domain\nshifts induced by differences in techniques, materials, and styles. In this\npaper, we present a multi-style feature fusion approach that successfully\nreduces the domain gap and improves retrieval results without labelled data or\ncurated image collections. Our region-based voting with GPU-accelerated\napproximate nearest-neighbour search allows us to find and localize even small\nmotifs within an extensive dataset in a few seconds. We obtain state-of-the-art\nresults on the Brueghel dataset and demonstrate its generalization to\ninhomogeneous collections with a large number of distractors.",
          "link": "http://arxiv.org/abs/2107.06935",
          "publishedOn": "2021-07-16T00:48:23.356Z",
          "wordCount": 647,
          "title": "Object Retrieval and Localization in Large Art Collections using Deep Multi-Style Feature Fusion and Iterative Voting. (arXiv:2107.06935v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael Ng</a>",
          "description": "Image smoothing is a fundamental procedure in applications of both computer\nvision and graphics. The required smoothing properties can be different or even\ncontradictive among different tasks. Nevertheless, the inherent smoothing\nnature of one smoothing operator is usually fixed and thus cannot meet the\nvarious requirements of different applications. In this paper, we first\nintroduce the truncated Huber penalty function which shows strong flexibility\nunder different parameter settings. A generalized framework is then proposed\nwith the introduced truncated Huber penalty function. When combined with its\nstrong flexibility, our framework is able to achieve diverse smoothing natures\nwhere contradictive smoothing behaviors can even be achieved. It can also yield\nthe smoothing behavior that can seldom be achieved by previous methods, and\nsuperior performance is thus achieved in challenging cases. These together\nenable our framework capable of a range of applications and able to outperform\nthe state-of-the-art approaches in several tasks, such as image detail\nenhancement, clip-art compression artifacts removal, guided depth map\nrestoration, image texture removal, etc. In addition, an efficient numerical\nsolution is provided and its convergence is theoretically guaranteed even the\noptimization framework is non-convex and non-smooth. A simple yet effective\napproach is further proposed to reduce the computational cost of our method\nwhile maintaining its performance. The effectiveness and superior performance\nof our approach are validated through comprehensive experiments in a range of\napplications. Our code is available at\nhttps://github.com/wliusjtu/Generalized-Smoothing-Framework.",
          "link": "http://arxiv.org/abs/2107.07058",
          "publishedOn": "2021-07-16T00:48:23.350Z",
          "wordCount": 707,
          "title": "A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengjie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaoqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiyu Song</a>",
          "description": "Up-to-date High-Definition (HD) maps are essential for self-driving cars. To\nachieve constantly updated HD maps, we present a deep neural network (DNN),\nDiff-Net, to detect changes in them. Compared to traditional methods based on\nobject detectors, the essential design in our work is a parallel feature\ndifference calculation structure that infers map changes by comparing features\nextracted from the camera and rasterized images. To generate these rasterized\nimages, we project map elements onto images in the camera view, yielding\nmeaningful map representations that can be consumed by a DNN accordingly. As we\nformulate the change detection task as an object detection problem, we leverage\nthe anchor-based structure that predicts bounding boxes with different change\nstatus categories. Furthermore, rather than relying on single frame input, we\nintroduce a spatio-temporal fusion module that fuses features from history\nframes into the current, thus improving the overall performance. Finally, we\ncomprehensively validate our method's effectiveness using freshly collected\ndatasets. Results demonstrate that our Diff-Net achieves better performance\nthan the baseline methods and is ready to be integrated into a map production\npipeline maintaining an up-to-date HD map.",
          "link": "http://arxiv.org/abs/2107.07030",
          "publishedOn": "2021-07-16T00:48:23.333Z",
          "wordCount": 631,
          "title": "Diff-Net: Image Feature Difference based High-Definition Map Change Detection. (arXiv:2107.07030v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendez_Ruiz_M/0/1/0/all/0/1\">Mauricio Mendez-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Zapata_I/0/1/0/all/0/1\">Ivan Garcia Jorge Gonzalez-Zapata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_Vazquez_A/0/1/0/all/0/1\">Andres Mendez-Vazquez</a>",
          "description": "Few-shot learning is a relatively new technique that specializes in problems\nwhere we have little amounts of data. The goal of these methods is to classify\ncategories that have not been seen before with just a handful of samples.\nRecent approaches, such as metric learning, adopt the meta-learning strategy in\nwhich we have episodic tasks conformed by support (training) data and query\n(test) data. Metric learning methods have demonstrated that simple models can\nachieve good performance by learning a similarity function to compare the\nsupport and the query data. However, the feature space learned by a given\nmetric learning approach may not exploit the information given by a specific\nfew-shot task. In this work, we explore the use of dimension reduction\ntechniques as a way to find task-significant features helping to make better\npredictions. We measure the performance of the reduced features by assigning a\nscore based on the intra-class and inter-class distance, and selecting a\nfeature reduction method in which instances of different classes are far away\nand instances of the same class are close. This module helps to improve the\naccuracy performance by allowing the similarity function, given by the metric\nlearning method, to have more discriminative features for the classification.\nOur method outperforms the metric learning baselines in the miniImageNet\ndataset by around 2% in accuracy performance.",
          "link": "http://arxiv.org/abs/2107.06992",
          "publishedOn": "2021-07-16T00:48:23.325Z",
          "wordCount": 677,
          "title": "Finding Significant Features for Few-Shot Learning using Dimensionality Reduction. (arXiv:2107.06992v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plotka_S/0/1/0/all/0/1\">Szymon P&#x142;otka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wlodarczyk_T/0/1/0/all/0/1\">Tomasz W&#x142;odarczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klasa_A/0/1/0/all/0/1\">Adam Klasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipa_M/0/1/0/all/0/1\">Micha&#x142; Lipa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitek_A/0/1/0/all/0/1\">Arkadiusz Sitek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>",
          "description": "In this paper, we propose an end-to-end multi-task neural network called\nFetalNet with an attention mechanism and stacked module for spatio-temporal\nfetal ultrasound scan video analysis. Fetal biometric measurement is a standard\nexamination during pregnancy used for the fetus growth monitoring and\nestimation of gestational age and fetal weight. The main goal in fetal\nultrasound scan video analysis is to find proper standard planes to measure the\nfetal head, abdomen and femur. Due to natural high speckle noise and shadows in\nultrasound data, medical expertise and sonographic experience are required to\nfind the appropriate acquisition plane and perform accurate measurements of the\nfetus. In addition, existing computer-aided methods for fetal US biometric\nmeasurement address only one single image frame without considering temporal\nfeatures. To address these shortcomings, we propose an end-to-end multi-task\nneural network for spatio-temporal ultrasound scan video analysis to\nsimultaneously localize, classify and measure the fetal body parts. We propose\na new encoder-decoder segmentation architecture that incorporates a\nclassification branch. Additionally, we employ an attention mechanism with a\nstacked module to learn salient maps to suppress irrelevant US regions and\nefficient scan plane localization. We trained on the fetal ultrasound video\ncomes from routine examinations of 700 different patients. Our method called\nFetalNet outperforms existing state-of-the-art methods in both classification\nand segmentation in fetal ultrasound video recordings.",
          "link": "http://arxiv.org/abs/2107.06943",
          "publishedOn": "2021-07-16T00:48:23.318Z",
          "wordCount": 682,
          "title": "FetalNet: Multi-task deep learning framework for fetal ultrasound biometric measurements. (arXiv:2107.06943v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinglu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yinyu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jian Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Jun Zhang</a>",
          "description": "Automatic surgical instruction generation is a prerequisite towards\nintra-operative context-aware surgical assistance. However, generating\ninstructions from surgical scenes is challenging, as it requires jointly\nunderstanding the surgical activity of current view and modelling relationships\nbetween visual information and textual description. Inspired by the neural\nmachine translation and imaging captioning tasks in open domain, we introduce a\ntransformer-backboned encoder-decoder network with self-critical reinforcement\nlearning to generate instructions from surgical images. We evaluate the\neffectiveness of our method on DAISI dataset, which includes 290 procedures\nfrom various medical disciplines. Our approach outperforms the existing\nbaseline over all caption evaluation metrics. The results demonstrate the\nbenefits of the encoder-decoder structure backboned by transformer in handling\nmultimodal context.",
          "link": "http://arxiv.org/abs/2107.06964",
          "publishedOn": "2021-07-16T00:48:23.301Z",
          "wordCount": 550,
          "title": "Surgical Instruction Generation with Transformers. (arXiv:2107.06964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kostic_Z/0/1/0/all/0/1\">Zona Kostic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jevremovic_A/0/1/0/all/0/1\">Aleksandar Jevremovic</a>",
          "description": "The attractiveness of a property is one of the most interesting, yet\nchallenging, categories to model. Image characteristics are used to describe\ncertain attributes, and to examine the influence of visual factors on the price\nor timeframe of the listing. In this paper, we propose a set of techniques for\nthe extraction of visual features for efficient numerical inclusion in\nmodern-day predictive algorithms. We discuss techniques such as Shannon's\nentropy, calculating the center of gravity, employing image segmentation, and\nusing Convolutional Neural Networks. After comparing these techniques as\napplied to a set of property-related images (indoor, outdoor, and satellite),\nwe conclude the following: (i) the entropy is the most efficient single-digit\nvisual measure for housing price prediction; (ii) image segmentation is the\nmost important visual feature for the prediction of housing lifespan; and (iii)\ndeep image features can be used to quantify interior characteristics and\ncontribute to captivation modeling. The set of 40 image features selected here\ncarries a significant amount of predictive power and outperforms some of the\nstrongest metadata predictors. Without any need to replace a human expert in a\nreal-estate appraisal process, we conclude that the techniques presented in\nthis paper can efficiently describe visible characteristics, thus introducing\nperceived attractiveness as a quantitative measure into the predictive modeling\nof housing.",
          "link": "http://arxiv.org/abs/2107.07148",
          "publishedOn": "2021-07-16T00:48:23.284Z",
          "wordCount": 649,
          "title": "What Image Features Boost Housing Market Predictions?. (arXiv:2107.07148v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bohong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wei Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "The mainstream approach for filter pruning is usually either to force a\nhard-coded importance estimation upon a computation-heavy pretrained model to\nselect \"important\" filters, or to impose a hyperparameter-sensitive sparse\nconstraint on the loss objective to regularize the network training. In this\npaper, we present a novel filter pruning method, dubbed dynamic-coded filter\nfusion (DCFF), to derive compact CNNs in a computation-economical and\nregularization-free manner for efficient image classification. Each filter in\nour DCFF is firstly given an inter-similarity distribution with a temperature\nparameter as a filter proxy, on top of which, a fresh Kullback-Leibler\ndivergence based dynamic-coded criterion is proposed to evaluate the filter\nimportance. In contrast to simply keeping high-score filters in other methods,\nwe propose the concept of filter fusion, i.e., the weighted averages using the\nassigned proxies, as our preserved filters. We obtain a one-hot\ninter-similarity distribution as the temperature parameter approaches infinity.\nThus, the relative importance of each filter can vary along with the training\nof the compact CNN, leading to dynamically changeable fused filters without\nboth the dependency on the pretrained model and the introduction of sparse\nconstraints. Extensive experiments on classification benchmarks demonstrate the\nsuperiority of our DCFF over the compared counterparts. For example, our DCFF\nderives a compact VGGNet-16 with only 72.77M FLOPs and 1.06M parameters while\nreaching top-1 accuracy of 93.47% on CIFAR-10. A compact ResNet-50 is obtained\nwith 63.8% FLOPs and 58.6% parameter reductions, retaining 75.60% top-1\naccuracy on ILSVRC-2012. Our code, narrower models and training logs are\navailable at https://github.com/lmbxmu/DCFF.",
          "link": "http://arxiv.org/abs/2107.06916",
          "publishedOn": "2021-07-16T00:48:23.278Z",
          "wordCount": 705,
          "title": "Training Compact CNNs for Image Classification using Dynamic-coded Filter Fusion. (arXiv:2107.06916v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1\">Matteo Stefanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1\">Silvia Cascianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Connecting Vision and Language plays an essential role in Generative\nIntelligence. For this reason, in the last few years, a large research effort\nhas been devoted to image captioning, i.e. the task of describing images with\nsyntactically and semantically meaningful sentences. Starting from 2015 the\ntask has generally been addressed with pipelines composed of a visual encoding\nstep and a language model for text generation. During these years, both\ncomponents have evolved considerably through the exploitation of object\nregions, attributes, and relationships and the introduction of multi-modal\nconnections, fully-attentive approaches, and BERT-like early-fusion strategies.\nHowever, regardless of the impressive results obtained, research in image\ncaptioning has not reached a conclusive answer yet. This work aims at providing\na comprehensive overview and categorization of image captioning approaches,\nfrom visual encoding and text generation to training strategies, used datasets,\nand evaluation metrics. In this respect, we quantitatively compare many\nrelevant state-of-the-art approaches to identify the most impactful technical\ninnovations in image captioning architectures and training strategies.\nMoreover, many variants of the problem and its open challenges are analyzed and\ndiscussed. The final goal of this work is to serve as a tool for understanding\nthe existing state-of-the-art and highlighting the future directions for an\narea of research where Computer Vision and Natural Language Processing can find\nan optimal synergy.",
          "link": "http://arxiv.org/abs/2107.06912",
          "publishedOn": "2021-07-16T00:48:23.264Z",
          "wordCount": 664,
          "title": "From Show to Tell: A Survey on Image Captioning. (arXiv:2107.06912v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Langlois_T/0/1/0/all/0/1\">Thomas A. Langlois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">H. Charles Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1\">Erin Grant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1\">Ishita Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1\">Nori Jacoby</a>",
          "description": "Developments in machine learning interpretability techniques over the past\ndecade have provided new tools to observe the image regions that are most\ninformative for classification and localization in artificial neural networks\n(ANNs). Are the same regions similarly informative to human observers? Using\ndata from 78 new experiments and 6,610 participants, we show that passive\nattention techniques reveal a significant overlap with human visual selectivity\nestimates derived from 6 distinct behavioral tasks including visual\ndiscrimination, spatial localization, recognizability, free-viewing,\ncued-object search, and saliency search fixations. We find that input\nvisualizations derived from relatively simple ANN architectures probed using\nguided backpropagation methods are the best predictors of a shared component in\nthe joint variability of the human measures. We validate these correlational\nresults with causal manipulations using recognition experiments. We show that\nimages masked with ANN attention maps were easier for humans to classify than\ncontrol masks in a speeded recognition experiment. Similarly, we find that\nrecognition performance in the same ANN models was likewise influenced by\nmasking input images using human visual selectivity maps. This work contributes\na new approach to evaluating the biological and psychological validity of\nleading ANNs as models of human vision: by examining their similarities and\ndifferences in terms of their visual selectivity to the information contained\nin images.",
          "link": "http://arxiv.org/abs/2107.07013",
          "publishedOn": "2021-07-16T00:48:23.237Z",
          "wordCount": 658,
          "title": "Passive attention in artificial neural networks predicts human visual selectivity. (arXiv:2107.07013v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kakaletsis_E/0/1/0/all/0/1\">Efstratios Kakaletsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaidis_N/0/1/0/all/0/1\">Nikos Nikolaidis</a>",
          "description": "In this paper, a simple technique for Unmanned Aerial Vehicles (UAVs)\npotential landing site detection using terrain information through\nidentification of flat areas, is presented. The algorithm utilizes digital\nelevation models (DEM) that represent the height distribution of an area. Flat\nareas which constitute appropriate landing zones for UAVs in normal or\nemergency situations result by thresholding the image gradient magnitude of the\ndigital surface model (DSM). The proposed technique also uses connected\ncomponents evaluation on the thresholded gradient image in order to discover\nconnected regions of sufficient size for landing. Moreover, man-made structures\nand vegetation areas are detected and excluded from the potential landing\nsites. Quantitative performance evaluation of the proposed landing site\ndetection algorithm in a number of areas on real world and synthetic datasets,\naccompanied by a comparison with a state-of-the-art algorithm, proves its\nefficiency and superiority.",
          "link": "http://arxiv.org/abs/2107.06921",
          "publishedOn": "2021-07-16T00:48:23.157Z",
          "wordCount": 604,
          "title": "Potential UAV Landing Sites Detection through Digital Elevation Models Analysis. (arXiv:2107.06921v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Postels_J/0/1/0/all/0/1\">Janis Postels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segu_M/0/1/0/all/0/1\">Mattia Segu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "A set of novel approaches for estimating epistemic uncertainty in deep neural\nnetworks with a single forward pass has recently emerged as a valid alternative\nto Bayesian Neural Networks. On the premise of informative representations,\nthese deterministic uncertainty methods (DUMs) achieve strong performance on\ndetecting out-of-distribution (OOD) data while adding negligible computational\ncosts at inference time. However, it remains unclear whether DUMs are well\ncalibrated and can seamlessly scale to real-world applications - both\nprerequisites for their practical deployment. To this end, we first provide a\ntaxonomy of DUMs, evaluate their calibration under continuous distributional\nshifts and their performance on OOD detection for image classification tasks.\nThen, we extend the most promising approaches to semantic segmentation. We find\nthat, while DUMs scale to realistic vision tasks and perform well on OOD\ndetection, the practicality of current methods is undermined by poor\ncalibration under realistic distributional shifts.",
          "link": "http://arxiv.org/abs/2107.00649",
          "publishedOn": "2021-07-15T01:59:04.389Z",
          "wordCount": 602,
          "title": "On the Practicality of Deterministic Epistemic Uncertainty. (arXiv:2107.00649v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06808",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_Q/0/1/0/all/0/1\">Qi Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1\">Qian Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_Y/0/1/0/all/0/1\">Yong Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "As a common weather, rain streaks adversely degrade the image quality. Hence,\nremoving rains from an image has become an important issue in the field. To\nhandle such an ill-posed single image deraining task, in this paper, we\nspecifically build a novel deep architecture, called rain convolutional\ndictionary network (RCDNet), which embeds the intrinsic priors of rain streaks\nand has clear interpretability. In specific, we first establish a RCD model for\nrepresenting rain streaks and utilize the proximal gradient descent technique\nto design an iterative algorithm only containing simple operators for solving\nthe model. By unfolding it, we then build the RCDNet in which every network\nmodule has clear physical meanings and corresponds to each operation involved\nin the algorithm. This good interpretability greatly facilitates an easy\nvisualization and analysis on what happens inside the network and why it works\nwell in inference process. Moreover, taking into account the domain gap issue\nin real scenarios, we further design a novel dynamic RCDNet, where the rain\nkernels can be dynamically inferred corresponding to input rainy images and\nthen help shrink the space for rain layer estimation with few rain maps so as\nto ensure a fine generalization performance in the inconsistent scenarios of\nrain types between training and testing data. By end-to-end training such an\ninterpretable network, all involved rain kernels and proximal operators can be\nautomatically extracted, faithfully characterizing the features of both rain\nand clean background layers, and thus naturally lead to better deraining\nperformance. Comprehensive experiments substantiate the superiority of our\nmethod, especially on its well generality to diverse testing scenarios and good\ninterpretability for all its modules. Code is available in\n\\emph{\\url{https://github.com/hongwang01/DRCDNet}}.",
          "link": "http://arxiv.org/abs/2107.06808",
          "publishedOn": "2021-07-15T01:59:04.260Z",
          "wordCount": 730,
          "title": "RCDNet: An Interpretable Rain Convolutional Dictionary Network for Single Image Deraining. (arXiv:2107.06808v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Muchen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1\">Leonid Sigal</a>",
          "description": "As an important step towards visual reasoning, visual grounding (e.g., phrase\nlocalization, referring expression comprehension/segmentation) has been widely\nexplored Previous approaches to referring expression comprehension (REC) or\nsegmentation (RES) either suffer from limited performance, due to a two-stage\nsetup, or require the designing of complex task-specific one-stage\narchitectures. In this paper, we propose a simple one-stage multi-task\nframework for visual grounding tasks. Specifically, we leverage a transformer\narchitecture, where two modalities are fused in a visual-lingual encoder. In\nthe decoder, the model learns to generate contextualized lingual queries which\nare then decoded and used to directly regress the bounding box and produce a\nsegmentation mask for the corresponding referred regions. With this simple but\nhighly contextualized model, we outperform state-of-the-arts methods by a large\nmargin on both REC and RES tasks. We also show that a simple pre-training\nschedule (on an external dataset) further improves the performance. Extensive\nexperiments and ablations illustrate that our model benefits greatly from\ncontextualized information and multi-task training.",
          "link": "http://arxiv.org/abs/2106.03089",
          "publishedOn": "2021-07-15T01:59:03.915Z",
          "wordCount": 624,
          "title": "Referring Transformer: A One-step Approach to Multi-task Visual Grounding. (arXiv:2106.03089v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bucci_S/0/1/0/all/0/1\">Silvia Bucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borlino_F/0/1/0/all/0/1\">Francesco Cappio Borlino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tommasi_T/0/1/0/all/0/1\">Tatiana Tommasi</a>",
          "description": "Vision systems trained in closed-world scenarios will inevitably fail when\npresented with new environmental conditions, new data distributions and novel\nclasses at deployment time. How to move towards open-world learning is a long\nstanding research question, but the existing solutions mainly focus on specific\naspects of the problem (single domain Open-Set, multi-domain Closed-Set), or\npropose complex strategies which combine multiple losses and manually tuned\nhyperparameters. In this work we tackle multi-source Open-Set domain adaptation\nby introducing HyMOS: a straightforward supervised model that exploits the\npower of contrastive learning and the properties of its hyperspherical feature\nspace to correctly predict known labels on the target, while rejecting samples\nbelonging to any unknown class. HyMOS includes a tailored data balancing to\nenforce cross-source alignment and introduces style transfer among the instance\ntransformations of contrastive learning for source-target adaptation, avoiding\nthe risk of negative transfer. Finally a self-training strategy refines the\nmodel without the need for handcrafted thresholds. We validate our method over\nthree challenging datasets and provide an extensive quantitative and\nqualitative experimental analysis. The obtained results show that HyMOS\noutperforms several Open-Set and universal domain adaptation approaches,\ndefining the new state-of-the-art.",
          "link": "http://arxiv.org/abs/2107.02067",
          "publishedOn": "2021-07-15T01:59:03.857Z",
          "wordCount": 644,
          "title": "Distance-based Hyperspherical Classification for Multi-source Open-Set Domain Adaptation. (arXiv:2107.02067v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06532",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lao_L/0/1/0/all/0/1\">Lingjie Lao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>",
          "description": "Cartoon face recognition is challenging as they typically have smooth color\nregions and emphasized edges, the key to recognize cartoon faces is to\nprecisely perceive their sparse and critical shape patterns. However, it is\nquite difficult to learn a shape-oriented representation for cartoon face\nrecognition with convolutional neural networks (CNNs). To mitigate this issue,\nwe propose the GraphJigsaw that constructs jigsaw puzzles at various stages in\nthe classification network and solves the puzzles with the graph convolutional\nnetwork (GCN) in a progressive manner. Solving the puzzles requires the model\nto spot the shape patterns of the cartoon faces as the texture information is\nquite limited. The key idea of GraphJigsaw is constructing a jigsaw puzzle by\nrandomly shuffling the intermediate convolutional feature maps in the spatial\ndimension and exploiting the GCN to reason and recover the correct layout of\nthe jigsaw fragments in a self-supervised manner. The proposed GraphJigsaw\navoids training the classification model with the deconstructed images that\nwould introduce noisy patterns and are harmful for the final classification.\nSpecially, GraphJigsaw can be incorporated at various stages in a top-down\nmanner within the classification model, which facilitates propagating the\nlearned shape patterns gradually. GraphJigsaw does not rely on any extra manual\nannotation during the training process and incorporates no extra computation\nburden at inference time. Both quantitative and qualitative experimental\nresults have verified the feasibility of our proposed GraphJigsaw, which\nconsistently outperforms other face recognition or jigsaw-based methods on two\npopular cartoon face datasets with considerable improvements.",
          "link": "http://arxiv.org/abs/2107.06532",
          "publishedOn": "2021-07-15T01:59:03.810Z",
          "wordCount": 683,
          "title": "Graph Jigsaw Learning for Cartoon Face Recognition. (arXiv:2107.06532v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Inoue_S/0/1/0/all/0/1\">Sho Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonsalves_T/0/1/0/all/0/1\">Tad Gonsalves</a>",
          "description": "Unpaired image-to-image translation using Generative Adversarial Networks\n(GAN) is successful in converting images among multiple domains. Moreover,\nrecent studies have shown a way to diversify the outputs of the generator.\nHowever, since there are no restrictions on how the generator diversifies the\nresults, it is likely to translate some unexpected features. In this paper, we\npropose Style-Restricted GAN (SRGAN) to demonstrate the importance of\ncontrolling the encoded features used in style diversifying process. More\nspecifically, instead of KL divergence loss, we adopt three new losses to\nrestrict the distribution of the encoded features: batch KL divergence loss,\ncorrelation loss, and histogram imitation loss. Further, the encoder is\npre-trained with classification tasks before being used in translation process.\nThe study reports quantitative as well as qualitative results with Precision,\nRecall, Density, and Coverage. The proposed three losses lead to the\nenhancement of the level of diversity compared to the conventional KL loss. In\nparticular, SRGAN is found to be successful in translating with higher\ndiversity and without changing the class-unrelated features in the CelebA face\ndataset. To conclude, the importance of the encoded features being\nwell-regulated was proven with two experiments. Our implementation is available\nat https://github.com/shinshoji01/Style-Restricted_GAN.",
          "link": "http://arxiv.org/abs/2105.07621",
          "publishedOn": "2021-07-15T01:59:03.804Z",
          "wordCount": 674,
          "title": "Style-Restricted GAN: Multi-Modal Translation with Style Restriction Using Generative Adversarial Networks. (arXiv:2105.07621v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingjian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>",
          "description": "Vision transformer has achieved competitive performance on a variety of\ncomputer vision applications. However, their storage, run-time memory, and\ncomputational demands are hindering the deployment to mobile devices. Here we\npresent a vision transformer pruning approach, which identifies the impacts of\ndimensions in each layer of transformer and then executes pruning accordingly.\nBy encouraging dimension-wise sparsity in the transformer, important dimensions\nautomatically emerge. A great number of dimensions with small importance scores\ncan be discarded to achieve a high pruning ratio without significantly\ncompromising accuracy. The pipeline for vision transformer pruning is as\nfollows: 1) training with sparsity regularization; 2) pruning dimensions of\nlinear projections; 3) fine-tuning. The reduced parameters and FLOPs ratios of\nthe proposed algorithm are well evaluated and analyzed on ImageNet dataset to\ndemonstrate the effectiveness of our proposed method.",
          "link": "http://arxiv.org/abs/2104.08500",
          "publishedOn": "2021-07-15T01:59:03.447Z",
          "wordCount": 607,
          "title": "Visual Transformer Pruning. (arXiv:2104.08500v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.03464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiao Wang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tingzhang Zhao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaojun Ye</a> (2) ((1) Department of Computer Science, University of Science and Technology Beijing (2) School of Software, Tsinghua University)",
          "description": "Feature selection, an effective technique for dimensionality reduction, plays\nan important role in many machine learning systems. Supervised knowledge can\nsignificantly improve the performance. However, faced with the rapid growth of\nnewly emerging concepts, existing supervised methods might easily suffer from\nthe scarcity and validity of labeled data for training. In this paper, the\nauthors study the problem of zero-shot feature selection (i.e., building a\nfeature selection model that generalizes well to \"unseen\" concepts with limited\ntraining data of \"seen\" concepts). Specifically, they adopt class-semantic\ndescriptions (i.e., attributes) as supervision for feature selection, so as to\nutilize the supervised knowledge transferred from the seen concepts. For more\nreliable discriminative features, they further propose the\ncenter-characteristic loss which encourages the selected features to capture\nthe central characteristics of seen concepts. Extensive experiments conducted\non various real-world datasets demonstrate the effectiveness of the method.",
          "link": "http://arxiv.org/abs/1908.03464",
          "publishedOn": "2021-07-15T01:59:03.372Z",
          "wordCount": 643,
          "title": "Zero-Shot Feature Selection via Transferring Supervised Knowledge. (arXiv:1908.03464v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jia-Xin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaowei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>",
          "description": "While self-supervised representation learning (SSL) has received widespread\nattention from the community, recent research argue that its performance will\nsuffer a cliff fall when the model size decreases. The current method mainly\nrelies on contrastive learning to train the network and in this work, we\npropose a simple yet effective Distilled Contrastive Learning (DisCo) to ease\nthe issue by a large margin. Specifically, we find the final embedding obtained\nby the mainstream SSL methods contains the most fruitful information, and\npropose to distill the final embedding to maximally transmit a teacher's\nknowledge to a lightweight model by constraining the last embedding of the\nstudent to be consistent with that of the teacher. In addition, in the\nexperiment, we find that there exists a phenomenon termed Distilling BottleNeck\nand present to enlarge the embedding dimension to alleviate this problem. Our\nmethod does not introduce any extra parameter to lightweight models during\ndeployment. Experimental results demonstrate that our method achieves the\nstate-of-the-art on all lightweight models. Particularly, when\nResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear\nresult of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50,\nbut the number of parameters of EfficientNet-B0 is only 9.4%/16.3% of\nResNet-101/ResNet-50.",
          "link": "http://arxiv.org/abs/2104.09124",
          "publishedOn": "2021-07-15T01:59:03.232Z",
          "wordCount": 683,
          "title": "DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning. (arXiv:2104.09124v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ashukha_A/0/1/0/all/0/1\">Arsenii Ashukha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1\">Andrei Atanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1\">Dmitry Vetrov</a>",
          "description": "Averaging predictions over a set of models -- an ensemble -- is widely used\nto improve predictive performance and uncertainty estimation of deep learning\nmodels. At the same time, many machine learning systems, such as search,\nmatching, and recommendation systems, heavily rely on embeddings.\nUnfortunately, due to misalignment of features of independently trained models,\nembeddings, cannot be improved with a naive deep ensemble like approach. In\nthis work, we look at the ensembling of representations and propose mean\nembeddings with test-time augmentation (MeTTA) simple yet well-performing\nrecipe for ensembling representations. Empirically we demonstrate that MeTTA\nsignificantly boosts the quality of linear evaluation on ImageNet for both\nsupervised and self-supervised models. Even more exciting, we draw connections\nbetween MeTTA, image retrieval, and transformation invariant models. We believe\nthat spreading the success of ensembles to inference higher-quality\nrepresentations is the important step that will open many new applications of\nensembling.",
          "link": "http://arxiv.org/abs/2106.08038",
          "publishedOn": "2021-07-15T01:59:03.226Z",
          "wordCount": 615,
          "title": "Mean Embeddings with Test-Time Data Augmentation for Ensembling of Representations. (arXiv:2106.08038v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03348",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yufei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Transformers have shown great potential in various computer vision tasks\nowing to their strong capability in modeling long-range dependency using the\nself-attention mechanism. Nevertheless, vision transformers treat an image as\n1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in\nmodeling local visual structures and dealing with scale variance.\nAlternatively, they require large-scale training data and longer training\nschedules to learn the IB implicitly. In this paper, we propose a novel Vision\nTransformer Advanced by Exploring intrinsic IB from convolutions, ie, ViTAE.\nTechnically, ViTAE has several spatial pyramid reduction modules to downsample\nand embed the input image into tokens with rich multi-scale context by using\nmultiple convolutions with different dilation rates. In this way, it acquires\nan intrinsic scale invariance IB and is able to learn robust feature\nrepresentation for objects at various scales. Moreover, in each transformer\nlayer, ViTAE has a convolution block in parallel to the multi-head\nself-attention module, whose features are fused and fed into the feed-forward\nnetwork. Consequently, it has the intrinsic locality IB and is able to learn\nlocal features and global dependencies collaboratively. Experiments on ImageNet\nas well as downstream tasks prove the superiority of ViTAE over the baseline\ntransformer and concurrent works. Source code and pretrained models will be\navailable at GitHub.",
          "link": "http://arxiv.org/abs/2106.03348",
          "publishedOn": "2021-07-15T01:59:03.220Z",
          "wordCount": 694,
          "title": "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias. (arXiv:2106.03348v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1\">Dan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1\">Bastian Leibe</a>",
          "description": "In this preliminary work we attempt to apply submanifold sparse convolution\nto the task of 3D person detection. In particular, we present Person-MinkUNet,\na single-stage 3D person detection network based on Minkowski Engine with U-Net\narchitecture. The network achieves a 76.4% average precision (AP) on the JRDB\n3D detection benchmark.",
          "link": "http://arxiv.org/abs/2107.06780",
          "publishedOn": "2021-07-15T01:59:03.202Z",
          "wordCount": 497,
          "title": "Person-MinkUNet: 3D Person Detection with LiDAR Point Cloud. (arXiv:2107.06780v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shubin_D/0/1/0/all/0/1\">Dmitrii Shubin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eytan_D/0/1/0/all/0/1\">Danny Eytan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodfellow_S/0/1/0/all/0/1\">Sebastian D. Goodfellow</a>",
          "description": "Self-supervised learning methods for computer vision have demonstrated the\neffectiveness of pre-training feature representations, resulting in\nwell-generalizing Deep Neural Networks, even if the annotated data are limited.\nHowever, representation learning techniques require a significant amount of\ntime for model training, with most of it time spent on precise hyper-parameter\noptimization and selection of augmentation techniques. We hypothesized that if\nthe annotated dataset has enough morphological diversity to capture the general\npopulation's as is common in medical imaging, for example, due to conserved\nsimilarities of tissue mythologies, the variance error of the trained model is\nthe prevalent component of the Bias-Variance Trade-off. We propose the Variance\nAware Training (VAT) method that exploits this property by introducing the\nvariance error into the model loss function, i.e., enabling minimizing the\nvariance explicitly. Additionally, we provide the theoretical formulation and\nproof of the proposed method to aid in interpreting the approach. Our method\nrequires selecting only one hyper-parameter and was able to match or improve\nthe state-of-the-art performance of self-supervised methods while achieving an\norder of magnitude reduction in the GPU training time. We validated VAT on\nthree medical imaging datasets from diverse domains and various learning\nobjectives. These included a Magnetic Resonance Imaging (MRI) dataset for the\nheart semantic segmentation (MICCAI 2017 ACDC challenge), fundus photography\ndataset for ordinary regression of diabetic retinopathy progression (Kaggle\n2019 APTOS Blindness Detection challenge), and classification of\nhistopathologic scans of lymph node sections (PatchCamelyon dataset).",
          "link": "http://arxiv.org/abs/2105.14117",
          "publishedOn": "2021-07-15T01:59:03.195Z",
          "wordCount": 724,
          "title": "About Explicit Variance Minimization: Training Neural Networks for Medical Imaging With Limited Data Annotations. (arXiv:2105.14117v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shaghouri_A/0/1/0/all/0/1\">Anas Al Shaghouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alkhatib_R/0/1/0/all/0/1\">Rami Alkhatib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berjaoui_S/0/1/0/all/0/1\">Samir Berjaoui</a>",
          "description": "Roads are connecting line between different places, and used daily. Roads'\nperiodic maintenance keeps them safe and functional. Detecting and reporting\nthe existence of potholes to responsible departments can help in eliminating\nthem. This study deployed and tested on different deep learning architecture to\ndetect potholes. The images used for training were collected by cellphone\nmounted on the windshield of the car, in addition to many images downloaded\nfrom the internet to increase the size and variability of the database. Second,\nvarious object detection algorithms are employed and compared to detect\npotholes in real-time like SDD-TensorFlow, YOLOv3Darknet53 and YOLOv4Darknet53.\nYOLOv4 achieved the best performance with 81% recall, 85% precision and 85.39%\nmean Average Precision (mAP). The speed of processing was 20 frame per second.\nThe system was able to detect potholes from a range on 100 meters away from the\ncamera. The system can increase the safety of drivers and improve the\nperformance of self-driving cars by detecting pothole time ahead.",
          "link": "http://arxiv.org/abs/2107.06356",
          "publishedOn": "2021-07-15T01:59:03.187Z",
          "wordCount": 606,
          "title": "Real-Time Pothole Detection Using Deep Learning. (arXiv:2107.06356v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pei_M/0/1/0/all/0/1\">Meiqi Pei</a>",
          "description": "In recent years, monocular depth estimation is applied to understand the\nsurrounding 3D environment and has made great progress. However, there is an\nill-posed problem on how to gain depth information directly from a single\nimage. With the rapid development of deep learning, this problem is possible to\nbe solved. Although more and more approaches are proposed one after another,\nmost of existing methods inevitably lost details due to continuous downsampling\nwhen mapping from RGB space to depth space. To the end, we design a Multi-scale\nFeatures Network (MSFNet), which consists of Enhanced Diverse Attention (EDA)\nmodule and Upsample-Stage Fusion (USF) module. The EDA module employs the\nspatial attention method to learn significant spatial information, while USF\nmodule complements low-level detail information with high-level semantic\ninformation from the perspective of multi-scale feature fusion to improve the\npredicted effect. In addition, since the simple samples are always trained to a\nbetter effect first, the hard samples are difficult to converge. Therefore, we\ndesign a batch-loss to assign large loss factors to the harder samples in a\nbatch. Experiments on NYU-Depth V2 dataset and KITTI dataset demonstrate that\nour proposed approach is more competitive with the state-of-the-art methods in\nboth qualitative and quantitative evaluation.",
          "link": "http://arxiv.org/abs/2107.06445",
          "publishedOn": "2021-07-15T01:59:03.179Z",
          "wordCount": 634,
          "title": "MSFNet:Multi-scale features network for monocular depth estimation. (arXiv:2107.06445v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Trinh Man Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinjia Zhou</a>",
          "description": "COVID-19 leads to the high demand for remote interactive systems ever seen.\nOne of the key elements of these systems is video streaming, which requires a\nvery high network bandwidth due to its specific real-time demand, especially\nwith high-resolution video. Existing video compression methods are struggling\nin the trade-off between video quality and the speed requirement. Addressed\nthat the background information rarely changes in most remote meeting cases, we\nintroduce a Region-Of-Interests (ROI) based video compression framework (named\nRCLC) that leverages the cutting-edge learning-based and conventional\ntechnologies. In RCLC, each coming frame is marked as a background-updating\n(BU) or ROI-updating (RU) frame. By applying the conventional video codec, the\nBU frame is compressed with low-quality and high-compression, while the ROI\nfrom RU-frame is compressed with high-quality and low-compression. The\nlearning-based methods are applied to detect the ROI, blend background-ROI, and\nenhance video quality. The experimental results show that our RCLC can reduce\nup to 32.55\\% BD-rate for the ROI region compared to H.265 video codec under a\nsimilar compression time with 1080p resolution.",
          "link": "http://arxiv.org/abs/2107.06492",
          "publishedOn": "2021-07-15T01:59:03.173Z",
          "wordCount": 663,
          "title": "RCLC: ROI-based joint conventional and learning video compression. (arXiv:2107.06492v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyulim Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">JeongSoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Seungri Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jun-Ho Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_C/0/1/0/all/0/1\">Chulmin Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jong-Seok Lee</a>",
          "description": "A significant amount of work has been done on adversarial attacks that inject\nimperceptible noise to images to deteriorate the image classification\nperformance of deep models. However, most of the existing studies consider\nattacks in the digital (pixel) domain where an image acquired by an image\nsensor with sampling and quantization has been recorded. This paper, for the\nfirst time, introduces an optical adversarial attack, which physically alters\nthe light field information arriving at the image sensor so that the\nclassification model yields misclassification. More specifically, we modulate\nthe phase of the light in the Fourier domain using a spatial light modulator\nplaced in the photographic system. The operative parameters of the modulator\nare obtained by gradient-based optimization to maximize cross-entropy and\nminimize distortions. We present experiments based on both simulation and a\nreal hardware optical system, from which the feasibility of the proposed\noptical attack is demonstrated. It is also verified that the proposed attack is\ncompletely different from common optical-domain distortions such as spherical\naberration, defocus, and astigmatism in terms of both perturbation patterns and\nclassification results.",
          "link": "http://arxiv.org/abs/2106.09908",
          "publishedOn": "2021-07-15T01:59:03.157Z",
          "wordCount": 654,
          "title": "Light Lies: Optical Adversarial Attack. (arXiv:2106.09908v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1\">Laurent Kneip</a>",
          "description": "Camera calibration is an important prerequisite towards the solution of 3D\ncomputer vision problems. Traditional methods rely on static images of a\ncalibration pattern. This raises interesting challenges towards the practical\nusage of event cameras, which notably require image change to produce\nsufficient measurements. The current standard for event camera calibration\ntherefore consists of using flashing patterns. They have the advantage of\nsimultaneously triggering events in all reprojected pattern feature locations,\nbut it is difficult to construct or use such patterns in the field. We present\nthe first dynamic event camera calibration algorithm. It calibrates directly\nfrom events captured during relative motion between camera and calibration\npattern. The method is propelled by a novel feature extraction mechanism for\ncalibration patterns, and leverages existing calibration tools before\noptimizing all parameters through a multi-segment continuous-time formulation.\nAs demonstrated through our results on real data, the obtained calibration\nmethod is highly convenient and reliably calibrates from data sequences\nspanning less than 10 seconds.",
          "link": "http://arxiv.org/abs/2107.06749",
          "publishedOn": "2021-07-15T01:59:03.151Z",
          "wordCount": 603,
          "title": "Dynamic Event Camera Calibration. (arXiv:2107.06749v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.05630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yinan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_B/0/1/0/all/0/1\">Bei Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1\">Guojun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Luchuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1\">Lu Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>",
          "description": "The rapid progress of photorealistic synthesis techniques has reached at a\ncritical point where the boundary between real and manipulated images starts to\nblur. Thus, benchmarking and advancing digital forgery analysis have become a\npressing issue. However, existing face forgery datasets either have limited\ndiversity or only support coarse-grained analysis. To counter this emerging\nthreat, we construct the ForgeryNet dataset, an extremely large face forgery\ndataset with unified annotations in image- and video-level data across four\ntasks: 1) Image Forgery Classification, including two-way (real / fake),\nthree-way (real / fake with identity-replaced forgery approaches / fake with\nidentity-remained forgery approaches), and n-way (real and 15 respective\nforgery approaches) classification. 2) Spatial Forgery Localization, which\nsegments the manipulated area of fake images compared to their corresponding\nsource real images. 3) Video Forgery Classification, which re-defines the\nvideo-level forgery classification with manipulated frames in random positions.\nThis task is important because attackers in real world are free to manipulate\nany target frame. and 4) Temporal Forgery Localization, to localize the\ntemporal segments which are manipulated. ForgeryNet is by far the largest\npublicly available deep face forgery dataset in terms of data-scale (2.9\nmillion images, 221,247 videos), manipulations (7 image-level approaches, 8\nvideo-level approaches), perturbations (36 independent and more mixed\nperturbations) and annotations (6.3 million classification labels, 2.9 million\nmanipulated area annotations and 221,247 temporal forgery segment labels). We\nperform extensive benchmarking and studies of existing face forensics methods\nand obtain several valuable observations.",
          "link": "http://arxiv.org/abs/2103.05630",
          "publishedOn": "2021-07-15T01:59:03.144Z",
          "wordCount": 735,
          "title": "ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis. (arXiv:2103.05630v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bartz_C/0/1/0/all/0/1\">Christian Bartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratz_H/0/1/0/all/0/1\">Hendrik R&#xe4;tz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haojin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_J/0/1/0/all/0/1\">Joseph Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1\">Christoph Meinel</a>",
          "description": "One of the most pressing problems in the automated analysis of historical\ndocuments is the availability of annotated training data. In this paper, we\npropose a novel method for the synthesis of training data for semantic\nsegmentation of document images. We utilize clusters found in intermediate\nfeatures of a StyleGAN generator for the synthesis of RGB and label images at\nthe same time. Our model can be applied to any dataset of scanned documents\nwithout the need for manual annotation of individual images, as each model is\ncustom-fit to the dataset. In our experiments, we show that models trained on\nour synthetic data can reach competitive performance on open benchmark datasets\nfor line segmentation.",
          "link": "http://arxiv.org/abs/2107.06777",
          "publishedOn": "2021-07-15T01:59:03.136Z",
          "wordCount": 569,
          "title": "Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data. (arXiv:2107.06777v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roxo_T/0/1/0/all/0/1\">Tiago Roxo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proenca_H/0/1/0/all/0/1\">Hugo Proen&#xe7;a</a>",
          "description": "Soft biometrics inference in surveillance scenarios is a topic of interest\nfor various applications, particularly in security-related areas. However, soft\nbiometric analysis is not extensively reported in wild conditions. In\nparticular, previous works on gender recognition report their results in face\ndatasets, with relatively good image quality and frontal poses. Given the\nuncertainty of the availability of the facial region in wild conditions, we\nconsider that these methods are not adequate for surveillance settings. To\novercome these limitations, we: 1) present frontal and wild face versions of\nthree well-known surveillance datasets; and 2) propose a model that effectively\nand dynamically combines facial and body information, which makes it suitable\nfor gender recognition in wild conditions. The frontal and wild face datasets\nderive from widely used Pedestrian Attribute Recognition (PAR) sets (PETA,\nPA-100K, and RAP), using a pose-based approach to filter the frontal samples\nand facial regions. This approach retrieves the facial region of images with\nvarying image/subject conditions, where the state-of-the-art face detectors\noften fail. Our model combines facial and body information through a learnable\nfusion matrix and a channel-attention sub-network, focusing on the most\ninfluential body parts according to the specific image/subject features. We\ncompare it with five PAR methods, consistently obtaining state-of-the-art\nresults on gender recognition, and reducing the prediction errors by up to 24%\nin frontal samples. The announced PAR datasets versions and model serve as the\nbasis for wild soft biometrics classification and are available in\nhttps://github.com/Tiago-Roxo.",
          "link": "http://arxiv.org/abs/2107.06847",
          "publishedOn": "2021-07-15T01:59:03.128Z",
          "wordCount": 678,
          "title": "Faces in the Wild: Efficient Gender Recognition in Surveillance Conditions. (arXiv:2107.06847v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yang-tian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hao-zhi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lin Gao</a>",
          "description": "Pose transfer of human videos aims to generate a high fidelity video of a\ntarget person imitating actions of a source person. A few studies have made\ngreat progress either through image translation with deep latent features or\nneural rendering with explicit 3D features. However, both of them rely on large\namounts of training data to generate realistic results, and the performance\ndegrades on more accessible internet videos due to insufficient training\nframes. In this paper, we demonstrate that the dynamic details can be preserved\neven trained from short monocular videos. Overall, we propose a neural video\nrendering framework coupled with an image-translation-based dynamic details\ngeneration network (D2G-Net), which fully utilizes both the stability of\nexplicit 3D features and the capacity of learning components. To be specific, a\nnovel texture representation is presented to encode both the static and\npose-varying appearance characteristics, which is then mapped to the image\nspace and rendered as a detail-rich frame in the neural rendering stage.\nMoreover, we introduce a concise temporal loss in the training stage to\nsuppress the detail flickering that is made more visible due to high-quality\ndynamic details generated by our method. Through extensive comparisons, we\ndemonstrate that our neural human video renderer is capable of achieving both\nclearer dynamic details and more robust performance even on accessible short\nvideos with only 2k - 4k frames.",
          "link": "http://arxiv.org/abs/2106.14132",
          "publishedOn": "2021-07-15T01:59:03.111Z",
          "wordCount": 707,
          "title": "Robust Pose Transfer with Dynamic Details using Neural Video Rendering. (arXiv:2106.14132v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Lei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaofu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaojie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuebin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruzzone_L/0/1/0/all/0/1\">Lorenzo Bruzzone</a>",
          "description": "Long-range context information is crucial for the semantic segmentation of\nHigh-Resolution (HR) Remote Sensing Images (RSIs). The image cropping\noperations, commonly used for training neural networks, limit the perception of\nlong-range context information in large RSIs. To break this limitation, we\npropose a Wide-Context Network (WiCoNet) for the semantic segmentation of HR\nRSIs. In the WiCoNet, apart from a conventional feature extraction network that\naggregates the local information, an extra context branch is designed to\nexplicitly model the spatial information in a larger image area. The\ninformation between the two branches is communicated through a Context\nTransformer, which is a novel design derived from the Vision Transformer to\nmodel the long-range context correlations. Ablation studies and comparative\nexperiments conducted on several benchmark datasets prove the effectiveness of\nthe proposed method. In addition, we present a new Beijing Land-Use (BLU)\ndataset. This is a large-scale HR satellite dataset provided with high-quality\nand fine-grained reference labels, which can boost future studies in this\nfield.",
          "link": "http://arxiv.org/abs/2106.15754",
          "publishedOn": "2021-07-15T01:59:03.100Z",
          "wordCount": 646,
          "title": "Looking Outside the Window: Wide-Context Transformer for the Semantic Segmentation of High-Resolution Remote Sensing Images. (arXiv:2106.15754v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reinhold_J/0/1/0/all/0/1\">Jacob C. Reinhold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carass_A/0/1/0/all/0/1\">Aaron Carass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1\">Jerry L. Prince</a>",
          "description": "Precision medicine involves answering counterfactual questions such as \"Would\nthis patient respond better to treatment A or treatment B?\" These types of\nquestions are causal in nature and require the tools of causal inference to be\nanswered, e.g., with a structural causal model (SCM). In this work, we develop\nan SCM that models the interaction between demographic information, disease\ncovariates, and magnetic resonance (MR) images of the brain for people with\nmultiple sclerosis. Inference in the SCM generates counterfactual images that\nshow what an MR image of the brain would look like if demographic or disease\ncovariates are changed. These images can be used for modeling disease\nprogression or used for image processing tasks where controlling for\nconfounders is necessary.",
          "link": "http://arxiv.org/abs/2103.03158",
          "publishedOn": "2021-07-15T01:59:03.094Z",
          "wordCount": 616,
          "title": "A Structural Causal Model for MR Images of Multiple Sclerosis. (arXiv:2103.03158v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lukyanenko_S/0/1/0/all/0/1\">Stanislav Lukyanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_W/0/1/0/all/0/1\">Won-Dong Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Donglai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Struyven_R/0/1/0/all/0/1\">Robbert Struyven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leahy_B/0/1/0/all/0/1\">Brian Leahy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Helen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander Rush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Yosef_D/0/1/0/all/0/1\">Dalit Ben-Yosef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Needleman_D/0/1/0/all/0/1\">Daniel Needleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>",
          "description": "The developmental process of embryos follows a monotonic order. An embryo can\nprogressively cleave from one cell to multiple cells and finally transform to\nmorula and blastocyst. For time-lapse videos of embryos, most existing\ndevelopmental stage classification methods conduct per-frame predictions using\nan image frame at each time step. However, classification using only images\nsuffers from overlapping between cells and imbalance between stages. Temporal\ninformation can be valuable in addressing this problem by capturing movements\nbetween neighboring frames. In this work, we propose a two-stream model for\ndevelopmental stage classification. Unlike previous methods, our two-stream\nmodel accepts both temporal and image information. We develop a linear-chain\nconditional random field (CRF) on top of neural network features extracted from\nthe temporal and image streams to make use of both modalities. The linear-chain\nCRF formulation enables tractable training of global sequential models over\nmultiple frames while also making it possible to inject monotonic development\norder constraints into the learning process explicitly. We demonstrate our\nalgorithm on two time-lapse embryo video datasets: i) mouse and ii) human\nembryo datasets. Our method achieves 98.1 % and 80.6 % for mouse and human\nembryo stage classification, respectively. Our approach will enable more\nprofound clinical and biological studies and suggests a new direction for\ndevelopmental stage classification by utilizing temporal information.",
          "link": "http://arxiv.org/abs/2107.06360",
          "publishedOn": "2021-07-15T01:59:03.089Z",
          "wordCount": 683,
          "title": "Developmental Stage Classification of EmbryosUsing Two-Stream Neural Network with Linear-Chain Conditional Random Field. (arXiv:2107.06360v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1\">Ning Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1\">Jiajun Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sheng Zhou</a>",
          "description": "Present domain adaptation methods usually perform explicit representation\nalignment by simultaneously accessing the source data and target data. However,\nthe source data are not always available due to the privacy preserving\nconsideration or bandwidth limitation. Source-free domain adaptation aims to\nsolve the above problem by performing domain adaptation without accessing the\nsource data. The adaptation paradigm is receiving more and more attention in\nrecent years, and multiple works have been proposed for unsupervised\nsource-free domain adaptation. However, without utilizing any supervised signal\nand source data at the adaptation stage, the optimization of the target model\nis unstable and fragile. To alleviate the problem, we focus on semi-supervised\ndomain adaptation under source-free setting. More specifically, we propose\nuncertainty-guided Mixup to reduce the representation's intra-domain\ndiscrepancy and perform inter-domain alignment without directly accessing the\nsource data. Finally, we conduct extensive semi-supervised domain adaptation\nexperiments on various datasets. Our method outperforms the recent\nsemi-supervised baselines and the unsupervised variant also achieves\ncompetitive performance. The experiment codes will be released in the future.",
          "link": "http://arxiv.org/abs/2107.06707",
          "publishedOn": "2021-07-15T01:59:03.083Z",
          "wordCount": 606,
          "title": "Uncertainty-Guided Mixup for Semi-Supervised Domain Adaptation without Source Data. (arXiv:2107.06707v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06538",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinda Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>",
          "description": "Fine-grained image recognition is challenging because discriminative clues\nare usually fragmented, whether from a single image or multiple images. Despite\ntheir significant improvements, most existing methods still focus on the most\ndiscriminative parts from a single image, ignoring informative details in other\nregions and lacking consideration of clues from other associated images. In\nthis paper, we analyze the difficulties of fine-grained image recognition from\na new perspective and propose a transformer architecture with the peak\nsuppression module and knowledge guidance module, which respects the\ndiversification of discriminative features in a single image and the\naggregation of discriminative clues among multiple images. Specifically, the\npeak suppression module first utilizes a linear projection to convert the input\nimage into sequential tokens. It then blocks the token based on the attention\nresponse generated by the transformer encoder. This module penalizes the\nattention to the most discriminative parts in the feature learning process,\ntherefore, enhancing the information exploitation of the neglected regions. The\nknowledge guidance module compares the image-based representation generated\nfrom the peak suppression module with the learnable knowledge embedding set to\nobtain the knowledge response coefficients. Afterwards, it formalizes the\nknowledge learning as a classification problem using response coefficients as\nthe classification scores. Knowledge embeddings and image-based representations\nare updated during training so that the knowledge embedding includes\ndiscriminative clues for different images. Finally, we incorporate the acquired\nknowledge embeddings into the image-based representations as comprehensive\nrepresentations, leading to significantly higher performance. Extensive\nevaluations on the six popular datasets demonstrate the advantage of the\nproposed method.",
          "link": "http://arxiv.org/abs/2107.06538",
          "publishedOn": "2021-07-15T01:59:03.066Z",
          "wordCount": 693,
          "title": "Transformer with Peak Suppression and Knowledge Guidance for Fine-grained Image Recognition. (arXiv:2107.06538v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Doughty_M/0/1/0/all/0/1\">Mitchell Doughty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Karan Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghugre_N/0/1/0/all/0/1\">Nilesh R. Ghugre</a>",
          "description": "We present SurgeonAssist-Net: a lightweight framework making\naction-and-workflow-driven virtual assistance, for a set of predefined surgical\ntasks, accessible to commercially available optical see-through head-mounted\ndisplays (OST-HMDs). On a widely used benchmark dataset for laparoscopic\nsurgical workflow, our implementation competes with state-of-the-art approaches\nin prediction accuracy for automated task recognition, and yet requires 7.4x\nfewer parameters, 10.2x fewer floating point operations per second (FLOPS), is\n7.0x faster for inference on a CPU, and is capable of near real-time\nperformance on the Microsoft HoloLens 2 OST-HMD. To achieve this, we make use\nof an efficient convolutional neural network (CNN) backbone to extract\ndiscriminative features from image data, and a low-parameter recurrent neural\nnetwork (RNN) architecture to learn long-term temporal dependencies. To\ndemonstrate the feasibility of our approach for inference on the HoloLens 2 we\ncreated a sample dataset that included video of several surgical tasks recorded\nfrom a user-centric point-of-view. After training, we deployed our model and\ncataloged its performance in an online simulated surgical scenario for the\nprediction of the current surgical task. The utility of our approach is\nexplored in the discussion of several relevant clinical use-cases. Our code is\npublicly available at https://github.com/doughtmw/surgeon-assist-net.",
          "link": "http://arxiv.org/abs/2107.06397",
          "publishedOn": "2021-07-15T01:59:03.060Z",
          "wordCount": 644,
          "title": "SurgeonAssist-Net: Towards Context-Aware Head-Mounted Display-Based Augmented Reality for Surgical Guidance. (arXiv:2107.06397v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reichardt_L/0/1/0/all/0/1\">Laurenz Reichardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangat_P/0/1/0/all/0/1\">Patrick Mangat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasenmuller_O/0/1/0/all/0/1\">Oliver Wasenm&#xfc;ller</a>",
          "description": "LiDAR depth maps provide environmental guidance in a variety of applications.\nHowever, such depth maps are typically sparse and insufficient for complex\ntasks such as autonomous navigation. State of the art methods use image guided\nneural networks for dense depth completion. We develop a guided convolutional\nneural network focusing on gathering dense and valid information from sparse\ndepth maps. To this end, we introduce a novel layer with spatially variant and\ncontent-depended dilation to include additional data from sparse input.\nFurthermore, we propose a sparsity invariant residual bottleneck block. We\nevaluate our Dense Validity Mask Network (DVMN) on the KITTI depth completion\nbenchmark and achieve state of the art results. At the time of submission, our\nnetwork is the leading method using sparsity invariant convolution.",
          "link": "http://arxiv.org/abs/2107.06709",
          "publishedOn": "2021-07-15T01:59:03.055Z",
          "wordCount": 580,
          "title": "DVMN: Dense Validity Mask Network for Depth Completion. (arXiv:2107.06709v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06463",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Haisheng Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_F/0/1/0/all/0/1\">Feng Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jianping Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akbari_M/0/1/0/all/0/1\">Mohammad Akbari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1\">Jie Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Guohe Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tu_C/0/1/0/all/0/1\">Chengjie Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_J/0/1/0/all/0/1\">Jingning Han</a>",
          "description": "Recently deep learning-based image compression methods have achieved\nsignificant achievements and gradually outperformed traditional approaches\nincluding the latest standard Versatile Video Coding (VVC) in both PSNR and\nMS-SSIM metrics. Two key components of learned image compression frameworks are\nthe entropy model of the latent representations and the encoding/decoding\nnetwork architectures. Various models have been proposed, such as\nautoregressive, softmax, logistic mixture, Gaussian mixture, and Laplacian.\nExisting schemes only use one of these models. However, due to the vast\ndiversity of images, it is not optimal to use one model for all images, even\ndifferent regions of one image. In this paper, we propose a more flexible\ndiscretized Gaussian-Laplacian-Logistic mixture model (GLLMM) for the latent\nrepresentations, which can adapt to different contents in different images and\ndifferent regions of one image more accurately. Besides, in the\nencoding/decoding network design part, we propose a concatenated residual\nblocks (CRB), where multiple residual blocks are serially connected with\nadditional shortcut connections. The CRB can improve the learning ability of\nthe network, which can further improve the compression performance.\nExperimental results using the Kodak and Tecnick datasets show that the\nproposed scheme outperforms all the state-of-the-art learning-based methods and\nexisting compression standards including VVC intra coding (4:4:4 and 4:2:0) in\nterms of the PSNR and MS-SSIM.",
          "link": "http://arxiv.org/abs/2107.06463",
          "publishedOn": "2021-07-15T01:59:03.048Z",
          "wordCount": 683,
          "title": "Learned Image Compression with Discretized Gaussian-Laplacian-Logistic Mixture Model and Concatenated Residual Modules. (arXiv:2107.06463v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Windsor_R/0/1/0/all/0/1\">Rhydian Windsor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamaludin_A/0/1/0/all/0/1\">Amir Jamaludin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadir_T/0/1/0/all/0/1\">Timor Kadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>",
          "description": "This paper explores the use of self-supervised deep learning in medical\nimaging in cases where two scan modalities are available for the same subject.\nSpecifically, we use a large publicly-available dataset of over 20,000 subjects\nfrom the UK Biobank with both whole body Dixon technique magnetic resonance\n(MR) scans and also dual-energy x-ray absorptiometry (DXA) scans. We make three\ncontributions: (i) We introduce a multi-modal image-matching contrastive\nframework, that is able to learn to match different-modality scans of the same\nsubject with high accuracy. (ii) Without any adaption, we show that the\ncorrespondences learnt during this contrastive training step can be used to\nperform automatic cross-modal scan registration in a completely unsupervised\nmanner. (iii) Finally, we use these registrations to transfer segmentation maps\nfrom the DXA scans to the MR scans where they are used to train a network to\nsegment anatomical regions without requiring ground-truth MR examples. To aid\nfurther research, our code will be made publicly available.",
          "link": "http://arxiv.org/abs/2107.06652",
          "publishedOn": "2021-07-15T01:59:03.042Z",
          "wordCount": 613,
          "title": "Self-Supervised Multi-Modal Alignment for Whole Body Medical Imaging. (arXiv:2107.06652v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1\">Xuguang Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>",
          "description": "Human motion prediction is an important and challenging topic that has\npromising prospects in efficient and safe human-robot-interaction systems.\nCurrently, the majority of the human motion prediction algorithms are based on\ndeterministic models, which may lead to risky decisions for robots. To solve\nthis problem, we propose a probabilistic model for human motion prediction in\nthis paper. The key idea of our approach is to extend the conventional\ndeterministic motion prediction neural network to a Bayesian one. On one hand,\nour model could generate several future motions when given an observed motion\nsequence. On the other hand, by calculating the Epistemic Uncertainty and the\nHeteroscedastic Aleatoric Uncertainty, our model could tell the robot if the\nobservation has been seen before and also give the optimal result among all\npossible predictions. We extensively validate our approach on a large scale\nbenchmark dataset Human3.6m. The experiments show that our approach performs\nbetter than deterministic methods. We further evaluate our approach in a\nHuman-Robot-Interaction (HRI) scenario. The experimental results show that our\napproach makes the interaction more efficient and safer.",
          "link": "http://arxiv.org/abs/2107.06564",
          "publishedOn": "2021-07-15T01:59:03.027Z",
          "wordCount": 619,
          "title": "Probabilistic Human Motion Prediction via A Bayesian Neural Network. (arXiv:2107.06564v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sitek_A/0/1/0/all/0/1\">Arkadiusz Sitek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sangtae Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asma_E/0/1/0/all/0/1\">Evren Asma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandler_A/0/1/0/all/0/1\">Adam Chandler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihsani_A/0/1/0/all/0/1\">Alvin Ihsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prevrhal_S/0/1/0/all/0/1\">Sven Prevrhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmim_A/0/1/0/all/0/1\">Arman Rahmim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saboury_B/0/1/0/all/0/1\">Babak Saboury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thielemans_K/0/1/0/all/0/1\">Kris Thielemans</a>",
          "description": "Artificial intelligence (AI) has significant potential to positively impact\nand advance medical imaging, including positron emission tomography (PET)\nimaging applications. AI has the ability to enhance and optimize all aspects of\nthe PET imaging chain from patient scheduling, patient setup, protocoling, data\nacquisition, detector signal processing, reconstruction, image processing and\ninterpretation. AI poses industry-specific challenges which will need to be\naddressed and overcome to maximize the future potentials of AI in PET. This\npaper provides an overview of these industry-specific challenges for the\ndevelopment, standardization, commercialization, and clinical adoption of AI,\nand explores the potential enhancements to PET imaging brought on by AI in the\nnear future. In particular, the combination of on-demand image reconstruction,\nAI, and custom designed data processing workflows may open new possibilities\nfor innovation which would positively impact the industry and ultimately\npatients.",
          "link": "http://arxiv.org/abs/2107.06747",
          "publishedOn": "2021-07-15T01:59:03.021Z",
          "wordCount": 584,
          "title": "Artificial Intelligence in PET: an Industry Perspective. (arXiv:2107.06747v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Orzechowski_P/0/1/0/all/0/1\">Patryk Orzechowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1\">Jason H. Moore</a>",
          "description": "Understanding the strengths and weaknesses of machine learning (ML)\nalgorithms is crucial for determine their scope of application. Here, we\nintroduce the DIverse and GENerative ML Benchmark (DIGEN) - a collection of\nsynthetic datasets for comprehensive, reproducible, and interpretable\nbenchmarking of machine learning algorithms for classification of binary\noutcomes. The DIGEN resource consists of 40 mathematical functions which map\ncontinuous features to discrete endpoints for creating synthetic datasets.\nThese 40 functions were discovered using a heuristic algorithm designed to\nmaximize the diversity of performance among multiple popular machine learning\nalgorithms thus providing a useful test suite for evaluating and comparing new\nmethods. Access to the generative functions facilitates understanding of why a\nmethod performs poorly compared to other algorithms thus providing ideas for\nimprovement. The resource with extensive documentation and analyses is\nopen-source and available on GitHub.",
          "link": "http://arxiv.org/abs/2107.06475",
          "publishedOn": "2021-07-15T01:59:03.015Z",
          "wordCount": 615,
          "title": "Generative and reproducible benchmarks for comprehensive evaluation of machine learning classifiers. (arXiv:2107.06475v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1\">Bharadwaj Manda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhaskare_P/0/1/0/all/0/1\">Pranjal Bhaskare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1\">Ramanathan Muthuganapathy</a>",
          "description": "This paper presents a deep learning approach for the classification of\nEngineering (CAD) models using Convolutional Neural Networks (CNNs). Owing to\nthe availability of large annotated datasets and also enough computational\npower in the form of GPUs, many deep learning-based solutions for object\nclassification have been proposed of late, especially in the domain of images\nand graphical models. Nevertheless, very few solutions have been proposed for\nthe task of functional classification of CAD models. Hence, for this research,\nCAD models have been collected from Engineering Shape Benchmark (ESB), National\nDesign Repository (NDR) and augmented with newer models created using a\nmodelling software to form a dataset - 'CADNET'. It is proposed to use a\nresidual network architecture for CADNET, inspired by the popular ResNet. A\nweighted Light Field Descriptor (LFD) scheme is chosen as the method of feature\nextraction, and the generated images are fed as inputs to the CNN. The problem\nof class imbalance in the dataset is addressed using a class weights approach.\nExperiments have been conducted with other signatures such as geodesic distance\netc. using deep networks as well as other network architectures on the CADNET.\nThe LFD-based CNN approach using the proposed network architecture, along with\ngradient boosting yielded the best classification accuracy on CADNET.",
          "link": "http://arxiv.org/abs/2107.06481",
          "publishedOn": "2021-07-15T01:59:03.008Z",
          "wordCount": 671,
          "title": "A Convolutional Neural Network Approach to the Classification of Engineering Models. (arXiv:2107.06481v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaolong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qimeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>",
          "description": "Temporal action detection (TAD) aims to determine the semantic label and the\nboundaries of every action instance in an untrimmed video. It is a fundamental\nand challenging task in video understanding and significant progress has been\nmade. Previous methods involve multiple stages or networks and hand-designed\nrules or operations, which fall short in efficiency and flexibility. In this\npaper, we propose an end-to-end framework for TAD upon Transformer, termed\n\\textit{TadTR}, which maps a set of learnable embeddings to action instances in\nparallel. TadTR is able to adaptively extract temporal context information\nrequired for making action predictions, by selectively attending to a sparse\nset of snippets in a video. As a result, it simplifies the pipeline of TAD and\nrequires lower computation cost than previous detectors, while preserving\nremarkable detection performance. TadTR achieves state-of-the-art performance\non HACS Segments (+3.35% average mAP). As a single-network detector, TadTR runs\n10$\\times$ faster than its comparable competitor. It outperforms existing\nsingle-network detectors by a large margin on THUMOS14 (+5.0% average mAP) and\nActivityNet (+7.53% average mAP). When combined with other detectors, it\nreports 54.1% mAP at IoU=0.5 on THUMOS14, and 34.55% average mAP on\nActivityNet-1.3. Our code will be released at\n\\url{https://github.com/xlliu7/TadTR}.",
          "link": "http://arxiv.org/abs/2106.10271",
          "publishedOn": "2021-07-15T01:59:03.002Z",
          "wordCount": 667,
          "title": "End-to-end Temporal Action Detection with Transformer. (arXiv:2106.10271v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yijie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Peng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xi Peng</a>",
          "description": "Image hazing aims to render a hazy image from a given clean one, which could\nbe applied to a variety of practical applications such as gaming, filming,\nphotographic filtering, and image dehazing. To generate plausible haze, we\nstudy two less-touched but challenging problems in hazy image rendering,\nnamely, i) how to estimate the transmission map from a single image without\nauxiliary information, and ii) how to adaptively learn the airlight from\nexemplars, i.e., unpaired real hazy images. To this end, we propose a neural\nrendering method for image hazing, dubbed as HazeGEN. To be specific, HazeGEN\nis a knowledge-driven neural network which estimates the transmission map by\nleveraging a new prior, i.e., there exists the structure similarity (e.g.,\ncontour and luminance) between the transmission map and the input clean image.\nTo adaptively learn the airlight, we build a neural module based on another new\nprior, i.e., the rendered hazy image and the exemplar are similar in the\nairlight distribution. To the best of our knowledge, this could be the first\nattempt to deeply rendering hazy images in an unsupervised fashion. Comparing\nwith existing haze generation methods, HazeGEN renders the hazy images in an\nunsupervised, learnable, and controllable manner, thus avoiding the\nlabor-intensive efforts in paired data collection and the domain-shift issue in\nhaze generation. Extensive experiments show the promising performance of our\nmethod comparing with some baselines in both qualitative and quantitative\ncomparisons. The code will be released on GitHub after acceptance.",
          "link": "http://arxiv.org/abs/2107.06681",
          "publishedOn": "2021-07-15T01:59:02.996Z",
          "wordCount": 678,
          "title": "Unsupervised Neural Rendering for Image Hazing. (arXiv:2107.06681v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Watson_J/0/1/0/all/0/1\">Jamie Watson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1\">Oisin Mac Aodha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1\">Victor Prisacariu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brostow_G/0/1/0/all/0/1\">Gabriel Brostow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firman_M/0/1/0/all/0/1\">Michael Firman</a>",
          "description": "Self-supervised monocular depth estimation networks are trained to predict\nscene depth using nearby frames as a supervision signal during training.\nHowever, for many applications, sequence information in the form of video\nframes is also available at test time. The vast majority of monocular networks\ndo not make use of this extra signal, thus ignoring valuable information that\ncould be used to improve the predicted depth. Those that do, either use\ncomputationally expensive test-time refinement techniques or off-the-shelf\nrecurrent networks, which only indirectly make use of the geometric information\nthat is inherently available.\n\nWe propose ManyDepth, an adaptive approach to dense depth estimation that can\nmake use of sequence information at test time, when it is available. Taking\ninspiration from multi-view stereo, we propose a deep end-to-end cost volume\nbased approach that is trained using self-supervision only. We present a novel\nconsistency loss that encourages the network to ignore the cost volume when it\nis deemed unreliable, e.g. in the case of moving objects, and an augmentation\nscheme to cope with static cameras. Our detailed experiments on both KITTI and\nCityscapes show that we outperform all published self-supervised baselines,\nincluding those that use single or multiple frames at test time.",
          "link": "http://arxiv.org/abs/2104.14540",
          "publishedOn": "2021-07-15T01:59:02.978Z",
          "wordCount": 670,
          "title": "The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth. (arXiv:2104.14540v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_B/0/1/0/all/0/1\">Baolian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Gangming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chengwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huiguang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Licheng Jiao</a>",
          "description": "Locating diseases in chest X-ray images with few careful annotations saves\nlarge human effort. Recent works approached this task with innovative\nweakly-supervised algorithms such as multi-instance learning (MIL) and class\nactivation maps (CAM), however, these methods often yield inaccurate or\nincomplete regions. One of the reasons is the neglection of the pathological\nimplications hidden in the relationship across anatomical regions within each\nimage and the relationship across images. In this paper, we argue that the\ncross-region and cross-image relationship, as contextual and compensating\ninformation, is vital to obtain more consistent and integral regions. To model\nthe relationship, we propose the Graph Regularized Embedding Network (GREN),\nwhich leverages the intra-image and inter-image information to locate diseases\non chest X-ray images. GREN uses a pre-trained U-Net to segment the lung lobes,\nand then models the intra-image relationship between the lung lobes using an\nintra-image graph to compare different regions. Meanwhile, the relationship\nbetween in-batch images is modeled by an inter-image graph to compare multiple\nimages. This process mimics the training and decision-making process of a\nradiologist: comparing multiple regions and images for diagnosis. In order for\nthe deep embedding layers of the neural network to retain structural\ninformation (important in the localization task), we use the Hash coding and\nHamming distance to compute the graphs, which are used as regularizers to\nfacilitate training. By means of this, our approach achieves the\nstate-of-the-art result on NIH chest X-ray dataset for weakly-supervised\ndisease localization. Our codes are accessible online.",
          "link": "http://arxiv.org/abs/2107.06442",
          "publishedOn": "2021-07-15T01:59:02.971Z",
          "wordCount": 703,
          "title": "GREN: Graph-Regularized Embedding Network for Weakly-Supervised Disease Localization in X-ray images. (arXiv:2107.06442v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06618",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bannur_S/0/1/0/all/0/1\">Shruthi Bannur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oktay_O/0/1/0/all/0/1\">Ozan Oktay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bernhardt_M/0/1/0/all/0/1\">Melanie Bernhardt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schwaighofer_A/0/1/0/all/0/1\">Anton Schwaighofer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jena_R/0/1/0/all/0/1\">Rajesh Jena</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nushi_B/0/1/0/all/0/1\">Besmira Nushi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wadhwani_S/0/1/0/all/0/1\">Sharan Wadhwani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nori_A/0/1/0/all/0/1\">Aditya Nori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Natarajan_K/0/1/0/all/0/1\">Kal Natarajan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ashraf_S/0/1/0/all/0/1\">Shazad Ashraf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alvarez_Valle_J/0/1/0/all/0/1\">Javier Alvarez-Valle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Castro_D/0/1/0/all/0/1\">Daniel C. Castro</a>",
          "description": "Chest radiography has been a recommended procedure for patient triaging and\nresource management in intensive care units (ICUs) throughout the COVID-19\npandemic. The machine learning efforts to augment this workflow have been long\nchallenged due to deficiencies in reporting, model evaluation, and failure mode\nanalysis. To address some of those shortcomings, we model radiological features\nwith a human-interpretable class hierarchy that aligns with the radiological\ndecision process. Also, we propose the use of a data-driven error analysis\nmethodology to uncover the blind spots of our model, providing further\ntransparency on its clinical utility. For example, our experiments show that\nmodel failures highly correlate with ICU imaging conditions and with the\ninherent difficulty in distinguishing certain types of radiological features.\nAlso, our hierarchical interpretation and analysis facilitates the comparison\nwith respect to radiologists' findings and inter-variability, which in return\nhelps us to better assess the clinical applicability of models.",
          "link": "http://arxiv.org/abs/2107.06618",
          "publishedOn": "2021-07-15T01:59:02.965Z",
          "wordCount": 673,
          "title": "Hierarchical Analysis of Visual COVID-19 Features from Chest Radiographs. (arXiv:2107.06618v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suneung-Kim/0/1/0/all/0/1\">Suneung-Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Due to the recent outbreak of COVID-19, many classes, exams, and meetings\nhave been conducted non-face-to-face. However, the foundation for video\nconferencing solutions is still insufficient. So this technology has become an\nimportant issue. In particular, these technologies are essential for\nnon-face-to-face testing, and technology dissemination is urgent. In this\npaper, we present a single video conferencing solution using gaze estimation in\npreparation for these problems. Gaze is an important cue for the tasks such as\nanalysis of human behavior. Hence, numerous studies have been proposed to solve\ngaze estimation using deep learning, which is one of the most prominent methods\nup to date. We use these gaze estimation methods to detect abnormal behavior of\nvideo conferencing participants. Our contribution is as follows. i) We find and\napply the optimal network for the gaze estimation method and apply a\nself-supervised method to improve accuracy. ii) For anomaly detection, we\npresent a new dataset that aggregates the values of a new gaze, head pose, etc.\niii) We train newly created data on Multi Layer Perceptron (MLP) models to\ndetect anomaly behavior based on deep learning. We demonstrate the robustness\nof our method through experiments.",
          "link": "http://arxiv.org/abs/2107.06530",
          "publishedOn": "2021-07-15T01:59:02.959Z",
          "wordCount": 685,
          "title": "Detection of Abnormal Behavior with Self-Supervised Gaze Estimation. (arXiv:2107.06530v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhiying Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>",
          "description": "As a key component of talking face generation, lip movements generation\ndetermines the naturalness and coherence of the generated talking face video.\nPrior literature mainly focuses on speech-to-lip generation while there is a\npaucity in text-to-lip (T2L) generation. T2L is a challenging task and existing\nend-to-end works depend on the attention mechanism and autoregressive (AR)\ndecoding manner. However, the AR decoding manner generates current lip frame\nconditioned on frames generated previously, which inherently hinders the\ninference speed, and also has a detrimental effect on the quality of generated\nlip frames due to error propagation. This encourages the research of parallel\nT2L generation. In this work, we propose a novel parallel decoding model for\nhigh-speed and high-quality text-to-lip generation (HH-T2L). Specifically, we\npredict the duration of the encoded linguistic features and model the target\nlip frames conditioned on the encoded linguistic features with their duration\nin a non-autoregressive manner. Furthermore, we incorporate the structural\nsimilarity index loss and adversarial learning to improve perceptual quality of\ngenerated lip frames and alleviate the blurry prediction problem. Extensive\nexperiments conducted on GRID and TCD-TIMIT datasets show that 1) HH-T2L\ngenerates lip movements with competitive quality compared with the\nstate-of-the-art AR T2L model DualLip and exceeds the baseline AR model\nTransformerT2L by a notable margin benefiting from the mitigation of the error\npropagation problem; and 2) exhibits distinct superiority in inference speed\n(an average speedup of 19$\\times$ than DualLip on TCD-TIMIT).",
          "link": "http://arxiv.org/abs/2107.06831",
          "publishedOn": "2021-07-15T01:59:02.942Z",
          "wordCount": 668,
          "title": "High-Speed and High-Quality Text-to-Lip Generation. (arXiv:2107.06831v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">YiMin Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianbing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yingjie Xi</a>",
          "description": "Detection faults in seismic data is a crucial step for seismic structural\ninterpretation, reservoir characterization and well placement. Some recent\nworks regard it as an image segmentation task. The task of image segmentation\nrequires huge labels, especially 3D seismic data, which has a complex structure\nand lots of noise. Therefore, its annotation requires expert experience and a\nhuge workload. In this study, we present lambda-BCE and lambda-smooth L1loss to\neffectively train 3D-CNN by some slices from 3D seismic data, so that the model\ncan learn the segmentation of 3D seismic data from a few 2D slices. In order to\nfully extract information from limited data and suppress seismic noise, we\npropose an attention module that can be used for active supervision training\nand embedded in the network. The attention heatmap label is generated by the\noriginal label, and letting it supervise the attention module using the\nlambda-smooth L1loss. The experiment demonstrates the effectiveness of our loss\nfunction, the method can extract 3D seismic features from a few 2D slice\nlabels. And it also shows the advanced performance of the attention module,\nwhich can significantly suppress the noise in the seismic data while increasing\nthe model's sensitivity to the foreground. Finally, on the public test set, we\nonly use the 2D slice labels training that accounts for 3.3% of the 3D volume\nlabel, and achieve similar performance to the 3D volume label training.",
          "link": "http://arxiv.org/abs/2105.03857",
          "publishedOn": "2021-07-15T01:59:02.936Z",
          "wordCount": 754,
          "title": "Attention-Based 3D Seismic Fault Segmentation Training by a Few 2D Slice Labels. (arXiv:2105.03857v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_A/0/1/0/all/0/1\">Akshay L Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Sai Vikas Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1\">Chaitanya Devaguptapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>",
          "description": "Active Learning (AL) techniques aim to minimize the training data required to\ntrain a model for a given task. Pool-based AL techniques start with a small\ninitial labeled pool and then iteratively pick batches of the most informative\nsamples for labeling. Generally, the initial pool is sampled randomly and\nlabeled to seed the AL iterations. While recent studies have focused on\nevaluating the robustness of various query functions in AL, little to no\nattention has been given to the design of the initial labeled pool for deep\nactive learning. Given the recent successes of learning representations in\nself-supervised/unsupervised ways, we study if an intelligently sampled initial\nlabeled pool can improve deep AL performance. We investigate the effect of\nintelligently sampled initial labeled pools, including the use of\nself-supervised and unsupervised strategies, on deep AL methods. The setup,\nhypotheses, methodology, and implementation details were evaluated by peer\nreview before experiments were conducted. Experimental results could not\nconclusively prove that intelligently sampled initial pools are better for AL\nthan random initial pools in the long run, although a Variational\nAutoencoder-based initial pool sampling strategy showed interesting trends that\nmerit deeper investigation.",
          "link": "http://arxiv.org/abs/2011.14696",
          "publishedOn": "2021-07-15T01:59:02.929Z",
          "wordCount": 683,
          "title": "On Initial Pools for Deep Active Learning. (arXiv:2011.14696v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Sandip Roy</a>",
          "description": "Terse representation of high-dimensional weather scene data is explored, in\nsupport of strategic air traffic flow management objectives. Specifically, we\nconsider whether aviation-relevant weather scenes are compressible, in the\nsense that each scene admits a possibly-different sparse representation in a\nbasis of interest. Here, compression of weather scenes extracted from METAR\ndata (including temperature, flight categories, and visibility profiles for the\ncontiguous United States) is examined, for the graph-spectral basis. The scenes\nare found to be compressible, with 75-95% of the scene content captured using\n0.5-4% of the basis vectors. Further, the dominant basis vectors for each scene\nare seen to identify time-varying spatial characteristics of the weather, and\nreconstruction from the compressed representation is demonstrated. Finally,\npotential uses of the compressive representations in strategic TFM design are\nbriefly scoped.",
          "link": "http://arxiv.org/abs/2107.06394",
          "publishedOn": "2021-07-15T01:59:02.923Z",
          "wordCount": 572,
          "title": "Compressive Representations of Weather Scenes for Strategic Air Traffic Flow Management. (arXiv:2107.06394v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koner_R/0/1/0/all/0/1\">Rajat Koner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hildebrandt_M/0/1/0/all/0/1\">Marcel Hildebrandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Deepan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "Visual Question Answering (VQA) is concerned with answering free-form\nquestions about an image. Since it requires a deep semantic and linguistic\nunderstanding of the question and the ability to associate it with various\nobjects that are present in the image, it is an ambitious task and requires\nmulti-modal reasoning from both computer vision and natural language\nprocessing. We propose Graphhopper, a novel method that approaches the task by\nintegrating knowledge graph reasoning, computer vision, and natural language\nprocessing techniques. Concretely, our method is based on performing\ncontext-driven, sequential reasoning based on the scene entities and their\nsemantic and spatial relationships. As a first step, we derive a scene graph\nthat describes the objects in the image, as well as their attributes and their\nmutual relationships. Subsequently, a reinforcement learning agent is trained\nto autonomously navigate in a multi-hop manner over the extracted scene graph\nto generate reasoning paths, which are the basis for deriving answers. We\nconduct an experimental study on the challenging dataset GQA, based on both\nmanually curated and automatically generated scene graphs. Our results show\nthat we keep up with a human performance on manually curated scene graphs.\nMoreover, we find that Graphhopper outperforms another state-of-the-art scene\ngraph reasoning model on both manually curated and automatically generated\nscene graphs by a significant margin.",
          "link": "http://arxiv.org/abs/2107.06325",
          "publishedOn": "2021-07-15T01:59:02.917Z",
          "wordCount": 665,
          "title": "Graphhopper: Multi-Hop Scene Graph Reasoning for Visual Question Answering. (arXiv:2107.06325v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.05732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deguerre_B/0/1/0/all/0/1\">Benjamin Deguerre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatelain_C/0/1/0/all/0/1\">Clement Chatelain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasso_G/0/1/0/all/0/1\">Gilles Gasso</a>",
          "description": "Object detection in images has reached unprecedented performances. The\nstate-of-the-art methods rely on deep architectures that extract salient\nfeatures and predict bounding boxes enclosing the objects of interest. These\nmethods essentially run on RGB images. However, the RGB images are often\ncompressed by the acquisition devices for storage purpose and transfer\nefficiency. Hence, their decompression is required for object detectors. To\ngain in efficiency, this paper proposes to take advantage of the compressed\nrepresentation of images to carry out object detection usable in constrained\nresources conditions.\n\nSpecifically, we focus on JPEG images and propose a thorough analysis of\ndetection architectures newly designed in regard of the peculiarities of the\nJPEG norm. This leads to a $\\times 1.7$ speed up in comparison with a standard\nRGB-based architecture, while only reducing the detection performance by 5.5%.\nAdditionally, our empirical findings demonstrate that only part of the\ncompressed JPEG information, namely the luminance component, may be required to\nmatch detection accuracy of the full input methods.",
          "link": "http://arxiv.org/abs/2006.05732",
          "publishedOn": "2021-07-15T01:59:02.911Z",
          "wordCount": 645,
          "title": "Object Detection in the DCT Domain: is Luminance the Solution?. (arXiv:2006.05732v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1\">Anqi Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haimin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minye Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>",
          "description": "Recent neural rendering approaches for human activities achieve remarkable\nview synthesis results, but still rely on dense input views or dense training\nwith all the capture frames, leading to deployment difficulty and inefficient\ntraining overload. However, existing advances will be ill-posed if the input is\nboth spatially and temporally sparse. To fill this gap, in this paper we\npropose a few-shot neural human rendering approach (FNHR) from only sparse RGBD\ninputs, which exploits the temporal and spatial redundancy to generate\nphoto-realistic free-view output of human activities. Our FNHR is trained only\non the key-frames which expand the motion manifold in the input sequences. We\nintroduce a two-branch neural blending to combine the neural point render and\nclassical graphics texturing pipeline, which integrates reliable observations\nover sparse key-frames. Furthermore, we adopt a patch-based adversarial\ntraining process to make use of the local redundancy and avoids over-fitting to\nthe key-frames, which generates fine-detailed rendering results. Extensive\nexperiments demonstrate the effectiveness of our approach to generate\nhigh-quality free view-point results for challenging human performances under\nthe sparse setting.",
          "link": "http://arxiv.org/abs/2107.06505",
          "publishedOn": "2021-07-15T01:59:02.895Z",
          "wordCount": 625,
          "title": "Few-shot Neural Human Performance Rendering from Sparse RGBD Videos. (arXiv:2107.06505v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Duhun Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">Eunjung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1\">Wonjong Rhee</a>",
          "description": "We propose an AID-purifier that can boost the robustness of\nadversarially-trained networks by purifying their inputs. AID-purifier is an\nauxiliary network that works as an add-on to an already trained main\nclassifier. To keep it computationally light, it is trained as a discriminator\nwith a binary cross-entropy loss. To obtain additionally useful information\nfrom the adversarial examples, the architecture design is closely related to\ninformation maximization principles where two layers of the main classification\nnetwork are piped to the auxiliary network. To assist the iterative\noptimization procedure of purification, the auxiliary network is trained with\nAVmixup. AID-purifier can be used together with other purifiers such as\nPixelDefend for an extra enhancement. The overall results indicate that the\nbest performing adversarially-trained networks can be enhanced by the best\nperforming purification networks, where AID-purifier is a competitive candidate\nthat is light and robust.",
          "link": "http://arxiv.org/abs/2107.06456",
          "publishedOn": "2021-07-15T01:59:02.889Z",
          "wordCount": 590,
          "title": "AID-Purifier: A Light Auxiliary Network for Boosting Adversarial Defense. (arXiv:2107.06456v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06449",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Guo_H/0/1/0/all/0/1\">Hengtao Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xuanang Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1\">Sheng Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wood_B/0/1/0/all/0/1\">Bradford J. Wood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_P/0/1/0/all/0/1\">Pingkun Yan</a>",
          "description": "Fusing intra-operative 2D transrectal ultrasound (TRUS) image with\npre-operative 3D magnetic resonance (MR) volume to guide prostate biopsy can\nsignificantly increase the yield. However, such a multimodal 2D/3D registration\nproblem is a very challenging task. In this paper, we propose an end-to-end\nframe-to-volume registration network (FVR-Net), which can efficiently bridge\nthe previous research gaps by aligning a 2D TRUS frame with a 3D TRUS volume\nwithout requiring hardware tracking. The proposed FVR-Net utilizes a\ndual-branch feature extraction module to extract the information from TRUS\nframe and volume to estimate transformation parameters. We also introduce a\ndifferentiable 2D slice sampling module which allows gradients backpropagating\nfrom an unsupervised image similarity loss for content correspondence learning.\nOur model shows superior efficiency for real-time interventional guidance with\nhighly competitive registration accuracy.",
          "link": "http://arxiv.org/abs/2107.06449",
          "publishedOn": "2021-07-15T01:59:02.883Z",
          "wordCount": 576,
          "title": "End-to-end Ultrasound Frame to Volume Registration. (arXiv:2107.06449v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yihao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_W/0/1/0/all/0/1\">Weikai Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1\">Geguang Pu</a>",
          "description": "High-level representation-guided pixel denoising and adversarial training are\nindependent solutions to enhance the robustness of CNNs against adversarial\nattacks by pre-processing input data and re-training models, respectively. Most\nrecently, adversarial training techniques have been widely studied and improved\nwhile the pixel denoising-based method is getting less attractive. However, it\nis still questionable whether there exists a more advanced pixel\ndenoising-based method and whether the combination of the two solutions\nbenefits each other. To this end, we first comprehensively investigate two\nkinds of pixel denoising methods for adversarial robustness enhancement (i.e.,\nexisting additive-based and unexplored filtering-based methods) under the loss\nfunctions of image-level and semantic-level restorations, respectively, showing\nthat pixel-wise filtering can obtain much higher image quality (e.g., higher\nPSNR) as well as higher robustness (e.g., higher accuracy on adversarial\nexamples) than existing pixel-wise additive-based method. However, we also\nobserve that the robustness results of the filtering-based method rely on the\nperturbation amplitude of adversarial examples used for training. To address\nthis problem, we propose predictive perturbation-aware pixel-wise filtering,\nwhere dual-perturbation filtering and an uncertainty-aware fusion module are\ndesigned and employed to automatically perceive the perturbation amplitude\nduring the training and testing process. The proposed method is termed as\nAdvFilter. Moreover, we combine adversarial pixel denoising methods with three\nadversarial training-based methods, hinting that considering data and models\njointly is able to achieve more robust CNNs. The experiments conduct on\nNeurIPS-2017DEV, SVHN, and CIFAR10 datasets and show the advantages over\nenhancing CNNs' robustness, high generalization to different models, and noise\nlevels.",
          "link": "http://arxiv.org/abs/2107.06501",
          "publishedOn": "2021-07-15T01:59:02.877Z",
          "wordCount": 716,
          "title": "AdvFilter: Predictive Perturbation-aware Filtering against Adversarial Attack via Multi-domain Learning. (arXiv:2107.06501v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Teutscher_D/0/1/0/all/0/1\">Dennis Teutscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangat_P/0/1/0/all/0/1\">Patrick Mangat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasenmuller_O/0/1/0/all/0/1\">Oliver Wasenm&#xfc;ller</a>",
          "description": "Depth completion from sparse LiDAR and high-resolution RGB data is one of the\nfoundations for autonomous driving techniques. Current approaches often rely on\nCNN-based methods with several known drawbacks: flying pixel at depth\ndiscontinuities, overfitting to both a given data set as well as error metric,\nand many more. Thus, we propose our novel Piecewise Depth Completion (PDC),\nwhich works completely without deep learning. PDC segments the RGB image into\nsuperpixels corresponding the regions with similar depth value. Superpixels\ncorresponding to same objects are gathered using a cost map. At the end, we\nreceive detailed depth images with state of the art accuracy. In our\nevaluation, we can show both the influence of the individual proposed\nprocessing steps and the overall performance of our method on the challenging\nKITTI dataset.",
          "link": "http://arxiv.org/abs/2107.06711",
          "publishedOn": "2021-07-15T01:59:02.861Z",
          "wordCount": 574,
          "title": "PDC: Piecewise Depth Completion utilizing Superpixels. (arXiv:2107.06711v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1\">Ibrahim Alabdulmohsin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1\">Larisa Markeeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1\">Daniel Keysers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolstikhin_I/0/1/0/all/0/1\">Ilya Tolstikhin</a>",
          "description": "We introduce a generalization to the lottery ticket hypothesis in which the\nnotion of \"sparsity\" is relaxed by choosing an arbitrary basis in the space of\nparameters. We present evidence that the original results reported for the\ncanonical basis continue to hold in this broader setting. We describe how\nstructured pruning methods, including pruning units or factorizing\nfully-connected layers into products of low-rank matrices, can be cast as\nparticular instances of this \"generalized\" lottery ticket hypothesis. The\ninvestigations reported here are preliminary and are provided to encourage\nfurther research along this direction.",
          "link": "http://arxiv.org/abs/2107.06825",
          "publishedOn": "2021-07-15T01:59:02.855Z",
          "wordCount": 542,
          "title": "A Generalized Lottery Ticket Hypothesis. (arXiv:2107.06825v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+More_A/0/1/0/all/0/1\">Amit More</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Subhasis Chaudhuri</a>",
          "description": "Predicting novel views of a scene from real-world images has always been a\nchallenging task. In this work, we propose a deep convolutional neural network\n(CNN) which learns to predict novel views of a scene from given collection of\nimages. In comparison to prior deep learning based approaches, which can handle\nonly a fixed number of input images to predict novel view, proposed approach\nworks with different numbers of input images. The proposed model explicitly\nperforms feature extraction and matching from a given pair of input images and\nestimates, at each pixel, the probability distribution (pdf) over possible\ndepth levels in the scene. This pdf is then used for estimating the novel view.\nThe model estimates multiple predictions of novel view, one estimate per input\nimage pair, from given image collection. The model also estimates an occlusion\nmask and combines multiple novel view estimates in to a single optimal\nprediction. The finite number of depth levels used in the analysis may cause\noccasional blurriness in the estimated view. We mitigate this issue with simple\nmulti-resolution analysis which improves the quality of the estimates. We\nsubstantiate the performance on different datasets and show competitive\nperformance.",
          "link": "http://arxiv.org/abs/2107.06812",
          "publishedOn": "2021-07-15T01:59:02.849Z",
          "wordCount": 621,
          "title": "Deep Learning based Novel View Synthesis. (arXiv:2107.06812v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1\">Ning Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1\">Jiajun Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Lixian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jun Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>",
          "description": "Domain Adaptation has been widely used to deal with the distribution shift in\nvision, language, multimedia etc. Most domain adaptation methods learn\ndomain-invariant features with data from both domains available. However, such\na strategy might be infeasible in practice when source data are unavailable due\nto data-privacy concerns. To address this issue, we propose a novel adaptation\nmethod via hypothesis transfer without accessing source data at adaptation\nstage. In order to fully use the limited target data, a semi-supervised mutual\nenhancement method is proposed, in which entropy minimization and augmented\nlabel propagation are used iteratively to perform inter-domain and intra-domain\nalignments. Compared with state-of-the-art methods, the experimental results on\nthree public datasets demonstrate that our method gets up to 19.9% improvements\non semi-supervised adaptation tasks.",
          "link": "http://arxiv.org/abs/2107.06735",
          "publishedOn": "2021-07-15T01:59:02.843Z",
          "wordCount": 564,
          "title": "Semi-Supervised Hypothesis Transfer for Source-Free Domain Adaptation. (arXiv:2107.06735v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DAmario_V/0/1/0/all/0/1\">Vanessa D&#x27;Amario</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>",
          "description": "Datasets often contain input dimensions that are unnecessary to predict the\noutput label, e.g. background in object recognition, which lead to more\ntrainable parameters. Deep Neural Networks (DNNs) are robust to increasing the\nnumber of parameters in the hidden layers, but it is unclear whether this holds\ntrue for the input layer. In this letter, we investigate the impact of\nunnecessary input dimensions on a central issue of DNNs: their data efficiency,\nie. the amount of examples needed to achieve certain generalization\nperformance. Our results show that unnecessary input dimensions that are\ntask-unrelated substantially degrade data efficiency. This highlights the need\nfor mechanisms that remove {task-unrelated} dimensions to enable data\nefficiency gains.",
          "link": "http://arxiv.org/abs/2107.06409",
          "publishedOn": "2021-07-15T01:59:02.836Z",
          "wordCount": 554,
          "title": "The Foes of Neural Network's Data Efficiency Among Unnecessary Input Dimensions. (arXiv:2107.06409v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06281",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Mhiri_I/0/1/0/all/0/1\">Islem Mhiri</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nebli_A/0/1/0/all/0/1\">Ahmed Nebli</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mahjoub_M/0/1/0/all/0/1\">Mohamed Ali Mahjoub</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rekik_I/0/1/0/all/0/1\">Islem Rekik</a>",
          "description": "Brain graph synthesis marked a new era for predicting a target brain graph\nfrom a source one without incurring the high acquisition cost and processing\ntime of neuroimaging data. However, existing multi-modal graph synthesis\nframeworks have several limitations. First, they mainly focus on generating\ngraphs from the same domain (intra-modality), overlooking the rich multimodal\nrepresentations of brain connectivity (inter-modality). Second, they can only\nhandle isomorphic graph generation tasks, limiting their generalizability to\nsynthesizing target graphs with a different node size and topological structure\nfrom those of the source one. More importantly, both target and source domains\nmight have different distributions, which causes a domain fracture between them\n(i.e., distribution misalignment). To address such challenges, we propose an\ninter-modality aligner of non-isomorphic graphs (IMANGraphNet) framework to\ninfer a target graph modality based on a given modality. Our three core\ncontributions lie in (i) predicting a target graph (e.g., functional) from a\nsource graph (e.g., morphological) based on a novel graph generative\nadversarial network (gGAN); (ii) using non-isomorphic graphs for both source\nand target domains with a different number of nodes, edges and structure; and\n(iii) enforcing the predicted target distribution to match that of the ground\ntruth graphs using a graph autoencoder to relax the designed loss oprimization.\nTo handle the unstable behavior of gGAN, we design a new Ground\nTruth-Preserving (GT-P) loss function to guide the generator in learning the\ntopological structure of ground truth brain graphs. Our comprehensive\nexperiments on predicting functional from morphological graphs demonstrate the\noutperformance of IMANGraphNet in comparison with its variants. This can be\nfurther leveraged for integrative and holistic brain mapping in health and\ndisease.",
          "link": "http://arxiv.org/abs/2107.06281",
          "publishedOn": "2021-07-15T01:59:02.828Z",
          "wordCount": 719,
          "title": "Non-isomorphic Inter-modality Graph Alignment and Synthesis for Holistic Brain Mapping. (arXiv:2107.06281v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2011.04408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hanjiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baoquan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Zhijian Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>",
          "description": "Different environments pose a great challenge on the outdoor robust visual\nperception for long-term autonomous driving and the generalization of\nlearning-based algorithms on different environmental effects is still an open\nproblem. Although monocular depth prediction has been well studied recently,\nthere is few work focusing on the robust learning-based depth prediction across\ndifferent environments, e.g., changing illumination and seasons, owing to the\nlack of such a multi-environment real-world dataset and benchmark. To this end,\nthe first cross-season monocular depth prediction dataset and benchmark\nSeasonDepth (available on https://seasondepth.github.io/) is built based on CMU\nVisual Localization dataset. To benchmark the depth estimation performance\nunder different environments, we investigate representative and recent\nstate-of-the-art open-source supervised, self-supervised and domain adaptation\ndepth prediction methods from KITTI benchmark using several newly-formulated\nmetrics. Through extensive experimental evaluation on the proposed dataset, the\ninfluence of multiple environments on performance and robustness is analyzed\nboth qualitatively and quantitatively, showing that the long-term monocular\ndepth prediction is far from solved even with fine-tuning. We further give\npromising avenues that self-supervised training and stereo geometry constraint\nhelp to enhance the robustness to changing environments.",
          "link": "http://arxiv.org/abs/2011.04408",
          "publishedOn": "2021-07-15T01:59:02.821Z",
          "wordCount": 668,
          "title": "SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments. (arXiv:2011.04408v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06536",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1\">Meng Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1\">Jiasong Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_X/0/1/0/all/0/1\">Xiuping Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1\">Sen Jia</a>",
          "description": "Image super-resolution (SR) methods can generate remote sensing images with\nhigh spatial resolution without increasing the cost, thereby providing a\nfeasible way to acquire high-resolution remote sensing images, which are\ndifficult to obtain due to the high cost of acquisition equipment and complex\nweather. Clearly, image super-resolution is a severe ill-posed problem.\nFortunately, with the development of deep learning, the powerful fitting\nability of deep neural networks has solved this problem to some extent. In this\npaper, we propose a network based on the generative adversarial network (GAN)\nto generate high resolution remote sensing images, named the multi-attention\ngenerative adversarial network (MA-GAN). We first designed a GAN-based\nframework for the image SR task. The core to accomplishing the SR task is the\nimage generator with post-upsampling that we designed. The main body of the\ngenerator contains two blocks; one is the pyramidal convolution in the\nresidual-dense block (PCRDB), and the other is the attention-based upsample\n(AUP) block. The attentioned pyramidal convolution (AttPConv) in the PCRDB\nblock is a module that combines multi-scale convolution and channel attention\nto automatically learn and adjust the scaling of the residuals for better\nresults. The AUP block is a module that combines pixel attention (PA) to\nperform arbitrary multiples of upsampling. These two blocks work together to\nhelp generate better quality images. For the loss function, we design a loss\nfunction based on pixel loss and introduce both adversarial loss and feature\nloss to guide the generator learning. We have compared our method with several\nstate-of-the-art methods on a remote sensing scene image dataset, and the\nexperimental results consistently demonstrate the effectiveness of the proposed\nMA-GAN.",
          "link": "http://arxiv.org/abs/2107.06536",
          "publishedOn": "2021-07-15T01:59:02.814Z",
          "wordCount": 720,
          "title": "Multi-Attention Generative Adversarial Network for Remote Sensing Image Super-Resolution. (arXiv:2107.06536v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.05255",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Ce Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Haimiao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shang_K/0/1/0/all/0/1\">Kun Shang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lyu_Y/0/1/0/all/0/1\">Yuanyuan Lyu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_B/0/1/0/all/0/1\">Bin Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>",
          "description": "Computed tomography (CT) reconstruction from X-ray projections acquired\nwithin a limited angle range is challenging, especially when the angle range is\nextremely small. Both analytical and iterative models need more projections for\neffective modeling. Deep learning methods have gained prevalence due to their\nexcellent reconstruction performances, but such success is mainly limited\nwithin the same dataset and does not generalize across datasets with different\ndistributions. Hereby we propose ExtraPolationNetwork for limited-angle CT\nreconstruction via the introduction of a sinogram extrapolation module, which\nis theoretically justified. The module complements extra sinogram information\nand boots model generalizability. Extensive experimental results show that our\nreconstruction model achieves state-of-the-art performance on NIH-AAPM dataset,\nsimilar to existing approaches. More importantly, we show that using such a\nsinogram extrapolation module significantly improves the generalization\ncapability of the model on unseen datasets (e.g., COVID-19 and LIDC datasets)\nwhen compared to existing approaches.",
          "link": "http://arxiv.org/abs/2103.05255",
          "publishedOn": "2021-07-15T01:59:02.789Z",
          "wordCount": 670,
          "title": "Improving Generalizability in Limited-Angle CT Reconstruction with Sinogram Extrapolation. (arXiv:2103.05255v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hayat_N/0/1/0/all/0/1\">Nasir Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lashen_H/0/1/0/all/0/1\">Hazem Lashen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1\">Farah E. Shamout</a>",
          "description": "Despite the success of deep neural networks in chest X-ray (CXR) diagnosis,\nsupervised learning only allows the prediction of disease classes that were\nseen during training. At inference, these networks cannot predict an unseen\ndisease class. Incorporating a new class requires the collection of labeled\ndata, which is not a trivial task, especially for less frequently-occurring\ndiseases. As a result, it becomes inconceivable to build a model that can\ndiagnose all possible disease classes. Here, we propose a multi-label\ngeneralized zero shot learning (CXR-ML-GZSL) network that can simultaneously\npredict multiple seen and unseen diseases in CXR images. Given an input image,\nCXR-ML-GZSL learns a visual representation guided by the input's corresponding\nsemantics extracted from a rich medical text corpus. Towards this ambitious\ngoal, we propose to map both visual and semantic modalities to a latent feature\nspace using a novel learning objective. The objective ensures that (i) the most\nrelevant labels for the query image are ranked higher than irrelevant labels,\n(ii) the network learns a visual representation that is aligned with its\nsemantics in the latent feature space, and (iii) the mapped semantics preserve\ntheir original inter-class representation. The network is end-to-end trainable\nand requires no independent pre-training for the offline feature extractor.\nExperiments on the NIH Chest X-ray dataset show that our network outperforms\ntwo strong baselines in terms of recall, precision, f1 score, and area under\nthe receiver operating characteristic curve. Our code is publicly available at:\nhttps://github.com/nyuad-cai/CXR-ML-GZSL.git",
          "link": "http://arxiv.org/abs/2107.06563",
          "publishedOn": "2021-07-15T01:59:02.775Z",
          "wordCount": 698,
          "title": "Multi-Label Generalized Zero Shot Learning for the Classification of Disease in Chest Radiographs. (arXiv:2107.06563v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_X/0/1/0/all/0/1\">Xiaofei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiangtao Xie</a>",
          "description": "This is a short technical report introducing the solution of Team Rat for\nShort-video Parsing Face Parsing Track of The 3rd Person in Context (PIC)\nWorkshop and Challenge at CVPR 2021.\n\nIn this report, we propose an Edge-Aware Network (EANet) that uses edge\ninformation to refine the segmentation edge. To further obtain the finer edge\nresults, we introduce edge attention loss that only compute cross entropy on\nthe edges, it can effectively reduce the classification error around edge and\nget more smooth boundary. Benefiting from the edge information and edge\nattention loss, the proposed EANet achieves 86.16\\% accuracy in the Short-video\nFace Parsing track of the 3rd Person in Context (PIC) Workshop and Challenge,\nranked the third place.",
          "link": "http://arxiv.org/abs/2106.07409",
          "publishedOn": "2021-07-15T01:59:02.759Z",
          "wordCount": 588,
          "title": "3rd Place Solution for Short-video Face Parsing Challenge. (arXiv:2106.07409v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1\">Anton Obukhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakhuba_M/0/1/0/all/0/1\">Maxim Rakhuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiwu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "We study low-rank parameterizations of weight matrices with embedded spectral\nproperties in the Deep Learning context. The low-rank property leads to\nparameter efficiency and permits taking computational shortcuts when computing\nmappings. Spectral properties are often subject to constraints in optimization\nproblems, leading to better models and stability of optimization. We start by\nlooking at the compact SVD parameterization of weight matrices and identifying\nredundancy sources in the parameterization. We further apply the Tensor Train\n(TT) decomposition to the compact SVD components, and propose a non-redundant\ndifferentiable parameterization of fixed TT-rank tensor manifolds, termed the\nSpectral Tensor Train Parameterization (STTP). We demonstrate the effects of\nneural network compression in the image classification setting and both\ncompression and improved training stability in the generative adversarial\ntraining setting.",
          "link": "http://arxiv.org/abs/2103.04217",
          "publishedOn": "2021-07-15T01:59:02.743Z",
          "wordCount": 609,
          "title": "Spectral Tensor Train Parameterization of Deep Learning Layers. (arXiv:2103.04217v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guochen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Q/0/1/0/all/0/1\">Qiang Ling</a>",
          "description": "Semi-supervised Fine-Grained Recognition is a challenge task due to the\ndifficulty of data imbalance, high inter-class similarity and domain mismatch.\nRecent years, this field has witnessed great progress and many methods has\ngained great performance. However, these methods can hardly generalize to the\nlarge-scale datasets, such as Semi-iNat, as they are prone to suffer from noise\nin unlabeled data and the incompetence for learning features from imbalanced\nfine-grained data. In this work, we propose Bilateral-Branch Self-Training\nFramework (BiSTF), a simple yet effective framework to improve existing\nsemi-supervised learning methods on class-imbalanced and domain-shifted\nfine-grained data. By adjusting the update frequency through stochastic epoch\nupdate, BiSTF iteratively retrains a baseline SSL model with a labeled set\nexpanded by selectively adding pseudo-labeled samples from an unlabeled set,\nwhere the distribution of pseudo-labeled samples are the same as the labeled\ndata. We show that BiSTF outperforms the existing state-of-the-art SSL\nalgorithm on Semi-iNat dataset.",
          "link": "http://arxiv.org/abs/2107.06768",
          "publishedOn": "2021-07-15T01:59:02.736Z",
          "wordCount": 600,
          "title": "BiSTF: Bilateral-Branch Self-Training Framework for Semi-Supervised Large-scale Fine-Grained Recognition. (arXiv:2107.06768v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06552",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Eun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Face anti-spoofing (FAS) plays an important role in protecting face\nrecognition systems from face representation attacks. Many recent studies in\nFAS have approached this problem with domain generalization technique. Domain\ngeneralization aims to increase generalization performance to better detect\nvarious types of attacks and unseen attacks. However, previous studies in this\narea have defined each domain simply as an anti-spoofing datasets and focused\non developing learning techniques. In this paper, we proposed a method that\nenables network to judge its domain by itself with the clustered convolutional\nfeature statistics from intermediate layers of the network, without labeling\ndomains as datasets. We obtained pseudo-domain labels by not only using the\nnetwork extracting features, but also using depth estimators, which were\npreviously used only as an auxiliary task in FAS. In our experiments, we\ntrained with three datasets and evaluated the performance with the remaining\none dataset to demonstrate the effectiveness of the proposed method by\nconducting a total of four sets of experiments.",
          "link": "http://arxiv.org/abs/2107.06552",
          "publishedOn": "2021-07-15T01:59:02.730Z",
          "wordCount": 600,
          "title": "Domain Generalization with Pseudo-Domain Label for Face Anti-Spoofing. (arXiv:2107.06552v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongxu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1\">Pavlo Molchanov</a>",
          "description": "Understanding the behavior and vulnerability of pre-trained deep neural\nnetworks (DNNs) can help to improve them. Analysis can be performed via\nreversing the network's flow to generate inputs from internal representations.\nMost existing work relies on priors or data-intensive optimization to invert a\nmodel, yet struggles to scale to deep architectures and complex datasets. This\npaper presents a zero-shot direct model inversion framework that recovers the\ninput to the trained model given only the internal representation. The crux of\nour method is to inverse the DNN in a divide-and-conquer manner while\nre-syncing the inverted layers via cycle-consistency guidance with the help of\nsynthesized data. As a result, we obtain a single feed-forward model capable of\ninversion with a single forward pass without seeing any real data of the\noriginal task. With the proposed approach, we scale zero-shot direct inversion\nto deep architectures and complex datasets. We empirically show that modern\nclassification models on ImageNet can, surprisingly, be inverted, allowing an\napproximate recovery of the original 224x224px images from a representation\nafter more than 20 layers. Moreover, inversion of generators in GANs unveils\nlatent code of a given synthesized face image at 128x128px, which can even, in\nturn, improve defective synthesized images from GANs.",
          "link": "http://arxiv.org/abs/2107.06304",
          "publishedOn": "2021-07-15T01:59:02.724Z",
          "wordCount": 673,
          "title": "Deep Neural Networks are Surprisingly Reversible: A Baseline for Zero-Shot Inversion. (arXiv:2107.06304v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lahtinen_T/0/1/0/all/0/1\">Tuomo Lahtinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turtiainen_H/0/1/0/all/0/1\">Hannu Turtiainen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costin_A/0/1/0/all/0/1\">Andrei Costin</a>",
          "description": "Image annotation and large annotated datasets are crucial parts within the\nComputer Vision and Artificial Intelligence fields.At the same time, it is\nwell-known and acknowledged by the research community that the image annotation\nprocess is challenging, time-consuming and hard to scale. Therefore, the\nresearchers and practitioners are always seeking ways to perform the\nannotations easier, faster, and at higher quality. Even though several widely\nused tools exist and the tools' landscape evolved considerably, most of the\ntools still require intricate technical setups and high levels of technical\nsavviness from its operators and crowdsource contributors.\n\nIn order to address such challenges, we develop and present BRIMA -- a\nflexible and open-source browser extension that allows BRowser-only IMage\nAnnotation at considerably lower overheads. Once added to the browser, it\ninstantly allows the user to annotate images easily and efficiently directly\nfrom the browser without any installation or setup on the client-side. It also\nfeatures cross-browser and cross-platform functionality thus presenting itself\nas a neat tool for researchers within the Computer Vision, Artificial\nIntelligence, and privacy-related fields.",
          "link": "http://arxiv.org/abs/2107.06351",
          "publishedOn": "2021-07-15T01:59:02.718Z",
          "wordCount": 612,
          "title": "BRIMA: low-overhead BRowser-only IMage Annotation tool (Preprint). (arXiv:2107.06351v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Tuan Anh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_K/0/1/0/all/0/1\">Katherine M. Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_L/0/1/0/all/0/1\">Luke Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1\">Kevin Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+N_S/0/1/0/all/0/1\">Siddharth N</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gershman_S/0/1/0/all/0/1\">Samuel J. Gershman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>",
          "description": "Modeling complex phenomena typically involves the use of both discrete and\ncontinuous variables. Such a setting applies across a wide range of problems,\nfrom identifying trends in time-series data to performing effective\ncompositional scene understanding in images. Here, we propose Hybrid Memoised\nWake-Sleep (HMWS), an algorithm for effective inference in such hybrid\ndiscrete-continuous models. Prior approaches to learning suffer as they need to\nperform repeated expensive inner-loop discrete inference. We build on a recent\napproach, Memoised Wake-Sleep (MWS), which alleviates part of the problem by\nmemoising discrete variables, and extend it to allow for a principled and\neffective way to handle continuous variables by learning a separate recognition\nmodel used for importance-sampling based approximate inference and\nmarginalization. We evaluate HMWS in the GP-kernel learning and 3D scene\nunderstanding domains, and show that it outperforms current state-of-the-art\ninference methods.",
          "link": "http://arxiv.org/abs/2107.06393",
          "publishedOn": "2021-07-15T01:59:02.702Z",
          "wordCount": 595,
          "title": "Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface. (arXiv:2107.06393v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "Most existing Vision-and-Language (V&L) models rely on pre-trained visual\nencoders, using a relatively small set of manually-annotated data (as compared\nto web-crawled data), to perceive the visual world. However, it has been\nobserved that large-scale pretraining usually can result in better\ngeneralization performance, e.g., CLIP (Contrastive Language-Image\nPre-training), trained on a massive amount of image-caption pairs, has shown a\nstrong zero-shot capability on various vision tasks. To further study the\nadvantage brought by CLIP, we propose to use CLIP as the visual encoder in\nvarious V&L models in two typical scenarios: 1) plugging CLIP into\ntask-specific fine-tuning; 2) combining CLIP with V&L pre-training and\ntransferring to downstream tasks. We show that CLIP significantly outperforms\nwidely-used visual encoders trained with in-domain annotated data, such as\nBottomUp-TopDown. We achieve competitive or better results on diverse V&L\ntasks, while establishing new state-of-the-art results on Visual Question\nAnswering, Visual Entailment, and V&L Navigation tasks. We release our code at\nhttps://github.com/clip-vil/CLIP-ViL.",
          "link": "http://arxiv.org/abs/2107.06383",
          "publishedOn": "2021-07-15T01:59:02.694Z",
          "wordCount": 618,
          "title": "How Much Can CLIP Benefit Vision-and-Language Tasks?. (arXiv:2107.06383v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>",
          "description": "High-definition map (HD map) construction is a crucial problem for autonomous\ndriving. This problem typically involves collecting high-quality point clouds,\nfusing multiple point clouds of the same scene, annotating map elements, and\nupdating maps constantly. This pipeline, however, requires a vast amount of\nhuman efforts and resources which limits its scalability. Additionally,\ntraditional HD maps are coupled with centimeter-level accurate localization\nwhich is unreliable in many scenarios. In this paper, we argue that online map\nlearning, which dynamically constructs the HD maps based on local sensor\nobservations, is a more scalable way to provide semantic and geometry priors to\nself-driving vehicles than traditional pre-annotated HD maps. Meanwhile, we\nintroduce an online map learning method, titled HDMapNet. It encodes image\nfeatures from surrounding cameras and/or point clouds from LiDAR, and predicts\nvectorized map elements in the bird's-eye view. We benchmark HDMapNet on the\nnuScenes dataset and show that in all settings, it performs better than\nbaseline methods. Of note, our fusion-based HDMapNet outperforms existing\nmethods by more than 50% in all metrics. To accelerate future research, we\ndevelop customized metrics to evaluate map learning performance, including both\nsemantic-level and instance-level ones. By introducing this method and metrics,\nwe invite the community to study this novel map learning problem. We will\nrelease our code and evaluation kit to facilitate future development.",
          "link": "http://arxiv.org/abs/2107.06307",
          "publishedOn": "2021-07-15T01:59:02.688Z",
          "wordCount": 660,
          "title": "HDMapNet: An Online HD Map Construction and Evaluation Framework. (arXiv:2107.06307v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mordvintsev_A/0/1/0/all/0/1\">Alexander Mordvintsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Randazzo_E/0/1/0/all/0/1\">Ettore Randazzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niklasson_E/0/1/0/all/0/1\">Eyvind Niklasson</a>",
          "description": "Reaction-Diffusion (RD) systems provide a computational framework that\ngoverns many pattern formation processes in nature. Current RD system design\npractices boil down to trial-and-error parameter search. We propose a\ndifferentiable optimization method for learning the RD system parameters to\nperform example-based texture synthesis on a 2D plane. We do this by\nrepresenting the RD system as a variant of Neural Cellular Automata and using\ntask-specific differentiable loss functions. RD systems generated by our method\nexhibit robust, non-trivial 'life-like' behavior.",
          "link": "http://arxiv.org/abs/2107.06862",
          "publishedOn": "2021-07-15T01:59:02.641Z",
          "wordCount": 522,
          "title": "Differentiable Programming of Reaction-Diffusion Patterns. (arXiv:2107.06862v1 [cs.NE])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.00088",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Hooten_S/0/1/0/all/0/1\">Sean Hooten</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Beausoleil_R/0/1/0/all/0/1\">Raymond G. Beausoleil</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Vaerenbergh_T/0/1/0/all/0/1\">Thomas Van Vaerenbergh</a>",
          "description": "We present a proof-of-concept technique for the inverse design of\nelectromagnetic devices motivated by the policy gradient method in\nreinforcement learning, named PHORCED (PHotonic Optimization using REINFORCE\nCriteria for Enhanced Design). This technique uses a probabilistic generative\nneural network interfaced with an electromagnetic solver to assist in the\ndesign of photonic devices, such as grating couplers. We show that PHORCED\nobtains better performing grating coupler designs than local gradient-based\ninverse design via the adjoint method, while potentially providing faster\nconvergence over competing state-of-the-art generative methods. Furthermore, we\nimplement transfer learning with PHORCED, demonstrating that a neural network\ntrained to optimize 8$^\\circ$ grating couplers can then be re-trained on\ngrating couplers with alternate scattering angles while requiring >$10\\times$\nfewer simulations than control cases.",
          "link": "http://arxiv.org/abs/2107.00088",
          "publishedOn": "2021-07-20T02:04:49.165Z",
          "wordCount": 591,
          "title": "Inverse Design of Grating Couplers Using the Policy Gradient Method from Reinforcement Learning. (arXiv:2107.00088v2 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03920",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dalmasso_N/0/1/0/all/0/1\">Niccol&#xf2; Dalmasso</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhao_D/0/1/0/all/0/1\">David Zhao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Izbicki_R/0/1/0/all/0/1\">Rafael Izbicki</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1\">Ann B. Lee</a>",
          "description": "Many areas of science make extensive use of computer simulators that\nimplicitly encode likelihood functions of complex systems. Classical\nstatistical methods are poorly suited for these so-called likelihood-free\ninference (LFI) settings, outside the asymptotic and low-dimensional regimes.\nAlthough new machine learning methods, such as normalizing flows, have\nrevolutionized the sample efficiency and capacity of LFI methods, it remains an\nopen question whether they produce reliable measures of uncertainty. This paper\npresents a statistical framework for LFI that unifies classical statistics with\nmodern machine learning to: (1) efficiently construct frequentist confidence\nsets and hypothesis tests with finite-sample guarantees of nominal coverage\n(type I error control) and power; (2) provide practical diagnostics for\nassessing empirical coverage over the entire parameter space. We refer to our\nframework as likelihood-free frequentist inference (LF2I). Any method that\nestimates a test statistic, like the likelihood ratio, can be plugged into our\nframework to create valid confidence sets and compute diagnostics, without\ncostly Monte Carlo samples at fixed parameter settings. In this work, we\nspecifically study the power of two test statistics (ACORE and BFF), which,\nrespectively, maximize versus integrate an odds function over the parameter\nspace. Our study offers multifaceted perspectives on the challenges in LF2I.",
          "link": "http://arxiv.org/abs/2107.03920",
          "publishedOn": "2021-07-20T02:04:49.148Z",
          "wordCount": 674,
          "title": "Likelihood-Free Frequentist Inference: Bridging Classical Statistics and Machine Learning in Simulation and Uncertainty Quantification. (arXiv:2107.03920v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wagener_N/0/1/0/all/0/1\">Nolan Wagener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boots_B/0/1/0/all/0/1\">Byron Boots</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Ching-An Cheng</a>",
          "description": "Many sequential decision problems involve finding a policy that maximizes\ntotal reward while obeying safety constraints. Although much recent research\nhas focused on the development of safe reinforcement learning (RL) algorithms\nthat produce a safe policy after training, ensuring safety during training as\nwell remains an open problem. A fundamental challenge is performing exploration\nwhile still satisfying constraints in an unknown Markov decision process (MDP).\nIn this work, we address this problem for the chance-constrained setting. We\npropose a new algorithm, SAILR, that uses an intervention mechanism based on\nadvantage functions to keep the agent safe throughout training and optimizes\nthe agent's policy using off-the-shelf RL algorithms designed for unconstrained\nMDPs. Our method comes with strong guarantees on safety during both training\nand deployment (i.e., after training and without the intervention mechanism)\nand policy performance compared to the optimal safety-constrained policy. In\nour experiments, we show that SAILR violates constraints far less during\ntraining than standard safe RL and constrained MDP approaches and converges to\na well-performing policy that can be deployed safely without intervention. Our\ncode is available at https://github.com/nolanwagener/safe_rl.",
          "link": "http://arxiv.org/abs/2106.09110",
          "publishedOn": "2021-07-20T02:04:49.130Z",
          "wordCount": 653,
          "title": "Safe Reinforcement Learning Using Advantage-Based Intervention. (arXiv:2106.09110v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.01089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_C/0/1/0/all/0/1\">Chirag Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_S/0/1/0/all/0/1\">Sivaraman Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramdas_A/0/1/0/all/0/1\">Aaditya Ramdas</a>",
          "description": "We derive bounds on the path length $\\zeta$ of gradient descent (GD) and\ngradient flow (GF) curves for various classes of smooth convex and nonconvex\nfunctions. Among other results, we prove that: (a) if the iterates are linearly\nconvergent with factor $(1-c)$, then $\\zeta$ is at most $\\mathcal{O}(1/c)$; (b)\nunder the Polyak-Kurdyka-Lojasiewicz (PKL) condition, $\\zeta$ is at most\n$\\mathcal{O}(\\sqrt{\\kappa})$, where $\\kappa$ is the condition number, and at\nleast $\\widetilde\\Omega(\\sqrt{d} \\wedge \\kappa^{1/4})$; (c) for quadratics,\n$\\zeta$ is $\\Theta(\\min\\{\\sqrt{d},\\sqrt{\\log \\kappa}\\})$ and in some cases can\nbe independent of $\\kappa$; (d) assuming just convexity, $\\zeta$ can be at most\n$2^{4d\\log d}$; (e) for separable quasiconvex functions, $\\zeta$ is\n${\\Theta}(\\sqrt{d})$. Thus, we advance current understanding of the properties\nof GD and GF curves beyond rates of convergence. We expect our techniques to\nfacilitate future studies for other algorithms.",
          "link": "http://arxiv.org/abs/1908.01089",
          "publishedOn": "2021-07-20T02:04:49.113Z",
          "wordCount": 637,
          "title": "Path Length Bounds for Gradient Descent and Flow. (arXiv:1908.01089v4 [cs.LG] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04086",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_M/0/1/0/all/0/1\">Mohit Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1\">Lingyang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zi Yu Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lanjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_P/0/1/0/all/0/1\">Peter Cho-Ho Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>",
          "description": "Massive deployment of Graph Neural Networks (GNNs) in high-stake applications\ngenerates a strong demand for explanations that are robust to noise and align\nwell with human intuition. Most existing methods generate explanations by\nidentifying a subgraph of an input graph that has a strong correlation with the\nprediction. These explanations are not robust to noise because independently\noptimizing the correlation for a single input can easily overfit noise.\nMoreover, they do not align well with human intuition because removing an\nidentified subgraph from an input graph does not necessarily change the\nprediction result. In this paper, we propose a novel method to generate robust\ncounterfactual explanations on GNNs by explicitly modelling the common decision\nlogic of GNNs on similar input graphs. Our explanations are naturally robust to\nnoise because they are produced from the common decision boundaries of a GNN\nthat govern the predictions of many similar input graphs. The explanations also\nalign well with human intuition because removing the set of edges identified by\nan explanation from the input graph changes the prediction significantly.\nExhaustive experiments on many public datasets demonstrate the superior\nperformance of our method.",
          "link": "http://arxiv.org/abs/2107.04086",
          "publishedOn": "2021-07-20T02:04:49.068Z",
          "wordCount": 645,
          "title": "Robust Counterfactual Explanations on Graph Neural Networks. (arXiv:2107.04086v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04631",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_F/0/1/0/all/0/1\">Fangcao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cervone_G/0/1/0/all/0/1\">Guido Cervone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salvador_M/0/1/0/all/0/1\">Mark Salvador</a>",
          "description": "Atmospheric correction is a fundamental task in remote sensing because\nobservations are taken either of the atmosphere or looking through the\natmosphere. Atmospheric correction errors can significantly alter the spectral\nsignature of the observations, and lead to invalid classifications or target\ndetection. This is even more crucial when working with hyperspectral data,\nwhere a precise measurement of spectral properties is required.\nState-of-the-art physics-based atmospheric correction approaches require\nextensive prior knowledge about sensor characteristics, collection geometry,\nand environmental characteristics of the scene being collected. These\napproaches are computationally expensive, prone to inaccuracy due to lack of\nsufficient environmental and collection information, and often impossible for\nreal-time applications. In this paper, a geometry-dependent hybrid neural\nnetwork is proposed for automatic atmospheric correction using multi-scan\nhyperspectral data collected from different geometries. The proposed network\ncan characterize the atmosphere without any additional meteorological data. A\ngrid-search method is also proposed to solve the temperature emissivity\nseparation problem. Results show that the proposed network has the capacity to\naccurately characterize the atmosphere and estimate target emissivity spectra\nwith a Mean Absolute Error (MAE) under 0.02 for 29 different materials. This\nsolution can lead to accurate atmospheric correction to improve target\ndetection for real time applications.",
          "link": "http://arxiv.org/abs/2107.04631",
          "publishedOn": "2021-07-20T02:04:49.049Z",
          "wordCount": 670,
          "title": "Ill-posed Surface Emissivity Retrieval from Multi-Geometry Hyperspectral Images using a Hybrid Deep Neural Network. (arXiv:2107.04631v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05318",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Skandarani_Y/0/1/0/all/0/1\">Youssef Skandarani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lalande_A/0/1/0/all/0/1\">Alain Lalande</a>",
          "description": "Generative Adversarial Networks (GANs) have become increasingly powerful,\ngenerating mind-blowing photorealistic images that mimic the content of\ndatasets they were trained to replicate. One recurrent theme in medical imaging\nis whether GANs can also be effective at generating workable medical data as\nthey are for generating realistic RGB images. In this paper, we perform a\nmulti-GAN and multi-application study to gauge the benefits of GANs in medical\nimaging. We tested various GAN architectures from basic DCGAN to more\nsophisticated style-based GANs on three medical imaging modalities and organs\nnamely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on\nwell-known and widely utilized datasets from which their FID score were\ncomputed to measure the visual acuity of their generated images. We further\ntested their usefulness by measuring the segmentation accuracy of a U-Net\ntrained on these generated images.\n\nResults reveal that GANs are far from being equal as some are ill-suited for\nmedical imaging applications while others are much better off. The\ntop-performing GANs are capable of generating realistic-looking medical images\nby FID standards that can fool trained experts in a visual Turing test and\ncomply to some metrics. However, segmentation results suggests that no GAN is\ncapable of reproducing the full richness of a medical datasets.",
          "link": "http://arxiv.org/abs/2105.05318",
          "publishedOn": "2021-07-20T02:04:49.029Z",
          "wordCount": 674,
          "title": "GANs for Medical Image Synthesis: An Empirical Study. (arXiv:2105.05318v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yifan Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tianjun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1\">David Wipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We propose a hierarchical graph neural network (GNN) model that learns how to\ncluster a set of images into an unknown number of identities using a training\nset of images annotated with labels belonging to a disjoint set of identities.\nOur hierarchical GNN uses a novel approach to merge connected components\npredicted at each level of the hierarchy to form a new graph at the next level.\nUnlike fully unsupervised hierarchical clustering, the choice of grouping and\ncomplexity criteria stems naturally from supervision in the training set. The\nresulting method, Hi-LANDER, achieves an average of 54% improvement in F-score\nand 8% increase in Normalized Mutual Information (NMI) relative to current\nGNN-based clustering algorithms. Additionally, state-of-the-art GNN-based\nmethods rely on separate models to predict linkage probabilities and node\ndensities as intermediate steps of the clustering process. In contrast, our\nunified framework achieves a seven-fold decrease in computational cost. We\nrelease our training and inference code at\nhttps://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander.",
          "link": "http://arxiv.org/abs/2107.01319",
          "publishedOn": "2021-07-20T02:04:49.013Z",
          "wordCount": 624,
          "title": "Learning Hierarchical Graph Neural Networks for Image Clustering. (arXiv:2107.01319v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.10564",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gupta_C/0/1/0/all/0/1\">Chirag Gupta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Podkopaev_A/0/1/0/all/0/1\">Aleksandr Podkopaev</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1\">Aaditya Ramdas</a>",
          "description": "We study three notions of uncertainty quantification -- calibration,\nconfidence intervals and prediction sets -- for binary classification in the\ndistribution-free setting, that is without making any distributional\nassumptions on the data. With a focus towards calibration, we establish a\n'tripod' of theorems that connect these three notions for score-based\nclassifiers. A direct implication is that distribution-free calibration is only\npossible, even asymptotically, using a scoring function whose level sets\npartition the feature space into at most countably many sets. Parametric\ncalibration schemes such as variants of Platt scaling do not satisfy this\nrequirement, while nonparametric schemes based on binning do. To close the\nloop, we derive distribution-free confidence intervals for binned probabilities\nfor both fixed-width and uniform-mass binning. As a consequence of our 'tripod'\ntheorems, these confidence intervals for binned probabilities lead to\ndistribution-free calibration. We also derive extensions to settings with\nstreaming data and covariate shift.",
          "link": "http://arxiv.org/abs/2006.10564",
          "publishedOn": "2021-07-20T02:04:48.995Z",
          "wordCount": 637,
          "title": "Distribution-free binary classification: prediction sets, confidence intervals and calibration. (arXiv:2006.10564v3 [stat.ML] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_E/0/1/0/all/0/1\">Erin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1\">Meher Anand Kasam</a>",
          "description": "Data imbalance is a ubiquitous problem in machine learning. In large scale\ncollected and annotated datasets, data imbalance is either mitigated manually\nby undersampling frequent classes and oversampling rare classes, or planned for\nwith imputation and augmentation techniques. In both cases balancing data\nrequires labels. In other words, only annotated data can be balanced.\nCollecting fully annotated datasets is challenging, especially for large scale\nsatellite systems such as the unlabeled NASA's 35 PB Earth Imagery dataset.\nAlthough the NASA Earth Imagery dataset is unlabeled, there are implicit\nproperties of the data source that we can rely on to hypothesize about its\nimbalance, such as distribution of land and water in the case of the Earth's\nimagery. We present a new iterative method to balance unlabeled data. Our\nmethod utilizes image embeddings as a proxy for image labels that can be used\nto balance data, and ultimately when trained increases overall accuracy.",
          "link": "http://arxiv.org/abs/2107.03227",
          "publishedOn": "2021-07-20T02:04:48.978Z",
          "wordCount": 618,
          "title": "Scalable Data Balancing for Unlabeled Satellite Imagery. (arXiv:2107.03227v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burkhalter_L/0/1/0/all/0/1\">Lukas Burkhalter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lycklama_H/0/1/0/all/0/1\">Hidde Lycklama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viand_A/0/1/0/all/0/1\">Alexander Viand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuchler_N/0/1/0/all/0/1\">Nicolas K&#xfc;chler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hithnawi_A/0/1/0/all/0/1\">Anwar Hithnawi</a>",
          "description": "Federated Learning is an emerging decentralized machine learning paradigm\nthat allows a large number of clients to train a joint model without the need\nto share their private data. Participants instead only share ephemeral updates\nnecessary to train the model. To ensure the confidentiality of the client\nupdates, Federated Learning systems employ secure aggregation; clients encrypt\ntheir gradient updates, and only the aggregated model is revealed to the\nserver. Achieving this level of data protection, however, presents new\nchallenges to the robustness of Federated Learning, i.e., the ability to\ntolerate failures and attacks. Unfortunately, in this setting, a malicious\nclient can now easily exert influence on the model behavior without being\ndetected. As Federated Learning is being deployed in practice in a range of\nsensitive applications, its robustness is growing in importance. In this paper,\nwe take a step towards understanding and improving the robustness of secure\nFederated Learning. We start this paper with a systematic study that evaluates\nand analyzes existing attack vectors and discusses potential defenses and\nassesses their effectiveness. We then present RoFL, a secure Federated Learning\nsystem that improves robustness against malicious clients through input checks\non the encrypted model updates. RoFL extends Federated Learning's secure\naggregation protocol to allow expressing a variety of properties and\nconstraints on model updates using zero-knowledge proofs. To enable RoFL to\nscale to typical Federated Learning settings, we introduce several ML and\ncryptographic optimizations specific to Federated Learning. We implement and\nevaluate a prototype of RoFL and show that realistic ML models can be trained\nin a reasonable time while improving robustness.",
          "link": "http://arxiv.org/abs/2107.03311",
          "publishedOn": "2021-07-20T02:04:48.935Z",
          "wordCount": 731,
          "title": "RoFL: Attestable Robustness for Secure Federated Learning. (arXiv:2107.03311v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12627",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Huang_H/0/1/0/all/0/1\">Hsin-Yuan Huang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kueng_R/0/1/0/all/0/1\">Richard Kueng</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Torlai_G/0/1/0/all/0/1\">Giacomo Torlai</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Albert_V/0/1/0/all/0/1\">Victor V. Albert</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Preskill_J/0/1/0/all/0/1\">John Preskill</a>",
          "description": "Classical machine learning (ML) provides a potentially powerful approach to\nsolving challenging quantum many-body problems in physics and chemistry.\nHowever, the advantages of ML over more traditional methods have not been\nfirmly established. In this work, we prove that classical ML algorithms can\nefficiently predict ground state properties of gapped Hamiltonians in finite\nspatial dimensions, after learning from data obtained by measuring other\nHamiltonians in the same quantum phase of matter. In contrast, under widely\naccepted complexity theory assumptions, classical algorithms that do not learn\nfrom data cannot achieve the same guarantee. We also prove that classical ML\nalgorithms can efficiently classify a wide range of quantum phases of matter.\nOur arguments are based on the concept of a classical shadow, a succinct\nclassical description of a many-body quantum state that can be constructed in\nfeasible quantum experiments and be used to predict many properties of the\nstate. Extensive numerical experiments corroborate our theoretical results in a\nvariety of scenarios, including Rydberg atom systems, 2D random Heisenberg\nmodels, symmetry-protected topological phases, and topologically ordered\nphases.",
          "link": "http://arxiv.org/abs/2106.12627",
          "publishedOn": "2021-07-20T02:04:48.915Z",
          "wordCount": 642,
          "title": "Provably efficient machine learning for quantum many-body problems. (arXiv:2106.12627v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poskitt_C/0/1/0/all/0/1\">Christopher M. Poskitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>",
          "description": "Cyber-physical systems (CPSs) are widespread in critical domains, and\nsignificant damage can be caused if an attacker is able to modify the code of\ntheir programmable logic controllers (PLCs). Unfortunately, traditional\ntechniques for attesting code integrity (i.e. verifying that it has not been\nmodified) rely on firmware access or roots-of-trust, neither of which\nproprietary or legacy PLCs are likely to provide. In this paper, we propose a\npractical code integrity checking solution based on privacy-preserving black\nbox models that instead attest the input/output behaviour of PLC programs.\nUsing faithful offline copies of the PLC programs, we identify their most\nimportant inputs through an information flow analysis, execute them on multiple\ncombinations to collect data, then train neural networks able to predict PLC\noutputs (i.e. actuator commands) from their inputs. By exploiting the black box\nnature of the model, our solution maintains the privacy of the original PLC\ncode and does not assume that attackers are unaware of its presence. The trust\ninstead comes from the fact that it is extremely hard to attack the PLC code\nand neural networks at the same time and with consistent outcomes. We evaluated\nour approach on a modern six-stage water treatment plant testbed, finding that\nit could predict actuator states from PLC inputs with near-100% accuracy, and\nthus could detect all 120 effective code mutations that we subjected the PLCs\nto. Finally, we found that it is not practically possible to simultaneously\nmodify the PLC code and apply discreet adversarial noise to our attesters in a\nway that leads to consistent (mis-)predictions.",
          "link": "http://arxiv.org/abs/2106.07851",
          "publishedOn": "2021-07-20T02:04:48.896Z",
          "wordCount": 750,
          "title": "Code Integrity Attestation for PLCs using Black Box Neural Network Predictions. (arXiv:2106.07851v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_S/0/1/0/all/0/1\">Sara Hajj Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassar_M/0/1/0/all/0/1\">Mohamed Nassar</a>",
          "description": "Deep learning is a type of machine learning that adapts a deep hierarchy of\nconcepts. Deep learning classifiers link the most basic version of concepts at\nthe input layer to the most abstract version of concepts at the output layer,\nalso known as a class or label. However, once trained over a finite set of\nclasses, some deep learning models do not have the power to say that a given\ninput does not belong to any of the classes and simply cannot be linked.\nCorrectly invalidating the prediction of unrelated classes is a challenging\nproblem that has been tackled in many ways in the literature. Novelty detection\ngives deep learning the ability to output \"do not know\" for novel/unseen\nclasses. Still, no attention has been given to the security aspects of novelty\ndetection. In this paper, we consider the case study of abstraction-based\nnovelty detection and show that it is not robust against adversarial samples.\nMoreover, we show the feasibility of crafting adversarial samples that fool the\ndeep learning classifier and bypass the novelty detection monitoring at the\nsame time. In other words, these monitoring boxes are hackable. We demonstrate\nthat novelty detection itself ends up as an attack surface.",
          "link": "http://arxiv.org/abs/2107.04764",
          "publishedOn": "2021-07-20T02:04:48.877Z",
          "wordCount": 661,
          "title": "Hack The Box: Fooling Deep Learning Abstraction-Based Monitors. (arXiv:2107.04764v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1\">Gail Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yahav_E/0/1/0/all/0/1\">Eran Yahav</a>",
          "description": "What is the computational model behind a Transformer? Where recurrent neural\nnetworks have direct parallels in finite state machines, allowing clear\ndiscussion and thought around architecture variants or trained models,\nTransformers have no such familiar parallel. In this paper we aim to change\nthat, proposing a computational model for the transformer-encoder in the form\nof a programming language. We map the basic components of a transformer-encoder\n-- attention and feed-forward computation -- into simple primitives, around\nwhich we form a programming language: the Restricted Access Sequence Processing\nLanguage (RASP). We show how RASP can be used to program solutions to tasks\nthat could conceivably be learned by a Transformer, and how a Transformer can\nbe trained to mimic a RASP solution. In particular, we provide RASP programs\nfor histograms, sorting, and Dyck-languages. We further use our model to relate\ntheir difficulty in terms of the number of required layers and attention heads:\nanalyzing a RASP program implies a maximum number of heads and layers necessary\nto encode a task in a transformer. Finally, we see how insights gained from our\nabstraction might be used to explain phenomena seen in recent works.",
          "link": "http://arxiv.org/abs/2106.06981",
          "publishedOn": "2021-07-20T02:04:48.833Z",
          "wordCount": 651,
          "title": "Thinking Like Transformers. (arXiv:2106.06981v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1\">Dongsheng An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_N/0/1/0/all/0/1\">Na Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xianfeng Gu</a>",
          "description": "Optimal transport (OT) plays an essential role in various areas like machine\nlearning and deep learning. However, computing discrete optimal transport plan\nfor large scale problems with adequate accuracy and efficiency is still highly\nchallenging. Recently, methods based on the Sinkhorn algorithm add an entropy\nregularizer to the prime problem and get a trade off between efficiency and\naccuracy. In this paper, we propose a novel algorithm to further improve the\nefficiency and accuracy based on Nesterov's smoothing technique. Basically, the\nnon-smooth c-transform of the Kantorovich potential is approximated by the\nsmooth Log-Sum-Exp function, which finally smooths the original non-smooth\nKantorovich dual functional (energy). The smooth Kantorovich functional can be\noptimized by the fast proximal gradient algorithm (FISTA) efficiently.\nTheoretically, the computational complexity of the proposed method is given by\n$O(n^{\\frac{5}{2}} \\sqrt{\\log n} /\\epsilon)$, which is lower than that of the\nSinkhorn algorithm. Empirically, compared with the Sinkhorn algorithm, our\nexperimental results demonstrate that the proposed method achieves faster\nconvergence and better accuracy with the same parameter.",
          "link": "http://arxiv.org/abs/2104.05802",
          "publishedOn": "2021-07-20T02:04:48.814Z",
          "wordCount": 634,
          "title": "Efficient Optimal Transport Algorithm by Accelerated Gradient descent. (arXiv:2104.05802v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1\">Prashant Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_M/0/1/0/all/0/1\">Mrigank Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varambally_S/0/1/0/all/0/1\">Sumanth Varambally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1\">Prathosh AP</a>",
          "description": "Generalization of machine learning models trained on a set of source domains\non unseen target domains with different statistics, is a challenging problem.\nWhile many approaches have been proposed to solve this problem, they only\nutilize source data during training but do not take advantage of the fact that\na single target example is available at the time of inference. Motivated by\nthis, we propose a method that effectively uses the target sample during\ninference beyond mere classification. Our method has three components - (i) A\nlabel-preserving feature or metric transformation on source data such that the\nsource samples are clustered in accordance with their class irrespective of\ntheir domain (ii) A generative model trained on the these features (iii) A\nlabel-preserving projection of the target point on the source-feature manifold\nduring inference via solving an optimization problem on the input space of the\ngenerative model using the learned metric. Finally, the projected target is\nused in the classifier. Since the projected target feature comes from the\nsource manifold and has the same label as the real target by design, the\nclassifier is expected to perform better on it than the true target. We\ndemonstrate that our method outperforms the state-of-the-art Domain\nGeneralization methods on multiple datasets and tasks.",
          "link": "http://arxiv.org/abs/2103.01134",
          "publishedOn": "2021-07-20T02:04:48.795Z",
          "wordCount": 678,
          "title": "Domain Generalization via Inference-time Label-Preserving Target Projections. (arXiv:2103.01134v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all the previously\nmentioned drawbacks). The entropic out-of-distribution detection solution\ncomprises the IsoMax loss for training and the entropic score for\nout-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in\nreplacement because swapping the SoftMax loss with the IsoMax loss requires no\nchanges in the model's architecture or training procedures/hyperparameters. In\nthis paper, we propose to perform what we call an isometrization of the\ndistances used in the IsoMax loss. Additionally, we propose to replace the\nentropic score with the minimum distance score. Our experiments showed that\nthese simple modifications increase out-of-distribution detection performance\nwhile keeping the solution seamless. Code available at\n$\\href{https://github.com/dlmacedo/entropic-out-of-distribution-detection}{\\text{entropic\nout-of-distribution detection}}$.",
          "link": "http://arxiv.org/abs/2105.14399",
          "publishedOn": "2021-07-20T02:04:48.643Z",
          "wordCount": 638,
          "title": "Improving Entropic Out-of-Distribution Detection using Isometric Distances and the Minimum Distance Score. (arXiv:2105.14399v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lixin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhanxing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "This is the Proceedings of ICML 2021 Workshop on Theoretic Foundation,\nCriticism, and Application Trend of Explainable AI. Deep neural networks (DNNs)\nhave undoubtedly brought great success to a wide range of applications in\ncomputer vision, computational linguistics, and AI. However, foundational\nprinciples underlying the DNNs' success and their resilience to adversarial\nattacks are still largely missing. Interpreting and theorizing the internal\nmechanisms of DNNs becomes a compelling yet controversial topic. This workshop\npays a special interest in theoretic foundations, limitations, and new\napplication trends in the scope of XAI. These issues reflect new bottlenecks in\nthe future development of XAI.",
          "link": "http://arxiv.org/abs/2107.08821",
          "publishedOn": "2021-07-20T02:04:48.623Z",
          "wordCount": 567,
          "title": "Proceedings of ICML 2021 Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI. (arXiv:2107.08821v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Zhenhou Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoyang Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chendong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>",
          "description": "Text to speech (TTS) is a crucial task for user interaction, but TTS model\ntraining relies on a sizable set of high-quality original datasets. Due to\nprivacy and security issues, the original datasets are usually unavailable\ndirectly. Recently, federated learning proposes a popular distributed machine\nlearning paradigm with an enhanced privacy protection mechanism. It offers a\npractical and secure framework for data owners to collaborate with others, thus\nobtaining a better global model trained on the larger dataset. However, due to\nthe high complexity of transformer models, the convergence process becomes slow\nand unstable in the federated learning setting. Besides, the transformer model\ntrained in federated learning is costly communication and limited computational\nspeed on clients, impeding its popularity. To deal with these challenges, we\npropose the federated dynamic transformer. On the one hand, the performance is\ngreatly improved comparing with the federated transformer, approaching\ncentralize-trained Transformer-TTS when increasing clients number. On the other\nhand, it achieves faster and more stable convergence in the training phase and\nsignificantly reduces communication time. Experiments on the LJSpeech dataset\nalso strongly prove our method's advantage.",
          "link": "http://arxiv.org/abs/2107.08795",
          "publishedOn": "2021-07-20T02:04:48.580Z",
          "wordCount": 624,
          "title": "Federated Learning with Dynamic Transformer for Text to Speech. (arXiv:2107.08795v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiuqi/0/1/0/all/0/1\">Jiuqi</a> (Elise) <a href=\"http://arxiv.org/find/cs/1/au:+Zhang/0/1/0/all/0/1\">Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulet_B/0/1/0/all/0/1\">Benoit Boulet</a>",
          "description": "With the rapid increase in the integration of renewable energy generation and\nthe wide adoption of various electric appliances, power grids are now faced\nwith more and more challenges. One prominent challenge is to implement\nefficient anomaly detection for different types of anomalous behaviors within\npower grids. These anomalous behaviors might be induced by unusual consumption\npatterns of the users, faulty grid infrastructures, outages, external\ncyberattacks, or energy fraud. Identifying such anomalies is of critical\nimportance for the reliable and efficient operation of modern power grids.\nVarious methods have been proposed for anomaly detection on power grid\ntime-series data. This paper presents a short survey of the recent advances in\nanomaly detection for power grid time-series data. Specifically, we first\noutline current research challenges in the power grid anomaly detection domain\nand further review the major anomaly detection approaches. Finally, we conclude\nthe survey by identifying the potential directions for future research.",
          "link": "http://arxiv.org/abs/2107.08835",
          "publishedOn": "2021-07-20T02:04:48.542Z",
          "wordCount": 602,
          "title": "Time Series Anomaly Detection for Smart Grids: A Survey. (arXiv:2107.08835v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wentao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiawei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bolin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wentao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bin Cui</a>",
          "description": "End-to-end AutoML has attracted intensive interests from both academia and\nindustry, which automatically searches for ML pipelines in a space induced by\nfeature engineering, algorithm/model selection, and hyper-parameter tuning.\nExisting AutoML systems, however, suffer from scalability issues when applying\nto application domains with large, high-dimensional search spaces. We present\nVolcanoML, a scalable and extensible framework that facilitates systematic\nexploration of large AutoML search spaces. VolcanoML introduces and implements\nbasic building blocks that decompose a large search space into smaller ones,\nand allows users to utilize these building blocks to compose an execution plan\nfor the AutoML problem at hand. VolcanoML further supports a Volcano-style\nexecution model - akin to the one supported by modern database systems - to\nexecute the plan constructed. Our evaluation demonstrates that, not only does\nVolcanoML raise the level of expressiveness for search space decomposition in\nAutoML, it also leads to actual findings of decomposition strategies that are\nsignificantly more efficient than the ones employed by state-of-the-art AutoML\nsystems such as auto-sklearn.",
          "link": "http://arxiv.org/abs/2107.08861",
          "publishedOn": "2021-07-20T02:04:48.525Z",
          "wordCount": 616,
          "title": "VolcanoML: Speeding up End-to-End AutoML via Scalable Search Space Decomposition. (arXiv:2107.08861v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingqi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_M/0/1/0/all/0/1\">Mon-on Pun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haojun Li</a>",
          "description": "Maintaining long-term exploration ability remains one of the challenges of\ndeep reinforcement learning (DRL). In practice, the reward shaping-based\napproaches are leveraged to provide intrinsic rewards for the agent to\nincentivize motivation. However, most existing IRS modules rely on attendant\nmodels or additional memory to record and analyze learning procedures, which\nleads to high computational complexity and low robustness. Moreover, they\noveremphasize the influence of a single state on exploration, which cannot\nevaluate the exploration performance from a global perspective. To tackle the\nproblem, state entropy-based methods are proposed to encourage the agent to\nvisit the state space more equitably. However, the estimation error and sample\ncomplexity are prohibitive when handling environments with high-dimensional\nobservation. In this paper, we introduce a novel metric entitled Jain's\nfairness index (JFI) to replace the entropy regularizer, which requires no\nadditional models or memory. In particular, JFI overcomes the vanishing\nintrinsic rewards problem and can be generalized into arbitrary tasks.\nFurthermore, we use a variational auto-encoder (VAE) model to capture the\nlife-long novelty of states. Finally, the global JFI score and local state\nnovelty are combined to form a multimodal intrinsic reward, controlling the\nexploration extent more precisely. Finally, extensive simulation results\ndemonstrate that our multimodal reward shaping (MMRS) method can achieve higher\nperformance in contrast to other benchmark schemes.",
          "link": "http://arxiv.org/abs/2107.08888",
          "publishedOn": "2021-07-20T02:04:48.508Z",
          "wordCount": 658,
          "title": "Multimodal Reward Shaping for Efficient Exploration in Reinforcement Learning. (arXiv:2107.08888v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koutini_K/0/1/0/all/0/1\">Khaled Koutini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eghbal_zadeh_H/0/1/0/all/0/1\">Hamid Eghbal-zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henkel_F/0/1/0/all/0/1\">Florian Henkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_J/0/1/0/all/0/1\">Jan Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1\">Gerhard Widmer</a>",
          "description": "Convolutional Neural Networks (CNNs) have been dominating classification\ntasks in various domains, such as machine vision, machine listening, and\nnatural language processing. In machine listening, while generally exhibiting\nvery good generalization capabilities, CNNs are sensitive to the specific audio\nrecording device used, which has been recognized as a substantial problem in\nthe acoustic scene classification (DCASE) community. In this study, we\ninvestigate the relationship between over-parameterization of acoustic scene\nclassification models, and their resulting generalization abilities.\nSpecifically, we test scaling CNNs in width and depth, under different\nconditions. Our results indicate that increasing width improves generalization\nto unseen devices, even without an increase in the number of parameters.",
          "link": "http://arxiv.org/abs/2107.08933",
          "publishedOn": "2021-07-20T02:04:48.490Z",
          "wordCount": 562,
          "title": "Over-Parameterization and Generalization in Audio Classification. (arXiv:2107.08933v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1\">Ian Osband</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zheng Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asghari_M/0/1/0/all/0/1\">Mohammad Asghari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahimi_M/0/1/0/all/0/1\">Morteza Ibrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1\">Benjamin Van Roy</a>",
          "description": "We introduce the \\textit{epistemic neural network} (ENN) as an interface for\nuncertainty modeling in deep learning. All existing approaches to uncertainty\nmodeling can be expressed as ENNs, and any ENN can be identified with a\nBayesian neural network. However, this new perspective provides several\npromising directions for future research. Where prior work has developed\nprobabilistic inference tools for neural networks; we ask instead, `which\nneural networks are suitable as tools for probabilistic inference?'. We propose\na clear and simple metric for progress in ENNs: the KL-divergence with respect\nto a target distribution. We develop a computational testbed based on inference\nin a neural network Gaussian process and release our code as a benchmark at\n\\url{https://github.com/deepmind/enn}. We evaluate several canonical approaches\nto uncertainty modeling in deep learning, and find they vary greatly in their\nperformance. We provide insight to the sensitivity of these results and show\nthat our metric is highly correlated with performance in sequential decision\nproblems. Finally, we provide indications that new ENN architectures can\nimprove performance in both the statistical quality and computational cost.",
          "link": "http://arxiv.org/abs/2107.08924",
          "publishedOn": "2021-07-20T02:04:48.445Z",
          "wordCount": 612,
          "title": "Epistemic Neural Networks. (arXiv:2107.08924v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ntakouris_T/0/1/0/all/0/1\">Theodoros Ntakouris</a>",
          "description": "In this document, a neural network is employed in order to estimate the\nsolution of the initial value problem in the context of non linear\ntrajectories. Such trajectories can be subject to gravity, thrust, drag,\ncentrifugal force, temperature, ambient air density and pressure. First, we\ngenerate a grid of trajectory points given a specified uniform density as a\ndesign parameter and then we investigate the performance of a neural network in\na compression and inverse problem task: the network is trained to predict the\ninitial conditions of the dynamics model we used in the simulation, given a\ntarget point in space. We investigate this as a regression task, with error\npropagation in consideration. For target points, up to a radius of 2\nkilometers, the model is able to accurately predict the initial conditions of\nthe trajectories, with sub-meter deviation. This simulation-based training\nprocess and novel real-world evaluation method is capable of computing\ntrajectories of arbitrary dimensions.",
          "link": "http://arxiv.org/abs/2107.08849",
          "publishedOn": "2021-07-20T02:04:48.411Z",
          "wordCount": 604,
          "title": "Exploring the efficacy of neural networks for trajectory compression and the inverse problem. (arXiv:2107.08849v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_B/0/1/0/all/0/1\">Bhumika Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1\">Anuj Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anjum/0/1/0/all/0/1\">Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katarya_R/0/1/0/all/0/1\">Rahul Katarya</a>",
          "description": "Cyberbullying is of extreme prevalence today. Online-hate comments, toxicity,\ncyberbullying amongst children and other vulnerable groups are only growing\nover online classes, and increased access to social platforms, especially post\nCOVID-19. It is paramount to detect and ensure minors' safety across social\nplatforms so that any violence or hate-crime is automatically detected and\nstrict action is taken against it. In our work, we explore binary\nclassification by using a combination of datasets from various social media\nplatforms that cover a wide range of cyberbullying such as sexism, racism,\nabusive, and hate-speech. We experiment through multiple models such as\nBi-LSTM, GloVe, state-of-the-art models like BERT, and apply a unique\npreprocessing technique by introducing a slang-abusive corpus, achieving a\nhigher precision in comparison to models without slang preprocessing.",
          "link": "http://arxiv.org/abs/2107.08902",
          "publishedOn": "2021-07-20T02:04:48.393Z",
          "wordCount": 626,
          "title": "Analysing Cyberbullying using Natural Language Processing by Understanding Jargon in Social Media. (arXiv:2107.08902v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_K/0/1/0/all/0/1\">Ke Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chunhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhijia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1\">Tierui Gong</a>",
          "description": "Federated learning is a widely used distributed deep learning framework that\nprotects the privacy of each client by exchanging model parameters rather than\nraw data. However, federated learning suffers from high communication costs, as\na considerable number of model parameters need to be transmitted many times\nduring the training process, making the approach inefficient, especially when\nthe communication network bandwidth is limited. This article proposes RingFed,\na novel framework to reduce communication overhead during the training process\nof federated learning. Rather than transmitting parameters between the center\nserver and each client, as in original federated learning, in the proposed\nRingFed, the updated parameters are transmitted between each client in turn,\nand only the final result is transmitted to the central server, thereby\nreducing the communication overhead substantially. After several local updates,\nclients first send their parameters to another proximal client, not to the\ncenter server directly, to preaggregate. Experiments on two different public\ndatasets show that RingFed has fast convergence, high model accuracy, and low\ncommunication cost.",
          "link": "http://arxiv.org/abs/2107.08873",
          "publishedOn": "2021-07-20T02:04:48.329Z",
          "wordCount": 609,
          "title": "RingFed: Reducing Communication Costs in Federated Learning on Non-IID Data. (arXiv:2107.08873v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1\">Adish Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafferty_A/0/1/0/all/0/1\">Anna N. Rafferty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radanovic_G/0/1/0/all/0/1\">Goran Radanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_N/0/1/0/all/0/1\">Neil T. Heffernan</a>",
          "description": "This survey article has grown out of the RL4ED workshop organized by the\nauthors at the Educational Data Mining (EDM) 2021 conference. We organized this\nworkshop as part of a community-building effort to bring together researchers\nand practitioners interested in the broad areas of reinforcement learning (RL)\nand education (ED). This article aims to provide an overview of the workshop\nactivities and summarize the main research directions in the area of RL for ED.",
          "link": "http://arxiv.org/abs/2107.08828",
          "publishedOn": "2021-07-20T02:04:48.308Z",
          "wordCount": 509,
          "title": "Reinforcement Learning for Education: Opportunities and Challenges. (arXiv:2107.08828v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08787",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1\">Yichen Lu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fridlyand_J/0/1/0/all/0/1\">Jane Fridlyand</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tang_T/0/1/0/all/0/1\">Tiffany Tang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Qi_T/0/1/0/all/0/1\">Ting Qi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simon_N/0/1/0/all/0/1\">Noah Simon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Leng_N/0/1/0/all/0/1\">Ning Leng</a>",
          "description": "Finding translational biomarkers stands center stage of the future of\npersonalized medicine in healthcare. We observed notable challenges in\nidentifying robust biomarkers as some with great performance in one scenario\noften fail to perform well in new trials (e.g. different population,\nindications). With rapid development in the clinical trial world (e.g. assay,\ndisease definition), new trials very likely differ from legacy ones in many\nperspectives and in development of biomarkers this heterogeneity should be\nconsidered. In response, we recommend considering building in the heterogeneity\nwhen evaluating biomarkers. In this paper, we present one evaluation strategy\nby using leave-one-study-out (LOSO) in place of conventional cross-validation\n(cv) methods to account for the potential heterogeneity across trials used for\nbuilding and testing the biomarkers. To demonstrate the performance of K-fold\nvs LOSO cv in estimating the effect size of biomarkers, we leveraged data from\nclinical trials and simulation studies. In our assessment, LOSO cv provided a\nmore objective estimate of the future performance. This conclusion remained\ntrue across different evaluation metrics and different statistical methods.",
          "link": "http://arxiv.org/abs/2107.08787",
          "publishedOn": "2021-07-20T02:04:48.291Z",
          "wordCount": 633,
          "title": "The Future will be Different than Today: Model Evaluation Considerations when Developing Translational Clinical Biomarker. (arXiv:2107.08787v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01683",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Azzimonti_L/0/1/0/all/0/1\">Laura Azzimonti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Corani_G/0/1/0/all/0/1\">Giorgio Corani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scutari_M/0/1/0/all/0/1\">Marco Scutari</a>",
          "description": "Score functions for learning the structure of Bayesian networks in the\nliterature assume that data are a homogeneous set of observations; whereas it\nis often the case that they comprise different related, but not homogeneous,\ndata sets collected in different ways. In this paper we propose a new Bayesian\nDirichlet score, which we call Bayesian Hierarchical Dirichlet (BHD). The\nproposed score is based on a hierarchical model that pools information across\ndata sets to learn a single encompassing network structure, while taking into\naccount the differences in their probabilistic structures. We derive a\nclosed-form expression for BHD using a variational approximation of the\nmarginal likelihood, we study the associated computational cost and we evaluate\nits performance using simulated data. We find that, when data comprise multiple\nrelated data sets, BHD outperforms the Bayesian Dirichlet equivalent uniform\n(BDeu) score in terms of reconstruction accuracy as measured by the Structural\nHamming distance, and that it is as accurate as BDeu when data are homogeneous.\nThis improvement is particularly clear when either the number of variables in\nthe network or the number of observations is large. Moreover, the estimated\nnetworks are sparser and therefore more interpretable than those obtained with\nBDeu thanks to a lower number of false positive arcs.",
          "link": "http://arxiv.org/abs/2008.01683",
          "publishedOn": "2021-07-20T02:04:47.806Z",
          "wordCount": 682,
          "title": "A Bayesian Hierarchical Score for Structure Learning from Related Data Sets. (arXiv:2008.01683v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takeuchi_K/0/1/0/all/0/1\">Koh Takeuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishida_R/0/1/0/all/0/1\">Ryo Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1\">Hisashi Kashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onishi_M/0/1/0/all/0/1\">Masaki Onishi</a>",
          "description": "Crowd movement guidance has been a fascinating problem in various fields,\nsuch as easing traffic congestion in unusual events and evacuating people from\nan emergency-affected area. To grab the reins of crowds, there has been\nconsiderable demand for a decision support system that can answer a typical\nquestion: ``what will be the outcomes of each of the possible options in the\ncurrent situation. In this paper, we consider the problem of estimating the\neffects of crowd movement guidance from past data. To cope with limited amount\nof available data biased by past decision-makers, we leverage two recent\ntechniques in deep representation learning for spatial data analysis and causal\ninference. We use a spatial convolutional operator to extract effective spatial\nfeatures of crowds from a small amount of data and use balanced representation\nlearning based on the integral probability metrics to mitigate the selection\nbias and missing counterfactual outcomes. To evaluate the performance on\nestimating the treatment effects of possible guidance, we use a multi-agent\nsimulator to generate realistic data on evacuation scenarios in a crowded\ntheater, since there are no available datasets recording outcomes of all\npossible crowd movement guidance. The results of three experiments demonstrate\nthat our proposed method reduces the estimation error by at most 56% from\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2102.03980",
          "publishedOn": "2021-07-20T02:04:47.788Z",
          "wordCount": 685,
          "title": "Grab the Reins of Crowds: Estimating the Effects of Crowd Movement Guidance Using Causal Inference. (arXiv:2102.03980v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malach_E/0/1/0/all/0/1\">Eran Malach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yehudai_G/0/1/0/all/0/1\">Gilad Yehudai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalev_Shwartz_S/0/1/0/all/0/1\">Shai Shalev-Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1\">Ohad Shamir</a>",
          "description": "Several recent works have shown separation results between deep neural\nnetworks, and hypothesis classes with inferior approximation capacity such as\nshallow networks or kernel classes. On the other hand, the fact that deep\nnetworks can efficiently express a target function does not mean that this\ntarget function can be learned efficiently by deep neural networks. In this\nwork we study the intricate connection between learnability and approximation\ncapacity. We show that learnability with deep networks of a target function\ndepends on the ability of simpler classes to approximate the target.\nSpecifically, we show that a necessary condition for a function to be learnable\nby gradient descent on deep neural networks is to be able to approximate the\nfunction, at least in a weak sense, with shallow neural networks. We also show\nthat a class of functions can be learned by an efficient statistical query\nalgorithm if and only if it can be approximated in a weak sense by some kernel\nclass. We give several examples of functions which demonstrate depth\nseparation, and conclude that they cannot be efficiently learned, even by a\nhypothesis class that can efficiently approximate them.",
          "link": "http://arxiv.org/abs/2102.00434",
          "publishedOn": "2021-07-20T02:04:47.732Z",
          "wordCount": 672,
          "title": "The Connection Between Approximation, Depth Separation and Learnability in Neural Networks. (arXiv:2102.00434v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16336",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Goren_E/0/1/0/all/0/1\">Emily M. Goren</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1\">Ranjan Maitra</a>",
          "description": "Partially recorded data are frequently encountered in many applications and\nusually clustered by first removing incomplete cases or features with missing\nvalues, or by imputing missing values, followed by application of a clustering\nalgorithm to the resulting altered dataset. Here, we develop clustering\nmethodology through a model-based approach using the marginal density for the\nobserved values, assuming a finite mixture model of multivariate $t$\ndistributions. We compare our approximate algorithm to the corresponding full\nexpectation-maximization (EM) approach that considers the missing values in the\nincomplete data set and makes a missing at random (MAR) assumption, as well as\ncase deletion and imputation methods. Since only the observed values are\nutilized, our approach is computationally more efficient than imputation or\nfull EM. Simulation studies demonstrate that our approach has favorable\nrecovery of the true cluster partition compared to case deletion and imputation\nunder various missingness mechanisms, and is at least competitive with the full\nEM approach, even when MAR assumptions are violated. Our methodology is\ndemonstrated on a problem of clustering gamma-ray bursts and is implemented at\nhttps://github.com/emilygoren/MixtClust.",
          "link": "http://arxiv.org/abs/2103.16336",
          "publishedOn": "2021-07-20T02:04:47.479Z",
          "wordCount": null,
          "title": "Model-based clustering of partial records. (arXiv:2103.16336v3 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04046",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ahfock_D/0/1/0/all/0/1\">Daniel Ahfock</a>, <a href=\"http://arxiv.org/find/stat/1/au:+McLachlan_G/0/1/0/all/0/1\">Geoffrey J. McLachlan</a>",
          "description": "There has been increasing attention to semi-supervised learning (SSL)\napproaches in machine learning to forming a classifier in situations where the\ntraining data for a classifier consists of a limited number of classified\nobservations but a much larger number of unclassified observations. This is\nbecause the procurement of classified data can be quite costly due to high\nacquisition costs and subsequent financial, time, and ethical issues that can\narise in attempts to provide the true class labels for the unclassified data\nthat have been acquired. We provide here a review of statistical SSL approaches\nto this problem, focussing on the recent result that a classifier formed from a\npartially classified sample can actually have smaller expected error rate than\nthat if the sample were completely classified.",
          "link": "http://arxiv.org/abs/2104.04046",
          "publishedOn": "2021-07-20T02:04:47.476Z",
          "wordCount": null,
          "title": "Semi-Supervised Learning of Classifiers from a Statistical Perspective: A Brief Review. (arXiv:2104.04046v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haibin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>",
          "description": "Automatic speaker verification (ASV) is a well developed technology for\nbiometric identification, and has been ubiquitous implemented in\nsecurity-critic applications, such as banking and access control. However,\nprevious works have shown that ASV is under the radar of adversarial attacks,\nwhich are very similar to their original counterparts from human's perception,\nyet will manipulate the ASV render wrong prediction. Due to the very late\nemergence of adversarial attacks for ASV, effective countermeasures against\nthem are limited. Given that the security of ASV is of high priority, in this\nwork, we propose the idea of \"voting for the right answer\" to prevent risky\ndecisions of ASV in blind spot areas, by employing random sampling and voting.\nExperimental results show that our proposed method improves the robustness\nagainst both the limited-knowledge attackers by pulling the adversarial samples\nout of the blind spots, and the perfect-knowledge attackers by introducing\nrandomness and increasing the attackers' budgets.",
          "link": "http://arxiv.org/abs/2106.07868",
          "publishedOn": "2021-07-20T02:04:47.463Z",
          "wordCount": null,
          "title": "Voting for the right answer: Adversarial defense for speaker verification. (arXiv:2106.07868v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1\">Wei Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_G/0/1/0/all/0/1\">Guang Tan</a>",
          "description": "We consider graph representation learning in a self-supervised manner. Graph\nneural networks (GNNs) use neighborhood aggregation as a core component that\nresults in feature smoothing among nodes in proximity. While successful in\nvarious prediction tasks, such a paradigm falls short of capturing nodes'\nsimilarities over a long distance, which proves to be important for\nhigh-quality learning. To tackle this problem, we strengthen the graph with two\nadditional graph views, in which nodes are directly linked to those with the\nmost similar features or local structures. Not restricted by connectivity in\nthe original graph, the generated views allow the model to enhance its\nexpressive power with new and complementary perspectives from which to look at\nthe relationship between nodes. Following a contrastive learning approach, we\npropose a method that aims to maximize the agreement between representations\nacross generated views and the original graph. We also propose a channel-level\ncontrast approach that greatly reduces computation cost, compared to the\ncommonly used node level contrast, which requires computation cost quadratic in\nthe number of nodes. Extensive experiments on seven assortative graphs and four\ndisassortative graphs demonstrate the effectiveness of our approach.",
          "link": "http://arxiv.org/abs/2106.03723",
          "publishedOn": "2021-07-20T02:04:47.461Z",
          "wordCount": null,
          "title": "Self-Supervised Graph Learning with Proximity-based Views and Channel Contrast. (arXiv:2106.03723v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moreira_T/0/1/0/all/0/1\">T&#xfa;lio Marcondes Moreira</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Faria_J/0/1/0/all/0/1\">Jackson Geraldo de Faria Jr</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Melo_P/0/1/0/all/0/1\">Pedro O.S. Vaz de Melo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chaimowicz_L/0/1/0/all/0/1\">Luiz Chaimowicz</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Medeiros_Ribeiro_G/0/1/0/all/0/1\">Gilberto Medeiros-Ribeiro</a> (1) ((1) Universidade Federal de Minas Gerais, Belo Horizonte, Brazil)",
          "description": "Tidal range structures have been considered for large scale electricity\ngeneration for their potential ability to produce reasonable predictable energy\nwithout the emission of greenhouse gases. Once the main forcing components for\ndriving the tides have deterministic dynamics, the available energy in a given\ntidal power plant has been estimated, through analytical and numerical\noptimisation routines, as a mostly predictable event. This constraint imposes\nstate-of-art flexible operation methods to rely on tidal predictions\n(concurrent with measured data and up to a multiple of half-tidal cycles into\nthe future) to infer best operational strategies for tidal lagoons, with the\nadditional cost of requiring to run optimisation routines for every new tide.\nIn this paper, we propose a novel optimised operation of tidal lagoons with\nproximal policy optimisation through Unity ML-Agents. We compare this technique\nwith 6 different operation optimisation approaches (baselines) devised from the\nliterature, utilising the Swansea Bay Tidal Lagoon as a case study. We show\nthat our approach is successful in maximising energy generation through an\noptimised operational policy of turbines and sluices, yielding competitive\nresults with state-of-the-art methods of optimisation, regardless of test data\nused, requiring training once and performing real-time flexible control with\nmeasured ocean data only.",
          "link": "http://arxiv.org/abs/2106.10360",
          "publishedOn": "2021-07-20T02:04:47.421Z",
          "wordCount": null,
          "title": "Prediction-Free, Real-Time Flexible Control of Tidal Lagoons through Proximal Policy Optimisation: A Case Study for the Swansea Lagoon. (arXiv:2106.10360v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hafez_Kolahi_H/0/1/0/all/0/1\">Hassan Hafez-Kolahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniri_B/0/1/0/all/0/1\">Behrad Moniri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1\">Shohreh Kasaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baghshah_M/0/1/0/all/0/1\">Mahdieh Soleymani Baghshah</a>",
          "description": "In parametric Bayesian learning, a prior is assumed on the parameter $W$\nwhich determines the distribution of samples. In this setting, Minimum Excess\nRisk (MER) is defined as the difference between the minimum expected loss\nachievable when learning from data and the minimum expected loss that could be\nachieved if $W$ was observed. In this paper, we build upon and extend the\nrecent results of (Xu & Raginsky, 2020) to analyze the MER in Bayesian learning\nand derive information-theoretic bounds on it. We formulate the problem as a\n(constrained) rate-distortion optimization and show how the solution can be\nbounded above and below by two other rate-distortion functions that are easier\nto study. The lower bound represents the minimum possible excess risk\nachievable by any process using $R$ bits of information from the parameter $W$.\nFor the upper bound, the optimization is further constrained to use $R$ bits\nfrom the training set, a setting which relates MER to information-theoretic\nbounds on the generalization gap in frequentist learning. We derive\ninformation-theoretic bounds on the difference between these upper and lower\nbounds and show that they can provide order-wise tight rates for MER under\ncertain conditions. This analysis gives more insight into the\ninformation-theoretic nature of Bayesian learning as well as providing novel\nbounds.",
          "link": "http://arxiv.org/abs/2105.04180",
          "publishedOn": "2021-07-20T02:04:47.413Z",
          "wordCount": null,
          "title": "Rate-Distortion Analysis of Minimum Excess Risk in Bayesian Learning. (arXiv:2105.04180v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05556",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ozcelik_R/0/1/0/all/0/1\">R&#x131;za &#xd6;z&#xe7;elik</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bag_A/0/1/0/all/0/1\">Alperen Ba&#x11f;</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Atil_B/0/1/0/all/0/1\">Berk At&#x131;l</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ozgur_A/0/1/0/all/0/1\">Arzucan &#xd6;zg&#xfc;r</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ozkirimli_E/0/1/0/all/0/1\">Elif &#xd6;zk&#x131;r&#x131;ml&#x131;</a>",
          "description": "Motivation: Computational models that accurately identify high-affinity\nprotein-compound pairs can accelerate drug discovery pipelines. These models\naim to learn binding mechanics through drug-target interaction datasets and use\nthe learned knowledge for predicting the affinity of an input protein-compound\npair. However, the datasets they rely on bear misleading patterns that bias\nmodels towards memorizing dataset-specific biomolecule properties, instead of\nlearning binding mechanics. This results in models that struggle while\npredicting drug-target affinities (DTA), especially between de novo\nbiomolecules. Here we present DebiasedDTA, the first DTA model debiasing\napproach that avoids dataset biases in order to boost affinity prediction for\nnovel biomolecules. DebiasedDTA uses ensemble learning and sample weight\nadaptation for bias identification and avoidance and is applicable to almost\nall existing DTA prediction models. Results: The results show that DebiasedDTA\ncan boost models while predicting the interactions between novel biomolecules.\nKnown biomolecules also benefit from the performance improvement, especially\nwhen the test biomolecules are dissimilar to the training set. The experiments\nalso show that DebiasedDTA can augment DTA prediction models of different input\nand model structures and is able to avoid biases of different sources.\nAvailability and Implementation: The source code, the models, and the datasets\nare freely available for download at\nhttps://github.com/boun-tabi/debiaseddta-reproduce, implementation in Python3,\nand supported for Linux, MacOS and MS Windows. Contact:\narzucan.ozgur@boun.edu.tr, elif.ozkirimli@roche.com",
          "link": "http://arxiv.org/abs/2107.05556",
          "publishedOn": "2021-07-20T02:04:47.411Z",
          "wordCount": null,
          "title": "DebiasedDTA: Model Debiasing to Boost Drug-Target Affinity Prediction. (arXiv:2107.05556v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiaqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1\">Rex Ying</a>",
          "description": "Structural features are important features in graph datasets. However,\nalthough there are some correlation analysis of features based on covariance,\nthere is no relevant research on exploring structural feature correlation on\ngraphs with graph neural network based models. In this paper, we introduce\ngraph feature to feature (Fea2Fea) prediction pipelines in a low dimensional\nspace to explore some preliminary results on structural feature correlation,\nwhich is based on graph neural network. The results show that there exists high\ncorrelation between some of the structural features. A non-redundant feature\ncombination with initial node features, which is filtered by graph neural\nnetwork has improved its classification accuracy in some graph datasets. We\ncompare the difference between concatenation methods on connecting embeddings\nbetween features and show that the simplest is the best. We generalize on the\nsynthetic geometric graphs and certify the results on prediction difficulty\nbetween two structural features.",
          "link": "http://arxiv.org/abs/2106.13061",
          "publishedOn": "2021-07-20T02:04:47.406Z",
          "wordCount": null,
          "title": "Fea2Fea: Exploring Structural Feature Correlations via Graph Neural Networks. (arXiv:2106.13061v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06419",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jinke Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lv_P/0/1/0/all/0/1\">Peiqing Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haiying Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1\">Changfa Shi</a>",
          "description": "Background and objective: In this paper, a modified U-Net based framework is\npresented, which leverages techniques from Squeeze-and-Excitation (SE) block,\nAtrous Spatial Pyramid Pooling (ASPP) and residual learning for accurate and\nrobust liver CT segmentation, and the effectiveness of the proposed method was\ntested on two public datasets LiTS17 and SLiver07.\n\nMethods: A new network architecture called SAR-U-Net was designed. Firstly,\nthe SE block is introduced to adaptively extract image features after each\nconvolution in the U-Net encoder, while suppressing irrelevant regions, and\nhighlighting features of specific segmentation task; Secondly, ASPP was\nemployed to replace the transition layer and the output layer, and acquire\nmulti-scale image information via different receptive fields. Thirdly, to\nalleviate the degradation problem, the traditional convolution block was\nreplaced with the residual block and thus prompt the network to gain accuracy\nfrom considerably increased depth.\n\nResults: In the LiTS17 experiment, the mean values of Dice, VOE, RVD, ASD and\nMSD were 95.71, 9.52, -0.84, 1.54 and 29.14, respectively. Compared with other\nclosely related 2D-based models, the proposed method achieved the highest\naccuracy. In the experiment of the SLiver07, the mean values of Dice, VOE, RVD,\nASD and MSD were 97.31, 5.37, -1.08, 1.85 and 27.45, respectively. Compared\nwith other closely related models, the proposed method achieved the highest\nsegmentation accuracy except for the RVD.\n\nConclusion: The proposed model enables a great improvement on the accuracy\ncompared to 2D-based models, and its robustness in circumvent challenging\nproblems, such as small liver regions, discontinuous liver regions, and fuzzy\nliver boundaries, is also well demonstrated and validated.",
          "link": "http://arxiv.org/abs/2103.06419",
          "publishedOn": "2021-07-20T02:04:47.391Z",
          "wordCount": null,
          "title": "SAR-U-Net: squeeze-and-excitation block and atrous spatial pyramid pooling based residual U-Net for automatic liver segmentation in Computed Tomography. (arXiv:2103.06419v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09022",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chung_H/0/1/0/all/0/1\">Hyungjin Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huh_J/0/1/0/all/0/1\">Jaeyoung Huh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_G/0/1/0/all/0/1\">Geon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1\">Yong Keun Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>",
          "description": "Optical diffraction tomography (ODT) produces three dimensional distribution\nof refractive index (RI) by measuring scattering fields at various angles.\nAlthough the distribution of RI index is highly informative, due to the missing\ncone problem stemming from the limited-angle acquisition of holograms,\nreconstructions have very poor resolution along axial direction compared to the\nhorizontal imaging plane. To solve this issue, here we present a novel\nunsupervised deep learning framework, which learns the probability distribution\nof missing projection views through optimal transport driven cycleGAN.\nExperimental results show that missing cone artifact in ODT can be\nsignificantly resolved by the proposed method.",
          "link": "http://arxiv.org/abs/2103.09022",
          "publishedOn": "2021-07-20T02:04:47.260Z",
          "wordCount": null,
          "title": "Missing Cone Artifacts Removal in ODT using Unsupervised Deep Learning in Projection Domain. (arXiv:2103.09022v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiqi Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1\">Qi Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>",
          "description": "In deep learning with differential privacy (DP), the neural network achieves\nthe privacy usually at the cost of slower convergence (and thus lower\nperformance) than its non-private counterpart. This work gives the first\nconvergence analysis of the DP deep learning, through the lens of training\ndynamics and the neural tangent kernel (NTK). Our convergence theory\nsuccessfully characterizes the effects of two key components in the DP\ntraining: the per-sample clipping (flat or layerwise) and the noise addition.\nOur analysis not only initiates a general principled framework to understand\nthe DP deep learning with any network architecture and loss function, but also\nmotivates a new clipping method -- the global clipping, that significantly\nimproves the convergence while preserving the same privacy guarantee as the\nexisting local clipping.\n\nIn terms of theoretical results, we establish the precise connection between\nthe per-sample clipping and NTK matrix. We show that in the gradient flow,\ni.e., with infinitesimal learning rate, the noise level of DP optimizers does\nnot affect the convergence. We prove that DP gradient descent (GD) with global\nclipping guarantees the monotone convergence to zero loss, which can be\nviolated by the existing DP-GD with local clipping. Notably, our analysis\nframework easily extends to other optimizers, e.g., DP-Adam. Empirically\nspeaking, DP optimizers equipped with global clipping perform strongly on a\nwide range of classification and regression tasks. In particular, our global\nclipping is surprisingly effective at learning calibrated classifiers, in\ncontrast to the existing DP classifiers which are oftentimes over-confident and\nunreliable. Implementation-wise, the new clipping can be realized by adding one\nline of code into the Opacus library.",
          "link": "http://arxiv.org/abs/2106.07830",
          "publishedOn": "2021-07-20T02:04:47.255Z",
          "wordCount": null,
          "title": "On the Convergence of Deep Learning with Differential Privacy. (arXiv:2106.07830v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaolin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zejin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongji Wang</a>",
          "description": "The increasing concerns about data privacy and security drive an emerging\nfield of studying privacy-preserving machine learning from isolated data\nsources, i.e., federated learning. A class of federated learning, vertical\nfederated learning, where different parties hold different features for common\nusers, has a great potential of driving a more variety of business cooperation\namong enterprises in many fields. In machine learning, decision tree ensembles\nsuch as gradient boosting decision tree (GBDT) and random forest are widely\napplied powerful models with high interpretability and modeling efficiency.\nHowever, the interpretability is compromised in state-of-the-art vertical\nfederated learning frameworks such as SecureBoost with anonymous features to\navoid possible data breaches. To address this issue in the inference process,\nin this paper, we propose Fed-EINI to protect data privacy and allow the\ndisclosure of feature meaning by concealing decision paths with a\ncommunication-efficient secure computation method for inference outputs. The\nadvantages of Fed-EINI will be demonstrated through both theoretical analysis\nand extensive numerical results.",
          "link": "http://arxiv.org/abs/2105.09540",
          "publishedOn": "2021-07-20T02:04:47.253Z",
          "wordCount": null,
          "title": "Fed-EINI: An Efficient and Interpretable Inference Framework for Decision Tree Ensembles in Federated Learning. (arXiv:2105.09540v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11793",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Wanjun Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1\">Xiang Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1\">Minghua Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Low_S/0/1/0/all/0/1\">Steven H. Low</a>",
          "description": "AC optimal power flow (AC-OPF) problems need to be solved more frequently in\nthe future to maintain stable and economic power system operation. To tackle\nthis challenge, a deep neural network-based voltage-constrained approach\n(DeepOPF-V) is proposed to solve AC-OPF problems with high computational\nefficiency. Its unique design predicts voltages of all buses and then uses them\nto reconstruct the remaining variables without solving non-linear AC power flow\nequations. A fast post-processing process is developed to enforce the box\nconstraints. The effectiveness of DeepOPF-V is validated by simulations on IEEE\n118/300-bus systems and a 2000-bus test system. Compared with existing studies,\nDeepOPF-V achieves decent computation speedup up to four orders of magnitude\nand comparable performance in optimality gap and preserving the feasibility of\nthe solution.",
          "link": "http://arxiv.org/abs/2103.11793",
          "publishedOn": "2021-07-20T02:04:47.241Z",
          "wordCount": null,
          "title": "DeepOPF-V: Solving AC-OPF Problems Efficiently. (arXiv:2103.11793v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1\">Meng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zuo-Jun Shen</a>",
          "description": "In this work, we propose a deep reinforcement learning (DRL) model for\nfinding a feasible solution for (mixed) integer programming (MIP) problems.\nFinding a feasible solution for MIP problems is critical because many\nsuccessful heuristics rely on a known initial feasible solution. However, it is\nin general NP-hard. Inspired by the feasibility pump (FP), a well-known\nheuristic for searching feasible MIP solutions, we develop a smart feasibility\npump (SFP) method using DRL. In addition to multi-layer perception (MLP), we\npropose a novel convolution neural network (CNN) structure for the policy\nnetwork to capture the hidden information of the constraint matrix of the MIP\nproblem. Numerical experiments on various problem instances show that SFP\nsignificantly outperforms the classic FP in terms of the number of steps\nrequired to reach the first feasible solution. Moreover, the CNN structure\nworks without the projection of the current solution as the input, which saves\nthe computational effort at each step of the FP algorithms to find projections.\nThis highlights the representational power of the CNN structure.",
          "link": "http://arxiv.org/abs/2102.09663",
          "publishedOn": "2021-07-20T02:04:47.225Z",
          "wordCount": null,
          "title": "Smart Feasibility Pump: Reinforcement Learning for (Mixed) Integer Programming. (arXiv:2102.09663v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1\">Aaron M. Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>",
          "description": "We present a novel sensor-based learning navigation algorithm to compute a\ncollision-free trajectory for a robot in dense and dynamic environments with\nmoving obstacles or targets. Our approach uses deep reinforcement\nlearning-based expert policy that is trained using a sim2real paradigm. In\norder to increase the reliability and handle the failure cases of the expert\npolicy, we combine with a policy extraction technique to transform the\nresulting policy into a decision tree format. The resulting decision tree has\nproperties which we use to analyze and modify the policy and improve\nperformance on navigation metrics including smoothness, frequency of\noscillation, frequency of immobilization, and obstruction of target. We are\nable to modify the policy to address these imperfections without retraining,\ncombining the learning power of deep learning with the control of\ndomain-specific algorithms. We highlight the benefits of our algorithm in\nsimulated environments and navigating a Clearpath Jackal robot among moving\npedestrians. (Videos at this url:\nhttps://gamma.umd.edu/researchdirections/xrl/navviper)",
          "link": "http://arxiv.org/abs/2104.10818",
          "publishedOn": "2021-07-20T02:04:47.223Z",
          "wordCount": null,
          "title": "XAI-N: Sensor-based Robot Navigation using Expert Policies and Decision Trees. (arXiv:2104.10818v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xueying Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "Automated surgical gesture recognition is of great importance in\nrobot-assisted minimally invasive surgery. However, existing methods assume\nthat training and testing data are from the same domain, which suffers from\nsevere performance degradation when a domain gap exists, such as the simulator\nand real robot. In this paper, we propose a novel unsupervised domain\nadaptation framework which can simultaneously transfer multi-modality\nknowledge, i.e., both kinematic and visual data, from simulator to real robot.\nIt remedies the domain gap with enhanced transferable features by using\ntemporal cues in videos, and inherent correlations in multi-modal towards\nrecognizing gesture. Specifically, we first propose an MDO-K to align\nkinematics, which exploits temporal continuity to transfer motion directions\nwith smaller gap rather than position values, relieving the adaptation burden.\nMoreover, we propose a KV-Relation-ATT to transfer the co-occurrence signals of\nkinematics and vision. Such features attended by correlation similarity are\nmore informative for enhancing domain-invariance of the model. Two feature\nalignment strategies benefit the model mutually during the end-to-end learning\nprocess. We extensively evaluate our method for gesture recognition using DESK\ndataset with peg transfer procedure. Results show that our approach recovers\nthe performance with great improvement gains, up to 12.91% in ACC and 20.16% in\nF1score without using any annotations in real robot.",
          "link": "http://arxiv.org/abs/2103.04075",
          "publishedOn": "2021-07-20T02:04:47.219Z",
          "wordCount": null,
          "title": "Domain Adaptive Robotic Gesture Recognition with Unsupervised Kinematic-Visual Data Alignment. (arXiv:2103.04075v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Michelle M. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1\">Marinka Zitnik</a>",
          "description": "Spatial context is central to understanding health and disease. Yet reference\nprotein interaction networks lack such contextualization, thereby limiting the\nstudy of where protein interactions likely occur in the human body and how they\nmay be altered in disease. Contextualized protein interactions could better\ncharacterize genes with disease-specific interactions and elucidate diseases'\nmanifestation in specific cell types. Here, we introduce AWARE, a graph neural\nmessage passing approach to inject cellular and tissue context into protein\nembeddings. AWARE optimizes for a multi-scale embedding space, whose structure\nreflects network topology at a single-cell resolution. We construct a\nmulti-scale network of the Human Cell Atlas and apply AWARE to learn protein,\ncell type, and tissue embeddings that uphold cell type and tissue hierarchies.\nWe demonstrate AWARE's utility on the novel task of predicting whether a\nprotein is altered in disease and where that association most likely manifests\nin the human body. To this end, AWARE outperforms generic embeddings without\ncontextual information by at least 12.5%, showing AWARE's potential to reveal\ncontext-dependent roles of proteins in disease.",
          "link": "http://arxiv.org/abs/2106.02246",
          "publishedOn": "2021-07-20T02:04:47.216Z",
          "wordCount": null,
          "title": "Deep Contextual Learners for Protein Networks. (arXiv:2106.02246v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvirul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1\">Muhammad Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1\">Ferda Ofli</a>",
          "description": "Images shared on social media help crisis managers gain situational awareness\nand assess incurred damages, among other response tasks. As the volume and\nvelocity of such content are typically high, real-time image classification has\nbecome an urgent need for a faster disaster response. Recent advances in\ncomputer vision and deep neural networks have enabled the development of models\nfor real-time image classification for a number of tasks, including detecting\ncrisis incidents, filtering irrelevant images, classifying images into specific\nhumanitarian categories, and assessing the severity of the damage. To develop\nrobust real-time models, it is necessary to understand the capability of the\npublicly available pre-trained models for these tasks, which remains to be\nunder-explored in the crisis informatics literature. In this study, we address\nsuch limitations by investigating ten different network architectures for four\ndifferent tasks using the largest publicly available datasets for these tasks.\nWe also explore various data augmentation strategies, semi-supervised\ntechniques, and a multitask learning setup. In our extensive experiments, we\nachieve promising results.",
          "link": "http://arxiv.org/abs/2104.04184",
          "publishedOn": "2021-07-20T02:04:47.190Z",
          "wordCount": null,
          "title": "Robust Training of Social Media Image Classification Models for Rapid Disaster Response. (arXiv:2104.04184v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zuowei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shijun Zhang</a>",
          "description": "This paper concentrates on the approximation power of deep feed-forward\nneural networks in terms of width and depth. It is proved by construction that\nReLU networks with width $\\mathcal{O}\\big(\\max\\{d\\lfloor N^{1/d}\\rfloor,\\,\nN+2\\}\\big)$ and depth $\\mathcal{O}(L)$ can approximate a H\\\"older continuous\nfunction on $[0,1]^d$ with an approximation rate\n$\\mathcal{O}\\big(\\lambda\\sqrt{d} (N^2L^2\\ln N)^{-\\alpha/d}\\big)$, where\n$\\alpha\\in (0,1]$ and $\\lambda>0$ are H\\\"older order and constant,\nrespectively. Such a rate is optimal up to a constant in terms of width and\ndepth separately, while existing results are only nearly optimal without the\nlogarithmic factor in the approximation rate. More generally, for an arbitrary\ncontinuous function $f$ on $[0,1]^d$, the approximation rate becomes\n$\\mathcal{O}\\big(\\,\\sqrt{d}\\,\\omega_f\\big( (N^2L^2\\ln N)^{-1/d}\\big)\\,\\big)$,\nwhere $\\omega_f(\\cdot)$ is the modulus of continuity. We also extend our\nanalysis to any continuous function $f$ on a bounded set. Particularly, if ReLU\nnetworks with depth $31$ and width $\\mathcal{O}(N)$ are used to approximate\none-dimensional Lipschitz continuous functions on $[0,1]$ with a Lipschitz\nconstant $\\lambda>0$, the approximation rate in terms of the total number of\nparameters, $W=\\mathcal{O}(N^2)$, becomes $\\mathcal{O}(\\tfrac{\\lambda}{W\\ln\nW})$, which has not been discovered in the literature for fixed-depth ReLU\nnetworks.",
          "link": "http://arxiv.org/abs/2103.00502",
          "publishedOn": "2021-07-20T02:04:47.189Z",
          "wordCount": null,
          "title": "Optimal Approximation Rate of ReLU Networks in terms of Width and Depth. (arXiv:2103.00502v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12534",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Kim_J/0/1/0/all/0/1\">Joonho Kim</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Oz_Y/0/1/0/all/0/1\">Yaron Oz</a>",
          "description": "We consider information spreading measures in randomly initialized\nvariational quantum circuits and introduce entanglement diagnostics for\nefficient variational quantum/classical computations. We establish a robust\nconnection between entanglement measures and optimization accuracy by solving\ntwo eigensolver problems for Ising Hamiltonians with nearest-neighbor and\nlong-range spin interactions. As the circuit depth affects the average\nentanglement of random circuit states, the entanglement diagnostics can\nidentify a high-performing depth range for optimization tasks encoded in local\nHamiltonians. We argue, based on an eigensolver problem for the\nSachdev-Ye-Kitaev model, that entanglement alone is insufficient as a\ndiagnostic to the approximation of volume-law entangled target states and that\na large number of circuit parameters is needed for such an optimization task.",
          "link": "http://arxiv.org/abs/2102.12534",
          "publishedOn": "2021-07-20T02:04:47.181Z",
          "wordCount": null,
          "title": "Entanglement Diagnostics for Efficient Quantum Computation. (arXiv:2102.12534v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>",
          "description": "Generalization to out-of-distribution (OOD) data is a capability natural to\nhumans yet challenging for machines to reproduce. This is because most learning\nalgorithms strongly rely on the i.i.d.~assumption on source/target data, which\nis often violated in practice due to domain shift. Domain generalization (DG)\naims to achieve OOD generalization by using only source data for model\nlearning. Since first introduced in 2011, research in DG has made great\nprogresses. In particular, intensive research in this topic has led to a broad\nspectrum of methodologies, e.g., those based on domain alignment,\nmeta-learning, data augmentation, or ensemble learning, just to name a few; and\nhas covered various vision applications such as object recognition,\nsegmentation, action recognition, and person re-identification. In this paper,\nfor the first time a comprehensive literature review is provided to summarize\nthe developments in DG for computer vision over the past decade. Specifically,\nwe first cover the background by formally defining DG and relating it to other\nresearch fields like domain adaptation and transfer learning. Second, we\nconduct a thorough review into existing methods and present a categorization\nbased on their methodologies and motivations. Finally, we conclude this survey\nwith insights and discussions on future research directions.",
          "link": "http://arxiv.org/abs/2103.02503",
          "publishedOn": "2021-07-20T02:04:47.181Z",
          "wordCount": null,
          "title": "Domain Generalization in Vision: A Survey. (arXiv:2103.02503v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.05461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mello_R/0/1/0/all/0/1\">Rodrigo Fernandes de Mello</a>",
          "description": "The Statistical Learning Theory (SLT) provides the foundation to ensure that\na supervised algorithm generalizes the mapping $f: \\mathcal{X} \\to \\mathcal{Y}$\ngiven $f$ is selected from its search space bias $\\mathcal{F}$. SLT depends on\nthe Shattering coefficient function $\\mathcal{N}(\\mathcal{F},n)$ to upper bound\nthe empirical risk minimization principle, from which one can estimate the\nnecessary training sample size to ensure the probabilistic learning convergence\nand, most importantly, the characterization of the capacity of $\\mathcal{F}$,\nincluding its underfitting and overfitting abilities while addressing specific\ntarget problems. However, the analytical solution of the Shattering coefficient\nis still an open problem since the first studies by Vapnik and Chervonenkis in\n$1962$, which we address on specific datasets, in this paper, by employing\nequivalence relations from Topology, data separability results by Har-Peled and\nJones, and combinatorics. Our approach computes the Shattering coefficient for\nboth binary and multi-class datasets, leading to the following additional\ncontributions: (i) the estimation of the required number of hyperplanes in the\nworst and best-case classification scenarios and the respective $\\Omega$ and\n$O$ complexities; (ii) the estimation of the training sample sizes required to\nensure supervised learning; and (iii) the comparison of dataset embeddings,\nonce they (re)organize samples into some new space configuration. All results\nintroduced and discussed along this paper are supported by the R package\nshattering (https://cran.r-project.org/web/packages/shattering).",
          "link": "http://arxiv.org/abs/1911.05461",
          "publishedOn": "2021-07-20T02:04:47.162Z",
          "wordCount": null,
          "title": "On the Complexity of Labeled Datasets. (arXiv:1911.05461v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Paniagua_V/0/1/0/all/0/1\">V&#xed;ctor Su&#xe1;rez-Paniagua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteley_W/0/1/0/all/0/1\">William Whiteley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>",
          "description": "Diagnostic or procedural coding of clinical notes aims to derive a coded\nsummary of disease-related information about patients. Such coding is usually\ndone manually in hospitals but could potentially be automated to improve the\nefficiency and accuracy of medical coding. Recent studies on deep learning for\nautomated medical coding achieved promising performances. However, the\nexplainability of these models is usually poor, preventing them to be used\nconfidently in supporting clinical practice. Another limitation is that these\nmodels mostly assume independence among labels, ignoring the complex\ncorrelation among medical codes which can potentially be exploited to improve\nthe performance. We propose a Hierarchical Label-wise Attention Network (HLAN),\nwhich aimed to interpret the model by quantifying importance (as attention\nweights) of words and sentences related to each of the labels. Secondly, we\npropose to enhance the major deep learning models with a label embedding (LE)\ninitialisation approach, which learns a dense, continuous vector representation\nand then injects the representation into the final layers and the label-wise\nattention layers in the models. We evaluated the methods using three settings\non the MIMIC-III discharge summaries: full codes, top-50 codes, and the UK NHS\nCOVID-19 shielding codes. Experiments were conducted to compare HLAN and LE\ninitialisation to the state-of-the-art neural network based methods. HLAN\nachieved the best Micro-level AUC and $F_1$ on the top-50 code prediction and\ncomparable results on the NHS COVID-19 shielding code prediction to other\nmodels. By highlighting the most salient words and sentences for each label,\nHLAN showed more meaningful and comprehensive model interpretation compared to\nits downgraded baselines and the CNN-based models. LE initialisation\nconsistently boosted most deep learning models for automated medical coding.",
          "link": "http://arxiv.org/abs/2010.15728",
          "publishedOn": "2021-07-20T02:04:47.162Z",
          "wordCount": null,
          "title": "Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation. (arXiv:2010.15728v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.07545",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wadia_N/0/1/0/all/0/1\">Neha S. Wadia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duckworth_D/0/1/0/all/0/1\">Daniel Duckworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoenholz_S/0/1/0/all/0/1\">Samuel S. Schoenholz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_E/0/1/0/all/0/1\">Ethan Dyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1\">Jascha Sohl-Dickstein</a>",
          "description": "Machine learning is predicated on the concept of generalization: a model\nachieving low error on a sufficiently large training set should also perform\nwell on novel samples from the same distribution. We show that both data\nwhitening and second order optimization can harm or entirely prevent\ngeneralization. In general, model training harnesses information contained in\nthe sample-sample second moment matrix of a dataset. For a general class of\nmodels, namely models with a fully connected first layer, we prove that the\ninformation contained in this matrix is the only information which can be used\nto generalize. Models trained using whitened data, or with certain second order\noptimization schemes, have less access to this information, resulting in\nreduced or nonexistent generalization ability. We experimentally verify these\npredictions for several architectures, and further demonstrate that\ngeneralization continues to be harmed even when theoretical requirements are\nrelaxed. However, we also show experimentally that regularized second order\noptimization can provide a practical tradeoff, where training is accelerated\nbut less information is lost, and generalization can in some circumstances even\nimprove.",
          "link": "http://arxiv.org/abs/2008.07545",
          "publishedOn": "2021-07-20T02:04:47.161Z",
          "wordCount": null,
          "title": "Whitening and second order optimization both make information in the dataset unusable during training, and can reduce or prevent generalization. (arXiv:2008.07545v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1\">C&#xe9;dric Colas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karch_T/0/1/0/all/0/1\">Tristan Karch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigaud_O/0/1/0/all/0/1\">Olivier Sigaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>",
          "description": "Building autonomous machines that can explore open-ended environments,\ndiscover possible interactions and autonomously build repertoires of skills is\na general objective of artificial intelligence. Developmental approaches argue\nthat this can only be achieved by autonomous and intrinsically motivated\nlearning agents that can generate, select and learn to solve their own\nproblems. In recent years, we have seen a convergence of developmental\napproaches, and developmental robotics in particular, with deep reinforcement\nlearning (RL) methods, forming the new domain of developmental machine\nlearning. Within this new domain, we review here a set of methods where deep RL\nalgorithms are trained to tackle the developmental robotics problem of the\nautonomous acquisition of open-ended repertoires of skills. Intrinsically\nmotivated goal-conditioned RL algorithms train agents to learn to represent,\ngenerate and pursue their own goals. The self-generation of goals requires the\nlearning of compact goal encodings as well as their associated goal-achievement\nfunctions, which results in new challenges compared to traditional RL\nalgorithms designed to tackle pre-defined sets of goals using external reward\nsignals. This paper proposes a typology of these methods at the intersection of\ndeep RL and developmental approaches, surveys recent approaches and discusses\nfuture avenues.",
          "link": "http://arxiv.org/abs/2012.09830",
          "publishedOn": "2021-07-20T02:04:47.161Z",
          "wordCount": null,
          "title": "Intrinsically Motivated Goal-Conditioned Reinforcement Learning: a Short Survey. (arXiv:2012.09830v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rojas_J/0/1/0/all/0/1\">Junior Rojas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifakis_E/0/1/0/all/0/1\">Eftychios Sifakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavan_L/0/1/0/all/0/1\">Ladislav Kavan</a>",
          "description": "We present a differentiable soft-body physics simulator that can be composed\nwith neural networks as a differentiable layer. In contrast to other\ndifferentiable physics approaches that use explicit forward models to define\nstate transitions, we focus on implicit state transitions defined via function\nminimization. Implicit state transitions appear in implicit numerical\nintegration methods, which offer the benefits of large time steps and excellent\nnumerical stability, but require a special treatment to achieve\ndifferentiability due to the absence of an explicit differentiable forward\npass. In contrast to other implicit differentiation approaches that require\nexplicit formulas for the force function and the force Jacobian matrix, we\npresent an energy-based approach that allows us to compute these derivatives\nautomatically and in a matrix-free fashion via reverse-mode automatic\ndifferentiation. This allows for more flexibility and productivity when\ndefining physical models and is particularly important in the context of neural\nnetwork training, which often relies on reverse-mode automatic differentiation\n(backpropagation). We demonstrate the effectiveness of our differentiable\nsimulator in policy optimization for locomotion tasks and show that it achieves\nbetter sample efficiency than model-free reinforcement learning.",
          "link": "http://arxiv.org/abs/2102.05791",
          "publishedOn": "2021-07-20T02:04:47.160Z",
          "wordCount": null,
          "title": "Differentiable Implicit Soft-Body Physics. (arXiv:2102.05791v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08925",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Martin_C/0/1/0/all/0/1\">Christoph Martin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sharafi_N/0/1/0/all/0/1\">Nahal Sharafi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hallerberg_S/0/1/0/all/0/1\">Sarah Hallerberg</a>",
          "description": "Covariant Lyapunov vectors (CLVs) characterize the directions along which\nperturbations in dynamical systems grow. They have also been studied as\npotential predictors of critical transitions and extreme events. For many\napplications, it is, however, necessary to estimate the vectors from data since\nmodel equations are unknown for many interesting phenomena. We propose a novel\nmethod for estimating CLVs based on data records without knowing the underlying\nequations of the system which is suitable also for high-dimensional data and\ncomputationally inexpensive. We demonstrate that this purely data-driven\napproach can accurately estimate CLVs from data records generated by chaotic\ndynamical systems of dimension 128 and multiple lower-dimensional systems and\nthus provides the foundation for numerous future applications in data-analysis\nand data-based predictions.",
          "link": "http://arxiv.org/abs/2107.08925",
          "publishedOn": "2021-07-20T02:04:47.152Z",
          "wordCount": null,
          "title": "Estimating covariant Lyapunov vectors from data. (arXiv:2107.08925v1 [physics.data-an])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1\">Kyeongbo Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyunghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Woo-Jin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Suk-Ju Kang</a>",
          "description": "Conditional generative adversarial networks (cGANs) have demonstrated\nremarkable success due to their class-wise controllability and superior quality\nfor complex generation tasks. Typical cGANs solve the joint distribution\nmatching problem by decomposing two easier sub-problems: marginal matching and\nconditional matching. From our toy experiments, we found that it is the best to\napply only conditional matching to certain samples due to the content-aware\noptimization of the discriminator. This paper proposes a simple (a few lines of\ncode) but effective training methodology, selective focusing learning, which\nenforces the discriminator and generator to learn easy samples of each class\nrapidly while maintaining diversity. Our key idea is to selectively apply\nconditional and joint matching for the data in each mini-batch. We conducted\nexperiments on recent cGAN variants in ImageNet (64x64 and 128x128), CIFAR-10,\nand CIFAR-100 datasets, and improved the performance significantly (up to\n35.18% in terms of FID) without sacrificing diversity.",
          "link": "http://arxiv.org/abs/2107.08792",
          "publishedOn": "2021-07-20T02:04:47.039Z",
          "wordCount": null,
          "title": "Selective Focusing Learning in Conditional GANs. (arXiv:2107.08792v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miura_T/0/1/0/all/0/1\">Takayuki Miura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_S/0/1/0/all/0/1\">Satoshi Hasegawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shibahara_T/0/1/0/all/0/1\">Toshiki Shibahara</a>",
          "description": "The advance of explainable artificial intelligence, which provides reasons\nfor its predictions, is expected to accelerate the use of deep neural networks\nin the real world like Machine Learning as a Service (MLaaS) that returns\npredictions on queried data with the trained model. Deep neural networks\ndeployed in MLaaS face the threat of model extraction attacks. A model\nextraction attack is an attack to violate intellectual property and privacy in\nwhich an adversary steals trained models in a cloud using only their\npredictions. In particular, a data-free model extraction attack has been\nproposed recently and is more critical. In this attack, an adversary uses a\ngenerative model instead of preparing input data. The feasibility of this\nattack, however, needs to be studied since it requires more queries than that\nwith surrogate datasets. In this paper, we propose MEGEX, a data-free model\nextraction attack against a gradient-based explainable AI. In this method, an\nadversary uses the explanations to train the generative model and reduces the\nnumber of queries to steal the model. Our experiments show that our proposed\nmethod reconstructs high-accuracy models -- 0.97$\\times$ and 0.98$\\times$ the\nvictim model accuracy on SVHN and CIFAR-10 datasets given 2M and 20M queries,\nrespectively. This implies that there is a trade-off between the\ninterpretability of models and the difficulty of stealing them.",
          "link": "http://arxiv.org/abs/2107.08909",
          "publishedOn": "2021-07-20T02:04:47.019Z",
          "wordCount": null,
          "title": "MEGEX: Data-Free Model Extraction Attack against Gradient-Based Explainable AI. (arXiv:2107.08909v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1\">Petar Veli&#x10d;kovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosnjak_M/0/1/0/all/0/1\">Matko Bo&#x161;njak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kipf_T/0/1/0/all/0/1\">Thomas Kipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerchner_A/0/1/0/all/0/1\">Alexander Lerchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadsell_R/0/1/0/all/0/1\">Raia Hadsell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1\">Charles Blundell</a>",
          "description": "Neural networks leverage robust internal representations in order to\ngeneralise. Learning them is difficult, and often requires a large training set\nthat covers the data distribution densely. We study a common setting where our\ntask is not purely opaque. Indeed, very often we may have access to information\nabout the underlying system (e.g. that observations must obey certain laws of\nphysics) that any \"tabula rasa\" neural network would need to re-learn from\nscratch, penalising data efficiency. We incorporate this information into a\npre-trained reasoning module, and investigate its role in shaping the\ndiscovered representations in diverse self-supervised learning settings from\npixels. Our approach paves the way for a new class of data-efficient\nrepresentation learning.",
          "link": "http://arxiv.org/abs/2107.08881",
          "publishedOn": "2021-07-20T02:04:47.018Z",
          "wordCount": null,
          "title": "Reasoning-Modulated Representations. (arXiv:2107.08881v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1\">Jakub M. Tomczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eiben_A/0/1/0/all/0/1\">Agoston E. Eiben</a>",
          "description": "When controllers (brains) and morphologies (bodies) of robots simultaneously\nevolve, this can lead to a problem, namely the brain & body mismatch problem.\nIn this research, we propose a solution of lifetime learning. We set up a\nsystem where modular robots can create offspring that inherit the bodies of\nparents by recombination and mutation. With regards to the brains of the\noffspring, we use two methods to create them. The first one entails solely\nevolution which means the brain of a robot child is inherited from its parents.\nThe second approach is evolution plus learning which means the brain of a child\nis inherited as well, but additionally is developed by a learning algorithm -\nRevDEknn. We compare these two methods by running experiments in a simulator\ncalled Revolve and use efficiency, efficacy, and the morphology intelligence of\nthe robots for the comparison. The experiments show that the evolution plus\nlearning method does not only lead to a higher fitness level, but also to more\nmorphologically evolving robots. This constitutes a quantitative demonstration\nthat changes in the brain can induce changes in the body, leading to the\nconcept of morphological intelligence, which is quantified by the learning\ndelta, meaning the ability of a morphology to facilitate learning.",
          "link": "http://arxiv.org/abs/2107.08249",
          "publishedOn": "2021-07-20T02:04:46.996Z",
          "wordCount": null,
          "title": "The Effects of Learning in Morphologically Evolving Robot Systems. (arXiv:2107.08249v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guoliang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Ting Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jin Song Dong</a>",
          "description": "Although deep learning has demonstrated astonishing performance in many\napplications, there are still concerns on their dependability. One desirable\nproperty of deep learning applications with societal impact is fairness (i.e.,\nnon-discrimination). Unfortunately, discrimination might be intrinsically\nembedded into the models due to discrimination in the training data. As a\ncountermeasure, fairness testing systemically identifies discriminative\nsamples, which can be used to retrain the model and improve its fairness.\nExisting fairness testing approaches however have two major limitations. First,\nthey only work well on traditional machine learning models and have poor\nperformance (e.g., effectiveness and efficiency) on deep learning models.\nSecond, they only work on simple tabular data and are not applicable for\ndomains such as text. In this work, we bridge the gap by proposing a scalable\nand effective approach for systematically searching for discriminative samples\nwhile extending fairness testing to address a challenging domain, i.e., text\nclassification. Compared with state-of-the-art methods, our approach only\nemploys lightweight procedures like gradient computation and clustering, which\nmakes it significantly more scalable. Experimental results show that on\naverage, our approach explores the search space more effectively (9.62 and 2.38\ntimes more than the state-of-art methods respectively on tabular and text\ndatasets) and generates much more individual discriminatory instances (24.95\nand 2.68 times) within reasonable time. The retrained models reduce\ndiscrimination by 57.2% and 60.2% respectively on average.",
          "link": "http://arxiv.org/abs/2107.08176",
          "publishedOn": "2021-07-20T02:04:46.989Z",
          "wordCount": null,
          "title": "Automatic Fairness Testing of Neural Classifiers through Adversarial Sampling. (arXiv:2107.08176v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schott_L/0/1/0/all/0/1\">Lukas Schott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1\">Frederik Tr&#xe4;uble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1\">Peter Gehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1\">Chris Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>",
          "description": "An important component for generalization in machine learning is to uncover\nunderlying latent factors of variation as well as the mechanism through which\neach factor acts in the world. In this paper, we test whether 17 unsupervised,\nweakly supervised, and fully supervised representation learning approaches\ncorrectly infer the generative factors of variation in simple datasets\n(dSprites, Shapes3D, MPI3D). In contrast to prior robustness work that\nintroduces novel factors of variation during test time, such as blur or other\n(un)structured noise, we here recompose, interpolate, or extrapolate only\nexisting factors of variation from the training data set (e.g., small and\nmedium-sized objects during training and large objects during testing). Models\nthat learn the correct mechanism should be able to generalize to this\nbenchmark. In total, we train and test 2000+ models and observe that all of\nthem struggle to learn the underlying mechanism regardless of supervision\nsignal and architectural bias. Moreover, the generalization capabilities of all\ntested models drop significantly as we move from artificial datasets towards\nmore realistic real-world datasets. Despite their inability to identify the\ncorrect mechanism, the models are quite modular as their ability to infer other\nin-distribution factors remains fairly stable, providing only a single factor\nis out-of-distribution. These results point to an important yet understudied\nproblem of learning mechanistic models of observations that can facilitate\ngeneralization.",
          "link": "http://arxiv.org/abs/2107.08221",
          "publishedOn": "2021-07-20T02:04:46.986Z",
          "wordCount": null,
          "title": "Visual Representation Learning Does Not Generalize Strongly Within the Same Domain. (arXiv:2107.08221v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08225",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Muehlebach_M/0/1/0/all/0/1\">Michael Muehlebach</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "We introduce a class of first-order methods for smooth constrained\noptimization that are based on an analogy to non-smooth dynamical systems. Two\ndistinctive features of our approach are that (i) projections or optimizations\nover the entire feasible set are avoided, in stark contrast to projected\ngradient methods or the Frank-Wolfe method, and (ii) iterates are allowed to\nbecome infeasible, which differs from active set or feasible direction methods,\nwhere the descent motion stops as soon as a new constraint is encountered. The\nresulting algorithmic procedure is simple to implement even when constraints\nare nonlinear, and is suitable for large-scale constrained optimization\nproblems in which the feasible set fails to have a simple structure. The key\nunderlying idea is that constraints are expressed in terms of velocities\ninstead of positions, which has the algorithmic consequence that optimizations\nover feasible sets at each iteration are replaced with optimizations over\nlocal, sparse convex approximations. The result is a simplified suite of\nalgorithms and an expanded range of possible applications in machine learning.",
          "link": "http://arxiv.org/abs/2107.08225",
          "publishedOn": "2021-07-20T02:04:46.985Z",
          "wordCount": null,
          "title": "On Constraints in First-Order Optimization: A View from Non-Smooth Dynamical Systems. (arXiv:2107.08225v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Geun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Carole-Jean Wu</a>",
          "description": "Federated learning enables a cluster of decentralized mobile devices at the\nedge to collaboratively train a shared machine learning model, while keeping\nall the raw training samples on device. This decentralized training approach is\ndemonstrated as a practical solution to mitigate the risk of privacy leakage.\nHowever, enabling efficient FL deployment at the edge is challenging because of\nnon-IID training data distribution, wide system heterogeneity and\nstochastic-varying runtime effects in the field. This paper jointly optimizes\ntime-to-convergence and energy efficiency of state-of-the-art FL use cases by\ntaking into account the stochastic nature of edge execution. We propose AutoFL\nby tailor-designing a reinforcement learning algorithm that learns and\ndetermines which K participant devices and per-device execution targets for\neach FL model aggregation round in the presence of stochastic runtime variance,\nsystem and data heterogeneity. By considering the unique characteristics of FL\nedge deployment judiciously, AutoFL achieves 3.6 times faster model convergence\ntime and 4.7 and 5.2 times higher energy efficiency for local clients and\nglobally over the cluster of K participants, respectively.",
          "link": "http://arxiv.org/abs/2107.08147",
          "publishedOn": "2021-07-20T02:04:46.984Z",
          "wordCount": null,
          "title": "AutoFL: Enabling Heterogeneity-Aware Energy Efficient Federated Learning. (arXiv:2107.08147v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Avati_A/0/1/0/all/0/1\">Anand Avati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seneviratne_M/0/1/0/all/0/1\">Martin Seneviratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_E/0/1/0/all/0/1\">Emily Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>",
          "description": "Machine learning has recently demonstrated impressive progress in predictive\naccuracy across a wide array of tasks. Most ML approaches focus on\ngeneralization performance on unseen data that are similar to the training data\n(In-Distribution, or IND). However, real world applications and deployments of\nML rarely enjoy the comfort of encountering examples that are always IND. In\nsuch situations, most ML models commonly display erratic behavior on\nOut-of-Distribution (OOD) examples, such as assigning high confidence to wrong\npredictions, or vice-versa. Implications of such unusual model behavior are\nfurther exacerbated in the healthcare setting, where patient health can\npotentially be put at risk. It is crucial to study the behavior and robustness\nproperties of models under distributional shift, understand common failure\nmodes, and take mitigation steps before the model is deployed. Having a\nbenchmark that shines light upon these aspects of a model is a first and\nnecessary step in addressing the issue. Recent work and interest in increasing\nmodel robustness in OOD settings have focused more on image modality, while the\nElectronic Health Record (EHR) modality is still largely under-explored. We aim\nto bridge this gap by releasing BEDS-Bench, a benchmark for quantifying the\nbehavior of ML models over EHR data under OOD settings. We use two open access,\nde-identified EHR datasets to construct several OOD data settings to run tests\non, and measure relevant metrics that characterize crucial aspects of a model's\nOOD behavior. We evaluate several learning algorithms under BEDS-Bench and find\nthat all of them show poor generalization performance under distributional\nshift in general. Our results highlight the need and the potential to improve\nrobustness of EHR models under distributional shift, and BEDS-Bench provides\none way to measure progress towards that goal.",
          "link": "http://arxiv.org/abs/2107.08189",
          "publishedOn": "2021-07-20T02:04:46.984Z",
          "wordCount": null,
          "title": "BEDS-Bench: Behavior of EHR-models under Distributional Shift--A Benchmark. (arXiv:2107.08189v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korb_K/0/1/0/all/0/1\">Kevin B Korb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allison_L/0/1/0/all/0/1\">Lloyd Allison</a>",
          "description": "Causal discovery automates the learning of causal Bayesian networks from data\nand has been of active interest from their beginning. With the sourcing of\nlarge data sets off the internet, interest in scaling up to very large data\nsets has grown. One approach to this is to parallelize search using Markov\nBlanket (MB) discovery as a first step, followed by a process of combining MBs\nin a global causal model. We develop and explore three new methods of MB\ndiscovery using Minimum Message Length (MML) and compare them empirically to\nthe best existing methods, whether developed specifically as MB discovery or as\nfeature selection. Our best MML method is consistently competitive and has some\nadvantageous features.",
          "link": "http://arxiv.org/abs/2107.08140",
          "publishedOn": "2021-07-20T02:04:46.983Z",
          "wordCount": null,
          "title": "Markov Blanket Discovery using Minimum Message Length. (arXiv:2107.08140v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petrenko_A/0/1/0/all/0/1\">Aleksei Petrenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijmans_E/0/1/0/all/0/1\">Erik Wijmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shacklett_B/0/1/0/all/0/1\">Brennan Shacklett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>",
          "description": "We present Megaverse, a new 3D simulation platform for reinforcement learning\nand embodied AI research. The efficient design of our engine enables\nphysics-based simulation with high-dimensional egocentric observations at more\nthan 1,000,000 actions per second on a single 8-GPU node. Megaverse is up to\n70x faster than DeepMind Lab in fully-shaded 3D scenes with interactive\nobjects. We achieve this high simulation performance by leveraging batched\nsimulation, thereby taking full advantage of the massive parallelism of modern\nGPUs. We use Megaverse to build a new benchmark that consists of several\nsingle-agent and multi-agent tasks covering a variety of cognitive challenges.\nWe evaluate model-free RL on this benchmark to provide baselines and facilitate\nfuture research. The source code is available at https://www.megaverse.info",
          "link": "http://arxiv.org/abs/2107.08170",
          "publishedOn": "2021-07-20T02:04:46.983Z",
          "wordCount": null,
          "title": "Megaverse: Simulating Embodied Agents at One Million Experiences per Second. (arXiv:2107.08170v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Molino_P/0/1/0/all/0/1\">Piero Molino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>",
          "description": "In the last years machine learning (ML) has moved from a academic endeavor to\na pervasive technology adopted in almost every aspect of computing. ML-powered\nproducts are now embedded in our digital lives: from recommendations of what to\nwatch, to divining our search intent, to powering virtual assistants in\nconsumer and enterprise settings. Recent successes in applying ML in natural\nsciences revealed that ML can be used to tackle some of the hardest real-world\nproblems humanity faces today. For these reasons ML has become central in the\nstrategy of tech companies and has gathered even more attention from academia\nthan ever before. Despite these successes, what we have witnessed so far is\njust the beginning. Right now the people training and using ML models are\nexpert developers working within large organizations, but we believe the next\nwave of ML systems will allow a larger amount of people, potentially without\ncoding skills, to perform the same tasks. These new ML systems will not require\nusers to fully understand all the details of how models are trained and\nutilized for obtaining predictions. Declarative interfaces are well suited for\nthis goal, by hiding complexity and favouring separation of interests, and can\nlead to increased productivity. We worked on such abstract interfaces by\ndeveloping two declarative ML systems, Overton and Ludwig, that require users\nto declare only their data schema (names and types of inputs) and tasks rather\nthen writing low level ML code. In this article we will describe how ML systems\nare currently structured, highlight important factors for their success and\nadoption, what are the issues current ML systems are facing and how the systems\nwe developed addressed them. Finally we will talk about learnings from the\ndevelopment of ML systems throughout the years and how we believe the next\ngeneration of ML systems will look like.",
          "link": "http://arxiv.org/abs/2107.08148",
          "publishedOn": "2021-07-20T02:04:46.981Z",
          "wordCount": null,
          "title": "Declarative Machine Learning Systems. (arXiv:2107.08148v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Girgis_A/0/1/0/all/0/1\">Antonious M. Girgis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Data_D/0/1/0/all/0/1\">Deepesh Data</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diggavi_S/0/1/0/all/0/1\">Suhas Diggavi</a>",
          "description": "We study privacy in a distributed learning framework, where clients\ncollaboratively build a learning model iteratively through interactions with a\nserver from whom we need privacy. Motivated by stochastic optimization and the\nfederated learning (FL) paradigm, we focus on the case where a small fraction\nof data samples are randomly sub-sampled in each round to participate in the\nlearning process, which also enables privacy amplification. To obtain even\nstronger local privacy guarantees, we study this in the shuffle privacy model,\nwhere each client randomizes its response using a local differentially private\n(LDP) mechanism and the server only receives a random permutation (shuffle) of\nthe clients' responses without their association to each client. The principal\nresult of this paper is a privacy-optimization performance trade-off for\ndiscrete randomization mechanisms in this sub-sampled shuffle privacy model.\nThis is enabled through a new theoretical technique to analyze the Renyi\nDifferential Privacy (RDP) of the sub-sampled shuffle model. We numerically\ndemonstrate that, for important regimes, with composition our bound yields\nsignificant improvement in privacy guarantee over the state-of-the-art\napproximate Differential Privacy (DP) guarantee (with strong composition) for\nsub-sampled shuffled models. We also demonstrate numerically significant\nimprovement in privacy-learning performance operating point using real data\nsets.",
          "link": "http://arxiv.org/abs/2107.08763",
          "publishedOn": "2021-07-20T02:04:46.980Z",
          "wordCount": null,
          "title": "Renyi Differential Privacy of the Subsampled Shuffle Model in Distributed Learning. (arXiv:2107.08763v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zecevic_M/0/1/0/all/0/1\">Matej Ze&#x10d;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhami_D/0/1/0/all/0/1\">Devendra Singh Dhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_A/0/1/0/all/0/1\">Athresh Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_S/0/1/0/all/0/1\">Sriraam Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>",
          "description": "While probabilistic models are an important tool for studying causality,\ndoing so suffers from the intractability of inference. As a step towards\ntractable causal models, we consider the problem of learning interventional\ndistributions using sum-product networks (SPNs) that are over-parameterized by\ngate functions, e.g., neural networks. Providing an arbitrarily intervened\ncausal graph as input, effectively subsuming Pearl's do-operator, the gate\nfunction predicts the parameters of the SPN. The resulting interventional SPNs\nare motivated and illustrated by a structural causal model themed around\npersonal health. Our empirical evaluation on three benchmark data sets as well\nas a synthetic health data set clearly demonstrates that interventional SPNs\nindeed are both expressive in modelling and flexible in adapting to the\ninterventions.",
          "link": "http://arxiv.org/abs/2102.10440",
          "publishedOn": "2021-07-20T02:04:46.980Z",
          "wordCount": null,
          "title": "Interventional Sum-Product Networks: Causal Inference with Tractable Probabilistic Models. (arXiv:2102.10440v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_I/0/1/0/all/0/1\">Iker Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skalski_P/0/1/0/all/0/1\">Piotr Skalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barns_Graham_A/0/1/0/all/0/1\">Alec Barns-Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Jason Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_D/0/1/0/all/0/1\">David Sutton</a>",
          "description": "Enabling interpretations of model uncertainties is of key importance in\nBayesian machine learning applications. Often, this requires to meaningfully\nattribute predictive uncertainties to source features in an image, text or\ncategorical array. However, popular attribution methods are particularly\ndesigned for classification and regression scores. In order to explain\nuncertainties, state of the art alternatives commonly procure counterfactual\nfeature vectors, and proceed by making direct comparisons. In this paper, we\nleverage path integrals to attribute uncertainties in Bayesian differentiable\nmodels. We present a novel algorithm that relies on in-distribution curves\nconnecting a feature vector to some counterfactual counterpart, and we retain\ndesirable properties of interpretability methods. We validate our approach on\nbenchmark image data sets with varying resolution, and show that it\nsignificantly simplifies interpretability over the existing alternatives.",
          "link": "http://arxiv.org/abs/2107.08756",
          "publishedOn": "2021-07-20T02:04:46.979Z",
          "wordCount": null,
          "title": "Path Integrals for the Attribution of Model Uncertainties. (arXiv:2107.08756v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.05601",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tsai_K/0/1/0/all/0/1\">Katherine Tsai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kolar_M/0/1/0/all/0/1\">Mladen Kolar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Koyejo_O/0/1/0/all/0/1\">Oluwasanmi Koyejo</a>",
          "description": "We propose a flexible yet interpretable model for high-dimensional data with\ntime-varying second order statistics, motivated and applied to functional\nneuroimaging data. Motivated by the neuroscience literature, we factorize the\ncovariances into sparse spatial and smooth temporal components. While this\nfactorization results in both parsimony and domain interpretability, the\nresulting estimation problem is nonconvex. To this end, we design a two-stage\noptimization scheme with a carefully tailored spectral initialization, combined\nwith iteratively refined alternating projected gradient descent. We prove a\nlinear convergence rate up to a nontrivial statistical error for the proposed\ndescent scheme and establish sample complexity guarantees for the estimator. We\nfurther quantify the statistical error for the multivariate Gaussian case.\nEmpirical results using simulated and real brain imaging data illustrate that\nour approach outperforms existing baselines.",
          "link": "http://arxiv.org/abs/2011.05601",
          "publishedOn": "2021-07-20T02:04:46.979Z",
          "wordCount": null,
          "title": "A Nonconvex Framework for Structured Dynamic Covariance Recovery. (arXiv:2011.05601v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhizhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Kun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenfeng Zhu</a>",
          "description": "Treatment effect estimation, which refers to the estimation of causal effects\nand aims to measure the strength of the causal relationship, is of great\nimportance in many fields but is a challenging problem in practice. As present,\ndata-driven causal effect estimation faces two main challenges, i.e., selection\nbias and the missing of counterfactual. To address these two issues, most of\nthe existing approaches tend to reduce the selection bias by learning a\nbalanced representation, and then to estimate the counterfactual through the\nrepresentation. However, they heavily rely on the finely hand-crafted metric\nfunctions when learning balanced representations, which generally doesn't work\nwell for the situations where the original distribution is complicated. In this\npaper, we propose a CETransformer model for casual effect estimation via\ntransformer based representation learning. To learn the representation of\ncovariates(features) robustly, a self-supervised transformer is proposed, by\nwhich the correlation between covariates can be well exploited through\nself-attention mechanism. In addition, an adversarial network is adopted to\nbalance the distribution of the treated and control groups in the\nrepresentation space. Experimental results on three real-world datasets\ndemonstrate the advantages of the proposed CETransformer, compared with the\nstate-of-the-art treatment effect estimation methods.",
          "link": "http://arxiv.org/abs/2107.08714",
          "publishedOn": "2021-07-20T02:04:46.978Z",
          "wordCount": null,
          "title": "CETransformer: Casual Effect Estimation via Transformer Based Representation Learning. (arXiv:2107.08714v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xueting Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhenhuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1\">Bang An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jing Bai</a>",
          "description": "Graph neural networks (GNNs) is widely used to learn a powerful\nrepresentation of graph-structured data. Recent work demonstrates that\ntransferring knowledge from self-supervised tasks to downstream tasks could\nfurther improve graph representation. However, there is an inherent gap between\nself-supervised tasks and downstream tasks in terms of optimization objective\nand training data. Conventional pre-training methods may be not effective\nenough on knowledge transfer since they do not make any adaptation for\ndownstream tasks. To solve such problems, we propose a new transfer learning\nparadigm on GNNs which could effectively leverage self-supervised tasks as\nauxiliary tasks to help the target task. Our methods would adaptively select\nand combine different auxiliary tasks with the target task in the fine-tuning\nstage. We design an adaptive auxiliary loss weighting model to learn the\nweights of auxiliary tasks by quantifying the consistency between auxiliary\ntasks and the target task. In addition, we learn the weighting model through\nmeta-learning. Our methods can be applied to various transfer learning\napproaches, it performs well not only in multi-task learning but also in\npre-training and fine-tuning. Comprehensive experiments on multiple downstream\ntasks demonstrate that the proposed methods can effectively combine auxiliary\ntasks with the target task and significantly improve the performance compared\nto state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.08765",
          "publishedOn": "2021-07-20T02:04:46.977Z",
          "wordCount": null,
          "title": "Adaptive Transfer Learning on Graph Neural Networks. (arXiv:2107.08765v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08721",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Chen_Q/0/1/0/all/0/1\">Qinkai Chen</a>",
          "description": "News events can greatly influence equity markets. In this paper, we are\ninterested in predicting the short-term movement of stock prices after\nfinancial news events using only the headlines of the news. To achieve this\ngoal, we introduce a new text mining method called Fine-Tuned\nContextualized-Embedding Recurrent Neural Network (FT-CE-RNN). Compared with\nprevious approaches which use static vector representations of the news (static\nembedding), our model uses contextualized vector representations of the\nheadlines (contextualized embeddings) generated from Bidirectional Encoder\nRepresentations from Transformers (BERT). Our model obtains the\nstate-of-the-art result on this stock movement prediction task. It shows\nsignificant improvement compared with other baseline models, in both accuracy\nand trading simulations. Through various trading simulations based on millions\nof headlines from Bloomberg News, we demonstrate the ability of this model in\nreal scenarios.",
          "link": "http://arxiv.org/abs/2107.08721",
          "publishedOn": "2021-07-20T02:04:46.976Z",
          "wordCount": null,
          "title": "Stock Movement Prediction with Financial News using Contextualized Embedding from BERT. (arXiv:2107.08721v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuangjia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Ying Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jiahua Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuedong Yang</a>",
          "description": "Constructing appropriate representations of molecules lies at the core of\nnumerous tasks such as material science, chemistry and drug designs. Recent\nresearches abstract molecules as attributed graphs and employ graph neural\nnetworks (GNN) for molecular representation learning, which have made\nremarkable achievements in molecular graph modeling. Albeit powerful, current\nmodels either are based on local aggregation operations and thus miss\nhigher-order graph properties or focus on only node information without fully\nusing the edge information. For this sake, we propose a Communicative Message\nPassing Transformer (CoMPT) neural network to improve the molecular graph\nrepresentation by reinforcing message interactions between nodes and edges\nbased on the Transformer architecture. Unlike the previous transformer-style\nGNNs that treat molecules as fully connected graphs, we introduce a message\ndiffusion mechanism to leverage the graph connectivity inductive bias and\nreduce the message enrichment explosion. Extensive experiments demonstrated\nthat the proposed model obtained superior performances (around 4$\\%$ on\naverage) against state-of-the-art baselines on seven chemical property datasets\n(graph-level tasks) and two chemical shift datasets (node-level tasks). Further\nvisualization studies also indicated a better representation capacity achieved\nby our model.",
          "link": "http://arxiv.org/abs/2107.08773",
          "publishedOn": "2021-07-20T02:04:46.975Z",
          "wordCount": null,
          "title": "Learning Attributed Graph Representations with Communicative Message Passing Transformer. (arXiv:2107.08773v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shehabi_S/0/1/0/all/0/1\">Shadi Al Shehabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baba_A/0/1/0/all/0/1\">Abdullatif Baba</a>",
          "description": "Association rules are useful to discover relationships, which are mostly\nhidden, between the different items in large datasets. Symbolic models are the\nprincipal tools to extract association rules. This basic technique is\ntime-consuming, and it generates a big number of associated rules. To overcome\nthis drawback, we suggest a new method, called MARC, to extract the more\nimportant association rules of two important levels: Type I, and Type II. This\napproach relies on a multi-topographic unsupervised neural network model as\nwell as clustering quality measures that evaluate the success of a given\nnumerical classification model to behave as a natural symbolic model.",
          "link": "http://arxiv.org/abs/2107.08814",
          "publishedOn": "2021-07-20T02:04:46.975Z",
          "wordCount": null,
          "title": "MARC: Mining Association Rules from datasets by using Clustering models. (arXiv:2107.08814v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jo_H/0/1/0/all/0/1\">Ha Young Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Recently, semiconductors' demand has exploded in virtual reality,\nsmartphones, wearable devices, the internet of things, robotics, and\nautomobiles. Semiconductor manufacturers want to make semiconductors with high\nyields. To do this, manufacturers conduct many quality assurance activities.\nWafer map pattern classification is a typical way of quality assurance. The\ndefect pattern on the wafer map can tell us which process has a problem. Most\nof the existing wafer map classification methods are based on supervised\nmethods. The supervised methods tend to have high performance, but they require\nextensive labor and expert knowledge to produce labeled datasets with a\nbalanced distribution in mind. In the semiconductor manufacturing process, it\nis challenging to get defect data with balanced distribution. In this paper, we\npropose a one-class classification method using an Adversarial Autoencoder\n(AAE) with Deep Support Vector Data Description (DSVDD) prior, which generates\nrandom vectors within the hypersphere of DSVDD. We use the WM-811k dataset,\nwhich consists of a real-world wafer map. We compare the F1 score performance\nof our model with DSVDD and AAE.",
          "link": "http://arxiv.org/abs/2107.08823",
          "publishedOn": "2021-07-20T02:04:46.974Z",
          "wordCount": null,
          "title": "One-Class Classification for Wafer Map using Adversarial Autoencoder with DSVDD Prior. (arXiv:2107.08823v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rafailov_R/0/1/0/all/0/1\">Rafael Rafailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajeswaran_A/0/1/0/all/0/1\">Aravind Rajeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>",
          "description": "Reward function specification, which requires considerable human effort and\niteration, remains a major impediment for learning behaviors through deep\nreinforcement learning. In contrast, providing visual demonstrations of desired\nbehaviors often presents an easier and more natural way to teach agents. We\nconsider a setting where an agent is provided a fixed dataset of visual\ndemonstrations illustrating how to perform a task, and must learn to solve the\ntask using the provided demonstrations and unsupervised environment\ninteractions. This setting presents a number of challenges including\nrepresentation learning for visual observations, sample complexity due to high\ndimensional spaces, and learning instability due to the lack of a fixed reward\nor learning signal. Towards addressing these challenges, we develop a\nvariational model-based adversarial imitation learning (V-MAIL) algorithm. The\nmodel-based approach provides a strong signal for representation learning,\nenables sample efficiency, and improves the stability of adversarial training\nby enabling on-policy learning. Through experiments involving several\nvision-based locomotion and manipulation tasks, we find that V-MAIL learns\nsuccessful visuomotor policies in a sample-efficient manner, has better\nstability compared to prior work, and also achieves higher asymptotic\nperformance. We further find that by transferring the learned models, V-MAIL\ncan learn new tasks from visual demonstrations without any additional\nenvironment interactions. All results including videos can be found online at\n\\url{https://sites.google.com/view/variational-mail}.",
          "link": "http://arxiv.org/abs/2107.08829",
          "publishedOn": "2021-07-20T02:04:46.973Z",
          "wordCount": null,
          "title": "Visual Adversarial Imitation Learning using Variational Models. (arXiv:2107.08829v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaolong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanegas_F/0/1/0/all/0/1\">Fernando Vanegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1\">Felipe Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanderson_C/0/1/0/all/0/1\">Conrad Sanderson</a>",
          "description": "The use of multi-rotor Unmanned Aerial Vehicles (UAVs) for search and rescue\nas well as remote sensing is rapidly increasing. Multi-rotor UAVs, however,\nhave limited endurance. The range of UAV applications can be widened if teams\nof multiple UAVs are used. We propose a framework for a team of UAVs to\ncooperatively explore and find a target in complex GPS-denied environments with\nobstacles. The team of UAVs autonomously navigates, explores, detects, and\nfinds the target in a cluttered environment with a known map. Examples of such\nenvironments include indoor scenarios, urban or natural canyons, caves, and\ntunnels, where the GPS signal is limited or blocked. The framework is based on\na probabilistic decentralised Partially Observable Markov Decision Process\nwhich accounts for the uncertainties in sensing and the environment. The team\ncan cooperate efficiently, with each UAV sharing only limited processed\nobservations and their locations during the mission. The system is simulated\nusing the Robotic Operating System and Gazebo. Performance of the system with\nan increasing number of UAVs in several indoor scenarios with obstacles is\ntested. Results indicate that the proposed multi-UAV system has improvements in\nterms of time-cost, the proportion of search area surveyed, as well as\nsuccessful rates for search and rescue missions.",
          "link": "http://arxiv.org/abs/2107.08834",
          "publishedOn": "2021-07-20T02:04:46.973Z",
          "wordCount": null,
          "title": "A Multi-UAV System for Exploration and Target Finding in Cluttered and GPS-Denied Environments. (arXiv:2107.08834v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Itoh_T/0/1/0/all/0/1\">Takeshi D. Itoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubo_T/0/1/0/all/0/1\">Takatomi Kubo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikeda_K/0/1/0/all/0/1\">Kazushi Ikeda</a>",
          "description": "Graph neural networks (GNNs) have been widely used to learn vector\nrepresentation of graph-structured data and achieved better task performance\nthan conventional methods. The foundation of GNNs is the message passing\nprocedure, which propagates the information in a node to its neighbors. Since\nthis procedure proceeds one step per layer, the range of the information\npropagation among nodes is small in the lower layers, and it expands toward the\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\nstructural information in a graph. On the other hand, it is known that deep GNN\nmodels suffer from performance degradation because they lose nodes' local\ninformation, which would be essential for good model performance, through many\nmessage passing steps. In this study, we propose a multi-level attention\npooling (MLAP) for graph-level classification tasks, which can adapt to both\nlocal and global structural information in a graph. It has an attention pooling\nlayer for each message passing step and computes the final graph representation\nby unifying the layer-wise graph representations. The MLAP architecture allows\nmodels to utilize the structural information of graphs with multiple levels of\nlocalities because it preserves layer-wise information before losing them due\nto oversmoothing. Results of our experiments show that the MLAP architecture\nimproves deeper models' performance in graph classification tasks compared to\nthe baseline architectures. In addition, analyses on the layer-wise graph\nrepresentations suggest that aggregating information from multiple levels of\nlocalities indeed has the potential to improve the discriminability of learned\ngraph representations.",
          "link": "http://arxiv.org/abs/2103.01488",
          "publishedOn": "2021-07-20T02:04:46.972Z",
          "wordCount": null,
          "title": "Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities. (arXiv:2103.01488v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xiaoyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shuai L&#xfc;</a>",
          "description": "Generative Adversarial Networks (GAN) is an adversarial model, and it has\nbeen demonstrated to be effective for various generative tasks. However, GAN\nand its variants also suffer from many training problems, such as mode collapse\nand gradient vanish. In this paper, we firstly propose a general crossover\noperator, which can be widely applied to GANs using evolutionary strategies.\nThen we design an evolutionary GAN framework C-GAN based on it. And we combine\nthe crossover operator with evolutionary generative adversarial networks (EGAN)\nto implement the evolutionary generative adversarial networks with crossover\n(CE-GAN). Under the premise that a variety of loss functions are used as\nmutation operators to generate mutation individuals, we evaluate the generated\nsamples and allow the mutation individuals to learn experiences from the output\nin a knowledge distillation manner, imitating the best output outcome,\nresulting in better offspring. Then, we greedily selected the best offspring as\nparents for subsequent training using discriminator as evaluator. Experiments\non real datasets demonstrate the effectiveness of CE-GAN and show that our\nmethod is competitive in terms of generated images quality and time efficiency.",
          "link": "http://arxiv.org/abs/2101.11186",
          "publishedOn": "2021-07-20T02:04:46.971Z",
          "wordCount": null,
          "title": "Evolutionary Generative Adversarial Networks with Crossover Based Knowledge Distillation. (arXiv:2101.11186v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1\">Aske Plaat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosters_W/0/1/0/all/0/1\">Walter Kosters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preuss_M/0/1/0/all/0/1\">Mike Preuss</a>",
          "description": "Deep reinforcement learning has shown remarkable success in the past few\nyears. Highly complex sequential decision making problems from game playing and\nrobotics have been solved with deep model-free methods. Unfortunately, the\nsample complexity of model-free methods is often high. To reduce the number of\nenvironment samples, model-based reinforcement learning creates an explicit\nmodel of the environment dynamics. Achieving high model accuracy is a challenge\nin high-dimensional problems. In recent years, a diverse landscape of\nmodel-based methods has been introduced to improve model accuracy, using\nmethods such as uncertainty modeling, model-predictive control, latent models,\nand end-to-end learning and planning. Some of these methods succeed in\nachieving high accuracy at low sample complexity, most do so either in a\nrobotics or in a games context. In this paper, we survey these methods; we\nexplain in detail how they work and what their strengths and weaknesses are. We\nconclude with a research agenda for future work to make the methods more robust\nand more widely applicable to other applications.",
          "link": "http://arxiv.org/abs/2107.08241",
          "publishedOn": "2021-07-20T02:04:46.880Z",
          "wordCount": null,
          "title": "High-Accuracy Model-Based Reinforcement Learning, a Survey. (arXiv:2107.08241v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08265",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jain_A/0/1/0/all/0/1\">Ayush Jain</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Srijith_P/0/1/0/all/0/1\">P. K. Srijith</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Emtiyaz Khan</a> (2) ((1) Department of Computer Science and Engineering, Indian Institute of Technology Hyderabad, India, (2) RIKEN Center for AI Project, Tokyo, Japan)",
          "description": "Deep Gaussian Processes (DGPs) are multi-layer, flexible extensions of\nGaussian processes but their training remains challenging. Sparse\napproximations simplify the training but often require optimization over a\nlarge number of inducing inputs and their locations across layers. In this\npaper, we simplify the training by setting the locations to a fixed subset of\ndata and sampling the inducing inputs from a variational distribution. This\nreduces the trainable parameters and computation cost without significant\nperformance degradations, as demonstrated by our empirical results on\nregression problems. Our modifications simplify and stabilize DGP training\nwhile making it amenable to sampling schemes for setting the inducing inputs.",
          "link": "http://arxiv.org/abs/2107.08265",
          "publishedOn": "2021-07-20T02:04:46.879Z",
          "wordCount": null,
          "title": "Subset-of-Data Variational Inference for Deep Gaussian-Processes Regression. (arXiv:2107.08265v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2008.12623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milli_S/0/1/0/all/0/1\">Smitha Milli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belli_L/0/1/0/all/0/1\">Luca Belli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1\">Moritz Hardt</a>",
          "description": "Most recommendation engines today are based on predicting user engagement,\ne.g. predicting whether a user will click on an item or not. However, there is\npotentially a large gap between engagement signals and a desired notion of\n\"value\" that is worth optimizing for. We use the framework of measurement\ntheory to (a) confront the designer with a normative question about what the\ndesigner values, (b) provide a general latent variable model approach that can\nbe used to operationalize the target construct and directly optimize for it,\nand (c) guide the designer in evaluating and revising their operationalization.\nWe implement our approach on the Twitter platform on millions of users. In line\nwith established approaches to assessing the validity of measurements, we\nperform a qualitative evaluation of how well our model captures a desired\nnotion of \"value\".",
          "link": "http://arxiv.org/abs/2008.12623",
          "publishedOn": "2021-07-20T02:04:46.878Z",
          "wordCount": null,
          "title": "From Optimizing Engagement to Measuring Value. (arXiv:2008.12623v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parry_H/0/1/0/all/0/1\">Hishan Parry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xun_L/0/1/0/all/0/1\">Lei Xun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_A/0/1/0/all/0/1\">Amin Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jia Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrett_G/0/1/0/all/0/1\">Geoff V. Merrett</a>",
          "description": "The Transformer architecture is widely used for machine translation tasks.\nHowever, its resource-intensive nature makes it challenging to implement on\nconstrained embedded devices, particularly where available hardware resources\ncan vary at run-time. We propose a dynamic machine translation model that\nscales the Transformer architecture based on the available resources at any\nparticular time. The proposed approach, 'Dynamic-HAT', uses a HAT\nSuperTransformer as the backbone to search for SubTransformers with different\naccuracy-latency trade-offs at design time. The optimal SubTransformers are\nsampled from the SuperTransformer at run-time, depending on latency\nconstraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses\ninherited SubTransformers sampled directly from the SuperTransformer with a\nswitching time of <1s. Using inherited SubTransformers results in a BLEU score\nloss of <1.5% because the SubTransformer configuration is not retrained from\nscratch after sampling. However, to recover this loss in performance, the\ndimensions of the design space can be reduced to tailor it to a family of\ntarget hardware. The new reduced design space results in a BLEU score increase\nof approximately 1% for sub-optimal models from the original design space, with\na wide range for performance scaling between 0.356s - 1.526s for the GPU and\n2.9s - 7.31s for the CPU.",
          "link": "http://arxiv.org/abs/2107.08199",
          "publishedOn": "2021-07-20T02:04:46.868Z",
          "wordCount": null,
          "title": "Dynamic Transformer for Efficient Machine Translation on Embedded Devices. (arXiv:2107.08199v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.10483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Linjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zheyan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wei Cui</a>",
          "description": "Nowadays fairness issues have raised great concerns in decision-making\nsystems. Various fairness notions have been proposed to measure the degree to\nwhich an algorithm is unfair. In practice, there frequently exist a certain set\nof variables we term as fair variables, which are pre-decision covariates such\nas users' choices. The effects of fair variables are irrelevant in assessing\nthe fairness of the decision support algorithm. We thus define conditional\nfairness as a more sound fairness metric by conditioning on the fairness\nvariables. Given different prior knowledge of fair variables, we demonstrate\nthat traditional fairness notations, such as demographic parity and equalized\nodds, are special cases of our conditional fairness notations. Moreover, we\npropose a Derivable Conditional Fairness Regularizer (DCFR), which can be\nintegrated into any decision-making model, to track the trade-off between\nprecision and fairness of algorithmic decision making. Specifically, an\nadversarial representation based conditional independence loss is proposed in\nour DCFR to measure the degree of unfairness. With extensive experiments on\nthree real-world datasets, we demonstrate the advantages of our conditional\nfairness notation and DCFR.",
          "link": "http://arxiv.org/abs/2006.10483",
          "publishedOn": "2021-07-20T02:04:46.867Z",
          "wordCount": null,
          "title": "Algorithmic Decision Making with Conditional Fairness. (arXiv:2006.10483v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1705.07164",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guo_X/0/1/0/all/0/1\">Xin Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hong_J/0/1/0/all/0/1\">Johnny Hong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lin_T/0/1/0/all/0/1\">Tianyi Lin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>",
          "description": "Wasserstein Generative Adversarial Networks (WGANs) provide a versatile class\nof models, which have attracted great attention in various applications.\nHowever, this framework has two main drawbacks: (i) Wasserstein-1 (or\nEarth-Mover) distance is restrictive such that WGANs cannot always fit data\ngeometry well; (ii) It is difficult to achieve fast training of WGANs. In this\npaper, we propose a new class of \\textit{Relaxed Wasserstein} (RW) distances by\ngeneralizing Wasserstein-1 distance with Bregman cost functions. We show that\nRW distances achieve nice statistical properties while not sacrificing the\ncomputational tractability. Combined with the GANs framework, we develop\nRelaxed WGANs (RWGANs) which are not only statistically flexible but can be\napproximated efficiently using heuristic approaches. Experiments on real images\ndemonstrate that the RWGAN with Kullback-Leibler (KL) cost function outperforms\nother competing approaches, e.g., WGANs, even with gradient penalty.",
          "link": "http://arxiv.org/abs/1705.07164",
          "publishedOn": "2021-07-20T02:04:46.866Z",
          "wordCount": null,
          "title": "Relaxed Wasserstein with Applications to GANs. (arXiv:1705.07164v8 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sayak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>",
          "description": "Floods wreak havoc throughout the world, causing billions of dollars in\ndamages, and uprooting communities, ecosystems and economies. Accurate and\nrobust flood detection including delineating open water flood areas and\nidentifying flood levels can aid in disaster response and mitigation. However,\nestimating flood levels remotely is of essence as physical access to flooded\nareas is limited and the ability to deploy instruments in potential flood zones\ncan be dangerous. Aligning flood extent mapping with local topography can\nprovide a plan-of-action that the disaster response team can consider. Thus,\nremote flood level estimation via satellites like Sentinel-1 can prove to be\nremedial. The Emerging Techniques in Computational Intelligence (ETCI)\ncompetition on Flood Detection tasked participants with predicting flooded\npixels after training with synthetic aperture radar (SAR) images in a\nsupervised setting. We use a cyclical approach involving two stages (1)\ntraining an ensemble model of multiple UNet architectures with available high\nand low confidence labeled data and, (2) generating pseudo labels or low\nconfidence labels on the unlabeled test dataset, and then, combining the\ngenerated labels with the previously available high confidence labeled dataset.\nThis assimilated dataset is used for the next round of training ensemble\nmodels. This cyclical process is repeated until the performance improvement\nplateaus. Additionally, we post process our results with Conditional Random\nFields. Our approach sets a high score on the public leaderboard for the ETCI\ncompetition with 0.7654 IoU. Our method, which we release with all the code\nincluding trained models, can also be used as an open science benchmark for the\nSentinel-1 released dataset on GitHub.",
          "link": "http://arxiv.org/abs/2107.08369",
          "publishedOn": "2021-07-20T02:04:46.865Z",
          "wordCount": null,
          "title": "Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1910.09739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Chuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meng Chang Chen</a>",
          "description": "This work investigates the framework and performance issues of the composite\nneural network, which is composed of a collection of pre-trained and\nnon-instantiated neural network models connected as a rooted directed acyclic\ngraph for solving complicated applications. A pre-trained neural network model\nis generally well trained, targeted to approximate a specific function. Despite\na general belief that a composite neural network may perform better than a\nsingle component, the overall performance characteristics are not clear. In\nthis work, we construct the framework of a composite network, and prove that a\ncomposite neural network performs better than any of its pre-trained components\nwith a high probability bound. In addition, if an extra pre-trained component\nis added to a composite network, with high probability, the overall performance\nwill not be degraded. In the study, we explore a complicated application --\nPM2.5 prediction -- to illustrate the correctness of the proposed composite\nnetwork theory. In the empirical evaluations of PM2.5 prediction, the\nconstructed composite neural network models support the proposed theory and\nperform better than other machine learning models, demonstrate the advantages\nof the proposed framework.",
          "link": "http://arxiv.org/abs/1910.09739",
          "publishedOn": "2021-07-20T02:04:46.865Z",
          "wordCount": null,
          "title": "Composite Neural Network: Theory and Application to PM2.5 Prediction. (arXiv:1910.09739v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dandekar_A/0/1/0/all/0/1\">Abhishek Dandekar</a>",
          "description": "Machine learning (ML) techniques are being increasingly used in mobile\nnetworks for network planning, operation, management, optimisation and much\nmore. These techniques are realised using a set of logical nodes known as ML\npipeline. A single network operator might have thousands of such ML pipelines\ndistributed across its network. These pipelines need to be managed and\norchestrated across network domains. Thus it is essential to have autonomic\nmulti-domain orchestration of ML pipelines in mobile networks. International\nTelecommunications Union (ITU) has provided an architectural framework for\nmanagement and orchestration of ML pipelines in future networks. We extend this\nframework to enable autonomic orchestration of ML pipelines across multiple\nnetwork domains. We present our system architecture and describe its\napplication using a smart factory use case. Our work allows autonomic\norchestration of multi-domain ML pipelines in a standardised, technology\nagnostic, privacy preserving fashion.",
          "link": "http://arxiv.org/abs/2107.08194",
          "publishedOn": "2021-07-20T02:04:46.864Z",
          "wordCount": null,
          "title": "Towards autonomic orchestration of machine learning pipelines in future networks. (arXiv:2107.08194v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2002.06470",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ashukha_A/0/1/0/all/0/1\">Arsenii Ashukha</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lyzhov_A/0/1/0/all/0/1\">Alexander Lyzhov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Molchanov_D/0/1/0/all/0/1\">Dmitry Molchanov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vetrov_D/0/1/0/all/0/1\">Dmitry Vetrov</a>",
          "description": "Uncertainty estimation and ensembling methods go hand-in-hand. Uncertainty\nestimation is one of the main benchmarks for assessment of ensembling\nperformance. At the same time, deep learning ensembles have provided\nstate-of-the-art results in uncertainty estimation. In this work, we focus on\nin-domain uncertainty for image classification. We explore the standards for\nits quantification and point out pitfalls of existing metrics. Avoiding these\npitfalls, we perform a broad study of different ensembling techniques. To\nprovide more insight in this study, we introduce the deep ensemble equivalent\nscore (DEE) and show that many sophisticated ensembling techniques are\nequivalent to an ensemble of only few independently trained networks in terms\nof test performance.",
          "link": "http://arxiv.org/abs/2002.06470",
          "publishedOn": "2021-07-20T02:04:46.864Z",
          "wordCount": null,
          "title": "Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning. (arXiv:2002.06470v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.08170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingfeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chongjie Zhang</a>",
          "description": "Meta reinforcement learning (meta-RL) extracts knowledge from previous tasks\nand achieves fast adaptation to new tasks. Despite recent progress, efficient\nexploration in meta-RL remains a key challenge in sparse-reward tasks, as it\nrequires quickly finding informative task-relevant experiences in both\nmeta-training and adaptation. To address this challenge, we explicitly model an\nexploration policy learning problem for meta-RL, which is separated from\nexploitation policy learning, and introduce a novel empowerment-driven\nexploration objective, which aims to maximize information gain for task\nidentification. We derive a corresponding intrinsic reward and develop a new\noff-policy meta-RL framework, which efficiently learns separate context-aware\nexploration and exploitation policies by sharing the knowledge of task\ninference. Experimental evaluation shows that our meta-RL method significantly\noutperforms state-of-the-art baselines on various sparse-reward MuJoCo\nlocomotion tasks and more complex sparse-reward Meta-World tasks.",
          "link": "http://arxiv.org/abs/2006.08170",
          "publishedOn": "2021-07-20T02:04:46.863Z",
          "wordCount": null,
          "title": "MetaCURE: Meta Reinforcement Learning with Empowerment-Driven Exploration. (arXiv:2006.08170v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>",
          "description": "Previous studies have shown that initializing neural machine translation\n(NMT) models with the pre-trained language models (LM) can speed up the model\ntraining and boost the model performance. In this work, we identify a critical\nside-effect of pre-training for NMT, which is due to the discrepancy between\nthe training objectives of LM-based pre-training and NMT. Since the LM\nobjective learns to reconstruct a few source tokens and copy most of them, the\npre-training initialization would affect the copying behaviors of NMT models.\nWe provide a quantitative analysis of copying behaviors by introducing a metric\ncalled copying ratio, which empirically shows that pre-training based NMT\nmodels have a larger copying ratio than the standard one. In response to this\nproblem, we propose a simple and effective method named copying penalty to\ncontrol the copying behaviors in decoding. Extensive experiments on both\nin-domain and out-of-domain benchmarks show that the copying penalty method\nconsistently improves translation performance by controlling copying behaviors\nfor pre-training based NMT models. Source code is freely available at\nhttps://github.com/SunbowLiu/CopyingPenalty.",
          "link": "http://arxiv.org/abs/2107.08212",
          "publishedOn": "2021-07-20T02:04:46.862Z",
          "wordCount": null,
          "title": "On the Copying Behaviors of Pre-Training for Neural Machine Translation. (arXiv:2107.08212v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.11860",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Garg_A/0/1/0/all/0/1\">Aksh Garg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salehi_S/0/1/0/all/0/1\">Sana Salehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rocca_M/0/1/0/all/0/1\">Marianna La Rocca</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garner_R/0/1/0/all/0/1\">Rachael Garner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duncan_D/0/1/0/all/0/1\">Dominique Duncan</a>",
          "description": "With COVID-19 cases rising rapidly, deep learning has emerged as a promising\ndiagnosis technique. However, identifying the most accurate models to\ncharacterize COVID-19 patients is challenging because comparing results\nobtained with different types of data and acquisition processes is non-trivial.\nIn this paper we designed, evaluated, and compared the performance of 20\nconvolutional neutral networks in classifying patients as COVID-19 positive,\nhealthy, or suffering from other pulmonary lung infections based on Chest CT\nscans, serving as the first to consider the EfficientNet family for COVID-19\ndiagnosis and employ intermediate activation maps for visualizing model\nperformance. All models are trained and evaluated in Python using 4173 Chest CT\nimages from the dataset entitled \"A COVID multiclass dataset of CT scans,\" with\n2168, 758, and 1247 images of patients that are COVID-19 positive, healthy, or\nsuffering from other pulmonary infections, respectively. EfficientNet-B5 was\nidentified as the best model with an F1 score of 0.9769+/-0.0046, accuracy of\n0.9759+/-0.0048, sensitivity of 0.9788+/-0.0055, specificity of\n0.9730+/-0.0057, and precision of 0.9751 +/- 0.0051. On an alternate 2-class\ndataset, EfficientNetB5 obtained an accuracy of 0.9845+/-0.0109, F1 score of\n0.9599+/-0.0251, sensitivity of 0.9682+/-0.0099, specificity of\n0.9883+/-0.0150, and precision of 0.9526 +/- 0.0523. Intermediate activation\nmaps and Gradient-weighted Class Activation Mappings offered\nhuman-interpretable evidence of the model's perception of ground-class\nopacities and consolidations, hinting towards a promising use-case of\nartificial intelligence-assisted radiology tools. With a prediction speed of\nunder 0.1 seconds on GPUs and 0.5 seconds on CPUs, our proposed model offers a\nrapid, scalable, and accurate diagnostic for COVID-19.",
          "link": "http://arxiv.org/abs/2012.11860",
          "publishedOn": "2021-07-20T02:04:46.862Z",
          "wordCount": null,
          "title": "Efficient and Visualizable Convolutional Neural Networks for COVID-19 Classification Using Chest CT. (arXiv:2012.11860v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08135",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yamane_I/0/1/0/all/0/1\">Ikko Yamane</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Honda_J/0/1/0/all/0/1\">Junya Honda</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yger_F/0/1/0/all/0/1\">Florian Yger</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "Ordinary supervised learning is useful when we have paired training data of\ninput $X$ and output $Y$. However, such paired data can be difficult to collect\nin practice. In this paper, we consider the task of predicting $Y$ from $X$\nwhen we have no paired data of them, but we have two separate, independent\ndatasets of $X$ and $Y$ each observed with some mediating variable $U$, that\nis, we have two datasets $S_X = \\{(X_i, U_i)\\}$ and $S_Y = \\{(U'_j, Y'_j)\\}$. A\nnaive approach is to predict $U$ from $X$ using $S_X$ and then $Y$ from $U$\nusing $S_Y$, but we show that this is not statistically consistent. Moreover,\npredicting $U$ can be more difficult than predicting $Y$ in practice, e.g.,\nwhen $U$ has higher dimensionality. To circumvent the difficulty, we propose a\nnew method that avoids predicting $U$ but directly learns $Y = f(X)$ by\ntraining $f(X)$ with $S_{X}$ to predict $h(U)$ which is trained with $S_{Y}$ to\napproximate $Y$. We prove statistical consistency and error bounds of our\nmethod and experimentally confirm its practical usefulness.",
          "link": "http://arxiv.org/abs/2107.08135",
          "publishedOn": "2021-07-20T02:04:46.861Z",
          "wordCount": null,
          "title": "Mediated Uncoupled Learning: Learning Functions without Direct Input-output Correspondences. (arXiv:2107.08135v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">JoonSung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">YeongHyeon Park</a>",
          "description": "Recently Autoencoder(AE) based models are widely used in the field of anomaly\ndetection. A model trained with normal data generates a larger restoration\nerror for abnormal data. Whether or not abnormal data is determined by\nobserving the restoration error. It takes a lot of cost and time to obtain\nabnormal data in the industrial field. Therefore the model trains only normal\ndata and detects abnormal data in the inference phase. However, the restoration\narea for the input data of AE is limited in the latent space. To solve this\nproblem, we propose Multiple-hypothesis Autoencoder(MH-AE) model composed of\nseveral decoders. MH-AE model increases the restoration area through contention\nbetween decoders. The proposed method shows that the anomaly detection\nperformance is improved compared to the traditional AE for various input\ndatasets.",
          "link": "http://arxiv.org/abs/2107.08790",
          "publishedOn": "2021-07-20T02:04:46.860Z",
          "wordCount": null,
          "title": "Anomaly Detection Based on Multiple-Hypothesis Autoencoder. (arXiv:2107.08790v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiahua Luo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Vong_C/0/1/0/all/0/1\">Chi-Man Vong</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jie Du</a> (2) ((1) Department of Computer and Information Science, University of Macau, Macao SAR, China, (2) School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, China)",
          "description": "Sparse Bayesian Learning (SBL) constructs an extremely sparse probabilistic\nmodel with very competitive generalization. However, SBL needs to invert a big\ncovariance matrix with complexity O(M^3 ) (M: feature size) for updating the\nregularization priors, making it difficult for practical use. There are three\nissues in SBL: 1) Inverting the covariance matrix may obtain singular solutions\nin some cases, which hinders SBL from convergence; 2) Poor scalability to\nproblems with high dimensional feature space or large data size; 3) SBL easily\nsuffers from memory overflow for large-scale data. This paper addresses these\nissues with a newly proposed diagonal Quasi-Newton (DQN) method for SBL called\nDQN-SBL where the inversion of big covariance matrix is ignored so that the\ncomplexity and memory storage are reduced to O(M). The DQN-SBL is thoroughly\nevaluated on non-linear classifiers and linear feature selection using various\nbenchmark datasets of different sizes. Experimental results verify that DQN-SBL\nreceives competitive generalization with a very sparse model and scales well to\nlarge-scale problems.",
          "link": "http://arxiv.org/abs/2107.08195",
          "publishedOn": "2021-07-20T02:04:46.829Z",
          "wordCount": null,
          "title": "Sparse Bayesian Learning with Diagonal Quasi-Newton Method For Large Scale Classification. (arXiv:2107.08195v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.12804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jon_J/0/1/0/all/0/1\">Josef Jon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>",
          "description": "This work demonstrates that, contrary to a common belief, using the objective\nwith independence assumption for modelling the span probability $P(a_s,a_e) =\nP(a_s)P(a_e)$ of span starting at position $a_s$ and ending at position $a_e$\nhas adverse effects. Therefore we propose multiple approaches to modelling\njoint probability $P(a_s,a_e)$ directly. Among those, we propose a compound\nobjective, composed from the joint probability while still keeping the\nobjective with independence assumption as an auxiliary objective. We find that\nthe compound objective is consistently superior or equal to other assumptions\nin exact match. Additionally, we identified common errors caused by the\nassumption of independence and manually checked the counterpart predictions,\ndemonstrating the impact of the compound objective on the real examples. Our\nfindings are supported via experiments with three extractive QA models (BIDAF,\nBERT, ALBERT) over six datasets and our code, individual results and manual\nanalysis are available online.",
          "link": "http://arxiv.org/abs/2008.12804",
          "publishedOn": "2021-07-20T02:04:46.806Z",
          "wordCount": null,
          "title": "Rethinking the Objectives of Extractive Question Answering. (arXiv:2008.12804v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chi-Heng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azabou_M/0/1/0/all/0/1\">Mehdi Azabou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_E/0/1/0/all/0/1\">Eva L. Dyer</a>",
          "description": "Optimal transport (OT) is a widely used technique for distribution alignment,\nwith applications throughout the machine learning, graphics, and vision\ncommunities. Without any additional structural assumptions on trans-port,\nhowever, OT can be fragile to outliers or noise, especially in high dimensions.\nHere, we introduce a new form of structured OT that simultaneously learns\nlow-dimensional structure in data while leveraging this structure to solve the\nalignment task. Compared with OT, the resulting transport plan has better\nstructural interpretability, highlighting the connections between individual\ndata points and local geometry, and is more robust to noise and sampling. We\napply the method to synthetic as well as real datasets, where we show that our\nmethod can facilitate alignment in noisy settings and can be used to both\ncorrect and interpret domain shift.",
          "link": "http://arxiv.org/abs/2012.11589",
          "publishedOn": "2021-07-20T02:04:46.806Z",
          "wordCount": null,
          "title": "Making transport more robust and interpretable by moving data through a small number of anchor points. (arXiv:2012.11589v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.13955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zafarani_F/0/1/0/all/0/1\">Farzad Zafarani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_C/0/1/0/all/0/1\">Chris Clifton</a>",
          "description": "With the increasing collection of users' data, protecting individual privacy\nhas gained more interest. Differential Privacy is a strong concept of\nprotecting individuals. Naive Bayes is one of the popular machine learning\nalgorithm, used as a baseline for many tasks. In this work, we have provided a\ndifferentially private Naive Bayes classifier that adds noise proportional to\nthe Smooth Sensitivity of its parameters. We have compared our result to\nVaidya, Shafiq, Basu, and Hong in which they have scaled the noise to the\nglobal sensitivity of the parameters. Our experiment results on the real-world\ndatasets show that the accuracy of our method has improved significantly while\nstill preserving $\\varepsilon$-differential privacy.",
          "link": "http://arxiv.org/abs/2003.13955",
          "publishedOn": "2021-07-20T02:04:46.805Z",
          "wordCount": null,
          "title": "Differentially Private Naive Bayes Classifier using Smooth Sensitivity. (arXiv:2003.13955v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen-Yu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chung-Wei Lee</a>",
          "description": "Policy optimization is a widely-used method in reinforcement learning. Due to\nits local-search nature, however, theoretical guarantees on global optimality\noften rely on extra assumptions on the Markov Decision Processes (MDPs) that\nbypass the challenge of global exploration. To eliminate the need of such\nassumptions, in this work, we develop a general solution that adds dilated\nbonuses to the policy update to facilitate global exploration. To showcase the\npower and generality of this technique, we apply it to several episodic MDP\nsettings with adversarial losses and bandit feedback, improving and\ngeneralizing the state-of-the-art. Specifically, in the tabular case, we obtain\n$\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret where $T$ is the number of episodes,\nimproving the $\\widetilde{\\mathcal{O}}({T}^{2/3})$ regret bound by Shani et al.\n(2020). When the number of states is infinite, under the assumption that the\nstate-action values are linear in some low-dimensional features, we obtain\n$\\widetilde{\\mathcal{O}}({T}^{2/3})$ regret with the help of a simulator,\nmatching the result of Neu and Olkhovskaya (2020) while importantly removing\nthe need of an exploratory policy that their algorithm requires. When a\nsimulator is unavailable, we further consider a linear MDP setting and obtain\n$\\widetilde{\\mathcal{O}}({T}^{14/15})$ regret, which is the first result for\nlinear MDPs with adversarial losses and bandit feedback.",
          "link": "http://arxiv.org/abs/2107.08346",
          "publishedOn": "2021-07-20T02:04:46.804Z",
          "wordCount": null,
          "title": "Policy Optimization in Adversarial MDPs: Improved Exploration via Dilated Bonuses. (arXiv:2107.08346v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meiyazhagan_J/0/1/0/all/0/1\">J.Meiyazhagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudharsan_S/0/1/0/all/0/1\">S. Sudharsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senthilvelan_M/0/1/0/all/0/1\">M. Senthilvelan</a>",
          "description": "We predict the emergence of extreme events in a parametrically driven\nnonlinear dynamical system using three Deep Learning models, namely Multi-Layer\nPerceptron, Convolutional Neural Network and Long Short-Term Memory. The Deep\nLearning models are trained using the training set and are allowed to predict\nthe test set data. After prediction, the time series of the actual and the\npredicted values are plotted one over the other in order to visualize the\nperformance of the models. Upon evaluating the Root Mean Square Error value\nbetween predicted and the actual values of all three models, we find that the\nLong Short-Term Memory model can serve as the best model to forecast the\nchaotic time series and to predict the emergence of extreme events for the\nconsidered system.",
          "link": "http://arxiv.org/abs/2107.08819",
          "publishedOn": "2021-07-20T02:04:46.804Z",
          "wordCount": null,
          "title": "Model-free prediction of emergence of extreme events in a parametrically driven nonlinear dynamical system by Deep Learning. (arXiv:2107.08819v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.09478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parunandi_K/0/1/0/all/0/1\">Karthikeya S. Parunandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Aayushman Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_R/0/1/0/all/0/1\">Raman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravorty_S/0/1/0/all/0/1\">Suman Chakravorty</a>",
          "description": "The problem of Reinforcement Learning (RL) in an unknown nonlinear dynamical\nsystem is equivalent to the search for an optimal feedback law utilizing the\nsimulations/ rollouts of the unknown dynamical system. Most RL techniques\nsearch over a complex global nonlinear feedback parametrization making them\nsuffer from high training times as well as variance. Instead, we advocate\nsearching over a local feedback representation consisting of an open-loop\nsequence, and an associated optimal linear feedback law completely determined\nby the open-loop. We show that this alternate approach results in highly\nefficient training, the answers obtained are repeatable and hence reliable, and\nthe resulting closed performance is superior to global state-of-the-art RL\ntechniques. Finally, if we replan, whenever required, which is feasible due to\nthe fast and reliable local solution, allows us to recover global optimality of\nthe resulting feedback law.",
          "link": "http://arxiv.org/abs/2002.09478",
          "publishedOn": "2021-07-20T02:04:46.803Z",
          "wordCount": null,
          "title": "On the Search for Feedback in Reinforcement Learning. (arXiv:2002.09478v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.02463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mounir_R/0/1/0/all/0/1\">Ramy Mounir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gula_R/0/1/0/all/0/1\">Roman Gula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theuerkauf_J/0/1/0/all/0/1\">J&#xf6;rn Theuerkauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Sudeep Sarkar</a>",
          "description": "Using offline training schemes, researchers have tackled the event\nsegmentation problem by providing full or weak-supervision through manually\nannotated labels or self-supervised epoch-based training. Most works consider\nvideos that are at most 10's of minutes long. We present a self-supervised\nperceptual prediction framework capable of temporal event segmentation by\nbuilding stable representations of objects over time and demonstrate it on long\nvideos, spanning several days. The approach is deceptively simple but quite\neffective. We rely on predictions of high-level features computed by a standard\ndeep learning backbone. For prediction, we use an LSTM, augmented with an\nattention mechanism, trained in a self-supervised manner using the prediction\nerror. The self-learned attention maps effectively localize and track the\nevent-related objects in each frame. The proposed approach does not require\nlabels. It requires only a single pass through the video, with no separate\ntraining set. Given the lack of datasets of very long videos, we demonstrate\nour method on video from 10 days (254 hours) of continuous wildlife monitoring\ndata that we had collected with required permissions. We find that the approach\nis robust to various environmental conditions such as day/night conditions,\nrain, sharp shadows, and windy conditions. For the task of temporally locating\nevents, we had an 80% recall rate at 20% false-positive rate for frame-level\nsegmentation. At the activity level, we had an 80% activity recall rate for one\nfalse activity detection every 50 minutes. We will make the dataset, which is\nthe first of its kind, and the code available to the research community.",
          "link": "http://arxiv.org/abs/2005.02463",
          "publishedOn": "2021-07-20T02:04:46.803Z",
          "wordCount": null,
          "title": "Spatio-Temporal Event Segmentation and Localization for Wildlife Extended Videos. (arXiv:2005.02463v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07458",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tianyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chenyou Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuturi_M/0/1/0/all/0/1\">Marco Cuturi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Projection robust Wasserstein (PRW) distance, or Wasserstein projection\npursuit (WPP), is a robust variant of the Wasserstein distance. Recent work\nsuggests that this quantity is more robust than the standard Wasserstein\ndistance, in particular when comparing probability measures in high-dimensions.\nHowever, it is ruled out for practical application because the optimization\nmodel is essentially non-convex and non-smooth which makes the computation\nintractable. Our contribution in this paper is to revisit the original\nmotivation behind WPP/PRW, but take the hard route of showing that, despite its\nnon-convexity and lack of nonsmoothness, and even despite some hardness results\nproved by~\\citet{Niles-2019-Estimation} in a minimax sense, the original\nformulation for PRW/WPP \\textit{can} be efficiently computed in practice using\nRiemannian optimization, yielding in relevant cases better behavior than its\nconvex relaxation. More specifically, we provide three simple algorithms with\nsolid theoretical guarantee on their complexity bound (one in the appendix),\nand demonstrate their effectiveness and efficiency by conducing extensive\nexperiments on synthetic and real data. This paper provides a first step into a\ncomputational theory of the PRW distance and provides the links between optimal\ntransport and Riemannian optimization.",
          "link": "http://arxiv.org/abs/2006.07458",
          "publishedOn": "2021-07-20T02:04:46.802Z",
          "wordCount": null,
          "title": "Projection Robust Wasserstein Distance and Riemannian Optimization. (arXiv:2006.07458v8 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.00038",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Birmpa_P/0/1/0/all/0/1\">Panagiota Birmpa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Katsoulakis_M/0/1/0/all/0/1\">Markos A. Katsoulakis</a>",
          "description": "We present an information-based uncertainty quantification method for general\nMarkov Random Fields. Markov Random Fields (MRF) are structured, probabilistic\ngraphical models over undirected graphs, and provide a fundamental unifying\nmodeling tool for statistical mechanics, probabilistic machine learning, and\nartificial intelligence. Typically MRFs are complex and high-dimensional with\nnodes and edges (connections) built in a modular fashion from simpler,\nlow-dimensional probabilistic models and their local connections; in turn, this\nmodularity allows to incorporate available data to MRFs and efficiently\nsimulate them by leveraging their graph-theoretic structure. Learning graphical\nmodels from data and/or constructing them from physical modeling and\nconstraints necessarily involves uncertainties inherited from data, modeling\nchoices, or numerical approximations. These uncertainties in the MRF can be\nmanifested either in the graph structure or the probability distribution\nfunctions, and necessarily will propagate in predictions for quantities of\ninterest. Here we quantify such uncertainties using tight, information based\nbounds on the predictions of quantities of interest; these bounds take\nadvantage of the graphical structure of MRFs and are capable of handling the\ninherent high-dimensionality of such graphical models. We demonstrate our\nmethods in MRFs for medical diagnostics and statistical mechanics models. In\nthe latter, we develop uncertainty quantification bounds for finite size\neffects and phase diagrams, which constitute two of the typical predictions\ngoals of statistical mechanics modeling.",
          "link": "http://arxiv.org/abs/2009.00038",
          "publishedOn": "2021-07-20T02:04:46.801Z",
          "wordCount": null,
          "title": "Uncertainty quantification for Markov Random Fields. (arXiv:2009.00038v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_G/0/1/0/all/0/1\">Gautam Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carnahan_M/0/1/0/all/0/1\">Mason Carnahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamapant_S/0/1/0/all/0/1\">Shilpa Shamapant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surendranath_Y/0/1/0/all/0/1\">Yashitha Surendranath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saumya Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Arundhati Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1\">Co Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millan_J/0/1/0/all/0/1\">Jose del R Millan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1\">Ahmed H Tewfik</a>",
          "description": "In this paper, we propose a deep learning-based algorithm to improve the\nperformance of automatic speech recognition (ASR) systems for aphasia, apraxia,\nand dysarthria speech by utilizing electroencephalography (EEG) features\nrecorded synchronously with aphasia, apraxia, and dysarthria speech. We\ndemonstrate a significant decoding performance improvement by more than 50\\%\nduring test time for isolated speech recognition task and we also provide\npreliminary results indicating performance improvement for the more challenging\ncontinuous speech recognition task by utilizing EEG features. The results\npresented in this paper show the first step towards demonstrating the\npossibility of utilizing non-invasive neural signals to design a real-time\nrobust speech prosthetic for stroke survivors recovering from aphasia, apraxia,\nand dysarthria. Our aphasia, apraxia, and dysarthria speech-EEG data set will\nbe released to the public to help further advance this interesting and crucial\nresearch.",
          "link": "http://arxiv.org/abs/2103.00383",
          "publishedOn": "2021-07-20T02:04:46.801Z",
          "wordCount": null,
          "title": "Brain Signals to Rescue Aphasia, Apraxia and Dysarthria Speech Recognition. (arXiv:2103.00383v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jack Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1\">Raphael Lenain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1\">Udeepa Meepegama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1\">Emil Fristed</a>",
          "description": "We introduce ParaBLEU, a paraphrase representation learning model and\nevaluation metric for text generation. Unlike previous approaches, ParaBLEU\nlearns to understand paraphrasis using generative conditioning as a pretraining\nobjective. ParaBLEU correlates more strongly with human judgements than\nexisting metrics, obtaining new state-of-the-art results on the 2017 WMT\nMetrics Shared Task. We show that our model is robust to data scarcity,\nexceeding previous state-of-the-art performance using only $50\\%$ of the\navailable training data and surpassing BLEU, ROUGE and METEOR with only $40$\nlabelled examples. Finally, we demonstrate that ParaBLEU can be used to\nconditionally generate novel paraphrases from a single demonstration, which we\nuse to confirm our hypothesis that it learns abstract, generalized paraphrase\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.08251",
          "publishedOn": "2021-07-20T02:04:46.799Z",
          "wordCount": null,
          "title": "Generative Pretraining for Paraphrase Evaluation. (arXiv:2107.08251v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Ting Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lijun Zhang</a>",
          "description": "Fairness is crucial for neural networks which are used in applications with\nimportant societal implication. Recently, there have been multiple attempts on\nimproving fairness of neural networks, with a focus on fairness testing (e.g.,\ngenerating individual discriminatory instances) and fairness training (e.g.,\nenhancing fairness through augmented training). In this work, we propose an\napproach to formally verify neural networks against fairness, with a focus on\nindependence-based fairness such as group fairness. Our method is built upon an\napproach for learning Markov Chains from a user-provided neural network (i.e.,\na feed-forward neural network or a recurrent neural network) which is\nguaranteed to facilitate sound analysis. The learned Markov Chain not only\nallows us to verify (with Probably Approximate Correctness guarantee) whether\nthe neural network is fair or not, but also facilities sensitivity analysis\nwhich helps to understand why fairness is violated. We demonstrate that with\nour analysis results, the neural weights can be optimized to improve fairness.\nOur approach has been evaluated with multiple models trained on benchmark\ndatasets and the experiment results show that our approach is effective and\nefficient.",
          "link": "http://arxiv.org/abs/2107.08362",
          "publishedOn": "2021-07-20T02:04:46.799Z",
          "wordCount": null,
          "title": "Probabilistic Verification of Neural Networks Against Group Fairness. (arXiv:2107.08362v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.11068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhengping Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>",
          "description": "In deep model compression, the recent finding \"Lottery Ticket Hypothesis\"\n(LTH) (Frankle & Carbin, 2018) pointed out that there could exist a winning\nticket (i.e., a properly pruned sub-network together with original weight\ninitialization) that can achieve competitive performance than the original\ndense network. However, it is not easy to observe such winning property in many\nscenarios, where for example, a relatively large learning rate is used even if\nit benefits training the original dense model. In this work, we investigate the\nunderlying condition and rationale behind the winning property, and find that\nthe underlying reason is largely attributed to the correlation between\ninitialized weights and final-trained weights when the learning rate is not\nsufficiently large. Thus, the existence of winning property is correlated with\nan insufficient DNN pretraining, and is unlikely to occur for a well-trained\nDNN. To overcome this limitation, we propose the \"pruning & fine-tuning\" method\nthat consistently outperforms lottery ticket sparse training under the same\npruning algorithm and the same total training epochs. Extensive experiments\nover multiple deep models (VGG, ResNet, MobileNet-v2) on different datasets\nhave been conducted to justify our proposals.",
          "link": "http://arxiv.org/abs/2102.11068",
          "publishedOn": "2021-07-20T02:04:46.798Z",
          "wordCount": null,
          "title": "Lottery Ticket Preserves Weight Correlation: Is It Desirable or Not?. (arXiv:2102.11068v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08850",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ganz_J/0/1/0/all/0/1\">Jonathan Ganz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kirsch_T/0/1/0/all/0/1\">Tobias Kirsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_L/0/1/0/all/0/1\">Lucas Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bertram_C/0/1/0/all/0/1\">Christof A. Bertram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_C/0/1/0/all/0/1\">Christoph Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blumcke_I/0/1/0/all/0/1\">Ingmar Bl&#xfc;mcke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jabari_S/0/1/0/all/0/1\">Samir Jabari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aubreville_M/0/1/0/all/0/1\">Marc Aubreville</a>",
          "description": "Meningioma is one of the most prevalent brain tumors in adults. To determine\nits malignancy, it is graded by a pathologist into three grades according to\nWHO standards. This grade plays a decisive role in treatment, and yet may be\nsubject to inter-rater discordance. In this work, we present and compare three\napproaches towards fully automatic meningioma grading from histology whole\nslide images. All approaches are following a two-stage paradigm, where we first\nidentify a region of interest based on the detection of mitotic figures in the\nslide using a state-of-the-art object detection deep learning network. This\nregion of highest mitotic rate is considered characteristic for biological\ntumor behavior. In the second stage, we calculate a score corresponding to\ntumor malignancy based on information contained in this region using three\ndifferent settings. In a first approach, image patches are sampled from this\nregion and regression is based on morphological features encoded by a\nResNet-based network. We compare this to learning a logistic regression from\nthe determined mitotic count, an approach which is easily traceable and\nexplainable. Lastly, we combine both approaches in a single network. We trained\nthe pipeline on 951 slides from 341 patients and evaluated them on a separate\nset of 141 slides from 43 patients. All approaches yield a high correlation to\nthe WHO grade. The logistic regression and the combined approach had the best\nresults in our experiments, yielding correct predictions in 32 and 33 of all\ncases, respectively, with the image-based approach only predicting 25 cases\ncorrectly. Spearman's correlation was 0.716, 0.792 and 0.790 respectively. It\nmay seem counterintuitive at first that morphological features provided by\nimage patches do not improve model performance. Yet, this mirrors the criteria\nof the grading scheme, where mitotic count is the only unequivocal parameter.",
          "link": "http://arxiv.org/abs/2107.08850",
          "publishedOn": "2021-07-20T02:04:46.797Z",
          "wordCount": null,
          "title": "Automatic and explainable grading of meningiomas from histopathology images. (arXiv:2107.08850v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jack Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1\">Raphael Lenain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1\">Udeepa Meepegama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1\">Emil Fristed</a>",
          "description": "We propose a method for learning de-identified prosody representations from\nraw audio using a contrastive self-supervised signal. Whereas prior work has\nrelied on conditioning models on bottlenecks, we introduce a set of inductive\nbiases that exploit the natural structure of prosody to minimize timbral\ninformation and decouple prosody from speaker representations. Despite\naggressive downsampling of the input and having no access to linguistic\ninformation, our model performs comparably to state-of-the-art speech\nrepresentations on DAMMP, a new benchmark we introduce for spoken language\nunderstanding. We use minimum description length probing to show that our\nrepresentations have selectively learned the subcomponents of non-timbral\nprosody, and that the product quantizer naturally disentangles them without\nusing bottlenecks. We derive an information-theoretic definition of speech\nde-identifiability and use it to demonstrate that our prosody representations\nare less identifiable than other speech representations.",
          "link": "http://arxiv.org/abs/2107.08248",
          "publishedOn": "2021-07-20T02:04:46.796Z",
          "wordCount": null,
          "title": "Learning De-identified Representations of Prosody from Raw Audio. (arXiv:2107.08248v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Alan Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_H/0/1/0/all/0/1\">Hugo Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Sungsu Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozuno_T/0/1/0/all/0/1\">Tadashi Kozuno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1\">A. Rupam Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1\">Martha White</a>",
          "description": "Approximate Policy Iteration (API) algorithms alternate between (approximate)\npolicy evaluation and (approximate) greedification. Many different approaches\nhave been explored for approximate policy evaluation, but less is understood\nabout approximate greedification and what choices guarantee policy improvement.\nIn this work, we investigate approximate greedification when reducing the KL\ndivergence between the parameterized policy and the Boltzmann distribution over\naction values. In particular, we investigate the difference between the forward\nand reverse KL divergences, with varying degrees of entropy regularization. We\nshow that the reverse KL has stronger policy improvement guarantees, but that\nreducing the forward KL can result in a worse policy. We also demonstrate,\nhowever, that a large enough reduction of the forward KL can induce improvement\nunder additional assumptions. Empirically, we show on simple continuous-action\nenvironments that the forward KL can induce more exploration, but at the cost\nof a more suboptimal policy. No significant differences were observed in the\ndiscrete-action setting or on a suite of benchmark problems. Throughout, we\nhighlight that many policy gradient methods can be seen as an instance of API,\nwith either the forward or reverse KL for the policy update, and discuss next\nsteps for understanding and improving our policy optimization algorithms.",
          "link": "http://arxiv.org/abs/2107.08285",
          "publishedOn": "2021-07-20T02:04:46.795Z",
          "wordCount": null,
          "title": "Greedification Operators for Policy Optimization: Investigating Forward and Reverse KL Divergences. (arXiv:2107.08285v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1\">Jiandong Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengdi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feiwen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>",
          "description": "Recently, neural network compression schemes like channel pruning have been\nwidely used to reduce the model size and computational complexity of deep\nneural network (DNN) for applications in power-constrained scenarios such as\nembedded systems. Reinforcement learning (RL)-based auto-pruning has been\nfurther proposed to automate the DNN pruning process to avoid expensive\nhand-crafted work. However, the RL-based pruner involves a time-consuming\ntraining process and the high expense of each sample further exacerbates this\nproblem. These impediments have greatly restricted the real-world application\nof RL-based auto-pruning. Thus, in this paper, we propose an efficient\nauto-pruning framework which solves this problem by taking advantage of the\nhistorical data from the previous auto-pruning process. In our framework, we\nfirst boost the convergence of the RL-pruner by transfer learning. Then, an\naugmented transfer learning scheme is proposed to further speed up the training\nprocess by improving the transferability. Finally, an assistant learning\nprocess is proposed to improve the sample efficiency of the RL agent. The\nexperiments have shown that our framework can accelerate the auto-pruning\nprocess by 1.5-2.5 times for ResNet20, and 1.81-2.375 times for other neural\nnetworks like ResNet56, ResNet18, and MobileNet v1.",
          "link": "http://arxiv.org/abs/2107.08815",
          "publishedOn": "2021-07-20T02:04:46.794Z",
          "wordCount": null,
          "title": "Boosting the Convergence of Reinforcement Learning-based Auto-pruning Using Historical Data. (arXiv:2107.08815v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fotakis_D/0/1/0/all/0/1\">Dimitris Fotakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gergatsouli_E/0/1/0/all/0/1\">Evangelia Gergatsouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouleakis_T/0/1/0/all/0/1\">Themis Gouleakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patris_N/0/1/0/all/0/1\">Nikolas Patris</a>",
          "description": "Following the research agenda initiated by Munoz & Vassilvitskii [1] and\nLykouris & Vassilvitskii [2] on learning-augmented online algorithms for\nclassical online optimization problems, in this work, we consider the Online\nFacility Location problem under this framework. In Online Facility Location\n(OFL), demands arrive one-by-one in a metric space and must be (irrevocably)\nassigned to an open facility upon arrival, without any knowledge about future\ndemands.\n\nWe present an online algorithm for OFL that exploits potentially imperfect\npredictions on the locations of the optimal facilities. We prove that the\ncompetitive ratio decreases smoothly from sublogarithmic in the number of\ndemands to constant, as the error, i.e., the total distance of the predicted\nlocations to the optimal facility locations, decreases towards zero. We\ncomplement our analysis with a matching lower bound establishing that the\ndependence of the algorithm's competitive ratio on the error is optimal, up to\nconstant factors. Finally, we evaluate our algorithm on real world data and\ncompare our learning augmented approach with the current best online algorithm\nfor the problem.",
          "link": "http://arxiv.org/abs/2107.08277",
          "publishedOn": "2021-07-20T02:04:46.793Z",
          "wordCount": null,
          "title": "Learning Augmented Online Facility Location. (arXiv:2107.08277v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Triet H. M. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huaming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babar_M/0/1/0/all/0/1\">M. Ali Babar</a>",
          "description": "Software Vulnerabilities (SVs) are increasing in complexity and scale, posing\ngreat security risks to many software systems. Given the limited resources in\npractice, SV assessment and prioritization help practitioners devise optimal SV\nmitigation plans based on various SV characteristics. The surge in SV data\nsources and data-driven techniques such as Machine Learning and Deep Learning\nhave taken SV assessment and prioritization to the next level. Our survey\nprovides a taxonomy of the past research efforts and highlights the best\npractices for data-driven SV assessment and prioritization. We also discuss the\ncurrent limitations and propose potential solutions to address such issues.",
          "link": "http://arxiv.org/abs/2107.08364",
          "publishedOn": "2021-07-20T02:04:46.792Z",
          "wordCount": null,
          "title": "A Survey on Data-driven Software Vulnerability Assessment and Prioritization. (arXiv:2107.08364v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sushant Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabbari_S/0/1/0/all/0/1\">Shahin Jabbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Sohini Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiwei Steven Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>",
          "description": "As machine learning black boxes are increasingly being deployed in critical\ndomains such as healthcare and criminal justice, there has been a growing\nemphasis on developing techniques for explaining these black boxes in a post\nhoc manner. In this work, we analyze two popular post hoc interpretation\ntechniques: SmoothGrad which is a gradient based method, and a variant of LIME\nwhich is a perturbation based method. More specifically, we derive explicit\nclosed form expressions for the explanations output by these two methods and\nshow that they both converge to the same explanation in expectation, i.e., when\nthe number of perturbed samples used by these methods is large. We then\nleverage this connection to establish other desirable properties, such as\nrobustness, for these techniques. We also derive finite sample complexity\nbounds for the number of perturbations required for these methods to converge\nto their expected explanation. Finally, we empirically validate our theory\nusing extensive experimentation on both synthetic and real world datasets.",
          "link": "http://arxiv.org/abs/2102.10618",
          "publishedOn": "2021-07-20T02:04:46.792Z",
          "wordCount": null,
          "title": "Towards the Unification and Robustness of Perturbation and Gradient Based Explanations. (arXiv:2102.10618v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feriani_A/0/1/0/all/0/1\">Amal Feriani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mezghani_A/0/1/0/all/0/1\">Amine Mezghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1\">Ekram Hossain</a>",
          "description": "We consider an Intelligent Reflecting Surface (IRS)-aided multiple-input\nsingle-output (MISO) system for downlink transmission. We compare the\nperformance of Deep Reinforcement Learning (DRL) and conventional optimization\nmethods in finding optimal phase shifts of the IRS elements to maximize the\nuser signal-to-noise (SNR) ratio. Furthermore, we evaluate the robustness of\nthese methods to channel impairments and changes in the system. We demonstrate\nnumerically that DRL solutions show more robustness to noisy channels and user\nmobility.",
          "link": "http://arxiv.org/abs/2107.08293",
          "publishedOn": "2021-07-20T02:04:46.791Z",
          "wordCount": null,
          "title": "On the Robustness of Deep Reinforcement Learning in IRS-Aided Wireless Communications Systems. (arXiv:2107.08293v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_K/0/1/0/all/0/1\">Karishma Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1\">Emilio Ferrara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>",
          "description": "Identifying and characterizing disinformation in political discourse on\nsocial media is critical to ensure the integrity of elections and democratic\nprocesses around the world. Persistent manipulation of social media has\nresulted in increased concerns regarding the 2020 U.S. Presidential Election,\ndue to its potential to influence individual opinions and social dynamics. In\nthis work, we focus on the identification of distorted facts, in the form of\nunreliable and conspiratorial narratives in election-related tweets, to\ncharacterize discourse manipulation prior to the election. We apply a detection\nmodel to separate factual from unreliable (or conspiratorial) claims analyzing\na dataset of 242 million election-related tweets. The identified claims are\nused to investigate targeted topics of disinformation, and conspiracy groups,\nmost notably the far-right QAnon conspiracy group. Further, we characterize\naccount engagements with unreliable and conspiracy tweets, and with the QAnon\nconspiracy group, by political leaning and tweet types. Finally, using a\nregression discontinuity design, we investigate whether Twitter's actions to\ncurb QAnon activity on the platform were effective, and how QAnon accounts\nadapt to Twitter's restrictions.",
          "link": "http://arxiv.org/abs/2107.08319",
          "publishedOn": "2021-07-20T02:04:46.791Z",
          "wordCount": null,
          "title": "Characterizing Online Engagement with Disinformation and Conspiracies in the 2020 U.S. Presidential Election. (arXiv:2107.08319v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2009.06342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paassen_B/0/1/0/all/0/1\">Benjamin Paa&#xdf;en</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_A/0/1/0/all/0/1\">Alexander Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stewart_T/0/1/0/all/0/1\">Terrence C. Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1\">Barbara Hammer</a>",
          "description": "Differentiable neural computers extend artificial neural networks with an\nexplicit memory without interference, thus enabling the model to perform\nclassic computation tasks such as graph traversal. However, such models are\ndifficult to train, requiring long training times and large datasets. In this\nwork, we achieve some of the computational capabilities of differentiable\nneural computers with a model that can be trained very efficiently, namely an\necho state network with an explicit memory without interference. This extension\nenables echo state networks to recognize all regular languages, including those\nthat contractive echo state networks provably can not recognize. Further, we\ndemonstrate experimentally that our model performs comparably to its\nfully-trained deep version on several typical benchmark tasks for\ndifferentiable neural computers.",
          "link": "http://arxiv.org/abs/2009.06342",
          "publishedOn": "2021-07-20T02:04:46.774Z",
          "wordCount": null,
          "title": "Reservoir Memory Machines as Neural Computers. (arXiv:2009.06342v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.06070",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laval_J/0/1/0/all/0/1\">Jorge Laval</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_A/0/1/0/all/0/1\">Anye Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Wenchao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qing_Z/0/1/0/all/0/1\">Zhu Qing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peeta_S/0/1/0/all/0/1\">Srinivas Peeta</a>",
          "description": "Self-driving technology companies and the research community are accelerating\ntheir pace to use machine learning longitudinal motion planning (mMP) for\nautonomous vehicles (AVs). This paper reviews the current state of the art in\nmMP, with an exclusive focus on its impact on traffic congestion. We identify\nthe availability of congestion scenarios in current datasets, and summarize the\nrequired features for training mMP. For learning methods, we survey the major\nmethods in both imitation learning and non-imitation learning. We also\nhighlight the emerging technologies adopted by some leading AV companies, e.g.\nTesla, Waymo, and Comma.ai. We find that: i) the AV industry has been mostly\nfocusing on the long tail problem related to safety and overlooked the impact\non traffic congestion, ii) the current public self-driving datasets have not\nincluded enough congestion scenarios, and mostly lack the necessary input\nfeatures/output labels to train mMP, and iii) albeit reinforcement learning\n(RL) approach can integrate congestion mitigation into the learning goal, the\nmajor mMP method adopted by industry is still behavior cloning (BC), whose\ncapability to learn a congestion-mitigating mMP remains to be seen. Based on\nthe review, the study identifies the research gaps in current mMP development.\nSome suggestions towards congestion mitigation for future mMP studies are\nproposed: i) enrich data collection to facilitate the congestion learning, ii)\nincorporate non-imitation learning methods to combine traffic efficiency into a\nsafety-oriented technical route, and iii) integrate domain knowledge from the\ntraditional car following (CF) theory to improve the string stability of mMP.",
          "link": "http://arxiv.org/abs/1910.06070",
          "publishedOn": "2021-07-20T02:04:46.773Z",
          "wordCount": null,
          "title": "Review of Learning-based Longitudinal Motion Planning for Autonomous Vehicles: Research Gaps between Self-driving and Traffic Congestion. (arXiv:1910.06070v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shibata_K/0/1/0/all/0/1\">Katsunari Shibata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ejima_T/0/1/0/all/0/1\">Takuya Ejima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tokumaru_Y/0/1/0/all/0/1\">Yuki Tokumaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuki_T/0/1/0/all/0/1\">Toshitaka Matsuki</a>",
          "description": "Here, we introduce a fully local index named \"sensitivity\" for each neuron to\ncontrol chaoticity or gradient globally in a neural network (NN). We also\npropose a learning method to adjust it named \"sensitivity adjustment learning\n(SAL)\". The index is the gradient magnitude of its output with respect to its\ninputs. By adjusting its time average to 1.0 in each neuron, information\ntransmission in the neuron changes to be moderate without shrinking or\nexpanding for both forward and backward computations. That results in moderate\ninformation transmission through a layer of neurons when the weights and inputs\nare random. Therefore, SAL can control the chaoticity of the network dynamics\nin a recurrent NN (RNN). It can also solve the vanishing gradient problem in\nerror backpropagation (BP) learning in a deep feedforward NN or an RNN. We\ndemonstrate that when applying SAL to an RNN with small and random initial\nweights, log-sensitivity, which is the logarithm of RMS (root mean square)\nsensitivity over all the neurons, is equivalent to the maximum Lyapunov\nexponent until it reaches 0.0. We also show that SAL works with BP or BPTT (BP\nthrough time) to avoid the vanishing gradient problem in a 300-layer NN or an\nRNN that learns a problem with a lag of 300 steps between the first input and\nthe output. Compared with manually fine-tuning the spectral radius of the\nweight matrix before learning, SAL's continuous nonlinear learning nature\nprevents loss of sensitivities during learning, resulting in a significant\nimprovement in learning performance.",
          "link": "http://arxiv.org/abs/2012.13134",
          "publishedOn": "2021-07-20T02:04:46.773Z",
          "wordCount": null,
          "title": "Sensitivity -- Local Index to Control Chaoticity or Gradient Globally. (arXiv:2012.13134v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08865",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yunfan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1\">David Hsu</a>",
          "description": "This paper presents Particle-based Object Manipulation (Prompt), a new\napproach to robot manipulation of novel objects ab initio, without prior object\nmodels or pre-training on a large object data set. The key element of Prompt is\na particle-based object representation, in which each particle represents a\npoint in the object, the local geometric, physical, and other features of the\npoint, and also its relation with other particles. Like the model-based\nanalytic approaches to manipulation, the particle representation enables the\nrobot to reason about the object's geometry and dynamics in order to choose\nsuitable manipulation actions. Like the data-driven approaches, the particle\nrepresentation is learned online in real-time from visual sensor input,\nspecifically, multi-view RGB images. The particle representation thus connects\nvisual perception with robot control. Prompt combines the benefits of both\nmodel-based reasoning and data-driven learning. We show empirically that Prompt\nsuccessfully handles a variety of everyday objects, some of which are\ntransparent. It handles various manipulation tasks, including grasping,\npushing, etc,. Our experiments also show that Prompt outperforms a\nstate-of-the-art data-driven grasping method on the daily objects, even though\nit does not use any offline training data.",
          "link": "http://arxiv.org/abs/2107.08865",
          "publishedOn": "2021-07-20T02:04:46.772Z",
          "wordCount": null,
          "title": "Ab Initio Particle-based Object Manipulation. (arXiv:2107.08865v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_D/0/1/0/all/0/1\">Divya Shanmugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabanian_S/0/1/0/all/0/1\">Samira Shabanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finck_M/0/1/0/all/0/1\">Mich&#xe8;le Finck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biega_A/0/1/0/all/0/1\">Asia Biega</a>",
          "description": "Data minimization is a legal obligation defined in the European Union's\nGeneral Data Protection Regulation (GDPR) as the responsibility to process an\nadequate, relevant, and limited amount of personal data in relation to a\nprocessing purpose. However, unlike fairness or transparency, the principle has\nnot seen wide adoption for machine learning systems due to a lack of\ncomputational interpretation. In this paper, we build on literature in machine\nlearning and law to propose the first learning framework for limiting data\ncollection based on an interpretation that ties the data collection purpose to\nsystem performance. We formalize a data minimization criterion based on\nperformance curve derivatives and provide an effective and interpretable\npiecewise power law technique that models distinct stages of an algorithm's\nperformance throughout data collection. Results from our empirical\ninvestigation offer deeper insights into the relevant considerations when\ndesigning a data minimization framework, including the choice of feature\nacquisition algorithm, initialization conditions, as well as impacts on\nindividuals that hint at tensions between data minimization and fairness.",
          "link": "http://arxiv.org/abs/2107.08096",
          "publishedOn": "2021-07-20T02:04:46.699Z",
          "wordCount": null,
          "title": "Learning to Limit Data Collection via Scaling Laws: Data Minimization Compliance in Practice. (arXiv:2107.08096v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elflein_S/0/1/0/all/0/1\">Sven Elflein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charpentier_B/0/1/0/all/0/1\">Bertrand Charpentier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zugner_D/0/1/0/all/0/1\">Daniel Z&#xfc;gner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "Several density estimation methods have shown to fail to detect\nout-of-distribution (OOD) samples by assigning higher likelihoods to anomalous\ndata. Energy-based models (EBMs) are flexible, unnormalized density models\nwhich seem to be able to improve upon this failure mode. In this work, we\nprovide an extensive study investigating OOD detection with EBMs trained with\ndifferent approaches on tabular and image data and find that EBMs do not\nprovide consistent advantages. We hypothesize that EBMs do not learn semantic\nfeatures despite their discriminative structure similar to Normalizing Flows.\nTo verify this hypotheses, we show that supervision and architectural\nrestrictions improve the OOD detection of EBMs independent of the training\napproach.",
          "link": "http://arxiv.org/abs/2107.08785",
          "publishedOn": "2021-07-20T02:04:46.698Z",
          "wordCount": null,
          "title": "On Out-of-distribution Detection with Energy-based Models. (arXiv:2107.08785v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chepurko_N/0/1/0/all/0/1\">Nadiia Chepurko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarkson_K/0/1/0/all/0/1\">Kenneth L. Clarkson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kacham_P/0/1/0/all/0/1\">Praneeth Kacham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1\">David P. Woodruff</a>",
          "description": "Currently, in the numerical linear algebra community, it is thought that to\nobtain nearly-optimal bounds for various problems such as rank computation,\nfinding a maximal linearly independent subset of columns, regression, low rank\napproximation, maximum matching on general graphs and linear matroid union, one\nwould need to resolve the main open question of Nelson and Nguyen (FOCS, 2013)\nregarding the logarithmic factors in the sketching dimension for existing\nconstant factor approximation oblivious subspace embeddings. We show how to\nbypass this question using a refined sketching technique, and obtain optimal or\nnearly optimal bounds for these problems. A key technique we use is an explicit\nmapping of Indyk based on uncertainty principles and extractors, which after\nfirst applying known oblivious subspace embeddings, allows us to quickly spread\nout the mass of the vector so that sampling is now effective, and we avoid a\nlogarithmic factor that is standard in the sketching dimension resulting from\nmatrix Chernoff bounds. For the fundamental problems of rank computation and\nfinding a linearly independent subset of columns, our algorithms improve\nCheung, Kwok, and Lau (JACM, 2013) and are optimal to within a constant factor\nand a $\\log\\log(n)$-factor, respectively. Further, for constant factor\nregression and low rank approximation we give the first optimal algorithms, for\nthe current matrix multiplication exponent.",
          "link": "http://arxiv.org/abs/2107.08090",
          "publishedOn": "2021-07-20T02:04:46.697Z",
          "wordCount": null,
          "title": "Near-Optimal Algorithms for Linear Algebra in the Current Matrix Multiplication Time. (arXiv:2107.08090v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">JaeYoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_J/0/1/0/all/0/1\">Junyu Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Christy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_F/0/1/0/all/0/1\">Farookh Hussain</a>",
          "description": "The high-dimensional or sparse reward task of a reinforcement learning (RL)\nenvironment requires a superior potential controller such as hierarchical\nreinforcement learning (HRL) rather than an atomic RL because it absorbs the\ncomplexity of commands to achieve the purpose of the task in its hierarchical\nstructure. One of the HRL issues is how to train each level policy with the\noptimal data collection from its experience. That is to say, how to synchronize\nadjacent level policies optimally. Our research finds that a HRL model through\nthe off-policy correction technique of HRL, which trains a higher-level policy\nwith the goal of reflecting a lower-level policy which is newly trained using\nthe off-policy method, takes the critical role of synchronizing both level\npolicies at all times while they are being trained. We propose a novel HRL\nmodel supporting the optimal level synchronization using the off-policy\ncorrection technique with a deep generative model. This uses the advantage of\nthe inverse operation of a flow-based deep generative model (FDGM) to achieve\nthe goal corresponding to the current state of the lower-level policy. The\nproposed model also considers the freedom of the goal dimension between HRL\npolicies which makes it the generalized inverse model of the model-free RL in\nHRL with the optimal synchronization method. The comparative experiment results\nshow the performance of our proposed model.",
          "link": "http://arxiv.org/abs/2107.08183",
          "publishedOn": "2021-07-20T02:04:46.690Z",
          "wordCount": null,
          "title": "Hierarchical Reinforcement Learning with Optimal Level Synchronization based on a Deep Generative Model. (arXiv:2107.08183v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lui_K/0/1/0/all/0/1\">Kry Yik Chau Lui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Leal_P/0/1/0/all/0/1\">Pablo Hernandez-Leal</a>",
          "description": "Trading markets represent a real-world financial application to deploy\nreinforcement learning agents, however, they carry hard fundamental challenges\nsuch as high variance and costly exploration. Moreover, markets are inherently\na multiagent domain composed of many actors taking actions and changing the\nenvironment. To tackle these type of scenarios agents need to exhibit certain\ncharacteristics such as risk-awareness, robustness to perturbations and low\nlearning variance. We take those as building blocks and propose a family of\nfour algorithms. First, we contribute with two algorithms that use risk-averse\nobjective functions and variance reduction techniques. Then, we augment the\nframework to multi-agent learning and assume an adversary which can take over\nand perturb the learning process. Our third and fourth algorithms perform well\nunder this setting and balance theoretical guarantees with practical use.\nAdditionally, we consider the multi-agent nature of the environment and our\nwork is the first one extending empirical game theory analysis for multi-agent\nlearning by considering risk-sensitive payoffs.",
          "link": "http://arxiv.org/abs/2107.08083",
          "publishedOn": "2021-07-20T02:04:46.689Z",
          "wordCount": null,
          "title": "Robust Risk-Sensitive Reinforcement Learning Agents for Trading Markets. (arXiv:2107.08083v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1810.03024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheung_W/0/1/0/all/0/1\">Wang Chi Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simchi_Levi_D/0/1/0/all/0/1\">David Simchi-Levi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Ruihao Zhu</a>",
          "description": "We introduce algorithms that achieve state-of-the-art \\emph{dynamic regret}\nbounds for non-stationary linear stochastic bandit setting. It captures natural\napplications such as dynamic pricing and ads allocation in a changing\nenvironment. We show how the difficulty posed by the non-stationarity can be\novercome by a novel marriage between stochastic and adversarial bandits\nlearning algorithms. Defining $d,B_T,$ and $T$ as the problem dimension, the\n\\emph{variation budget}, and the total time horizon, respectively, our main\ncontributions are the tuned Sliding Window UCB (\\texttt{SW-UCB}) algorithm with\noptimal $\\widetilde{O}(d^{2/3}(B_T+1)^{1/3}T^{2/3})$ dynamic regret, and the\ntuning free bandit-over-bandit (\\texttt{BOB}) framework built on top of the\n\\texttt{SW-UCB} algorithm with best\n$\\widetilde{O}(d^{2/3}(B_T+1)^{1/4}T^{3/4})$ dynamic regret.",
          "link": "http://arxiv.org/abs/1810.03024",
          "publishedOn": "2021-07-20T02:04:46.689Z",
          "wordCount": null,
          "title": "Learning to Optimize under Non-Stationarity. (arXiv:1810.03024v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1\">Anderson da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "This works proposes a methodology to searching for automatically Artificial\nNeural Networks (ANN) by using Cellular Genetic Algorithm (CGA). The goal of\nthis methodology is to find compact networks whit good performance for\nclassification problems. The main reason for developing this work is centered\nat the difficulties of configuring compact ANNs with good performance rating.\nThe use of CGAs aims at seeking the components of the RNA in the same way that\na common Genetic Algorithm (GA), but it has the differential of incorporating a\nCellular Automaton (CA) to give location for the GA individuals. The location\nimposed by the CA aims to control the spread of solutions in the populations to\nmaintain the genetic diversity for longer time. This genetic diversity is\nimportant for obtain good results with the GAs.",
          "link": "http://arxiv.org/abs/2107.08326",
          "publishedOn": "2021-07-20T02:04:46.687Z",
          "wordCount": null,
          "title": "Otimizacao de Redes Neurais atraves de Algoritmos Geneticos Celulares. (arXiv:2107.08326v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peiris_V/0/1/0/all/0/1\">Vinesha Peiris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhorukova_N/0/1/0/all/0/1\">Nadezda Sukhorukova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roshchina_V/0/1/0/all/0/1\">Vera Roshchina</a>",
          "description": "We explore the potential for using a nonsmooth loss function based on the\nmax-norm in the training of an artificial neural network. We hypothesise that\nthis may lead to superior classification results in some special cases where\nthe training data is either very small or unbalanced.\n\nOur numerical experiments performed on a simple artificial neural network\nwith no hidden layers (a setting immediately amenable to standard nonsmooth\noptimisation techniques) appear to confirm our hypothesis that uniform\napproximation based approaches may be more suitable for the datasets with\nreliable training data that either is limited size or biased in terms of\nrelative cluster sizes.",
          "link": "http://arxiv.org/abs/2107.08800",
          "publishedOn": "2021-07-20T02:04:46.686Z",
          "wordCount": null,
          "title": "Deep Learning with Nonsmooth Objectives. (arXiv:2107.08800v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1\">Rong Pan</a>",
          "description": "Recurrence data arise from multi-disciplinary domains spanning reliability,\ncyber security, healthcare, online retailing, etc. This paper investigates an\nadditive-tree-based approach, known as Boost-R (Boosting for Recurrence Data),\nfor recurrent event data with both static and dynamic features. Boost-R\nconstructs an ensemble of gradient boosted additive trees to estimate the\ncumulative intensity function of the recurrent event process, where a new tree\nis added to the ensemble by minimizing the regularized L2 distance between the\nobserved and predicted cumulative intensity. Unlike conventional regression\ntrees, a time-dependent function is constructed by Boost-R on each tree leaf.\nThe sum of these functions, from multiple trees, yields the ensemble estimator\nof the cumulative intensity. The divide-and-conquer nature of tree-based\nmethods is appealing when hidden sub-populations exist within a heterogeneous\npopulation. The non-parametric nature of regression trees helps to avoid\nparametric assumptions on the complex interactions between event processes and\nfeatures. Critical insights and advantages of Boost-R are investigated through\ncomprehensive numerical examples. Datasets and computer code of Boost-R are\nmade available on GitHub. To our best knowledge, Boost-R is the first gradient\nboosted additive-tree-based approach for modeling large-scale recurrent event\ndata with both static and dynamic feature information.",
          "link": "http://arxiv.org/abs/2107.08784",
          "publishedOn": "2021-07-20T02:04:46.685Z",
          "wordCount": null,
          "title": "Boost-R: Gradient Boosted Trees for Recurrence Data. (arXiv:2107.08784v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Navarro_C/0/1/0/all/0/1\">Carlos Mougan Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanellos_G/0/1/0/all/0/1\">Georgios Kanellos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottron_T/0/1/0/all/0/1\">Thomas Gottron</a>",
          "description": "Explainable AI constitutes a fundamental step towards establishing fairness\nand addressing bias in algorithmic decision-making. Despite the large body of\nwork on the topic, the benefit of solutions is mostly evaluated from a\nconceptual or theoretical point of view and the usefulness for real-world use\ncases remains uncertain. In this work, we aim to state clear user-centric\ndesiderata for explainable AI reflecting common explainability needs\nexperienced in statistical production systems of the European Central Bank. We\nlink the desiderata to archetypical user roles and give examples of techniques\nand methods which can be used to address the user's needs. To this end, we\nprovide two concrete use cases from the domain of statistical data production\nin central banks: the detection of outliers in the Centralised Securities\nDatabase and the data-driven identification of data quality checks for the\nSupervisory Banking data system.",
          "link": "http://arxiv.org/abs/2107.08045",
          "publishedOn": "2021-07-20T02:04:46.586Z",
          "wordCount": null,
          "title": "Desiderata for Explainable AI in statistical production systems of the European Central Bank. (arXiv:2107.08045v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanchao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feriani_A/0/1/0/all/0/1\">Amal Feriani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1\">Ekram Hossain</a>",
          "description": "Multi-Agent Reinforcement Learning (MARL) is a challenging subarea of\nReinforcement Learning due to the non-stationarity of the environments and the\nlarge dimensionality of the combined action space. Deep MARL algorithms have\nbeen applied to solve different task offloading problems. However, in\nreal-world applications, information required by the agents (i.e. rewards and\nstates) are subject to noise and alterations. The stability and the robustness\nof deep MARL to practical challenges is still an open research problem. In this\nwork, we apply state-of-the art MARL algorithms to solve task offloading with\nreward uncertainty. We show that perturbations in the reward signal can induce\ndecrease in the performance compared to learning with perfect rewards. We\nexpect this paper to stimulate more research in studying and addressing the\npractical challenges of deploying deep MARL solutions in wireless\ncommunications systems.",
          "link": "http://arxiv.org/abs/2107.08114",
          "publishedOn": "2021-07-20T02:04:46.584Z",
          "wordCount": null,
          "title": "Decentralized Multi-Agent Reinforcement Learning for Task Offloading Under Uncertainty. (arXiv:2107.08114v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guevara_J/0/1/0/all/0/1\">Jorge Guevara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borges_D/0/1/0/all/0/1\">Dario Borges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watson_C/0/1/0/all/0/1\">Campbell Watson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadrozny_B/0/1/0/all/0/1\">Bianca Zadrozny</a>",
          "description": "Future climate change scenarios are usually hypothesized using simulations\nfrom weather generators. However, there only a few works comparing and\nevaluating promising deep learning models for weather generation against\nclassical approaches. This study shows preliminary results making such\nevaluations for the multisite precipitation synthesis task. We compared two\nopen-source weather generators: IBMWeathergen (an extension of the Weathergen\nlibrary) and RGeneratePrec, and two deep generative models: GAN and VAE, on a\nvariety of metrics. Our preliminary results can serve as a guide for improving\nthe design of deep learning architectures and algorithms for the multisite\nprecipitation synthesis task.",
          "link": "http://arxiv.org/abs/2107.08074",
          "publishedOn": "2021-07-20T02:04:46.581Z",
          "wordCount": null,
          "title": "A comparative study of stochastic and deep generative models for multisite precipitation synthesis. (arXiv:2107.08074v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bartz_E/0/1/0/all/0/1\">Eva Bartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaefferer_M/0/1/0/all/0/1\">Martin Zaefferer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mersmann_O/0/1/0/all/0/1\">Olaf Mersmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartz_Beielstein_T/0/1/0/all/0/1\">Thomas Bartz-Beielstein</a>",
          "description": "Machine learning algorithms such as random forests or xgboost are gaining\nmore importance and are increasingly incorporated into production processes in\norder to enable comprehensive digitization and, if possible, automation of\nprocesses. Hyperparameters of these algorithms used have to be set\nappropriately, which can be referred to as hyperparameter tuning or\noptimization. Based on the concept of tunability, this article presents an\noverview of theoretical and practical results for popular machine learning\nalgorithms. This overview is accompanied by an experimental analysis of 30\nhyperparameters from six relevant machine learning algorithms. In particular,\nit provides (i) a survey of important hyperparameters, (ii) two parameter\ntuning studies, and (iii) one extensive global parameter tuning study, as well\nas (iv) a new way, based on consensus ranking, to analyze results from multiple\nalgorithms. The R package mlr is used as a uniform interface to the machine\nlearning models. The R package SPOT is used to perform the actual tuning\n(optimization). All additional code is provided together with this paper.",
          "link": "http://arxiv.org/abs/2107.08761",
          "publishedOn": "2021-07-20T02:04:46.575Z",
          "wordCount": null,
          "title": "Experimental Investigation and Evaluation of Model-based Hyperparameter Optimization. (arXiv:2107.08761v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08751",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Memmel_M/0/1/0/all/0/1\">Marius Memmel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonzalez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>",
          "description": "Deep learning for medical imaging suffers from temporal and privacy-related\nrestrictions on data availability. To still obtain viable models, continual\nlearning aims to train in sequential order, as and when data is available. The\nmain challenge that continual learning methods face is to prevent catastrophic\nforgetting, i.e., a decrease in performance on the data encountered earlier.\nThis issue makes continuous training of segmentation models for medical\napplications extremely difficult. Yet, often, data from at least two different\ndomains is available which we can exploit to train the model in a way that it\ndisregards domain-specific information. We propose an architecture that\nleverages the simultaneous availability of two or more datasets to learn a\ndisentanglement between the content and domain in an adversarial fashion. The\ndomain-invariant content representation then lays the base for continual\nsemantic segmentation. Our approach takes inspiration from domain adaptation\nand combines it with continual learning for hippocampal segmentation in brain\nMRI. We showcase that our method reduces catastrophic forgetting and\noutperforms state-of-the-art continual learning methods.",
          "link": "http://arxiv.org/abs/2107.08751",
          "publishedOn": "2021-07-20T02:04:46.574Z",
          "wordCount": null,
          "title": "Adversarial Continual Learning for Multi-Domain Hippocampal Segmentation. (arXiv:2107.08751v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Soumyadeep Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjay Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_J/0/1/0/all/0/1\">Janu Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Awanish Kumar</a>",
          "description": "In order to train robust deep learning models, large amounts of labelled data\nis required. However, in the absence of such large repositories of labelled\ndata, unlabeled data can be exploited for the same. Semi-Supervised learning\naims to utilize such unlabeled data for training classification models. Recent\nprogress of self-training based approaches have shown promise in this area,\nwhich leads to this study where we utilize an ensemble approach for the same. A\nby-product of any semi-supervised approach may be loss of calibration of the\ntrained model especially in scenarios where unlabeled data may contain\nout-of-distribution samples, which leads to this investigation on how to adapt\nto such effects. Our proposed algorithm carefully avoids common pitfalls in\nutilizing unlabeled data and leads to a more accurate and calibrated supervised\nmodel compared to vanilla self-training based student-teacher algorithms. We\nperform several experiments on the popular STL-10 database followed by an\nextensive analysis of our approach and study its effects on model accuracy and\ncalibration.",
          "link": "http://arxiv.org/abs/2107.08211",
          "publishedOn": "2021-07-20T02:04:46.557Z",
          "wordCount": null,
          "title": "Self Training with Ensemble of Teacher Models. (arXiv:2107.08211v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yao Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Haipeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Despite being a critical communication skill, grasping humor is challenging\n-- a successful use of humor requires a mixture of both engaging content\nbuild-up and an appropriate vocal delivery (e.g., pause). Prior studies on\ncomputational humor emphasize the textual and audio features immediately next\nto the punchline, yet overlooking longer-term context setup. Moreover, the\ntheories are usually too abstract for understanding each concrete humor\nsnippet. To fill in the gap, we develop DeHumor, a visual analytical system for\nanalyzing humorous behaviors in public speaking. To intuitively reveal the\nbuilding blocks of each concrete example, DeHumor decomposes each humorous\nvideo into multimodal features and provides inline annotations of them on the\nvideo script. In particular, to better capture the build-ups, we introduce\ncontent repetition as a complement to features introduced in theories of\ncomputational humor and visualize them in a context linking graph. To help\nusers locate the punchlines that have the desired features to learn, we\nsummarize the content (with keywords) and humor feature statistics on an\naugmented time matrix. With case studies on stand-up comedy shows and TED\ntalks, we show that DeHumor is able to highlight various building blocks of\nhumor examples. In addition, expert interviews with communication coaches and\nhumor researchers demonstrate the effectiveness of DeHumor for multimodal humor\nanalysis of speech content and vocal delivery.",
          "link": "http://arxiv.org/abs/2107.08356",
          "publishedOn": "2021-07-20T02:04:46.339Z",
          "wordCount": null,
          "title": "DeHumor: Visual Analytics for Decomposing Humor. (arXiv:2107.08356v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.05094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murray_M/0/1/0/all/0/1\">Michael Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanner_J/0/1/0/all/0/1\">Jared Tanner</a>",
          "description": "In its most elementary form, compressed sensing studies the design of\ndecoding algorithms to recover a sufficiently sparse vector or code from a\nlower dimensional linear measurement vector. Typically it is assumed that the\ndecoder has access to the encoder matrix, which in the combinatorial case is\nsparse and binary. In this paper we consider the problem of designing a decoder\nto recover a set of sparse codes from their linear measurements alone, that is\nwithout access to encoder matrix. To this end we study the matrix factorisation\ntask of recovering both the encoder and sparse coding matrices from the\nassociated linear measurement matrix. The contribution of this paper is a\ncomputationally efficient decoding algorithm, Decoder-Expander Based\nFactorisation, with strong performance guarantees. In particular, under mild\nassumptions on the sparse coding matrix and by deploying a novel random encoder\nmatrix, we prove that Decoder-Expander Based Factorisation recovers both the\nencoder and sparse coding matrix at the optimal measurement rate with high\nprobability and from a near optimal number of measurement vectors. In addition,\nour experiments demonstrate the efficacy and computational efficiency of our\nalgorithm in practice. Beyond compressed sensing our results may be of interest\nfor researchers working in areas such as linear sketching, coding theory and\nmatrix compression.",
          "link": "http://arxiv.org/abs/2004.05094",
          "publishedOn": "2021-07-20T02:04:46.339Z",
          "wordCount": null,
          "title": "Encoder blind combinatorial compressed sensing. (arXiv:2004.05094v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_C/0/1/0/all/0/1\">Chirag Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramdas_A/0/1/0/all/0/1\">Aaditya K. Ramdas</a>",
          "description": "We study the problem of post-hoc calibration for multiclass classification,\nwith an emphasis on histogram binning. Multiple works have focused on\ncalibration with respect to the confidence of just the predicted class (or\n'top-label'). We find that the popular notion of confidence calibration [Guo et\nal., 2017] is not sufficiently strong -- there exist predictors that are not\ncalibrated in any meaningful way but are perfectly confidence calibrated. We\npropose a closely related (but subtly different) notion, top-label calibration,\nthat accurately captures the intuition and simplicity of confidence\ncalibration, but addresses its drawbacks. We formalize a histogram binning (HB)\nalgorithm that reduces top-label multiclass calibration to the binary case,\nprove that it has clean theoretical guarantees without distributional\nassumptions, and perform a methodical study of its practical performance. Some\nprediction tasks require stricter notions of multiclass calibration such as\nclass-wise or canonical calibration. We formalize appropriate HB algorithms\ncorresponding to each of these goals. In experiments with deep neural nets, we\nfind that our principled versions of HB are often better than temperature\nscaling, for both top-label and class-wise calibration. Code for this work will\nbe made publicly available at https://github.com/aigen/df-posthoc-calibration.",
          "link": "http://arxiv.org/abs/2107.08353",
          "publishedOn": "2021-07-20T02:04:46.338Z",
          "wordCount": null,
          "title": "Top-label calibration. (arXiv:2107.08353v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.11830",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Richter_L/0/1/0/all/0/1\">Lorenz Richter</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sallandt_L/0/1/0/all/0/1\">Leon Sallandt</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nusken_N/0/1/0/all/0/1\">Nikolas N&#xfc;sken</a>",
          "description": "High-dimensional partial differential equations (PDEs) are ubiquitous in\neconomics, science and engineering. However, their numerical treatment poses\nformidable challenges since traditional grid-based methods tend to be\nfrustrated by the curse of dimensionality. In this paper, we argue that tensor\ntrains provide an appealing approximation framework for parabolic PDEs: the\ncombination of reformulations in terms of backward stochastic differential\nequations and regression-type methods in the tensor format holds the promise of\nleveraging latent low-rank structures enabling both compression and efficient\ncomputation. Following this paradigm, we develop novel iterative schemes,\ninvolving either explicit and fast or implicit and accurate updates. We\ndemonstrate in a number of examples that our methods achieve a favorable\ntrade-off between accuracy and computational efficiency in comparison with\nstate-of-the-art neural network based approaches.",
          "link": "http://arxiv.org/abs/2102.11830",
          "publishedOn": "2021-07-20T02:04:46.338Z",
          "wordCount": null,
          "title": "Solving high-dimensional parabolic PDEs using the tensor train format. (arXiv:2102.11830v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12056",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1\">Changfa Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1\">Min Xian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiancheng Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haotian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_H/0/1/0/all/0/1\">Heng-Da Cheng</a>",
          "description": "Liver segmentation from abdominal CT images is an essential step for liver\ncancer computer-aided diagnosis and surgical planning. However, both the\naccuracy and robustness of existing liver segmentation methods cannot meet the\nrequirements of clinical applications. In particular, for the common clinical\ncases where the liver tissue contains major pathology, current segmentation\nmethods show poor performance. In this paper, we propose a novel low-rank\ntensor decomposition (LRTD) based multi-atlas segmentation (MAS) framework that\nachieves accurate and robust pathological liver segmentation of CT images.\nFirstly, we propose a multi-slice LRTD scheme to recover the underlying\nlow-rank structure embedded in 3D medical images. It performs the LRTD on small\nimage segments consisting of multiple consecutive image slices. Then, we\npresent an LRTD-based atlas construction method to generate tumor-free liver\natlases that mitigates the performance degradation of liver segmentation due to\nthe presence of tumors. Finally, we introduce an LRTD-based MAS algorithm to\nderive patient-specific liver atlases for each test image, and to achieve\naccurate pairwise image registration and label propagation. Extensive\nexperiments on three public databases of pathological liver cases validate the\neffectiveness of the proposed method. Both qualitative and quantitative results\ndemonstrate that, in the presence of major pathology, the proposed method is\nmore accurate and robust than state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2102.12056",
          "publishedOn": "2021-07-20T02:04:46.337Z",
          "wordCount": null,
          "title": "Multi-Slice Low-Rank Tensor Decomposition Based Multi-Atlas Segmentation: Application to Automatic Pathological Liver CT Segmentation. (arXiv:2102.12056v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blanzeisky_W/0/1/0/all/0/1\">William Blanzeisky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunningham_P/0/1/0/all/0/1\">P&#xe1;draig Cunningham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_K/0/1/0/all/0/1\">Kenneth Kennedy</a>",
          "description": "A significant impediment to progress in research on bias in machine learning\n(ML) is the availability of relevant datasets. This situation is unlikely to\nchange much given the sensitivity of such data. For this reason, there is a\nrole for synthetic data in this research. In this short paper, we present one\nsuch family of synthetic data sets. We provide an overview of the data,\ndescribe how the level of bias can be varied, and present a simple example of\nan experiment on the data.",
          "link": "http://arxiv.org/abs/2107.08928",
          "publishedOn": "2021-07-20T02:04:46.336Z",
          "wordCount": null,
          "title": "Introducing a Family of Synthetic Datasets for Research on Bias in Machine Learning. (arXiv:2107.08928v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08273",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hengguan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye Wang</a>",
          "description": "Perception of time from sequentially acquired sensory inputs is rooted in\neveryday behaviors of individual organisms. Yet, most algorithms for\ntime-series modeling fail to learn dynamics of random event timings directly\nfrom visual or audio inputs, requiring timing annotations during training that\nare usually unavailable for real-world applications. For instance, neuroscience\nperspectives on postdiction imply that there exist variable temporal ranges\nwithin which the incoming sensory inputs can affect the earlier perception, but\nsuch temporal ranges are mostly unannotated for real applications such as\nautomatic speech recognition (ASR). In this paper, we present a probabilistic\nordinary differential equation (ODE), called STochastic boundaRy ODE (STRODE),\nthat learns both the timings and the dynamics of time series data without\nrequiring any timing annotations during training. STRODE allows the usage of\ndifferential equations to sample from the posterior point processes,\nefficiently and analytically. We further provide theoretical guarantees on the\nlearning of STRODE. Our empirical results show that our approach successfully\ninfers event timings of time series data. Our method achieves competitive or\nsuperior performances compared to existing state-of-the-art methods for both\nsynthetic and real-world datasets.",
          "link": "http://arxiv.org/abs/2107.08273",
          "publishedOn": "2021-07-20T02:04:46.247Z",
          "wordCount": null,
          "title": "STRODE: Stochastic Boundary Ordinary Differential Equation. (arXiv:2107.08273v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.",
          "link": "http://arxiv.org/abs/2107.08264",
          "publishedOn": "2021-07-20T02:04:46.210Z",
          "wordCount": null,
          "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhe Yu</a>",
          "description": "This paper aims to improve machine learning fairness on multiple protected\nat-tributes. Machine learning fairness has attracted increasing attention since\nmachine learning models are increasingly used for high-stakes and high-risk\ndecisions. Most existing solutions for machine learning fairness only target\none protected attribute(e.g. sex) at a time. These solutions cannot generate a\nmachine learning model which is fair against every protected attribute (e.g.\nboth sex and race) at the same time. To solve this problem, we propose\nFairBalance in this paper to balance the distribution of training data across\nevery protected attribute before training the machine learning models. Our\nresults show that, under the assumption of unbiased ground truth labels,\nFairBalance can significantly reduce bias metrics (AOD, EOD, and SPD) on every\nknown protected attribute without much, if not any damage to the prediction\nperformance.",
          "link": "http://arxiv.org/abs/2107.08310",
          "publishedOn": "2021-07-20T02:04:46.210Z",
          "wordCount": null,
          "title": "Fair Balance: Mitigating Machine Learning Bias Against Multiple Protected Attributes With Data Balancing. (arXiv:2107.08310v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huaimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haibo Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy M. Hospedales</a>",
          "description": "Federated learning (FL) enables distributed participants to collectively\nlearn a strong global model without sacrificing their individual data privacy.\nMainstream FL approaches require each participant to share a common network\narchitecture and further assume that data are are sampled IID across\nparticipants. However, in real-world deployments participants may require\nheterogeneous network architectures; and the data distribution is almost\ncertainly non-uniform across participants. To address these issues we introduce\nFedH2L, which is agnostic to both the model architecture and robust to\ndifferent data distributions across participants. In contrast to approaches\nsharing parameters or gradients, FedH2L relies on mutual distillation,\nexchanging only posteriors on a shared seed set between participants in a\ndecentralized manner. This makes it extremely bandwidth efficient, model\nagnostic, and crucially produces models capable of performing well on the whole\ndata distribution when learning from heterogeneous silos.",
          "link": "http://arxiv.org/abs/2101.11296",
          "publishedOn": "2021-07-20T02:04:46.209Z",
          "wordCount": null,
          "title": "FedH2L: Federated Learning with Model and Statistical Heterogeneity. (arXiv:2101.11296v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1\">Masahiro Kato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teshima_T/0/1/0/all/0/1\">Takeshi Teshima</a>",
          "description": "Density ratio estimation (DRE) is at the core of various machine learning\ntasks such as anomaly detection and domain adaptation. In existing studies on\nDRE, methods based on Bregman divergence (BD) minimization have been\nextensively studied. However, BD minimization when applied with highly flexible\nmodels, such as deep neural networks, tends to suffer from what we call\ntrain-loss hacking, which is a source of overfitting caused by a typical\ncharacteristic of empirical BD estimators. In this paper, to mitigate\ntrain-loss hacking, we propose a non-negative correction for empirical BD\nestimators. Theoretically, we confirm the soundness of the proposed method\nthrough a generalization error bound. Through our experiments, the proposed\nmethods show a favorable performance in inlier-based outlier detection.",
          "link": "http://arxiv.org/abs/2006.06979",
          "publishedOn": "2021-07-20T02:04:46.206Z",
          "wordCount": null,
          "title": "Non-Negative Bregman Divergence Minimization for Deep Direct Density Ratio Estimation. (arXiv:2006.06979v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.02443",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pomponi_J/0/1/0/all/0/1\">Jary Pomponi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scardapane_S/0/1/0/all/0/1\">Simone Scardapane</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Uncini_A/0/1/0/all/0/1\">Aurelio Uncini</a>",
          "description": "Catastrophic forgetting (CF) happens whenever a neural network overwrites\npast knowledge while being trained on new tasks. Common techniques to handle CF\ninclude regularization of the weights (using, e.g., their importance on past\ntasks), and rehearsal strategies, where the network is constantly re-trained on\npast data. Generative models have also been applied for the latter, in order to\nhave endless sources of data. In this paper, we propose a novel method that\ncombines the strengths of regularization and generative-based rehearsal\napproaches. Our generative model consists of a normalizing flow (NF), a\nprobabilistic and invertible neural network, trained on the internal embeddings\nof the network. By keeping a single NF conditioned on the task, we show that\nour memory overhead remains constant. In addition, exploiting the invertibility\nof the NF, we propose a simple approach to regularize the network's embeddings\nwith respect to past tasks. We show that our method performs favorably with\nrespect to state-of-the-art approaches in the literature, with bounded\ncomputational power and memory overheads.",
          "link": "http://arxiv.org/abs/2007.02443",
          "publishedOn": "2021-07-20T02:04:46.205Z",
          "wordCount": null,
          "title": "Pseudo-Rehearsal for Continual Learning with Normalizing Flows. (arXiv:2007.02443v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08209",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tasche_D/0/1/0/all/0/1\">Dirk Tasche</a>",
          "description": "For the binary prevalence quantification problem under prior probability\nshift, we determine the asymptotic variance of the maximum likelihood\nestimator. We find that it is a function of the Brier score for the regression\nof the class label against the features under the test data set distribution.\nThis observation suggests that optimising the accuracy of a base classifier on\nthe training data set helps to reduce the variance of the related quantifier on\nthe test data set. Therefore, we also point out training criteria for the base\nclassifier that imply optimisation of both of the Brier scores on the training\nand the test data sets.",
          "link": "http://arxiv.org/abs/2107.08209",
          "publishedOn": "2021-07-20T02:04:46.127Z",
          "wordCount": null,
          "title": "Minimising quantifier variance under prior probability shift. (arXiv:2107.08209v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03088",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xianghao Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rumeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1\">Xiaoqing Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">You Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhiyuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guang Li</a>",
          "description": "Electronic nose has been proven to be effective in alternative herbal\nmedicine classification, but due to the nature of supervised learning, previous\nresearch heavily relies on the labelled training data, which are time-costly\nand labor-intensive to collect. To alleviate the critical dependency on the\ntraining data in real-world applications, this study aims to improve\nclassification accuracy via data augmentation strategies. The effectiveness of\nfive data augmentation strategies under different training data inadequacy are\ninvestigated in two scenarios: the noise-free scenario where different\navailabilities of unlabelled data were considered, and the noisy scenario where\ndifferent levels of Gaussian noises and translational shifts were added to\nrepresent sensor drifts. The five augmentation strategies, namely noise-adding\ndata augmentation, semi-supervised learning, classifier-based online learning,\nInductive Conformal Prediction (ICP) online learning and our novel ensemble ICP\nonline learning proposed in this study, are experimented and compared against\nsupervised learning baseline, with Linear Discriminant Analysis (LDA) and\nSupport Vector Machine (SVM) as the classifiers. Our novel strategy, ensemble\nICP online learning, outperforms the others by showing non-decreasing\nclassification accuracy on all tasks and a significant improvement on most\nsimulated tasks (25out of 36 tasks,p<=0.05). Furthermore, this study provides a\nsystematic analysis of different augmentation strategies. It shows at least one\nstrategy significantly improved the classification accuracy with LDA (p<=0.05)\nand non-decreasing classification accuracy with SVM in each task. In\nparticular, our proposed strategy demonstrated both effectiveness and\nrobustness in boosting the classification model generalizability, which can be\nemployed in other machine learning applications.",
          "link": "http://arxiv.org/abs/2102.03088",
          "publishedOn": "2021-07-20T02:04:46.115Z",
          "wordCount": null,
          "title": "Boost AI Power: Data Augmentation Strategies with unlabelled Data and Conformal Prediction, a Case in Alternative Herbal Medicine Discrimination with Electronic Nose. (arXiv:2102.03088v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12301",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lin_T/0/1/0/all/0/1\">Tianyi Lin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zheng_Z/0/1/0/all/0/1\">Zeyu Zheng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_E/0/1/0/all/0/1\">Elynn Y. Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cuturi_M/0/1/0/all/0/1\">Marco Cuturi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Optimal transport (OT) distances are increasingly used as loss functions for\nstatistical inference, notably in the learning of generative models or\nsupervised learning. Yet, the behavior of minimum Wasserstein estimators is\npoorly understood, notably in high-dimensional regimes or under model\nmisspecification. In this work we adopt the viewpoint of projection robust (PR)\nOT, which seeks to maximize the OT cost between two measures by choosing a\n$k$-dimensional subspace onto which they can be projected. Our first\ncontribution is to establish several fundamental statistical properties of PR\nWasserstein distances, complementing and improving previous literature that has\nbeen restricted to one-dimensional and well-specified cases. Next, we propose\nthe integral PR Wasserstein (IPRW) distance as an alternative to the PRW\ndistance, by averaging rather than optimizing on subspaces. Our complexity\nbounds can help explain why both PRW and IPRW distances outperform Wasserstein\ndistances empirically in high-dimensional inference tasks. Finally, we consider\nparametric inference using the PRW distance. We provide an asymptotic guarantee\nof two types of minimum PRW estimators and formulate a central limit theorem\nfor max-sliced Wasserstein estimator under model misspecification. To enable\nour analysis on PRW with projection dimension larger than one, we devise a\nnovel combination of variational analysis and statistical theory.",
          "link": "http://arxiv.org/abs/2006.12301",
          "publishedOn": "2021-07-20T02:04:46.114Z",
          "wordCount": null,
          "title": "On Projection Robust Optimal Transport: Sample Complexity and Model Misspecification. (arXiv:2006.12301v5 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ashesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1\">Luca Del Pero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1\">Hugo Grimmett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1\">Peter Ondruska</a>",
          "description": "Despite the numerous successes of machine learning over the past decade\n(image recognition, decision-making, NLP, image synthesis), self-driving\ntechnology has not yet followed the same trend. In this paper, we study the\nhistory, composition, and development bottlenecks of the modern self-driving\nstack. We argue that the slow progress is caused by approaches that require too\nmuch hand-engineering, an over-reliance on road testing, and high fleet\ndeployment costs. We observe that the classical stack has several bottlenecks\nthat preclude the necessary scale needed to capture the long tail of rare\nevents. To resolve these problems, we outline the principles of Autonomy 2.0,\nan ML-first approach to self-driving, as a viable alternative to the currently\nadopted state-of-the-art. This approach is based on (i) a fully differentiable\nAV stack trainable from human demonstrations, (ii) closed-loop data-driven\nreactive simulation, and (iii) large-scale, low-cost data collections as\ncritical solutions towards scalability issues. We outline the general\narchitecture, survey promising works in this direction and propose key\nchallenges to be addressed by the community in the future.",
          "link": "http://arxiv.org/abs/2107.08142",
          "publishedOn": "2021-07-20T02:04:46.113Z",
          "wordCount": null,
          "title": "Autonomy 2.0: Why is self-driving always 5 years away?. (arXiv:2107.08142v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08179",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Birmpa_P/0/1/0/all/0/1\">Panagiota Birmpa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Feng_J/0/1/0/all/0/1\">Jinchao Feng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Katsoulakis_M/0/1/0/all/0/1\">Markos A. Katsoulakis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rey_Bellet_L/0/1/0/all/0/1\">Luc Rey-Bellet</a>",
          "description": "Probabilistic graphical models are a fundamental tool in probabilistic\nmodeling, machine learning and artificial intelligence. They allow us to\nintegrate in a natural way expert knowledge, physical modeling, heterogeneous\nand correlated data and quantities of interest. For exactly this reason,\nmultiple sources of model uncertainty are inherent within the modular structure\nof the graphical model. In this paper we develop information-theoretic, robust\nuncertainty quantification methods and non-parametric stress tests for directed\ngraphical models to assess the effect and the propagation through the graph of\nmulti-sourced model uncertainties to quantities of interest. These methods\nallow us to rank the different sources of uncertainty and correct the graphical\nmodel by targeting its most impactful components with respect to the quantities\nof interest. Thus, from a machine learning perspective, we provide a\nmathematically rigorous approach to correctability that guarantees a systematic\nselection for improvement of components of a graphical model while controlling\npotential new errors created in the process in other parts of the model. We\ndemonstrate our methods in two physico-chemical examples, namely quantum\nscale-informed chemical kinetics and materials screening to improve the\nefficiency of fuel cells.",
          "link": "http://arxiv.org/abs/2107.08179",
          "publishedOn": "2021-07-20T02:04:46.106Z",
          "wordCount": null,
          "title": "Model Uncertainty and Correctability for Directed Graphical Models. (arXiv:2107.08179v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samo_Y/0/1/0/all/0/1\">Yves-Laurent Kom Samo</a>",
          "description": "We introduce the first application of the lean methodology to machine\nlearning projects. Similar to lean startups and lean manufacturing, we argue\nthat lean machine learning (LeanML) can drastically slash avoidable wastes in\ncommercial machine learning projects, reduce the business risk in investing in\nmachine learning capabilities and, in so doing, further democratize access to\nmachine learning. The lean design pattern we propose in this paper is based on\ntwo realizations. First, it is possible to estimate the best performance one\nmay achieve when predicting an outcome $y \\in \\mathcal{Y}$ using a given set of\nexplanatory variables $x \\in \\mathcal{X}$, for a wide range of performance\nmetrics, and without training any predictive model. Second, doing so is\nconsiderably easier, faster, and cheaper than learning the best predictive\nmodel. We derive formulae expressing the best $R^2$, MSE, classification\naccuracy, and log-likelihood per observation achievable when using $x$ to\npredict $y$ as a function of the mutual information $I\\left(y; x\\right)$, and\npossibly a measure of the variability of $y$ (e.g. its Shannon entropy in the\ncase of classification accuracy, and its variance in the case regression MSE).\nWe illustrate the efficacy of the LeanML design pattern on a wide range of\nregression and classification problems, synthetic and real-life.",
          "link": "http://arxiv.org/abs/2107.08066",
          "publishedOn": "2021-07-20T02:04:46.095Z",
          "wordCount": null,
          "title": "LeanML: A Design Pattern To Slash Avoidable Wastes in Machine Learning Projects. (arXiv:2107.08066v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">J. G. Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gluzman_M/0/1/0/all/0/1\">Mark Gluzman</a>",
          "description": "The policy improvement bound on the difference of the discounted returns\nplays a crucial role in the theoretical justification of the trust-region\npolicy optimization (TRPO) algorithm. The existing bound leads to a degenerate\nbound when the discount factor approaches one, making the applicability of TRPO\nand related algorithms questionable when the discount factor is close to one.\nWe refine the results in \\cite{Schulman2015, Achiam2017} and propose a novel\nbound that is \"continuous\" in the discount factor. In particular, our bound is\napplicable for MDPs with the long-run average rewards as well.",
          "link": "http://arxiv.org/abs/2107.08068",
          "publishedOn": "2021-07-20T02:04:46.094Z",
          "wordCount": null,
          "title": "Refined Policy Improvement Bounds for MDPs. (arXiv:2107.08068v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Holderrieth_P/0/1/0/all/0/1\">Peter Holderrieth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutchinson_M/0/1/0/all/0/1\">Michael Hutchinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>",
          "description": "Motivated by objects such as electric fields or fluid streams, we study the\nproblem of learning stochastic fields, i.e. stochastic processes whose samples\nare fields like those occurring in physics and engineering. Considering general\ntransformations such as rotations and reflections, we show that spatial\ninvariance of stochastic fields requires an inference model to be equivariant.\nLeveraging recent advances from the equivariance literature, we study\nequivariance in two classes of models. Firstly, we fully characterise\nequivariant Gaussian processes. Secondly, we introduce Steerable Conditional\nNeural Processes (SteerCNPs), a new, fully equivariant member of the Neural\nProcess family. In experiments with Gaussian process vector fields, images, and\nreal-world weather data, we observe that SteerCNPs significantly improve the\nperformance of previous models and equivariance leads to improvements in\ntransfer learning tasks.",
          "link": "http://arxiv.org/abs/2011.12916",
          "publishedOn": "2021-07-20T02:04:45.372Z",
          "wordCount": null,
          "title": "Equivariant Learning of Stochastic Fields: Gaussian Processes and Steerable Conditional Neural Processes. (arXiv:2011.12916v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tengyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yingbin Liang</a>",
          "description": "Designing off-policy reinforcement learning algorithms is typically a very\nchallenging task, because a desirable iteration update often involves an\nexpectation over an on-policy distribution. Prior off-policy actor-critic (AC)\nalgorithms have introduced a new critic that uses the density ratio for\nadjusting the distribution mismatch in order to stabilize the convergence, but\nat the cost of potentially introducing high biases due to the estimation errors\nof both the density ratio and value function. In this paper, we develop a\ndoubly robust off-policy AC (DR-Off-PAC) for discounted MDP, which can take\nadvantage of learned nuisance functions to reduce estimation errors. Moreover,\nDR-Off-PAC adopts a single timescale structure, in which both actor and critics\nare updated simultaneously with constant stepsize, and is thus more sample\nefficient than prior algorithms that adopt either two timescale or nested-loop\nstructure. We study the finite-time convergence rate and characterize the\nsample complexity for DR-Off-PAC to attain an $\\epsilon$-accurate optimal\npolicy. We also show that the overall convergence of DR-Off-PAC is doubly\nrobust to the approximation errors that depend only on the expressive power of\napproximation functions. To the best of our knowledge, our study establishes\nthe first overall sample complexity analysis for a single time-scale off-policy\nAC algorithm.",
          "link": "http://arxiv.org/abs/2102.11866",
          "publishedOn": "2021-07-20T02:04:45.244Z",
          "wordCount": null,
          "title": "Doubly Robust Off-Policy Actor-Critic: Convergence and Optimality. (arXiv:2102.11866v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eren_M/0/1/0/all/0/1\">Maksim E. Eren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solovyev_N/0/1/0/all/0/1\">Nick Solovyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamer_C/0/1/0/all/0/1\">Chris Hamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1\">Renee McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexandrov_B/0/1/0/all/0/1\">Boian S. Alexandrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicholas_C/0/1/0/all/0/1\">Charles Nicholas</a>",
          "description": "The unprecedented outbreak of Severe Acute Respiratory Syndrome Coronavirus-2\n(SARS-CoV-2), or COVID-19, continues to be a significant worldwide problem. As\na result, a surge of new COVID-19 related research has followed suit. The\ngrowing number of publications requires document organization methods to\nidentify relevant information. In this paper, we expand upon our previous work\nwith clustering the CORD-19 dataset by applying multi-dimensional analysis\nmethods. Tensor factorization is a powerful unsupervised learning method\ncapable of discovering hidden patterns in a document corpus. We show that a\nhigher-order representation of the corpus allows for the simultaneous grouping\nof similar articles, relevant journals, authors with similar research\ninterests, and topic keywords. These groupings are identified within and among\nthe latent components extracted via tensor decomposition. We further\ndemonstrate the application of this method with a publicly available\ninteractive visualization of the dataset.",
          "link": "http://arxiv.org/abs/2107.08190",
          "publishedOn": "2021-07-20T02:04:44.973Z",
          "wordCount": null,
          "title": "COVID-19 Multidimensional Kaggle Literature Organization. (arXiv:2107.08190v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chrostoforidis_A/0/1/0/all/0/1\">Aristeidis Chrostoforidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyriakides_G/0/1/0/all/0/1\">George Kyriakides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Margaritis_K/0/1/0/all/0/1\">Konstantinos Margaritis</a>",
          "description": "In this work, we propose a novel evolutionary algorithm for neural\narchitecture search, applicable to global search spaces. The algorithm's\narchitectural representation organizes the topology in multiple hierarchical\nmodules, while the design process exploits this representation, in order to\nexplore the search space. We also employ a curation system, which promotes the\nutilization of well performing sub-structures to subsequent generations. We\napply our method to Fashion-MNIST and NAS-Bench101, achieving accuracies of\n$93.2\\%$ and $94.8\\%$ respectively in a relatively small number of generations.",
          "link": "http://arxiv.org/abs/2107.08484",
          "publishedOn": "2021-07-20T02:04:44.663Z",
          "wordCount": 519,
          "title": "A Novel Evolutionary Algorithm for Hierarchical Neural Architecture Search. (arXiv:2107.08484v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08649",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lim_D/0/1/0/all/0/1\">Dong-Young Lim</a>, <a href=\"http://arxiv.org/find/math/1/au:+Neufeld_A/0/1/0/all/0/1\">Ariel Neufeld</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sabanis_S/0/1/0/all/0/1\">Sotirios Sabanis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>",
          "description": "We consider non-convex stochastic optimization problems where the objective\nfunctions have super-linearly growing and discontinuous stochastic gradients.\nIn such a setting, we provide a non-asymptotic analysis for the tamed\nunadjusted stochastic Langevin algorithm (TUSLA) introduced in Lovas et al.\n(2021). In particular, we establish non-asymptotic error bounds for the TUSLA\nalgorithm in Wasserstein-1 and Wasserstein-2 distances. The latter result\nenables us to further derive non-asymptotic estimates for the expected excess\nrisk. To illustrate the applicability of the main results, we consider an\nexample from transfer learning with ReLU neural networks, which represents a\nkey paradigm in machine learning. Numerical experiments are presented for the\naforementioned example which supports our theoretical findings. Hence, in this\nsetting, we demonstrate both theoretically and numerically that the TUSLA\nalgorithm can solve the optimization problem involving neural networks with\nReLU activation function. Besides, we provide simulation results for synthetic\nexamples where popular algorithms, e.g. ADAM, AMSGrad, RMSProp, and (vanilla)\nSGD, may fail to find the minimizer of the objective functions due to the\nsuper-linear growth and the discontinuity of the corresponding stochastic\ngradient, while the TUSLA algorithm converges rapidly to the optimal solution.",
          "link": "http://arxiv.org/abs/2107.08649",
          "publishedOn": "2021-07-20T02:04:44.607Z",
          "wordCount": 655,
          "title": "Non-asymptotic estimates for TUSLA algorithm for non-convex learning with applications to neural networks with ReLU activation function. (arXiv:2107.08649v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huafeng Wang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chonggang Lu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhimin Hu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaodong Yuan</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingshu Zhang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanquan Liu</a> (3) ((1) School of Information, North China University of Technology,(2) Department of Neurology, Kailuan General Hospital, Tangshan,(3) School of Intelligent Systems Engineering, Sun Yat-sen University)",
          "description": "Sleep staging assumes an important role in the diagnosis of sleep disorders.\nIn general, experts classify sleep stages manually based on polysomnography\n(PSG), which is quite time-consuming. Meanwhile, the acquisition of multiple\nsignals is complex, which can affect the subject's sleep. Therefore, the use of\nsingle-channel electroencephalogram (EEG) for automatic sleep staging has\nbecome mainstream. In the literature, a large number of sleep staging methods\nbased on single-channel EEG have been proposed with good results and realize\nthe preliminary automation of sleep staging. However, the performance for most\nof these methods in the N1 stage is generally not high. In this paper, we\npropose a deep learning model SDAN based on raw EEG. The method utilises a\none-dimensional convolutional neural network (CNN) to automatically extract\nfeatures from raw EEG. It serially combines the channel attention and spatial\nattention mechanisms to filter and highlight key information and then uses soft\nthreshold to eliminate redundant information. Additionally, we introduce a\nresidual network to avoid degradation problems caused by network deepening.\nExperiments were conducted using two datasets with 5-fold cross-validation and\nhold-out validation method. The final average accuracy, overall accuracy, macro\nF1 score and Cohen's Kappa coefficient of the model reach 96.74%, 91.86%,\n82.64% and 0.8742 on the Sleep-EDF dataset, and 95.98%, 89.96%, 79.08% and\n0.8216 on the Sleep-EDFx dataset. Significantly, our model performed superiorly\nin the N1 stage, with F1 scores of 54.08% and 52.49% on the two datasets\nrespectively. The results show the superiority of our network over the best\nexisting methods, reaching a new state-of-the-art. In particular, the present\nmethod achieves excellent results in the N1 sleep stage compared to other\nmethods.",
          "link": "http://arxiv.org/abs/2107.08442",
          "publishedOn": "2021-07-20T02:04:44.589Z",
          "wordCount": 744,
          "title": "Sleep Staging Based on Serialized Dual Attention Network. (arXiv:2107.08442v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mrabah_N/0/1/0/all/0/1\">Nairouz Mrabah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouguessa_M/0/1/0/all/0/1\">Mohamed Bouguessa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Touati_M/0/1/0/all/0/1\">Mohamed Fawzi Touati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ksantini_R/0/1/0/all/0/1\">Riadh Ksantini</a>",
          "description": "Most recent graph clustering methods have resorted to Graph Auto-Encoders\n(GAEs) to perform joint clustering and embedding learning. However, two\ncritical issues have been overlooked. First, the accumulative error, inflicted\nby learning with noisy clustering assignments, degrades the effectiveness and\nrobustness of the clustering model. This problem is called Feature Randomness.\nSecond, reconstructing the adjacency matrix sets the model to learn irrelevant\nsimilarities for the clustering task. This problem is called Feature Drift.\nInterestingly, the theoretical relation between the aforementioned problems has\nnot yet been investigated. We study these issues from two aspects: (1) the\nexistence of a trade-off between Feature Randomness and Feature Drift when\nclustering and reconstruction are performed at the same level, and (2) the\nproblem of Feature Drift is more pronounced for GAE models, compared with\nvanilla auto-encoder models, due to the graph convolutional operation and the\ngraph decoding design. Motivated by these findings, we reformulate the\nGAE-based clustering methodology. Our solution is two-fold. First, we propose a\nsampling operator $\\Xi$ that triggers a protection mechanism against the noisy\nclustering assignments. Second, we propose an operator $\\Upsilon$ that triggers\na correction mechanism against Feature Drift by gradually transforming the\nreconstructed graph into a clustering-oriented one. As principal advantages,\nour solution grants a considerable improvement in clustering effectiveness and\nrobustness and can be easily tailored to existing GAE models.",
          "link": "http://arxiv.org/abs/2107.08562",
          "publishedOn": "2021-07-20T02:04:44.571Z",
          "wordCount": 653,
          "title": "Rethinking Graph Autoencoder Models for Attributed Graph Clustering. (arXiv:2107.08562v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08593",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Wang_Y/0/1/0/all/0/1\">Yiran Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>",
          "description": "In this work, we use an explainable convolutional neural network (NLS-Net) to\nsolve an inverse problem of the nonlinear Schr\\\"odinger equation, which is\nwidely used in fiber-optic communications. The landscape and minimizers of the\nnon-convex loss function of the learning problem are studied empirically. It\nprovides a guidance for choosing hyper-parameters of the method. The estimation\nerror of the optimal solution is discussed in terms of expressive power of the\nNLS-Net and data. Besides, we compare the performance of several training\nalgorithms that are popular in deep learning. It is shown that one can obtain a\nrelatively accurate estimate of the considered parameters using the proposed\nmethod. The study provides a natural framework of solving inverse problems of\nnonlinear partial differential equations with deep learning.",
          "link": "http://arxiv.org/abs/2107.08593",
          "publishedOn": "2021-07-20T02:04:44.554Z",
          "wordCount": 566,
          "title": "Inverse Problem of Nonlinear Schr\\\"odinger Equation as Learning of Convolutional Neural Network. (arXiv:2107.08593v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Peng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiabao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xuemin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Libin Zheng</a>",
          "description": "With the rapid development of smart mobile devices, the car-hailing platforms\n(e.g., Uber or Lyft) have attracted much attention from both the academia and\nthe industry. In this paper, we consider an important dynamic car-hailing\nproblem, namely \\textit{maximum revenue vehicle dispatching} (MRVD), in which\nrider requests dynamically arrive and drivers need to serve as many riders as\npossible such that the entire revenue of the platform is maximized. We prove\nthat the MRVD problem is NP-hard and intractable. In addition, the dynamic\ncar-hailing platforms have no information of the future riders, which makes the\nproblem even harder. To handle the MRVD problem, we propose a queueing-based\nvehicle dispatching framework, which first uses existing machine learning\nalgorithms to predict the future vehicle demand of each region, then estimates\nthe idle time periods of drivers through a queueing model for each region. With\nthe information of the predicted vehicle demands and estimated idle time\nperiods of drivers, we propose two batch-based vehicle dispatching algorithms\nto efficiently assign suitable drivers to riders such that the expected overall\nrevenue of the platform is maximized during each batch processing. Through\nextensive experiments, we demonstrate the efficiency and effectiveness of our\nproposed approaches over both real and synthetic datasets.",
          "link": "http://arxiv.org/abs/2107.08662",
          "publishedOn": "2021-07-20T02:04:44.536Z",
          "wordCount": 649,
          "title": "A Queueing-Theoretic Framework for Vehicle Dispatching in Dynamic Car-Hailing [technical report]. (arXiv:2107.08662v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Momeni_A/0/1/0/all/0/1\">Ali Momeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleury_R/0/1/0/all/0/1\">Romain Fleury</a>",
          "description": "Wave-based analog signal processing holds the promise of extremely fast,\non-the-fly, power-efficient data processing, occurring as a wave propagates\nthrough an artificially engineered medium. Yet, due to the fundamentally weak\nnon-linearities of traditional wave materials, such analog processors have been\nso far largely confined to simple linear projections such as image edge\ndetection or matrix multiplications. Complex neuromorphic computing tasks,\nwhich inherently require strong non-linearities, have so far remained\nout-of-reach of wave-based solutions, with a few attempts that implemented\nnon-linearities on the digital front, or used weak and inflexible non-linear\nsensors, restraining the learning performance. Here, we tackle this issue by\ndemonstrating the relevance of Time-Floquet physics to induce a strong\nnon-linear entanglement between signal inputs at different frequencies,\nenabling a power-efficient and versatile wave platform for analog extreme deep\nlearning involving a single, uniformly modulated dielectric layer and a\nscattering medium. We prove the efficiency of the method for extreme learning\nmachines and reservoir computing to solve a range of challenging learning\ntasks, from forecasting chaotic time series to the simultaneous classification\nof distinct datasets. Our results open the way for wave-based machine learning\nwith high energy efficiency, speed, and scalability.",
          "link": "http://arxiv.org/abs/2107.08564",
          "publishedOn": "2021-07-20T02:04:44.473Z",
          "wordCount": 643,
          "title": "Wave-based extreme deep learning based on non-linear time-Floquet entanglement. (arXiv:2107.08564v1 [cs.ET])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Podda_M/0/1/0/all/0/1\">Marco Podda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>",
          "description": "The problem of labeled graph generation is gaining attention in the Deep\nLearning community. The task is challenging due to the sparse and discrete\nnature of graph spaces. Several approaches have been proposed in the\nliterature, most of which require to transform the graphs into sequences that\nencode their structure and labels and to learn the distribution of such\nsequences through an auto-regressive generative model. Among this family of\napproaches, we focus on the GraphGen model. The preprocessing phase of GraphGen\ntransforms graphs into unique edge sequences called Depth-First Search (DFS)\ncodes, such that two isomorphic graphs are assigned the same DFS code. Each\nelement of a DFS code is associated with a graph edge: specifically, it is a\nquintuple comprising one node identifier for each of the two endpoints, their\nnode labels, and the edge label. GraphGen learns to generate such sequences\nauto-regressively and models the probability of each component of the quintuple\nindependently. While effective, the independence assumption made by the model\nis too loose to capture the complex label dependencies of real-world graphs\nprecisely. By introducing a novel graph preprocessing approach, we are able to\nprocess the labeling information of both nodes and edges jointly. The\ncorresponding model, which we term GraphGen-Redux, improves upon the generative\nperformances of GraphGen in a wide range of datasets of chemical and social\ngraphs. In addition, it uses approximately 78% fewer parameters than the\nvanilla variant and requires 50% fewer epochs of training on average.",
          "link": "http://arxiv.org/abs/2107.08396",
          "publishedOn": "2021-07-20T02:04:44.455Z",
          "wordCount": 677,
          "title": "GraphGen-Redux: a Fast and Lightweight Recurrent Model for labeled Graph Generation. (arXiv:2107.08396v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alon_N/0/1/0/all/0/1\">Noga Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1\">Steve Hanneke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holzman_R/0/1/0/all/0/1\">Ron Holzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1\">Shay Moran</a>",
          "description": "We extend the theory of PAC learning in a way which allows to model a rich\nvariety of learning tasks where the data satisfy special properties that ease\nthe learning process. For example, tasks where the distance of the data from\nthe decision boundary is bounded away from zero. The basic and simple idea is\nto consider partial concepts: these are functions that can be undefined on\ncertain parts of the space. When learning a partial concept, we assume that the\nsource distribution is supported only on points where the partial concept is\ndefined.\n\nThis way, one can naturally express assumptions on the data such as lying on\na lower dimensional surface or margin conditions. In contrast, it is not at all\nclear that such assumptions can be expressed by the traditional PAC theory. In\nfact we exhibit easy-to-learn partial concept classes which provably cannot be\ncaptured by the traditional PAC theory. This also resolves a question posed by\nAttias, Kontorovich, and Mansour 2019.\n\nWe characterize PAC learnability of partial concept classes and reveal an\nalgorithmic landscape which is fundamentally different than the classical one.\nFor example, in the classical PAC model, learning boils down to Empirical Risk\nMinimization (ERM). In stark contrast, we show that the ERM principle fails in\nexplaining learnability of partial concept classes. In fact, we demonstrate\nclasses that are incredibly easy to learn, but such that any algorithm that\nlearns them must use an hypothesis space with unbounded VC dimension. We also\nfind that the sample compression conjecture fails in this setting.\n\nThus, this theory features problems that cannot be represented nor solved in\nthe traditional way. We view this as evidence that it might provide insights on\nthe nature of learnability in realistic scenarios which the classical theory\nfails to explain.",
          "link": "http://arxiv.org/abs/2107.08444",
          "publishedOn": "2021-07-20T02:04:44.435Z",
          "wordCount": 746,
          "title": "A Theory of PAC Learnability of Partial Concept Classes. (arXiv:2107.08444v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klimek_J/0/1/0/all/0/1\">Jacek Klimek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klimek_J/0/1/0/all/0/1\">Jakub Klimek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraskiewicz_W/0/1/0/all/0/1\">Witold Kraskiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topolewski_M/0/1/0/all/0/1\">Mateusz Topolewski</a>",
          "description": "Various modifications of TRANSFORMER were recently used to solve time-series\nforecasting problem. We propose Query Selector - an efficient, deterministic\nalgorithm for sparse attention matrix. Experiments show it achieves\nstate-of-the art results on ETT data set.",
          "link": "http://arxiv.org/abs/2107.08687",
          "publishedOn": "2021-07-20T02:04:44.416Z",
          "wordCount": 478,
          "title": "Long-term series forecasting with Query Selector -- efficient model of sparse attention. (arXiv:2107.08687v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08595",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tuo_R/0/1/0/all/0/1\">Rui Tuo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaowei Zhang</a>",
          "description": "High-dimensional simulation optimization is notoriously challenging. We\npropose a new sampling algorithm that converges to a global optimal solution\nand suffers minimally from the curse of dimensionality. The algorithm consists\nof two stages. First, we take samples following a sparse grid experimental\ndesign and approximate the response surface via kernel ridge regression with a\nBrownian field kernel. Second, we follow the expected improvement strategy --\nwith critical modifications that boost the algorithm's sample efficiency -- to\niteratively sample from the next level of the sparse grid. Under mild\nconditions on the smoothness of the response surface and the simulation noise,\nwe establish upper bounds on the convergence rate for both noise-free and noisy\nsimulation samples. These upper rates deteriorate only slightly in the\ndimension of the feasible set, and they can be improved if the objective\nfunction is known be of a higher-order smoothness. Extensive numerical\nexperiments demonstrate that the proposed algorithm dramatically outperforms\ntypical alternatives in practice.",
          "link": "http://arxiv.org/abs/2107.08595",
          "publishedOn": "2021-07-20T02:04:44.400Z",
          "wordCount": 615,
          "title": "High-Dimensional Simulation Optimization via Brownian Fields and Sparse Grids. (arXiv:2107.08595v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gautam Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peri_S/0/1/0/all/0/1\">Skand Peri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunseok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungjin Ahn</a>",
          "description": "Object-centric world models provide structured representation of the scene\nand can be an important backbone in reinforcement learning and planning.\nHowever, existing approaches suffer in partially-observable environments due to\nthe lack of belief states. In this paper, we propose Structured World Belief, a\nmodel for learning and inference of object-centric belief states. Inferred by\nSequential Monte Carlo (SMC), our belief states provide multiple object-centric\nscene hypotheses. To synergize the benefits of SMC particles with object\nrepresentations, we also propose a new object-centric dynamics model that\nconsiders the inductive bias of object permanence. This enables tracking of\nobject states even when they are invisible for a long time. To further\nfacilitate object tracking in this regime, we allow our model to attend\nflexibly to any spatial location in the image which was restricted in previous\nmodels. In experiments, we show that object-centric belief provides a more\naccurate and robust performance for filtering and generation. Furthermore, we\nshow the efficacy of structured world belief in improving the performance of\nreinforcement learning, planning and supervised reasoning.",
          "link": "http://arxiv.org/abs/2107.08577",
          "publishedOn": "2021-07-20T02:04:44.344Z",
          "wordCount": 614,
          "title": "Structured World Belief for Reinforcement Learning in POMDP. (arXiv:2107.08577v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lonkar_S/0/1/0/all/0/1\">Subodh Lonkar</a>",
          "description": "Over the centuries, humans have developed and acquired a number of ways to\ncommunicate. But hardly any of them can be as natural and instinctive as facial\nexpressions. On the other hand, neural networks have taken the world by storm.\nAnd no surprises, that the area of Computer Vision and the problem of facial\nexpressions recognitions hasn't remained untouched. Although a wide range of\ntechniques have been applied, achieving extremely high accuracies and preparing\nhighly robust FER systems still remains a challenge due to heterogeneous\ndetails in human faces. In this paper, we will be deep diving into implementing\na system for recognition of facial expressions (FER) by leveraging neural\nnetworks, and more specifically, Convolutional Neural Networks (CNNs). We adopt\nthe fundamental concepts of deep learning and computer vision with various\narchitectures, fine-tune it's hyperparameters and experiment with various\noptimization methods and demonstrate a state-of-the-art single-network-accuracy\nof 70.10% on the FER2013 dataset without using any additional training data.",
          "link": "http://arxiv.org/abs/2107.08640",
          "publishedOn": "2021-07-20T02:04:44.326Z",
          "wordCount": 592,
          "title": "Facial Expressions Recognition with Convolutional Neural Networks. (arXiv:2107.08640v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>",
          "description": "Generalization performance of stochastic optimization stands a central place\nin machine learning. In this paper, we investigate the excess risk performance\nand towards improved learning rates for two popular approaches of stochastic\noptimization: empirical risk minimization (ERM) and stochastic gradient descent\n(SGD). Although there exists plentiful generalization analysis of ERM and SGD\nfor supervised learning, current theoretical understandings of ERM and SGD are\neither have stronger assumptions in convex learning, e.g., strong convexity\ncondition, or show slow rates and less studied in nonconvex learning. Motivated\nby these problems, we aim to provide improved rates under milder assumptions in\nconvex learning and derive faster rates in nonconvex learning. It is notable\nthat our analysis span two popular theoretical viewpoints: stability and\nuniform convergence. To be specific, in stability regime, we present high\nprobability rates of order $\\mathcal{O} (1/n)$ w.r.t. the sample size $n$ for\nERM and SGD with milder assumptions in convex learning and similar high\nprobability rates of order $\\mathcal{O} (1/n)$ in nonconvex learning, rather\nthan in expectation. Furthermore, this type of learning rate is improved to\nfaster order $\\mathcal{O} (1/n^2)$ in uniform convergence regime. To the best\nof our knowledge, for ERM and SGD, the learning rates presented in this paper\nare all state-of-the-art.",
          "link": "http://arxiv.org/abs/2107.08686",
          "publishedOn": "2021-07-20T02:04:44.309Z",
          "wordCount": 645,
          "title": "Improved Learning Rates for Stochastic Optimization: Two Theoretical Viewpoints. (arXiv:2107.08686v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1\">Roi Pomerantz</a>",
          "description": "We present Translatotron 2, a neural direct speech-to-speech translation\nmodel that can be trained end-to-end. Translatotron 2 consists of a speech\nencoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention\nmodule that connects all the previous three components. Experimental results\nsuggest that Translatotron 2 outperforms the original Translatotron by a large\nmargin in terms of translation quality and predicted speech naturalness, and\ndrastically improves the robustness of the predicted speech by mitigating\nover-generation, such as babbling or long pause. We also propose a new method\nfor retaining the source speaker's voice in the translated speech. The trained\nmodel is restricted to retain the source speaker's voice, and unlike the\noriginal Translatotron, it is not able to generate speech in a different\nspeaker's voice, making the model more robust for production deployment, by\nmitigating potential misuse for creating spoofing audio artifacts. When the new\nmethod is used together with a simple concatenation-based data augmentation,\nthe trained Translatotron 2 model is able to retain each speaker's voice for\ninput with speaker turns.",
          "link": "http://arxiv.org/abs/2107.08661",
          "publishedOn": "2021-07-20T02:04:44.288Z",
          "wordCount": 614,
          "title": "Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maran_D/0/1/0/all/0/1\">D. Maran</a>",
          "description": "We improve a theoretical result of the article \"On Exploiting Spectral\nProperties for Solving MDP with Large State Space\" showing that their\nalgorithm, which was proved to converge under some unrealistic assumptions, is\nactually guaranteed to converge always.",
          "link": "http://arxiv.org/abs/2107.08488",
          "publishedOn": "2021-07-20T02:04:44.270Z",
          "wordCount": 484,
          "title": "A note on the article \"On Exploiting Spectral Properties for Solving MDP with Large State Space\". (arXiv:2107.08488v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ibeling_D/0/1/0/all/0/1\">Duligur Ibeling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Icard_T/0/1/0/all/0/1\">Thomas Icard</a>",
          "description": "This paper presents a topological learning-theoretic perspective on causal\ninference by introducing a series of topologies defined on general spaces of\nstructural causal models (SCMs). As an illustration of the framework we prove a\ntopological causal hierarchy theorem, showing that substantive assumption-free\ncausal inference is possible only in a meager set of SCMs. Thanks to a known\ncorrespondence between open sets in the weak topology and statistically\nverifiable hypotheses, our results show that inductive assumptions sufficient\nto license valid causal inferences are statistically unverifiable in principle.\nSimilar to no-free-lunch theorems for statistical inference, the present\nresults clarify the inevitability of substantial assumptions for causal\ninference. An additional benefit of our topological approach is that it easily\naccommodates SCMs with infinitely many variables. We finally suggest that the\nframework may be helpful for the positive project of exploring and assessing\nalternative causal-inductive assumptions.",
          "link": "http://arxiv.org/abs/2107.08558",
          "publishedOn": "2021-07-20T02:04:44.206Z",
          "wordCount": 578,
          "title": "A Topological Perspective on Causal Inference. (arXiv:2107.08558v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>",
          "description": "We study multi-task reinforcement learning (RL) in tabular episodic Markov\ndecision processes (MDPs). We formulate a heterogeneous multi-player RL\nproblem, in which a group of players concurrently face similar but not\nnecessarily identical MDPs, with a goal of improving their collective\nperformance through inter-player information sharing. We design and analyze an\nalgorithm based on the idea of model transfer, and provide gap-dependent and\ngap-independent upper and lower bounds that characterize the intrinsic\ncomplexity of the problem.",
          "link": "http://arxiv.org/abs/2107.08622",
          "publishedOn": "2021-07-20T02:04:44.188Z",
          "wordCount": 502,
          "title": "Provably Efficient Multi-Task Reinforcement Learning with Model Transfer. (arXiv:2107.08622v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdelhack_M/0/1/0/all/0/1\">Mohamed Abdelhack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Sandhya Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_B/0/1/0/all/0/1\">Bradley Fritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avidan_M/0/1/0/all/0/1\">Michael Avidan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_C/0/1/0/all/0/1\">Christopher King</a>",
          "description": "Data quality is a common problem in machine learning, especially in\nhigh-stakes settings such as healthcare. Missing data affects accuracy,\ncalibration, and feature attribution in complex patterns. Developers often\ntrain models on carefully curated datasets to minimize missing data bias;\nhowever, this reduces the usability of such models in production environments,\nsuch as real-time healthcare records. Making machine learning models robust to\nmissing data is therefore crucial for practical application. While some\nclassifiers naturally handle missing data, others, such as deep neural\nnetworks, are not designed for unknown values. We propose a novel neural\nnetwork modification to mitigate the impacts of missing data. The approach is\ninspired by neuromodulation that is performed by biological neural networks.\nOur proposal replaces the fixed weights of a fully-connected layer with a\nfunction of an additional input (reliability score) at each input, mimicking\nthe ability of cortex to up- and down-weight inputs based on the presence of\nother data. The modulation function is jointly learned with the main task using\na multi-layer perceptron. We tested our modulating fully connected layer on\nmultiple classification, regression, and imputation problems, and it either\nimproved performance or generated comparable performance to conventional neural\nnetwork architectures concatenating reliability to the inputs. Models with\nmodulating layers were more robust against degradation of data quality by\nintroducing additional missingness at evaluation time. These results suggest\nthat explicitly accounting for reduced information quality with a modulating\nfully connected layer can enable the deployment of artificial intelligence\nsystems in real-time settings.",
          "link": "http://arxiv.org/abs/2107.08574",
          "publishedOn": "2021-07-20T02:04:44.170Z",
          "wordCount": 695,
          "title": "A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues. (arXiv:2107.08574v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sisejkovic_D/0/1/0/all/0/1\">Dominik Sisejkovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merchant_F/0/1/0/all/0/1\">Farhad Merchant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimann_L/0/1/0/all/0/1\">Lennart M. Reimann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leupers_R/0/1/0/all/0/1\">Rainer Leupers</a>",
          "description": "Logic locking has emerged as a prominent key-driven technique to protect the\nintegrity of integrated circuits. However, novel machine-learning-based attacks\nhave recently been introduced to challenge the security foundations of locking\nschemes. These attacks are able to recover a significant percentage of the key\nwithout having access to an activated circuit. This paper address this issue\nthrough two focal points. First, we present a theoretical model to test locking\nschemes for key-related structural leakage that can be exploited by machine\nlearning. Second, based on the theoretical model, we introduce D-MUX: a\ndeceptive multiplexer-based logic-locking scheme that is resilient against\nstructure-exploiting machine learning attacks. Through the design of D-MUX, we\nuncover a major fallacy in existing multiplexer-based locking schemes in the\nform of a structural-analysis attack. Finally, an extensive cost evaluation of\nD-MUX is presented. To the best of our knowledge, D-MUX is the first\nmachine-learning-resilient locking scheme capable of protecting against all\nknown learning-based attacks. Hereby, the presented work offers a starting\npoint for the design and evaluation of future-generation logic locking in the\nera of machine learning.",
          "link": "http://arxiv.org/abs/2107.08695",
          "publishedOn": "2021-07-20T02:04:44.143Z",
          "wordCount": 626,
          "title": "Deceptive Logic Locking for Hardware Integrity Protection against Machine Learning Attacks. (arXiv:2107.08695v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jinke Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chonghe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Guanding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Dongning Guo</a>",
          "description": "Generative adversarial networks (GANs) are emerging machine learning models\nfor generating synthesized data similar to real data by jointly training a\ngenerator and a discriminator. In many applications, data and computational\nresources are distributed over many devices, so centralized computation with\nall data in one location is infeasible due to privacy and/or communication\nconstraints. This paper proposes a new framework for training GANs in a\ndistributed fashion: Each device computes a local discriminator using local\ndata; a single server aggregates their results and computes a global GAN.\nSpecifically, in each iteration, the server sends the global GAN to the\ndevices, which then update their local discriminators; the devices send their\nresults to the server, which then computes their average as the global\ndiscriminator and updates the global generator accordingly. Two different\nupdate schedules are designed with different levels of parallelism between the\ndevices and the server. Numerical results obtained using three popular datasets\ndemonstrate that the proposed framework can outperform a state-of-the-art\nframework in terms of convergence speed.",
          "link": "http://arxiv.org/abs/2107.08681",
          "publishedOn": "2021-07-20T02:04:44.124Z",
          "wordCount": 627,
          "title": "A New Distributed Method for Training Generative Adversarial Networks. (arXiv:2107.08681v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pimpley_A/0/1/0/all/0/1\">Anish Pimpley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Anubha Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohra_V/0/1/0/all/0/1\">Vishal Rohra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Soundararajan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jindal_A/0/1/0/all/0/1\">Alekh Jindal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1\">Hiren Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Shi Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_R/0/1/0/all/0/1\">Rathijit Sen</a>",
          "description": "Optimizing resource allocation for analytical workloads is vital for reducing\ncosts of cloud-data services. At the same time, it is incredibly hard for users\nto allocate resources per query in serverless processing systems, and they\nfrequently misallocate by orders of magnitude. Unfortunately, prior work\nfocused on predicting peak allocation while ignoring aggressive trade-offs\nbetween resource allocation and run-time. Additionally, these methods fail to\npredict allocation for queries that have not been observed in the past. In this\npaper, we tackle both these problems. We introduce a system for optimal\nresource allocation that can predict performance with aggressive trade-offs,\nfor both new and past observed queries. We introduce the notion of a\nperformance characteristic curve (PCC) as a parameterized representation that\ncan compactly capture the relationship between resources and performance. To\ntackle training data sparsity, we introduce a novel data augmentation technique\nto efficiently synthesize the entire PCC using a single run of the query.\nLastly, we demonstrate the advantages of a constrained loss function coupled\nwith GNNs, over traditional ML methods, for capturing the domain specific\nbehavior through an extensive experimental evaluation over SCOPE big data\nworkloads at Microsoft.",
          "link": "http://arxiv.org/abs/2107.08594",
          "publishedOn": "2021-07-20T02:04:44.093Z",
          "wordCount": 628,
          "title": "Optimal Resource Allocation for Serverless Queries. (arXiv:2107.08594v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yinjun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weimer_J/0/1/0/all/0/1\">James Weimer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_S/0/1/0/all/0/1\">Susan B. Davidson</a>",
          "description": "High-quality labels are expensive to obtain for many machine learning tasks,\nsuch as medical image classification tasks. Therefore, probabilistic (weak)\nlabels produced by weak supervision tools are used to seed a process in which\ninfluential samples with weak labels are identified and cleaned by several\nhuman annotators to improve the model performance. To lower the overall cost\nand computational overhead of this process, we propose a solution called\nChef(CHEap and Fast label cleaning), which consists of the following three\ncomponents. First, to reduce the cost of human annotators, we use Infl, which\nprioritizes the most influential training samples for cleaning and provides\ncleaned labels to save the cost of one human annotator. Second, to accelerate\nthe sample selector phase and the model constructor phase, we use Increm-Infl\nto incrementally produce influential samples, and DeltaGrad-L to incrementally\nupdate the model. Third, we redesign the typical label cleaning pipeline so\nthat human annotators iteratively clean smaller batch of samples rather than\none big batch of samples. This yields better over all model performance and\nenables possible early termination when the expected model performance has been\nachieved. Extensive experiments show that our approach gives good model\nprediction performance while achieving significant speed-ups.",
          "link": "http://arxiv.org/abs/2107.08588",
          "publishedOn": "2021-07-20T02:04:44.023Z",
          "wordCount": 636,
          "title": "Chef: a cheap and fast pipeline for iteratively cleaning label uncertainties. (arXiv:2107.08588v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08673",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Massalimova_A/0/1/0/all/0/1\">Aidana Massalimova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varol_H/0/1/0/all/0/1\">Huseyin Atakan Varol</a>",
          "description": "Alzheimer's disease (AD) is a progressive brain disorder that causes memory\nand functional impairments. The advances in machine learning and publicly\navailable medical datasets initiated multiple studies in AD diagnosis. In this\nwork, we utilize a multi-modal deep learning approach in classifying normal\ncognition, mild cognitive impairment and AD classes on the basis of structural\nMRI and diffusion tensor imaging (DTI) scans from the OASIS-3 dataset. In\naddition to a conventional multi-modal network, we also present an input\nagnostic architecture that allows diagnosis with either sMRI or DTI scan, which\ndistinguishes our method from previous multi-modal machine learning-based\nmethods. The results show that the input agnostic model achieves 0.96 accuracy\nwhen both structural MRI and DTI scans are provided as inputs.",
          "link": "http://arxiv.org/abs/2107.08673",
          "publishedOn": "2021-07-20T02:04:44.004Z",
          "wordCount": 581,
          "title": "Input Agnostic Deep Learning for Alzheimer's Disease Classification Using Multimodal MRI Images. (arXiv:2107.08673v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Velichko_A/0/1/0/all/0/1\">Andrei Velichko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidari_H/0/1/0/all/0/1\">Hanif Heidari</a>",
          "description": "Measuring the predictability and complexity of time series is an essential\ntool in designing and controlling the nonlinear system. There exist different\nentropy measures in the literature to analyze the predictability and complexity\nof time series. However, these measures have some drawbacks especially in short\ntime series. To overcome the difficulties, this paper proposes a new method for\nestimating the entropy of a time series using the LogNNet 784:25:10 neural\nnetwork model. The LogNNet reservoir matrix consists of 19625 elements which is\nfilled with the time series elements. After that, the network is trained on\nMNIST-10 dataset and the classification accuracy is calculated. The accuracy is\nconsidered as the entropy measure and denoted by NNetEn. A more complex\ntransformation of the input information by the time series in the reservoir\nleads to higher NNetEn values. Many practical time series data have less than\n19625 elements. Some duplicating or stretching methods are investigated to\novercome this difficulty and the most successful method is identified for\npractical applications. The epochs number in the training process of LogNNet is\nconsidered as the input parameter. A new time series characteristic called time\nseries learning inertia is introduced to investigate the effect of epochs\nnumber in the efficiency of neural network. To show the robustness and\nefficiency of the proposed method, it is applied on some chaotic, periodic,\nrandom, binary and constant time series. The NNetEn is compared with some\nexisting entropy measures. The results show that the proposed method is more\nrobust and accurate than existing methods.",
          "link": "http://arxiv.org/abs/2107.08399",
          "publishedOn": "2021-07-20T02:04:43.986Z",
          "wordCount": 712,
          "title": "A method for estimating the entropy of time series using artificial neural network. (arXiv:2107.08399v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhou Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tong Lin</a>",
          "description": "Adaptive gradient methods, especially Adam-type methods (such as Adam,\nAMSGrad, and AdaBound), have been proposed to speed up the training process\nwith an element-wise scaling term on learning rates. However, they often\ngeneralize poorly compared with stochastic gradient descent (SGD) and its\naccelerated schemes such as SGD with momentum (SGDM). In this paper, we propose\na new adaptive method called DecGD, which simultaneously achieves good\ngeneralization like SGDM and obtain rapid convergence like Adam-type methods.\nIn particular, DecGD decomposes the current gradient into the product of two\nterms including a surrogate gradient and a loss based vector. Our method\nadjusts the learning rates adaptively according to the current loss based\nvector instead of the squared gradients used in Adam-type methods. The\nintuition for adaptive learning rates of DecGD is that a good optimizer, in\ngeneral cases, needs to decrease the learning rates as the loss decreases,\nwhich is similar to the learning rates decay scheduling technique. Therefore,\nDecGD gets a rapid convergence in the early phases of training and controls the\neffective learning rates according to the loss based vectors which help lead to\na better generalization. Convergence analysis is discussed in both convex and\nnon-convex situations. Finally, empirical results on widely-used tasks and\nmodels demonstrate that DecGD shows better generalization performance than SGDM\nand rapid convergence like Adam-type methods.",
          "link": "http://arxiv.org/abs/2107.08377",
          "publishedOn": "2021-07-20T02:04:43.967Z",
          "wordCount": 652,
          "title": "A New Adaptive Gradient Method with Gradient Decomposition. (arXiv:2107.08377v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08710",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Higham_C/0/1/0/all/0/1\">Catherine F. Higham</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Bedford_A/0/1/0/all/0/1\">Adrian Bedford</a>",
          "description": "We demonstrate the feasibility of framing a classically learned deep neural\nnetwork as an energy based model that can be processed on a one-step quantum\nannealer in order to exploit fast sampling times. We propose approaches to\novercome two hurdles for high resolution image classification on a quantum\nprocessing unit (QPU): the required number and binary nature of the model\nstates. With this novel method we successfully transfer a convolutional neural\nnetwork to the QPU and show the potential for classification speedup of at\nleast one order of magnitude.",
          "link": "http://arxiv.org/abs/2107.08710",
          "publishedOn": "2021-07-20T02:04:43.913Z",
          "wordCount": 522,
          "title": "Quantum Deep Learning: Sampling Neural Nets with a Quantum Annealer. (arXiv:2107.08710v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08596",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Katsman_I/0/1/0/all/0/1\">Isay Katsman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lou_A/0/1/0/all/0/1\">Aaron Lou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lim_D/0/1/0/all/0/1\">Derek Lim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jiang_Q/0/1/0/all/0/1\">Qingxuan Jiang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sa_C/0/1/0/all/0/1\">Christopher De Sa</a>",
          "description": "Tractably modelling distributions over manifolds has long been an important\ngoal in the natural sciences. Recent work has focused on developing general\nmachine learning models to learn such distributions. However, for many\napplications these distributions must respect manifold symmetries -- a trait\nwhich most previous models disregard. In this paper, we lay the theoretical\nfoundations for learning symmetry-invariant distributions on arbitrary\nmanifolds via equivariant manifold flows. We demonstrate the utility of our\napproach by using it to learn gauge invariant densities over $SU(n)$ in the\ncontext of quantum field theory.",
          "link": "http://arxiv.org/abs/2107.08596",
          "publishedOn": "2021-07-20T02:04:43.826Z",
          "wordCount": 526,
          "title": "Equivariant Manifold Flows. (arXiv:2107.08596v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nieto_J/0/1/0/all/0/1\">Juan Jos&#xe9; Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creus_R/0/1/0/all/0/1\">Roger Creus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1\">Xavier Giro-i-Nieto</a>",
          "description": "Pre-training Reinforcement Learning agents in a task-agnostic manner has\nshown promising results. However, previous works still struggle in learning and\ndiscovering meaningful skills in high-dimensional state-spaces, such as\npixel-spaces. We approach the problem by leveraging unsupervised skill\ndiscovery and self-supervised learning of state representations. In our work,\nwe learn a compact latent representation by making use of variational and\ncontrastive techniques. We demonstrate that both enable RL agents to learn a\nset of basic navigation skills by maximizing an information theoretic\nobjective. We assess our method in Minecraft 3D pixel maps with different\ncomplexities. Our results show that representations and conditioned policies\nlearned from pixels are enough for toy examples, but do not scale to realistic\nand complex maps. To overcome these limitations, we explore alternative input\nobservations such as the relative position of the agent along with the raw\npixels.",
          "link": "http://arxiv.org/abs/2107.08398",
          "publishedOn": "2021-07-20T02:04:43.808Z",
          "wordCount": 579,
          "title": "Unsupervised Skill-Discovery and Skill-Learning in Minecraft. (arXiv:2107.08398v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08429",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Naik_S/0/1/0/all/0/1\">Shibabrat Naik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Krajnak_V/0/1/0/all/0/1\">Vladim&#xed;r Kraj&#x148;&#xe1;k</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wiggins_S/0/1/0/all/0/1\">Stephen Wiggins</a>",
          "description": "We develop a machine learning framework that can be applied to data sets\nderived from the trajectories of Hamilton's equations. The goal is to learn the\nphase space structures that play the governing role for phase space transport\nrelevant to particular applications. Our focus is on learning reactive islands\nin two degrees-of-freedom Hamiltonian systems. Reactive islands are constructed\nfrom the stable and unstable manifolds of unstable periodic orbits and play the\nrole of quantifying transition dynamics. We show that support vector machines\n(SVM) is an appropriate machine learning framework for this purpose as it\nprovides an approach for finding the boundaries between qualitatively distinct\ndynamical behaviors, which is in the spirit of the phase space transport\nframework. We show how our method allows us to find reactive islands directly\nin the sense that we do not have to first compute unstable periodic orbits and\ntheir stable and unstable manifolds. We apply our approach to the\nH\\'enon-Heiles Hamiltonian system, which is a benchmark system in the dynamical\nsystems community. We discuss different sampling and learning approaches and\ntheir advantages and disadvantages.",
          "link": "http://arxiv.org/abs/2107.08429",
          "publishedOn": "2021-07-20T02:04:43.788Z",
          "wordCount": 625,
          "title": "Support vector machines for learning reactive islands. (arXiv:2107.08429v1 [math.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ampanavos_S/0/1/0/all/0/1\">Spyridon Ampanavos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkawi_A/0/1/0/all/0/1\">Ali Malkawi</a>",
          "description": "Current performance-driven building design methods are not widely adopted\noutside the research field for several reasons that make them difficult to\nintegrate into a typical design process. In the early design phase, in\nparticular, the time-intensity and the cognitive load associated with\noptimization and form parametrization are incompatible with design exploration,\nwhich requires quick iteration. This research introduces a novel method for\nperformance-driven geometry generation that can afford interaction directly in\nthe 3d modeling environment, eliminating the need for explicit parametrization,\nand is multiple orders faster than the equivalent form optimization. The method\nuses Machine Learning techniques to train a generative model offline. The\ngenerative model learns a distribution of optimal performing geometries and\ntheir simulation contexts based on a dataset that addresses the performance(s)\nof interest. By navigating the generative model's latent space, geometries with\nthe desired characteristics can be quickly generated. A case study is\npresented, demonstrating the generation of a synthetic dataset and the use of a\nVariational Autoencoder (VAE) as a generative model for geometries with optimal\nsolar gain. The results show that the VAE-generated geometries perform on\naverage at least as well as the optimized ones, suggesting that the introduced\nmethod shows a feasible path towards more intuitive and interactive early-phase\nperformance-driven design assistance.",
          "link": "http://arxiv.org/abs/2107.08572",
          "publishedOn": "2021-07-20T02:04:43.769Z",
          "wordCount": 637,
          "title": "Early-Phase Performance-Driven Design using Generative Models. (arXiv:2107.08572v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08470",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ho_Y/0/1/0/all/0/1\">Yung-Han Ho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_C/0/1/0/all/0/1\">Chih-Chun Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_W/0/1/0/all/0/1\">Wen-Hsiao Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hang_H/0/1/0/all/0/1\">Hsueh-Ming Hang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Domanski_M/0/1/0/all/0/1\">Marek Domanski</a>",
          "description": "This paper introduces an end-to-end learned image compression system, termed\nANFIC, based on Augmented Normalizing Flows (ANF). ANF is a new type of flow\nmodel, which stacks multiple variational autoencoders (VAE) for greater model\nexpressiveness. The VAE-based image compression has gone mainstream, showing\npromising compression performance. Our work presents the first attempt to\nleverage VAE-based compression in a flow-based framework. ANFIC advances\nfurther compression efficiency by stacking and extending hierarchically\nmultiple VAE's. The invertibility of ANF, together with our training\nstrategies, enables ANFIC to support a wide range of quality levels without\nchanging the encoding and decoding networks. Extensive experimental results\nshow that in terms of PSNR-RGB, ANFIC performs comparably to or better than the\nstate-of-the-art learned image compression. Moreover, it performs close to VVC\nintra coding, from low-rate compression up to nearly-lossless compression. In\nparticular, ANFIC achieves the state-of-the-art performance, when extended with\nconditional convolution for variable rate compression with a single model.",
          "link": "http://arxiv.org/abs/2107.08470",
          "publishedOn": "2021-07-20T02:04:43.752Z",
          "wordCount": 602,
          "title": "ANFIC: Image Compression Using Augmented Normalizing Flows. (arXiv:2107.08470v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chin_H/0/1/0/all/0/1\">Hsu-Hsun Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsay_R/0/1/0/all/0/1\">Ren-Song Tsay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hsin-I Wu</a>",
          "description": "Recent convolutional neural network (CNN) development continues to advance\nthe state-of-the-art model accuracy for various applications. However, the\nenhanced accuracy comes at the cost of substantial memory bandwidth and storage\nrequirements and demanding computational resources. Although in the past the\nquantization methods have effectively reduced the deployment cost for edge\ndevices, it suffers from significant information loss when processing the\nbiased activations of contemporary CNNs. In this paper, we hence introduce an\nadaptive high-performance quantization method to resolve the issue of biased\nactivation by dynamically adjusting the scaling and shifting factors based on\nthe task loss. Our proposed method has been extensively evaluated on image\nclassification models (ResNet-18/34/50, MobileNet-V2, EfficientNet-B0) with\nImageNet dataset, object detection model (YOLO-V4) with COCO dataset, and\nlanguage models with PTB dataset. The results show that our 4-bit integer\n(INT4) quantization models achieve better accuracy than the state-of-the-art\n4-bit models, and in some cases, even surpass the golden full-precision models.\nThe final designs have been successfully deployed onto extremely\nresource-constrained edge devices for many practical applications.",
          "link": "http://arxiv.org/abs/2107.08382",
          "publishedOn": "2021-07-20T02:04:43.694Z",
          "wordCount": 609,
          "title": "A High-Performance Adaptive Quantization Approach for Edge CNN Applications. (arXiv:2107.08382v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Chihcheng Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_C/0/1/0/all/0/1\">Catarina Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1\">Chun Ouyang</a>",
          "description": "Predictive process analytics often apply machine learning to predict the\nfuture states of a running business process. However, the internal mechanisms\nof many existing predictive algorithms are opaque and a human decision-maker is\nunable to understand \\emph{why} a certain activity was predicted. Recently,\ncounterfactuals have been proposed in the literature to derive\nhuman-understandable explanations from predictive models. Current\ncounterfactual approaches consist of finding the minimum feature change that\ncan make a certain prediction flip its outcome. Although many algorithms have\nbeen proposed, their application to the sequence and multi-dimensional data\nlike event logs has not been explored in the literature.\n\nIn this paper, we explore the use of a recent, popular model-agnostic\ncounterfactual algorithm, DiCE, in the context of predictive process analytics.\nThe analysis reveals that the algorithm is limited when being applied to derive\nexplanations of process predictions, due to (1) process domain knowledge not\nbeing taken into account, (2) long traces that often tend to be less\nunderstandable, and (3) difficulties in optimising the counterfactual search\nwith categorical variables. We design an extension of DiCE that can generate\ncounterfactuals for process predictions, and propose an approach that supports\nderiving milestone-aware counterfactuals at different stages of a trace to\npromote interpretability. We apply our approach to BPIC2012 event log and the\nanalysis results demonstrate the effectiveness of the proposed approach.",
          "link": "http://arxiv.org/abs/2107.08697",
          "publishedOn": "2021-07-20T02:04:43.676Z",
          "wordCount": 652,
          "title": "Interpreting Process Predictions using a Milestone-Aware Counterfactual Approach. (arXiv:2107.08697v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Assayag_S/0/1/0/all/0/1\">Shai Ben-Assayag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Yaniv_R/0/1/0/all/0/1\">Ran El-Yaniv</a>",
          "description": "Playing board games is considered a major challenge for both humans and AI\nresearchers. Because some complicated board games are quite hard to learn,\nhumans usually begin with playing on smaller boards and incrementally advance\nto master larger board strategies. Most neural network frameworks that are\ncurrently tasked with playing board games neither perform such incremental\nlearning nor possess capabilities to automatically scale up. In this work, we\nlook at the board as a graph and combine a graph neural network architecture\ninside the AlphaZero framework, along with some other innovative improvements.\nOur ScalableAlphaZero is capable of learning to play incrementally on small\nboards, and advancing to play on large ones. Our model can be trained quickly\nto play different challenging board games on multiple board sizes, without\nusing any domain knowledge. We demonstrate the effectiveness of\nScalableAlphaZero and show, for example, that by training it for only three\ndays on small Othello boards, it can defeat the AlphaZero model on a large\nboard, which was trained to play the large board for $30$ days.",
          "link": "http://arxiv.org/abs/2107.08387",
          "publishedOn": "2021-07-20T02:04:43.657Z",
          "wordCount": 616,
          "title": "Train on Small, Play the Large: Scaling Up Board Games with AlphaZero and GNN. (arXiv:2107.08387v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuesi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huzhang_G/0/1/0/all/0/1\">Guangda Huzhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qianying Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Da_Q/0/1/0/all/0/1\">Qing Da</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dan Shen</a>",
          "description": "Ensemble models in E-commerce combine predictions from multiple sub-models\nfor ranking and revenue improvement. Industrial ensemble models are typically\ndeep neural networks, following the supervised learning paradigm to infer\nconversion rate given inputs from sub-models. However, this process has the\nfollowing two problems. Firstly, the point-wise scoring approach disregards the\nrelationships between items and leads to homogeneous displayed results, while\ndiversified display benefits user experience and revenue. Secondly, the\nlearning paradigm focuses on the ranking metrics and does not directly optimize\nthe revenue. In our work, we propose a new Learning-To-Ensemble (LTE) framework\nRAEGO, which replaces the ensemble model with a contextual Rank Aggregator (RA)\nand explores the best weights of sub-models by the Evaluator-Generator\nOptimization (EGO). To achieve the best online performance, we propose a new\nrank aggregation algorithm TournamentGreedy as a refinement of classic rank\naggregators, which also produces the best average weighted Kendall Tau Distance\n(KTD) amongst all the considered algorithms with quadratic time complexity.\nUnder the assumption that the best output list should be Pareto Optimal on the\nKTD metric for sub-models, we show that our RA algorithm has higher efficiency\nand coverage in exploring the optimal weights. Combined with the idea of\nBayesian Optimization and gradient descent, we solve the online contextual\nBlack-Box Optimization task that finds the optimal weights for sub-models given\na chosen RA model. RA-EGO has been deployed in our online system and has\nimproved the revenue significantly.",
          "link": "http://arxiv.org/abs/2107.08598",
          "publishedOn": "2021-07-20T02:04:43.636Z",
          "wordCount": 670,
          "title": "Learning-To-Ensemble by Contextual Rank Aggregation in E-Commerce. (arXiv:2107.08598v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Onoszko_N/0/1/0/all/0/1\">Noa Onoszko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_G/0/1/0/all/0/1\">Gustav Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mogren_O/0/1/0/all/0/1\">Olof Mogren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zec_E/0/1/0/all/0/1\">Edvin Listo Zec</a>",
          "description": "We tackle the non-convex problem of learning a personalized deep learning\nmodel in a decentralized setting. More specifically, we study decentralized\nfederated learning, a peer-to-peer setting where data is distributed among many\nclients and where there is no central server to orchestrate the training. In\nreal world scenarios, the data distributions are often heterogeneous between\nclients. Therefore, in this work we study the problem of how to efficiently\nlearn a model in a peer-to-peer system with non-iid client data. We propose a\nmethod named Performance-Based Neighbor Selection (PENS) where clients with\nsimilar data distributions detect each other and cooperate by evaluating their\ntraining losses on each other's data to learn a model suitable for the local\ndata distribution. Our experiments on benchmark datasets show that our proposed\nmethod is able to achieve higher accuracies as compared to strong baselines.",
          "link": "http://arxiv.org/abs/2107.08517",
          "publishedOn": "2021-07-20T02:04:43.618Z",
          "wordCount": 579,
          "title": "Decentralized federated learning of deep neural networks on non-iid data. (arXiv:2107.08517v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08471",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dengshan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chengjun Xie</a>",
          "description": "Artificial neural networks that simulate human achieves great successes. From\nthe perspective of simulating human memory method, we propose a stepped sampler\nbased on the \"repeated input\". We repeatedly inputted data to the LSTM model\nstepwise in a batch. The stepped sampler is used to strengthen the ability of\nfusing the temporal information in LSTM. We tested the stepped sampler on the\nLSTM built-in in PyTorch. Compared with the traditional sampler of PyTorch,\nsuch as sequential sampler, batch sampler, the training loss of the proposed\nstepped sampler converges faster in the training of the model, and the training\nloss after convergence is more stable. Meanwhile, it can maintain a higher test\naccuracy. We quantified the algorithm of the stepped sampler.",
          "link": "http://arxiv.org/abs/2107.08471",
          "publishedOn": "2021-07-20T02:04:43.555Z",
          "wordCount": 556,
          "title": "A stepped sampling method for video detection using LSTM. (arXiv:2107.08471v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdolshah_M/0/1/0/all/0/1\">Majid Abdolshah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_T/0/1/0/all/0/1\">Thommen Karimpanal George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_S/0/1/0/all/0/1\">Santu Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>",
          "description": "Transfer in reinforcement learning is usually achieved through generalisation\nacross tasks. Whilst many studies have investigated transferring knowledge when\nthe reward function changes, they have assumed that the dynamics of the\nenvironments remain consistent. Many real-world RL problems require transfer\namong environments with different dynamics. To address this problem, we propose\nan approach based on successor features in which we model successor feature\nfunctions with Gaussian Processes permitting the source successor features to\nbe treated as noisy measurements of the target successor feature function. Our\ntheoretical analysis proves the convergence of this approach as well as the\nbounded error on modelling successor feature functions with Gaussian Processes\nin environments with both different dynamics and rewards. We demonstrate our\nmethod on benchmark datasets and show that it outperforms current baselines.",
          "link": "http://arxiv.org/abs/2107.08426",
          "publishedOn": "2021-07-20T02:04:43.537Z",
          "wordCount": 569,
          "title": "A New Representation of Successor Features for Transfer across Dissimilar Environments. (arXiv:2107.08426v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Feiyang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yanrong Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_A/0/1/0/all/0/1\">Ao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "The exploration/exploitation (E&E) dilemma lies at the core of interactive\nsystems such as online advertising, for which contextual bandit algorithms have\nbeen proposed. Bayesian approaches provide guided exploration with principled\nuncertainty estimation, but the applicability is often limited due to\nover-simplified assumptions. Non-Bayesian bootstrap methods, on the other hand,\ncan apply to complex problems by using deep reward models, but lacks clear\nguidance to the exploration behavior. It still remains largely unsolved to\ndevelop a practical method for complex deep contextual bandits.\n\nIn this paper, we introduce Guided Bootstrap (GuideBoot for short), combining\nthe best of both worlds. GuideBoot provides explicit guidance to the\nexploration behavior by training multiple models over both real samples and\nnoisy samples with fake labels, where the noise is added according to the\npredictive uncertainty. The proposed method is efficient as it can make\ndecisions on-the-fly by utilizing only one randomly chosen model, but is also\neffective as we show that it can be viewed as a non-Bayesian approximation of\nThompson sampling. Moreover, we extend it to an online version that can learn\nsolely from streaming data, which is favored in real applications. Extensive\nexperiments on both synthetic task and large-scale advertising environments\nshow that GuideBoot achieves significant improvements against previous\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.08383",
          "publishedOn": "2021-07-20T02:04:43.515Z",
          "wordCount": 649,
          "title": "GuideBoot: Guided Bootstrap for Deep Contextual Bandits. (arXiv:2107.08383v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Liangqiong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandar_N/0/1/0/all/0/1\">Niranjan Balachandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel L Rubin</a>",
          "description": "Federated learning enables multiple institutions to collaboratively train\nmachine learning models on their local data in a privacy-preserving way.\nHowever, its distributed nature often leads to significant heterogeneity in\ndata distributions across institutions. In this paper, we investigate the\ndeleterious impact of a taxonomy of data heterogeneity regimes on federated\nlearning methods, including quantity skew, label distribution skew, and imaging\nacquisition skew. We show that the performance degrades with the increasing\ndegrees of data heterogeneity. We present several mitigation strategies to\novercome performance drops from data heterogeneity, including weighted average\nfor data quantity skew, weighted loss and batch normalization averaging for\nlabel distribution skew. The proposed optimizations to federated learning\nmethods improve their capability of handling heterogeneity across institutions,\nwhich provides valuable guidance for the deployment of federated learning in\nreal clinical applications.",
          "link": "http://arxiv.org/abs/2107.08371",
          "publishedOn": "2021-07-20T02:04:43.497Z",
          "wordCount": 583,
          "title": "An Experimental Study of Data Heterogeneity in Federated Learning Methods for Medical Imaging. (arXiv:2107.08371v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gruenbacher_S/0/1/0/all/0/1\">Sophie Gruenbacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1\">Mathias Lechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1\">Ramin Hasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henzinger_T/0/1/0/all/0/1\">Thomas A. Henzinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smolka_S/0/1/0/all/0/1\">Scott Smolka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosu_R/0/1/0/all/0/1\">Radu Grosu</a>",
          "description": "We introduce a new stochastic verification algorithm that formally quantifies\nthe behavioral robustness of any time-continuous process formulated as a\ncontinuous-depth model. The algorithm solves a set of global optimization (Go)\nproblems over a given time horizon to construct a tight enclosure (Tube) of the\nset of all process executions starting from a ball of initial states. We call\nour algorithm GoTube. Through its construction, GoTube ensures that the\nbounding tube is conservative up to a desired probability. GoTube is\nimplemented in JAX and optimized to scale to complex continuous-depth models.\nCompared to advanced reachability analysis tools for time-continuous neural\nnetworks, GoTube provably does not accumulate over-approximation errors between\ntime steps and avoids the infamous wrapping effect inherent in symbolic\ntechniques. We show that GoTube substantially outperforms state-of-the-art\nverification tools in terms of the size of the initial ball, speed,\ntime-horizon, task completion, and scalability, on a large set of experiments.\nGoTube is stable and sets the state-of-the-art for its ability to scale up to\ntime horizons well beyond what has been possible before.",
          "link": "http://arxiv.org/abs/2107.08467",
          "publishedOn": "2021-07-20T02:04:43.457Z",
          "wordCount": 631,
          "title": "GoTube: Scalable Stochastic Verification of Continuous-Depth Models. (arXiv:2107.08467v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ampanavos_S/0/1/0/all/0/1\">Spyridon Ampanavos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nourbakhsh_M/0/1/0/all/0/1\">Mehdi Nourbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Chin-Yi Cheng</a>",
          "description": "Structural engineering knowledge can be of significant importance to the\narchitectural design team during the early design phase. However, architects\nand engineers do not typically work together during the conceptual phase; in\nfact, structural engineers are often called late into the process. As a result,\nupdates in the design are more difficult and time-consuming to complete. At the\nsame time, there is a lost opportunity for better design exploration guided by\nstructural feedback. In general, the earlier in the design process the\niteration happens, the greater the benefits in cost efficiency and informed\nde-sign exploration, which can lead to higher-quality creative results. In\norder to facilitate an informed exploration in the early design stage, we\nsuggest the automation of fundamental structural engineering tasks and\nintroduce ApproxiFramer, a Machine Learning-based system for the automatic\ngeneration of structural layouts from building plan sketches in real-time. The\nsystem aims to assist architects by presenting them with feasible structural\nsolutions during the conceptual phase so that they proceed with their design\nwith adequate knowledge of its structural implications. In this paper, we\ndescribe the system and evaluate the performance of a proof-of-concept\nimplementation in the domain of orthogonal, metal, rigid structures. We trained\na Convolutional Neural Net to iteratively generate structural design solutions\nfor sketch-level building plans using a synthetic dataset and achieved an\naverage error of 2.2% in the predicted positions of the columns.",
          "link": "http://arxiv.org/abs/2107.08567",
          "publishedOn": "2021-07-20T02:04:43.390Z",
          "wordCount": 671,
          "title": "Structural Design Recommendations in the Early Design Phase using Machine Learning. (arXiv:2107.08567v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiyiwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiqi Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1\">Qi Long</a>",
          "description": "Bayesian neural network (BNN) allows for uncertainty quantification in\nprediction, offering an advantage over regular neural networks that has not\nbeen explored in the differential privacy (DP) framework. We fill this\nimportant gap by leveraging recent development in Bayesian deep learning and\nprivacy accounting to offer a more precise analysis of the trade-off between\nprivacy and accuracy in BNN. We propose three DP-BNNs that characterize the\nweight uncertainty for the same network architecture in distinct ways, namely\nDP-SGLD (via the noisy gradient method), DP-BBP (via changing the parameters of\ninterest) and DP-MC Dropout (via the model architecture). Interestingly, we\nshow a new equivalence between DP-SGD and DP-SGLD, implying that some\nnon-Bayesian DP training naturally allows for uncertainty quantification.\nHowever, the hyperparameters such as learning rate and batch size, can have\ndifferent or even opposite effects in DP-SGD and DP-SGLD.\n\nExtensive experiments are conducted to compare DP-BNNs, in terms of privacy\nguarantee, prediction accuracy, uncertainty quantification, calibration,\ncomputation speed, and generalizability to network architecture. As a result,\nwe observe a new tradeoff between the privacy and the reliability. When\ncompared to non-DP and non-Bayesian approaches, DP-SGLD is remarkably accurate\nunder strong privacy guarantee, demonstrating the great potential of DP-BNN in\nreal-world tasks.",
          "link": "http://arxiv.org/abs/2107.08461",
          "publishedOn": "2021-07-20T02:04:43.317Z",
          "wordCount": 645,
          "title": "Differentially Private Bayesian Neural Networks on Accuracy, Privacy and Reliability. (arXiv:2107.08461v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08514",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kokate_P/0/1/0/all/0/1\">Pranali Kokate</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pancholi_S/0/1/0/all/0/1\">Sidharth Pancholi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joshi_A/0/1/0/all/0/1\">Amit M. Joshi</a>",
          "description": "The Brain-Computer Interface system is a profoundly developing area of\nexperimentation for Motor activities which plays vital role in decoding\ncognitive activities. Classification of Cognitive-Motor Imagery activities from\nEEG signals is a critical task. Hence proposed a unique algorithm for\nclassifying left/right-hand movements by utilizing Multi-layer Perceptron\nNeural Network. Handcrafted statistical Time domain and Power spectral density\nfrequency domain features were extracted and obtained a combined accuracy of\n96.02%. Results were compared with the deep learning framework. In addition to\naccuracy, Precision, F1-Score, and recall was considered as the performance\nmetrics. The intervention of unwanted signals contaminates the EEG signals\nwhich influence the performance of the algorithm. Therefore, a novel approach\nwas approached to remove the artifacts using Independent Components Analysis\nwhich boosted the performance. Following the selection of appropriate feature\nvectors that provided acceptable accuracy. The same method was used on all nine\nsubjects. As a result, intra-subject accuracy was obtained for 9 subjects\n94.72%. The results show that the proposed approach would be useful to classify\nthe upper limb movements accurately.",
          "link": "http://arxiv.org/abs/2107.08514",
          "publishedOn": "2021-07-20T02:04:43.244Z",
          "wordCount": 639,
          "title": "Classification of Upper Arm Movements from EEG signals using Machine Learning with ICA Analysis. (arXiv:2107.08514v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tahmasebian_F/0/1/0/all/0/1\">Farnaz Tahmasebian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Li Xiong</a>",
          "description": "Federated learning is a prominent framework that enables clients (e.g.,\nmobile devices or organizations) to train a collaboratively global model under\na central server's orchestration while keeping local training datasets'\nprivacy. However, the aggregation step in federated learning is vulnerable to\nadversarial attacks as the central server cannot manage clients' behavior.\nTherefore, the global model's performance and convergence of the training\nprocess will be affected under such attacks.To mitigate this vulnerability\nissue, we propose a novel robust aggregation algorithm inspired by the truth\ninference methods in crowdsourcing via incorporating the worker's reliability\ninto aggregation. We evaluate our solution on three real-world datasets with a\nvariety of machine learning models. Experimental results show that our solution\nensures robust federated learning and is resilient to various types of attacks,\nincluding noisy data attacks, Byzantine attacks, and label flipping attacks.",
          "link": "http://arxiv.org/abs/2107.08402",
          "publishedOn": "2021-07-20T02:04:43.220Z",
          "wordCount": 582,
          "title": "RobustFed: A Truth Inference Approach for Robust Federated Learning. (arXiv:2107.08402v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingfeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>",
          "description": "Graphically-rich applications such as games are ubiquitous with attractive\nvisual effects of Graphical User Interface (GUI) that offers a bridge between\nsoftware applications and end-users. However, various types of graphical\nglitches may arise from such GUI complexity and have become one of the main\ncomponent of software compatibility issues. Our study on bug reports from game\ndevelopment teams in NetEase Inc. indicates that graphical glitches frequently\noccur during the GUI rendering and severely degrade the quality of\ngraphically-rich applications such as video games. Existing automated testing\ntechniques for such applications focus mainly on generating various GUI test\nsequences and check whether the test sequences can cause crashes. These\ntechniques require constant human attention to captures non-crashing bugs such\nas bugs causing graphical glitches. In this paper, we present the first step in\nautomating the test oracle for detecting non-crashing bugs in graphically-rich\napplications. Specifically, we propose \\texttt{GLIB} based on a code-based data\naugmentation technique to detect game GUI glitches. We perform an evaluation of\n\\texttt{GLIB} on 20 real-world game apps (with bug reports available) and the\nresult shows that \\texttt{GLIB} can achieve 100\\% precision and 99.5\\% recall\nin detecting non-crashing bugs such as game GUI glitches. Practical application\nof \\texttt{GLIB} on another 14 real-world games (without bug reports) further\ndemonstrates that \\texttt{GLIB} can effectively uncover GUI glitches, with 48\nof 53 bugs reported by \\texttt{GLIB} having been confirmed and fixed so far.",
          "link": "http://arxiv.org/abs/2106.10507",
          "publishedOn": "2021-07-19T01:59:51.065Z",
          "wordCount": 727,
          "title": "GLIB: Towards Automated Test Oracle for Graphically-Rich Applications. (arXiv:2106.10507v3 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1\">Jessica Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Um_I/0/1/0/all/0/1\">In Hwa Um</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1\">Ognjen Arandjelovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1\">David J Harrison</a>",
          "description": "Multiplex immunofluorescence and immunohistochemistry benefit patients by\nallowing cancer pathologists to identify several proteins expressed on the\nsurface of cells, enabling cell classification, better understanding of the\ntumour micro-environment, more accurate diagnoses, prognoses, and tailored\nimmunotherapy based on the immune status of individual patients. However, they\nare expensive and time consuming processes which require complex staining and\nimaging techniques by expert technicians. Hoechst staining is much cheaper and\neasier to perform, but is not typically used in this case as it binds to DNA\nrather than to the proteins targeted by immunofluorescent techniques, and it\nwas not previously thought possible to differentiate cells expressing these\nproteins based only on DNA morphology. In this work we show otherwise, training\na deep convolutional neural network to identify cells expressing three proteins\n(T lymphocyte markers CD3 and CD8, and the B lymphocyte marker CD20) with\ngreater than 90% precision and recall, from Hoechst 33342 stained tissue only.\nOur model learns previously unknown morphological features associated with\nexpression of these proteins which can be used to accurately differentiate\nlymphocyte subtypes for use in key prognostic metrics such as assessment of\nimmune cell infiltration,and thereby predict and improve patient outcomes\nwithout the need for costly multiplex immunofluorescence.",
          "link": "http://arxiv.org/abs/2107.04388",
          "publishedOn": "2021-07-19T01:59:51.035Z",
          "wordCount": 673,
          "title": "Hoechst Is All You Need: Lymphocyte Classification with Deep Learning. (arXiv:2107.04388v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qinghua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miryoosefi_S/0/1/0/all/0/1\">Sobhan Miryoosefi</a>",
          "description": "Finding the minimal structural assumptions that empower sample-efficient\nlearning is one of the most important research directions in Reinforcement\nLearning (RL). This paper advances our understanding of this fundamental\nquestion by introducing a new complexity measure -- Bellman Eluder (BE)\ndimension. We show that the family of RL problems of low BE dimension is\nremarkably rich, which subsumes a vast majority of existing tractable RL\nproblems including but not limited to tabular MDPs, linear MDPs, reactive\nPOMDPs, low Bellman rank problems as well as low Eluder dimension problems.\nThis paper further designs a new optimization-based algorithm -- GOLF, and\nreanalyzes a hypothesis elimination-based algorithm -- OLIVE (proposed in Jiang\net al., 2017). We prove that both algorithms learn the near-optimal policies of\nlow BE dimension problems in a number of samples that is polynomial in all\nrelevant parameters, but independent of the size of state-action space. Our\nregret and sample complexity results match or improve the best existing results\nfor several well-known subclasses of low BE dimension problems.",
          "link": "http://arxiv.org/abs/2102.00815",
          "publishedOn": "2021-07-19T00:49:08.148Z",
          "wordCount": 659,
          "title": "Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms. (arXiv:2102.00815v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03686",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thanthrige_U/0/1/0/all/0/1\">Udaya S.K.P. Miriya Thanthrige</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_P/0/1/0/all/0/1\">Peter Jung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sezgin_A/0/1/0/all/0/1\">Aydin Sezgin</a>",
          "description": "We address the detection of material defects, which are inside a layered\nmaterial structure using compressive sensing based multiple-input and\nmultiple-output (MIMO) wireless radar. Here, the strong clutter due to the\nreflection of the layered structure's surface often makes the detection of the\ndefects challenging. Thus, sophisticated signal separation methods are required\nfor improved defect detection. In many scenarios, the number of defects that we\nare interested in is limited and the signaling response of the layered\nstructure can be modeled as a low-rank structure. Therefore, we propose joint\nrank and sparsity minimization for defect detection. In particular, we propose\na non-convex approach based on the iteratively reweighted nuclear and\n$\\ell_1-$norm (a double-reweighted approach) to obtain a higher accuracy\ncompared to the conventional nuclear norm and $\\ell_1-$norm minimization. To\nthis end, an iterative algorithm is designed to estimate the low-rank and\nsparse contributions. Further, we propose deep learning to learn the parameters\nof the algorithm (i.e., algorithm unfolding) to improve the accuracy and the\nspeed of convergence of the algorithm. Our numerical results show that the\nproposed approach outperforms the conventional approaches in terms of mean\nsquare errors of the recovered low-rank and sparse components and the speed of\nconvergence.",
          "link": "http://arxiv.org/abs/2106.03686",
          "publishedOn": "2021-07-19T00:49:08.127Z",
          "wordCount": 681,
          "title": "Deep Unfolding of Iteratively Reweighted ADMM for Wireless RF Sensing. (arXiv:2106.03686v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1\">Yann Dubois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwab_D/0/1/0/all/0/1\">David J. Schwab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedantam_R/0/1/0/all/0/1\">Ramakrishna Vedantam</a>",
          "description": "We address the question of characterizing and finding optimal representations\nfor supervised learning. Traditionally, this question has been tackled using\nthe Information Bottleneck, which compresses the inputs while retaining\ninformation about the targets, in a decoder-agnostic fashion. In machine\nlearning, however, our goal is not compression but rather generalization, which\nis intimately linked to the predictive family or decoder of interest (e.g.\nlinear classifier). We propose the Decodable Information Bottleneck (DIB) that\nconsiders information retention and compression from the perspective of the\ndesired predictive family. As a result, DIB gives rise to representations that\nare optimal in terms of expected test performance and can be estimated with\nguarantees. Empirically, we show that the framework can be used to enforce a\nsmall generalization gap on downstream classifiers and to predict the\ngeneralization ability of neural networks.",
          "link": "http://arxiv.org/abs/2009.12789",
          "publishedOn": "2021-07-19T00:49:08.103Z",
          "wordCount": 611,
          "title": "Learning Optimal Representations with the Decodable Information Bottleneck. (arXiv:2009.12789v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yujiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1\">Bernhard Sick</a>",
          "description": "Catastrophic forgetting means that a trained neural network model gradually\nforgets the previously learned tasks when being retrained on new tasks.\nOvercoming the forgetting problem is a major problem in machine learning.\nNumerous continual learning algorithms are very successful in incremental\nlearning of classification tasks, where new samples with their labels appear\nfrequently. However, there is currently no research that addresses the\ncatastrophic forgetting problem in regression tasks as far as we know. This\nproblem has emerged as one of the primary constraints in some applications,\nsuch as renewable energy forecasts. This article clarifies problem-related\ndefinitions and proposes a new methodological framework that can forecast\ntargets and update itself by means of continual learning. The framework\nconsists of forecasting neural networks and buffers, which store newly\ncollected data from a non-stationary data stream in an application. The changed\nprobability distribution of the data stream, which the framework has\nidentified, will be learned sequentially. The framework is called CLeaR\n(Continual Learning for Regression Tasks), where components can be flexibly\ncustomized for a specific application scenario. We design two sets of\nexperiments to evaluate the CLeaR framework concerning fitting error\n(training), prediction error (test), and forgetting ratio. The first one is\nbased on an artificial time series to explore how hyperparameters affect the\nCLeaR framework. The second one is designed with data collected from European\nwind farms to evaluate the CLeaR framework's performance in a real-world\napplication. The experimental results demonstrate that the CLeaR framework can\ncontinually acquire knowledge in the data stream and improve the prediction\naccuracy. The article concludes with further research issues arising from\nrequirements to extend the framework.",
          "link": "http://arxiv.org/abs/2101.00926",
          "publishedOn": "2021-07-19T00:49:08.077Z",
          "wordCount": 751,
          "title": "CLeaR: An Adaptive Continual Learning Framework for Regression Tasks. (arXiv:2101.00926v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shulman_Y/0/1/0/all/0/1\">Yaniv Shulman</a>",
          "description": "Quantization based model compression serves as high performing and fast\napproach for inference that yields models which are highly compressed when\ncompared to their full-precision floating point counterparts. The most extreme\nquantization is a 1-bit representation of parameters such that they have only\ntwo possible values, typically -1(0) or +1, enabling efficient implementation\nof the ubiquitous dot product using only additions. The main contribution of\nthis work is the introduction of a method to smooth the combinatorial problem\nof determining a binary vector of weights to minimize the expected loss for a\ngiven objective by means of empirical risk minimization with backpropagation.\nThis is achieved by approximating a multivariate binary state over the weights\nutilizing a deterministic and differentiable transformation of real-valued,\ncontinuous parameters. The proposed method adds little overhead in training,\ncan be readily applied without any substantial modifications to the original\narchitecture, does not introduce additional saturating nonlinearities or\nauxiliary losses, and does not prohibit applying other methods for binarizing\nthe activations. Contrary to common assertions made in the literature, it is\ndemonstrated that binary weighted networks can train well with the same\nstandard optimization techniques and similar hyperparameter settings as their\nfull-precision counterparts, specifically momentum SGD with large learning\nrates and $L_2$ regularization. To conclude experiments demonstrate the method\nperforms remarkably well across a number of inductive image classification\ntasks with various architectures compared to their full-precision counterparts.\nThe source code is publicly available at\nhttps://bitbucket.org/YanivShu/binary_weighted_networks_public.",
          "link": "http://arxiv.org/abs/2107.01400",
          "publishedOn": "2021-07-19T00:49:08.072Z",
          "wordCount": 691,
          "title": "Exact Backpropagation in Binary Weighted Networks with Group Weight Transformations. (arXiv:2107.01400v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chari_S/0/1/0/all/0/1\">Shruthi Chari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Prithwish Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghalwash_M/0/1/0/all/0/1\">Mohamed Ghalwash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seneviratne_O/0/1/0/all/0/1\">Oshani Seneviratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eyigoz_E/0/1/0/all/0/1\">Elif K. Eyigoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruen_D/0/1/0/all/0/1\">Daniel M. Gruen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saiz_F/0/1/0/all/0/1\">Fernando Suarez Saiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Ching-Hua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rojas_P/0/1/0/all/0/1\">Pablo Meyer Rojas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_D/0/1/0/all/0/1\">Deborah L. McGuinness</a>",
          "description": "Academic advances of AI models in high-precision domains, like healthcare,\nneed to be made explainable in order to enhance real-world adoption. Our past\nstudies and ongoing interactions indicate that medical experts can use AI\nsystems with greater trust if there are ways to connect the model inferences\nabout patients to explanations that are tied back to the context of use.\nSpecifically, risk prediction is a complex problem of diagnostic and\ninterventional importance to clinicians wherein they consult different sources\nto make decisions. To enable the adoption of the ever improving AI risk\nprediction models in practice, we have begun to explore techniques to\ncontextualize such models along three dimensions of interest: the patients'\nclinical state, AI predictions about their risk of complications, and\nalgorithmic explanations supporting the predictions. We validate the importance\nof these dimensions by implementing a proof-of-concept (POC) in type-2 diabetes\n(T2DM) use case where we assess the risk of chronic kidney disease (CKD) - a\ncommon T2DM comorbidity. Within the POC, we include risk prediction models for\nCKD, post-hoc explainers of the predictions, and other natural-language modules\nwhich operationalize domain knowledge and CPGs to provide context. With primary\ncare physicians (PCP) as our end-users, we present our initial results and\nclinician feedback in this paper. Our POC approach covers multiple knowledge\nsources and clinical scenarios, blends knowledge to explain data and\npredictions to PCPs, and received an enthusiastic response from our medical\nexpert.",
          "link": "http://arxiv.org/abs/2107.02359",
          "publishedOn": "2021-07-19T00:49:07.963Z",
          "wordCount": 746,
          "title": "Leveraging Clinical Context for User-Centered Explainability: A Diabetes Use Case. (arXiv:2107.02359v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14172",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kileel_J/0/1/0/all/0/1\">Joe Kileel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moscovich_A/0/1/0/all/0/1\">Amit Moscovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelesko_N/0/1/0/all/0/1\">Nathan Zelesko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_A/0/1/0/all/0/1\">Amit Singer</a>",
          "description": "Manifold learning methods play a prominent role in nonlinear dimensionality\nreduction and other tasks involving high-dimensional data sets with low\nintrinsic dimensionality. Many of these methods are graph-based: they associate\na vertex with each data point and a weighted edge with each pair. Existing\ntheory shows that the Laplacian matrix of the graph converges to the\nLaplace-Beltrami operator of the data manifold, under the assumption that the\npairwise affinities are based on the Euclidean norm. In this paper, we\ndetermine the limiting differential operator for graph Laplacians constructed\nusing $\\textit{any}$ norm. Our proof involves an interplay between the second\nfundamental form of the manifold and the convex geometry of the given norm's\nunit ball. To demonstrate the potential benefits of non-Euclidean norms in\nmanifold learning, we consider the task of mapping the motion of large\nmolecules with continuous variability. In a numerical simulation we show that a\nmodified Laplacian eigenmaps algorithm, based on the Earthmover's distance,\noutperforms the classic Euclidean Laplacian eigenmaps, both in terms of\ncomputational cost and the sample size needed to recover the intrinsic\ngeometry.",
          "link": "http://arxiv.org/abs/2012.14172",
          "publishedOn": "2021-07-19T00:49:07.957Z",
          "wordCount": 651,
          "title": "Manifold learning with arbitrary norms. (arXiv:2012.14172v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhizhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1\">Charless Fowlkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1\">Marzia Polito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhotika_R/0/1/0/all/0/1\">Rahul Bhotika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "Traditionally, distillation has been used to train a student model to emulate\nthe input/output functionality of a teacher. A more useful goal than emulation,\nyet under-explored, is for the student to learn feature representations that\ntransfer well to future tasks. However, we observe that standard distillation\nof task-specific teachers actually *reduces* the transferability of student\nrepresentations to downstream tasks. We show that a multi-head, multi-task\ndistillation method using an unlabeled proxy dataset and a generalist teacher\nis sufficient to consolidate representations from task-specific teacher(s) and\nimprove downstream performance, outperforming the teacher(s) and the strong\nbaseline of ImageNet pretrained features. Our method can also combine the\nrepresentational knowledge of multiple teachers trained on one or multiple\ndomains into a single model, whose representation is improved on all teachers'\ndomain(s).",
          "link": "http://arxiv.org/abs/2107.08039",
          "publishedOn": "2021-07-19T00:49:07.951Z",
          "wordCount": 568,
          "title": "Representation Consolidation for Training Expert Students. (arXiv:2107.08039v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zombori_Z/0/1/0/all/0/1\">Zsolt Zombori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1\">Josef Urban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsak_M/0/1/0/all/0/1\">Miroslav Ol&#x161;&#xe1;k</a>",
          "description": "In this work we study how to learn good algorithms for selecting reasoning\nsteps in theorem proving. We explore this in the connection tableau calculus\nimplemented by leanCoP where the partial tableau provides a clean and compact\nnotion of a state to which a limited number of inferences can be applied. We\nstart by incorporating a state-of-the-art learning algorithm -- a graph neural\nnetwork (GNN) -- into the plCoP theorem prover. Then we use it to observe the\nsystem's behaviour in a reinforcement learning setting, i.e., when learning\ninference guidance from successful Monte-Carlo tree searches on many problems.\nDespite its better pattern matching capability, the GNN initially performs\nworse than a simpler previously used learning algorithm. We observe that the\nsimpler algorithm is less confident, i.e., its recommendations have higher\nentropy. This leads us to explore how the entropy of the inference selection\nimplemented via the neural network influences the proof search. This is related\nto research in human decision-making under uncertainty, and in particular the\nprobability matching theory. Our main result shows that a proper entropy\nregularisation, i.e., training the GNN not to be overconfident, greatly\nimproves plCoP's performance on a large mathematical corpus.",
          "link": "http://arxiv.org/abs/2105.14706",
          "publishedOn": "2021-07-19T00:49:07.934Z",
          "wordCount": 664,
          "title": "The Role of Entropy in Guiding a Connection Prover. (arXiv:2105.14706v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ablett_T/0/1/0/all/0/1\">Trevor Ablett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1\">Yifan Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1\">Jonathan Kelly</a>",
          "description": "Learned visuomotor policies have shown considerable success as an alternative\nto traditional, hand-crafted frameworks for robotic manipulation. Surprisingly,\nan extension of these methods to the multiview domain is relatively unexplored.\nA successful multiview policy could be deployed on a mobile manipulation\nplatform, allowing the robot to complete a task regardless of its view of the\nscene. In this work, we demonstrate that a multiview policy can be found\nthrough imitation learning by collecting data from a variety of viewpoints. We\nillustrate the general applicability of the method by learning to complete\nseveral challenging multi-stage and contact-rich tasks, from numerous\nviewpoints, both in a simulated environment and on a real mobile manipulation\nplatform. Furthermore, we analyze our policies to determine the benefits of\nlearning from multiview data compared to learning with data collected from a\nfixed perspective. We show that learning from multiview data results in little,\nif any, penalty to performance for a fixed-view task compared to learning with\nan equivalent amount of fixed-view data. Finally, we examine the visual\nfeatures learned by the multiview and fixed-view policies. Our results indicate\nthat multiview policies implicitly learn to identify spatially correlated\nfeatures.",
          "link": "http://arxiv.org/abs/2104.13907",
          "publishedOn": "2021-07-19T00:49:07.928Z",
          "wordCount": 673,
          "title": "Seeing All the Angles: Learning Multiview Manipulation Policies for Contact-Rich Tasks from Demonstrations. (arXiv:2104.13907v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Charles Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lemay_A/0/1/0/all/0/1\">Andreanne Lemay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoebel_K/0/1/0/all/0/1\">Katharina Hoebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>",
          "description": "As machine learning (ML) continue to be integrated into healthcare systems\nthat affect clinical decision making, new strategies will need to be\nincorporated in order to effectively detect and evaluate subgroup disparities\nto ensure accountability and generalizability in clinical workflows. In this\npaper, we explore how epistemic uncertainty can be used to evaluate disparity\nin patient demographics (race) and data acquisition (scanner) subgroups for\nbreast density assessment on a dataset of 108,190 mammograms collected from 33\nclinical sites. Our results show that even if aggregate performance is\ncomparable, the choice of uncertainty quantification metric can significantly\nthe subgroup level. We hope this analysis can promote further work on how\nuncertainty can be leveraged to increase transparency of machine learning\napplications for clinical deployment.",
          "link": "http://arxiv.org/abs/2107.02716",
          "publishedOn": "2021-07-19T00:49:07.921Z",
          "wordCount": 591,
          "title": "Evaluating subgroup disparity using epistemic uncertainty in mammography. (arXiv:2107.02716v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.09048",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Rolle_A/0/1/0/all/0/1\">Alexander Rolle</a>, <a href=\"http://arxiv.org/find/math/1/au:+Scoccola_L/0/1/0/all/0/1\">Luis Scoccola</a>",
          "description": "We present a multiscale, consistent approach to density-based clustering that\nsatisfies stability theorems -- in both the input data and in the parameters --\nwhich hold without distributional assumptions. The stability in the input data\nis with respect to the Gromov--Hausdorff--Prokhorov distance on metric\nprobability spaces and interleaving distances between (multi-parameter)\nhierarchical clusterings we introduce. We prove stability results for standard\nsimplification procedures for hierarchical clusterings, which can be combined\nwith our approach to yield a stable flat clustering algorithm. We illustrate\nthe stability of the approach with computational examples. Our framework is\nbased on the concepts of persistence and interleaving distance from Topological\nData Analysis.",
          "link": "http://arxiv.org/abs/2005.09048",
          "publishedOn": "2021-07-19T00:49:07.914Z",
          "wordCount": 564,
          "title": "Stable and consistent density-based clustering. (arXiv:2005.09048v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02522",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Wu_J/0/1/0/all/0/1\">Junran Wu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Chen_X/0/1/0/all/0/1\">Xueyuan Chen</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Li_S/0/1/0/all/0/1\">Shangzhe Li</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zhao_J/0/1/0/all/0/1\">Jichang Zhao</a>",
          "description": "Great research efforts have been devoted to exploiting deep neural networks\nin stock prediction. While long-range dependencies and chaotic property are\nstill two major issues that lower the performance of state-of-the-art deep\nlearning models in forecasting future price trends. In this study, we propose a\nnovel framework to address both issues. Specifically, in terms of transforming\ntime series into complex networks, we convert market price series into graphs.\nThen, structural information, referring to associations among temporal points\nand the node weights, is extracted from the mapped graphs to resolve the\nproblems regarding long-range dependencies and the chaotic property. We take\ngraph embeddings to represent the associations among temporal points as the\nprediction model inputs. Node weights are used as a priori knowledge to enhance\nthe learning of temporal attention. The effectiveness of our proposed framework\nis validated using real-world stock data, and our approach obtains the best\nperformance among several state-of-the-art benchmarks. Moreover, in the\nconducted trading simulations, our framework further obtains the highest\ncumulative profits. Our results supplement the existing applications of complex\nnetwork methods in the financial realm and provide insightful implications for\ninvestment applications regarding decision support in financial markets.",
          "link": "http://arxiv.org/abs/2106.02522",
          "publishedOn": "2021-07-19T00:49:07.896Z",
          "wordCount": 671,
          "title": "Price graphs: Utilizing the structural information of financial time series for stock prediction. (arXiv:2106.02522v3 [q-fin.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Tyler L. Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>",
          "description": "Humans are incredibly good at transferring knowledge from one domain to\nanother, enabling rapid learning of new tasks. Likewise, transfer learning has\nenabled enormous success in many computer vision problems using pretraining.\nHowever, the benefits of transfer in multi-domain learning, where a network\nlearns multiple tasks defined by different datasets, has not been adequately\nstudied. Learning multiple domains could be beneficial or these domains could\ninterfere with each other given limited network capacity. In this work, we\ndecipher the conditions where interference and knowledge transfer occur in\nmulti-domain learning. We propose new metrics disentangling interference and\ntransfer and set up experimental protocols. We further examine the roles of\nnetwork capacity, task grouping, and dynamic loss weighting in reducing\ninterference and facilitating transfer. We demonstrate our findings on the\nCIFAR-100, MiniPlaces, and Tiny-ImageNet datasets.",
          "link": "http://arxiv.org/abs/2107.05445",
          "publishedOn": "2021-07-19T00:49:07.890Z",
          "wordCount": 591,
          "title": "Disentangling Transfer and Interference in Multi-Domain Learning. (arXiv:2107.05445v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1\">Spencer Frei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Difan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We consider a binary classification problem when the data comes from a\nmixture of two isotropic distributions satisfying concentration and\nanti-concentration properties enjoyed by log-concave distributions among\nothers. We show that there exists a universal constant $C_{\\mathrm{err}}>0$\nsuch that if a pseudolabeler $\\boldsymbol{\\beta}_{\\mathrm{pl}}$ can achieve\nclassification error at most $C_{\\mathrm{err}}$, then for any $\\varepsilon>0$,\nan iterative self-training algorithm initialized at $\\boldsymbol{\\beta}_0 :=\n\\boldsymbol{\\beta}_{\\mathrm{pl}}$ using pseudolabels $\\hat y =\n\\mathrm{sgn}(\\langle \\boldsymbol{\\beta}_t, \\mathbf{x}\\rangle)$ and using at\nmost $\\tilde O(d/\\varepsilon^2)$ unlabeled examples suffices to learn the\nBayes-optimal classifier up to $\\varepsilon$ error, where $d$ is the ambient\ndimension. That is, self-training converts weak learners to strong learners\nusing only unlabeled examples. We additionally show that by running gradient\ndescent on the logistic loss one can obtain a pseudolabeler\n$\\boldsymbol{\\beta}_{\\mathrm{pl}}$ with classification error $C_{\\mathrm{err}}$\nusing only $O(d)$ labeled examples (i.e., independent of $\\varepsilon$).\nTogether our results imply that mixture models can be learned to within\n$\\varepsilon$ of the Bayes-optimal accuracy using at most $O(d)$ labeled\nexamples and $\\tilde O(d/\\varepsilon^2)$ unlabeled examples by way of a\nsemi-supervised self-training algorithm.",
          "link": "http://arxiv.org/abs/2106.13805",
          "publishedOn": "2021-07-19T00:49:07.884Z",
          "wordCount": 650,
          "title": "Self-training Converts Weak Learners to Strong Learners in Mixture Models. (arXiv:2106.13805v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Guanya Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honig_W/0/1/0/all/0/1\">Wolfgang H&#xf6;nig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xichen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yisong Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1\">Soon-Jo Chung</a>",
          "description": "We present Neural-Swarm2, a learning-based method for motion planning and\ncontrol that allows heterogeneous multirotors in a swarm to safely fly in close\nproximity. Such operation for drones is challenging due to complex aerodynamic\ninteraction forces, such as downwash generated by nearby drones and ground\neffect. Conventional planning and control methods neglect capturing these\ninteraction forces, resulting in sparse swarm configuration during flight. Our\napproach combines a physics-based nominal dynamics model with learned Deep\nNeural Networks (DNNs) with strong Lipschitz properties. We make use of two\ntechniques to accurately predict the aerodynamic interactions between\nheterogeneous multirotors: i) spectral normalization for stability and\ngeneralization guarantees of unseen data and ii) heterogeneous deep sets for\nsupporting any number of heterogeneous neighbors in a permutation-invariant\nmanner without reducing expressiveness. The learned residual dynamics benefit\nboth the proposed interaction-aware multi-robot motion planning and the\nnonlinear tracking control design because the learned interaction forces reduce\nthe modelling errors. Experimental results demonstrate that Neural-Swarm2 is\nable to generalize to larger swarms beyond training cases and significantly\noutperforms a baseline nonlinear tracking controller with up to three times\nreduction in worst-case tracking errors.",
          "link": "http://arxiv.org/abs/2012.05457",
          "publishedOn": "2021-07-19T00:49:07.877Z",
          "wordCount": 683,
          "title": "Neural-Swarm2: Planning and Control of Heterogeneous Multirotor Swarms using Learned Interactions. (arXiv:2012.05457v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Ming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yizhen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>",
          "description": "Graph representation learning plays a vital role in processing\ngraph-structured data. However, prior arts on graph representation learning\nheavily rely on labeling information. To overcome this problem, inspired by the\nrecent success of graph contrastive learning and Siamese networks in visual\nrepresentation learning, we propose a novel self-supervised approach in this\npaper to learn node representations by enhancing Siamese self-distillation with\nmulti-scale contrastive learning. Specifically, we first generate two augmented\nviews from the input graph based on local and global perspectives. Then, we\nemploy two objectives called cross-view and cross-network contrastiveness to\nmaximize the agreement between node representations across different views and\nnetworks. To demonstrate the effectiveness of our approach, we perform\nempirical experiments on five real-world datasets. Our method not only achieves\nnew state-of-the-art results but also surpasses some semi-supervised\ncounterparts by large margins. Code is made available at\nhttps://github.com/GRAND-Lab/MERIT",
          "link": "http://arxiv.org/abs/2105.05682",
          "publishedOn": "2021-07-19T00:49:07.866Z",
          "wordCount": 632,
          "title": "Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning. (arXiv:2105.05682v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muniz_I/0/1/0/all/0/1\">I. Muniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camargo_F/0/1/0/all/0/1\">F. H. F. Camargo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marques_A/0/1/0/all/0/1\">A. Marques</a>",
          "description": "With the constant advancements of genetic engineering, a common concern is to\nbe able to identify the lab-of-origin of genetically engineered DNA sequences.\nFor that reason, AltLabs has hosted the genetic Engineering Attribution\nChallenge to gather many teams to propose new tools to solve this problem. Here\nwe show our proposed method to rank the most likely labs-of-origin and generate\nembeddings for DNA sequences and labs. These embeddings can also perform\nvarious other tasks, like clustering both DNA sequences and labs and using them\nas features for Machine Learning models applied to solve other problems. This\nwork demonstrates that our method outperforms the classic training method for\nthis task while generating other helpful information.",
          "link": "http://arxiv.org/abs/2107.07878",
          "publishedOn": "2021-07-19T00:49:07.860Z",
          "wordCount": 567,
          "title": "Ranking labs-of-origin for genetically engineered DNA using Metric Learning. (arXiv:2107.07878v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10785",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Godaz_R/0/1/0/all/0/1\">Reza Godaz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Monsefi_R/0/1/0/all/0/1\">Reza Monsefi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Toutounian_F/0/1/0/all/0/1\">Faezeh Toutounian</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hosseini_R/0/1/0/all/0/1\">Reshad Hosseini</a>",
          "description": "In this paper we tackle two important challenges related to the accurate\npartial singular value decomposition (SVD) and numerical rank estimation of a\nhuge matrix to use in low-rank learning problems in a fast way. We use the\nconcepts of Krylov subspaces such as the Golub-Kahan bidiagonalization process\nas well as Ritz vectors to achieve these goals. Our experiments identify\nvarious advantages of the proposed methods compared to traditional and\nrandomized SVD (R-SVD) methods with respect to the accuracy of the singular\nvalues and corresponding singular vectors computed in a similar execution time.\nThe proposed methods are appropriate for applications involving huge matrices\nwhere accuracy in all spectrum of the desired singular values, and also all of\ncorresponding singular vectors is essential. We evaluate our method in the real\napplication of Riemannian similarity learning (RSL) between two various image\ndatasets of MNIST and USPS.",
          "link": "http://arxiv.org/abs/2104.10785",
          "publishedOn": "2021-07-19T00:49:07.855Z",
          "wordCount": 611,
          "title": "Accurate and fast matrix factorization for low-rank learning. (arXiv:2104.10785v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.04785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Neural architecture search (NAS) with an accuracy predictor that predicts the\naccuracy of candidate architectures has drawn increasing attention due to its\nsimplicity and effectiveness. Previous works usually employ neural\nnetwork-based predictors which require more delicate design and are easy to\noverfit. Considering that most architectures are represented as sequences of\ndiscrete symbols which are more like tabular data and preferred by non-neural\npredictors, in this paper, we study an alternative approach which uses\nnon-neural model for accuracy prediction. Specifically, as decision tree based\nmodels can better handle tabular data, we leverage gradient boosting decision\ntree (GBDT) as the predictor for NAS. We demonstrate that the GBDT predictor\ncan achieve comparable (if not better) prediction accuracy than neural network\nbased predictors. Moreover, considering that a compact search space can ease\nthe search process, we propose to prune the search space gradually according to\nimportant features derived from GBDT. In this way, NAS can be performed by\nfirst pruning the search space and then searching a neural architecture, which\nis more efficient and effective. Experiments on NASBench-101 and ImageNet\ndemonstrate the effectiveness of using GBDT as predictor for NAS: (1) On\nNASBench-101, it is 22x, 8x, and 6x more sample efficient than random search,\nregularized evolution, and Monte Carlo Tree Search (MCTS) in finding the global\noptimum; (2) It achieves 24.2% top-1 error rate on ImageNet, and further\nachieves 23.4% top-1 error rate on ImageNet when enhanced with search space\npruning. Code is provided in the supplementary materials.",
          "link": "http://arxiv.org/abs/2007.04785",
          "publishedOn": "2021-07-19T00:49:07.849Z",
          "wordCount": 738,
          "title": "Accuracy Prediction with Non-neural Model for Neural Architecture Search. (arXiv:2007.04785v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_P/0/1/0/all/0/1\">Parikshit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_P/0/1/0/all/0/1\">Prathamesh Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1\">Sunita Sarawagi</a>",
          "description": "We present DeepMVI, a deep learning method for missing value imputation in\nmultidimensional time-series datasets. Missing values are commonplace in\ndecision support platforms that aggregate data over long time stretches from\ndisparate sources, and reliable data analytics calls for careful handling of\nmissing data. One strategy is imputing the missing values, and a wide variety\nof algorithms exist spanning simple interpolation, matrix factorization methods\nlike SVD, statistical models like Kalman filters, and recent deep learning\nmethods. We show that often these provide worse results on aggregate analytics\ncompared to just excluding the missing data. DeepMVI uses a neural network to\ncombine fine-grained and coarse-grained patterns along a time series, and\ntrends from related series across categorical dimensions. After failing with\noff-the-shelf neural architectures, we design our own network that includes a\ntemporal transformer with a novel convolutional window feature, and kernel\nregression with learned embeddings. The parameters and their training are\ndesigned carefully to generalize across different placements of missing blocks\nand data characteristics. Experiments across nine real datasets, four different\nmissing scenarios, comparing seven existing methods show that DeepMVI is\nsignificantly more accurate, reducing error by more than 50% in more than half\nthe cases, compared to the best existing method. Although slower than simpler\nmatrix factorization methods, we justify the increased time overheads by\nshowing that DeepMVI is the only option that provided overall more accurate\nanalytics than dropping missing values.",
          "link": "http://arxiv.org/abs/2103.01600",
          "publishedOn": "2021-07-19T00:49:07.816Z",
          "wordCount": 694,
          "title": "Missing Value Imputation on Multidimensional Time Series. (arXiv:2103.01600v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schoeffer_J/0/1/0/all/0/1\">Jakob Schoeffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_N/0/1/0/all/0/1\">Niklas Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valera_I/0/1/0/all/0/1\">Isabel Valera</a>",
          "description": "Algorithmic decision systems are increasingly used in areas such as hiring,\nschool admission, or loan approval. Typically, these systems rely on labeled\ndata for training a classification model. However, in many scenarios,\nground-truth labels are unavailable, and instead we have only access to\nimperfect labels as the result of (potentially biased) human-made decisions.\nDespite being imperfect, historical decisions often contain some useful\ninformation on the unobserved true labels. In this paper, we focus on scenarios\nwhere only imperfect labels are available and propose a new fair ranking-based\ndecision system based on monotonic relationships between legitimate features\nand the outcome. Our approach is both intuitive and easy to implement, and thus\nparticularly suitable for adoption in real-world settings. More in detail, we\nintroduce a distance-based decision criterion, which incorporates useful\ninformation from historical decisions and accounts for unwanted correlation\nbetween protected and legitimate features. Through extensive experiments on\nsynthetic and real-world data, we show that our method is fair in the sense\nthat a) it assigns the desirable outcome to the most qualified individuals, and\nb) it removes the effect of stereotypes in decision-making, thereby\noutperforming traditional classification algorithms. Additionally, we are able\nto show theoretically that our method is consistent with a prominent concept of\nindividual fairness which states that \"similar individuals should be treated\nsimilarly.\"",
          "link": "http://arxiv.org/abs/2102.04565",
          "publishedOn": "2021-07-19T00:49:07.808Z",
          "wordCount": 688,
          "title": "A Ranking Approach to Fair Classification. (arXiv:2102.04565v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jia-Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shuguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_T/0/1/0/all/0/1\">Tao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaoyi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_B/0/1/0/all/0/1\">Bin Tong</a>",
          "description": "Conversion rate (CVR) prediction is one of the most critical tasks for\ndigital display advertising. Commercial systems often require to update models\nin an online learning manner to catch up with the evolving data distribution.\nHowever, conversions usually do not happen immediately after a user click. This\nmay result in inaccurate labeling, which is called delayed feedback problem. In\nprevious studies, delayed feedback problem is handled either by waiting\npositive label for a long period of time, or by consuming the negative sample\non its arrival and then insert a positive duplicate when a conversion happens\nlater. Indeed, there is a trade-off between waiting for more accurate labels\nand utilizing fresh data, which is not considered in existing works. To strike\na balance in this trade-off, we propose Elapsed-Time Sampling Delayed Feedback\nModel (ES-DFM), which models the relationship between the observed conversion\ndistribution and the true conversion distribution. Then we optimize the\nexpectation of true conversion distribution via importance sampling under the\nelapsed-time sampling distribution. We further estimate the importance weight\nfor each instance, which is used as the weight of loss function in CVR\nprediction. To demonstrate the effectiveness of ES-DFM, we conduct extensive\nexperiments on a public data and a private industrial dataset. Experimental\nresults confirm that our method consistently outperforms the previous\nstate-of-the-art results.",
          "link": "http://arxiv.org/abs/2012.03245",
          "publishedOn": "2021-07-19T00:49:07.797Z",
          "wordCount": 713,
          "title": "Capturing Delayed Feedback in Conversion Rate Prediction via Elapsed-Time Sampling. (arXiv:2012.03245v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galindo_Torres_S/0/1/0/all/0/1\">S.A. Galindo-Torres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>",
          "description": "A better understanding of dispersion in natural streams requires knowledge of\nlongitudinal dispersion coefficient(LDC). Various methods have been proposed\nfor predictions of LDC. Those studies can be grouped into three types:\nanalytical, statistical and ML-driven researches(Implicit and explicit).\nHowever, a comprehensive evaluation of them is still lacking. In this paper, we\nfirst present an in-depth analysis of those methods and find out their defects.\nThis is carried out on an extensive database composed of 660 samples of\nhydraulic and channel properties worldwide. The reliability and\nrepresentativeness of utilized data are enhanced through the deployment of the\nSubset Selection of Maximum Dissimilarity(SSMD) for testing set selection and\nthe Inter Quartile Range(IQR) for removal of the outlier. The evaluation\nreveals the rank of those methods as: ML-driven method > the statistical method\n> the analytical method. Whereas implicit ML-driven methods are black-boxes in\nnature, explicit ML-driven methods have more potential in prediction of LDC.\nBesides, overfitting is a universal problem in existing models. Those models\nalso suffer from a fixed parameter combination. To establish an interpretable\nmodel for LDC prediction with higher performance, we then design a novel\nsymbolic regression method called evolutionary symbolic regression\nnetwork(ESRN). It is a combination of genetic algorithms and neural networks.\nStrategies are introduced to avoid overfitting and explore more parameter\ncombinations. Results show that the ESRN model has superiorities over other\nexisting symbolic models in performance. The proposed model is suitable for\npractical engineering problems due to its advantage in low requirement of\nparameters (only w and U* are required). It can provide convincing solutions\nfor situations where the field test cannot be carried out or limited field\ninformation can be obtained.",
          "link": "http://arxiv.org/abs/2106.11026",
          "publishedOn": "2021-07-19T00:49:07.788Z",
          "wordCount": 788,
          "title": "A data-based comparative review and AI-driven symbolic model for longitudinal dispersion coefficient in natural streams. (arXiv:2106.11026v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07863",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Sengupta_U/0/1/0/all/0/1\">Ushnish Sengupta</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kontogiannis_A/0/1/0/all/0/1\">Alexandros Kontogiannis</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Juniper_M/0/1/0/all/0/1\">Matthew P. Juniper</a>",
          "description": "Magnetic resonance velocimetry (MRV) is a non-invasive experimental technique\nwidely used in medicine and engineering to measure the velocity field of a\nfluid. These measurements are dense but have a low signal-to-noise ratio (SNR).\nThe measurements can be de-noised by imposing physical constraints on the flow,\nwhich are encapsulated in governing equations for mass and momentum. Previous\nstudies have required the shape of the boundary (for example, a blood vessel)\nto be known a priori. This, however, requires a set of additional measurements,\nwhich can be expensive to obtain. In this paper, we present a physics-informed\nneural network that instead uses the noisy MRV data alone to simultaneously\ninfer the most likely boundary shape and de-noised velocity field. We achieve\nthis by training an auxiliary neural network that takes the value 1.0 within\nthe inferred domain of the governing PDE and 0.0 outside. This network is used\nto weight the PDE residual term in the loss function accordingly and implicitly\nlearns the geometry of the system. We test our algorithm by assimilating both\nsynthetic and real MRV measurements for flows that can be well modeled by the\nPoisson and Stokes equations. We find that we are able to reconstruct very\nnoisy (SNR = 2.5) MRV signals and recover the ground truth with low\nreconstruction errors of 3.7 - 7.5%. The simplicity and flexibility of our\nphysics-informed neural network approach can readily scale to assimilating MRV\ndata with complex 3D geometries, time-varying 4D data, or unknown parameters in\nthe physical model.",
          "link": "http://arxiv.org/abs/2107.07863",
          "publishedOn": "2021-07-19T00:49:07.736Z",
          "wordCount": 698,
          "title": "Simultaneous boundary shape estimation and velocity field de-noising in Magnetic Resonance Velocimetry using Physics-informed Neural Networks. (arXiv:2107.07863v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chaochao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhuai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1\">Jo&#x15b;e Miguel Hern&#xe1;ndez-Lobato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "Due to spurious correlations, machine learning systems often fail to\ngeneralize to environments whose distributions differ from the ones used at\ntraining time. Prior work addressing this, either explicitly or implicitly,\nattempted to find a data representation that has an invariant relationship with\nthe target. This is done by leveraging a diverse set of training environments\nto reduce the effect of spurious features and build an invariant predictor.\nHowever, these methods have generalization guarantees only when both data\nrepresentation and classifiers come from a linear model class. We propose\ninvariant Causal Representation Learning (iCaRL), an approach that enables\nout-of-distribution (OOD) generalization in the nonlinear setting (i.e.,\nnonlinear representations and nonlinear classifiers). It builds upon a\npractical and general assumption: the prior over the data representation (i.e.,\na set of latent variables encoding the data) given the target and the\nenvironment belongs to general exponential family distributions. Based on this,\nwe show that it is possible to identify the data representation up to simple\ntransformations. We also prove that all direct causes of the target can be\nfully discovered, which further enables us to obtain generalization guarantees\nin the nonlinear setting. Extensive experiments on both synthetic and\nreal-world datasets show that our approach outperforms a variety of baseline\nmethods. Finally, in the discussion, we further explore the aforementioned\nassumption and propose a more general hypothesis, called the Agnostic\nHypothesis: there exist a set of hidden causal factors affecting both inputs\nand outcomes. The Agnostic Hypothesis can provide a unifying view of machine\nlearning. More importantly, it can inspire a new direction to explore a general\ntheory for identifying hidden causal factors, which is key to enabling the OOD\ngeneralization guarantees.",
          "link": "http://arxiv.org/abs/2102.12353",
          "publishedOn": "2021-07-19T00:49:07.724Z",
          "wordCount": 752,
          "title": "Nonlinear Invariant Risk Minimization: A Causal Approach. (arXiv:2102.12353v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minhui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shiqian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1\">Lifeng Lai</a>",
          "description": "Collecting and aggregating information from several probability measures or\nhistograms is a fundamental task in machine learning. One of the popular\nsolution methods for this task is to compute the barycenter of the probability\nmeasures under the Wasserstein metric. However, approximating the Wasserstein\nbarycenter is numerically challenging because of the curse of dimensionality.\nThis paper proposes the projection robust Wasserstein barycenter (PRWB) that\nhas the potential to mitigate the curse of dimensionality. Since PRWB is\nnumerically very challenging to solve, we further propose a relaxed PRWB\n(RPRWB) model, which is more tractable. The RPRWB projects the probability\nmeasures onto a lower-dimensional subspace that maximizes the Wasserstein\nbarycenter objective. The resulting problem is a max-min problem over the\nStiefel manifold. By combining the iterative Bregman projection algorithm and\nRiemannian optimization, we propose two new algorithms for computing the RPRWB.\nThe complexity of arithmetic operations of the proposed algorithms for\nobtaining an $\\epsilon$-stationary solution is analyzed. We incorporate the\nRPRWB into a discrete distribution clustering algorithm, and the numerical\nresults on real text datasets confirm that our RPRWB model helps improve the\nclustering performance significantly.",
          "link": "http://arxiv.org/abs/2102.03390",
          "publishedOn": "2021-07-19T00:49:07.718Z",
          "wordCount": 644,
          "title": "Projection Robust Wasserstein Barycenters. (arXiv:2102.03390v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minhui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shiqian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1\">Lifeng Lai</a>",
          "description": "The Wasserstein distance has become increasingly important in machine\nlearning and deep learning. Despite its popularity, the Wasserstein distance is\nhard to approximate because of the curse of dimensionality. A recently proposed\napproach to alleviate the curse of dimensionality is to project the sampled\ndata from the high dimensional probability distribution onto a\nlower-dimensional subspace, and then compute the Wasserstein distance between\nthe projected data. However, this approach requires to solve a max-min problem\nover the Stiefel manifold, which is very challenging in practice. The only\nexisting work that solves this problem directly is the RGAS (Riemannian\nGradient Ascent with Sinkhorn Iteration) algorithm, which requires to solve an\nentropy-regularized optimal transport problem in each iteration, and thus can\nbe costly for large-scale problems. In this paper, we propose a Riemannian\nblock coordinate descent (RBCD) method to solve this problem, which is based on\na novel reformulation of the regularized max-min problem over the Stiefel\nmanifold. We show that the complexity of arithmetic operations for RBCD to\nobtain an $\\epsilon$-stationary point is $O(\\epsilon^{-3})$. This significantly\nimproves the corresponding complexity of RGAS, which is $O(\\epsilon^{-12})$.\nMoreover, our RBCD has very low per-iteration complexity, and hence is suitable\nfor large-scale problems. Numerical results on both synthetic and real datasets\ndemonstrate that our method is more efficient than existing methods, especially\nwhen the number of sampled data is very large.",
          "link": "http://arxiv.org/abs/2012.05199",
          "publishedOn": "2021-07-19T00:49:07.702Z",
          "wordCount": 720,
          "title": "A Riemannian Block Coordinate Descent Method for Computing the Projection Robust Wasserstein Distance. (arXiv:2012.05199v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Traganitis_P/0/1/0/all/0/1\">Panagiotis A. Traganitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannakis_G/0/1/0/all/0/1\">Georgios B. Giannakis</a>",
          "description": "Crowdsourcing has emerged as a powerful paradigm for efficiently labeling\nlarge datasets and performing various learning tasks, by leveraging crowds of\nhuman annotators. When additional information is available about the data,\nsemi-supervised crowdsourcing approaches that enhance the aggregation of labels\nfrom human annotators are well motivated. This work deals with semi-supervised\ncrowdsourced classification, under two regimes of semi-supervision: a) label\nconstraints, that provide ground-truth labels for a subset of data; and b)\npotentially easier to obtain instance-level constraints, that indicate\nrelationships between pairs of data. Bayesian algorithms based on variational\ninference are developed for each regime, and their quantifiably improved\nperformance, compared to unsupervised crowdsourcing, is analytically and\nempirically validated on several crowdsourcing datasets.",
          "link": "http://arxiv.org/abs/2012.11048",
          "publishedOn": "2021-07-19T00:49:07.695Z",
          "wordCount": 574,
          "title": "Bayesian Crowdsourcing with Constraints. (arXiv:2012.11048v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1\">Pang Wei Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagawa_S/0/1/0/all/0/1\">Shiori Sagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marklund_H/0/1/0/all/0/1\">Henrik Marklund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Marvin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balsubramani_A/0/1/0/all/0/1\">Akshay Balsubramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weihua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_R/0/1/0/all/0/1\">Richard Lanas Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_I/0/1/0/all/0/1\">Irena Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tony Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_E/0/1/0/all/0/1\">Etienne David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stavness_I/0/1/0/all/0/1\">Ian Stavness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Earnshaw_B/0/1/0/all/0/1\">Berton A. Earnshaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haque_I/0/1/0/all/0/1\">Imran S. Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1\">Sara Beery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundaje_A/0/1/0/all/0/1\">Anshul Kundaje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierson_E/0/1/0/all/0/1\">Emma Pierson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>",
          "description": "Distribution shifts -- where the training distribution differs from the test\ndistribution -- can substantially degrade the accuracy of machine learning (ML)\nsystems deployed in the wild. Despite their ubiquity in the real-world\ndeployments, these distribution shifts are under-represented in the datasets\nwidely used in the ML community today. To address this gap, we present WILDS, a\ncurated benchmark of 10 datasets reflecting a diverse range of distribution\nshifts that naturally arise in real-world applications, such as shifts across\nhospitals for tumor identification; across camera traps for wildlife\nmonitoring; and across time and location in satellite imaging and poverty\nmapping. On each dataset, we show that standard training yields substantially\nlower out-of-distribution than in-distribution performance. This gap remains\neven with models trained by existing methods for tackling distribution shifts,\nunderscoring the need for new methods for training models that are more robust\nto the types of distribution shifts that arise in practice. To facilitate\nmethod development, we provide an open-source package that automates dataset\nloading, contains default model architectures and hyperparameters, and\nstandardizes evaluations. Code and leaderboards are available at\nhttps://wilds.stanford.edu.",
          "link": "http://arxiv.org/abs/2012.07421",
          "publishedOn": "2021-07-19T00:49:07.662Z",
          "wordCount": 696,
          "title": "WILDS: A Benchmark of in-the-Wild Distribution Shifts. (arXiv:2012.07421v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1\">Jacob Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>",
          "description": "Recent results in end-to-end automatic speech recognition have demonstrated\nthe efficacy of pseudo-labeling for semi-supervised models trained both with\nConnectionist Temporal Classification (CTC) and Sequence-to-Sequence (seq2seq)\nlosses. Iterative Pseudo-Labeling (IPL), which continuously trains a single\nmodel using pseudo-labels iteratively re-generated as the model learns, has\nbeen shown to further improve performance in ASR. We improve upon the IPL\nalgorithm: as the model learns, we propose to iteratively re-generate\ntranscriptions with hard labels (the most probable tokens), that is, without a\nlanguage model. We call this approach Language-Model-Free IPL (slimIPL) and\ngive a resultant training setup for low-resource settings with CTC-based\nmodels. slimIPL features a dynamic cache for pseudo-labels which reduces\nsensitivity to changes in relabeling hyperparameters and results in improves\ntraining stability. slimIPL is also highly-efficient and requires 3.5-4x fewer\ncomputational resources to converge than other state-of-the-art\nsemi/self-supervised approaches. With only 10 hours of labeled audio, slimIPL\nis competitive with self-supervised approaches, and is state-of-the-art with\n100 hours of labeled audio without the use of a language model both at test\ntime and during pseudo-label generation.",
          "link": "http://arxiv.org/abs/2010.11524",
          "publishedOn": "2021-07-19T00:49:07.642Z",
          "wordCount": 654,
          "title": "SlimIPL: Language-Model-Free Iterative Pseudo-Labeling. (arXiv:2010.11524v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1\">Arpita Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samanta_D/0/1/0/all/0/1\">Debasis Samanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarma_M/0/1/0/all/0/1\">Monalisa Sarma</a>",
          "description": "User intention which often changes dynamically is considered to be an\nimportant factor for modeling users in the design of recommendation systems.\nRecent studies are starting to focus on predicting user intention (what users\nwant) beyond user preference (what users like). In this work, a user intention\nmodel is proposed based on deep sequential topic analysis. The model predicts a\nuser's intention in terms of the topic of interest. The Hybrid Topic Model\n(HTM) comprising Latent Dirichlet Allocation (LDA) and Word2Vec is proposed to\nderive the topic of interest of users and the history of preferences. HTM finds\nthe true topics of papers estimating word-topic distribution which includes\nsyntactic and semantic correlations among words. Next, to model user intention,\na Long Short Term Memory (LSTM) based sequential deep learning model is\nproposed. This model takes into account temporal context, namely the time\ndifference between clicks of two consecutive papers seen by a user. Extensive\nexperiments with the real-world research paper dataset indicate that the\nproposed approach significantly outperforms the state-of-the-art methods.\nFurther, the proposed approach introduces a new road map to model a user\nactivity suitable for the design of a research paper recommendation system.",
          "link": "http://arxiv.org/abs/2107.07831",
          "publishedOn": "2021-07-19T00:49:07.625Z",
          "wordCount": 630,
          "title": "Modeling User Behaviour in Research Paper Recommendation System. (arXiv:2107.07831v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.08853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yehudai_G/0/1/0/all/0/1\">Gilad Yehudai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1\">Ethan Fetaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meirom_E/0/1/0/all/0/1\">Eli Meirom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1\">Haggai Maron</a>",
          "description": "Graph neural networks (GNNs) can process graphs of different sizes, but their\nability to generalize across sizes, specifically from small to large graphs, is\nstill not well understood. In this paper, we identify an important type of data\nwhere generalization from small to large graphs is challenging: graph\ndistributions for which the local structure depends on the graph size. This\neffect occurs in multiple important graph learning domains, including social\nand biological networks. We first prove that when there is a difference between\nthe local structures, GNNs are not guaranteed to generalize across sizes: there\nare \"bad\" global minima that do well on small graphs but fail on large graphs.\nWe then study the size-generalization problem empirically and demonstrate that\nwhen there is a discrepancy in local structure, GNNs tend to converge to\nnon-generalizing solutions. Finally, we suggest two approaches for improving\nsize generalization, motivated by our findings. Notably, we propose a novel\nSelf-Supervised Learning (SSL) task aimed at learning meaningful\nrepresentations of local structures that appear in large graphs. Our SSL task\nimproves classification accuracy on several popular datasets.",
          "link": "http://arxiv.org/abs/2010.08853",
          "publishedOn": "2021-07-19T00:49:07.610Z",
          "wordCount": 674,
          "title": "From Local Structures to Size Generalization in Graph Neural Networks. (arXiv:2010.08853v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.13688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Shaan Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattheakis_M/0/1/0/all/0/1\">Marios Mattheakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>",
          "description": "Recent advances show that neural networks embedded with physics-informed\npriors significantly outperform vanilla neural networks in learning and\npredicting the long term dynamics of complex physical systems from noisy data.\nDespite this success, there has only been a limited study on how to optimally\ncombine physics priors to improve predictive performance. To tackle this\nproblem we unpack and generalize recent innovations into individual inductive\nbias segments. As such, we are able to systematically investigate all possible\ncombinations of inductive biases of which existing methods are a natural\nsubset. Using this framework we introduce Variational Integrator Graph Networks\n- a novel method that unifies the strengths of existing approaches by combining\nan energy constraint, high-order symplectic variational integrators, and graph\nneural networks. We demonstrate, across an extensive ablation, that the\nproposed unifying framework outperforms existing methods, for data-efficient\nlearning and in predictive accuracy, across both single and many-body problems\nstudied in recent literature. We empirically show that the improvements arise\nbecause high order variational integrators combined with a potential energy\nconstraint induce coupled learning of generalized position and momentum updates\nwhich can be formalized via the Partitioned Runge-Kutta method.",
          "link": "http://arxiv.org/abs/2004.13688",
          "publishedOn": "2021-07-19T00:49:07.603Z",
          "wordCount": 669,
          "title": "Variational Integrator Graph Networks for Learning Energy Conserving Dynamical Systems. (arXiv:2004.13688v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07757",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Musso_D/0/1/0/all/0/1\">Daniele Musso</a>",
          "description": "Local entropic loss functions provide a versatile framework to define\narchitecture-aware regularization procedures. Besides the possibility of being\nanisotropic in the synaptic space, the local entropic smoothening of the loss\nfunction can vary during training, thus yielding a tunable model complexity. A\nscoping protocol where the regularization is strong in the early-stage of the\ntraining and then fades progressively away constitutes an alternative to\nstandard initialization procedures for deep convolutional neural networks,\nnonetheless, it has wider applicability. We analyze anisotropic, local entropic\nsmoothenings in the language of statistical physics and information theory,\nproviding insight into both their interpretation and workings. We comment some\naspects related to the physics of renormalization and the spacetime structure\nof convolutional networks.",
          "link": "http://arxiv.org/abs/2107.07757",
          "publishedOn": "2021-07-19T00:49:07.595Z",
          "wordCount": 563,
          "title": "Entropic alternatives to initialization. (arXiv:2107.07757v1 [cond-mat.dis-nn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ishii_Y/0/1/0/all/0/1\">Yasunori Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_T/0/1/0/all/0/1\">Takayoshi Yamashita</a>",
          "description": "It is difficult to collect data on a large scale in a monocular depth\nestimation because the task requires the simultaneous acquisition of RGB images\nand depths. Data augmentation is thus important to this task. However, there\nhas been little research on data augmentation for tasks such as monocular depth\nestimation, where the transformation is performed pixel by pixel. In this\npaper, we propose a data augmentation method, called CutDepth. In CutDepth,\npart of the depth is pasted onto an input image during training. The method\nextends variations data without destroying edge features. Experiments\nobjectively and subjectively show that the proposed method outperforms\nconventional methods of data augmentation. The estimation accuracy is improved\nwith CutDepth even though there are few training data at long distances.",
          "link": "http://arxiv.org/abs/2107.07684",
          "publishedOn": "2021-07-19T00:49:07.581Z",
          "wordCount": 561,
          "title": "CutDepth:Edge-aware Data Augmentation in Depth Estimation. (arXiv:2107.07684v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chung_L/0/1/0/all/0/1\">Long Kiu Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Adam Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knowles_D/0/1/0/all/0/1\">Derek Knowles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kousik_S/0/1/0/all/0/1\">Shreyas Kousik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Grace X. Gao</a>",
          "description": "Neural networks have recently become popular for a wide variety of uses, but\nhave seen limited application in safety-critical domains such as robotics near\nand around humans. This is because it remains an open challenge to train a\nneural network to obey safety constraints. Most existing safety-related methods\nonly seek to verify that already-trained networks obey constraints, requiring\nalternating training and verification. Instead, this work proposes a\nconstrained method to simultaneously train and verify a feedforward neural\nnetwork with rectified linear unit (ReLU) nonlinearities. Constraints are\nenforced by computing the network's output-space reachable set and ensuring\nthat it does not intersect with unsafe sets; training is achieved by\nformulating a novel collision-check loss function between the reachable set and\nunsafe portions of the output space. The reachable and unsafe sets are\nrepresented by constrained zonotopes, a convex polytope representation that\nenables differentiable collision checking. The proposed method is demonstrated\nsuccessfully on a network with one nonlinearity layer and approximately 50\nparameters.",
          "link": "http://arxiv.org/abs/2107.07696",
          "publishedOn": "2021-07-19T00:49:07.574Z",
          "wordCount": 608,
          "title": "Constrained Feedforward Neural Network Training via Reachability Analysis. (arXiv:2107.07696v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00628",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Frasch_M/0/1/0/all/0/1\">Martin G. Frasch</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Strong_S/0/1/0/all/0/1\">Shadrian B. Strong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nilosek_D/0/1/0/all/0/1\">David Nilosek</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Leaverton_J/0/1/0/all/0/1\">Joshua Leaverton</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Schifrin_B/0/1/0/all/0/1\">Barry S. Schifrin</a>",
          "description": "Despite broad application during labor and delivery, there remains\nconsiderable debate about the value of electronic fetal monitoring (EFM). EFM\nincludes the surveillance of the fetal heart rate (FHR) patterns in conjunction\nwith the maternal uterine contractions providing a wealth of data about fetal\nbehavior and the threat of diminished oxygenation and perfusion. Adverse\noutcomes universally associate a fetal injury with the failure to timely\nrespond to FHR pattern information. Historically, the EFM data, stored\ndigitally, are available only as rasterized pdf images for contemporary or\nhistorical discussion and examination. In reality, however, they are rarely\nreviewed systematically. Using a unique archive of EFM collected over 50 years\nof practice in conjunction with adverse outcomes, we present a deep learning\nframework for training and detection of incipient or past fetal injury. We\nreport 94% accuracy in identifying early, preventable fetal injury intrapartum.\nThis framework is suited for automating an early warning and decision support\nsystem for maintaining fetal well-being during the stresses of labor.\nUltimately, such a system could enable a physician to timely respond during\nlabor and prevent adverse outcomes. When adverse outcomes cannot be avoided,\nthey can provide guidance to the early neuroprotective treatment of the\nnewborn.",
          "link": "http://arxiv.org/abs/2106.00628",
          "publishedOn": "2021-07-19T00:49:07.517Z",
          "wordCount": 666,
          "title": "Detection of preventable fetal distress during labor from scanned cardiotocogram tracings using deep learning. (arXiv:2106.00628v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silver_T/0/1/0/all/0/1\">Tom Silver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitnis_R/0/1/0/all/0/1\">Rohan Chitnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1\">Leslie Pack Kaelbling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1\">Tomas Lozano-Perez</a>",
          "description": "Robotic planning problems in hybrid state and action spaces can be solved by\nintegrated task and motion planners (TAMP) that handle the complex interaction\nbetween motion-level decisions and task-level plan feasibility. TAMP approaches\nrely on domain-specific symbolic operators to guide the task-level search,\nmaking planning efficient. In this work, we formalize and study the problem of\noperator learning for TAMP. Central to this study is the view that operators\ndefine a lossy abstraction of the transition model of a domain. We then propose\na bottom-up relational learning method for operator learning and show how the\nlearned operators can be used for planning in a TAMP system. Experimentally, we\nprovide results in three domains, including long-horizon robotic planning\ntasks. We find our approach to substantially outperform several baselines,\nincluding three graph neural network-based model-free approaches from the\nrecent literature. Video: https://youtu.be/iVfpX9BpBRo Code:\nhttps://git.io/JCT0g",
          "link": "http://arxiv.org/abs/2103.00589",
          "publishedOn": "2021-07-19T00:49:07.500Z",
          "wordCount": 619,
          "title": "Learning Symbolic Operators for Task and Motion Planning. (arXiv:2103.00589v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1\">Frederik Tr&#xe4;uble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creager_E/0/1/0/all/0/1\">Elliot Creager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilbertus_N/0/1/0/all/0/1\">Niki Kilbertus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1\">Andrea Dittadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1\">Anirudh Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>",
          "description": "The focus of disentanglement approaches has been on identifying independent\nfactors of variation in data. However, the causal variables underlying\nreal-world observations are often not statistically independent. In this work,\nwe bridge the gap to real-world scenarios by analyzing the behavior of the most\nprominent disentanglement approaches on correlated data in a large-scale\nempirical study (including 4260 models). We show and quantify that\nsystematically induced correlations in the dataset are being learned and\nreflected in the latent representations, which has implications for downstream\napplications of disentanglement such as fairness. We also demonstrate how to\nresolve these latent correlations, either using weak supervision during\ntraining or by post-hoc correcting a pre-trained model with a small number of\nlabels.",
          "link": "http://arxiv.org/abs/2006.07886",
          "publishedOn": "2021-07-19T00:49:07.484Z",
          "wordCount": 610,
          "title": "On Disentangled Representations Learned From Correlated Data. (arXiv:2006.07886v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07634",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_T/0/1/0/all/0/1\">Takuya Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1\">Anmol Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dhir_C/0/1/0/all/0/1\">Chandra Dhir</a>",
          "description": "Keyword spotting (KWS) is an important technique for speech applications,\nwhich enables users to activate devices by speaking a keyword phrase. Although\na phoneme classifier can be used for KWS, exploiting a large amount of\ntranscribed data for automatic speech recognition (ASR), there is a mismatch\nbetween the training criterion (phoneme recognition) and the target task (KWS).\nRecently, multi-task learning has been applied to KWS to exploit both ASR and\nKWS training data. In this approach, an output of an acoustic model is split\ninto two branches for the two tasks, one for phoneme transcription trained with\nthe ASR data and one for keyword classification trained with the KWS data. In\nthis paper, we introduce a cross attention decoder in the multi-task learning\nframework. Unlike the conventional multi-task learning approach with the simple\nsplit of the output layer, the cross attention decoder summarizes information\nfrom a phonetic encoder by performing cross attention between the encoder\noutputs and a trainable query sequence to predict a confidence score for the\nKWS task. Experimental results on KWS tasks show that the proposed approach\noutperformed the conventional multi-task learning with split branches and a\nbi-directional long short-team memory decoder by 12% on average.",
          "link": "http://arxiv.org/abs/2107.07634",
          "publishedOn": "2021-07-19T00:49:07.478Z",
          "wordCount": 652,
          "title": "Multi-task Learning with Cross Attention for Keyword Spotting. (arXiv:2107.07634v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05073",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Mikuni_V/0/1/0/all/0/1\">Vinicius Mikuni</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Canelli_F/0/1/0/all/0/1\">Florencia Canelli</a>",
          "description": "Methods for processing point cloud information have seen a great success in\ncollider physics applications. One recent breakthrough in machine learning is\nthe usage of Transformer networks to learn semantic relationships between\nsequences in language processing. In this work, we apply a modified Transformer\nnetwork called Point Cloud Transformer as a method to incorporate the\nadvantages of the Transformer architecture to an unordered set of particles\nresulting from collision events. To compare the performance with other\nstrategies, we study jet-tagging applications for highly-boosted particles.",
          "link": "http://arxiv.org/abs/2102.05073",
          "publishedOn": "2021-07-19T00:49:07.471Z",
          "wordCount": 559,
          "title": "Point Cloud Transformers applied to Collider Physics. (arXiv:2102.05073v2 [physics.data-an] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungyeop Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Junghyo Jo</a>",
          "description": "The outstanding performance of deep learning in various fields has been a\nfundamental query, which can be potentially examined using information theory\nthat interprets the learning process as the transmission and compression of\ninformation. Information plane analyses of the mutual information between the\ninput-hidden-output layers demonstrated two distinct learning phases of fitting\nand compression. It is debatable if the compression phase is necessary to\ngeneralize the input-output relations extracted from training data. In this\nstudy, we investigated this through experiments with various species of\nautoencoders and evaluated their information processing phase with an accurate\nkernel-based estimator of mutual information. Given sufficient training data,\nvanilla autoencoders demonstrated the compression phase, which was amplified\nafter imposing sparsity regularization for hidden activities. However, we found\nthat the compression phase is not universally observed in different species of\nautoencoders, including variational autoencoders, that have special constraints\non network weights or manifold of hidden space. These types of autoencoders\nexhibited perfect generalization ability for test data without requiring the\ncompression phase. Thus, we conclude that the compression phase is not\nnecessary for generalization in representation learning.",
          "link": "http://arxiv.org/abs/2102.07402",
          "publishedOn": "2021-07-19T00:49:07.466Z",
          "wordCount": 633,
          "title": "Information flows of diverse autoencoders. (arXiv:2102.07402v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaojian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wuyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuchen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yonggan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingyan Lin</a>",
          "description": "Semantic segmentation for scene understanding is nowadays widely demanded,\nraising significant challenges for the algorithm efficiency, especially its\napplications on resource-limited platforms. Current segmentation models are\ntrained and evaluated on massive high-resolution scene images (\"data level\")\nand suffer from the expensive computation arising from the required multi-scale\naggregation(\"network level\"). In both folds, the computational and energy costs\nin training and inference are notable due to the often desired large input\nresolutions and heavy computational burden of segmentation models. To this end,\nwe propose DANCE, general automated DAta-Network Co-optimization for Efficient\nsegmentation model training and inference. Distinct from existing efficient\nsegmentation approaches that focus merely on light-weight network design, DANCE\ndistinguishes itself as an automated simultaneous data-network co-optimization\nvia both input data manipulation and network architecture slimming.\nSpecifically, DANCE integrates automated data slimming which adaptively\ndownsamples/drops input images and controls their corresponding contribution to\nthe training loss guided by the images' spatial complexity. Such a downsampling\noperation, in addition to slimming down the cost associated with the input size\ndirectly, also shrinks the dynamic range of input object and context scales,\ntherefore motivating us to also adaptively slim the network to match the\ndownsampled data. Extensive experiments and ablating studies (on four SOTA\nsegmentation models with three popular segmentation datasets under two training\nsettings) demonstrate that DANCE can achieve \"all-win\" towards efficient\nsegmentation(reduced training cost, less expensive inference, and better mean\nIntersection-over-Union (mIoU)).",
          "link": "http://arxiv.org/abs/2107.07706",
          "publishedOn": "2021-07-19T00:49:07.450Z",
          "wordCount": 687,
          "title": "DANCE: DAta-Network Co-optimization for Efficient Segmentation Model Training and Inference. (arXiv:2107.07706v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berg_J/0/1/0/all/0/1\">Jan Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drossos_K/0/1/0/all/0/1\">Konstantinos Drossos</a>",
          "description": "Automated audio captioning (AAC) is the task of automatically creating\ntextual descriptions (i.e. captions) for the contents of a general audio\nsignal. Most AAC methods are using existing datasets to optimize and/or\nevaluate upon. Given the limited information held by the AAC datasets, it is\nvery likely that AAC methods learn only the information contained in the\nutilized datasets. In this paper we present a first approach for continuously\nadapting an AAC method to new information, using a continual learning method.\nIn our scenario, a pre-optimized AAC method is used for some unseen general\naudio signals and can update its parameters in order to adapt to the new\ninformation, given a new reference caption. We evaluate our method using a\nfreely available, pre-optimized AAC method and two freely available AAC\ndatasets. We compare our proposed method with three scenarios, two of training\non one of the datasets and evaluating on the other and a third of training on\none dataset and fine-tuning on the other. Obtained results show that our method\nachieves a good balance between distilling new knowledge and not forgetting the\nprevious one.",
          "link": "http://arxiv.org/abs/2107.08028",
          "publishedOn": "2021-07-19T00:49:07.440Z",
          "wordCount": 628,
          "title": "Continual Learning for Automated Audio Captioning Using The Learning Without Forgetting Approach. (arXiv:2107.08028v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_M/0/1/0/all/0/1\">Meng Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junkun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xhonneux_L/0/1/0/all/0/1\">Louis-Pascal Xhonneux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>",
          "description": "This paper studies learning logic rules for reasoning on knowledge graphs.\nLogic rules provide interpretable explanations when used for prediction as well\nas being able to generalize to other tasks, and hence are critical to learn.\nExisting methods either suffer from the problem of searching in a large search\nspace (e.g., neural logic programming) or ineffective optimization due to\nsparse rewards (e.g., techniques based on reinforcement learning). To address\nthese limitations, this paper proposes a probabilistic model called RNNLogic.\nRNNLogic treats logic rules as a latent variable, and simultaneously trains a\nrule generator as well as a reasoning predictor with logic rules. We develop an\nEM-based algorithm for optimization. In each iteration, the reasoning predictor\nis first updated to explore some generated logic rules for reasoning. Then in\nthe E-step, we select a set of high-quality rules from all generated rules with\nboth the rule generator and reasoning predictor via posterior inference; and in\nthe M-step, the rule generator is updated with the rules selected in the\nE-step. Experiments on four datasets prove the effectiveness of RNNLogic.",
          "link": "http://arxiv.org/abs/2010.04029",
          "publishedOn": "2021-07-19T00:49:07.391Z",
          "wordCount": 647,
          "title": "RNNLogic: Learning Logic Rules for Reasoning on Knowledge Graphs. (arXiv:2010.04029v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1809.09910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fanghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Lei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suykens_J/0/1/0/all/0/1\">Johan A.K. Suykens</a>",
          "description": "This paper generalizes regularized regression problems in a hyper-reproducing\nkernel Hilbert space (hyper-RKHS), illustrates its utility for kernel learning\nand out-of-sample extensions, and proves asymptotic convergence results for the\nintroduced regression models in an approximation theory view. Algorithmically,\nwe consider two regularized regression models with bivariate forms in this\nspace, including kernel ridge regression (KRR) and support vector regression\n(SVR) endowed with hyper-RKHS, and further combine divide-and-conquer with\nNystr\\\"{o}m approximation for scalability in large sample cases. This framework\nis general: the underlying kernel is learned from a broad class, and can be\npositive definite or not, which adapts to various requirements in kernel\nlearning. Theoretically, we study the convergence behavior of regularized\nregression algorithms in hyper-RKHS and derive the learning rates, which goes\nbeyond the classical analysis on RKHS due to the non-trivial independence of\npairwise samples and the characterisation of hyper-RKHS. Experimentally,\nresults on several benchmarks suggest that the employed framework is able to\nlearn a general kernel function form an arbitrary similarity matrix, and thus\nachieves a satisfactory performance on classification tasks.",
          "link": "http://arxiv.org/abs/1809.09910",
          "publishedOn": "2021-07-19T00:49:07.386Z",
          "wordCount": 656,
          "title": "Generalization Properties of hyper-RKHS and its Applications. (arXiv:1809.09910v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Achaji_L/0/1/0/all/0/1\">Lina Achaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_J/0/1/0/all/0/1\">Julien Moreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fouqueray_T/0/1/0/all/0/1\">Thibault Fouqueray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aioun_F/0/1/0/all/0/1\">Francois Aioun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charpillet_F/0/1/0/all/0/1\">Francois Charpillet</a>",
          "description": "The human driver is no longer the only one concerned with the complexity of\nthe driving scenarios. Autonomous vehicles (AV) are similarly becoming involved\nin the process. Nowadays, the development of AV in urban places underpins\nessential safety concerns for vulnerable road users (VRUs) such as pedestrians.\nTherefore, to make the roads safer, it is critical to classify and predict\ntheir future behavior. In this paper, we present a framework based on multiple\nvariations of the Transformer models to reason attentively about the dynamic\nevolution of the pedestrians' past trajectory and predict its future actions of\ncrossing or not crossing the street. We proved that using only bounding boxes\nas input to our model can outperform the previous state-of-the-art models and\nreach a prediction accuracy of 91 % and an F1-score of 0.83 on the PIE dataset\nup to two seconds ahead in the future. In addition, we introduced a large-size\nsimulated dataset (CP2A) using CARLA for action prediction. Our model has\nsimilarly reached high accuracy (91 %) and F1-score (0.91) on this dataset.\nInterestingly, we showed that pre-training our Transformer model on the\nsimulated dataset and then fine-tuning it on the real dataset can be very\neffective for the action prediction task.",
          "link": "http://arxiv.org/abs/2107.08031",
          "publishedOn": "2021-07-19T00:49:07.373Z",
          "wordCount": 656,
          "title": "Is attention to bounding boxes all you need for pedestrian action prediction?. (arXiv:2107.08031v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1\">Krzysztof Choromanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Han Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoxian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1\">Jack Parker-Holder</a>",
          "description": "We introduce a new class of graph neural networks (GNNs), by combining\nseveral concepts that were so far studied independently - graph kernels,\nattention-based networks with structural priors and more recently, efficient\nTransformers architectures applying small memory footprint implicit attention\nmethods via low rank decomposition techniques. The goal of the paper is\ntwofold. Proposed by us Graph Kernel Attention Transformers (or GKATs) are much\nmore expressive than SOTA GNNs as capable of modeling longer-range dependencies\nwithin a single layer. Consequently, they can use more shallow architecture\ndesign. Furthermore, GKAT attention layers scale linearly rather than\nquadratically in the number of nodes of the input graphs, even when those\ngraphs are dense, requiring less compute than their regular graph attention\ncounterparts. They achieve it by applying new classes of graph kernels\nadmitting random feature map decomposition via random walks on graphs. As a\nbyproduct of the introduced techniques, we obtain a new class of learnable\ngraph sketches, called graphots, compactly encoding topological graph\nproperties as well as nodes' features. We conducted exhaustive empirical\ncomparison of our method with nine different GNN classes on tasks ranging from\nmotif detection through social network classification to bioinformatics\nchallenges, showing consistent gains coming from GKATs.",
          "link": "http://arxiv.org/abs/2107.07999",
          "publishedOn": "2021-07-19T00:49:07.330Z",
          "wordCount": 630,
          "title": "Graph Kernel Attention Transformers. (arXiv:2107.07999v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1\">Tanveer Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalas_A/0/1/0/all/0/1\">Antonis Michalas</a>",
          "description": "Social networking and micro-blogging services, such as Twitter, play an\nimportant role in sharing digital information. Despite the popularity and\nusefulness of social media, there have been many instances where corrupted\nusers found ways to abuse it, as for instance, through raising or lowering\nuser's credibility. As a result, while social media facilitates an\nunprecedented ease of access to information, it also introduces a new challenge\n- that of ascertaining the credibility of shared information. Currently, there\nis no automated way of determining which news or users are credible and which\nare not. Hence, establishing a system that can measure the social media user's\ncredibility has become an issue of great importance. Assigning a credibility\nscore to a user has piqued the interest of not only the research community but\nalso most of the big players on both sides - such as Facebook, on the side of\nindustry, and political parties on the societal one. In this work, we created a\nmodel which, we hope, will ultimately facilitate and support the increase of\ntrust in the social network communities. Our model collected data and analysed\nthe behaviour of~50,000 politicians on Twitter. Influence score, based on\nseveral chosen features, was assigned to each evaluated user. Further, we\nclassified the political Twitter users as either trusted or untrusted using\nrandom forest, multilayer perceptron, and support vector machine. An active\nlearning model was used to classify any unlabelled ambiguous records from our\ndataset. Finally, to measure the performance of the proposed model, we used\nprecision, recall, F1 score, and accuracy as the main evaluation metrics.",
          "link": "http://arxiv.org/abs/2107.08027",
          "publishedOn": "2021-07-19T00:49:07.323Z",
          "wordCount": 704,
          "title": "SOK: Seeing and Believing: Evaluating the Trustworthiness of Twitter Users. (arXiv:2107.08027v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi-Gang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whatmough_P/0/1/0/all/0/1\">Paul N. Whatmough</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattina_M/0/1/0/all/0/1\">Matthew Mattina</a>",
          "description": "Exploiting sparsity is a key technique in accelerating quantized\nconvolutional neural network (CNN) inference on mobile devices. Prior sparse\nCNN accelerators largely exploit un-structured sparsity and achieve significant\nspeedups. Due to the unbounded, largely unpredictable sparsity patterns,\nhowever, exploiting unstructured sparsity requires complicated hardware design\nwith significant energy and area overhead, which is particularly detrimental to\nmobile/IoT inference scenarios where energy and area efficiency are crucial. We\npropose to exploit structured sparsity, more specifically, Density Bound Block\n(DBB) sparsity for both weights and activations. DBB block tensors bound the\nmaximum number of non-zeros per block. DBB thus exposes statically predictable\nsparsity patterns that enable lean sparsity-exploiting hardware. We propose new\nhardware primitives to implement DBB sparsity for (static) weights and\n(dynamic) activations, respectively, with very low overheads. Building on top\nof the primitives, we describe S2TA, a systolic array-based CNN accelerator\nthat exploits joint weight and activation DBB sparsity and new dimensions of\ndata reuse unavailable on the traditional systolic array. S2TA in 16nm achieves\nmore than 2x speedup and energy reduction compared to a strong baseline of a\nsystolic array with zero-value clock gating, over five popular CNN benchmarks.\nCompared to two recent non-systolic sparse accelerators, Eyeriss v2 (65nm) and\nSparTen (45nm), S2TA in 65nm uses about 2.2x and 3.1x less energy per\ninference, respectively.",
          "link": "http://arxiv.org/abs/2107.07983",
          "publishedOn": "2021-07-19T00:49:07.318Z",
          "wordCount": 652,
          "title": "S2TA: Exploiting Structured Sparsity for Energy-Efficient Mobile CNN Acceleration. (arXiv:2107.07983v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2004.02653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sigrist_F/0/1/0/all/0/1\">Fabio Sigrist</a>",
          "description": "We introduce a novel way to combine boosting with Gaussian process and mixed\neffects models. This allows for relaxing, first, the linearity assumption for\nthe mean function in Gaussian process and grouped random effects models in a\nflexible non-parametric way and, second, the independence assumption made in\nmost boosting algorithms. The former is advantageous for predictive accuracy\nand for avoiding model misspecifications. The latter is important for more\nefficient learning of the mean function and for obtaining probabilistic\npredictions. In addition, we present an extension that scales to large data\nusing a Vecchia approximation for the Gaussian process model relying on novel\nresults for covariance parameter inference. We obtain increased predictive\naccuracy compared to existing approaches on several simulated and real-world\ndata sets.",
          "link": "http://arxiv.org/abs/2004.02653",
          "publishedOn": "2021-07-19T00:49:07.312Z",
          "wordCount": 583,
          "title": "Gaussian Process Boosting. (arXiv:2004.02653v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08013",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Miles_C/0/1/0/all/0/1\">Cole Miles</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Carbone_M/0/1/0/all/0/1\">Matthew R. Carbone</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Sturm_E/0/1/0/all/0/1\">Erica J. Sturm</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Lu_D/0/1/0/all/0/1\">Deyu Lu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Weichselbaum_A/0/1/0/all/0/1\">Andreas Weichselbaum</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Barros_K/0/1/0/all/0/1\">Kipton Barros</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Konik_R/0/1/0/all/0/1\">Robert M. Konik</a>",
          "description": "We employ variational autoencoders to extract physical insight from a dataset\nof one-particle Anderson impurity model spectral functions. Autoencoders are\ntrained to find a low-dimensional, latent space representation that faithfully\ncharacterizes each element of the training set, as measured by a reconstruction\nerror. Variational autoencoders, a probabilistic generalization of standard\nautoencoders, further condition the learned latent space to promote highly\ninterpretable features. In our study, we find that the learned latent space\ncomponents strongly correlate with well known, but nontrivial, parameters that\ncharacterize emergent behaviors in the Anderson impurity model. In particular,\none latent space component correlates with particle-hole asymmetry, while\nanother is in near one-to-one correspondence with the Kondo temperature, a\ndynamically generated low-energy scale in the impurity model. With symbolic\nregression, we model this component as a function of bare physical input\nparameters and \"rediscover\" the non-perturbative formula for the Kondo\ntemperature. The machine learning pipeline we develop opens opportunities to\ndiscover new domain knowledge in other physical systems.",
          "link": "http://arxiv.org/abs/2107.08013",
          "publishedOn": "2021-07-19T00:49:07.294Z",
          "wordCount": 623,
          "title": "Machine-learning Kondo physics using variational autoencoders. (arXiv:2107.08013v1 [cond-mat.str-el])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lulan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guikang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>",
          "description": "Multiple studies in the past have shown that there is a strong correlation\nbetween human vocal characteristics and facial features. However, existing\napproaches generate faces simply from voice, without exploring the set of\nfeatures that contribute to these observed correlations. A computational\nmethodology to explore this can be devised by rephrasing the question to: \"how\nmuch would a target face have to change in order to be perceived as the\noriginator of a source voice?\" With this in perspective, we propose a framework\nto morph a target face in response to a given voice in a way that facial\nfeatures are implicitly guided by learned voice-face correlation in this paper.\nOur framework includes a guided autoencoder that converts one face to another,\ncontrolled by a unique model-conditioning component called a gating controller\nwhich modifies the reconstructed face based on input voice recordings. We\nevaluate the framework on VoxCelab and VGGFace datasets through human subjects\nand face retrieval. Various experiments demonstrate the effectiveness of our\nproposed model.",
          "link": "http://arxiv.org/abs/2107.07988",
          "publishedOn": "2021-07-19T00:49:07.288Z",
          "wordCount": 621,
          "title": "Controlled AutoEncoders to Generate Faces from Voices. (arXiv:2107.07988v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08001",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gabrie_M/0/1/0/all/0/1\">Marylou Gabri&#xe9;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rotskoff_G/0/1/0/all/0/1\">Grant M. Rotskoff</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vanden_Eijnden_E/0/1/0/all/0/1\">Eric Vanden-Eijnden</a>",
          "description": "Normalizing flows can generate complex target distributions and thus show\npromise in many applications in Bayesian statistics as an alternative or\ncomplement to MCMC for sampling posteriors. Since no data set from the target\nposterior distribution is available beforehand, the flow is typically trained\nusing the reverse Kullback-Leibler (KL) divergence that only requires samples\nfrom a base distribution. This strategy may perform poorly when the posterior\nis complicated and hard to sample with an untrained normalizing flow. Here we\nexplore a distinct training strategy, using the direct KL divergence as loss,\nin which samples from the posterior are generated by (i) assisting a local MCMC\nalgorithm on the posterior with a normalizing flow to accelerate its mixing\nrate and (ii) using the data generated this way to train the flow. The method\nonly requires a limited amount of \\textit{a~priori} input about the posterior,\nand can be used to estimate the evidence required for model validation, as we\nillustrate on examples.",
          "link": "http://arxiv.org/abs/2107.08001",
          "publishedOn": "2021-07-19T00:49:07.282Z",
          "wordCount": 613,
          "title": "Efficient Bayesian Sampling Using Normalizing Flows to Assist Markov Chain Monte Carlo Methods. (arXiv:2107.08001v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07886",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Jiang_H/0/1/0/all/0/1\">Haodi Jiang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Jing_J/0/1/0/all/0/1\">Ju Jing</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wang_J/0/1/0/all/0/1\">Jiasheng Wang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Li_Q/0/1/0/all/0/1\">Qin Li</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wang_J/0/1/0/all/0/1\">Jason T. L. Wang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wang_H/0/1/0/all/0/1\">Haimin Wang</a>",
          "description": "We present a new deep learning method, dubbed FibrilNet, for tracing\nchromospheric fibrils in Halpha images of solar observations. Our method\nconsists of a data pre-processing component that prepares training data from a\nthreshold-based tool, a deep learning model implemented as a Bayesian\nconvolutional neural network for probabilistic image segmentation with\nuncertainty quantification to predict fibrils, and a post-processing component\ncontaining a fibril-fitting algorithm to determine fibril orientations. The\nFibrilNet tool is applied to high-resolution Halpha images from an active\nregion (AR 12665) collected by the 1.6 m Goode Solar Telescope (GST) equipped\nwith high-order adaptive optics at the Big Bear Solar Observatory (BBSO). We\nquantitatively assess the FibrilNet tool, comparing its image segmentation\nalgorithm and fibril-fitting algorithm with those employed by the\nthreshold-based tool. Our experimental results and major findings are\nsummarized as follows. First, the image segmentation results (i.e., detected\nfibrils) of the two tools are quite similar, demonstrating the good learning\ncapability of FibrilNet. Second, FibrilNet finds more accurate and smoother\nfibril orientation angles than the threshold-based tool. Third, FibrilNet is\nfaster than the threshold-based tool and the uncertainty maps produced by\nFibrilNet not only provide a quantitative way to measure the confidence on each\ndetected fibril, but also help identify fibril structures that are not detected\nby the threshold-based tool but are inferred through machine learning. Finally,\nwe apply FibrilNet to full-disk Halpha images from other solar observatories\nand additional high-resolution Halpha images collected by BBSO/GST,\ndemonstrating the tool's usability in diverse datasets.",
          "link": "http://arxiv.org/abs/2107.07886",
          "publishedOn": "2021-07-19T00:49:07.276Z",
          "wordCount": 700,
          "title": "Tracing Halpha Fibrils through Bayesian Deep Learning. (arXiv:2107.07886v1 [astro-ph.SR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1\">Hyeon Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hyung-Kwon Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Jaemin Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngtaek Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jinwook Seo</a>",
          "description": "We propose Steadiness and Cohesiveness, two novel metrics to measure the\ninter-cluster reliability of multidimensional projection (MDP), specifically\nhow well the inter-cluster structures are preserved between the original\nhigh-dimensional space and the low-dimensional projection space. Measuring\ninter-cluster reliability is crucial as it directly affects how well\ninter-cluster tasks (e.g., identifying cluster relationships in the original\nspace from a projected view) can be conducted; however, despite the importance\nof inter-cluster tasks, we found that previous metrics, such as Trustworthiness\nand Continuity, fail to measure inter-cluster reliability. Our metrics consider\ntwo aspects of the inter-cluster reliability: Steadiness measures the extent to\nwhich clusters in the projected space form clusters in the original space, and\nCohesiveness measures the opposite. They extract random clusters with arbitrary\nshapes and positions in one space and evaluate how much the clusters are\nstretched or dispersed in the other space. Furthermore, our metrics can\nquantify pointwise distortions, allowing for the visualization of inter-cluster\nreliability in a projection, which we call a reliability map. Through\nquantitative experiments, we verify that our metrics precisely capture the\ndistortions that harm inter-cluster reliability while previous metrics have\ndifficulty capturing the distortions. A case study also demonstrates that our\nmetrics and the reliability map 1) support users in selecting the proper\nprojection techniques or hyperparameters and 2) prevent misinterpretation while\nperforming inter-cluster tasks, thus allow an adequate identification of\ninter-cluster structure.",
          "link": "http://arxiv.org/abs/2107.07859",
          "publishedOn": "2021-07-19T00:49:07.270Z",
          "wordCount": 686,
          "title": "Measuring and Explaining the Inter-Cluster Reliability of Multidimensional Projections. (arXiv:2107.07859v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1\">Abulikemu Abuduweili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>",
          "description": "Molecular property prediction plays a fundamental role in drug discovery to\ndiscover candidate molecules with target properties. However, molecular\nproperty prediction is essentially a few-shot problem which makes it hard to\nobtain regular models. In this paper, we propose a property-aware adaptive\nrelation networks (PAR) for the few-shot molecular property prediction problem.\nIn comparison to existing works, we leverage the facts that both substructures\nand relationships among molecules are different considering various molecular\nproperties. Our PAR is compatible with existing graph-based molecular encoders,\nand are further equipped with the ability to obtain property-aware molecular\nembedding and model molecular relation graph adaptively. The resultant relation\ngraph also facilitates effective label propagation within each task. Extensive\nexperiments on benchmark molecular property prediction datasets show that our\nmethod consistently outperforms state-of-the-art methods and is able to obtain\nproperty-aware molecular embedding and model molecular relation graph properly.",
          "link": "http://arxiv.org/abs/2107.07994",
          "publishedOn": "2021-07-19T00:49:07.265Z",
          "wordCount": 579,
          "title": "Property-aware Adaptive Relation Networks for Molecular Property Prediction. (arXiv:2107.07994v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hahn_T/0/1/0/all/0/1\">Tim Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernsting_J/0/1/0/all/0/1\">Jan Ernsting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winter_N/0/1/0/all/0/1\">Nils R. Winter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holstein_V/0/1/0/all/0/1\">Vincent Holstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leenings_R/0/1/0/all/0/1\">Ramona Leenings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beisemann_M/0/1/0/all/0/1\">Marie Beisemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisch_L/0/1/0/all/0/1\">Lukas Fisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarink_K/0/1/0/all/0/1\">Kelvin Sarink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emden_D/0/1/0/all/0/1\">Daniel Emden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opel_N/0/1/0/all/0/1\">Nils Opel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redlich_R/0/1/0/all/0/1\">Ronny Redlich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Repple_J/0/1/0/all/0/1\">Jonathan Repple</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grotegerd_D/0/1/0/all/0/1\">Dominik Grotegerd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meinert_S/0/1/0/all/0/1\">Susanne Meinert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirsch_J/0/1/0/all/0/1\">Jochen G. Hirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niendorf_T/0/1/0/all/0/1\">Thoralf Niendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Endemann_B/0/1/0/all/0/1\">Beate Endemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamberg_F/0/1/0/all/0/1\">Fabian Bamberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kroncke_T/0/1/0/all/0/1\">Thomas Kr&#xf6;ncke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulow_R/0/1/0/all/0/1\">Robin B&#xfc;low</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volzke_H/0/1/0/all/0/1\">Henry V&#xf6;lzke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stackelberg_O/0/1/0/all/0/1\">Oyunbileg von Stackelberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sowade_R/0/1/0/all/0/1\">Ramona Felizitas Sowade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umutlu_L/0/1/0/all/0/1\">Lale Umutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_B/0/1/0/all/0/1\">B&#xf6;rge Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caspers_S/0/1/0/all/0/1\">Svenja Caspers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Consortium_G/0/1/0/all/0/1\">German National Cohort Study Center Consortium</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugel_H/0/1/0/all/0/1\">Harald Kugel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kircher_T/0/1/0/all/0/1\">Tilo Kircher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risse_B/0/1/0/all/0/1\">Benjamin Risse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaser_C/0/1/0/all/0/1\">Christian Gaser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1\">James H. Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dannlowski_U/0/1/0/all/0/1\">Udo Dannlowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_K/0/1/0/all/0/1\">Klaus Berger</a>",
          "description": "The deviation between chronological age and age predicted from neuroimaging\ndata has been identified as a sensitive risk-marker of cross-disorder brain\nchanges, growing into a cornerstone of biological age-research. However,\nMachine Learning models underlying the field do not consider uncertainty,\nthereby confounding results with training data density and variability. Also,\nexisting models are commonly based on homogeneous training sets, often not\nindependently validated, and cannot be shared due to data protection issues.\nHere, we introduce an uncertainty-aware, shareable, and transparent Monte-Carlo\nDropout Composite-Quantile-Regression (MCCQR) Neural Network trained on\nN=10,691 datasets from the German National Cohort. The MCCQR model provides\nrobust, distribution-free uncertainty quantification in high-dimensional\nneuroimaging data, achieving lower error rates compared to existing models\nacross ten recruitment centers and in three independent validation samples\n(N=4,004). In two examples, we demonstrate that it prevents spurious\nassociations and increases power to detect accelerated brain-aging. We make the\npre-trained model publicly available.",
          "link": "http://arxiv.org/abs/2107.07977",
          "publishedOn": "2021-07-19T00:49:07.248Z",
          "wordCount": 661,
          "title": "An Uncertainty-Aware, Shareable and Transparent Neural Network Architecture for Brain-Age Modeling. (arXiv:2107.07977v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08020",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jiang_Y/0/1/0/all/0/1\">Yiye Jiang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bigot_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;mie Bigot</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maabout_S/0/1/0/all/0/1\">Sofian Maabout</a>",
          "description": "This paper is concerned with the statistical analysis of matrix-valued time\nseries. These are data collected over a network of sensors (typically a set of\nspatial locations), recording, over time, observations of multiple\nmeasurements. From such data, we propose to learn, in an online fashion, a\ngraph that captures two aspects of dependency: one describing the sparse\nspatial relationship between sensors, and the other characterizing the\nmeasurement relationship. To this purpose, we introduce a novel multivariate\nautoregressive model to infer the graph topology encoded in the coefficient\nmatrix which captures the sparse Granger causality dependency structure present\nin such matrix-valued time series. We decompose the graph by imposing a\nKronecker sum structure on the coefficient matrix. We develop two online\napproaches to learn the graph in a recursive way. The first one uses Wald test\nfor the projected OLS estimation, where we derive the asymptotic distribution\nfor the estimator. For the second one, we formalize a Lasso-type optimization\nproblem. We rely on homotopy algorithms to derive updating rules for estimating\nthe coefficient matrix. Furthermore, we provide an adaptive tuning procedure\nfor the regularization parameter. Numerical experiments using both synthetic\nand real data, are performed to support the effectiveness of the proposed\nlearning approaches.",
          "link": "http://arxiv.org/abs/2107.08020",
          "publishedOn": "2021-07-19T00:49:07.242Z",
          "wordCount": 635,
          "title": "Online Graph Topology Learning from Matrix-valued Time Series. (arXiv:2107.08020v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tavazza_F/0/1/0/all/0/1\">Francesca Tavazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cost_B/0/1/0/all/0/1\">Brian De Cost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_K/0/1/0/all/0/1\">Kamal Choudhary</a>",
          "description": "Uncertainty quantification in Artificial Intelligence (AI)-based predictions\nof material properties is of immense importance for the success and reliability\nof AI applications in material science. While confidence intervals are commonly\nreported for machine learning (ML) models, prediction intervals, i.e., the\nevaluation of the uncertainty on each prediction, are seldomly available. In\nthis work we compare 3 different approaches to obtain such individual\nuncertainty, testing them on 12 ML-physical properties. Specifically, we\ninvestigated using the Quantile loss function, machine learning the prediction\nintervals directly and using Gaussian Processes. We identify each approachs\nadvantages and disadvantages and end up slightly favoring the modeling of the\nindividual uncertainties directly, as it is the easiest to fit and, in most\ncases, minimizes over-and under-estimation of the predicted errors. All data\nfor training and testing were taken from the publicly available JARVIS-DFT\ndatabase, and the codes developed for computing the prediction intervals are\navailable through JARVIS-Tools.",
          "link": "http://arxiv.org/abs/2107.07997",
          "publishedOn": "2021-07-19T00:49:07.236Z",
          "wordCount": 585,
          "title": "Uncertainty Prediction for Machine Learning Models of Material Properties. (arXiv:2107.07997v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Shaan Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattheakis_M/0/1/0/all/0/1\">Marios Mattheakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sondak_D/0/1/0/all/0/1\">David Sondak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protopapas_P/0/1/0/all/0/1\">Pavlos Protopapas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>",
          "description": "Accurately learning the temporal behavior of dynamical systems requires\nmodels with well-chosen learning biases. Recent innovations embed the\nHamiltonian and Lagrangian formalisms into neural networks and demonstrate a\nsignificant improvement over other approaches in predicting trajectories of\nphysical systems. These methods generally tackle autonomous systems that depend\nimplicitly on time or systems for which a control signal is known apriori.\nDespite this success, many real world dynamical systems are non-autonomous,\ndriven by time-dependent forces and experience energy dissipation. In this\nstudy, we address the challenge of learning from such non-autonomous systems by\nembedding the port-Hamiltonian formalism into neural networks, a versatile\nframework that can capture energy dissipation and time-dependent control\nforces. We show that the proposed \\emph{port-Hamiltonian neural network} can\nefficiently learn the dynamics of nonlinear physical systems of practical\ninterest and accurately recover the underlying stationary Hamiltonian,\ntime-dependent force, and dissipative coefficient. A promising outcome of our\nnetwork is its ability to learn and predict chaotic systems such as the Duffing\nequation, for which the trajectories are typically hard to learn.",
          "link": "http://arxiv.org/abs/2107.08024",
          "publishedOn": "2021-07-19T00:49:07.219Z",
          "wordCount": 619,
          "title": "Port-Hamiltonian Neural Networks for Learning Explicit Time-Dependent Dynamical Systems. (arXiv:2107.08024v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_S/0/1/0/all/0/1\">Syed Ali Raza Zaidi</a>",
          "description": "In this paper, we present an overview of Nearest neighbor (NN) methods, which\nare frequently employed for solving classification problems using supervised\nlearning. The article concisely introduces the theoretical background,\nalgorithmic, and implementation aspects along with the key applications. From\nan application standpoint, this article explores the challenges related to the\n5G and beyond wireless networks which can be solved using NN classification\ntechniques.",
          "link": "http://arxiv.org/abs/2107.07869",
          "publishedOn": "2021-07-19T00:49:07.212Z",
          "wordCount": 516,
          "title": "Nearest neighbor Methods and their Applications in Design of 5G & Beyond Wireless Networks. (arXiv:2107.07869v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07871",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Moseley_B/0/1/0/all/0/1\">Ben Moseley</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nissen_Meyer_T/0/1/0/all/0/1\">Tarje Nissen-Meyer</a>",
          "description": "Recently, physics-informed neural networks (PINNs) have offered a powerful\nnew paradigm for solving problems relating to differential equations. Compared\nto classical numerical methods PINNs have several advantages, for example their\nability to provide mesh-free solutions of differential equations and their\nability to carry out forward and inverse modelling within the same optimisation\nproblem. Whilst promising, a key limitation to date is that PINNs have\nstruggled to accurately and efficiently solve problems with large domains\nand/or multi-scale solutions, which is crucial for their real-world\napplication. Multiple significant and related factors contribute to this issue,\nincluding the increasing complexity of the underlying PINN optimisation problem\nas the problem size grows and the spectral bias of neural networks. In this\nwork we propose a new, scalable approach for solving large problems relating to\ndifferential equations called Finite Basis PINNs (FBPINNs). FBPINNs are\ninspired by classical finite element methods, where the solution of the\ndifferential equation is expressed as the sum of a finite set of basis\nfunctions with compact support. In FBPINNs neural networks are used to learn\nthese basis functions, which are defined over small, overlapping subdomains.\nFBINNs are designed to address the spectral bias of neural networks by using\nseparate input normalisation over each subdomain, and reduce the complexity of\nthe underlying optimisation problem by using many smaller neural networks in a\nparallel divide-and-conquer approach. Our numerical experiments show that\nFBPINNs are effective in solving both small and larger, multi-scale problems,\noutperforming standard PINNs in both accuracy and computational resources\nrequired, potentially paving the way to the application of PINNs on large,\nreal-world problems.",
          "link": "http://arxiv.org/abs/2107.07871",
          "publishedOn": "2021-07-19T00:49:07.205Z",
          "wordCount": 721,
          "title": "Finite Basis Physics-Informed Neural Networks (FBPINNs): a scalable domain decomposition approach for solving differential equations. (arXiv:2107.07871v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08011",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Antonakopoulos_K/0/1/0/all/0/1\">Kimon Antonakopoulos</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mertikopoulos_P/0/1/0/all/0/1\">Panayotis Mertikopoulos</a>",
          "description": "We propose a new family of adaptive first-order methods for a class of convex\nminimization problems that may fail to be Lipschitz continuous or smooth in the\nstandard sense. Specifically, motivated by a recent flurry of activity on\nnon-Lipschitz (NoLips) optimization, we consider problems that are continuous\nor smooth relative to a reference Bregman function - as opposed to a global,\nambient norm (Euclidean or otherwise). These conditions encompass a wide range\nof problems with singular objectives, such as Fisher markets, Poisson\ntomography, D-design, and the like. In this setting, the application of\nexisting order-optimal adaptive methods - like UnixGrad or AcceleGrad - is not\npossible, especially in the presence of randomness and uncertainty. The\nproposed method - which we call adaptive mirror descent (AdaMir) - aims to\nclose this gap by concurrently achieving min-max optimal rates in problems that\nare relatively continuous or smooth, including stochastic ones.",
          "link": "http://arxiv.org/abs/2107.08011",
          "publishedOn": "2021-07-19T00:49:07.198Z",
          "wordCount": 596,
          "title": "Adaptive first-order methods revisited: Convex optimization without Lipschitz requirements. (arXiv:2107.08011v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yiming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1\">David P. Woodruff</a>",
          "description": "In applications such as natural language processing or computer vision, one\nis given a large $n \\times d$ matrix $A = (a_{i,j})$ and would like to compute\na matrix decomposition, e.g., a low rank approximation, of a function $f(A) =\n(f(a_{i,j}))$ applied entrywise to $A$. A very important special case is the\nlikelihood function $f\\left( A \\right ) = \\log{\\left( \\left| a_{ij}\\right|\n+1\\right)}$. A natural way to do this would be to simply apply $f$ to each\nentry of $A$, and then compute the matrix decomposition, but this requires\nstoring all of $A$ as well as multiple passes over its entries. Recent work of\nLiang et al.\\ shows how to find a rank-$k$ factorization to $f(A)$ for an $n\n\\times n$ matrix $A$ using only $n \\cdot \\operatorname{poly}(\\epsilon^{-1}k\\log\nn)$ words of memory, with overall error $10\\|f(A)-[f(A)]_k\\|_F^2 +\n\\operatorname{poly}(\\epsilon/k) \\|f(A)\\|_{1,2}^2$, where $[f(A)]_k$ is the best\nrank-$k$ approximation to $f(A)$ and $\\|f(A)\\|_{1,2}^2$ is the square of the\nsum of Euclidean lengths of rows of $f(A)$. Their algorithm uses three passes\nover the entries of $A$. The authors pose the open question of obtaining an\nalgorithm with $n \\cdot \\operatorname{poly}(\\epsilon^{-1}k\\log n)$ words of\nmemory using only a single pass over the entries of $A$. In this paper we\nresolve this open question, obtaining the first single-pass algorithm for this\nproblem and for the same class of functions $f$ studied by Liang et al.\nMoreover, our error is $\\|f(A)-[f(A)]_k\\|_F^2 + \\operatorname{poly}(\\epsilon/k)\n\\|f(A)\\|_F^2$, where $\\|f(A)\\|_F^2$ is the sum of squares of Euclidean lengths\nof rows of $f(A)$. Thus our error is significantly smaller, as it removes the\nfactor of $10$ and also $\\|f(A)\\|_F^2 \\leq \\|f(A)\\|_{1,2}^2$. We also give an\nalgorithm for regression, pointing out an error in previous work, and\nempirically validate our results.",
          "link": "http://arxiv.org/abs/2107.07889",
          "publishedOn": "2021-07-19T00:49:07.186Z",
          "wordCount": 731,
          "title": "Single Pass Entrywise-Transformed Low Rank Approximation. (arXiv:2107.07889v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kolcun_R/0/1/0/all/0/1\">Roman Kolcun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_D/0/1/0/all/0/1\">Diana Andreea Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safronov_V/0/1/0/all/0/1\">Vadim Safronov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1\">Poonam Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandalari_A/0/1/0/all/0/1\">Anna Maria Mandalari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortier_R/0/1/0/all/0/1\">Richard Mortier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddadi_H/0/1/0/all/0/1\">Hamed Haddadi</a>",
          "description": "Internet-of-Things (IoT) devices are known to be the source of many security\nproblems, and as such, they would greatly benefit from automated management.\nThis requires robustly identifying devices so that appropriate network security\npolicies can be applied. We address this challenge by exploring how to\naccurately identify IoT devices based on their network behavior, while\nleveraging approaches previously proposed by other researchers.\n\nWe compare the accuracy of four different previously proposed machine\nlearning models (tree-based and neural network-based) for identifying IoT\ndevices. We use packet trace data collected over a period of six months from a\nlarge IoT test-bed. We show that, while all models achieve high accuracy when\nevaluated on the same dataset as they were trained on, their accuracy degrades\nover time, when evaluated on data collected outside the training set. We show\nthat on average the models' accuracy degrades after a couple of weeks by up to\n40 percentage points (on average between 12 and 21 percentage points). We argue\nthat, in order to keep the models' accuracy at a high level, these need to be\ncontinuously updated.",
          "link": "http://arxiv.org/abs/2107.07818",
          "publishedOn": "2021-07-19T00:49:07.140Z",
          "wordCount": 634,
          "title": "Revisiting IoT Device Identification. (arXiv:2107.07818v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thor_M/0/1/0/all/0/1\">Mathias Thor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manoonpong_P/0/1/0/all/0/1\">Poramate Manoonpong</a>",
          "description": "Legged robots have significant potential to operate in highly unstructured\nenvironments. The design of locomotion control is, however, still challenging.\nCurrently, controllers must be either manually designed for specific robots and\ntasks, or automatically designed via machine learning methods that require long\ntraining times and yield large opaque controllers. Drawing inspiration from\nanimal locomotion, we propose a simple yet versatile modular neural control\nstructure with fast learning. The key advantages of our approach are that\nbehavior-specific control modules can be added incrementally to obtain\nincreasingly complex emergent locomotion behaviors, and that neural connections\ninterfacing with existing modules can be quickly and automatically learned. In\na series of experiments, we show how eight modules can be quickly learned and\nadded to a base control module to obtain emergent adaptive behaviors allowing a\nhexapod robot to navigate in complex environments. We also show that modules\ncan be added and removed during operation without affecting the functionality\nof the remaining controller. Finally, the control approach was successfully\ndemonstrated on a physical hexapod robot. Taken together, our study reveals a\nsignificant step towards fast automatic design of versatile neural locomotion\ncontrol for complex robotic systems.",
          "link": "http://arxiv.org/abs/2107.07844",
          "publishedOn": "2021-07-19T00:49:07.134Z",
          "wordCount": 641,
          "title": "Versatile modular neural locomotion control with fast learning. (arXiv:2107.07844v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07875",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nalamada_T/0/1/0/all/0/1\">Trikay Nalamada</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Agarwal_S/0/1/0/all/0/1\">Shruti Agarwal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jahja_M/0/1/0/all/0/1\">Maria Jahja</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chakraborty_B/0/1/0/all/0/1\">Bibhas Chakraborty</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ghosh_P/0/1/0/all/0/1\">Palash Ghosh</a>",
          "description": "A dynamic treatment regimen (DTR) is a set of decision rules to personalize\ntreatments for an individual using their medical history. The Q-learning based\nQ-shared algorithm has been used to develop DTRs that involve decision rules\nshared across multiple stages of intervention. We show that the existing\nQ-shared algorithm can suffer from non-convergence due to the use of linear\nmodels in the Q-learning setup, and identify the condition in which Q-shared\nfails. Leveraging properties from expansion-constrained ordinary least-squares,\nwe give a penalized Q-shared algorithm that not only converges in settings that\nviolate the condition, but can outperform the original Q-shared algorithm even\nwhen the condition is satisfied. We give evidence for the proposed method in a\nreal-world application and several synthetic simulations.",
          "link": "http://arxiv.org/abs/2107.07875",
          "publishedOn": "2021-07-19T00:49:07.126Z",
          "wordCount": 561,
          "title": "A Penalized Shared-parameter Algorithm for Estimating Optimal Dynamic Treatment Regimens. (arXiv:2107.07875v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07853",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Konig_G/0/1/0/all/0/1\">Gunnar K&#xf6;nig</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Freiesleben_T/0/1/0/all/0/1\">Timo Freiesleben</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Grosse_Wentrup_M/0/1/0/all/0/1\">Moritz Grosse-Wentrup</a>",
          "description": "Algorithmic recourse explanations inform stakeholders on how to act to revert\nunfavorable predictions. However, in general ML models do not predict well in\ninterventional distributions. Thus, an action that changes the prediction in\nthe desired way may not lead to an improvement of the underlying target. Such\nrecourse is neither meaningful nor robust to model refits. Extending the work\nof Karimi et al. (2021), we propose meaningful algorithmic recourse (MAR) that\nonly recommends actions that improve both prediction and target. We justify\nthis selection constraint by highlighting the differences between model audit\nand meaningful, actionable recourse explanations. Additionally, we introduce a\nrelaxation of MAR called effective algorithmic recourse (EAR), which, under\ncertain assumptions, yields meaningful recourse by only allowing interventions\non causes of the target.",
          "link": "http://arxiv.org/abs/2107.07853",
          "publishedOn": "2021-07-19T00:49:07.073Z",
          "wordCount": 570,
          "title": "A Causal Perspective on Meaningful and Robust Algorithmic Recourse. (arXiv:2107.07853v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haan_P/0/1/0/all/0/1\">Puck de Haan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1\">Sindy L&#xf6;we</a>",
          "description": "Reliable detection of anomalies is crucial when deploying machine learning\nmodels in practice, but remains challenging due to the lack of labeled data. To\ntackle this challenge, contrastive learning approaches are becoming\nincreasingly popular, given the impressive results they have achieved in\nself-supervised representation learning settings. However, while most existing\ncontrastive anomaly detection and segmentation approaches have been applied to\nimages, none of them can use the contrastive losses directly for both anomaly\ndetection and segmentation. In this paper, we close this gap by making use of\nthe Contrastive Predictive Coding model (arXiv:1807.03748). We show that its\npatch-wise contrastive loss can directly be interpreted as an anomaly score,\nand how this allows for the creation of anomaly segmentation masks. The\nresulting model achieves promising results for both anomaly detection and\nsegmentation on the challenging MVTec-AD dataset.",
          "link": "http://arxiv.org/abs/2107.07820",
          "publishedOn": "2021-07-19T00:49:07.065Z",
          "wordCount": 583,
          "title": "Contrastive Predictive Coding for Anomaly Detection. (arXiv:2107.07820v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1\">Zahra Gharaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1\">Shreyas Kowshik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1\">Oliver Stromann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>",
          "description": "We present a novel learning-based approach to graph representations of road\nnetworks employing state-of-the-art graph convolutional neural networks. Our\napproach is applied to realistic road networks of 17 cities from Open Street\nMap. While edge features are crucial to generate descriptive graph\nrepresentations of road networks, graph convolutional networks usually rely on\nnode features only. We show that the highly representative edge features can\nstill be integrated into such networks by applying a line graph transformation.\nWe also propose a method for neighborhood sampling based on a topological\nneighborhood composed of both local and global neighbors. We compare the\nperformance of learning representations using different types of neighborhood\naggregation functions in transductive and inductive tasks and in supervised and\nunsupervised learning. Furthermore, we propose a novel aggregation approach,\nGraph Attention Isomorphism Network, GAIN. Our results show that GAIN\noutperforms state-of-the-art methods on the road type classification problem.",
          "link": "http://arxiv.org/abs/2107.07791",
          "publishedOn": "2021-07-19T00:49:07.046Z",
          "wordCount": 587,
          "title": "Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dunjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1\">Zhaoyan Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Mingwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>",
          "description": "Graph classification plays a significant role in network analysis. It also\nfaces potential security threat like adversarial attacks. Some defense methods\nmay sacrifice algorithm complexity for robustness like adversarial training,\nwhile others may sacrifice the clean example performance such as\nsmoothing-based defense. Most of them are suffered from high-complexity or less\ntransferability. To address this problem, we proposed EGC$^2$, an enhanced\ngraph classification model with easy graph compression. EGC$^2$ captures the\nrelationship between features of different nodes by constructing feature graphs\nand improving aggregate node-level representation. To achieve lower complexity\ndefense applied to various graph classification models, EGC$^2$ utilizes a\ncentrality-based edge importance index to compress graphs, filtering out\ntrivial structures and even adversarial perturbations of the input graphs, thus\nimproves its robustness. Experiments on seven benchmark datasets demonstrate\nthat the proposed feature read-out and graph compression mechanisms enhance the\nrobustness of various basic models, thus achieving the state-of-the-art\nperformance of accuracy and robustness in the threat of different adversarial\nattacks.",
          "link": "http://arxiv.org/abs/2107.07737",
          "publishedOn": "2021-07-19T00:49:07.040Z",
          "wordCount": 600,
          "title": "EGC2: Enhanced Graph Classification with Easy Graph Compression. (arXiv:2107.07737v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07729",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Shivshankar Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_A/0/1/0/all/0/1\">Anand Vir Singh Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Maneet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Karamjit Singh</a>",
          "description": "Temporal Point Processes (TPPs) are often used to represent the sequence of\nevents ordered as per the time of occurrence. Owing to their flexible nature,\nTPPs have been used to model different scenarios and have shown applicability\nin various real-world applications. While TPPs focus on modeling the event\noccurrence, Marked Temporal Point Process (MTPP) focuses on modeling the\ncategory/class of the event as well (termed as the marker). Research in MTPP\nhas garnered substantial attention over the past few years, with an extensive\nfocus on supervised algorithms. Despite the research focus, limited attention\nhas been given to the challenging problem of developing solutions in\nsemi-supervised settings, where algorithms have access to a mix of labeled and\nunlabeled data. This research proposes a novel algorithm for Semi-supervised\nLearning for Marked Temporal Point Processes (SSL-MTPP) applicable in such\nscenarios. The proposed SSL-MTPP algorithm utilizes a combination of labeled\nand unlabeled data for learning a robust marker prediction model. The proposed\nalgorithm utilizes an RNN-based Encoder-Decoder module for learning effective\nrepresentations of the time sequence. The efficacy of the proposed algorithm\nhas been demonstrated via multiple protocols on the Retweet dataset, where the\nproposed SSL-MTPP demonstrates improved performance in comparison to the\ntraditional supervised learning approach.",
          "link": "http://arxiv.org/abs/2107.07729",
          "publishedOn": "2021-07-19T00:49:07.033Z",
          "wordCount": 637,
          "title": "Semi-supervised Learning for Marked Temporal Point Processes. (arXiv:2107.07729v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henkel_C/0/1/0/all/0/1\">Christof Henkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_P/0/1/0/all/0/1\">Pascal Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_P/0/1/0/all/0/1\">Philipp Singer</a>",
          "description": "We present a robust classification approach for avian vocalization in complex\nand diverse soundscapes, achieving second place in the BirdCLEF2021 challenge.\nWe illustrate how to make full use of pre-trained convolutional neural\nnetworks, by using an efficient modeling and training routine supplemented by\nnovel augmentation methods. Thereby, we improve the generalization of weakly\nlabeled crowd-sourced data to productive data collected by autonomous recording\nunits. As such, we illustrate how to progress towards an accurate automated\nassessment of avian population which would enable global biodiversity\nmonitoring at scale, impossible by manual annotation.",
          "link": "http://arxiv.org/abs/2107.07728",
          "publishedOn": "2021-07-19T00:49:07.026Z",
          "wordCount": 541,
          "title": "Recognizing bird species in diverse soundscapes under weak supervision. (arXiv:2107.07728v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Q/0/1/0/all/0/1\">Qin Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>",
          "description": "Leveraging unlabelled data through weak or distant supervision is a\ncompelling approach to developing more effective text classification models.\nThis paper proposes a simple but effective data augmentation method, which\nleverages the idea of pseudo-labelling to select samples from noisy distant\nsupervision annotation datasets. The result shows that the proposed method\nimproves the accuracy of biased news detection models.",
          "link": "http://arxiv.org/abs/2107.07705",
          "publishedOn": "2021-07-19T00:49:07.019Z",
          "wordCount": 491,
          "title": "Pseudo-labelling Enhanced Media Bias Detection. (arXiv:2107.07705v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1\">Arnab Kumar Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asnani_H/0/1/0/all/0/1\">Himanshu Asnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1\">Parag Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1\">Prathosh AP</a>",
          "description": "Clustering single-cell RNA sequence (scRNA-seq) data poses statistical and\ncomputational challenges due to their high-dimensionality and data-sparsity,\nalso known as `dropout' events. Recently, Regularized Auto-Encoder (RAE) based\ndeep neural network models have achieved remarkable success in learning robust\nlow-dimensional representations. The basic idea in RAEs is to learn a\nnon-linear mapping from the high-dimensional data space to a low-dimensional\nlatent space and vice-versa, simultaneously imposing a distributional prior on\nthe latent space, which brings in a regularization effect. This paper argues\nthat RAEs suffer from the infamous problem of bias-variance trade-off in their\nnaive formulation. While a simple AE without a latent regularization results in\ndata over-fitting, a very strong prior leads to under-representation and thus\nbad clustering. To address the above issues, we propose a modified RAE\nframework (called the scRAE) for effective clustering of the single-cell RNA\nsequencing data. scRAE consists of deterministic AE with a flexibly learnable\nprior generator network, which is jointly trained with the AE. This facilitates\nscRAE to trade-off better between the bias and variance in the latent space. We\ndemonstrate the efficacy of the proposed method through extensive\nexperimentation on several real-world single-cell Gene expression datasets.",
          "link": "http://arxiv.org/abs/2107.07709",
          "publishedOn": "2021-07-19T00:49:07.002Z",
          "wordCount": 642,
          "title": "ScRAE: Deterministic Regularized Autoencoders with Flexible Priors for Clustering Single-cell Gene Expression Data. (arXiv:2107.07709v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rushil Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1\">Vishal Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_Y/0/1/0/all/0/1\">Yash Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yitao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1\">Guy Van den Broeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1\">Parag Singla</a>",
          "description": "We focus on the task of future frame prediction in video governed by\nunderlying physical dynamics. We work with models which are object-centric,\ni.e., explicitly work with object representations, and propagate a loss in the\nlatent space. Specifically, our research builds on recent work by Kipf et al.\n\\cite{kipf&al20}, which predicts the next state via contrastive learning of\nobject interactions in a latent space using a Graph Neural Network. We argue\nthat injecting explicit inductive bias in the model, in form of general\nphysical laws, can help not only make the model more interpretable, but also\nimprove the overall prediction of model. As a natural by-product, our model can\nlearn feature maps which closely resemble actual object positions in the image,\nwithout having any explicit supervision about the object positions at the\ntraining time. In comparison with earlier works \\cite{jaques&al20}, which\nassume a complete knowledge of the dynamics governing the motion in the form of\na physics engine, we rely only on the knowledge of general physical laws, such\nas, world consists of objects, which have position and velocity. We propose an\nadditional decoder based loss in the pixel space, imposed in a curriculum\nmanner, to further refine the latent space predictions. Experiments in multiple\ndifferent settings demonstrate that while Kipf et al. model is effective at\ncapturing object interactions, our model can be significantly more effective at\nlocalising objects, resulting in improved performance in 3 out of 4 domains\nthat we experiment with. Additionally, our model can learn highly intrepretable\nfeature maps, resembling actual object positions.",
          "link": "http://arxiv.org/abs/2107.07713",
          "publishedOn": "2021-07-19T00:49:06.996Z",
          "wordCount": 709,
          "title": "Towards an Interpretable Latent Space in Structured Models for Video Prediction. (arXiv:2107.07713v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07732",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Chen_X/0/1/0/all/0/1\">Xinyi Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ghai_U/0/1/0/all/0/1\">Udaya Ghai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hazan_E/0/1/0/all/0/1\">Elad Hazan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Megretski_A/0/1/0/all/0/1\">Alexandre Megretski</a>",
          "description": "We study online control of an unknown nonlinear dynamical system that is\napproximated by a time-invariant linear system with model misspecification. Our\nstudy focuses on robustness, which measures how much deviation from the assumed\nlinear approximation can be tolerated while maintaining a bounded $\\ell_2$-gain\ncompared to the optimal control in hindsight. Some models cannot be stabilized\neven with perfect knowledge of their coefficients: the robustness is limited by\nthe minimal distance between the assumed dynamics and the set of unstabilizable\ndynamics. Therefore it is necessary to assume a lower bound on this distance.\nUnder this assumption, and with full observation of the $d$ dimensional state,\nwe describe an efficient controller that attains $\\Omega(\\frac{1}{\\sqrt{d}})$\nrobustness together with an $\\ell_2$-gain whose dimension dependence is near\noptimal. We also give an inefficient algorithm that attains constant robustness\nindependent of the dimension, with a finite but sub-optimal $\\ell_2$-gain.",
          "link": "http://arxiv.org/abs/2107.07732",
          "publishedOn": "2021-07-19T00:49:06.989Z",
          "wordCount": 576,
          "title": "Robust Online Control with Model Misspecification. (arXiv:2107.07732v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07752",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cognolato_F/0/1/0/all/0/1\">Francesco Cognolato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+OBrien_K/0/1/0/all/0/1\">Kieran O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_J/0/1/0/all/0/1\">Jin Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robinson_S/0/1/0/all/0/1\">Simon Robinson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laun_F/0/1/0/all/0/1\">Frederik B. Laun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barth_M/0/1/0/all/0/1\">Markus Barth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bollmann_S/0/1/0/all/0/1\">Steffen Bollmann</a>",
          "description": "Deep learning based Quantitative Susceptibility Mapping (QSM) has shown great\npotential in recent years, outperforming traditional non-learning approaches in\nspeed and accuracy. However, many of the current deep learning approaches are\nnot data consistent, require in vivo training data or do not solve all steps of\nthe QSM processing pipeline. Here we aim to overcome these limitations and\ndeveloped a framework to solve the QSM processing steps jointly. We developed a\nnew hybrid training data generation method that enables the end-to-end training\nfor solving background field correction and dipole inversion in a\ndata-consistent fashion using a variational network that combines the QSM model\nterm and a learned regularizer. We demonstrate that NeXtQSM overcomes the\nlimitations of previous model-agnostic deep learning methods and show that\nNeXtQSM offers a complete deep learning based pipeline for computing robust,\nfast and accurate quantitative susceptibility maps.",
          "link": "http://arxiv.org/abs/2107.07752",
          "publishedOn": "2021-07-19T00:49:06.983Z",
          "wordCount": 613,
          "title": "NeXtQSM -- A complete deep learning pipeline for data-consistent quantitative susceptibility mapping trained with hybrid data. (arXiv:2107.07752v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07788",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhong-Ping Jiang</a>",
          "description": "This paper studies the optimal stationary control of continuous-time linear\nstochastic systems with both additive and multiplicative noises, using\nreinforcement learning techniques. Based on policy iteration, a novel\noff-policy reinforcement learning algorithm, named optimistic\nleast-squares-based policy iteration, is proposed which is able to iteratively\nfind near-optimal policies of the optimal stationary control problem directly\nfrom input/state data without explicitly identifying any system matrices,\nstarting from an initial admissible control policy. The solutions given by the\nproposed optimistic least-squares-based policy iteration are proved to converge\nto a small neighborhood of the optimal solution with probability one, under\nmild conditions. The application of the proposed algorithm to a triple inverted\npendulum example validates its feasibility and effectiveness.",
          "link": "http://arxiv.org/abs/2107.07788",
          "publishedOn": "2021-07-19T00:49:06.976Z",
          "wordCount": 567,
          "title": "Reinforcement Learning for Optimal Stationary Control of Linear Stochastic Systems. (arXiv:2107.07788v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07582",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Mamandipoor_B/0/1/0/all/0/1\">Behrooz Mamandipoor</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yeung_W/0/1/0/all/0/1\">Wesley Yeung</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Agha_Mir_Salim_L/0/1/0/all/0/1\">Louis Agha-Mir-Salim</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Stone_D/0/1/0/all/0/1\">David J. Stone</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Osmani_V/0/1/0/all/0/1\">Venet Osmani</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Celi_L/0/1/0/all/0/1\">Leo Anthony Celi</a>",
          "description": "Purpose. Elevations in initially obtained serum lactate levels are strong\npredictors of mortality in critically ill patients. Identifying patients whose\nserum lactate levels are more likely to increase can alert physicians to\nintensify care and guide them in the frequency of tending the blood test. We\ninvestigate whether machine learning models can predict subsequent serum\nlactate changes.\n\nMethods. We investigated serum lactate change prediction using the MIMIC-III\nand eICU-CRD datasets in internal as well as external validation of the eICU\ncohort on the MIMIC-III cohort. Three subgroups were defined based on the\ninitial lactate levels: i) normal group (<2 mmol/L), ii) mild group (2-4\nmmol/L), and iii) severe group (>4 mmol/L). Outcomes were defined based on\nincrease or decrease of serum lactate levels between the groups. We also\nperformed sensitivity analysis by defining the outcome as lactate change of\n>10% and furthermore investigated the influence of the time interval between\nsubsequent lactate measurements on predictive performance.\n\nResults. The LSTM models were able to predict deterioration of serum lactate\nvalues of MIMIC-III patients with an AUC of 0.77 (95% CI 0.762-0.771) for the\nnormal group, 0.77 (95% CI 0.768-0.772) for the mild group, and 0.85 (95% CI\n0.840-0.851) for the severe group, with a slightly lower performance in the\nexternal validation.\n\nConclusion. The LSTM demonstrated good discrimination of patients who had\ndeterioration in serum lactate levels. Clinical studies are needed to evaluate\nwhether utilization of a clinical decision support tool based on these results\ncould positively impact decision-making and patient outcomes.",
          "link": "http://arxiv.org/abs/2107.07582",
          "publishedOn": "2021-07-19T00:49:06.959Z",
          "wordCount": 727,
          "title": "Prediction of Blood Lactate Values in Critically Ill Patients: A Retrospective Multi-center Cohort Study. (arXiv:2107.07582v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1\">Christopher T.H Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1\">Ngai-Man Cheung</a>",
          "description": "Deep generative models have made much progress in improving training\nstability and quality of generated data. Recently there has been increased\ninterest in the fairness of deep-generated data. Fairness is important in many\napplications, e.g. law enforcement, as biases will affect efficacy. Central to\nfair data generation are the fairness metrics for the assessment and evaluation\nof different generative models. In this paper, we first review fairness metrics\nproposed in previous works and highlight potential weaknesses. We then discuss\na performance benchmark framework along with the assessment of alternative\nmetrics.",
          "link": "http://arxiv.org/abs/2107.07754",
          "publishedOn": "2021-07-19T00:49:06.952Z",
          "wordCount": 525,
          "title": "Measuring Fairness in Generative Models. (arXiv:2107.07754v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07659",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_T/0/1/0/all/0/1\">Toshinori Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lingwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_T/0/1/0/all/0/1\">Takamitsu Matsubara</a>",
          "description": "The recent booming of entropy-regularized literature reveals that\nKullback-Leibler (KL) regularization brings advantages to Reinforcement\nLearning (RL) algorithms by canceling out errors under mild assumptions.\nHowever, existing analyses focus on fixed regularization with a constant\nweighting coefficient and have not considered the case where the coefficient is\nallowed to change dynamically. In this paper, we study the dynamic coefficient\nscheme and present the first asymptotic error bound. Based on the dynamic\ncoefficient error bound, we propose an effective scheme to tune the coefficient\naccording to the magnitude of error in favor of more robust learning. On top of\nthis development, we propose a novel algorithm: Geometric Value Iteration (GVI)\nthat features a dynamic error-aware KL coefficient design aiming to mitigate\nthe impact of errors on the performance. Our experiments demonstrate that GVI\ncan effectively exploit the trade-off between learning speed and robustness\nover uniform averaging of constant KL coefficient. The combination of GVI and\ndeep networks shows stable learning behavior even in the absence of a target\nnetwork where algorithms with a constant KL coefficient would greatly oscillate\nor even fail to converge.",
          "link": "http://arxiv.org/abs/2107.07659",
          "publishedOn": "2021-07-19T00:49:06.946Z",
          "wordCount": 616,
          "title": "Geometric Value Iteration: Dynamic Error-Aware KL Regularization for Reinforcement Learning. (arXiv:2107.07659v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barata_R/0/1/0/all/0/1\">Ricardo Barata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leite_M/0/1/0/all/0/1\">Miguel Leite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacheco_R/0/1/0/all/0/1\">Ricardo Pacheco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampaio_M/0/1/0/all/0/1\">Marco O. P. Sampaio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ascensao_J/0/1/0/all/0/1\">Jo&#xe3;o Tiago Ascens&#xe3;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bizarro_P/0/1/0/all/0/1\">Pedro Bizarro</a>",
          "description": "Labeled data is essential in modern systems that rely on Machine Learning\n(ML) for predictive modelling. Such systems may suffer from the cold-start\nproblem: supervised models work well but, initially, there are no labels, which\nare costly or slow to obtain. This problem is even worse in imbalanced data\nscenarios. Online financial fraud detection is an example where labeling is: i)\nexpensive, or ii) it suffers from long delays, if relying on victims filing\ncomplaints. The latter may not be viable if a model has to be in place\nimmediately, so an option is to ask analysts to label events while minimizing\nthe number of annotations to control costs. We propose an Active Learning (AL)\nannotation system for datasets with orders of magnitude of class imbalance, in\na cold start streaming scenario. We present a computationally efficient\nOutlier-based Discriminative AL approach (ODAL) and design a novel 3-stage\nsequence of AL labeling policies where it is used as warm-up. Then, we perform\nempirical studies in four real world datasets, with various magnitudes of class\nimbalance. The results show that our method can more quickly reach a high\nperformance model than standard AL policies. Its observed gains over random\nsampling can reach 80% and be competitive with policies with an unlimited\nannotation budget or additional historical data (with 1/10 to 1/50 of the\nlabels).",
          "link": "http://arxiv.org/abs/2107.07724",
          "publishedOn": "2021-07-19T00:49:06.939Z",
          "wordCount": 678,
          "title": "Active learning for online training in imbalanced data streams under cold start. (arXiv:2107.07724v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07740",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Ming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhunan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Cunhang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huiguang He</a>",
          "description": "As an essential element for the diagnosis and rehabilitation of psychiatric\ndisorders, the electroencephalogram (EEG) based emotion recognition has\nachieved significant progress due to its high precision and reliability.\nHowever, one obstacle to practicality lies in the variability between subjects\nand sessions. Although several studies have adopted domain adaptation (DA)\napproaches to tackle this problem, most of them treat multiple EEG data from\ndifferent subjects and sessions together as a single source domain for\ntransfer, which either fails to satisfy the assumption of domain adaptation\nthat the source has a certain marginal distribution, or increases the\ndifficulty of adaptation. We therefore propose the multi-source marginal\ndistribution adaptation (MS-MDA) for EEG emotion recognition, which takes both\ndomain-invariant and domain-specific features into consideration. First, we\nassume that different EEG data share the same low-level features, then we\nconstruct independent branches for multiple EEG data source domains to adopt\none-to-one domain adaptation and extract domain-specific features. Finally, the\ninference is made by multiple branches. We evaluate our method on SEED and\nSEED-IV for recognizing three and four emotions, respectively. Experimental\nresults show that the MS-MDA outperforms the comparison methods and\nstate-of-the-art models in cross-session and cross-subject transfer scenarios\nin our settings. Codes at https://github.com/VoiceBeer/MS-MDA.",
          "link": "http://arxiv.org/abs/2107.07740",
          "publishedOn": "2021-07-19T00:49:06.933Z",
          "wordCount": 657,
          "title": "MS-MDA: Multisource Marginal Distribution Adaptation for Cross-subject and Cross-session EEG Emotion Recognition. (arXiv:2107.07740v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_N/0/1/0/all/0/1\">Niel Teng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rosanne Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yosinski_J/0/1/0/all/0/1\">Jason Yosinski</a>",
          "description": "Not all examples are created equal, but standard deep neural network training\nprotocols treat each training point uniformly. Each example is propagated\nforward and backward through the network the same amount of times, independent\nof how much the example contributes to the learning protocol. Recent work has\nproposed ways to accelerate training by deviating from this uniform treatment.\nPopular methods entail up-weighting examples that contribute more to the loss\nwith the intuition that examples with low loss have already been learned by the\nmodel, so their marginal value to the training procedure should be lower. This\nview assumes that updating the model with high loss examples will be beneficial\nto the model. However, this may not hold for noisy, real world data. In this\npaper, we theorize and then empirically demonstrate that loss-based\nacceleration methods degrade in scenarios with noisy and corrupted data. Our\nwork suggests measures of example difficulty need to correctly separate out\nnoise from other types of challenging examples.",
          "link": "http://arxiv.org/abs/2107.07741",
          "publishedOn": "2021-07-19T00:49:06.895Z",
          "wordCount": 589,
          "title": "When does loss-based prioritization fail?. (arXiv:2107.07741v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07738",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiazheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>",
          "description": "Scenario generation is a fundamental and crucial tool for decision-making in\npower systems with high-penetration renewables. Based on big historical data, a\nnovel federated deep generative learning framework, called Fed-LSGAN, is\nproposed by integrating federated learning and least square generative\nadversarial networks (LSGANs) for renewable scenario generation. Specifically,\nfederated learning learns a shared global model in a central server from\nrenewable sites at network edges, which enables the Fed-LSGAN to generate\nscenarios in a privacy-preserving manner without sacrificing the generation\nquality by transferring model parameters, rather than all data. Meanwhile, the\nLSGANs-based deep generative model generates scenarios that conform to the\ndistribution of historical data through fully capturing the spatial-temporal\ncharacteristics of renewable powers, which leverages the least squares loss\nfunction to improve the training stability and generation quality. The\nsimulation results demonstrate that the proposal manages to generate\nhigh-quality renewable scenarios and outperforms the state-of-the-art\ncentralized methods. Besides, an experiment with different federated learning\nsettings is designed and conducted to verify the robustness of our method.",
          "link": "http://arxiv.org/abs/2107.07738",
          "publishedOn": "2021-07-19T00:49:06.886Z",
          "wordCount": 626,
          "title": "Privacy-preserving Spatiotemporal Scenario Generation of Renewable Energies: A Federated Deep Generative Learning Approach. (arXiv:2107.07738v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07687",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yuming Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sanz_Alonso_D/0/1/0/all/0/1\">Daniel Sanz-Alonso</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Willett_R/0/1/0/all/0/1\">Rebecca Willett</a>",
          "description": "Data assimilation is concerned with sequentially estimating a\ntemporally-evolving state. This task, which arises in a wide range of\nscientific and engineering applications, is particularly challenging when the\nstate is high-dimensional and the state-space dynamics are unknown. This paper\nintroduces a machine learning framework for learning dynamical systems in data\nassimilation. Our auto-differentiable ensemble Kalman filters (AD-EnKFs) blend\nensemble Kalman filters for state recovery with machine learning tools for\nlearning the dynamics. In doing so, AD-EnKFs leverage the ability of ensemble\nKalman filters to scale to high-dimensional states and the power of automatic\ndifferentiation to train high-dimensional surrogate models for the dynamics.\nNumerical results using the Lorenz-96 model show that AD-EnKFs outperform\nexisting methods that use expectation-maximization or particle filters to merge\ndata assimilation and machine learning. In addition, AD-EnKFs are easy to\nimplement and require minimal tuning.",
          "link": "http://arxiv.org/abs/2107.07687",
          "publishedOn": "2021-07-19T00:49:06.870Z",
          "wordCount": 564,
          "title": "Auto-differentiable Ensemble Kalman Filters. (arXiv:2107.07687v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1\">Sanjoy Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navlakha_S/0/1/0/all/0/1\">Saket Navlakha</a>",
          "description": "Continual learning in computational systems is challenging due to\ncatastrophic forgetting. We discovered a two layer neural circuit in the fruit\nfly olfactory system that addresses this challenge by uniquely combining sparse\ncoding and associative learning. In the first layer, odors are encoded using\nsparse, high dimensional representations, which reduces memory interference by\nactivating non overlapping populations of neurons for different odors. In the\nsecond layer, only the synapses between odor activated neurons and the output\nneuron associated with the odor are modified during learning; the rest of the\nweights are frozen to prevent unrelated memories from being overwritten. We\nshow empirically and analytically that this simple and lightweight algorithm\nsignificantly boosts continual learning performance. The fly associative\nlearning algorithm is strikingly similar to the classic perceptron learning\nalgorithm, albeit two modifications, which we show are critical for reducing\ncatastrophic forgetting. Overall, fruit flies evolved an efficient lifelong\nlearning algorithm, and circuit mechanisms from neuroscience can be translated\nto improve machine computation.",
          "link": "http://arxiv.org/abs/2107.07617",
          "publishedOn": "2021-07-19T00:49:06.824Z",
          "wordCount": 595,
          "title": "Algorithmic insights on continual learning from fruit flies. (arXiv:2107.07617v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1\">Khondker Fariha Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1\">Sharif Amit Kamran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavakkoli_A/0/1/0/all/0/1\">Alireza Tavakkoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1\">Daniel Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasegarar_S/0/1/0/all/0/1\">Sutharshan Rajasegarar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmaker_C/0/1/0/all/0/1\">Chandan Karmaker</a>",
          "description": "Electrocardiogram (ECG) acquisition requires an automated system and analysis\npipeline for understanding specific rhythm irregularities. Deep neural networks\nhave become a popular technique for tracing ECG signals, outperforming human\nexperts. Despite this, convolutional neural networks are susceptible to\nadversarial examples that can misclassify ECG signals and decrease the model's\nprecision. Moreover, they do not generalize well on the out-of-distribution\ndataset. The GAN architecture has been employed in recent works to synthesize\nadversarial ECG signals to increase existing training data. However, they use a\ndisjointed CNN-based classification architecture to detect arrhythmia. Till\nnow, no versatile architecture has been proposed that can detect adversarial\nexamples and classify arrhythmia simultaneously. To alleviate this, we propose\na novel Conditional Generative Adversarial Network to simultaneously generate\nECG signals for different categories and detect cardiac abnormalities.\nMoreover, the model is conditioned on class-specific ECG signals to synthesize\nrealistic adversarial examples. Consequently, we compare our architecture and\nshow how it outperforms other classification models in normal/abnormal ECG\nsignal detection by benchmarking real world and adversarial signals.",
          "link": "http://arxiv.org/abs/2107.07677",
          "publishedOn": "2021-07-19T00:49:06.794Z",
          "wordCount": 618,
          "title": "ECG-Adv-GAN: Detecting ECG Adversarial Examples with Conditional Generative Adversarial Networks. (arXiv:2107.07677v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferdous_R/0/1/0/all/0/1\">Refat E Ferdous</a>",
          "description": "During the COVID-19 pandemic, most of the human-to-human interactions have\nbeen stopped. To mitigate the spread of deadly coronavirus, many offices took\nthe initiative so that the employees can work from home. But, tracking the\nemployees and finding out if they are really performing what they were supposed\nto turn out to be a serious challenge for all the companies and organizations\nwho are facilitating \"Work From Home\". To deal with the challenge effectively,\nwe came up with a solution to track the employees with face recognition. We\nhave been testing this system experimentally for our office. To train the face\nrecognition module, we used FaceNet with KNN using the Labeled Faces in the\nWild (LFW) dataset and achieved 97.8% accuracy. We integrated the trained model\ninto our central system, where the employees log their time. In this paper, we\ndiscuss in brief the system we have been experimenting with and the pros and\ncons of the system.",
          "link": "http://arxiv.org/abs/2107.07576",
          "publishedOn": "2021-07-19T00:49:06.772Z",
          "wordCount": 666,
          "title": "Real-Time Face Recognition System for Remote Employee Tracking. (arXiv:2107.07576v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carmona_C/0/1/0/all/0/1\">Chris U. Carmona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubet_F/0/1/0/all/0/1\">Fran&#xe7;ois-Xavier Aubet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flunkert_V/0/1/0/all/0/1\">Valentin Flunkert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasthaus_J/0/1/0/all/0/1\">Jan Gasthaus</a>",
          "description": "We introduce Neural Contextual Anomaly Detection (NCAD), a framework for\nanomaly detection on time series that scales seamlessly from the unsupervised\nto supervised setting, and is applicable to both univariate and multivariate\ntime series. This is achieved by effectively combining recent developments in\nrepresentation learning for multivariate time series, with techniques for deep\nanomaly detection originally developed for computer vision that we tailor to\nthe time series setting. Our window-based approach facilitates learning the\nboundary between normal and anomalous classes by injecting generic synthetic\nanomalies into the available data. Moreover, our method can effectively take\nadvantage of all the available information, be it as domain knowledge, or as\ntraining labels in the semi-supervised setting. We demonstrate empirically on\nstandard benchmark datasets that our approach obtains a state-of-the-art\nperformance in these settings.",
          "link": "http://arxiv.org/abs/2107.07702",
          "publishedOn": "2021-07-19T00:49:06.765Z",
          "wordCount": 570,
          "title": "Neural Contextual Anomaly Detection for Time Series. (arXiv:2107.07702v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07642",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Lohani_S/0/1/0/all/0/1\">Sanjaya Lohani</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lukens_J/0/1/0/all/0/1\">Joseph M. Lukens</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Jones_D/0/1/0/all/0/1\">Daniel E. Jones</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Searles_T/0/1/0/all/0/1\">Thomas A. Searles</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Glasser_R/0/1/0/all/0/1\">Ryan T. Glasser</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kirby_B/0/1/0/all/0/1\">Brian T. Kirby</a>",
          "description": "We consider the properties of a specific distribution of mixed quantum states\nof arbitrary dimension that can be biased towards a specific mean purity. In\nparticular, we analyze mixtures of Haar-random pure states with\nDirichlet-distributed coefficients. We analytically derive the concentration\nparameters required to match the mean purity of the Bures and Hilbert--Schmidt\ndistributions in any dimension. Numerical simulations suggest that this value\nrecovers the Hilbert--Schmidt distribution exactly, offering an alternative and\nintuitive physical interpretation for ensembles of Hilbert--Schmidt-distributed\nrandom quantum states. We then demonstrate how substituting these\nDirichlet-weighted Haar mixtures in place of the Bures and Hilbert--Schmidt\ndistributions results in measurable performance advantages in\nmachine-learning-based quantum state tomography systems and Bayesian quantum\nstate reconstruction. Finally, we experimentally characterize the distribution\nof quantum states generated by both a cloud-accessed IBM quantum computer and\nan in-house source of polarization-entangled photons. In each case, our method\ncan more closely match the underlying distribution than either Bures or\nHilbert--Schmidt distributed states for various experimental conditions.",
          "link": "http://arxiv.org/abs/2107.07642",
          "publishedOn": "2021-07-19T00:49:06.759Z",
          "wordCount": 611,
          "title": "Improving application performance with biased distributions of quantum states. (arXiv:2107.07642v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Magee_L/0/1/0/all/0/1\">Liam Magee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghahremanlou_L/0/1/0/all/0/1\">Lida Ghahremanlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldatic_K/0/1/0/all/0/1\">Karen Soldatic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_S/0/1/0/all/0/1\">Shanthi Robertson</a>",
          "description": "To examine whether intersectional bias can be observed in language\ngeneration, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size\nfrom 124 million to ~2.7 billion parameters. We conduct an experiment combining\nup to three social categories - gender, religion and disability - into\nunconditional or zero-shot prompts used to generate sentences that are then\nanalysed for sentiment. Our results confirm earlier tests conducted with\nauto-regressive causal models, including the \\emph{GPT} family of models. We\nalso illustrate why bias may be resistant to techniques that target single\ncategories (e.g. gender, religion and race), as it can also manifest, in often\nsubtle ways, in texts prompted by concatenated social categories. To address\nthese difficulties, we suggest technical and community-based approaches need to\ncombine to acknowledge and address complex and intersectional language model\nbias.",
          "link": "http://arxiv.org/abs/2107.07691",
          "publishedOn": "2021-07-19T00:49:06.746Z",
          "wordCount": 568,
          "title": "Intersectional Bias in Causal Language Models. (arXiv:2107.07691v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07603",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Scurll_J/0/1/0/all/0/1\">Joshua M. Scurll</a>",
          "description": "Clustering and visualizing high-dimensional (HD) data are important tasks in\na variety of fields. For example, in bioinformatics, they are crucial for\nanalyses of single-cell data such as mass cytometry (CyTOF) data. Some of the\nmost effective algorithms for clustering HD data are based on representing the\ndata by nodes in a graph, with edges connecting neighbouring nodes according to\nsome measure of similarity or distance. However, users of graph-based\nalgorithms are typically faced with the critical but challenging task of\nchoosing the value of an input parameter that sets the size of neighbourhoods\nin the graph, e.g. the number of nearest neighbours to which to connect each\nnode or a threshold distance for connecting nodes. The burden on the user could\nbe alleviated by a measure of inter-node similarity that can have value 0 for\ndissimilar nodes without requiring any user-defined parameters or thresholds.\nThis would determine the neighbourhoods automatically while still yielding a\nsparse graph. To this end, I propose a new method called ASTRICS to measure\nsimilarity between clusters of HD data points based on local dimensionality\nreduction and triangulation of critical alpha shapes. I show that my ASTRICS\nsimilarity measure can facilitate both clustering and visualization of HD data\nby using it in Stage 2 of a three-stage pipeline: Stage 1 = perform an initial\nclustering of the data by any method; Stage 2 = let graph nodes represent\ninitial clusters instead of individual data points and use ASTRICS to\nautomatically define edges between nodes; Stage 3 = use the graph for further\nclustering and visualization. This trades the critical task of choosing a graph\nneighbourhood size for the easier task of essentially choosing a resolution at\nwhich to view the data. The graph and consequently downstream clustering and\nvisualization are then automatically adapted to the chosen resolution.",
          "link": "http://arxiv.org/abs/2107.07603",
          "publishedOn": "2021-07-19T00:49:06.719Z",
          "wordCount": 764,
          "title": "Measuring inter-cluster similarities with Alpha Shape TRIangulation in loCal Subspaces (ASTRICS) facilitates visualization and clustering of high-dimensional data. (arXiv:2107.07603v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Johnson_D/0/1/0/all/0/1\">Daniel D. Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_R/0/1/0/all/0/1\">Rianne van den Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarlow_D/0/1/0/all/0/1\">Daniel Tarlow</a>",
          "description": "Denoising diffusion probabilistic models (DDPMs) have shown impressive\nresults on sequence generation by iteratively corrupting each example and then\nlearning to map corrupted versions back to the original. However, previous work\nhas largely focused on in-place corruption, adding noise to each pixel or token\nindividually while keeping their locations the same. In this work, we consider\na broader class of corruption processes and denoising models over sequence data\nthat can insert and delete elements, while still being efficient to train and\nsample from. We demonstrate that these models outperform standard in-place\nmodels on an arithmetic sequence task, and that when trained on the text8\ndataset they can be used to fix spelling errors without any fine-tuning.",
          "link": "http://arxiv.org/abs/2107.07675",
          "publishedOn": "2021-07-19T00:49:06.696Z",
          "wordCount": 572,
          "title": "Beyond In-Place Corruption: Insertion and Deletion In Denoising Probabilistic Models. (arXiv:2107.07675v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yukun Jiang</a>",
          "description": "Active learning is a state-of-art machine learning approach to deal with an\nabundance of unlabeled data. In the field of Natural Language Processing,\ntypically it is costly and time-consuming to have all the data annotated. This\ninefficiency inspires out our application of active learning in text\nclassification. Traditional unsupervised k-means clustering is first modified\ninto a semi-supervised version in this research. Then, a novel attempt is\napplied to further extend the algorithm into active learning scenario with\nPenalized Min-Max-selection, so as to make limited queries that yield more\nstable initial centroids. This method utilizes both the interactive query\nresults from users and the underlying distance representation. After tested on\na Chinese news dataset, it shows a consistent increase in accuracy while\nlowering the cost in training.",
          "link": "http://arxiv.org/abs/2107.07682",
          "publishedOn": "2021-07-19T00:49:06.689Z",
          "wordCount": 600,
          "title": "The Application of Active Query K-Means in Text Classification. (arXiv:2107.07682v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alarab_I/0/1/0/all/0/1\">Ismail Alarab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakoonwit_S/0/1/0/all/0/1\">Simant Prakoonwit</a>",
          "description": "We propose a novel method to capture data points near decision boundary in\nneural network that are often referred to a specific type of uncertainty. In\nour approach, we sought to perform uncertainty estimation based on the idea of\nadversarial attack method. In this paper, uncertainty estimates are derived\nfrom the input perturbations, unlike previous studies that provide\nperturbations on the model's parameters as in Bayesian approach. We are able to\nproduce uncertainty with couple of perturbations on the inputs. Interestingly,\nwe apply the proposed method to datasets derived from blockchain. We compare\nthe performance of model uncertainty with the most recent uncertainty methods.\nWe show that the proposed method has revealed a significant outperformance over\nother methods and provided less risk to capture model uncertainty in machine\nlearning.",
          "link": "http://arxiv.org/abs/2107.07618",
          "publishedOn": "2021-07-19T00:49:06.684Z",
          "wordCount": 576,
          "title": "Adversarial Attack for Uncertainty Estimation: Identifying Critical Regions in Neural Networks. (arXiv:2107.07618v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ganassali_L/0/1/0/all/0/1\">Luca Ganassali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massoulie_L/0/1/0/all/0/1\">Laurent Massouli&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lelarge_M/0/1/0/all/0/1\">Marc Lelarge</a>",
          "description": "We consider alignment of sparse graphs, which consists in finding a mapping\nbetween the nodes of two graphs which preserves most of the edges. Our approach\nis to compare local structures in the two graphs, matching two nodes if their\nneighborhoods are 'close enough': for correlated Erd\\H{o}s-R\\'enyi random\ngraphs, this problem can be locally rephrased in terms of testing whether a\npair of branching trees is drawn from either a product distribution, or a\ncorrelated distribution. We design an optimal test for this problem which gives\nrise to a message-passing algorithm for graph alignment, which provably returns\nin polynomial time a positive fraction of correctly matched vertices, and a\nvanishing fraction of mismatches. With an average degree $\\lambda = O(1)$ in\nthe graphs, and a correlation parameter $s \\in [0,1]$, this result holds with\n$\\lambda s$ large enough, and $1-s$ small enough, completing the recent\nstate-of-the-art diagram. Tighter conditions for determining whether partial\ngraph alignment (or correlation detection in trees) is feasible in polynomial\ntime are given in terms of Kullback-Leibler divergences.",
          "link": "http://arxiv.org/abs/2107.07623",
          "publishedOn": "2021-07-19T00:49:06.669Z",
          "wordCount": 629,
          "title": "Correlation detection in trees for partial graph alignment. (arXiv:2107.07623v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kopanicakova_A/0/1/0/all/0/1\">Alena Kopani&#x10d;&#xe1;kov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_R/0/1/0/all/0/1\">Rolf Krause</a>",
          "description": "We propose a globally convergent multilevel training method for deep residual\nnetworks (ResNets). The devised method can be seen as a novel variant of the\nrecursive multilevel trust-region (RMTR) method, which operates in hybrid\n(stochastic-deterministic) settings by adaptively adjusting mini-batch sizes\nduring the training. The multilevel hierarchy and the transfer operators are\nconstructed by exploiting a dynamical system's viewpoint, which interprets\nforward propagation through the ResNet as a forward Euler discretization of an\ninitial value problem. In contrast to traditional training approaches, our\nnovel RMTR method also incorporates curvature information on all levels of the\nmultilevel hierarchy by means of the limited-memory SR1 method. The overall\nperformance and the convergence properties of our multilevel training method\nare numerically investigated using examples from the field of classification\nand regression.",
          "link": "http://arxiv.org/abs/2107.07572",
          "publishedOn": "2021-07-19T00:49:06.659Z",
          "wordCount": 564,
          "title": "Globally Convergent Multilevel Training of Deep Residual Networks. (arXiv:2107.07572v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitros_J/0/1/0/all/0/1\">John Mitros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>",
          "description": "Neural networks are often utilised in critical domain applications\n(e.g.~self-driving cars, financial markets, and aerospace engineering), even\nthough they exhibit overconfident predictions for ambiguous inputs. This\ndeficiency demonstrates a fundamental flaw indicating that neural networks\noften overfit on spurious correlations. To address this problem in this work we\npresent two novel objectives that improve the ability of a network to detect\nout-of-distribution samples and therefore avoid overconfident predictions for\nambiguous inputs. We empirically demonstrate that our methods outperform the\nbaseline and perform better than the majority of existing approaches, while\nperforming competitively those that they don't outperform. Additionally, we\nempirically demonstrate the robustness of our approach against common\ncorruptions and demonstrate the importance of regularisation and auxiliary\ninformation in out-of-distribution detection.",
          "link": "http://arxiv.org/abs/2107.07564",
          "publishedOn": "2021-07-19T00:49:06.653Z",
          "wordCount": 554,
          "title": "On the Importance of Regularisation & Auxiliary Information in OOD Detection. (arXiv:2107.07564v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1\">Ondrej Bohdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_R/0/1/0/all/0/1\">Rajesh Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyeji Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas Lane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy Hospedales</a>",
          "description": "Meta-learning provides a popular and effective family of methods for\ndata-efficient learning of new tasks. However, several important issues in\nmeta-learning have proven hard to study thus far. For example, performance\ndegrades in real-world settings where meta-learners must learn from a wide and\npotentially multi-modal distribution of training tasks; and when distribution\nshift exists between meta-train and meta-test task distributions. These issues\nare typically hard to study since the shape of task distributions, and shift\nbetween them are not straightforward to measure or control in standard\nbenchmarks. We propose the channel coding problem as a benchmark for\nmeta-learning. Channel coding is an important practical application where task\ndistributions naturally arise, and fast adaptation to new tasks is practically\nvaluable. We use this benchmark to study several aspects of meta-learning,\nincluding the impact of task distribution breadth and shift, which can be\ncontrolled in the coding problem. Going forward, this benchmark provides a tool\nfor the community to study the capabilities and limitations of meta-learning,\nand to drive research on practically robust and effective meta-learners.",
          "link": "http://arxiv.org/abs/2107.07579",
          "publishedOn": "2021-07-19T00:49:06.648Z",
          "wordCount": 616,
          "title": "A Channel Coding Benchmark for Meta-Learning. (arXiv:2107.07579v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colbert_I/0/1/0/all/0/1\">Ian Colbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutz_Delgado_K/0/1/0/all/0/1\">Ken Kreutz-Delgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srinjoy Das</a>",
          "description": "A novel energy-efficient edge computing paradigm is proposed for real-time\ndeep learning-based image upsampling applications. State-of-the-art deep\nlearning solutions for image upsampling are currently trained using either\nresize or sub-pixel convolution to learn kernels that generate high fidelity\nimages with minimal artifacts. However, performing inference with these learned\nconvolution kernels requires memory-intensive feature map transformations that\ndominate time and energy costs in real-time applications. To alleviate this\npressure on memory bandwidth, we confine the use of resize or sub-pixel\nconvolution to training in the cloud by transforming learned convolution\nkernels to deconvolution kernels before deploying them for inference as a\nfunctionally equivalent deconvolution. These kernel transformations, intended\nas a one-time cost when shifting from training to inference, enable a systems\ndesigner to use each algorithm in their optimal context by preserving the image\nfidelity learned when training in the cloud while minimizing data transfer\npenalties during inference at the edge. We also explore existing variants of\ndeconvolution inference algorithms and introduce a novel variant for\nconsideration. We analyze and compare the inference properties of\nconvolution-based upsampling algorithms using a quantitative model of incurred\ntime and energy costs and show that using deconvolution for inference at the\nedge improves both system latency and energy efficiency when compared to their\nsub-pixel or resize convolution counterparts.",
          "link": "http://arxiv.org/abs/2107.07647",
          "publishedOn": "2021-07-19T00:49:06.633Z",
          "wordCount": 664,
          "title": "An Energy-Efficient Edge Computing Paradigm for Convolution-based Image Upsampling. (arXiv:2107.07647v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhicheng Cai</a>",
          "description": "Gradient descent algorithm is the most utilized method when optimizing\nmachine learning issues. However, there exists many local minimums and saddle\npoints in the loss function, especially for high dimensional non-convex\noptimization problems like deep learning. Gradient descent may make loss\nfunction trapped in these local intervals which impedes further optimization,\nresulting in poor generalization ability. This paper proposes the SA-GD\nalgorithm which introduces the thought of simulated annealing algorithm to\ngradient descent. SA-GD method offers model the ability of mounting hills in\nprobability, tending to enable the model to jump out of these local areas and\nconverge to a optimal state finally. We took CNN models as an example and\ntested the basic CNN models on various benchmark datasets. Compared to the\nbaseline models with traditional gradient descent algorithm, models with SA-GD\nalgorithm possess better generalization ability without sacrificing the\nefficiency and stability of model convergence. In addition, SA-GD can be\nutilized as an effective ensemble learning approach which improves the final\nperformance significantly.",
          "link": "http://arxiv.org/abs/2107.07558",
          "publishedOn": "2021-07-19T00:49:06.611Z",
          "wordCount": 596,
          "title": "SA-GD: Improved Gradient Descent Learning Strategy with Simulated Annealing. (arXiv:2107.07558v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_K/0/1/0/all/0/1\">Kevin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kai-Zhan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bareinboim_E/0/1/0/all/0/1\">Elias Bareinboim</a>",
          "description": "One of the central elements of any causal inference is an object called\nstructural causal model (SCM), which represents a collection of mechanisms and\nexogenous sources of random variation of the system under investigation (Pearl,\n2000). An important property of many kinds of neural networks is universal\napproximability: the ability to approximate any function to arbitrary\nprecision. Given this property, one may be tempted to surmise that a collection\nof neural nets is capable of learning any SCM by training on data generated by\nthat SCM. In this paper, we show this is not the case by disentangling the\nnotions of expressivity and learnability. Specifically, we show that the causal\nhierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits\nof what can be learned from data, still holds for neural models. For instance,\nan arbitrarily complex and expressive neural net is unable to predict the\neffects of interventions given observational data alone. Given this result, we\nintroduce a special type of SCM called a neural causal model (NCM), and\nformalize a new type of inductive bias to encode structural constraints\nnecessary for performing causal inferences. Building on this new class of\nmodels, we focus on solving two canonical tasks found in the literature known\nas causal identification and estimation. Leveraging the neural toolbox, we\ndevelop an algorithm that is both sufficient and necessary to determine whether\na causal effect can be learned from data (i.e., causal identifiability); it\nthen estimates the effect whenever identifiability holds (causal estimation).\nSimulations corroborate the proposed approach.",
          "link": "http://arxiv.org/abs/2107.00793",
          "publishedOn": "2021-07-16T00:48:26.348Z",
          "wordCount": 725,
          "title": "The Causal-Neural Connection: Expressiveness, Learnability, and Inference. (arXiv:2107.00793v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1\">Tero Karras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1\">Miika Aittala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1\">Samuli Laine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1\">Erik H&#xe4;rk&#xf6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1\">Janne Hellsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1\">Jaakko Lehtinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1\">Timo Aila</a>",
          "description": "We observe that despite their hierarchical convolutional nature, the\nsynthesis process of typical generative adversarial networks depends on\nabsolute pixel coordinates in an unhealthy manner. This manifests itself as,\ne.g., detail appearing to be glued to image coordinates instead of the surfaces\nof depicted objects. We trace the root cause to careless signal processing that\ncauses aliasing in the generator network. Interpreting all signals in the\nnetwork as continuous, we derive generally applicable, small architectural\nchanges that guarantee that unwanted information cannot leak into the\nhierarchical synthesis process. The resulting networks match the FID of\nStyleGAN2 but differ dramatically in their internal representations, and they\nare fully equivariant to translation and rotation even at subpixel scales. Our\nresults pave the way for generative models better suited for video and\nanimation.",
          "link": "http://arxiv.org/abs/2106.12423",
          "publishedOn": "2021-07-16T00:48:26.334Z",
          "wordCount": 614,
          "title": "Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samavatian_M/0/1/0/all/0/1\">Mohammad Hossein Samavatian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Saikat Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barber_K/0/1/0/all/0/1\">Kristin Barber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_R/0/1/0/all/0/1\">Radu Teodorescu</a>",
          "description": "Deep Neural Networks (DNNs) are employed in an increasing number of\napplications, some of which are safety critical. Unfortunately, DNNs are known\nto be vulnerable to so-called adversarial attacks that manipulate inputs to\ncause incorrect results that can be beneficial to an attacker or damaging to\nthe victim. Multiple defenses have been proposed to increase the robustness of\nDNNs. In general, these defenses have high overhead, some require\nattack-specific re-training of the model or careful tuning to adapt to\ndifferent attacks.\n\nThis paper presents HASI, a hardware-accelerated defense that uses a process\nwe call stochastic inference to detect adversarial inputs. We show that by\ncarefully injecting noise into the model at inference time, we can\ndifferentiate adversarial inputs from benign ones. HASI uses the output\ndistribution characteristics of noisy inference compared to a non-noisy\nreference to detect adversarial inputs. We show an adversarial detection rate\nof 86% when applied to VGG16 and 93% when applied to ResNet50, which exceeds\nthe detection rate of the state of the art approaches, with a much lower\noverhead. We demonstrate two software/hardware-accelerated co-designs, which\nreduces the performance impact of stochastic inference to 1.58X-2X relative to\nthe unprotected baseline, compared to 15X-20X overhead for a software-only GPU\nimplementation.",
          "link": "http://arxiv.org/abs/2106.05825",
          "publishedOn": "2021-07-16T00:48:26.309Z",
          "wordCount": 677,
          "title": "HASI: Hardware-Accelerated Stochastic Inference, A Defense Against Adversarial Machine Learning Attacks. (arXiv:2106.05825v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Casado_F/0/1/0/all/0/1\">Fernando E. Casado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lema_D/0/1/0/all/0/1\">Dylan Lema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Criado_M/0/1/0/all/0/1\">Marcos F. Criado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iglesias_R/0/1/0/all/0/1\">Roberto Iglesias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Regueiro_C/0/1/0/all/0/1\">Carlos V. Regueiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barro_S/0/1/0/all/0/1\">Sen&#xe9;n Barro</a>",
          "description": "Smart devices, such as smartphones, wearables, robots, and others, can\ncollect vast amounts of data from their environment. This data is suitable for\ntraining machine learning models, which can significantly improve their\nbehavior, and therefore, the user experience. Federated learning is a young and\npopular framework that allows multiple distributed devices to train deep\nlearning models collaboratively while preserving data privacy. Nevertheless,\nthis approach may not be optimal for scenarios where data distribution is\nnon-identical among the participants or changes over time, causing what is\nknown as concept drift. Little research has yet been done in this field, but\nthis kind of situation is quite frequent in real life and poses new challenges\nto both continual and federated learning. Therefore, in this work, we present a\nnew method, called Concept-Drift-Aware Federated Averaging (CDA-FedAvg). Our\nproposal is an extension of the most popular federated algorithm, Federated\nAveraging (FedAvg), enhancing it for continual adaptation under concept drift.\nWe empirically demonstrate the weaknesses of regular FedAvg and prove that\nCDA-FedAvg outperforms it in this type of scenario.",
          "link": "http://arxiv.org/abs/2105.13309",
          "publishedOn": "2021-07-16T00:48:26.297Z",
          "wordCount": 643,
          "title": "Concept drift detection and adaptation for federated and continual learning. (arXiv:2105.13309v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cabannes_V/0/1/0/all/0/1\">Vivien Cabannes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>",
          "description": "Machine learning approached through supervised learning requires expensive\nannotation of data. This motivates weakly supervised learning, where data are\nannotated with incomplete yet discriminative information. In this paper, we\nfocus on partial labelling, an instance of weak supervision where, from a given\ninput, we are given a set of potential targets. We review a disambiguation\nprinciple to recover full supervision from weak supervision, and propose an\nempirical disambiguation algorithm. We prove exponential convergence rates of\nour algorithm under classical learnability assumptions, and we illustrate the\nusefulness of our method on practical examples.",
          "link": "http://arxiv.org/abs/2102.02789",
          "publishedOn": "2021-07-16T00:48:26.220Z",
          "wordCount": 592,
          "title": "Disambiguation of weak supervision with exponential convergence rates. (arXiv:2102.02789v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhuoran Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chen Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lei Lyu</a>",
          "description": "Source code can be parsed into the abstract syntax tree (AST) based on\ndefined syntax rules. However, in pre-training, little work has considered the\nincorporation of tree structure into the learning process. In this paper, we\npresent TreeBERT, a tree-based pre-trained model for improving programming\nlanguage-oriented generation tasks. To utilize tree structure, TreeBERT\nrepresents the AST corresponding to the code as a set of composition paths and\nintroduces node position embedding. The model is trained by tree masked\nlanguage modeling (TMLM) and node order prediction (NOP) with a hybrid\nobjective. TMLM uses a novel masking strategy designed according to the tree's\ncharacteristics to help the model understand the AST and infer the missing\nsemantics of the AST. With NOP, TreeBERT extracts the syntactical structure by\nlearning the order constraints of nodes in AST. We pre-trained TreeBERT on\ndatasets covering multiple programming languages. On code summarization and\ncode documentation tasks, TreeBERT outperforms other pre-trained models and\nstate-of-the-art models designed for these tasks. Furthermore, TreeBERT\nperforms well when transferred to the pre-trained unseen programming language.",
          "link": "http://arxiv.org/abs/2105.12485",
          "publishedOn": "2021-07-16T00:48:26.207Z",
          "wordCount": 640,
          "title": "TreeBERT: A Tree-Based Pre-Trained Model for Programming Language. (arXiv:2105.12485v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12684",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Overes_B/0/1/0/all/0/1\">Bart H.L. Overes</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Wel_M/0/1/0/all/0/1\">Michel van der Wel</a>",
          "description": "Sovereign credit ratings summarize the creditworthiness of countries. These\nratings have a large influence on the economy and the yields at which\ngovernments can issue new debt. This paper investigates the use of a Multilayer\nPerceptron (MLP), Classification and Regression Trees (CART), Support Vector\nMachines (SVM), Na\\\"ive Bayes (NB), and an Ordered Logit (OL) model for the\nprediction of sovereign credit ratings. We show that MLP is best suited for\npredicting sovereign credit ratings, with a random cross-validated accuracy of\n68%, followed by CART (59%), SVM (41%), NB (38%), and OL (33%). Investigation\nof the determining factors shows that there is some heterogeneity in the\nimportant variables across the models. However, the two models with the highest\nout-of-sample predictive accuracy, MLP and CART, show a lot of similarities in\nthe influential variables, with regulatory quality, and GDP per capita as\ncommon important variables. Consistent with economic theory, a higher\nregulatory quality and/or GDP per capita are associated with a higher credit\nrating.",
          "link": "http://arxiv.org/abs/2101.12684",
          "publishedOn": "2021-07-16T00:48:26.183Z",
          "wordCount": 627,
          "title": "Modelling Sovereign Credit Ratings: Evaluating the Accuracy and Driving Factors using Machine Learning Techniques. (arXiv:2101.12684v2 [q-fin.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hande Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1\">Xin Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guli Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Keping Yang</a>",
          "description": "Recommender systems rely on user behavior data like ratings and clicks to\nbuild personalization model. However, the collected data is observational\nrather than experimental, causing various biases in the data which\nsignificantly affect the learned model. Most existing work for recommendation\ndebiasing, such as the inverse propensity scoring and imputation approaches,\nfocuses on one or two specific biases, lacking the universal capacity that can\naccount for mixed or even unknown biases in the data. Towards this research\ngap, we first analyze the origin of biases from the perspective of \\textit{risk\ndiscrepancy} that represents the difference between the expectation empirical\nrisk and the true risk. Remarkably, we derive a general learning framework that\nwell summarizes most existing debiasing strategies by specifying some\nparameters of the general framework. This provides a valuable opportunity to\ndevelop a universal solution for debiasing, e.g., by learning the debiasing\nparameters from data. However, the training data lacks important signal of how\nthe data is biased and what the unbiased data looks like. To move this idea\nforward, we propose \\textit{AotoDebias} that leverages another (small) set of\nuniform data to optimize the debiasing parameters by solving the bi-level\noptimization problem with meta-learning. Through theoretical analyses, we\nderive the generalization bound for AutoDebias and prove its ability to acquire\nthe appropriate debiasing strategy. Extensive experiments on two real datasets\nand a simulated dataset demonstrated effectiveness of AutoDebias. The code is\navailable at \\url{https://github.com/DongHande/AutoDebias}.",
          "link": "http://arxiv.org/abs/2105.04170",
          "publishedOn": "2021-07-16T00:48:26.178Z",
          "wordCount": 722,
          "title": "AutoDebias: Learning to Debias for Recommendation. (arXiv:2105.04170v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zehao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiayi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>",
          "description": "Domain generalization is challenging due to the domain shift and the\nuncertainty caused by the inaccessibility of target domain data. In this paper,\nwe address both challenges with a probabilistic framework based on variational\nBayesian inference, by incorporating uncertainty into neural network weights.\nWe couple domain invariance in a probabilistic formula with the variational\nBayesian inference. This enables us to explore domain-invariant learning in a\nprincipled way. Specifically, we derive domain-invariant representations and\nclassifiers, which are jointly established in a two-layer Bayesian neural\nnetwork. We empirically demonstrate the effectiveness of our proposal on four\nwidely used cross-domain visual recognition benchmarks. Ablation studies\nvalidate the synergistic benefits of our Bayesian treatment when jointly\nlearning domain-invariant representations and classifiers for domain\ngeneralization. Further, our method consistently delivers state-of-the-art mean\naccuracy on all benchmarks.",
          "link": "http://arxiv.org/abs/2105.04030",
          "publishedOn": "2021-07-16T00:48:26.165Z",
          "wordCount": 609,
          "title": "A Bit More Bayesian: Domain-Invariant Learning with Uncertainty. (arXiv:2105.04030v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02579",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Llorente_F/0/1/0/all/0/1\">F. Llorente</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Curbelo_E/0/1/0/all/0/1\">E. Curbelo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Martino_L/0/1/0/all/0/1\">L. Martino</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Elvira_V/0/1/0/all/0/1\">V. Elvira</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Delgado_D/0/1/0/all/0/1\">D. Delgado</a>",
          "description": "Monte Carlo methods are the standard procedure for estimating complicated\nintegrals of multidimensional Bayesian posterior distributions. In this work,\nwe focus on LAIS, a class of adaptive importance samplers where Markov chain\nMonte Carlo (MCMC) algorithms are employed to drive an underlying multiple\nimportance sampling (IS) scheme. Its power lies in the simplicity of the\nlayered framework: the upper layer locates proposal densities by means of MCMC\nalgorithms; while the lower layer handles the multiple IS scheme, in order to\ncompute the final estimators. The modular nature of LAIS allows for different\npossible choices in the upper and lower layers, that will have different\nperformance and computational costs. In this work, we propose different\nenhancements in order to increase the efficiency and reduce the computational\ncost, of both upper and lower layers. The different variants are essential if\nwe aim to address computational challenges arising in real-world applications,\nsuch as highly concentrated posterior distributions (due to large amounts of\ndata, etc.). Hamiltonian-driven importance samplers are presented and tested.\nFurthermore, we introduce different strategies for designing cheaper schemes,\nfor instance, recycling samples generated in the upper layer and using them in\nthe final estimators in the lower layer. Numerical experiments show the\nbenefits of the proposed schemes as compared to the vanilla version of LAIS and\nother benchmark methods.",
          "link": "http://arxiv.org/abs/2105.02579",
          "publishedOn": "2021-07-16T00:48:26.151Z",
          "wordCount": 669,
          "title": "MCMC-driven importance samplers. (arXiv:2105.02579v3 [stat.CO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>",
          "description": "We present CSWin Transformer, an efficient and effective Transformer-based\nbackbone for general-purpose vision tasks. A challenging issue in Transformer\ndesign is that global self-attention is very expensive to compute whereas local\nself-attention often limits the field of interactions of each token. To address\nthis issue, we develop the Cross-Shaped Window self-attention mechanism for\ncomputing self-attention in the horizontal and vertical stripes in parallel\nthat form a cross-shaped window, with each stripe obtained by splitting the\ninput feature into stripes of equal width. We provide a detailed mathematical\nanalysis of the effect of the stripe width and vary the stripe width for\ndifferent layers of the Transformer network which achieves strong modeling\ncapability while limiting the computation cost. We also introduce\nLocally-enhanced Positional Encoding (LePE), which handles the local positional\ninformation better than existing encoding schemes. LePE naturally supports\narbitrary input resolutions, and is thus especially effective and friendly for\ndownstream tasks. Incorporated with these designs and a hierarchical structure,\nCSWin Transformer demonstrates competitive performance on common vision tasks.\nSpecifically, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra\ntraining data or label, 53.9 box AP and 46.4 mask AP on the COCO detection\ntask, and 51.7 mIOU on the ADE20K semantic segmentation task, surpassing\nprevious state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and\n+2.0 respectively under the similar FLOPs setting. By further pretraining on\nthe larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K\nand state-of-the-art segmentation performance on ADE20K with 55.7 mIoU. The\ncode and models will be available at\nhttps://github.com/microsoft/CSWin-Transformer.",
          "link": "http://arxiv.org/abs/2107.00652",
          "publishedOn": "2021-07-16T00:48:26.119Z",
          "wordCount": 744,
          "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.08177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lara_J/0/1/0/all/0/1\">Juan S. Lara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1\">Fabio A. Gonz&#xe1;lez</a>",
          "description": "The dissimilarity mixture autoencoder (DMAE) is a neural network model for\nfeature-based clustering that incorporates a flexible dissimilarity function\nand can be integrated into any kind of deep learning architecture. It\ninternally represents a dissimilarity mixture model (DMM) that extends\nclassical methods like K-Means, Gaussian mixture models, or Bregman clustering\nto any convex and differentiable dissimilarity function through the\nreinterpretation of probabilities as neural network representations. DMAE can\nbe integrated with deep learning architectures into end-to-end models, allowing\nthe simultaneous estimation of the clustering and neural network's parameters.\nExperimental evaluation was performed on image and text clustering benchmark\ndatasets showing that DMAE is competitive in terms of unsupervised\nclassification accuracy and normalized mutual information. The source code with\nthe implementation of DMAE is publicly available at:\nhttps://github.com/juselara1/dmae",
          "link": "http://arxiv.org/abs/2006.08177",
          "publishedOn": "2021-07-16T00:48:26.114Z",
          "wordCount": 622,
          "title": "Dissimilarity Mixture Autoencoder for Deep Clustering. (arXiv:2006.08177v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10620",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Perdomo_J/0/1/0/all/0/1\">Juan C. Perdomo</a>, <a href=\"http://arxiv.org/find/math/1/au:+Simchowitz_M/0/1/0/all/0/1\">Max Simchowitz</a>, <a href=\"http://arxiv.org/find/math/1/au:+Agarwal_A/0/1/0/all/0/1\">Alekh Agarwal</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bartlett_P/0/1/0/all/0/1\">Peter Bartlett</a>",
          "description": "We study the problem of adaptive control of the linear quadratic regulator\nfor systems in very high, or even infinite dimension. We demonstrate that while\nsublinear regret requires finite dimensional inputs, the ambient state\ndimension of the system need not be bounded in order to perform online control.\nWe provide the first regret bounds for LQR which hold for infinite dimensional\nsystems, replacing dependence on ambient dimension with more natural notions of\nproblem complexity. Our guarantees arise from a novel perturbation bound for\ncertainty equivalence which scales with the prediction error in estimating the\nsystem parameters, without requiring consistent parameter recovery in more\nstringent measures like the operator norm. When specialized to finite\ndimensional settings, our bounds recover near optimal dimension and time\nhorizon dependence.",
          "link": "http://arxiv.org/abs/2103.10620",
          "publishedOn": "2021-07-16T00:48:26.106Z",
          "wordCount": 589,
          "title": "Towards a Dimension-Free Understanding of Adaptive Linear Control. (arXiv:2103.10620v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1\">Murray Shanahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplanis_C/0/1/0/all/0/1\">Christos Kaplanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitrovic_J/0/1/0/all/0/1\">Jovana Mitrovi&#x107;</a>",
          "description": "We present an architecture that is effective for continual learning in an\nespecially demanding setting, where task boundaries do not exist or are\nunknown. Our architecture comprises an encoder, pre-trained on a separate\ndataset, and an ensemble of simple one-layer classifiers. Two main innovations\nare required to make this combination work. First, the provision of suitably\ngeneric pre-trained encoders has been made possible thanks to recent progress\nin self-supervised training methods. Second, pairing each classifier in the\nensemble with a key, where the key-space is identical to the latent space of\nthe encoder, allows them to be used collectively, yet selectively, via\nk-nearest neighbour lookup. We show that models trained with the\nencoders-and-ensembles architecture are state-of-the-art for the task-free\nsetting on standard image classification continual learning benchmarks, and\nimprove on prior state-of-the-art by a large margin in the most challenging\ncases. We also show that the architecture learns well in a fully incremental\nsetting, where one class is learned at a time, and we demonstrate its\neffectiveness in this setting with up to 100 classes. Finally, we show that the\narchitecture works in a task-free continual learning context where the data\ndistribution changes gradually, and existing approaches requiring knowledge of\ntask boundaries cannot be applied.",
          "link": "http://arxiv.org/abs/2105.13327",
          "publishedOn": "2021-07-16T00:48:26.099Z",
          "wordCount": 658,
          "title": "Encoders and Ensembles for Task-Free Continual Learning. (arXiv:2105.13327v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esfandiari_H/0/1/0/all/0/1\">Hossein Esfandiari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1\">Vahab Mirrokni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shyam Narayanan</a>",
          "description": "Recently, due to an increasing interest for transparency in artificial\nintelligence, several methods of explainable machine learning have been\ndeveloped with the simultaneous goal of accuracy and interpretability by\nhumans. In this paper, we study a recent framework of explainable clustering\nfirst suggested by Dasgupta et al.~\\cite{dasgupta2020explainable}.\nSpecifically, we focus on the $k$-means and $k$-medians problems and provide\nnearly tight upper and lower bounds.\n\nFirst, we provide an $O(\\log k \\log \\log k)$-approximation algorithm for\nexplainable $k$-medians, improving on the best known algorithm of\n$O(k)$~\\cite{dasgupta2020explainable} and nearly matching the known\n$\\Omega(\\log k)$ lower bound~\\cite{dasgupta2020explainable}. In addition, in\nlow-dimensional spaces $d \\ll \\log k$, we show that our algorithm also provides\nan $O(d \\log^2 d)$-approximate solution for explainable $k$-medians. This\nimproves over the best known bound of $O(d \\log k)$ for low\ndimensions~\\cite{laber2021explainable}, and is a constant for constant\ndimensional spaces. To complement this, we show a nearly matching $\\Omega(d)$\nlower bound. Next, we study the $k$-means problem in this context and provide\nan $O(k \\log k)$-approximation algorithm for explainable $k$-means, improving\nover the $O(k^2)$ bound of Dasgupta et al. and the $O(d k \\log k)$ bound of\n\\cite{laber2021explainable}. To complement this we provide an almost tight\n$\\Omega(k)$ lower bound, improving over the $\\Omega(\\log k)$ lower bound of\nDasgupta et al. Given an approximate solution to the classic $k$-means and\n$k$-medians, our algorithm for $k$-medians runs in time $O(kd \\log^2 k )$ and\nour algorithm for $k$-means runs in time $ O(k^2 d)$.",
          "link": "http://arxiv.org/abs/2107.00774",
          "publishedOn": "2021-07-16T00:48:26.079Z",
          "wordCount": 720,
          "title": "Almost Tight Approximation Algorithms for Explainable Clustering. (arXiv:2107.00774v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00760",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Cabannes_V/0/1/0/all/0/1\">Vivien Cabannes</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>",
          "description": "Discrete supervised learning problems such as classification are often\ntackled by introducing a continuous surrogate problem akin to regression.\nBounding the original error, between estimate and solution, by the surrogate\nerror endows discrete problems with convergence rates already shown for\ncontinuous instances. Yet, current approaches do not leverage the fact that\ndiscrete problems are essentially predicting a discrete output when continuous\nproblems are predicting a continuous value. In this paper, we tackle this issue\nfor general structured prediction problems, opening the way to \"super fast\"\nrates, that is, convergence rates for the excess risk faster than $n^{-1}$,\nwhere $n$ is the number of observations, with even exponential rates with the\nstrongest assumptions. We first illustrate it for predictors based on nearest\nneighbors, generalizing rates known for binary classification to any discrete\nproblem within the framework of structured prediction. We then consider kernel\nridge regression where we improve known rates in $n^{-1/4}$ to arbitrarily fast\nrates, depending on a parameter characterizing the hardness of the problem,\nthus allowing, under smoothness assumptions, to bypass the curse of\ndimensionality.",
          "link": "http://arxiv.org/abs/2102.00760",
          "publishedOn": "2021-07-16T00:48:26.072Z",
          "wordCount": 668,
          "title": "Fast rates in structured prediction. (arXiv:2102.00760v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.15847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Faruqui_S/0/1/0/all/0/1\">Syed Hasib Akhter Faruqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alaeddini_A/0/1/0/all/0/1\">Adel Alaeddini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaramillo_C/0/1/0/all/0/1\">Carlos A. Jaramillo</a>",
          "description": "Bayesian networks are powerful statistical models to study the probabilistic\nrelationships among set random variables with major applications in disease\nmodeling and prediction. Here, we propose a continuous time Bayesian network\nwith conditional dependencies, represented as Poisson regression, to model the\nimpact of exogenous variables on the conditional dependencies of the network.\nWe also propose an adaptive regularization method with an intuitive early\nstopping feature based on density based clustering for efficient learning of\nthe structure and parameters of the proposed network. Using a dataset of\npatients with multiple chronic conditions extracted from electronic health\nrecords of the Department of Veterans Affairs we compare the performance of the\nproposed approach with some of the existing methods in the literature for both\nshort-term (one-year ahead) and long-term (multi-year ahead) predictions. The\nproposed approach provides a sparse intuitive representation of the complex\nfunctional relationships between multiple chronic conditions. It also provides\nthe capability of analyzing multiple disease trajectories over time given any\ncombination of prior conditions.",
          "link": "http://arxiv.org/abs/2007.15847",
          "publishedOn": "2021-07-16T00:48:26.063Z",
          "wordCount": 671,
          "title": "A Functional Model for Structure Learning and Parameter Estimation in Continuous Time Bayesian Network: An Application in Identifying Patterns of Multiple Chronic Conditions. (arXiv:2007.15847v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummins_C/0/1/0/all/0/1\">Chris Cummins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Brandon Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steiner_B/0/1/0/all/0/1\">Benoit Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linnan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>",
          "description": "Path planning, the problem of efficiently discovering high-reward\ntrajectories, often requires optimizing a high-dimensional and multimodal\nreward function. Popular approaches like CEM and CMA-ES greedily focus on\npromising regions of the search space and may get trapped in local maxima. DOO\nand VOOT balance exploration and exploitation, but use space partitioning\nstrategies independent of the reward function to be optimized. Recently, LaMCTS\nempirically learns to partition the search space in a reward-sensitive manner\nfor black-box optimization. In this paper, we develop a novel formal regret\nanalysis for when and why such an adaptive region partitioning scheme works. We\nalso propose a new path planning method PlaLaM which improves the function\nvalue estimation within each sub-region, and uses a latent representation of\nthe search space. Empirically, PlaLaM outperforms existing path planning\nmethods in 2D navigation tasks, especially in the presence of\ndifficult-to-escape local optima, and shows benefits when plugged into\nmodel-based RL with planning components such as PETS. These gains transfer to\nhighly multimodal real-world tasks, where we outperform strong baselines in\ncompiler phase ordering by up to 245% and in molecular design by up to 0.4 on\nproperties on a 0-1 scale. Code is available at\nhttps://github.com/yangkevin2/plalam.",
          "link": "http://arxiv.org/abs/2106.10544",
          "publishedOn": "2021-07-16T00:48:26.048Z",
          "wordCount": 674,
          "title": "Learning Space Partitions for Path Planning. (arXiv:2106.10544v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1\">Efthymios Tzinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhepei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xilin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1\">Paris Smaragdis</a>",
          "description": "Recent progress in audio source separation lead by deep learning has enabled\nmany neural network models to provide robust solutions to this fundamental\nestimation problem. In this study, we provide a family of efficient neural\nnetwork architectures for general purpose audio source separation while\nfocusing on multiple computational aspects that hinder the application of\nneural networks in real-world scenarios. The backbone structure of this\nconvolutional network is the SUccessive DOwnsampling and Resampling of\nMulti-Resolution Features (SuDoRM-RF) as well as their aggregation which is\nperformed through simple one-dimensional convolutions. This mechanism enables\nour models to obtain high fidelity signal separation in a wide variety of\nsettings where variable number of sources are present and with limited\ncomputational resources (e.g. floating point operations, memory footprint,\nnumber of parameters and latency). Our experiments show that SuDoRM-RF models\nperform comparably and even surpass several state-of-the-art benchmarks with\nsignificantly higher computational resource requirements. The causal variation\nof SuDoRM-RF is able to obtain competitive performance in real-time speech\nseparation of around 10dB scale-invariant signal-to-distortion ratio\nimprovement (SI-SDRi) while remaining up to 20 times faster than real-time on a\nlaptop device.",
          "link": "http://arxiv.org/abs/2103.02644",
          "publishedOn": "2021-07-16T00:48:26.042Z",
          "wordCount": 674,
          "title": "Compute and memory efficient universal sound source separation. (arXiv:2103.02644v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yingjun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Huan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1\">Qiang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "In this paper, we introduce variational semantic memory into meta-learning to\nacquire long-term knowledge for few-shot learning. The variational semantic\nmemory accrues and stores semantic information for the probabilistic inference\nof class prototypes in a hierarchical Bayesian framework. The semantic memory\nis grown from scratch and gradually consolidated by absorbing information from\ntasks it experiences. By doing so, it is able to accumulate long-term, general\nknowledge that enables it to learn new concepts of objects. We formulate memory\nrecall as the variational inference of a latent memory variable from addressed\ncontents, which offers a principled way to adapt the knowledge to individual\ntasks. Our variational semantic memory, as a new long-term memory module,\nconfers principled recall and update mechanisms that enable semantic\ninformation to be efficiently accrued and adapted for few-shot learning.\nExperiments demonstrate that the probabilistic modelling of prototypes achieves\na more informative representation of object classes compared to deterministic\nvectors. The consistent new state-of-the-art performance on four benchmarks\nshows the benefit of variational semantic memory in boosting few-shot\nrecognition.",
          "link": "http://arxiv.org/abs/2010.10341",
          "publishedOn": "2021-07-16T00:48:26.030Z",
          "wordCount": 654,
          "title": "Learning to Learn Variational Semantic Memory. (arXiv:2010.10341v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1\">Daniel Wontae Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Younghoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chan Y. Park</a>",
          "description": "In this paper, we devise a distributional framework on actor-critic as a\nsolution to distributional instability, action type restriction, and conflation\nbetween samples and statistics. We propose a new method that minimizes the\nCram\\'er distance with the multi-step Bellman target distribution generated\nfrom a novel Sample-Replacement algorithm denoted SR($\\lambda$), which learns\nthe correct value distribution under multiple Bellman operations.\nParameterizing a value distribution with Gaussian Mixture Model further\nimproves the efficiency and the performance of the method, which we name GMAC.\nWe empirically show that GMAC captures the correct representation of value\ndistributions and improves the performance of a conventional actor-critic\nmethod with low computational cost, in both discrete and continuous action\nspaces using Arcade Learning Environment (ALE) and PyBullet environment.",
          "link": "http://arxiv.org/abs/2105.11366",
          "publishedOn": "2021-07-16T00:48:26.025Z",
          "wordCount": 588,
          "title": "GMAC: A Distributional Perspective on Actor-Critic Framework. (arXiv:2105.11366v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Makarychev_Y/0/1/0/all/0/1\">Yury Makarychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakilian_A/0/1/0/all/0/1\">Ali Vakilian</a>",
          "description": "We present an $(e^{O(p)} \\frac{\\log \\ell}{\\log\\log\\ell})$-approximation\nalgorithm for socially fair clustering with the $\\ell_p$-objective. In this\nproblem, we are given a set of points in a metric space. Each point belongs to\none (or several) of $\\ell$ groups. The goal is to find a $k$-medians,\n$k$-means, or, more generally, $\\ell_p$-clustering that is simultaneously good\nfor all of the groups. More precisely, we need to find a set of $k$ centers $C$\nso as to minimize the maximum over all groups $j$ of $\\sum_{u \\text{ in group\n}j} d(u,C)^p$. The socially fair clustering problem was independently proposed\nby Ghadiri, Samadi, and Vempala [2021] and Abbasi, Bhaskara, and\nVenkatasubramanian [2021]. Our algorithm improves and generalizes their\n$O(\\ell)$-approximation algorithms for the problem. The natural LP relaxation\nfor the problem has an integrality gap of $\\Omega(\\ell)$. In order to obtain\nour result, we introduce a strengthened LP relaxation and show that it has an\nintegrality gap of $\\Theta(\\frac{\\log \\ell}{\\log\\log\\ell})$ for a fixed $p$.\nAdditionally, we present a bicriteria approximation algorithm, which\ngeneralizes the bicriteria approximation of Abbasi et al. [2021].",
          "link": "http://arxiv.org/abs/2103.02512",
          "publishedOn": "2021-07-16T00:48:26.007Z",
          "wordCount": 642,
          "title": "Approximation Algorithms for Socially Fair Clustering. (arXiv:2103.02512v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1\">Anastasios N. Angelopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1\">Stephen Bates</a>",
          "description": "Black-box machine learning learning methods are now routinely used in\nhigh-risk settings, like medical diagnostics, which demand uncertainty\nquantification to avoid consequential model failures. Distribution-free\nuncertainty quantification (distribution-free UQ) is a user-friendly paradigm\nfor creating statistically rigorous confidence intervals/sets for such\npredictions. Critically, the intervals/sets are valid without distributional\nassumptions or model assumptions, with explicit guarantees with finitely many\ndatapoints. Moreover, they adapt to the difficulty of the input; when the input\nexample is difficult, the uncertainty intervals/sets are large, signaling that\nthe model might be wrong. Without much work, one can use distribution-free\nmethods on any underlying algorithm, such as a neural network, to produce\nconfidence sets guaranteed to contain the ground truth with a user-specified\nprobability, such as 90%. Indeed, the methods are easy-to-understand and\ngeneral, applying to many modern prediction problems arising in the fields of\ncomputer vision, natural language processing, deep reinforcement learning, and\nso on. This hands-on introduction is aimed at a reader interested in the\npractical implementation of distribution-free UQ, including conformal\nprediction and related methods, who is not necessarily a statistician. We will\ninclude many explanatory illustrations, examples, and code samples in Python,\nwith PyTorch syntax. The goal is to provide the reader a working understanding\nof distribution-free UQ, allowing them to put confidence intervals on their\nalgorithms, with one self-contained document.",
          "link": "http://arxiv.org/abs/2107.07511",
          "publishedOn": "2021-07-16T00:48:26.000Z",
          "wordCount": 676,
          "title": "A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification. (arXiv:2107.07511v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tino_P/0/1/0/all/0/1\">Peter Ti&#x148;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ale&#x161; Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>",
          "description": "Along with the great success of deep neural networks, there is also growing\nconcern about their black-box nature. The interpretability issue affects\npeople's trust on deep learning systems. It is also related to many ethical\nproblems, e.g., algorithmic discrimination. Moreover, interpretability is a\ndesired property for deep networks to become powerful tools in other research\nfields, e.g., drug discovery and genomics. In this survey, we conduct a\ncomprehensive review of the neural network interpretability research. We first\nclarify the definition of interpretability as it has been used in many\ndifferent contexts. Then we elaborate on the importance of interpretability and\npropose a novel taxonomy organized along three dimensions: type of engagement\n(passive vs. active interpretation approaches), the type of explanation, and\nthe focus (from local to global interpretability). This taxonomy provides a\nmeaningful 3D view of distribution of papers from the relevant literature as\ntwo of the dimensions are not simply categorical but allow ordinal\nsubcategories. Finally, we summarize the existing interpretability evaluation\nmethods and suggest possible research directions inspired by our new taxonomy.",
          "link": "http://arxiv.org/abs/2012.14261",
          "publishedOn": "2021-07-16T00:48:25.993Z",
          "wordCount": 650,
          "title": "A Survey on Neural Network Interpretability. (arXiv:2012.14261v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07506",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Derek_K/0/1/0/all/0/1\">Kenneth Derek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>",
          "description": "In the natural world, life has found innumerable ways to survive and often\nthrive. Between and even within species, each individual is in some manner\nunique, and this diversity lends adaptability and robustness to life. In this\nwork, we aim to learn a space of diverse and high-reward policies on any given\nenvironment. To this end, we introduce a generative model of policies, which\nmaps a low-dimensional latent space to an agent policy space. Our method\nenables learning an entire population of agent policies, without requiring the\nuse of separate policy parameters. Just as real world populations can adapt and\nevolve via natural selection, our method is able to adapt to changes in our\nenvironment solely by selecting for policies in latent space. We test our\ngenerative model's capabilities in a variety of environments, including an\nopen-ended grid-world and a two-player soccer environment. Code,\nvisualizations, and additional experiments can be found at\nhttps://kennyderek.github.io/adap/.",
          "link": "http://arxiv.org/abs/2107.07506",
          "publishedOn": "2021-07-16T00:48:25.958Z",
          "wordCount": 594,
          "title": "Adaptable Agent Populations via a Generative Model of Policies. (arXiv:2107.07506v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.02161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ke_C/0/1/0/all/0/1\">Chuyang Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honorio_J/0/1/0/all/0/1\">Jean Honorio</a>",
          "description": "In this paper we propose an algorithm for exact partitioning of high-order\nmodels. We define a general class of $m$-degree Homogeneous Polynomial Models,\nwhich subsumes several examples motivated from prior literature. Exact\npartitioning can be formulated as a tensor optimization problem. We relax this\nhigh-order combinatorial problem to a convex conic form problem. To this end,\nwe carefully define the Carath\\'eodory symmetric tensor cone, and show its\nconvexity, and the convexity of its dual cone. This allows us to construct a\nprimal-dual certificate to show that the solution of the convex relaxation is\ncorrect (equal to the unobserved true group assignment) and to analyze the\nstatistical upper bound of exact partitioning.",
          "link": "http://arxiv.org/abs/1911.02161",
          "publishedOn": "2021-07-16T00:48:25.941Z",
          "wordCount": 577,
          "title": "Exact Partitioning of High-order Models with a Novel Convex Tensor Cone Relaxation. (arXiv:1911.02161v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1\">Bo Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1\">Tianyu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1\">Biyi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhihui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Luming Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yixin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Sheng Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1\">Xiao Tu</a>",
          "description": "Structured pruning is a commonly used technique in deploying deep neural\nnetworks (DNNs) onto resource-constrained devices. However, the existing\npruning methods are usually heuristic, task-specified, and require an extra\nfine-tuning procedure. To overcome these limitations, we propose a framework\nthat compresses DNNs into slimmer architectures with competitive performances\nand significant FLOPs reductions by Only-Train-Once (OTO). OTO contains two\nkeys: (i) we partition the parameters of DNNs into zero-invariant groups,\nenabling us to prune zero groups without affecting the output; and (ii) to\npromote zero groups, we then formulate a structured-sparsity optimization\nproblem and propose a novel optimization algorithm, Half-Space Stochastic\nProjected Gradient (HSPG), to solve it, which outperforms the standard proximal\nmethods on group sparsity exploration and maintains comparable convergence. To\ndemonstrate the effectiveness of OTO, we train and compress full models\nsimultaneously from scratch without fine-tuning for inference speedup and\nparameter reduction, and achieve state-of-the-art results on VGG16 for CIFAR10,\nResNet50 for CIFAR10/ImageNet and Bert for SQuAD.",
          "link": "http://arxiv.org/abs/2107.07467",
          "publishedOn": "2021-07-16T00:48:25.932Z",
          "wordCount": 608,
          "title": "Only Train Once: A One-Shot Neural Network Training And Pruning Framework. (arXiv:2107.07467v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.12368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1\">Theodoros Tsiligkaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1\">Jay Roberts</a>",
          "description": "Deep neural networks are easily fooled by small perturbations known as\nadversarial attacks. Adversarial Training (AT) is a technique that\napproximately solves a robust optimization problem to minimize the worst-case\nloss and is widely regarded as the most effective defense against such attacks.\nWe develop a theoretical framework for adversarial training with FW\noptimization (FW-AT) that reveals a geometric connection between the loss\nlandscape and the distortion of $\\ell_\\infty$ FW attacks (the attack's $\\ell_2$\nnorm). Specifically, we show that high distortion of FW attacks is equivalent\nto low variation along the attack path. It is then experimentally demonstrated\non various deep neural network architectures that $\\ell_\\infty$ attacks against\nrobust models achieve near maximal $\\ell_2$ distortion. This mathematical\ntransparency differentiates FW from the more popular Projected Gradient Descent\n(PGD) optimization. To demonstrate the utility of our theoretical framework we\ndevelop FW-Adapt, a novel adversarial training algorithm which uses simple\ndistortion measure to adaptively change number of attack steps during training.\nFW-Adapt provides strong robustness at lower training times in comparison to\nPGD-AT for a variety of white-box and black-box attacks.",
          "link": "http://arxiv.org/abs/2012.12368",
          "publishedOn": "2021-07-16T00:48:25.925Z",
          "wordCount": 649,
          "title": "Understanding Frank-Wolfe Adversarial Training. (arXiv:2012.12368v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.00394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H Ang Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>",
          "description": "In view of the difficulty in reconstructing object details in point cloud\ncompletion, we propose a shape prior learning method for object completion. The\nshape priors include geometric information in both complete and the partial\npoint clouds. We design a feature alignment strategy to learn the shape prior\nfrom complete points, and a coarse to fine strategy to incorporate partial\nprior in the fine stage. To learn the complete objects prior, we first train a\npoint cloud auto-encoder to extract the latent embeddings from complete points.\nThen we learn a mapping to transfer the point features from partial points to\nthat of the complete points by optimizing feature alignment losses. The feature\nalignment losses consist of a L2 distance and an adversarial loss obtained by\nMaximum Mean Discrepancy Generative Adversarial Network (MMD-GAN). The L2\ndistance optimizes the partial features towards the complete ones in the\nfeature space, and MMD-GAN decreases the statistical distance of two point\nfeatures in a Reproducing Kernel Hilbert Space. We achieve state-of-the-art\nperformances on the point cloud completion task. Our code is available at\nhttps://github.com/xiaogangw/point-cloud-completion-shape-prior.",
          "link": "http://arxiv.org/abs/2008.00394",
          "publishedOn": "2021-07-16T00:48:25.918Z",
          "wordCount": 661,
          "title": "Point Cloud Completion by Learning Shape Priors. (arXiv:2008.00394v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.10915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jankowski_M/0/1/0/all/0/1\">Mikolaj Jankowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1\">Krystian Mikolajczyk</a>",
          "description": "We study the image retrieval problem at the wireless edge, where an edge\ndevice captures an image, which is then used to retrieve similar images from an\nedge server. These can be images of the same person or a vehicle taken from\nother cameras at different times and locations. Our goal is to maximize the\naccuracy of the retrieval task under power and bandwidth constraints over the\nwireless link. Due to the stringent delay constraint of the underlying\napplication, sending the whole image at a sufficient quality is not possible.\nWe propose two alternative schemes based on digital and analog communications,\nrespectively. In the digital approach, we first propose a deep neural network\n(DNN) aided retrieval-oriented image compression scheme, whose output bit\nsequence is transmitted over the channel using conventional channel codes. In\nthe analog joint source and channel coding (JSCC) approach, the feature vectors\nare directly mapped into channel symbols. We evaluate both schemes on image\nbased re-identification (re-ID) tasks under different channel conditions,\nincluding both static and fading channels. We show that the JSCC scheme\nsignificantly increases the end-to-end accuracy, speeds up the encoding\nprocess, and provides graceful degradation with channel conditions. The\nproposed architecture is evaluated through extensive simulations on different\ndatasets and channel conditions, as well as through ablation studies.",
          "link": "http://arxiv.org/abs/2007.10915",
          "publishedOn": "2021-07-16T00:48:25.911Z",
          "wordCount": 671,
          "title": "Wireless Image Retrieval at the Edge. (arXiv:2007.10915v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.07982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sah_R/0/1/0/all/0/1\">Ramesh Kumar Sah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghasemzadeh_H/0/1/0/all/0/1\">Hassan Ghasemzadeh</a>",
          "description": "Machine learning is used for inference and decision making in wearable sensor\nsystems. However, recent studies have found that machine learning algorithms\nare easily fooled by the addition of adversarial perturbations to their inputs.\nWhat is more interesting is that adversarial examples generated for one machine\nlearning system is also effective against other systems. This property of\nadversarial examples is called transferability. In this work, we take the first\nstride in studying adversarial transferability in wearable sensor systems from\nthe following perspectives: 1) transferability between machine learning\nsystems, 2) transferability across subjects, 3) transferability across sensor\nbody locations, and 4) transferability across datasets. We found strong\nuntargeted transferability in most cases. Targeted attacks were less successful\nwith success scores from $0\\%$ to $80\\%$. The transferability of adversarial\nexamples depends on many factors such as the inclusion of data from all\nsubjects, sensor body position, number of samples in the dataset, type of\nlearning algorithm, and the distribution of source and target system dataset.\nThe transferability of adversarial examples decreases sharply when the data\ndistribution of the source and target system becomes more distinct. We also\nprovide guidelines for the community for designing robust sensor systems.",
          "link": "http://arxiv.org/abs/2003.07982",
          "publishedOn": "2021-07-16T00:48:25.897Z",
          "wordCount": 661,
          "title": "Adversarial Transferability in Wearable Sensor Systems. (arXiv:2003.07982v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.11094",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Koppel_A/0/1/0/all/0/1\">Alec Koppel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pradhan_H/0/1/0/all/0/1\">Hrusikesha Pradhan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rajawat_K/0/1/0/all/0/1\">Ketan Rajawat</a>",
          "description": "Gaussian processes provide a framework for nonlinear nonparametric Bayesian\ninference widely applicable across science and engineering. Unfortunately,\ntheir computational burden scales cubically with the training sample size,\nwhich in the case that samples arrive in perpetuity, approaches infinity. This\nissue necessitates approximations for use with streaming data, which to date\nmostly lack convergence guarantees. Thus, we develop the first online Gaussian\nprocess approximation that preserves convergence to the population posterior,\ni.e., asymptotic posterior consistency, while ameliorating its intractable\ncomplexity growth with the sample size. We propose an online compression scheme\nthat, following each a posteriori update, fixes an error neighborhood with\nrespect to the Hellinger metric centered at the current posterior, and greedily\ntosses out past kernel dictionary elements until its boundary is hit. We call\nthe resulting method Parsimonious Online Gaussian Processes (POG). For\ndiminishing error radius, exact asymptotic consistency is preserved (Theorem\n1(i)) at the cost of unbounded memory in the limit. On the other hand, for\nconstant error radius, POG converges to a neighborhood of the population\nposterior (Theorem 1(ii))but with finite memory at-worst determined by the\nmetric entropy of the feature space (Theorem 2). Experimental results are\npresented on several nonlinear regression problems which illuminates the merits\nof this approach as compared with alternatives that fix the subspace dimension\ndefining the history of past points.",
          "link": "http://arxiv.org/abs/2004.11094",
          "publishedOn": "2021-07-16T00:48:25.890Z",
          "wordCount": 678,
          "title": "Consistent Online Gaussian Process Regression Without the Sample Complexity Bottleneck. (arXiv:2004.11094v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11658",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Watanabe_C/0/1/0/all/0/1\">Chihiro Watanabe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1\">Taiji Suzuki</a>",
          "description": "Biclustering is a method for detecting homogeneous submatrices in a given\nobserved matrix, and it is an effective tool for relational data analysis.\nAlthough there are many studies that estimate the underlying bicluster\nstructure of a matrix, few have enabled us to determine the appropriate number\nof biclusters in an observed matrix. Recently, a statistical test on the number\nof biclusters has been proposed for a regular-grid bicluster structure, where\nwe assume that the latent bicluster structure can be represented by row-column\nclustering. However, when the latent bicluster structure does not satisfy such\nregular-grid assumption, the previous test requires a larger number of\nbiclusters than necessary (i.e., a finer bicluster structure than necessary)\nfor the null hypothesis to be accepted, which is not desirable in terms of\ninterpreting the accepted bicluster structure. In this study, we propose a new\nstatistical test on the number of biclusters that does not require the\nregular-grid assumption and derive the asymptotic behavior of the proposed test\nstatistic in both null and alternative cases. We illustrate the effectiveness\nof the proposed method by applying it to both synthetic and practical\nrelational data matrices.",
          "link": "http://arxiv.org/abs/2102.11658",
          "publishedOn": "2021-07-16T00:48:25.882Z",
          "wordCount": 656,
          "title": "A Goodness-of-fit Test on the Number of Biclusters in a Relational Data Matrix. (arXiv:2102.11658v3 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Shuyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>",
          "description": "Natural language often exhibits inherent hierarchical structure ingrained\nwith complex syntax and semantics. However, most state-of-the-art deep\ngenerative models learn embeddings only in Euclidean vector space, without\naccounting for this structural property of language. In this paper, we\ninvestigate text generation in a hyperbolic latent space to learn continuous\nhierarchical representations. An Adversarial Poincare Variational Autoencoder\n(APo-VAE) is presented, where both the prior and variational posterior of\nlatent variables are defined over a Poincare ball via wrapped normal\ndistributions. By adopting the primal-dual formulation of KL divergence, an\nadversarial learning procedure is introduced to empower robust model training.\nExtensive experiments in language modeling and dialog-response generation tasks\ndemonstrate the winning effectiveness of the proposed APo-VAE model over VAEs\nin Euclidean latent space, thanks to its superb capabilities in capturing\nlatent language hierarchies in hyperbolic space.",
          "link": "http://arxiv.org/abs/2005.00054",
          "publishedOn": "2021-07-16T00:48:25.876Z",
          "wordCount": 610,
          "title": "APo-VAE: Text Generation in Hyperbolic Space. (arXiv:2005.00054v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.03887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gural_A/0/1/0/all/0/1\">Albert Gural</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeau_P/0/1/0/all/0/1\">Phillip Nadeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikekar_M/0/1/0/all/0/1\">Mehul Tikekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murmann_B/0/1/0/all/0/1\">Boris Murmann</a>",
          "description": "The recent success of neural networks for solving difficult decision tasks\nhas incentivized incorporating smart decision making \"at the edge.\" However,\nthis work has traditionally focused on neural network inference, rather than\ntraining, due to memory and compute limitations, especially in emerging\nnon-volatile memory systems, where writes are energetically costly and reduce\nlifespan. Yet, the ability to train at the edge is becoming increasingly\nimportant as it enables real-time adaptability to device drift and\nenvironmental variation, user customization, and federated learning across\ndevices. In this work, we address two key challenges for training on edge\ndevices with non-volatile memory: low write density and low auxiliary memory.\nWe present a low-rank training scheme that addresses these challenges while\nmaintaining computational efficiency. We then demonstrate the technique on a\nrepresentative convolutional neural network across several adaptation problems,\nwhere it out-performs standard SGD both in accuracy and in number of weight\nwrites.",
          "link": "http://arxiv.org/abs/2009.03887",
          "publishedOn": "2021-07-16T00:48:25.859Z",
          "wordCount": 615,
          "title": "Low-Rank Training of Deep Neural Networks for Emerging Memory Technology. (arXiv:2009.03887v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tizikara_D/0/1/0/all/0/1\">Dativa K. Tizikara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serugunda_J/0/1/0/all/0/1\">Jonathan Serugunda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katumba_A/0/1/0/all/0/1\">Andrew Katumba</a>",
          "description": "Future communication systems are faced with increased demand for high\ncapacity, dynamic bandwidth, reliability and heterogeneous traffic. To meet\nthese requirements, networks have become more complex and thus require new\ndesign methods and monitoring techniques, as they evolve towards becoming\nautonomous. Machine learning has come to the forefront in recent years as a\npromising technology to aid in this evolution. Optical fiber communications can\nalready provide the high capacity required for most applications, however,\nthere is a need for increased scalability and adaptability to changing user\ndemands and link conditions. Accurate performance monitoring is an integral\npart of this transformation. In this paper we review optical performance\nmonitoring techniques where machine learning algorithms have been applied.\nMoreover, since alot of OPM depends on knowledge of the signal type, we also\nreview work for modulation format recognition and bitrate identification. We\nadditionally briefly introduce a neuromorphic approach to OPM as an emerging\ntechnique that has only recently been applied to this domain.",
          "link": "http://arxiv.org/abs/2107.07338",
          "publishedOn": "2021-07-16T00:48:25.825Z",
          "wordCount": 596,
          "title": "An Overview of Machine Learning-aided Optical Performance Monitoring Techniques. (arXiv:2107.07338v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Palermo_J/0/1/0/all/0/1\">Joseph Palermo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Johnny Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Alok Singh</a>",
          "description": "We convert the DeepMind Mathematics Dataset into a reinforcement learning\nenvironment by interpreting it as a program synthesis problem. Each action\ntaken in the environment adds an operator or an input into a discrete compute\ngraph. Graphs which compute correct answers yield positive reward, enabling the\noptimization of a policy to construct compute graphs conditioned on problem\nstatements. Baseline models are trained using Double DQN on various subsets of\nproblem types, demonstrating the capability to learn to correctly construct\ngraphs despite the challenges of combinatorial explosion and noisy rewards.",
          "link": "http://arxiv.org/abs/2107.07373",
          "publishedOn": "2021-07-16T00:48:25.816Z",
          "wordCount": 524,
          "title": "A Reinforcement Learning Environment for Mathematical Reasoning via Program Synthesis. (arXiv:2107.07373v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_R/0/1/0/all/0/1\">Rahul Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Ramon_M/0/1/0/all/0/1\">Manel Mart&#xed;nez-Ram&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busani_T/0/1/0/all/0/1\">Tito Busani</a>",
          "description": "This work investigates application of different machine learning based\nprediction methodologies to estimate the performance of silicon based textured\ncells. Concept of confidence bound regions is introduced and advantages of this\nconcept are discussed in detail. Results show that reflection profiles and\ndepth dependent optical generation profiles can be accurately estimated using\nGaussian processes with exact knowledge of uncertainty in the prediction\nvalues.It is also shown that cell design parameters can be estimated for a\ndesired performance metric.",
          "link": "http://arxiv.org/abs/2107.07342",
          "publishedOn": "2021-07-16T00:48:25.802Z",
          "wordCount": 522,
          "title": "Probabilistic analysis of solar cell optical performance using Gaussian processes. (arXiv:2107.07342v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1\">Jacopo Tagliabue</a>",
          "description": "We argue that immature data pipelines are preventing a large portion of\nindustry practitioners from leveraging the latest research on recommender\nsystems. We propose our template data stack for machine learning at \"reasonable\nscale\", and show how many challenges are solved by embracing a serverless\nparadigm. Leveraging our experience, we detail how modern open source can\nprovide a pipeline processing terabytes of data with limited infrastructure\nwork.",
          "link": "http://arxiv.org/abs/2107.07346",
          "publishedOn": "2021-07-16T00:48:25.796Z",
          "wordCount": 525,
          "title": "You Do Not Need a Bigger Boat: Recommendations at Reasonable Scale in a (Mostly) Serverless and Open Stack. (arXiv:2107.07346v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07480",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Derezinski_M/0/1/0/all/0/1\">Micha&#x142; Derezi&#x144;ski</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lacotte_J/0/1/0/all/0/1\">Jonathan Lacotte</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>",
          "description": "In second-order optimization, a potential bottleneck can be computing the\nHessian matrix of the optimized function at every iteration. Randomized\nsketching has emerged as a powerful technique for constructing estimates of the\nHessian which can be used to perform approximate Newton steps. This involves\nmultiplication by a random sketching matrix, which introduces a trade-off\nbetween the computational cost of sketching and the convergence rate of the\noptimization algorithm. A theoretically desirable but practically much too\nexpensive choice is to use a dense Gaussian sketching matrix, which produces\nunbiased estimates of the exact Newton step and which offers strong\nproblem-independent convergence guarantees. We show that the Gaussian sketching\nmatrix can be drastically sparsified, significantly reducing the computational\ncost of sketching, without substantially affecting its convergence properties.\nThis approach, called Newton-LESS, is based on a recently introduced sketching\ntechnique: LEverage Score Sparsified (LESS) embeddings. We prove that\nNewton-LESS enjoys nearly the same problem-independent local convergence rate\nas Gaussian embeddings, not just up to constant factors but even down to lower\norder terms, for a large class of optimization tasks. In particular, this leads\nto a new state-of-the-art convergence result for an iterative least squares\nsolver. Finally, we extend LESS embeddings to include uniformly sparsified\nrandom sign matrices which can be implemented efficiently and which perform\nwell in numerical experiments.",
          "link": "http://arxiv.org/abs/2107.07480",
          "publishedOn": "2021-07-16T00:48:25.778Z",
          "wordCount": 666,
          "title": "Newton-LESS: Sparsification without Trade-offs for the Sketched Newton Update. (arXiv:2107.07480v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malinin_A/0/1/0/all/0/1\">Andrey Malinin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Band_N/0/1/0/all/0/1\">Neil Band</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chesnokov_G/0/1/0/all/0/1\">German Chesnokov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noskov_A/0/1/0/all/0/1\">Alexey Noskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ploskonosov_A/0/1/0/all/0/1\">Andrey Ploskonosov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prokhorenkova_L/0/1/0/all/0/1\">Liudmila Prokhorenkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Provilkov_I/0/1/0/all/0/1\">Ivan Provilkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vatsal Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vyas Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmatova_M/0/1/0/all/0/1\">Mariya Shmatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tigas_P/0/1/0/all/0/1\">Panos Tigas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yangel_B/0/1/0/all/0/1\">Boris Yangel</a>",
          "description": "There has been significant research done on developing methods for improving\nrobustness to distributional shift and uncertainty estimation. In contrast,\nonly limited work has examined developing standard datasets and benchmarks for\nassessing these approaches. Additionally, most work on uncertainty estimation\nand robustness has developed new techniques based on small-scale regression or\nimage classification tasks. However, many tasks of practical interest have\ndifferent modalities, such as tabular data, audio, text, or sensor data, which\noffer significant challenges involving regression and discrete or continuous\nstructured prediction. Thus, given the current state of the field, a\nstandardized large-scale dataset of tasks across a range of modalities affected\nby distributional shifts is necessary. This will enable researchers to\nmeaningfully evaluate the plethora of recently developed uncertainty\nquantification methods, as well as assessment criteria and state-of-the-art\nbaselines. In this work, we propose the \\emph{Shifts Dataset} for evaluation of\nuncertainty estimates and robustness to distributional shift. The dataset,\nwhich has been collected from industrial sources and services, is composed of\nthree tasks, with each corresponding to a particular data modality: tabular\nweather prediction, machine translation, and self-driving car (SDC) vehicle\nmotion prediction. All of these data modalities and tasks are affected by real,\n`in-the-wild' distributional shifts and pose interesting challenges with\nrespect to uncertainty estimation. In this work we provide a description of the\ndataset and baseline results for all tasks.",
          "link": "http://arxiv.org/abs/2107.07455",
          "publishedOn": "2021-07-16T00:48:25.772Z",
          "wordCount": 693,
          "title": "Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks. (arXiv:2107.07455v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fang-Yi Yu</a>",
          "description": "This paper introduces an optimization problem for proper scoring rule design.\nConsider a principal who wants to collect an agent's prediction about an\nunknown state. The agent can either report his prior prediction or access a\ncostly signal and report the posterior prediction. Given a collection of\npossible distributions containing the agent's posterior prediction\ndistribution, the principal's objective is to design a bounded scoring rule to\nmaximize the agent's worst-case payoff increment between reporting his\nposterior prediction and reporting his prior prediction.\n\nWe study two settings of such optimization for proper scoring rules: static\nand asymptotic settings. In the static setting, where the agent can access one\nsignal, we propose an efficient algorithm to compute an optimal scoring rule\nwhen the collection of distributions is finite. The agent can adaptively and\nindefinitely refine his prediction in the asymptotic setting. We first consider\na sequence of collections of posterior distributions with vanishing covariance,\nwhich emulates general estimators with large samples, and show the optimality\nof the quadratic scoring rule. Then, when the agent's posterior distribution is\na Beta-Bernoulli process, we find that the log scoring rule is optimal. We also\nprove the optimality of the log scoring rule over a smaller set of functions\nfor categorical distributions with Dirichlet priors.",
          "link": "http://arxiv.org/abs/2107.07420",
          "publishedOn": "2021-07-16T00:48:25.765Z",
          "wordCount": 643,
          "title": "Optimal Scoring Rule Design. (arXiv:2107.07420v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07423",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ginige_N/0/1/0/all/0/1\">Nipuni Ginige</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manosha_K/0/1/0/all/0/1\">K. B. Shashika Manosha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajatheva_N/0/1/0/all/0/1\">Nandana Rajatheva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Latva_aho_M/0/1/0/all/0/1\">Matti Latva-aho</a>",
          "description": "Reconfigurable intelligent surface (RIS) is an emerging technology for\nimproving performance in fifth-generation (5G) and beyond networks. Practically\nchannel estimation of RIS-assisted systems is challenging due to the passive\nnature of the RIS. The purpose of this paper is to introduce a deep\nlearning-based, low complexity channel estimator for the RIS-assisted\nmulti-user single-input-multiple-output (SIMO) orthogonal frequency division\nmultiplexing (OFDM) system with hardware impairments. We propose an untrained\ndeep neural network (DNN) based on the deep image prior (DIP) network to\ndenoise the effective channel of the system obtained from the conventional\npilot-based least-square (LS) estimation and acquire a more accurate\nestimation. We have shown that our proposed method has high performance in\nterms of accuracy and low complexity compared to conventional methods. Further,\nwe have shown that the proposed estimator is robust to interference caused by\nthe hardware impairments at the transceiver and RIS.",
          "link": "http://arxiv.org/abs/2107.07423",
          "publishedOn": "2021-07-16T00:48:25.725Z",
          "wordCount": 595,
          "title": "Untrained DNN for Channel Estimation of RIS-Assisted Multi-User OFDM System with Hardware Impairments. (arXiv:2107.07423v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weilbach_J/0/1/0/all/0/1\">Juliane Weilbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerwinn_S/0/1/0/all/0/1\">Sebastian Gerwinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weilbach_C/0/1/0/all/0/1\">Christian Weilbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandemir_M/0/1/0/all/0/1\">Melih Kandemir</a>",
          "description": "Understanding physical phenomena oftentimes means understanding the\nunderlying dynamical system that governs observational measurements. While\naccurate prediction can be achieved with black box systems, they often lack\ninterpretability and are less amenable for further expert investigation.\nAlternatively, the dynamics can be analysed via symbolic regression. In this\npaper, we extend the approach by (Udrescu et al., 2020) called AIFeynman to the\ndynamic setting to perform symbolic regression on ODE systems based on\nobservations from the resulting trajectories. We compare this extension to\nstate-of-the-art approaches for symbolic regression empirically on several\ndynamical systems for which the ground truth equations of increasing complexity\nare available. Although the proposed approach performs best on this benchmark,\nwe observed difficulties of all the compared symbolic regression approaches on\nmore complex systems, such as Cart-Pole.",
          "link": "http://arxiv.org/abs/2107.07345",
          "publishedOn": "2021-07-16T00:48:25.703Z",
          "wordCount": 557,
          "title": "Inferring the Structure of Ordinary Differential Equations. (arXiv:2107.07345v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07436",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jethani_N/0/1/0/all/0/1\">Neil Jethani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sudarshan_M/0/1/0/all/0/1\">Mukund Sudarshan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Covert_I/0/1/0/all/0/1\">Ian Covert</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1\">Su-In Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ranganath_R/0/1/0/all/0/1\">Rajesh Ranganath</a>",
          "description": "Shapley values are widely used to explain black-box models, but they are\ncostly to calculate because they require many model evaluations. We introduce\nFastSHAP, a method for estimating Shapley values in a single forward pass using\na learned explainer model. FastSHAP amortizes the cost of explaining many\ninputs via a learning approach inspired by the Shapley value's weighted least\nsquares characterization, and it can be trained using standard stochastic\ngradient optimization. We compare FastSHAP to existing estimation approaches,\nrevealing that it generates high-quality explanations with orders of magnitude\nspeedup.",
          "link": "http://arxiv.org/abs/2107.07436",
          "publishedOn": "2021-07-16T00:48:25.693Z",
          "wordCount": 533,
          "title": "FastSHAP: Real-Time Shapley Value Estimation. (arXiv:2107.07436v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07483",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Valente_F/0/1/0/all/0/1\">Francisco Valente</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Paredes_S/0/1/0/all/0/1\">Sim&#xe3;o Paredes</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Henriques_J/0/1/0/all/0/1\">Jorge Henriques</a>",
          "description": "In this study, we present a novel clinical decision support system and\ndiscuss its interpretability-related properties. It combines a decision set of\nrules with a machine learning scheme to offer global and local\ninterpretability. More specifically, machine learning is used to predict the\nlikelihood of each of those rules to be correct for a particular patient, which\nmay also contribute to better predictive performances. Moreover, the\nreliability analysis of individual predictions is also addressed, contributing\nto further personalized interpretability. The combination of these several\nelements may be crucial to obtain the clinical stakeholders' trust, leading to\na better assessment of patients' conditions and improvement of the physicians'\ndecision-making.",
          "link": "http://arxiv.org/abs/2107.07483",
          "publishedOn": "2021-07-16T00:48:25.687Z",
          "wordCount": 559,
          "title": "Personalized and Reliable Decision Sets: Enhancing Interpretability in Clinical Decision Support Systems. (arXiv:2107.07483v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lang_N/0/1/0/all/0/1\">Nico Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1\">Jan Dirk Wegner</a>",
          "description": "The increasing demand for commodities is leading to changes in land use\nworldwide. In the tropics, deforestation, which causes high carbon emissions\nand threatens biodiversity, is often linked to agricultural expansion. While\nthe need for deforestation-free global supply chains is widely recognized,\nmaking progress in practice remains a challenge. Here, we propose an automated\napproach that aims to support conservation and sustainable land use planning\ndecisions by mapping tropical landscapes at large scale and high spatial\nresolution following the High Carbon Stock (HCS) approach. A deep learning\napproach is developed that estimates canopy height for each 10 m Sentinel-2\npixel by learning from sparse GEDI LIDAR reference data, achieving an overall\nRMSE of 6.3 m. We show that these wall-to-wall maps of canopy top height are\npredictive for classifying HCS forests and degraded areas with an overall\naccuracy of 86 % and produce a first high carbon stock map for Indonesia,\nMalaysia, and the Philippines.",
          "link": "http://arxiv.org/abs/2107.07431",
          "publishedOn": "2021-07-16T00:48:25.677Z",
          "wordCount": 606,
          "title": "High carbon stock mapping at large scale with optical satellite imagery and spaceborne LIDAR. (arXiv:2107.07431v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laszkiewicz_M/0/1/0/all/0/1\">Mike Laszkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lederer_J/0/1/0/all/0/1\">Johannes Lederer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1\">Asja Fischer</a>",
          "description": "Normalizing flows, which learn a distribution by transforming the data to\nsamples from a Gaussian base distribution, have proven powerful density\napproximations. But their expressive power is limited by this choice of the\nbase distribution. We, therefore, propose to generalize the base distribution\nto a more elaborate copula distribution to capture the properties of the target\ndistribution more accurately. In a first empirical analysis, we demonstrate\nthat this replacement can dramatically improve the vanilla normalizing flows in\nterms of flexibility, stability, and effectivity for heavy-tailed data. Our\nresults suggest that the improvements are related to an increased local\nLipschitz-stability of the learned flow.",
          "link": "http://arxiv.org/abs/2107.07352",
          "publishedOn": "2021-07-16T00:48:25.658Z",
          "wordCount": 550,
          "title": "Copula-Based Normalizing Flows. (arXiv:2107.07352v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiying Li</a>",
          "description": "Gradient Episodic Memory is indeed a novel method for continual learning,\nwhich solves new problems quickly without forgetting previously acquired\nknowledge. However, in the process of studying the paper, we found there were\nsome problems in the proof of the dual problem of Quadratic Program, so here we\ngive our fixed version for this problem.",
          "link": "http://arxiv.org/abs/2107.07384",
          "publishedOn": "2021-07-16T00:48:25.640Z",
          "wordCount": 485,
          "title": "A Fixed Version of Quadratic Program in Gradient Episodic Memory. (arXiv:2107.07384v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fickinger_A/0/1/0/all/0/1\">Arnaud Fickinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaques_N/0/1/0/all/0/1\">Natasha Jaques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parajuli_S/0/1/0/all/0/1\">Samyak Parajuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Michael Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhinehart_N/0/1/0/all/0/1\">Nicholas Rhinehart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berseth_G/0/1/0/all/0/1\">Glen Berseth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1\">Stuart Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Reinforcement learning (RL) provides a framework for learning goal-directed\npolicies given user-specified rewards. However, since designing rewards often\nrequires substantial engineering effort, we are interested in the problem of\nlearning without rewards, where agents must discover useful behaviors in the\nabsence of task-specific incentives. Intrinsic motivation is a family of\nunsupervised RL techniques which develop general objectives for an RL agent to\noptimize that lead to better exploration or the discovery of skills. In this\npaper, we propose a new unsupervised RL technique based on an adversarial game\nwhich pits two policies against each other to compete over the amount of\nsurprise an RL agent experiences. The policies each take turns controlling the\nagent. The Explore policy maximizes entropy, putting the agent into surprising\nor unfamiliar situations. Then, the Control policy takes over and seeks to\nrecover from those situations by minimizing entropy. The game harnesses the\npower of multi-agent competition to drive the agent to seek out increasingly\nsurprising parts of the environment while learning to gain mastery over them.\nWe show empirically that our method leads to the emergence of complex skills by\nexhibiting clear phase transitions. Furthermore, we show both theoretically\n(via a latent state space coverage argument) and empirically that our method\nhas the potential to be applied to the exploration of stochastic,\npartially-observed environments. We show that Adversarial Surprise learns more\ncomplex behaviors, and explores more effectively than competitive baselines,\noutperforming intrinsic motivation methods based on active inference,\nnovelty-seeking (Random Network Distillation (RND)), and multi-agent\nunsupervised RL (Asymmetric Self-Play (ASP)) in MiniGrid, Atari and VizDoom\nenvironments.",
          "link": "http://arxiv.org/abs/2107.07394",
          "publishedOn": "2021-07-16T00:48:25.631Z",
          "wordCount": 698,
          "title": "Explore and Control with Adversarial Surprise. (arXiv:2107.07394v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuda Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>",
          "description": "Model-based Reinforcement Learning (RL) is a popular learning paradigm due to\nits potential sample efficiency compared to model-free RL. However, existing\nempirical model-based RL approaches lack the ability to explore. This work\nstudies a computationally and statistically efficient model-based algorithm for\nboth Kernelized Nonlinear Regulators (KNR) and linear Markov Decision Processes\n(MDPs). For both models, our algorithm guarantees polynomial sample complexity\nand only uses access to a planning oracle. Experimentally, we first demonstrate\nthe flexibility and efficacy of our algorithm on a set of exploration\nchallenging control tasks where existing empirical model-based RL approaches\ncompletely fail. We then show that our approach retains excellent performance\neven in common dense reward control benchmarks that do not require heavy\nexploration. Finally, we demonstrate that our method can also perform\nreward-free exploration efficiently. Our code can be found at\nhttps://github.com/yudasong/PCMLP.",
          "link": "http://arxiv.org/abs/2107.07410",
          "publishedOn": "2021-07-16T00:48:25.625Z",
          "wordCount": 567,
          "title": "PC-MLP: Model-based Reinforcement Learning with Policy Cover Guided Exploration. (arXiv:2107.07410v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_E/0/1/0/all/0/1\">Elaine Pimentel</a> (UFRN), <a href=\"http://arxiv.org/find/cs/1/au:+Tassi_E/0/1/0/all/0/1\">Enrico Tassi</a> (Inria)",
          "description": "Logical frameworks and meta-languages form a common substrate for\nrepresenting, implementing and reasoning about a wide variety of deductive\nsystems of interest in logic and computer science. Their design, implementation\nand their use in reasoning tasks, ranging from the correctness of software to\nthe properties of formal systems, have been the focus of considerable research\nover the last two decades. This workshop brings together designers,\nimplementors and practitioners to discuss various aspects impinging on the\nstructure and utility of logical frameworks, including the treatment of\nvariable binding, inductive and co-inductive reasoning techniques and the\nexpressiveness and lucidity of the reasoning process.",
          "link": "http://arxiv.org/abs/2107.07376",
          "publishedOn": "2021-07-16T00:48:25.611Z",
          "wordCount": 567,
          "title": "Proceedings of the Sixteenth Workshop on Logical Frameworks and Meta-Languages: Theory and Practice. (arXiv:2107.07376v1 [cs.LO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1\">Ori Yoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talmor_A/0/1/0/all/0/1\">Alon Talmor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>",
          "description": "Models pre-trained with a language modeling objective possess ample world\nknowledge and language skills, but are known to struggle in tasks that require\nreasoning. In this work, we propose to leverage semi-structured tables, and\nautomatically generate at scale question-paragraph pairs, where answering the\nquestion requires reasoning over multiple facts in the paragraph. We add a\npre-training step over this synthetic data, which includes examples that\nrequire 16 different reasoning skills such as number comparison, conjunction,\nand fact composition. To improve data efficiency, we propose sampling\nstrategies that focus training on reasoning skills the model is currently\nlacking. We evaluate our approach on three reading comprehension datasets that\nare focused on reasoning, and show that our model, PReasM, substantially\noutperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling\nexamples based on current model errors leads to faster training and higher\noverall performance.",
          "link": "http://arxiv.org/abs/2107.07261",
          "publishedOn": "2021-07-16T00:48:25.604Z",
          "wordCount": 587,
          "title": "Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills. (arXiv:2107.07261v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rutwik Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Astuto_B/0/1/0/all/0/1\">Bruno Astuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleason_T/0/1/0/all/0/1\">Tyler Gleason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fletcher_W/0/1/0/all/0/1\">Will Fletcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banaga_J/0/1/0/all/0/1\">Justin Banaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sweetwood_K/0/1/0/all/0/1\">Kevin Sweetwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_A/0/1/0/all/0/1\">Allen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1\">Rina Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGill_K/0/1/0/all/0/1\">Kevin McGill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Link_T/0/1/0/all/0/1\">Thomas Link</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crane_J/0/1/0/all/0/1\">Jason Crane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedoia_V/0/1/0/all/0/1\">Valentina Pedoia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Sharmila Majumdar</a>",
          "description": "Radiologists today play a key role in making diagnostic decisions and\nlabeling images for training A.I. algorithms. Low inter-reader reliability\n(IRR) can be seen between experts when interpreting challenging cases. While\nteams-based decisions are known to outperform individual decisions,\ninter-personal biases often creep up in group interactions which limit\nnon-dominant participants from expressing true opinions. To overcome the dual\nproblems of low consensus and inter-personal bias, we explored a solution\nmodeled on biological swarms of bees. Two separate cohorts; three radiologists\nand five radiology residents collaborated on a digital swarm platform in real\ntime and in a blinded fashion, grading meniscal lesions on knee MR exams. These\nconsensus votes were benchmarked against clinical (arthroscopy) and\nradiological (senior-most radiologist) observations. The IRR of the consensus\nvotes was compared to the IRR of the majority and most confident votes of the\ntwo cohorts.The radiologist cohort saw an improvement of 23% in IRR of swarm\nvotes over majority vote. Similar improvement of 23% in IRR in 3-resident swarm\nvotes over majority vote, was observed. The 5-resident swarm had an even higher\nimprovement of 32% in IRR over majority vote. Swarm consensus votes also\nimproved specificity by up to 50%. The swarm consensus votes outperformed\nindividual and majority vote decisions in both the radiologists and resident\ncohorts. The 5-resident swarm had higher IRR than 3-resident swarm indicating\npositive effect of increased swarm size. The attending and resident swarms also\noutperformed predictions from a state-of-the-art A.I. algorithm. Utilizing a\ndigital swarm platform improved agreement and allows participants to express\njudgement free intent, resulting in superior clinical performance and robust\nA.I. training labels.",
          "link": "http://arxiv.org/abs/2107.07341",
          "publishedOn": "2021-07-16T00:48:25.589Z",
          "wordCount": 771,
          "title": "Leveraging wisdom of the crowds to improve consensus among radiologists by real time, blinded collaborations on a digital swarm platform. (arXiv:2107.07341v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bej_S/0/1/0/all/0/1\">Saptarshi Bej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schultz_K/0/1/0/all/0/1\">Kristian Schultz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1\">Prashant Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfien_M/0/1/0/all/0/1\">Markus Wolfien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolkenhauer_O/0/1/0/all/0/1\">Olaf Wolkenhauer</a>",
          "description": "Over 85 oversampling algorithms, mostly extensions of the SMOTE algorithm,\nhave been built over the past two decades, to solve the problem of imbalanced\ndatasets. However, it has been evident from previous studies that different\noversampling algorithms have different degrees of efficiency with different\nclassifiers. With numerous algorithms available, it is difficult to decide on\nan oversampling algorithm for a chosen classifier. Here, we overcome this\nproblem with a multi-schematic and classifier-independent oversampling\napproach: ProWRAS(Proximity Weighted Random Affine Shadowsampling). ProWRAS\nintegrates the Localized Random Affine Shadowsampling (LoRAS)algorithm and the\nProximity Weighted Synthetic oversampling (ProWSyn) algorithm. By controlling\nthe variance of the synthetic samples, as well as a proximity-weighted\nclustering system of the minority classdata, the ProWRAS algorithm improves\nperformance, compared to algorithms that generate synthetic samples through\nmodelling high dimensional convex spaces of the minority class. ProWRAS has\nfour oversampling schemes, each of which has its unique way to model the\nvariance of the generated data. Most importantly, the performance of ProWRAS\nwith proper choice of oversampling schemes, is independent of the classifier\nused. We have benchmarked our newly developed ProWRAS algorithm against five\nsate-of-the-art oversampling models and four different classifiers on 20\npublicly available datasets. ProWRAS outperforms other oversampling algorithms\nin a statistically significant way, in terms of both F1-score and Kappa-score.\nMoreover, we have introduced a novel measure for classifier independence\nI-score, and showed quantitatively that ProWRAS performs better, independent of\nthe classifier used. In practice, ProWRAS customizes synthetic sample\ngeneration according to a classifier of choice and thereby reduces benchmarking\nefforts.",
          "link": "http://arxiv.org/abs/2107.07349",
          "publishedOn": "2021-07-16T00:48:25.573Z",
          "wordCount": 695,
          "title": "A multi-schematic classifier-independent oversampling approach for imbalanced datasets. (arXiv:2107.07349v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azam_M/0/1/0/all/0/1\">Md Ali Azam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossen_A/0/1/0/all/0/1\">Abir Hossen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Hafizur Rahman</a>",
          "description": "Biologically inspired computing techniques are very effective and useful in\nmany areas of research including data clustering. Ant clustering algorithm is a\nnature-inspired clustering technique which is extensively studied for over two\ndecades. In this study, we extend the ant clustering algorithm (ACA) to a\nhybrid ant clustering algorithm (hACA). Specifically, we include a genetic\nalgorithm in standard ACA to extend the hybrid algorithm for better\nperformance. We also introduced novel pick up and drop off rules to speed up\nthe clustering performance. We study the performance of the hACA algorithm and\ncompare with standard ACA as a benchmark.",
          "link": "http://arxiv.org/abs/2107.07382",
          "publishedOn": "2021-07-16T00:48:25.558Z",
          "wordCount": 545,
          "title": "Hybrid Ant Swarm-Based Data Clustering. (arXiv:2107.07382v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiangyu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jifeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chulin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yong Yang</a>",
          "description": "We study the realistic potential of conducting backdoor attack against deep\nneural networks (DNNs) during deployment stage. Specifically, our goal is to\ndesign a deployment-stage backdoor attack algorithm that is both threatening\nand realistically implementable. To this end, we propose Subnet Replacement\nAttack (SRA), which is capable of embedding backdoor into DNNs by directly\nmodifying a limited number of model parameters. Considering the realistic\npracticability, we abandon the strong white-box assumption widely adopted in\nexisting studies, instead, our algorithm works in a gray-box setting, where\narchitecture information of the victim model is available but the adversaries\ndo not have any knowledge of parameter values. The key philosophy underlying\nour approach is -- given any neural network instance (regardless of its\nspecific parameter values) of a certain architecture, we can always embed a\nbackdoor into that model instance, by replacing a very narrow subnet of a\nbenign model (without backdoor) with a malicious backdoor subnet, which is\ndesigned to be sensitive (fire large activation value) to a particular backdoor\ntrigger pattern.",
          "link": "http://arxiv.org/abs/2107.07240",
          "publishedOn": "2021-07-16T00:48:25.541Z",
          "wordCount": 628,
          "title": "Subnet Replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting. (arXiv:2107.07240v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiayun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yubei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_B/0/1/0/all/0/1\">Brian Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>",
          "description": "We present a generic method for recurrently using the same parameters for\nmany different convolution layers to build a deep network. Specifically, for a\nnetwork, we create a recurrent parameter generator (RPG), from which the\nparameters of each convolution layer are generated. Though using recurrent\nmodels to build a deep convolutional neural network (CNN) is not entirely new,\nour method achieves significant performance gain compared to the existing\nworks. We demonstrate how to build a one-layer neural network to achieve\nsimilar performance compared to other traditional CNN models on various\napplications and datasets. Such a method allows us to build an arbitrarily\ncomplex neural network with any amount of parameters. For example, we build a\nResNet34 with model parameters reduced by more than $400$ times, which still\nachieves $41.6\\%$ ImageNet top-1 accuracy. Furthermore, we demonstrate the RPG\ncan be applied at different scales, such as layers, blocks, or even\nsub-networks. Specifically, we use the RPG to build a ResNet18 network with the\nnumber of weights equivalent to one convolutional layer of a conventional\nResNet and show this model can achieve $67.2\\%$ ImageNet top-1 accuracy. The\nproposed method can be viewed as an inverse approach to model compression.\nRather than removing the unused parameters from a large model, it aims to\nsqueeze more information into a small number of parameters. Extensive\nexperiment results are provided to demonstrate the power of the proposed\nrecurrent parameter generator.",
          "link": "http://arxiv.org/abs/2107.07110",
          "publishedOn": "2021-07-16T00:48:25.503Z",
          "wordCount": 666,
          "title": "Recurrent Parameter Generators. (arXiv:2107.07110v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jafrasteh_B/0/1/0/all/0/1\">Bahram Jafrasteh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villacampa_Calvo_C/0/1/0/all/0/1\">Carlos Villacampa-Calvo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_D/0/1/0/all/0/1\">Daniel Hern&#xe1;ndez-Lobato</a>",
          "description": "Gaussian Processes (GPs) are Bayesian models that provide uncertainty\nestimates associated to the predictions made. They are also very flexible due\nto their non-parametric nature. Nevertheless, GPs suffer from poor scalability\nas the number of training instances N increases. More precisely, they have a\ncubic cost with respect to $N$. To overcome this problem, sparse GP\napproximations are often used, where a set of $M \\ll N$ inducing points is\nintroduced during training. The location of the inducing points is learned by\nconsidering them as parameters of an approximate posterior distribution $q$.\nSparse GPs, combined with variational inference for inferring $q$, reduce the\ntraining cost of GPs to $\\mathcal{O}(M^3)$. Critically, the inducing points\ndetermine the flexibility of the model and they are often located in regions of\nthe input space where the latent function changes. A limitation is, however,\nthat for some learning tasks a large number of inducing points may be required\nto obtain a good prediction performance. To address this limitation, we propose\nhere to amortize the computation of the inducing points locations, as well as\nthe parameters of the variational posterior approximation q. For this, we use a\nneural network that receives the observed data as an input and outputs the\ninducing points locations and the parameters of $q$. We evaluate our method in\nseveral experiments, showing that it performs similar or better than other\nstate-of-the-art sparse variational GP approaches. However, with our method the\nnumber of inducing points is reduced drastically due to their dependency on the\ninput data. This makes our method scale to larger datasets and have faster\ntraining and prediction times.",
          "link": "http://arxiv.org/abs/2107.07281",
          "publishedOn": "2021-07-16T00:48:25.487Z",
          "wordCount": 695,
          "title": "Input Dependent Sparse Gaussian Processes. (arXiv:2107.07281v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kungurtsev_V/0/1/0/all/0/1\">Vyacheslav Kungurtsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobb_A/0/1/0/all/0/1\">Adam Cobb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1\">Tara Javidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalaian_B/0/1/0/all/0/1\">Brian Jalaian</a>",
          "description": "Federated learning performed by a decentralized networks of agents is\nbecoming increasingly important with the prevalence of embedded software on\nautonomous devices. Bayesian approaches to learning benefit from offering more\ninformation as to the uncertainty of a random quantity, and Langevin and\nHamiltonian methods are effective at realizing sampling from an uncertain\ndistribution with large parameter dimensions. Such methods have only recently\nappeared in the decentralized setting, and either exclusively use stochastic\ngradient Langevin and Hamiltonian Monte Carlo approaches that require a\ndiminishing stepsize to asymptotically sample from the posterior and are known\nin practice to characterize uncertainty less faithfully than constant step-size\nmethods with a Metropolis adjustment, or assume strong convexity properties of\nthe potential function. We present the first approach to incorporating constant\nstepsize Metropolis-adjusted HMC in the decentralized sampling framework, show\ntheoretical guarantees for consensus and probability distance to the posterior\nstationary distribution, and demonstrate their effectiveness numerically on\nstandard real world problems, including decentralized learning of neural\nnetworks which is known to be highly non-convex.",
          "link": "http://arxiv.org/abs/2107.07211",
          "publishedOn": "2021-07-16T00:48:25.480Z",
          "wordCount": 609,
          "title": "Decentralized Bayesian Learning with Metropolis-Adjusted Hamiltonian Monte Carlo. (arXiv:2107.07211v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Najdenkoska_I/0/1/0/all/0/1\">Ivona Najdenkoska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1\">Marcel Worring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Automating report generation for medical imaging promises to reduce workload\nand assist diagnosis in clinical practice. Recent work has shown that deep\nlearning models can successfully caption natural images. However, learning from\nmedical data is challenging due to the diversity and uncertainty inherent in\nthe reports written by different radiologists with discrepant expertise and\nexperience. To tackle these challenges, we propose variational topic inference\nfor automatic report generation. Specifically, we introduce a set of topics as\nlatent variables to guide sentence generation by aligning image and language\nmodalities in a latent space. The topics are inferred in a conditional\nvariational inference framework, with each topic governing the generation of a\nsentence in the report. Further, we adopt a visual attention module that\nenables the model to attend to different locations in the image and generate\nmore informative descriptions. We conduct extensive experiments on two\nbenchmarks, namely Indiana U. Chest X-rays and MIMIC-CXR. The results\ndemonstrate that our proposed variational topic inference method can generate\nnovel reports rather than mere copies of reports used in training, while still\nachieving comparable performance to state-of-the-art methods in terms of\nstandard language generation criteria.",
          "link": "http://arxiv.org/abs/2107.07314",
          "publishedOn": "2021-07-16T00:48:25.471Z",
          "wordCount": 653,
          "title": "Variational Topic Inference for Chest X-Ray Report Generation. (arXiv:2107.07314v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_M/0/1/0/all/0/1\">Mansheej Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Surya Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziugaite_G/0/1/0/all/0/1\">Gintare Karolina Dziugaite</a>",
          "description": "The recent success of deep learning has partially been driven by training\nincreasingly overparametrized networks on ever larger datasets. It is therefore\nnatural to ask: how much of the data is superfluous, which examples are\nimportant for generalization, and how do we find them? In this work, we make\nthe striking observation that, on standard vision benchmarks, the initial loss\ngradient norm of individual training examples, averaged over several weight\ninitializations, can be used to identify a smaller set of training data that is\nimportant for generalization. Furthermore, after only a few epochs of training,\nthe information in gradient norms is reflected in the normed error--L2 distance\nbetween the predicted probabilities and one hot labels--which can be used to\nprune a significant fraction of the dataset without sacrificing test accuracy.\nBased on this, we propose data pruning methods which use only local information\nearly in training, and connect them to recent work that prunes data by\ndiscarding examples that are rarely forgotten over the course of training. Our\nmethods also shed light on how the underlying data distribution shapes the\ntraining dynamics: they rank examples based on their importance for\ngeneralization, detect noisy examples and identify subspaces of the model's\ndata representation that are relatively stable over training.",
          "link": "http://arxiv.org/abs/2107.07075",
          "publishedOn": "2021-07-16T00:48:25.464Z",
          "wordCount": 650,
          "title": "Deep Learning on a Data Diet: Finding Important Examples Early in Training. (arXiv:2107.07075v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongming Liu</a>",
          "description": "Robust physics discovery is of great interest for many scientific and\nengineering fields. Inspired by the principle that a representative model is\nthe one simplest possible, a new model selection criteria considering both\nmodel's Parsimony and Sparsity is proposed. A Parsimony Enhanced Sparse\nBayesian Learning (PeSBL) method is developed for discovering the governing\nPartial Differential Equations (PDEs) of nonlinear dynamical systems. Compared\nwith the conventional Sparse Bayesian Learning (SBL) method, the PeSBL method\npromotes parsimony of the learned model in addition to its sparsity. In this\nmethod, the parsimony of model terms is evaluated using their locations in the\nprescribed candidate library, for the first time, considering the increased\ncomplexity with the power of polynomials and the order of spatial derivatives.\nSubsequently, the model parameters are updated through Bayesian inference with\nthe raw data. This procedure aims to reduce the error associated with the\npossible loss of information in data preprocessing and numerical\ndifferentiation prior to sparse regression. Results of numerical case studies\nindicate that the governing PDEs of many canonical dynamical systems can be\ncorrectly identified using the proposed PeSBL method from highly noisy data (up\nto 50% in the current study). Next, the proposed methodology is extended for\nstochastic PDE learning where all parameters and modeling error are considered\nas random variables. Hierarchical Bayesian Inference (HBI) is integrated with\nthe proposed framework for stochastic PDE learning from a population of\nobservations. Finally, the proposed PeSBL is demonstrated for system response\nprediction with uncertainties and anomaly diagnosis. Codes of all demonstrated\nexamples in this study are available on the website: https://github.com/ymlasu.",
          "link": "http://arxiv.org/abs/2107.07040",
          "publishedOn": "2021-07-16T00:48:25.449Z",
          "wordCount": 704,
          "title": "Parsimony-Enhanced Sparse Bayesian Learning for Robust Discovery of Partial Differential Equations. (arXiv:2107.07040v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zohdinasab_T/0/1/0/all/0/1\">Tahereh Zohdinasab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riccio_V/0/1/0/all/0/1\">Vincenzo Riccio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gambi_A/0/1/0/all/0/1\">Alessio Gambi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonella_P/0/1/0/all/0/1\">Paolo Tonella</a>",
          "description": "Deep Learning (DL) has been successfully applied to a wide range of\napplication domains, including safety-critical ones. Several DL testing\napproaches have been recently proposed in the literature but none of them aims\nto assess how different interpretable features of the generated inputs affect\nthe system's behaviour. In this paper, we resort to Illumination Search to find\nthe highest-performing test cases (i.e., misbehaving and closest to\nmisbehaving), spread across the cells of a map representing the feature space\nof the system. We introduce a methodology that guides the users of our approach\nin the tasks of identifying and quantifying the dimensions of the feature space\nfor a given domain. We developed DeepHyperion, a search-based tool for DL\nsystems that illuminates, i.e., explores at large, the feature space, by\nproviding developers with an interpretable feature map where automatically\ngenerated inputs are placed along with information about the exposed\nbehaviours.",
          "link": "http://arxiv.org/abs/2107.06997",
          "publishedOn": "2021-07-16T00:48:25.429Z",
          "wordCount": 629,
          "title": "DeepHyperion: Exploring the Feature Space of Deep Learning-Based Systems through Illumination Search. (arXiv:2107.06997v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07271",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Moyes_A/0/1/0/all/0/1\">Andrew Moyes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gault_R/0/1/0/all/0/1\">Richard Gault</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ming_J/0/1/0/all/0/1\">Ji Ming</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crookes_D/0/1/0/all/0/1\">Danny Crookes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>",
          "description": "Domain shift is a problem commonly encountered when developing automated\nhistopathology pipelines. The performance of machine learning models such as\nconvolutional neural networks within automated histopathology pipelines is\noften diminished when applying them to novel data domains due to factors\narising from differing staining and scanning protocols. The Dual-Channel\nAuto-Encoder (DCAE) model was previously shown to produce feature\nrepresentations that are less sensitive to appearance variation introduced by\ndifferent digital slide scanners. In this work, the Multi-Channel Auto-Encoder\n(MCAE) model is presented as an extension to DCAE which learns from more than\ntwo domains of data. Additionally, a synthetic dataset is generated using\nCycleGANs that contains aligned tissue images that have had their appearance\nsynthetically modified. Experimental results show that the MCAE model produces\nfeature representations that are less sensitive to inter-domain variations than\nthe comparative StaNoSA method when tested on the novel synthetic data.\nAdditionally, the MCAE and StaNoSA models are tested on a novel tissue\nclassification task. The results of this experiment show the MCAE model out\nperforms the StaNoSA model by 5 percentage-points in the f1-score. These\nresults show that the MCAE model is able to generalise better to novel data and\ntasks than existing approaches by actively learning normalised feature\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.07271",
          "publishedOn": "2021-07-16T00:48:25.382Z",
          "wordCount": 669,
          "title": "Multi-Channel Auto-Encoders and a Novel Dataset for Learning Domain Invariant Representations of Histopathology Images. (arXiv:2107.07271v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.06572",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cohen_M/0/1/0/all/0/1\">Michael B. Cohen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sidford_A/0/1/0/all/0/1\">Aaron Sidford</a>, <a href=\"http://arxiv.org/find/math/1/au:+Tian_K/0/1/0/all/0/1\">Kevin Tian</a>",
          "description": "We show that standard extragradient methods (i.e. mirror prox and dual\nextrapolation) recover optimal accelerated rates for first-order minimization\nof smooth convex functions. To obtain this result we provide a fine-grained\ncharacterization of the convergence rates of extragradient methods for solving\nmonotone variational inequalities in terms of a natural condition we call\nrelative Lipschitzness. We further generalize this framework to handle local\nand randomized notions of relative Lipschitzness and thereby recover rates for\nbox-constrained $\\ell_\\infty$ regression based on area convexity and complexity\nbounds achieved by accelerated (randomized) coordinate descent for smooth\nconvex function minimization.",
          "link": "http://arxiv.org/abs/2011.06572",
          "publishedOn": "2021-07-16T00:48:25.365Z",
          "wordCount": 581,
          "title": "Relative Lipschitzness in Extragradient Methods and a Direct Recipe for Acceleration. (arXiv:2011.06572v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neri_F/0/1/0/all/0/1\">Filippo Neri</a>",
          "description": "In the paper, we propose a novel methodology to map learning algorithms on\ndata (performance map) in order to gain more insights in the distribution of\ntheir performances across their parameter space. This methodology provides\nuseful information when selecting a learner's best configuration for the data\nat hand, and it also enhances the comparison of learners across learning\ncontexts. In order to explain the proposed methodology, the study introduces\nthe notions of learning context, performance map, and high performance\nfunction. It then applies these concepts to a variety of learning contexts to\nshow how their use can provide more insights in a learner's behavior, and can\nenhance the comparison of learners across learning contexts. The study is\ncompleted by an extensive experimental study describing how the proposed\nmethodology can be applied.",
          "link": "http://arxiv.org/abs/2107.06981",
          "publishedOn": "2021-07-16T00:48:25.343Z",
          "wordCount": 577,
          "title": "Mapping Learning Algorithms on Data, a useful step for optimizing performances and their comparison. (arXiv:2107.06981v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zetian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jason Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Leslie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peter Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Michelle A. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>",
          "description": "Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. It is a challenging yet crucial area\nwith numerous real-world applications in multimedia, affective computing,\nrobotics, finance, human-computer interaction, and healthcare. Unfortunately,\nmultimodal research has seen limited resources to study (1) generalization\nacross domains and modalities, (2) complexity during training and inference,\nand (3) robustness to noisy and missing modalities. In order to accelerate\nprogress towards understudied modalities and tasks while ensuring real-world\nrobustness, we release MultiBench, a systematic and unified large-scale\nbenchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6\nresearch areas. MultiBench provides an automated end-to-end machine learning\npipeline that simplifies and standardizes data loading, experimental setup, and\nmodel evaluation. To enable holistic evaluation, MultiBench offers a\ncomprehensive methodology to assess (1) generalization, (2) time and space\ncomplexity, and (3) modality robustness. MultiBench introduces impactful\nchallenges for future research, including scalability to large-scale multimodal\ndatasets and robustness to realistic imperfections. To accompany this\nbenchmark, we also provide a standardized implementation of 20 core approaches\nin multimodal learning. Simply applying methods proposed in different research\nareas can improve the state-of-the-art performance on 9/15 datasets. Therefore,\nMultiBench presents a milestone in unifying disjoint efforts in multimodal\nresearch and paves the way towards a better understanding of the capabilities\nand limitations of multimodal models, all the while ensuring ease of use,\naccessibility, and reproducibility. MultiBench, our standardized code, and\nleaderboards are publicly available, will be regularly updated, and welcomes\ninputs from the community.",
          "link": "http://arxiv.org/abs/2107.07502",
          "publishedOn": "2021-07-16T00:48:25.336Z",
          "wordCount": 722,
          "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07412",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sahmoud_T/0/1/0/all/0/1\">Thaer Sahmoud</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ashor_W/0/1/0/all/0/1\">Wesam Ashor</a>",
          "description": "Gaza Strip suffers from a chronic electricity deficit that affects all\nindustries including the telecommunication field, so there is a need to\noptimize and reduce power consumption of the telecommunication equipment. In\nthis paper we propose a new model that helps GSM radio frequency engineers to\nchoose the optimal value of hysteresis parameter for Ericsson BTS power saving\nalgorithm which aims to switch OFF unused frequency channels, our model is\nbased on unsupervised machine learning clustering K-means algorithm. By using\nour model with BTS power saving algorithm we reduce number of active TRX by\n20.9%.",
          "link": "http://arxiv.org/abs/2107.07412",
          "publishedOn": "2021-07-16T00:48:25.316Z",
          "wordCount": 543,
          "title": "Assign Hysteresis Parameter For Ericsson BTS Power Saving Algorithm Using Unsupervised Learning. (arXiv:2107.07412v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2002.00865",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Basioti_K/0/1/0/all/0/1\">Kalliopi Basioti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moustakides_G/0/1/0/all/0/1\">George V. Moustakides</a>",
          "description": "We are interested in the design of generative networks. The training of these\nmathematical structures is mostly performed with the help of adversarial\n(min-max) optimization problems. We propose a simple methodology for\nconstructing such problems assuring, at the same time, consistency of the\ncorresponding solution. We give characteristic examples developed by our\nmethod, some of which can be recognized from other applications, and some are\nintroduced here for the first time. We present a new metric, the likelihood\nratio, that can be employed online to examine the convergence and stability\nduring the training of different Generative Adversarial Networks (GANs).\nFinally, we compare various possibilities by applying them to well-known\ndatasets using neural networks of different configurations and sizes.",
          "link": "http://arxiv.org/abs/2002.00865",
          "publishedOn": "2021-07-16T00:48:25.301Z",
          "wordCount": 595,
          "title": "Designing GANs: A Likelihood Ratio Approach. (arXiv:2002.00865v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinyoung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bohyung Han</a>",
          "description": "We propose a generative adversarial network with multiple discriminators,\nwhere each discriminator is specialized to distinguish the subset of a real\ndataset. This approach facilitates learning a generator coinciding with the\nunderlying data distribution and thus mitigates the chronic mode collapse\nproblem. From the inspiration of multiple choice learning, we guide each\ndiscriminator to have expertise in the subset of the entire data and allow the\ngenerator to find reasonable correspondences between the latent and real data\nspaces automatically without supervision for training examples and the number\nof discriminators. Despite the use of multiple discriminators, the backbone\nnetworks are shared across the discriminators and the increase of training cost\nis minimized. We demonstrate the effectiveness of our algorithm in the standard\ndatasets using multiple evaluation metrics.",
          "link": "http://arxiv.org/abs/2107.07260",
          "publishedOn": "2021-07-16T00:48:25.283Z",
          "wordCount": 551,
          "title": "MCL-GAN: Generative Adversarial Networks with Multiple Specialized Discriminators. (arXiv:2107.07260v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07312",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tang_C/0/1/0/all/0/1\">Chong Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wenda Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vishwakarma_S/0/1/0/all/0/1\">Shelly Vishwakarma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_F/0/1/0/all/0/1\">Fangzhan Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Julier_S/0/1/0/all/0/1\">Simon Julier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chetty_K/0/1/0/all/0/1\">Kevin Chetty</a>",
          "description": "Micro-Doppler signatures contain considerable information about target\ndynamics. However, the radar sensing systems are easily affected by noisy\nsurroundings, resulting in uninterpretable motion patterns on the micro-Doppler\nspectrogram. Meanwhile, radar returns often suffer from multipath, clutter and\ninterference. These issues lead to difficulty in, for example motion feature\nextraction, activity classification using micro Doppler signatures ($\\mu$-DS),\netc. In this paper, we propose a latent feature-wise mapping strategy, called\nFeature Mapping Network (FMNet), to transform measured spectrograms so that\nthey more closely resemble the output from a simulation under the same\nconditions. Based on measured spectrogram and the matched simulated data, our\nframework contains three parts: an Encoder which is used to extract latent\nrepresentations/features, a Decoder outputs reconstructed spectrogram according\nto the latent features, and a Discriminator minimizes the distance of latent\nfeatures of measured and simulated data. We demonstrate the FMNet with six\nactivities data and two experimental scenarios, and final results show strong\nenhanced patterns and can keep actual motion information to the greatest\nextent. On the other hand, we also propose a novel idea which trains a\nclassifier with only simulated data and predicts new measured samples after\ncleaning them up with the FMNet. From final classification results, we can see\nsignificant improvements.",
          "link": "http://arxiv.org/abs/2107.07312",
          "publishedOn": "2021-07-16T00:48:25.277Z",
          "wordCount": 654,
          "title": "FMNet: Latent Feature-wise Mapping Network for Cleaning up Noisy Micro-Doppler Spectrogram. (arXiv:2107.07312v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runze Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haiyong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xuechun Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhiqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yida Zhu</a>",
          "description": "Human activity recognition (HAR) based on IMU sensors is an essential domain\nin ubiquitous computing. Because of the improving trend to deploy artificial\nintelligence into IoT devices or smartphones, more researchers design the HAR\nmodels for embedded devices. We propose a plug-and-play HAR modeling pipeline\nwith multi-level distillation to build deep convolutional HAR models with\nnative support of embedded devices. SMLDist consists of stage distillation,\nmemory distillation, and logits distillation, which covers all the information\nflow of the deep models. Stage distillation constrains the learning direction\nof the intermediate features. Memory distillation teaches the student models\nhow to explain and store the inner relationship between high-dimensional\nfeatures based on Hopfield networks. Logits distillation constructs distilled\nlogits by a smoothed conditional rule to keep the probable distribution and\nimprove the correctness of the soft target. We compare the performance of\naccuracy, F1 macro score, and energy cost on the embedded platform of various\nstate-of-the-art HAR frameworks with a MobileNet V3 model built by SMLDist. The\nproduced model has well balance with robustness, efficiency, and accuracy.\nSMLDist can also compress the models with minor performance loss in an equal\ncompression rate than other state-of-the-art knowledge distillation methods on\nseven public datasets.",
          "link": "http://arxiv.org/abs/2107.07331",
          "publishedOn": "2021-07-16T00:48:25.265Z",
          "wordCount": 674,
          "title": "Modeling Accurate Human Activity Recognition for Embedded Devices Using Multi-level Distillation. (arXiv:2107.07331v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yousefzadeh_A/0/1/0/all/0/1\">Amirreza Yousefzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifalakis_M/0/1/0/all/0/1\">Manolis Sifalakis</a>",
          "description": "Activation sparsity improves compute efficiency and resource utilization in\nsparsity-aware neural network accelerators. As the predominant operation in\nDNNs is multiply-accumulate (MAC) of activations with weights to compute inner\nproducts, skipping operations where (at least) one of the two operands is zero\ncan make inference more efficient in terms of latency and power. Spatial\nsparsification of activations is a popular topic in DNN literature and several\nmethods have already been established to bias a DNN for it. On the other hand,\ntemporal sparsity is an inherent feature of bio-inspired spiking neural\nnetworks (SNNs), which neuromorphic processing exploits for hardware\nefficiency. Introducing and exploiting spatio-temporal sparsity, is a topic\nmuch less explored in DNN literature, but in perfect resonance with the trend\nin DNN, to shift from static signal processing to more streaming signal\nprocessing. Towards this goal, in this paper we introduce a new DNN layer\n(called Delta Activation Layer), whose sole purpose is to promote temporal\nsparsity of activations during training. A Delta Activation Layer casts\ntemporal sparsity into spatial activation sparsity to be exploited when\nperforming sparse tensor multiplications in hardware. By employing delta\ninference and ``the usual'' spatial sparsification heuristics during training,\nthe resulting model learns to exploit not only spatial but also temporal\nactivation sparsity (for a given input data distribution). One may use the\nDelta Activation Layer either during vanilla training or during a refinement\nphase. We have implemented Delta Activation Layer as an extension of the\nstandard Tensoflow-Keras library, and applied it to train deep neural networks\non the Human Action Recognition (UCF101) dataset. We report an almost 3x\nimprovement of activation sparsity, with recoverable loss of model accuracy\nafter longer training.",
          "link": "http://arxiv.org/abs/2107.07305",
          "publishedOn": "2021-07-16T00:48:25.258Z",
          "wordCount": 728,
          "title": "Training for temporal sparsity in deep neural networks, application in video processing. (arXiv:2107.07305v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoang_L/0/1/0/all/0/1\">L&#xea;-Nguy&#xea;n Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faucon_L/0/1/0/all/0/1\">Louis Faucon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jungo_A/0/1/0/all/0/1\">Aidan Jungo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volodin_S/0/1/0/all/0/1\">Sergei Volodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papuc_D/0/1/0/all/0/1\">Dalia Papuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liossatos_O/0/1/0/all/0/1\">Orfeas Liossatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crulis_B/0/1/0/all/0/1\">Ben Crulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighanimine_M/0/1/0/all/0/1\">Mariame Tighanimine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constantin_I/0/1/0/all/0/1\">Isabela Constantin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kucherenko_A/0/1/0/all/0/1\">Anastasiia Kucherenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurer_A/0/1/0/all/0/1\">Alexandre Maurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimberg_F/0/1/0/all/0/1\">Felix Grimberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitu_V/0/1/0/all/0/1\">Vlad Nitu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vossen_C/0/1/0/all/0/1\">Chris Vossen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouault_S/0/1/0/all/0/1\">S&#xe9;bastien Rouault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Mhamdi_E/0/1/0/all/0/1\">El-Mahdi El-Mhamdi</a>",
          "description": "Today's large-scale algorithms have become immensely influential, as they\nrecommend and moderate the content that billions of humans are exposed to on a\ndaily basis. They are the de-facto regulators of our societies' information\ndiet, from shaping opinions on public health to organizing groups for social\nmovements. This creates serious concerns, but also great opportunities to\npromote quality information. Addressing the concerns and seizing the\nopportunities is a challenging, enormous and fabulous endeavor, as intuitively\nappealing ideas often come with unwanted {\\it side effects}, and as it requires\nus to think about what we deeply prefer.\n\nUnderstanding how today's large-scale algorithms are built is critical to\ndetermine what interventions will be most effective. Given that these\nalgorithms rely heavily on {\\it machine learning}, we make the following key\nobservation: \\emph{any algorithm trained on uncontrolled data must not be\ntrusted}. Indeed, a malicious entity could take control over the data, poison\nit with dangerously manipulative fabricated inputs, and thereby make the\ntrained algorithm extremely unsafe. We thus argue that the first step towards\nsafe and ethical large-scale algorithms must be the collection of a large,\nsecure and trustworthy dataset of reliable human judgments.\n\nTo achieve this, we introduce \\emph{Tournesol}, an open source platform\navailable at \\url{https://tournesol.app}. Tournesol aims to collect a large\ndatabase of human judgments on what algorithms ought to widely recommend (and\nwhat they ought to stop widely recommending). We outline the structure of the\nTournesol database, the key features of the Tournesol platform and the main\nhurdles that must be overcome to make it a successful project. Most\nimportantly, we argue that, if successful, Tournesol may then serve as the\nessential foundation for any safe and ethical large-scale algorithm.",
          "link": "http://arxiv.org/abs/2107.07334",
          "publishedOn": "2021-07-16T00:48:25.252Z",
          "wordCount": 766,
          "title": "Tournesol: A quest for a large, secure and trustworthy database of reliable human judgments. (arXiv:2107.07334v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07322",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyu Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_R/0/1/0/all/0/1\">Ruodu Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1\">Aaditya Ramdas</a>",
          "description": "In bandit multiple hypothesis testing, each arm corresponds to a different\nnull hypothesis that we wish to test, and the goal is to design adaptive\nalgorithms that correctly identify large set of interesting arms (true\ndiscoveries), while only mistakenly identifying a few uninteresting ones (false\ndiscoveries). One common metric in non-bandit multiple testing is the false\ndiscovery rate (FDR). We propose a unified, modular framework for bandit FDR\ncontrol that emphasizes the decoupling of exploration and summarization of\nevidence. We utilize the powerful martingale-based concept of ``e-processes''\nto ensure FDR control for arbitrary composite nulls, exploration rules and\nstopping times in generic problem settings. In particular, valid FDR control\nholds even if the reward distributions of the arms could be dependent, multiple\narms may be queried simultaneously, and multiple (cooperating or competing)\nagents may be querying arms, covering combinatorial semi-bandit type settings\nas well. Prior work has considered in great detail the setting where each arm's\nreward distribution is independent and sub-Gaussian, and a single arm is\nqueried at each step. Our framework recovers matching sample complexity\nguarantees in this special case, and performs comparably or better in practice.\nFor other settings, sample complexities will depend on the finer details of the\nproblem (composite nulls being tested, exploration algorithm, data dependence\nstructure, stopping rule) and we do not explore these; our contribution is to\nshow that the FDR guarantee is clean and entirely agnostic to these details.",
          "link": "http://arxiv.org/abs/2107.07322",
          "publishedOn": "2021-07-16T00:48:25.245Z",
          "wordCount": 675,
          "title": "A unified framework for bandit multiple testing. (arXiv:2107.07322v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_L/0/1/0/all/0/1\">Lennart Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfisterer_F/0/1/0/all/0/1\">Florian Pfisterer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_M/0/1/0/all/0/1\">Martin Binder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>",
          "description": "Neural architecture search (NAS) promises to make deep learning accessible to\nnon-experts by automating architecture engineering of deep neural networks.\nBANANAS is one state-of-the-art NAS method that is embedded within the Bayesian\noptimization framework. Recent experimental findings have demonstrated the\nstrong performance of BANANAS on the NAS-Bench-101 benchmark being determined\nby its path encoding and not its choice of surrogate model. We present\nexperimental results suggesting that the performance of BANANAS on the\nNAS-Bench-301 benchmark is determined by its acquisition function optimizer,\nwhich minimally mutates the incumbent.",
          "link": "http://arxiv.org/abs/2107.07343",
          "publishedOn": "2021-07-16T00:48:25.203Z",
          "wordCount": 535,
          "title": "Mutation is all you need. (arXiv:2107.07343v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "The increasing population of elderly people is associated with the need to\nmeet their increasing requirements and to provide solutions that can improve\ntheir quality of life in a smart home. In addition to fear and anxiety towards\ninterfacing with systems; cognitive disabilities, weakened memory, disorganized\nbehavior and even physical limitations are some of the problems that elderly\npeople tend to face with increasing age. The essence of providing\ntechnology-based solutions to address these needs of elderly people and to\ncreate smart and assisted living spaces for the elderly; lies in developing\nsystems that can adapt by addressing their diversity and can augment their\nperformances in the context of their day to day goals. Therefore, this work\nproposes a framework for development of a Personalized Intelligent Assistant to\nhelp elderly people perform Activities of Daily Living (ADLs) in a smart and\nconnected Internet of Things (IoT) based environment. This Personalized\nIntelligent Assistant can analyze different tasks performed by the user and\nrecommend activities by considering their daily routine, current affective\nstate and the underlining user experience. To uphold the efficacy of this\nproposed framework, it has been tested on a couple of datasets for modelling an\naverage user and a specific user respectively. The results presented show that\nthe model achieves a performance accuracy of 73.12% when modelling a specific\nuser, which is considerably higher than its performance while modelling an\naverage user, this upholds the relevance for development and implementation of\nthis proposed framework.",
          "link": "http://arxiv.org/abs/2107.07344",
          "publishedOn": "2021-07-16T00:48:25.194Z",
          "wordCount": 731,
          "title": "Framework for A Personalized Intelligent Assistant to Elderly People for Activities of Daily Living. (arXiv:2107.07344v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yufeng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tingsong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>",
          "description": "Deep neural networks (DNNs) have successfully learned useful data\nrepresentations in various tasks, however, assessing the reliability of these\nrepresentations remains a challenge. Deep Ensemble is widely considered the\nstate-of-the-art method for uncertainty estimation, but it is very expensive to\ntrain and test. MC-Dropout is another alternative method, which is less\nexpensive but lacks the diversity of predictions. To get more diverse\npredictions in less time, we introduce Randomized ReLU Activation (RRA)\nframework. Under the framework, we propose two strategies, MC-DropReLU and\nMC-RReLU, to estimate uncertainty. Instead of randomly dropping some neurons of\nthe network as in MC-Dropout, the RRA framework adds randomness to the\nactivation function module, making the outputs diverse. As far as we know, this\nis the first attempt to add randomness to the activation function module to\ngenerate predictive uncertainty. We analyze and compare the output diversity of\nMC-Dropout and our method from the variance perspective and obtain the\nrelationship between the hyperparameters and output diversity in the two\nmethods. Moreover, our method is simple to implement and does not need to\nmodify the existing model. We experimentally validate the RRA framework on\nthree widely used datasets, CIFAR10, CIFAR100, and TinyImageNet. The\nexperiments demonstrate that our method has competitive performance but is more\nfavorable in training time and memory requirements.",
          "link": "http://arxiv.org/abs/2107.07197",
          "publishedOn": "2021-07-16T00:48:25.172Z",
          "wordCount": 649,
          "title": "Randomized ReLU Activation for Uncertainty Estimation of Deep Neural Networks. (arXiv:2107.07197v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kostic_Z/0/1/0/all/0/1\">Zona Kostic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jevremovic_A/0/1/0/all/0/1\">Aleksandar Jevremovic</a>",
          "description": "The attractiveness of a property is one of the most interesting, yet\nchallenging, categories to model. Image characteristics are used to describe\ncertain attributes, and to examine the influence of visual factors on the price\nor timeframe of the listing. In this paper, we propose a set of techniques for\nthe extraction of visual features for efficient numerical inclusion in\nmodern-day predictive algorithms. We discuss techniques such as Shannon's\nentropy, calculating the center of gravity, employing image segmentation, and\nusing Convolutional Neural Networks. After comparing these techniques as\napplied to a set of property-related images (indoor, outdoor, and satellite),\nwe conclude the following: (i) the entropy is the most efficient single-digit\nvisual measure for housing price prediction; (ii) image segmentation is the\nmost important visual feature for the prediction of housing lifespan; and (iii)\ndeep image features can be used to quantify interior characteristics and\ncontribute to captivation modeling. The set of 40 image features selected here\ncarries a significant amount of predictive power and outperforms some of the\nstrongest metadata predictors. Without any need to replace a human expert in a\nreal-estate appraisal process, we conclude that the techniques presented in\nthis paper can efficiently describe visible characteristics, thus introducing\nperceived attractiveness as a quantitative measure into the predictive modeling\nof housing.",
          "link": "http://arxiv.org/abs/2107.07148",
          "publishedOn": "2021-07-16T00:48:25.154Z",
          "wordCount": 649,
          "title": "What Image Features Boost Housing Market Predictions?. (arXiv:2107.07148v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Egg_A/0/1/0/all/0/1\">Alex Egg</a>",
          "description": "We propose a method to easily modify existing offline Recommender Systems to\nrun online using Transfer Learning. Online Learning for Recommender Systems has\ntwo main advantages: quality and scale. Like many Machine Learning algorithms\nin production if not regularly retrained will suffer from Concept Drift. A\npolicy that is updated frequently online can adapt to drift faster than a batch\nsystem. This is especially true for user-interaction systems like recommenders\nwhere the underlying distribution can shift drastically to follow user\nbehaviour. As a platform grows rapidly like Grubhub, the cost of running batch\ntraining jobs becomes material. A shift from stateless batch learning offline\nto stateful incremental learning online can recover, for example, at Grubhub,\nup to a 45x cost savings and a +20% metrics increase. There are a few\nchallenges to overcome with the transition to online stateful learning, namely\nconvergence, non-stationary embeddings and off-policy evaluation, which we\nexplore from our experiences running this system in production.",
          "link": "http://arxiv.org/abs/2107.07106",
          "publishedOn": "2021-07-16T00:48:25.137Z",
          "wordCount": 583,
          "title": "Online Learning for Recommendations at Grubhub. (arXiv:2107.07106v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ruijuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Maolin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_F/0/1/0/all/0/1\">Feng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xinlei Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>",
          "description": "Federated learning enables a large number of clients to participate in\nlearning a shared model while maintaining the training data stored in each\nclient, which protects data privacy and security. Till now, federated learning\nframeworks are built in a centralized way, in which a central client is needed\nfor collecting and distributing information from every other client. This not\nonly leads to high communication pressure at the central client, but also\nrenders the central client highly vulnerable to failure and attack. Here we\npropose a principled decentralized federated learning algorithm (DeFed), which\nremoves the central client in the classical Federated Averaging (FedAvg)\nsetting and only relies information transmission between clients and their\nlocal neighbors. The proposed DeFed algorithm is proven to reach the global\nminimum with a convergence rate of $O(1/T)$ when the loss function is smooth\nand strongly convex, where $T$ is the number of iterations in gradient descent.\nFinally, the proposed algorithm has been applied to a number of toy examples to\ndemonstrate its effectiveness.",
          "link": "http://arxiv.org/abs/2107.07171",
          "publishedOn": "2021-07-16T00:48:25.121Z",
          "wordCount": 621,
          "title": "DeFed: A Principled Decentralized and Privacy-Preserving Federated Learning Algorithm. (arXiv:2107.07171v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kevin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_A/0/1/0/all/0/1\">Ashwin Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pong_V/0/1/0/all/0/1\">Vitchyr Pong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aurick Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Justin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Exploration in reinforcement learning is a challenging problem: in the worst\ncase, the agent must search for reward states that could be hidden anywhere in\nthe state space. Can we define a more tractable class of RL problems, where the\nagent is provided with examples of successful outcomes? In this problem\nsetting, the reward function can be obtained automatically by training a\nclassifier to categorize states as successful or not. If trained properly, such\na classifier can not only afford a reward function, but actually provide a\nwell-shaped objective landscape that both promotes progress toward good states\nand provides a calibrated exploration bonus. In this work, we we show that an\nuncertainty aware classifier can solve challenging reinforcement learning\nproblems by both encouraging exploration and provided directed guidance towards\npositive outcomes. We propose a novel mechanism for obtaining these calibrated,\nuncertainty-aware classifiers based on an amortized technique for computing the\nnormalized maximum likelihood (NML) distribution, also showing how these\ntechniques can be made computationally tractable by leveraging tools from\nmeta-learning. We show that the resulting algorithm has a number of intriguing\nconnections to both count-based exploration methods and prior algorithms for\nlearning reward functions, while also providing more effective guidance towards\nthe goal. We demonstrate that our algorithm solves a number of challenging\nnavigation and robotic manipulation tasks which prove difficult or impossible\nfor prior methods.",
          "link": "http://arxiv.org/abs/2107.07184",
          "publishedOn": "2021-07-16T00:48:25.115Z",
          "wordCount": 675,
          "title": "MURAL: Meta-Learning Uncertainty-Aware Rewards for Outcome-Driven Reinforcement Learning. (arXiv:2107.07184v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ororbia_A/0/1/0/all/0/1\">Alexander Ororbia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mali_A/0/1/0/all/0/1\">Ankur Mali</a>",
          "description": "In humans, perceptual awareness facilitates the fast recognition and\nextraction of information from sensory input. This awareness largely depends on\nhow the human agent interacts with the environment. In this work, we propose\nactive neural generative coding, a computational framework for learning\naction-driven generative models without backpropagation of errors (backprop) in\ndynamic environments. Specifically, we develop an intelligent agent that\noperates even with sparse rewards, drawing inspiration from the cognitive\ntheory of planning as inference. We demonstrate on several control problems, in\nthe online learning setting, that our proposed modeling framework performs\ncompetitively with deep Q-learning models. The robust performance of our agent\noffers promising evidence that a backprop-free approach for neural inference\nand learning can drive goal-directed behavior.",
          "link": "http://arxiv.org/abs/2107.07046",
          "publishedOn": "2021-07-16T00:48:25.095Z",
          "wordCount": 545,
          "title": "Backprop-Free Reinforcement Learning with Active Neural Generative Coding. (arXiv:2107.07046v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bicheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bailian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harp_D/0/1/0/all/0/1\">Dylan Robert Harp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawar_R/0/1/0/all/0/1\">Rajesh J. Pawar</a>",
          "description": "This paper contributes to the development and evaluation of a deep learning\nworkflow that accurately and efficiently predicts the temporal-spatial\nevolution of pressure and CO2 plumes during injection and post-injection\nperiods of geologic CO2 sequestration (GCS) operations. Based on a Fourier\nNeuron Operator, the deep learning workflow takes input variables or features\nincluding rock properties, well operational controls and time steps, and\npredicts the state variables of pressure and CO2 saturation. To further improve\nthe predictive fidelity, separate deep learning models are trained for CO2\ninjection and post-injection periods due the difference in primary driving\nforce of fluid flow and transport during these two phases. We also explore\ndifferent combinations of features to predict the state variables. We use a\nrealistic example of CO2 injection and storage in a 3D heterogeneous saline\naquifer, and apply the deep learning workflow that is trained from\nphysics-based simulation data and emulate the physics process. Through this\nnumerical experiment, we demonstrate that using two separate deep learning\nmodels to distinguish post-injection from injection period generates the most\naccurate prediction of pressure, and a single deep learning model of the whole\nGCS process including the cumulative injection volume of CO2 as a deep learning\nfeature, leads to the most accurate prediction of CO2 saturation. For the\npost-injection period, it is key to use cumulative CO2 injection volume to\ninform the deep learning models about the total carbon storage when predicting\neither pressure or saturation. The deep learning workflow not only provides\nhigh predictive fidelity across temporal and spatial scales, but also offers a\nspeedup of 250 times compared to full physics reservoir simulation, and thus\nwill be a significant predictive tool for engineers to manage the long term\nprocess of GCS.",
          "link": "http://arxiv.org/abs/2107.07274",
          "publishedOn": "2021-07-16T00:48:25.087Z",
          "wordCount": 747,
          "title": "A Robust Deep Learning Workflow to Predict Multiphase Flow Behavior during Geological CO2 Sequestration Injection and Post-Injection Periods. (arXiv:2107.07274v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1\">Efthymios Tzinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casebeer_J/0/1/0/all/0/1\">Jonah Casebeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhepei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1\">Paris Smaragdis</a>",
          "description": "We propose FEDENHANCE, an unsupervised federated learning (FL) approach for\nspeech enhancement and separation with non-IID distributed data across multiple\nclients. We simulate a real-world scenario where each client only has access to\na few noisy recordings from a limited and disjoint number of speakers (hence\nnon-IID). Each client trains their model in isolation using mixture invariant\ntraining while periodically providing updates to a central server. Our\nexperiments show that our approach achieves competitive enhancement performance\ncompared to IID training on a single device and that we can further facilitate\nthe convergence speed and the overall performance using transfer learning on\nthe server-side. Moreover, we show that we can effectively combine updates from\nclients trained locally with supervised and unsupervised losses. We also\nrelease a new dataset LibriFSD50K and its creation recipe in order to\nfacilitate FL research for source separation problems.",
          "link": "http://arxiv.org/abs/2105.04727",
          "publishedOn": "2021-07-16T00:48:25.080Z",
          "wordCount": 627,
          "title": "Separate but Together: Unsupervised Federated Learning for Speech Enhancement from Non-IID Data. (arXiv:2105.04727v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Galeano_S/0/1/0/all/0/1\">Sergio Rojas-Galeano</a>",
          "description": "One of the stratagems used to deceive spam filters is to substitute vocables\nwith synonyms or similar words that turn the message unrecognisable by the\ndetection algorithms. In this paper we investigate whether the recent\ndevelopment of language models sensitive to the semantics and context of words,\nsuch as Google's BERT, may be useful to overcome this adversarial attack\n(called \"Mad-lib\" as per the word substitution game). Using a dataset of 5572\nSMS spam messages, we first established a baseline of detection performance\nusing widely known document representation models (BoW and TFIDF) and the novel\nBERT model, coupled with a variety of classification algorithms (Decision Tree,\nkNN, SVM, Logistic Regression, Naive Bayes, Multilayer Perceptron). Then, we\nbuilt a thesaurus of the vocabulary contained in these messages, and set up a\nMad-lib attack experiment in which we modified each message of a held out\nsubset of data (not used in the baseline experiment) with different rates of\nsubstitution of original words with synonyms from the thesaurus. Lastly, we\nevaluated the detection performance of the three representation models (BoW,\nTFIDF and BERT) coupled with the best classifier from the baseline experiment\n(SVM). We found that the classic models achieved a 94% Balanced Accuracy (BA)\nin the original dataset, whereas the BERT model obtained 96%. On the other\nhand, the Mad-lib attack experiment showed that BERT encodings manage to\nmaintain a similar BA performance of 96% with an average substitution rate of\n1.82 words per message, and 95% with 3.34 words substituted per message. In\ncontrast, the BA performance of the BoW and TFIDF encoders dropped to chance.\nThese results hint at the potential advantage of BERT models to combat these\ntype of ingenious attacks, offsetting to some extent for the inappropriate use\nof semantic relationships in language.",
          "link": "http://arxiv.org/abs/2107.06400",
          "publishedOn": "2021-07-16T00:48:25.059Z",
          "wordCount": 736,
          "title": "Using BERT Encoding to Tackle the Mad-lib Attack in SMS Spam Detection. (arXiv:2107.06400v1 [cs.CL] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verine_A/0/1/0/all/0/1\">Alexandre Verine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negrevergne_B/0/1/0/all/0/1\">Benjamin Negrevergne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1\">Fabrice Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chevaleyre_Y/0/1/0/all/0/1\">Yann Chevaleyre</a>",
          "description": "An invertible function is bi-Lipschitz if both the function and its inverse\nhave bounded Lipschitz constants. Nowadays, most Normalizing Flows are\nbi-Lipschitz by design or by training to limit numerical errors (among other\nthings). In this paper, we discuss the expressivity of bi-Lipschitz Normalizing\nFlows and identify several target distributions that are difficult to\napproximate using such models. Then, we characterize the expressivity of\nbi-Lipschitz Normalizing Flows by giving several lower bounds on the Total\nVariation distance between these particularly unfavorable distributions and\ntheir best possible approximation. Finally, we discuss potential remedies which\ninclude using more complex latent distributions.",
          "link": "http://arxiv.org/abs/2107.07232",
          "publishedOn": "2021-07-16T00:48:25.051Z",
          "wordCount": 532,
          "title": "On the expressivity of bi-Lipschitz normalizing flows. (arXiv:2107.07232v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.07249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Creager_E/0/1/0/all/0/1\">Elliot Creager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobsen_J/0/1/0/all/0/1\">J&#xf6;rn-Henrik Jacobsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>",
          "description": "Learning models that gracefully handle distribution shifts is central to\nresearch on domain generalization, robust optimization, and fairness. A\npromising formulation is domain-invariant learning, which identifies the key\nissue of learning which features are domain-specific versus domain-invariant.\nAn important assumption in this area is that the training examples are\npartitioned into \"domains\" or \"environments\". Our focus is on the more common\nsetting where such partitions are not provided. We propose EIIL, a general\nframework for domain-invariant learning that incorporates Environment Inference\nto directly infer partitions that are maximally informative for downstream\nInvariant Learning. We show that EIIL outperforms invariant learning methods on\nthe CMNIST benchmark without using environment labels, and significantly\noutperforms ERM on worst-group performance in the Waterbirds and CivilComments\ndatasets. Finally, we establish connections between EIIL and algorithmic\nfairness, which enables EIIL to improve accuracy and calibration in a fair\nprediction problem.",
          "link": "http://arxiv.org/abs/2010.07249",
          "publishedOn": "2021-07-16T00:48:25.044Z",
          "wordCount": 626,
          "title": "Environment Inference for Invariant Learning. (arXiv:2010.07249v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bunne_C/0/1/0/all/0/1\">Charlotte Bunne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Papaxanthos_L/0/1/0/all/0/1\">Laetitia Meng-Papaxanthos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1\">Andreas Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuturi_M/0/1/0/all/0/1\">Marco Cuturi</a>",
          "description": "Consider a heterogeneous population of points evolving with time. While the\npopulation evolves, both in size and nature, we can observe it periodically,\nthrough snapshots taken at different timestamps. Each of these snapshots is\nformed by sampling points from the population at that time, and then creating\nfeatures to recover point clouds. While these snapshots describe the\npopulation's evolution on aggregate, they do not provide directly insights on\nindividual trajectories. This scenario is encountered in several applications,\nnotably single-cell genomics experiments, tracking of particles, or when\nstudying crowd motion. In this paper, we propose to model that dynamic as\nresulting from the celebrated Jordan-Kinderlehrer-Otto (JKO) proximal scheme.\nThe JKO scheme posits that the configuration taken by a population at time $t$\nis one that trades off a decrease w.r.t. an energy (the model we seek to learn)\npenalized by an optimal transport distance w.r.t. the previous configuration.\nTo that end, we propose JKOnet, a neural architecture that combines an energy\nmodel on measures, with (small) optimal displacements solved with input convex\nneural networks (ICNN). We demonstrate the applicability of our model to\nexplain and predict population dynamics.",
          "link": "http://arxiv.org/abs/2106.06345",
          "publishedOn": "2021-07-16T00:48:25.020Z",
          "wordCount": 644,
          "title": "JKOnet: Proximal Optimal Transport Modeling of Population Dynamics. (arXiv:2106.06345v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04150",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yilin Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+DeJong_J/0/1/0/all/0/1\">Jennifer DeJong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Halverson_T/0/1/0/all/0/1\">Tom Halverson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shuman_D/0/1/0/all/0/1\">David I Shuman</a>",
          "description": "Ranked data sets, where m judges/voters specify a preference ranking of n\nobjects/candidates, are increasingly prevalent in contexts such as political\nelections, computer vision, recommender systems, and bioinformatics. The vote\ncounts for each ranking can be viewed as an n! data vector lying on the\npermutahedron, which is a Cayley graph of the symmetric group with vertices\nlabeled by permutations and an edge when two permutations differ by an adjacent\ntransposition. Leveraging combinatorial representation theory and recent\nprogress in signal processing on graphs, we investigate a novel, scalable\ntransform method to interpret and exploit structure in ranked data. We\nrepresent data on the permutahedron using an overcomplete dictionary of atoms,\neach of which captures both smoothness information about the data (typically\nthe focus of spectral graph decomposition methods in graph signal processing)\nand structural information about the data (typically the focus of symmetry\ndecomposition methods from representation theory). These atoms have a more\nnaturally interpretable structure than any known basis for signals on the\npermutahedron, and they form a Parseval frame, ensuring beneficial numerical\nproperties such as energy preservation. We develop specialized algorithms and\nopen software that take advantage of the symmetry and structure of the\npermutahedron to improve the scalability of the proposed method, making it more\napplicable to the high-dimensional ranked data found in applications.",
          "link": "http://arxiv.org/abs/2103.04150",
          "publishedOn": "2021-07-16T00:48:25.014Z",
          "wordCount": 687,
          "title": "Signal Processing on the Permutahedron: Tight Spectral Frames for Ranked Data Analysis. (arXiv:2103.04150v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01708",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1\">Stephen Y. Zhang</a>",
          "description": "Non-negative matrix and tensor factorisations are a classical tool for\nfinding low-dimensional representations of high-dimensional datasets. In\napplications such as imaging, datasets can be regarded as distributions\nsupported on a space with metric structure. In such a setting, a loss function\nbased on the Wasserstein distance of optimal transportation theory is a natural\nchoice since it incorporates the underlying geometry of the data. We introduce\na general mathematical framework for computing non-negative factorisations of\nboth matrices and tensors with respect to an optimal transport loss. We derive\nan efficient computational method for its solution using a convex dual\nformulation, and demonstrate the applicability of this approach with several\nnumerical illustrations with both matrix and tensor-valued data.",
          "link": "http://arxiv.org/abs/2104.01708",
          "publishedOn": "2021-07-16T00:48:24.998Z",
          "wordCount": 584,
          "title": "A unified framework for non-negative matrix and tensor factorisations with a smoothed Wasserstein loss. (arXiv:2104.01708v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_L/0/1/0/all/0/1\">Lucas F. F. Cardoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_V/0/1/0/all/0/1\">Vitor C. A. Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frances_R/0/1/0/all/0/1\">Regiane S. Kawasaki Franc&#xea;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prudencio_R/0/1/0/all/0/1\">Ricardo B. C. Prud&#xea;ncio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alves_R/0/1/0/all/0/1\">Ronnie C. O. Alves</a>",
          "description": "The classification experiments covered by machine learning (ML) are composed\nby two important parts: the data and the algorithm. As they are a fundamental\npart of the problem, both must be considered when evaluating a model's\nperformance against a benchmark. The best classifiers need robust benchmarks to\nbe properly evaluated. For this, gold standard benchmarks such as OpenML-CC18\nare used. However, data complexity is commonly not considered along with the\nmodel during a performance evaluation. Recent studies employ Item Response\nTheory (IRT) as a new approach to evaluating datasets and algorithms, capable\nof evaluating both simultaneously. This work presents a new evaluation\nmethodology based on IRT and Glicko-2, jointly with the decodIRT tool developed\nto guide the estimation of IRT in ML. It explores the IRT as a tool to evaluate\nthe OpenML-CC18 benchmark for its algorithmic evaluation capability and checks\nif there is a subset of datasets more efficient than the original benchmark.\nSeveral classifiers, from classics to ensemble, are also evaluated using the\nIRT models. The Glicko-2 rating system was applied together with IRT to\nsummarize the innate ability and classifiers performance. It was noted that not\nall OpenML-CC18 datasets are really useful for evaluating algorithms, where\nonly 10% were rated as being really difficult. Furthermore, it was verified the\nexistence of a more efficient subset containing only 50% of the original size.\nWhile Randon Forest was singled out as the algorithm with the best innate\nability.",
          "link": "http://arxiv.org/abs/2107.07451",
          "publishedOn": "2021-07-16T00:48:24.981Z",
          "wordCount": 685,
          "title": "Data vs classifiers, who wins?. (arXiv:2107.07451v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2003.05383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Sheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe-Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-An Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Siang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Ya-Liang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>",
          "description": "We study the XAI (explainable AI) on the face recognition task, particularly\nthe face verification here. Face verification is a crucial task in recent days\nand it has been deployed to plenty of applications, such as access control,\nsurveillance, and automatic personal log-on for mobile devices. With the\nincreasing amount of data, deep convolutional neural networks can achieve very\nhigh accuracy for the face verification task. Beyond exceptional performances,\ndeep face verification models need more interpretability so that we can trust\nthe results they generate. In this paper, we propose a novel similarity metric,\ncalled explainable cosine ($xCos$), that comes with a learnable module that can\nbe plugged into most of the verification models to provide meaningful\nexplanations. With the help of $xCos$, we can see which parts of the two input\nfaces are similar, where the model pays its attention to, and how the local\nsimilarities are weighted to form the output $xCos$ score. We demonstrate the\neffectiveness of our proposed method on LFW and various competitive benchmarks,\nresulting in not only providing novel and desiring model interpretability for\nface verification but also ensuring the accuracy as plugging into existing face\nrecognition models.",
          "link": "http://arxiv.org/abs/2003.05383",
          "publishedOn": "2021-07-16T00:48:24.971Z",
          "wordCount": 699,
          "title": "xCos: An Explainable Cosine Metric for Face Verification Task. (arXiv:2003.05383v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tong_G/0/1/0/all/0/1\">Guangmo Tong</a>",
          "description": "Real-world decision-making systems are often subject to uncertainties that\nhave to be resolved through observational data. Therefore, we are frequently\nconfronted with combinatorial optimization problems of which the objective\nfunction is unknown and thus has to be debunked using empirical evidence. In\ncontrast to the common practice that relies on a learning-and-optimization\nstrategy, we consider the regression between combinatorial spaces, aiming to\ninfer high-quality optimization solutions from samples of input-solution pairs\n-- without the need to learn the objective function. Our main deliverable is a\nuniversal solver that is able to handle abstract undetermined stochastic\ncombinatorial optimization problems. For learning foundations, we present\nlearning-error analysis under the PAC-Bayesian framework using a new\nmargin-based analysis. In empirical studies, we demonstrate our design using\nproof-of-concept experiments, and compare it with other methods that are\npotentially applicable. Overall, we obtain highly encouraging experimental\nresults for several classic combinatorial problems on both synthetic and\nreal-world datasets.",
          "link": "http://arxiv.org/abs/2107.07508",
          "publishedOn": "2021-07-16T00:48:24.957Z",
          "wordCount": 574,
          "title": "USCO-Solver: Solving Undetermined Stochastic Combinatorial Optimization Problems. (arXiv:2107.07508v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chimmwal_N/0/1/0/all/0/1\">Neeraj Chimmwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1\">Rishabh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>",
          "description": "We present a CLSRIL-23, a self supervised learning based audio pre-trained\nmodel which learns cross lingual speech representations from raw audio across\n23 Indic languages. It is built on top of wav2vec 2.0 which is solved by\ntraining a contrastive task over masked latent speech representations and\njointly learns the quantization of latents shared across all languages. We\ncompare the language wise loss during pretraining to compare effects of\nmonolingual and multilingual pretraining. Performance on some downstream\nfine-tuning tasks for speech recognition is also compared and our experiments\nshow that multilingual pretraining outperforms monolingual training, in terms\nof learning speech representations which encodes phonetic similarity of\nlanguages and also in terms of performance on down stream tasks. A decrease of\n5% is observed in WER and 9.5% in CER when a multilingual pretrained model is\nused for finetuning in Hindi. All the code models are also open sourced.\nCLSRIL-23 is a model trained on $23$ languages and almost 10,000 hours of audio\ndata to facilitate research in speech recognition for Indic languages. We hope\nthat new state of the art systems will be created using the self supervised\napproach, especially for low resources Indic languages.",
          "link": "http://arxiv.org/abs/2107.07402",
          "publishedOn": "2021-07-16T00:48:24.904Z",
          "wordCount": 652,
          "title": "CLSRIL-23: Cross Lingual Speech Representations for Indic Languages. (arXiv:2107.07402v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rampasek_L/0/1/0/all/0/1\">Ladislav Ramp&#xe1;&#x161;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1\">Guy Wolf</a>",
          "description": "Graph neural networks (GNNs) based on message passing between neighboring\nnodes are known to be insufficient for capturing long-range interactions in\ngraphs. In this project we study hierarchical message passing models that\nleverage a multi-resolution representation of a given graph. This facilitates\nlearning of features that span large receptive fields without loss of local\ninformation, an aspect not studied in preceding work on hierarchical GNNs. We\nintroduce Hierarchical Graph Net (HGNet), which for any two connected nodes\nguarantees existence of message-passing paths of at most logarithmic length\nw.r.t. the input graph size. Yet, under mild assumptions, its internal\nhierarchy maintains asymptotic size equivalent to that of the input graph. We\nobserve that our HGNet outperforms conventional stacking of GCN layers\nparticularly in molecular property prediction benchmarks. Finally, we propose\ntwo benchmarking tasks designed to elucidate capability of GNNs to leverage\nlong-range interactions in graphs.",
          "link": "http://arxiv.org/abs/2107.07432",
          "publishedOn": "2021-07-16T00:48:24.898Z",
          "wordCount": 575,
          "title": "Hierarchical graph neural nets can capture long-range interactions. (arXiv:2107.07432v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Han-Chih Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>",
          "description": "The development of active and passive biometric authentication and\nidentification technology plays an increasingly important role in\ncybersecurity. Keystroke dynamics can be used to analyze the way that a user\ntypes based on various keyboard input. Previous work has shown that user\nauthentication and classification can be achieved based on keystroke dynamics.\nIn this research, we consider the problem of user classification based on\nkeystroke dynamics features collected from free-text. We implement and analyze\na novel a deep learning model that combines a convolutional neural network\n(CNN) and a gated recurrent unit (GRU). We optimize the resulting model and\nconsider several relevant related problems. Our model is competitive with the\nbest results obtained in previous comparable research.",
          "link": "http://arxiv.org/abs/2107.07409",
          "publishedOn": "2021-07-16T00:48:24.892Z",
          "wordCount": 542,
          "title": "Machine Learning-Based Analysis of Free-Text Keystroke Dynamics. (arXiv:2107.07409v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_D/0/1/0/all/0/1\">Dhasarathy Parthasarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansson_A/0/1/0/all/0/1\">Anton Johansson</a>",
          "description": "Automotive software testing continues to rely largely upon expensive field\ntests to ensure quality because alternatives like simulation-based testing are\nrelatively immature. As a step towards lowering reliance on field tests, we\npresent SilGAN, a deep generative model that eases specification, stimulus\ngeneration, and automation of automotive software-in-the-loop testing. The\nmodel is trained using data recorded from vehicles in the field. Upon training,\nthe model uses a concise specification for a driving scenario to generate\nrealistic vehicle state transitions that can occur during such a scenario. Such\nauthentic emulation of internal vehicle behavior can be used for rapid,\nsystematic and inexpensive testing of vehicle control software. In addition, by\npresenting a targeted method for searching through the information learned by\nthe model, we show how a test objective like code coverage can be automated.\nThe data driven end-to-end testing pipeline that we present vastly expands the\nscope and credibility of automotive simulation-based testing. This reduces time\nto market while helping maintain required standards of quality.",
          "link": "http://arxiv.org/abs/2107.07364",
          "publishedOn": "2021-07-16T00:48:24.885Z",
          "wordCount": 613,
          "title": "SilGAN: Generating driving maneuvers for scenario-based software-in-the-loop testing. (arXiv:2107.07364v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazaheri_B/0/1/0/all/0/1\">Bijan Mazaheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Siddharth Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruck_J/0/1/0/all/0/1\">Jehoshua Bruck</a>",
          "description": "Consider multiple experts with overlapping expertise working on a\nclassification problem under uncertain input. What constitutes a consistent set\nof opinions? How can we predict the opinions of experts on missing sub-domains?\nIn this paper, we define a framework of to analyze this problem, termed \"expert\ngraphs.\" In an expert graph, vertices represent classes and edges represent\nbinary opinions on the topics of their vertices. We derive necessary conditions\nfor expert graph validity and use them to create \"synthetic experts\" which\ndescribe opinions consistent with the observed opinions of other experts. We\nshow this framework to be equivalent to the well-studied linear ordering\npolytope. We show our conditions are not sufficient for describing all expert\ngraphs on cliques, but are sufficient for cycles.",
          "link": "http://arxiv.org/abs/2107.07054",
          "publishedOn": "2021-07-16T00:48:24.879Z",
          "wordCount": 573,
          "title": "Expert Graphs: Synthesizing New Expertise via Collaboration. (arXiv:2107.07054v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07494",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_T/0/1/0/all/0/1\">Tian Zhou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ren_L/0/1/0/all/0/1\">Lihua Ren</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karlsson_N/0/1/0/all/0/1\">Niklas Karlsson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Flores_A/0/1/0/all/0/1\">Aaron Flores</a>",
          "description": "For Verizon MediaDemand Side Platform(DSP), forecasting of ad campaign\nperformance not only feeds key information to the optimization server to allow\nthe system to operate on a high-performance mode, but also produces actionable\ninsights to the advertisers. In this paper, the forecasting problem for CPA\nlines in the middle of the flight is investigated by taking the bidding\nmechanism into account. The proposed methodology generates relationships\nbetween various key performance metrics and optimization signals. It can also\nbe used to estimate the sensitivity of ad campaign performance metrics to the\nadjustments of optimization signal, which is important to the design of a\ncampaign management system. The relationship between advertiser spends and\neffective Cost Per Action(eCPA) is also characterized, which serves as a\nguidance for mid-flight line adjustment to the advertisers. Several practical\nissues in implementation, such as downsampling of the dataset, are also\ndiscussed in the paper. At last, the forecasting results are validated against\nactual deliveries and demonstrates promising accuracy.",
          "link": "http://arxiv.org/abs/2107.07494",
          "publishedOn": "2021-07-16T00:48:24.872Z",
          "wordCount": 605,
          "title": "Mid-flight Forecasting for CPA Lines in Online Advertising. (arXiv:2107.07494v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glynn_P/0/1/0/all/0/1\">Peter W. Glynn</a>",
          "description": "We study the behavior of Thompson sampling from the perspective of weak\nconvergence. In the regime where the gaps between arm means scale as\n$1/\\sqrt{n}$ with the time horizon $n$, we show that the dynamics of Thompson\nsampling evolve according to discrete versions of SDEs and random ODEs. As $n\n\\to \\infty$, we show that the dynamics converge weakly to solutions of the\ncorresponding SDEs and random ODEs. (Recently, Wager and Xu (arXiv:2101.09855)\nindependently proposed this regime and developed similar SDE and random ODE\napproximations for Thompson sampling in the multi-armed bandit setting.) Our\nweak convergence theory, which covers both multi-armed and linear bandit\nsettings, is developed from first principles using the Continuous Mapping\nTheorem and can be directly adapted to analyze other sampling-based bandit\nalgorithms, for example, algorithms using the bootstrap for exploration. We\nalso establish an invariance principle for multi-armed bandits with gaps\nscaling as $1/\\sqrt{n}$ -- for Thompson sampling and related algorithms\ninvolving posterior approximation or the bootstrap, the weak diffusion limits\nare in general the same regardless of the specifics of the reward distributions\nor the choice of prior. In particular, as suggested by the classical\nBernstein-von Mises normal approximation for posterior distributions, the weak\ndiffusion limits generally coincide with the limit for normally-distributed\nrewards and priors.",
          "link": "http://arxiv.org/abs/2105.09232",
          "publishedOn": "2021-07-16T00:48:24.856Z",
          "wordCount": 671,
          "title": "Diffusion Approximations for Thompson Sampling. (arXiv:2105.09232v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12382",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tong_L/0/1/0/all/0/1\">Lang Tong</a>",
          "description": "An innovations sequence of a time series is a sequence of independent and\nidentically distributed random variables with which the original time series\nhas a causal representation. The innovation at a time is statistically\nindependent of the history of the time series. As such, it represents the new\ninformation contained at present but not in the past. Because of its simple\nprobability structure, an innovations sequence is the most efficient signature\nof the original. Unlike the principle or independent component analysis\nrepresentations, an innovations sequence preserves not only the complete\nstatistical properties but also the temporal order of the original time series.\nAn long-standing open problem is to find a computationally tractable way to\nextract an innovations sequence of non-Gaussian processes. This paper presents\na deep learning approach, referred to as Innovations Autoencoder (IAE), that\nextracts innovations sequences using a causal convolutional neural network. An\napplication of IAE to the one-class anomalous sequence detection problem with\nunknown anomaly and anomaly-free models is also presented.",
          "link": "http://arxiv.org/abs/2106.12382",
          "publishedOn": "2021-07-16T00:48:24.848Z",
          "wordCount": 616,
          "title": "Innovations Autoencoder and its Application in One-class Anomalous Sequence Detection. (arXiv:2106.12382v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendez_Ruiz_M/0/1/0/all/0/1\">Mauricio Mendez-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Zapata_I/0/1/0/all/0/1\">Ivan Garcia Jorge Gonzalez-Zapata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_Vazquez_A/0/1/0/all/0/1\">Andres Mendez-Vazquez</a>",
          "description": "Few-shot learning is a relatively new technique that specializes in problems\nwhere we have little amounts of data. The goal of these methods is to classify\ncategories that have not been seen before with just a handful of samples.\nRecent approaches, such as metric learning, adopt the meta-learning strategy in\nwhich we have episodic tasks conformed by support (training) data and query\n(test) data. Metric learning methods have demonstrated that simple models can\nachieve good performance by learning a similarity function to compare the\nsupport and the query data. However, the feature space learned by a given\nmetric learning approach may not exploit the information given by a specific\nfew-shot task. In this work, we explore the use of dimension reduction\ntechniques as a way to find task-significant features helping to make better\npredictions. We measure the performance of the reduced features by assigning a\nscore based on the intra-class and inter-class distance, and selecting a\nfeature reduction method in which instances of different classes are far away\nand instances of the same class are close. This module helps to improve the\naccuracy performance by allowing the similarity function, given by the metric\nlearning method, to have more discriminative features for the classification.\nOur method outperforms the metric learning baselines in the miniImageNet\ndataset by around 2% in accuracy performance.",
          "link": "http://arxiv.org/abs/2107.06992",
          "publishedOn": "2021-07-16T00:48:24.841Z",
          "wordCount": 677,
          "title": "Finding Significant Features for Few-Shot Learning using Dimensionality Reduction. (arXiv:2107.06992v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07443",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Alarcon_Y/0/1/0/all/0/1\">Yonatan Carlos Carranza Alarc&#xf3;n</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Destercke_S/0/1/0/all/0/1\">S&#xe9;bastien Destercke</a>",
          "description": "We present two different strategies to extend the classical multi-label\nchaining approach to handle imprecise probability estimates. These estimates\nuse convex sets of distributions (or credal sets) in order to describe our\nuncertainty rather than a precise one. The main reasons one could have for\nusing such estimations are (1) to make cautious predictions (or no decision at\nall) when a high uncertainty is detected in the chaining and (2) to make better\nprecise predictions by avoiding biases caused in early decisions in the\nchaining. Through the use of the naive credal classifier, we propose efficient\nprocedures with theoretical justifications to solve both strategies. Our\nexperimental results on missing labels, which investigate how reliable these\npredictions are in both approaches, indicate that our approaches produce\nrelevant cautiousness on those hard-to-predict instances where the precise\nmodels fail.",
          "link": "http://arxiv.org/abs/2107.07443",
          "publishedOn": "2021-07-16T00:48:24.835Z",
          "wordCount": 566,
          "title": "Multi-label Chaining with Imprecise Probabilities. (arXiv:2107.07443v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulinski_S/0/1/0/all/0/1\">Sean Kulinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1\">Saurabh Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inouye_D/0/1/0/all/0/1\">David I. Inouye</a>",
          "description": "While previous distribution shift detection approaches can identify if a\nshift has occurred, these approaches cannot localize which specific features\nhave caused a distribution shift -- a critical step in diagnosing or fixing any\nunderlying issue. For example, in military sensor networks, users will want to\ndetect when one or more of the sensors has been compromised, and critically,\nthey will want to know which specific sensors might be compromised. Thus, we\nfirst define a formalization of this problem as multiple conditional\ndistribution hypothesis tests and propose both non-parametric and parametric\nstatistical tests. For both efficiency and flexibility, we then propose to use\na test statistic based on the density model score function (i.e. gradient with\nrespect to the input) -- which can easily compute test statistics for all\ndimensions in a single forward and backward pass. Any density model could be\nused for computing the necessary statistics including deep density models such\nas normalizing flows or autoregressive models. We additionally develop methods\nfor identifying when and where a shift occurs in multivariate time-series data\nand show results for multiple scenarios using realistic attack models on both\nsimulated and real world data.",
          "link": "http://arxiv.org/abs/2107.06929",
          "publishedOn": "2021-07-16T00:48:24.821Z",
          "wordCount": 645,
          "title": "Feature Shift Detection: Localizing Which Features Have Shifted via Conditional Distribution Tests. (arXiv:2107.06929v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1\">Yikun Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingrui He</a>",
          "description": "Online advertising is ubiquitous in web business. Image displaying is\nconsidered as one of the most commonly used formats to interact with customers.\nContextual multi-armed bandit has shown success in the application of\nadvertising to solve the exploration-exploitation dilemma existed in the\nrecommendation procedure. Inspired by the visual-aware advertising, in this\npaper, we propose a contextual bandit algorithm, where the convolutional neural\nnetwork (CNN) is utilized to learn the reward function along with an upper\nconfidence bound (UCB) for exploration. We also prove a near-optimal regret\nbound $\\tilde{\\mathcal{O}}(\\sqrt{T})$ when the network is over-parameterized\nand establish strong connections with convolutional neural tangent kernel\n(CNTK). Finally, we evaluate the empirical performance of the proposed\nalgorithm and show that it outperforms other state-of-the-art UCB-based bandit\nalgorithms on real-world image data sets.",
          "link": "http://arxiv.org/abs/2107.07438",
          "publishedOn": "2021-07-16T00:48:24.807Z",
          "wordCount": 565,
          "title": "Convolutional Neural Bandit: Provable Algorithm for Visual-aware Advertising. (arXiv:2107.07438v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1\">Dobrik Georgiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1\">Pietro Barbiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazhdan_D/0/1/0/all/0/1\">Dmitry Kazhdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1\">Petar Veli&#x10d;kovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf2;</a>",
          "description": "Recent research on graph neural network (GNN) models successfully applied\nGNNs to classical graph algorithms and combinatorial optimisation problems.\nThis has numerous benefits, such as allowing applications of algorithms when\npreconditions are not satisfied, or reusing learned models when sufficient\ntraining data is not available or can't be generated. Unfortunately, a key\nhindrance of these approaches is their lack of explainability, since GNNs are\nblack-box models that cannot be interpreted directly. In this work, we address\nthis limitation by applying existing work on concept-based explanations to GNN\nmodels. We introduce concept-bottleneck GNNs, which rely on a modification to\nthe GNN readout mechanism. Using three case studies we demonstrate that: (i)\nour proposed model is capable of accurately learning concepts and extracting\npropositional formulas based on the learned concepts for each target class;\n(ii) our concept-based GNN models achieve comparative performance with\nstate-of-the-art models; (iii) we can derive global graph concepts, without\nexplicitly providing any supervision on graph-level concepts.",
          "link": "http://arxiv.org/abs/2107.07493",
          "publishedOn": "2021-07-16T00:48:24.799Z",
          "wordCount": 584,
          "title": "Algorithmic Concept-based Explainable Reasoning. (arXiv:2107.07493v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keswani_V/0/1/0/all/0/1\">Vijay Keswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celis_L/0/1/0/all/0/1\">L. Elisa Celis</a>",
          "description": "Assessing the diversity of a dataset of information associated with people is\ncrucial before using such data for downstream applications. For a given\ndataset, this often involves computing the imbalance or disparity in the\nempirical marginal distribution of a protected attribute (e.g. gender, dialect,\netc.). However, real-world datasets, such as images from Google Search or\ncollections of Twitter posts, often do not have protected attributes labeled.\nConsequently, to derive disparity measures for such datasets, the elements need\nto hand-labeled or crowd-annotated, which are expensive processes.\n\nWe propose a cost-effective approach to approximate the disparity of a given\nunlabeled dataset, with respect to a protected attribute, using a control set\nof labeled representative examples. Our proposed algorithm uses the pairwise\nsimilarity between elements in the dataset and elements in the control set to\neffectively bootstrap an approximation to the disparity of the dataset.\nImportantly, we show that using a control set whose size is much smaller than\nthe size of the dataset is sufficient to achieve a small approximation error.\nFurther, based on our theoretical framework, we also provide an algorithm to\nconstruct adaptive control sets that achieve smaller approximation errors than\nrandomly chosen control sets. Simulations on two image datasets and one Twitter\ndataset demonstrate the efficacy of our approach (using random and adaptive\ncontrol sets) in auditing the diversity of a wide variety of datasets.",
          "link": "http://arxiv.org/abs/2107.07393",
          "publishedOn": "2021-07-16T00:48:24.792Z",
          "wordCount": 659,
          "title": "Auditing for Diversity using Representative Examples. (arXiv:2107.07393v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valdes_G/0/1/0/all/0/1\">Gilmer Valdes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbelo_W/0/1/0/all/0/1\">Wilmer Arbelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Interian_Y/0/1/0/all/0/1\">Yannet Interian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_J/0/1/0/all/0/1\">Jerome H. Friedman</a>",
          "description": "Many regression and classification procedures fit a parameterized function\n$f(x;w)$ of predictor variables $x$ to data $\\{x_{i},y_{i}\\}_1^N$ based on some\nloss criterion $L(y,f)$. Often, regularization is applied to improve accuracy\nby placing a constraint $P(w)\\leq t$ on the values of the parameters $w$.\nAlthough efficient methods exist for finding solutions to these constrained\noptimization problems for all values of $t\\geq0$ in the special case when $f$\nis a linear function, none are available when $f$ is non-linear (e.g. Neural\nNetworks). Here we present a fast algorithm that provides all such solutions\nfor any differentiable function $f$ and loss $L$, and any constraint $P$ that\nis an increasing monotone function of the absolute value of each parameter.\nApplications involving sparsity inducing regularization of arbitrary Neural\nNetworks are discussed. Empirical results indicate that these sparse solutions\nare usually superior to their dense counterparts in both accuracy and\ninterpretability. This improvement in accuracy can often make Neural Networks\ncompetitive with, and sometimes superior to, state-of-the-art methods in the\nanalysis of tabular data.",
          "link": "http://arxiv.org/abs/2107.07160",
          "publishedOn": "2021-07-16T00:48:24.785Z",
          "wordCount": 601,
          "title": "Lockout: Sparse Regularization of Neural Networks. (arXiv:2107.07160v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiahui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+shi_H/0/1/0/all/0/1\">Han shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip L.H. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>",
          "description": "Transformer-based pre-trained language models like BERT and its variants have\nrecently achieved promising performance in various natural language processing\n(NLP) tasks. However, the conventional paradigm constructs the backbone by\npurely stacking the manually designed global self-attention layers, introducing\ninductive bias and thus leading to sub-optimal. In this work, we propose an\nOperation-Priority Neural Architecture Search (OP-NAS) algorithm to\nautomatically search for promising hybrid backbone architectures. Our\nwell-designed search space (i) contains primitive math operations in the\nintra-layer level to explore novel attention structures, and (ii) leverages\nconvolution blocks to be the supplementary for attention structure in the\ninter-layer level to better learn local dependency. We optimize both the search\nalgorithm and evaluation of candidate models to boost the efficiency of our\nproposed OP-NAS. Specifically, we propose Operation-Priority (OP) evolution\nstrategy to facilitate model search via balancing exploration and exploitation.\nFurthermore, we design a Bi-branch Weight-Sharing (BIWS) training strategy for\nfast model evaluation. Extensive experiments show that the searched\narchitecture (named AutoBERT-Zero) significantly outperforms BERT and its\nvariants of different model capacities in various downstream tasks, proving the\narchitecture's transfer and generalization abilities. Remarkably,\nAutoBERT-Zero-base outperforms RoBERTa-base (using much more data) and\nBERT-large (with much larger model size) by 2.4 and 1.4 higher score on GLUE\ntest set. Code and pre-trained models will be made publicly available.",
          "link": "http://arxiv.org/abs/2107.07445",
          "publishedOn": "2021-07-16T00:48:24.767Z",
          "wordCount": 660,
          "title": "AutoBERT-Zero: Evolving BERT Backbone from Scratch. (arXiv:2107.07445v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gohel_P/0/1/0/all/0/1\">Prashant Gohel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Priyanka Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_M/0/1/0/all/0/1\">Manoranjan Mohanty</a>",
          "description": "Explainable Artificial Intelligence (XAI) is an emerging area of research in\nthe field of Artificial Intelligence (AI). XAI can explain how AI obtained a\nparticular solution (e.g., classification or object detection) and can also\nanswer other \"wh\" questions. This explainability is not possible in traditional\nAI. Explainability is essential for critical applications, such as defense,\nhealth care, law and order, and autonomous driving vehicles, etc, where the\nknow-how is required for trust and transparency. A number of XAI techniques so\nfar have been purposed for such applications. This paper provides an overview\nof these techniques from a multimedia (i.e., text, image, audio, and video)\npoint of view. The advantages and shortcomings of these techniques have been\ndiscussed, and pointers to some future directions have also been provided.",
          "link": "http://arxiv.org/abs/2107.07045",
          "publishedOn": "2021-07-16T00:48:24.755Z",
          "wordCount": 556,
          "title": "Explainable AI: current status and future directions. (arXiv:2107.07045v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07105",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Stokes_J/0/1/0/all/0/1\">James Stokes</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+De_S/0/1/0/all/0/1\">Saibal De</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Veerapaneni_S/0/1/0/all/0/1\">Shravan Veerapaneni</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Carleo_G/0/1/0/all/0/1\">Giuseppe Carleo</a>",
          "description": "We initiate the study of neural-network quantum state algorithms for\nanalyzing continuous-variable lattice quantum systems in first quantization. A\nsimple family of continuous-variable trial wavefunctons is introduced which\nnaturally generalizes the restricted Boltzmann machine (RBM) wavefunction\nintroduced for analyzing quantum spin systems. By virtue of its simplicity, the\nsame variational Monte Carlo training algorithms that have been developed for\nground state determination and time evolution of spin systems have natural\nanalogues in the continuum. We offer a proof of principle demonstration in the\ncontext of ground state determination of a stoquastic quantum rotor\nHamiltonian. Results are compared against those obtained from partial\ndifferential equation (PDE) based scalable eigensolvers. This study serves as a\nbenchmark against which future investigation of continuous-variable neural\nquantum states can be compared, and points to the need to consider deep network\narchitectures and more sophisticated training algorithms.",
          "link": "http://arxiv.org/abs/2107.07105",
          "publishedOn": "2021-07-16T00:48:24.740Z",
          "wordCount": 591,
          "title": "Continuous-variable neural-network quantum states and the quantum rotor model. (arXiv:2107.07105v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1\">Jonathan Bragg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>",
          "description": "Few-shot NLP research is highly active, yet conducted in disjoint research\nthreads with evaluation suites that lack challenging-yet-realistic testing\nsetups and fail to employ careful experimental design. Consequently, the\ncommunity does not know which techniques perform best or even if they\noutperform simple baselines. We formulate desiderata for an ideal few-shot NLP\nbenchmark and present FLEX, the first benchmark, public leaderboard, and\nframework that provides unified, comprehensive measurement for few-shot NLP\ntechniques. FLEX incorporates and introduces new best practices for few-shot\nevaluation, including measurement of four transfer settings, textual labels for\nzero-shot evaluation, and a principled approach to benchmark design that\noptimizes statistical accuracy while keeping evaluation costs accessible to\nresearchers without large compute resources. In addition, we present UniFew, a\nsimple yet strong prompt-based model for few-shot learning which unifies the\npretraining and finetuning prompt formats, eschewing complex machinery of\nrecent prompt-based approaches in adapting downstream task formats to language\nmodel pretraining objectives. We demonstrate that despite simplicity UniFew\nachieves results competitive with both popular meta-learning and prompt-based\napproaches.",
          "link": "http://arxiv.org/abs/2107.07170",
          "publishedOn": "2021-07-16T00:48:24.711Z",
          "wordCount": 619,
          "title": "FLEX: Unifying Evaluation for Few-Shot NLP. (arXiv:2107.07170v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nam_A/0/1/0/all/0/1\">Andrew Joohun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McClelland_J/0/1/0/all/0/1\">James L. McClelland</a> (Stanford University)",
          "description": "Despite the groundbreaking successes of neural networks, contemporary models\nrequire extensive training with massive datasets and exhibit poor out-of-sample\ngeneralization. One proposed solution is to build systematicity and\ndomain-specific constraints into the model, echoing the tenets of classical,\nsymbolic cognitive architectures. In this paper, we consider the limitations of\nthis approach by examining human adults' ability to learn an abstract reasoning\ntask from a brief instructional tutorial and explanatory feedback for incorrect\nresponses, demonstrating that human learning dynamics and ability to generalize\noutside the range of the training examples differ drastically from those of a\nrepresentative neural network model, and that the model is brittle to changes\nin features not anticipated by its authors. We present further evidence from\nhuman data that the ability to consistently solve the puzzles was associated\nwith education, particularly basic mathematics education, and with the ability\nto provide a reliably identifiable, valid description of the strategy used. We\npropose that rapid learning and systematic generalization in humans may depend\non a gradual, experience-dependent process of learning-to-learn using\ninstructions and explanations to guide the construction of explicit abstract\nrules that support generalizable inferences.",
          "link": "http://arxiv.org/abs/2107.06994",
          "publishedOn": "2021-07-16T00:48:24.704Z",
          "wordCount": 644,
          "title": "What underlies rapid learning and systematic generalization in humans. (arXiv:2107.06994v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07049",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Keshavamurthy_B/0/1/0/all/0/1\">Bharath Keshavamurthy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Michelusi_N/0/1/0/all/0/1\">Nicolo Michelusi</a>",
          "description": "A novel LEarning-based Spectrum Sensing and Access (LESSA) framework is\nproposed, wherein a cognitive radio (CR) learns a time-frequency correlation\nmodel underlying spectrum occupancy of licensed users (LUs) in a radio\necosystem; concurrently, it devises an approximately optimal spectrum sensing\nand access policy under sensing constraints. A Baum-Welch algorithm is proposed\nto learn a parametric Markov transition model of LU spectrum occupancy based on\nnoisy spectrum measurements. Spectrum sensing and access are cast as a\nPartially-Observable Markov Decision Process, approximately optimized via\nrandomized point-based value iteration. Fragmentation, Hamming-distance state\nfilters and Monte-Carlo methods are proposed to alleviate the inherent\ncomputational complexity, and a weighted reward metric to regulate the\ntrade-off between CR throughput and LU interference. Numerical evaluations\ndemonstrate that LESSA performs within 5 percent of a genie-aided upper bound\nwith foreknowledge of LU spectrum occupancy, and outperforms state-of-the-art\nalgorithms across the entire trade-off region: 71 percent over\ncorrelation-based clustering, 26 percent over Neyman-Pearson detection, 6\npercent over the Viterbi algorithm, and 9 percent over an adaptive Deep\nQ-Network. LESSA is then extended to a distributed Multi-Agent setting\n(MA-LESSA), by proposing novel neighbor discovery and channel access rank\nallocation. MA-LESSA improves CR throughput by 43 percent over cooperative\nTD-SARSA, 84 percent over cooperative greedy distributed learning, and 3x over\nnon-cooperative learning via g-statistics and ACKs. Finally, MA-LESSA is\nimplemented on the DARPA SC2 platform, manifesting superior performance over\ncompetitors in a real-world TDWR-UNII WLAN emulation; its implementation\nfeasibility is further validated on a testbed of ESP32 radios, exhibiting 96\npercent success probability.",
          "link": "http://arxiv.org/abs/2107.07049",
          "publishedOn": "2021-07-16T00:48:24.684Z",
          "wordCount": 712,
          "title": "Learning-based Spectrum Sensing and Access in Cognitive Radios via Approximate POMDPs. (arXiv:2107.07049v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07098",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dowling_M/0/1/0/all/0/1\">Matthew Dowling</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sokol_P/0/1/0/all/0/1\">Piotr Sok&#xf3;&#x142;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Park_I/0/1/0/all/0/1\">Il Memming Park</a>",
          "description": "We present the class of Hida-Mat\\'ern kernels, which is the canonical family\nof covariance functions over the entire space of stationary Gauss-Markov\nProcesses. It extends upon Mat\\'ern kernels, by allowing for flexible\nconstruction of priors over processes with oscillatory components. Any\nstationary kernel, including the widely used squared-exponential and spectral\nmixture kernels, are either directly within this class or are appropriate\nasymptotic limits, demonstrating the generality of this class. Taking advantage\nof its Markovian nature we show how to represent such processes as state space\nmodels using only the kernel and its derivatives. In turn this allows us to\nperform Gaussian Process inference more efficiently and side step the usual\ncomputational burdens. We also show how exploiting special properties of the\nstate space representation enables improved numerical stability in addition to\nfurther reductions of computational complexity.",
          "link": "http://arxiv.org/abs/2107.07098",
          "publishedOn": "2021-07-16T00:48:24.677Z",
          "wordCount": 556,
          "title": "Hida-Mat\\'ern Kernel. (arXiv:2107.07098v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1\">Kyeongbo Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junggi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_Y/0/1/0/all/0/1\">Youngchul Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Young-Rae Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seong-Eun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Woo-Jin Song</a>",
          "description": "Because deep learning is vulnerable to noisy labels, sample selection\ntechniques, which train networks with only clean labeled data, have attracted a\ngreat attention. However, if the labels are dominantly corrupted by few\nclasses, these noisy samples are called dominant-noisy-labeled samples, the\nnetwork also learns dominant-noisy-labeled samples rapidly via content-aware\noptimization. In this study, we propose a compelling criteria to penalize\ndominant-noisy-labeled samples intensively through class-wise penalty labels.\nBy averaging prediction confidences for the each observed label, we obtain\nsuitable penalty labels that have high values if the labels are largely\ncorrupted by some classes. Experiments were performed using benchmarks\n(CIFAR-10, CIFAR-100, Tiny-ImageNet) and real-world datasets (ANIMAL-10N,\nClothing1M) to evaluate the proposed criteria in various scenarios with\ndifferent noise rates. Using the proposed sample selection, the learning\nprocess of the network becomes significantly robust to noisy labels compared to\nexisting methods in several noise types.",
          "link": "http://arxiv.org/abs/2107.07041",
          "publishedOn": "2021-07-16T00:48:24.619Z",
          "wordCount": 600,
          "title": "Mitigating Memorization in Sample Selection for Learning with Noisy Labels. (arXiv:2107.07041v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07044",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Haoxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fojtik_M/0/1/0/all/0/1\">Matthew Fojtik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khailany_B/0/1/0/all/0/1\">Brucek Khailany</a>",
          "description": "High quality standard cell layout automation in advanced technology nodes is\nstill challenging in the industry today because of complex design rules. In\nthis paper we introduce an automatic standard cell layout generator called\nNVCell that can generate layouts with equal or smaller area for over 90% of\nsingle row cells in an industry standard cell library on an advanced technology\nnode. NVCell leverages reinforcement learning (RL) to fix design rule\nviolations during routing and to generate efficient placements.",
          "link": "http://arxiv.org/abs/2107.07044",
          "publishedOn": "2021-07-16T00:48:24.611Z",
          "wordCount": 513,
          "title": "NVCell: Standard Cell Layout in Advanced Technology Nodes with Reinforcement Learning. (arXiv:2107.07044v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07087",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Finkelstein_N/0/1/0/all/0/1\">Noam Finkelstein</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zjawin_B/0/1/0/all/0/1\">Beata Zjawin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wolfe_E/0/1/0/all/0/1\">Elie Wolfe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shpitser_I/0/1/0/all/0/1\">Ilya Shpitser</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Spekkens_R/0/1/0/all/0/1\">Robert W. Spekkens</a>",
          "description": "Directed acyclic graphs (DAGs) with hidden variables are often used to\ncharacterize causal relations between variables in a system. When some\nvariables are unobserved, DAGs imply a notoriously complicated set of\nconstraints on the distribution of observed variables. In this work, we present\nentropic inequality constraints that are implied by $e$-separation relations in\nhidden variable DAGs with discrete observed variables. The constraints can\nintuitively be understood to follow from the fact that the capacity of\nvariables along a causal pathway to convey information is restricted by their\nentropy; e.g. at the extreme case, a variable with entropy $0$ can convey no\ninformation. We show how these constraints can be used to learn about the true\ncausal model from an observed data distribution. In addition, we propose a\nmeasure of causal influence called the minimal mediary entropy, and demonstrate\nthat it can augment traditional measures such as the average causal effect.",
          "link": "http://arxiv.org/abs/2107.07087",
          "publishedOn": "2021-07-16T00:48:24.603Z",
          "wordCount": 636,
          "title": "Entropic Inequality Constraints from $e$-separation Relations in Directed Acyclic Graphs with Hidden Variables. (arXiv:2107.07087v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael Ng</a>",
          "description": "Image smoothing is a fundamental procedure in applications of both computer\nvision and graphics. The required smoothing properties can be different or even\ncontradictive among different tasks. Nevertheless, the inherent smoothing\nnature of one smoothing operator is usually fixed and thus cannot meet the\nvarious requirements of different applications. In this paper, we first\nintroduce the truncated Huber penalty function which shows strong flexibility\nunder different parameter settings. A generalized framework is then proposed\nwith the introduced truncated Huber penalty function. When combined with its\nstrong flexibility, our framework is able to achieve diverse smoothing natures\nwhere contradictive smoothing behaviors can even be achieved. It can also yield\nthe smoothing behavior that can seldom be achieved by previous methods, and\nsuperior performance is thus achieved in challenging cases. These together\nenable our framework capable of a range of applications and able to outperform\nthe state-of-the-art approaches in several tasks, such as image detail\nenhancement, clip-art compression artifacts removal, guided depth map\nrestoration, image texture removal, etc. In addition, an efficient numerical\nsolution is provided and its convergence is theoretically guaranteed even the\noptimization framework is non-convex and non-smooth. A simple yet effective\napproach is further proposed to reduce the computational cost of our method\nwhile maintaining its performance. The effectiveness and superior performance\nof our approach are validated through comprehensive experiments in a range of\napplications. Our code is available at\nhttps://github.com/wliusjtu/Generalized-Smoothing-Framework.",
          "link": "http://arxiv.org/abs/2107.07058",
          "publishedOn": "2021-07-16T00:48:24.585Z",
          "wordCount": 707,
          "title": "A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyoungjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Myungchul Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_L/0/1/0/all/0/1\">Laihyuk Park</a>",
          "description": "Video streaming services strive to support high-quality videos at higher\nresolutions and frame rates to improve the quality of experience (QoE).\nHowever, high-quality videos consume considerable amounts of energy on mobile\ndevices. This paper proposes NeuSaver, which reduces the power consumption of\nmobile devices when streaming videos by applying an adaptive frame rate to each\nvideo chunk without compromising user experience. NeuSaver generates an optimal\npolicy that determines the appropriate frame rate for each video chunk using\nreinforcement learning (RL). The RL model automatically learns the policy that\nmaximizes the QoE goals based on previous observations. NeuSaver also uses an\nasynchronous advantage actor-critic algorithm to reinforce the RL model quickly\nand robustly. Streaming servers that support NeuSaver preprocesses videos into\nsegments with various frame rates, which is similar to the process of creating\nvideos with multiple bit rates in dynamic adaptive streaming over HTTP.\nNeuSaver utilizes the commonly used H.264 video codec. We evaluated NeuSaver in\nvarious experiments and a user study through four video categories along with\nthe state-of-the-art model. Our experiments showed that NeuSaver effectively\nreduces the power consumption of mobile devices when streaming video by an\naverage of 16.14% and up to 23.12% while achieving high QoE.",
          "link": "http://arxiv.org/abs/2107.07127",
          "publishedOn": "2021-07-16T00:48:24.579Z",
          "wordCount": 672,
          "title": "NeuSaver: Neural Adaptive Power Consumption Optimization for Mobile Video Streaming. (arXiv:2107.07127v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07116",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Feng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chonghan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashar_M/0/1/0/all/0/1\">Mohammad Khairul Bashar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_N/0/1/0/all/0/1\">Nikhil Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1\">Vijaykrishnan Narayanan</a>",
          "description": "CNF-based SAT and MaxSAT solvers are central to logic synthesis and\nverification systems. The increasing popularity of these constraint problems in\nelectronic design automation encourages studies on different SAT problems and\ntheir properties for further computational efficiency. There has been both\ntheoretical and practical success of modern Conflict-driven clause learning SAT\nsolvers, which allows solving very large industrial instances in a relatively\nshort amount of time. Recently, machine learning approaches provide a new\ndimension to solving this challenging problem. Neural symbolic models could\nserve as generic solvers that can be specialized for specific domains based on\ndata without any changes to the structure of the model. In this work, we\npropose a one-shot model derived from the Transformer architecture to solve the\nMaxSAT problem, which is the optimization version of SAT where the goal is to\nsatisfy the maximum number of clauses. Our model has a scale-free structure\nwhich could process varying size of instances. We use meta-path and\nself-attention mechanism to capture interactions among homogeneous nodes. We\nadopt cross-attention mechanisms on the bipartite graph to capture interactions\namong heterogeneous nodes. We further apply an iterative algorithm to our model\nto satisfy additional clauses, enabling a solution approaching that of an\nexact-SAT problem. The attention mechanisms leverage the parallelism for\nspeedup. Our evaluation indicates improved speedup compared to heuristic\napproaches and improved completion rate compared to machine learning\napproaches.",
          "link": "http://arxiv.org/abs/2107.07116",
          "publishedOn": "2021-07-16T00:48:24.572Z",
          "wordCount": 680,
          "title": "Transformer-based Machine Learning for Fast SAT Solvers and Logic Synthesis. (arXiv:2107.07116v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Daniel T. Chang</a>",
          "description": "Bayesian neural networks provide a direct and natural way to extend standard\ndeep neural networks to support probabilistic deep learning through the use of\nprobabilistic layers that, traditionally, encode weight (and bias) uncertainty.\nIn particular, hybrid Bayesian neural networks utilize standard deterministic\nlayers together with few probabilistic layers judicially positioned in the\nnetworks for uncertainty estimation. A major aspect and benefit of Bayesian\ninference is that priors, in principle, provide the means to encode prior\nknowledge for use in inference and prediction. However, it is difficult to\nspecify priors on weights since the weights have no intuitive interpretation.\nFurther, the relationships of priors on weights to the functions computed by\nnetworks are difficult to characterize. In contrast, functions are intuitive to\ninterpret and are direct since they map inputs to outputs. Therefore, it is\nnatural to specify priors on functions to encode prior knowledge, and to use\nthem in inference and prediction based on functions. To support this, we\npropose hybrid Bayesian neural networks with functional probabilistic layers\nthat encode function (and activation) uncertainty. We discuss their foundations\nin functional Bayesian inference, functional variational inference, sparse\nGaussian processes, and sparse variational Gaussian processes. We further\nperform few proof-of-concept experiments using GPflus, a new library that\nprovides Gaussian process layers and supports their use with deterministic\nKeras layers to form hybrid neural network and Gaussian process models.",
          "link": "http://arxiv.org/abs/2107.07014",
          "publishedOn": "2021-07-16T00:48:24.565Z",
          "wordCount": 656,
          "title": "Hybrid Bayesian Neural Networks with Functional Probabilistic Layers. (arXiv:2107.07014v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sit_M/0/1/0/all/0/1\">Muhammed Sit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demiray_B/0/1/0/all/0/1\">Bekir Demiray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1\">Ibrahim Demir</a>",
          "description": "The frequency and impact of floods are expected to increase due to climate\nchange. It is crucial to predict streamflow, consequently flooding, in order to\nprepare and mitigate its consequences in terms of property damage and\nfatalities. This paper presents a Graph Convolutional GRUs based model to\npredict the next 36 hours of streamflow for a sensor location using the\nupstream river network. As shown in experiment results, the model presented in\nthis study provides better performance than the persistence baseline and a\nConvolutional Bidirectional GRU network for the selected study area in\nshort-term streamflow prediction.",
          "link": "http://arxiv.org/abs/2107.07039",
          "publishedOn": "2021-07-16T00:48:24.552Z",
          "wordCount": 546,
          "title": "Short-term Hourly Streamflow Prediction with Graph Convolutional GRU Networks. (arXiv:2107.07039v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guohua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yongming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1\">Mingfeng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1\">Witold Pedrycz</a>",
          "description": "Vehicle routing problem (VRP) is a typical discrete combinatorial\noptimization problem, and many models and algorithms have been proposed to\nsolve VRP and variants. Although existing approaches has contributed a lot to\nthe development of this field, these approaches either are limited in problem\nsize or need manual intervening in choosing parameters. To tackle these\ndifficulties, many studies consider learning-based optimization algorithms to\nsolve VRP. This paper reviews recent advances in this field and divides\nrelevant approaches into end-to-end approaches and step-by-step approaches. We\ndesign three part experiments to justly evaluate performance of four\nrepresentative learning-based optimization algorithms and conclude that\ncombining heuristic search can effectively improve learning ability and sampled\nefficiency of LBO models. Finally we point out that research trend of LBO\nalgorithms is to solve large-scale and multiple constraints problems from real\nworld.",
          "link": "http://arxiv.org/abs/2107.07076",
          "publishedOn": "2021-07-16T00:48:24.544Z",
          "wordCount": 583,
          "title": "An Overview and Experimental Study of Learning-based Optimization Algorithms for Vehicle Routing Problem. (arXiv:2107.07076v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07115",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ishibashi_H/0/1/0/all/0/1\">Hideaki Ishibashi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Akaho_S/0/1/0/all/0/1\">Shotaro Akaho</a>",
          "description": "This paper proposes an extension of principal component analysis for Gaussian\nprocess posteriors denoted by GP-PCA. Since GP-PCA estimates a low-dimensional\nspace of GP posteriors, it can be used for meta-learning, which is a framework\nfor improving the precision of a new task by estimating a structure of a set of\ntasks. The issue is how to define a structure of a set of GPs with an\ninfinite-dimensional parameter, such as coordinate system and a divergence. In\nthis study, we reduce the infiniteness of GP to the finite-dimensional case\nunder the information geometrical framework by considering a space of GP\nposteriors that has the same prior. In addition, we propose an approximation\nmethod of GP-PCA based on variational inference and demonstrate the\neffectiveness of GP-PCA as meta-learning through experiments.",
          "link": "http://arxiv.org/abs/2107.07115",
          "publishedOn": "2021-07-16T00:48:24.525Z",
          "wordCount": 558,
          "title": "Principal component analysis for Gaussian process posteriors. (arXiv:2107.07115v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ayush Manish Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tendle_A/0/1/0/all/0/1\">Atharva Tendle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikka_H/0/1/0/all/0/1\">Harshvardhan Sikka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>",
          "description": "Interpreting the learning dynamics of neural networks can provide useful\ninsights into how networks learn and the development of better training and\ndesign approaches. We present an approach to interpret learning in neural\nnetworks by measuring relative weight change on a per layer basis and\ndynamically aggregating emerging trends through combination of dimensionality\nreduction and clustering which allows us to scale to very deep networks. We use\nthis approach to investigate learning in the context of vision tasks across a\nvariety of state-of-the-art networks and provide insights into the learning\nbehavior of these networks, including how task complexity affects layer-wise\nlearning in deeper layers of networks.",
          "link": "http://arxiv.org/abs/2107.07005",
          "publishedOn": "2021-07-16T00:48:24.513Z",
          "wordCount": 550,
          "title": "WeightScale: Interpreting Weight Change in Neural Networks. (arXiv:2107.07005v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ferrero_V/0/1/0/all/0/1\">Vincenzo Ferrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassani_K/0/1/0/all/0/1\">Kaveh Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandi_D/0/1/0/all/0/1\">Daniele Grandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuPont_B/0/1/0/all/0/1\">Bryony DuPont</a>",
          "description": "Function is defined as the ensemble of tasks that enable the product to\ncomplete the designed purpose. Functional tools, such as functional modeling,\noffer decision guidance in the early phase of product design, where explicit\ndesign decisions are yet to be made. Function-based design data is often sparse\nand grounded in individual interpretation. As such, function-based design tools\ncan benefit from automatic function classification to increase data fidelity\nand provide function representation models that enable function-based\nintelligent design agents. Function-based design data is commonly stored in\nmanually generated design repositories. These design repositories are a\ncollection of expert knowledge and interpretations of function in product\ndesign bounded by function-flow and component taxonomies. In this work, we\nrepresent a structured taxonomy-based design repository as assembly-flow\ngraphs, then leverage a graph neural network (GNN) model to perform automatic\nfunction classification. We support automated function classification by\nlearning from repository data to establish the ground truth of component\nfunction assignment. Experimental results show that our GNN model achieves a\nmicro-average F${_1}$-score of 0.832 for tier 1 (broad), 0.756 for tier 2, and\n0.783 for tier 3 (specific) functions. Given the imbalance of data features,\nthe results are encouraging. Our efforts in this paper can be a starting point\nfor more sophisticated applications in knowledge-based CAD systems and\nDesign-for-X consideration in function-based design.",
          "link": "http://arxiv.org/abs/2107.07042",
          "publishedOn": "2021-07-16T00:48:24.472Z",
          "wordCount": 660,
          "title": "Classifying Component Function in Product Assemblies with Graph Neural Networks. (arXiv:2107.07042v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey A. Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>",
          "description": "The world of empirical machine learning (ML) strongly relies on benchmarks in\norder to determine the relative effectiveness of different algorithms and\nmethods. This paper proposes the notion of \"a benchmark lottery\" that describes\nthe overall fragility of the ML benchmarking process. The benchmark lottery\npostulates that many factors, other than fundamental algorithmic superiority,\nmay lead to a method being perceived as superior. On multiple benchmark setups\nthat are prevalent in the ML community, we show that the relative performance\nof algorithms may be altered significantly simply by choosing different\nbenchmark tasks, highlighting the fragility of the current paradigms and\npotential fallacious interpretation derived from benchmarking ML methods. Given\nthat every benchmark makes a statement about what it perceives to be important,\nwe argue that this might lead to biased progress in the community. We discuss\nthe implications of the observed phenomena and provide recommendations on\nmitigating them using multiple machine learning domains and communities as use\ncases, including natural language processing, computer vision, information\nretrieval, recommender systems, and reinforcement learning.",
          "link": "http://arxiv.org/abs/2107.07002",
          "publishedOn": "2021-07-16T00:48:24.464Z",
          "wordCount": 600,
          "title": "The Benchmark Lottery. (arXiv:2107.07002v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_H/0/1/0/all/0/1\">Hugo Flores Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguilar_A/0/1/0/all/0/1\">Aldo Aguilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manilow_E/0/1/0/all/0/1\">Ethan Manilow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardo_B/0/1/0/all/0/1\">Bryan Pardo</a>",
          "description": "Deep learning work on musical instrument recognition has generally focused on\ninstrument classes for which we have abundant data. In this work, we exploit\nhierarchical relationships between instruments in a few-shot learning setup to\nenable classification of a wider set of musical instruments, given a few\nexamples at inference. We apply a hierarchical loss function to the training of\nprototypical networks, combined with a method to aggregate prototypes\nhierarchically, mirroring the structure of a predefined musical instrument\nhierarchy. These extensions require no changes to the network architecture and\nnew levels can be easily added or removed. Compared to a non-hierarchical\nfew-shot baseline, our method leads to a significant increase in classification\naccuracy and significant decrease mistake severity on instrument classes unseen\nin training.",
          "link": "http://arxiv.org/abs/2107.07029",
          "publishedOn": "2021-07-16T00:48:24.445Z",
          "wordCount": 563,
          "title": "Leveraging Hierarchical Structures for Few-Shot Musical Instrument Recognition. (arXiv:2107.07029v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Piqueras_M/0/1/0/all/0/1\">Manuel Garcia-Piqueras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Orallo_J/0/1/0/all/0/1\">Jos&#xe9; Hern&#xe1;ndez-Orallo</a>",
          "description": "Recent research in machine teaching has explored the instruction of any\nconcept expressed in a universal language. In this compositional context, new\nexperimental results have shown that there exist data teaching sets\nsurprisingly shorter than the concept description itself. However, there exists\na bound for those remarkable experimental findings through teaching size and\nconcept complexity that we further explore here. As concepts are rarely taught\nin isolation we investigate the best configuration of concepts to teach a given\nset of concepts, where those that have been acquired first can be reused for\nthe description of new ones. This new notion of conditional teaching size\nuncovers new insights, such as the interposition phenomenon: certain prior\nknowledge generates simpler compatible concepts that increase the teaching size\nof the concept that we want to teach. This does not happen for conditional\nKolmogorov complexity. Furthermore, we provide an algorithm that constructs\noptimal curricula based on interposition avoidance. This paper presents a\nseries of theoretical results, including their proofs, and some directions for\nfuture work. New research possibilities in curriculum teaching in compositional\nscenarios are now wide open to exploration.",
          "link": "http://arxiv.org/abs/2107.07038",
          "publishedOn": "2021-07-16T00:48:24.440Z",
          "wordCount": 618,
          "title": "Conditional Teaching Size. (arXiv:2107.07038v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06995",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shabani_M/0/1/0/all/0/1\">Mostafa Shabani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>",
          "description": "Financial market analysis, especially the prediction of movements of stock\nprices, is a challenging problem. The nature of financial time-series data,\nbeing non-stationary and nonlinear, is the main cause of these challenges. Deep\nlearning models have led to significant performance improvements in many\nproblems coming from different domains, including prediction problems of\nfinancial time-series data. Although the prediction performance is the main\ngoal of such models, dealing with ultra high-frequency data sets restrictions\nin terms of the number of model parameters and its inference speed. The\nTemporal Attention-Augmented Bilinear network was recently proposed as an\nefficient and high-performing model for Limit Order Book time-series\nforecasting. In this paper, we propose a low-rank tensor approximation of the\nmodel to further reduce the number of trainable parameters and increase its\nspeed.",
          "link": "http://arxiv.org/abs/2107.06995",
          "publishedOn": "2021-07-16T00:48:24.433Z",
          "wordCount": 558,
          "title": "Low-Rank Temporal Attention-Augmented Bilinear Network for financial time-series forecasting. (arXiv:2107.06995v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Han-Chih Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>",
          "description": "In this research, we consider the problem of verifying user identity based on\nkeystroke dynamics obtained from free-text. We employ a novel feature\nengineering method that generates image-like transition matrices. For this\nimage-like feature, a convolution neural network (CNN) with cutout achieves the\nbest results. A hybrid model consisting of a CNN and a recurrent neural network\n(RNN) is also shown to outperform previous research in this field.",
          "link": "http://arxiv.org/abs/2107.07009",
          "publishedOn": "2021-07-16T00:48:24.419Z",
          "wordCount": 491,
          "title": "Free-Text Keystroke Dynamics for User Authentication. (arXiv:2107.07009v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afrin_T/0/1/0/all/0/1\">Tazin Afrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1\">Elaine Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumura_L/0/1/0/all/0/1\">Lindsay C. Matsumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correnti_R/0/1/0/all/0/1\">Richard Correnti</a>",
          "description": "Automated writing evaluation systems can improve students' writing insofar as\nstudents attend to the feedback provided and revise their essay drafts in ways\naligned with such feedback. Existing research on revision of argumentative\nwriting in such systems, however, has focused on the types of revisions\nstudents make (e.g., surface vs. content) rather than the extent to which\nrevisions actually respond to the feedback provided and improve the essay. We\nintroduce an annotation scheme to capture the nature of sentence-level\nrevisions of evidence use and reasoning (the `RER' scheme) and apply it to 5th-\nand 6th-grade students' argumentative essays. We show that reliable manual\nannotation can be achieved and that revision annotations correlate with a\nholistic assessment of essay improvement in line with the feedback provided.\nFurthermore, we explore the feasibility of automatically classifying revisions\naccording to our scheme.",
          "link": "http://arxiv.org/abs/2107.06990",
          "publishedOn": "2021-07-16T00:48:24.413Z",
          "wordCount": 605,
          "title": "Annotation and Classification of Evidence and Reasoning Revisions in Argumentative Writing. (arXiv:2107.06990v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charles_Z/0/1/0/all/0/1\">Zachary Charles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_G/0/1/0/all/0/1\">Gauri Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMahan_H/0/1/0/all/0/1\">H. Brendan McMahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1\">Blaise Aguera y Arcas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Shedivat_M/0/1/0/all/0/1\">Maruan Al-Shedivat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrew_G/0/1/0/all/0/1\">Galen Andrew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daly_K/0/1/0/all/0/1\">Katharine Daly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Data_D/0/1/0/all/0/1\">Deepesh Data</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diggavi_S/0/1/0/all/0/1\">Suhas Diggavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eichner_H/0/1/0/all/0/1\">Hubert Eichner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadhikar_A/0/1/0/all/0/1\">Advait Gadhikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrett_Z/0/1/0/all/0/1\">Zachary Garrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girgis_A/0/1/0/all/0/1\">Antonious M. Girgis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanzely_F/0/1/0/all/0/1\">Filip Hanzely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hard_A/0/1/0/all/0/1\">Andrew Hard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chaoyang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvath_S/0/1/0/all/0/1\">Samuel Horvath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingerman_A/0/1/0/all/0/1\">Alex Ingerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1\">Tara Javidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1\">Peter Kairouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_S/0/1/0/all/0/1\">Satyen Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1\">Sai Praneeth Karimireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konecny_J/0/1/0/all/0/1\">Jakub Konecny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1\">Sanmi Koyejo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Luyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1\">Mehryar Mohri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddi_S/0/1/0/all/0/1\">Sashank J. Reddi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1\">Peter Richtarik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1\">Karan Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1\">Virginia Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1\">Mahdi Soltanolkotabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Weikang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1\">Ananda Theertha Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1\">Sebastian U. Stich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodworth_B/0/1/0/all/0/1\">Blake Woodworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shanshan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Felix X. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Honglin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chunxiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, et al. (1 additional author not shown)",
          "description": "Federated learning and analytics are a distributed approach for\ncollaboratively learning models (or statistics) from decentralized data,\nmotivated by and designed for privacy protection. The distributed learning\nprocess can be formulated as solving federated optimization problems, which\nemphasize communication efficiency, data heterogeneity, compatibility with\nprivacy and system requirements, and other constraints that are not primary\nconsiderations in other problem settings. This paper provides recommendations\nand guidelines on formulating, designing, evaluating and analyzing federated\noptimization algorithms through concrete examples and practical implementation,\nwith a focus on conducting effective simulations to infer real-world\nperformance. The goal of this work is not to survey the current literature, but\nto inspire researchers and practitioners to design federated learning\nalgorithms that can be used in various practical applications.",
          "link": "http://arxiv.org/abs/2107.06917",
          "publishedOn": "2021-07-16T00:48:24.404Z",
          "wordCount": 656,
          "title": "A Field Guide to Federated Optimization. (arXiv:2107.06917v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07064",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1\">Dae-Hyeok Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sung-Jin Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Brain-computer interface (BCI) is one of the tools which enables the\ncommunication between humans and devices by reflecting intention and status of\nhumans. With the development of artificial intelligence, the interest in\ncommunication between humans and drones using electroencephalogram (EEG) is\nincreased. Especially, in the case of controlling drone swarms such as\ndirection or formation, there are many advantages compared with controlling a\ndrone unit. Imagined speech is one of the endogenous BCI paradigms, which can\nidentify intentions of users. When conducting imagined speech, the users\nimagine the pronunciation as if actually speaking. In contrast, overt speech is\na task in which the users directly pronounce the words. When controlling drone\nswarms using imagined speech, complex commands can be delivered more\nintuitively, but decoding performance is lower than that of other endogenous\nBCI paradigms. We proposed the Deep-autoleaner (DAL) to learn EEG features of\novert speech for imagined speech-based EEG signals classification. To the best\nof our knowledge, this study is the first attempt to use EEG features of overt\nspeech to decode imagined speech-based EEG signals with an autoencoder. A total\nof eight subjects participated in the experiment. When classifying four words,\nthe average accuracy of the DAL was 48.41%. In addition, when comparing the\nperformance between w/o and w/ EEG features of overt speech, there was a\nperformance improvement of 7.42% when including EEG features of overt speech.\nHence, we demonstrated that EEG features of overt speech could improve the\ndecoding performance of imagined speech.",
          "link": "http://arxiv.org/abs/2107.07064",
          "publishedOn": "2021-07-16T00:48:24.396Z",
          "wordCount": 715,
          "title": "DAL: Feature Learning from Overt Speech to Decode Imagined Speech-based EEG Signals with Convolutional Autoencoder. (arXiv:2107.07064v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farley_J/0/1/0/all/0/1\">Jackson Farley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstlauer_A/0/1/0/all/0/1\">Andreas Gerstlauer</a>",
          "description": "A rising research challenge is running costly machine learning (ML) networks\nlocally on resource-constrained edge devices. ML networks with large\nconvolutional layers can easily exceed available memory, increasing latency due\nto excessive swapping. Previous memory reduction techniques such as pruning and\nquantization reduce model accuracy and often require retraining. Alternatively,\ndistributed methods partition the convolutions into equivalent smaller\nsub-computations, but the implementations introduce communication costs and\nrequire a network of devices. However, a distributed partitioning approach can\nalso be used to run in a reduced memory footprint on a single device by\nsubdividing the network into smaller operations.\n\nThis report extends prior work on distributed partitioning using tiling and\nfusing of convolutional layers into a memory-aware execution on a single\ndevice. Our approach extends prior fusing strategies to allow for two groups of\nconvolutional layers that are fused and tiled independently. This approach\nreduces overhead via data reuse, and reduces the memory footprint further. We\nalso propose a memory usage predictor coupled with a search algorithm to\nprovide fusing and tiling configurations for an arbitrary set of convolutional\nlayers. When applied to the YOLOv2 object detection network, results show that\nour approach can run in less than half the memory, and with a speedup of up to\n2.78 under severe memory constraints. Additionally, our algorithm will return a\nconfiguration with a latency that is within 6% of the best latency measured in\na manual search.",
          "link": "http://arxiv.org/abs/2107.06960",
          "publishedOn": "2021-07-16T00:48:24.390Z",
          "wordCount": 676,
          "title": "Memory-Aware Fusing and Tiling of Neural Networks for Accelerated Edge Inference. (arXiv:2107.06960v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zuohui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jingyang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1\">Qi Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoniu Yang</a>",
          "description": "Deep Neural Networks (DNN) are known to be vulnerable to adversarial samples,\nthe detection of which is crucial for the wide application of these DNN models.\nRecently, a number of deep testing methods in software engineering were\nproposed to find the vulnerability of DNN systems, and one of them, i.e., Model\nMutation Testing (MMT), was used to successfully detect various adversarial\nsamples generated by different kinds of adversarial attacks. However, the\nmutated models in MMT are always huge in number (e.g., over 100 models) and\nlack diversity (e.g., can be easily circumvented by high-confidence adversarial\nsamples), which makes it less efficient in real applications and less effective\nin detecting high-confidence adversarial samples. In this study, we propose\nGraph-Guided Testing (GGT) for adversarial sample detection to overcome these\naforementioned challenges. GGT generates pruned models with the guide of graph\ncharacteristics, each of them has only about 5% parameters of the mutated model\nin MMT, and graph guided models have higher diversity. The experiments on\nCIFAR10 and SVHN validate that GGT performs much better than MMT with respect\nto both effectiveness and efficiency.",
          "link": "http://arxiv.org/abs/2107.07043",
          "publishedOn": "2021-07-16T00:48:24.382Z",
          "wordCount": 635,
          "title": "GGT: Graph-Guided Testing for Adversarial Sample Detection of Deep Neural Network. (arXiv:2107.07043v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weikai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zheng Yan</a>",
          "description": "The troposphere is one of the atmospheric layers where most weather phenomena\noccur. Temperature variations in the troposphere, especially at 500 hPa, a\ntypical level of the middle troposphere, are significant indicators of future\nweather changes. Numerical weather prediction is effective for temperature\nprediction, but its computational complexity hinders a timely response. This\npaper proposes a novel temperature prediction approach in framework\nofphysics-informed deep learning. The new model, called PGnet, builds upon a\ngenerative neural network with a mask matrix. The mask is designed to\ndistinguish the low-quality predicted regions generated by the first physical\nstage. The generative neural network takes the mask as prior for the\nsecond-stage refined predictions. A mask-loss and a jump pattern strategy are\ndeveloped to train the generative neural network without accumulating errors\nduring making time-series predictions. Experiments on ERA5 demonstrate that\nPGnet can generate more refined temperature predictions than the\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2107.06991",
          "publishedOn": "2021-07-16T00:48:24.377Z",
          "wordCount": 590,
          "title": "Physics-informed generative neural network: an application to troposphere temperature prediction. (arXiv:2107.06991v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pinzon_C/0/1/0/all/0/1\">Carlos Pinz&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palamidessi_C/0/1/0/all/0/1\">Catuscia Palamidessi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valencia_F/0/1/0/all/0/1\">Frank Valencia</a>",
          "description": "One of the main concerns about fairness in machine learning (ML) is that, in\norder to achieve it, one may have to renounce to some accuracy. Having this\ntrade-off in mind, Hardt et al. have proposed the notion of equal opportunities\n(EO), designed so as to be compatible with accuracy. In fact, it can be shown\nthat if the source of input data is deterministic, the two notions go well\nalong with each other. In the probabilistic case, however, things change.\n\nAs we show, there are probabilistic data sources for which EO can only be\nachieved at the total detriment of accuracy, i.e. among the models that achieve\nEO, those whose prediction does not depend on the input have the highest\naccuracy.",
          "link": "http://arxiv.org/abs/2107.06944",
          "publishedOn": "2021-07-16T00:48:24.361Z",
          "wordCount": 561,
          "title": "On the impossibility of non-trivial accuracy under fairness constraints. (arXiv:2107.06944v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "We introduce HTLM, a hyper-text language model trained on a large-scale web\ncrawl. Modeling hyper-text has a number of advantages: (1) it is easily\ngathered at scale, (2) it provides rich document-level and end-task-adjacent\nsupervision (e.g. class and id attributes often encode document category\ninformation), and (3) it allows for new structured prompting that follows the\nestablished semantics of HTML (e.g. to do zero-shot summarization by infilling\ntitle tags for a webpage that contains the input text). We show that\npretraining with a BART-style denoising loss directly on simplified HTML\nprovides highly effective transfer for a wide range of end tasks and\nsupervision levels. HTLM matches or exceeds the performance of comparably sized\ntext-only LMs for zero-shot prompting and fine-tuning for classification\nbenchmarks, while also setting new state-of-the-art performance levels for\nzero-shot summarization. We also find that hyper-text prompts provide more\nvalue to HTLM, in terms of data efficiency, than plain text prompts do for\nexisting LMs, and that HTLM is highly effective at auto-prompting itself, by\nsimply generating the most likely hyper-text formatting for any available\ntraining data. We will release all code and models to support future HTLM\nresearch.",
          "link": "http://arxiv.org/abs/2107.06955",
          "publishedOn": "2021-07-16T00:48:24.349Z",
          "wordCount": 633,
          "title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models. (arXiv:2107.06955v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaorui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Wei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>",
          "description": "While many existing graph neural networks (GNNs) have been proven to perform\n$\\ell_2$-based graph smoothing that enforces smoothness globally, in this work\nwe aim to further enhance the local smoothness adaptivity of GNNs via\n$\\ell_1$-based graph smoothing. As a result, we introduce a family of GNNs\n(Elastic GNNs) based on $\\ell_1$ and $\\ell_2$-based graph smoothing. In\nparticular, we propose a novel and general message passing scheme into GNNs.\nThis message passing algorithm is not only friendly to back-propagation\ntraining but also achieves the desired smoothing properties with a theoretical\nconvergence guarantee. Experiments on semi-supervised learning tasks\ndemonstrate that the proposed Elastic GNNs obtain better adaptivity on\nbenchmark datasets and are significantly robust to graph adversarial attacks.\nThe implementation of Elastic GNNs is available at\n\\url{https://github.com/lxiaorui/ElasticGNN}.",
          "link": "http://arxiv.org/abs/2107.06996",
          "publishedOn": "2021-07-16T00:48:24.332Z",
          "wordCount": 575,
          "title": "Elastic Graph Neural Networks. (arXiv:2107.06996v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diddee_H/0/1/0/all/0/1\">Harshita Diddee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulay_A/0/1/0/all/0/1\">Ajinkya Mulay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardhan_A/0/1/0/all/0/1\">Aleti Vardhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1\">Krithika Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamzam_A/0/1/0/all/0/1\">Ahmed Zamzam</a>",
          "description": "In recent years, machine learning techniques utilizing large-scale datasets\nhave achieved remarkable performance. Differential privacy, by means of adding\nnoise, provides strong privacy guarantees for such learning algorithms. The\ncost of differential privacy is often a reduced model accuracy and a lowered\nconvergence speed. This paper investigates the impact of differential privacy\non learning algorithms in terms of their carbon footprint due to either longer\nrun-times or failed experiments. Through extensive experiments, further\nguidance is provided on choosing the noise levels which can strike a balance\nbetween desired privacy levels and reduced carbon emissions.",
          "link": "http://arxiv.org/abs/2107.06946",
          "publishedOn": "2021-07-16T00:48:24.316Z",
          "wordCount": 550,
          "title": "Towards Quantifying the Carbon Emissions of Differentially Private Machine Learning. (arXiv:2107.06946v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Sourav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1\">Suresh Sundaram</a>",
          "description": "In this paper, a novel confidence conditioned knowledge distillation (CCKD)\nscheme for transferring the knowledge from a teacher model to a student model\nis proposed. Existing state-of-the-art methods employ fixed loss functions for\nthis purpose and ignore the different levels of information that need to be\ntransferred for different samples. In addition to that, these methods are also\ninefficient in terms of data usage. CCKD addresses these issues by leveraging\nthe confidence assigned by the teacher model to the correct class to devise\nsample-specific loss functions (CCKD-L formulation) and targets (CCKD-T\nformulation). Further, CCKD improves the data efficiency by employing\nself-regulation to stop those samples from participating in the distillation\nprocess on which the student model learns faster. Empirical evaluations on\nseveral benchmark datasets show that CCKD methods achieve at least as much\ngeneralization performance levels as other state-of-the-art methods while being\ndata efficient in the process. Student models trained through CCKD methods do\nnot retain most of the misclassifications commited by the teacher model on the\ntraining set. Distillation through CCKD methods improves the resilience of the\nstudent models against adversarial attacks compared to the conventional KD\nmethod. Experiments show at least 3% increase in performance against\nadversarial attacks for the MNIST and the Fashion MNIST datasets, and at least\n6% increase for the CIFAR10 dataset.",
          "link": "http://arxiv.org/abs/2107.06993",
          "publishedOn": "2021-07-16T00:48:24.295Z",
          "wordCount": 650,
          "title": "Confidence Conditioned Knowledge Distillation. (arXiv:2107.06993v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06898",
          "author": "<a href=\"http://arxiv.org/find/hep-th/1/au:+Erdmenger_J/0/1/0/all/0/1\">Johanna Erdmenger</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Grosvenor_K/0/1/0/all/0/1\">Kevin T. Grosvenor</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Jefferson_R/0/1/0/all/0/1\">Ro Jefferson</a>",
          "description": "We investigate the analogy between the renormalization group (RG) and deep\nneural networks, wherein subsequent layers of neurons are analogous to\nsuccessive steps along the RG. In particular, we quantify the flow of\ninformation by explicitly computing the relative entropy or Kullback-Leibler\ndivergence in both the one- and two-dimensional Ising models under decimation\nRG, as well as in a feedforward neural network as a function of depth. We\nobserve qualitatively identical behavior characterized by the monotonic\nincrease to a parameter-dependent asymptotic value. On the quantum field theory\nside, the monotonic increase confirms the connection between the relative\nentropy and the c-theorem. For the neural networks, the asymptotic behavior may\nhave implications for various information maximization methods in machine\nlearning, as well as for disentangling compactness and generalizability.\nFurthermore, while both the two-dimensional Ising model and the random neural\nnetworks we consider exhibit non-trivial critical points, the relative entropy\nappears insensitive to the phase structure of either system. In this sense,\nmore refined probes are required in order to fully elucidate the flow of\ninformation in these models.",
          "link": "http://arxiv.org/abs/2107.06898",
          "publishedOn": "2021-07-16T00:48:24.270Z",
          "wordCount": 650,
          "title": "Towards quantifying information flows: relative entropy in deep neural networks and the renormalization group. (arXiv:2107.06898v1 [hep-th])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06936",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Barbier_J/0/1/0/all/0/1\">Jean Barbier</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Kuo Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Panchenko_D/0/1/0/all/0/1\">Dmitry Panchenko</a>, <a href=\"http://arxiv.org/find/math/1/au:+Saenz_M/0/1/0/all/0/1\">Manuel S&#xe1;enz</a>",
          "description": "For a model of high-dimensional linear regression with random design, we\nanalyze the performance of an estimator given by the mean of a log-concave\nBayesian posterior distribution with gaussian prior. The model is mismatched in\nthe following sense: like the model assumed by the statistician, the\nlabels-generating process is linear in the input data, but both the classifier\nground-truth prior and gaussian noise variance are unknown to her. This\ninference model can be rephrased as a version of the Gardner model in spin\nglasses and, using the cavity method, we provide fixed point equations for\nvarious overlap order parameters, yielding in particular an expression for the\nmean-square reconstruction error on the classifier (under an assumption of\nuniqueness of solutions). As a direct corollary we obtain an expression for the\nfree energy. Similar models have already been studied by Shcherbina and Tirozzi\nand by Talagrand, but our arguments are more straightforward and some\nassumptions are relaxed. An interesting consequence of our analysis is that in\nthe random design setting of ridge regression, the performance of the posterior\nmean is independent of the noise variance (or \"temperature\") assumed by the\nstatistician, and matches the one of the usual (zero temperature) ridge\nestimator.",
          "link": "http://arxiv.org/abs/2107.06936",
          "publishedOn": "2021-07-16T00:48:24.262Z",
          "wordCount": 660,
          "title": "Performance of Bayesian linear regression in a model with mismatch. (arXiv:2107.06936v1 [math.PR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06925",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shigang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1\">Torsten Hoefler</a>",
          "description": "Training large deep learning models at scale is very challenging. This paper\nproposes Chimera, a novel pipeline parallelism scheme which combines\nbidirectional pipelines for efficiently training large-scale models. Chimera is\na synchronous approach and therefore no loss of accuracy, which is more\nconvergence-friendly than asynchronous approaches. Compared with the latest\nsynchronous pipeline approach, Chimera reduces the number of bubbles by up to\n50%; benefiting from the sophisticated scheduling of bidirectional pipelines,\nChimera has a more balanced activation memory consumption. Evaluations are\nconducted on Transformer based language models. For a GPT-2 model with 1.3\nbillion parameters running on 2,048 GPU nodes of the Piz Daint supercomputer,\nChimera improves the training throughput by 1.16x-2.34x over the\nstate-of-the-art synchronous and asynchronous pipeline approaches.",
          "link": "http://arxiv.org/abs/2107.06925",
          "publishedOn": "2021-07-16T00:48:24.229Z",
          "wordCount": 585,
          "title": "Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines. (arXiv:2107.06925v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lily H. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_M/0/1/0/all/0/1\">Mark Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1\">Rajesh Ranganath</a>",
          "description": "Deep generative models (DGMs) seem a natural fit for detecting\nout-of-distribution (OOD) inputs, but such models have been shown to assign\nhigher probabilities or densities to OOD images than images from the training\ndistribution. In this work, we explain why this behavior should be attributed\nto model misestimation. We first prove that no method can guarantee performance\nbeyond random chance without assumptions on which out-distributions are\nrelevant. We then interrogate the typical set hypothesis, the claim that\nrelevant out-distributions can lie in high likelihood regions of the data\ndistribution, and that OOD detection should be defined based on the data\ndistribution's typical set. We highlight the consequences implied by assuming\nsupport overlap between in- and out-distributions, as well as the arbitrariness\nof the typical set for OOD detection. Our results suggest that estimation error\nis a more plausible explanation than the misalignment between likelihood-based\nOOD detection and out-distributions of interest, and we illustrate how even\nminimal estimation error can lead to OOD detection failures, yielding\nimplications for future work in deep generative modeling and OOD detection.",
          "link": "http://arxiv.org/abs/2107.06908",
          "publishedOn": "2021-07-16T00:48:24.223Z",
          "wordCount": 611,
          "title": "Understanding Failures in Out-of-Distribution Detection with Deep Generative Models. (arXiv:2107.06908v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plotka_S/0/1/0/all/0/1\">Szymon P&#x142;otka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wlodarczyk_T/0/1/0/all/0/1\">Tomasz W&#x142;odarczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klasa_A/0/1/0/all/0/1\">Adam Klasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipa_M/0/1/0/all/0/1\">Micha&#x142; Lipa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitek_A/0/1/0/all/0/1\">Arkadiusz Sitek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>",
          "description": "In this paper, we propose an end-to-end multi-task neural network called\nFetalNet with an attention mechanism and stacked module for spatio-temporal\nfetal ultrasound scan video analysis. Fetal biometric measurement is a standard\nexamination during pregnancy used for the fetus growth monitoring and\nestimation of gestational age and fetal weight. The main goal in fetal\nultrasound scan video analysis is to find proper standard planes to measure the\nfetal head, abdomen and femur. Due to natural high speckle noise and shadows in\nultrasound data, medical expertise and sonographic experience are required to\nfind the appropriate acquisition plane and perform accurate measurements of the\nfetus. In addition, existing computer-aided methods for fetal US biometric\nmeasurement address only one single image frame without considering temporal\nfeatures. To address these shortcomings, we propose an end-to-end multi-task\nneural network for spatio-temporal ultrasound scan video analysis to\nsimultaneously localize, classify and measure the fetal body parts. We propose\na new encoder-decoder segmentation architecture that incorporates a\nclassification branch. Additionally, we employ an attention mechanism with a\nstacked module to learn salient maps to suppress irrelevant US regions and\nefficient scan plane localization. We trained on the fetal ultrasound video\ncomes from routine examinations of 700 different patients. Our method called\nFetalNet outperforms existing state-of-the-art methods in both classification\nand segmentation in fetal ultrasound video recordings.",
          "link": "http://arxiv.org/abs/2107.06943",
          "publishedOn": "2021-07-16T00:48:24.213Z",
          "wordCount": 682,
          "title": "FetalNet: Multi-task deep learning framework for fetal ultrasound biometric measurements. (arXiv:2107.06943v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02266",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Khamaru_K/0/1/0/all/0/1\">Koulik Khamaru</a>, <a href=\"http://arxiv.org/find/math/1/au:+Deshpande_Y/0/1/0/all/0/1\">Yash Deshpande</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wainwright_M/0/1/0/all/0/1\">Martin J. Wainwright</a>",
          "description": "When data is collected in an adaptive manner, even simple methods like\nordinary least squares can exhibit non-normal asymptotic behavior. As an\nundesirable consequence, hypothesis tests and confidence intervals based on\nasymptotic normality can lead to erroneous results. We propose an online\ndebiasing estimator to correct these distributional anomalies in least squares\nestimation. Our proposed method takes advantage of the covariance structure\npresent in the dataset and provides sharper estimates in directions for which\nmore information has accrued. We establish an asymptotic normality property for\nour proposed online debiasing estimator under mild conditions on the data\ncollection process, and provide asymptotically exact confidence intervals. We\nadditionally prove a minimax lower bound for the adaptive linear regression\nproblem, thereby providing a baseline by which to compare estimators. There are\nvarious conditions under which our proposed estimator achieves the minimax\nlower bound up to logarithmic factors. We demonstrate the usefulness of our\ntheory via applications to multi-armed bandit, autoregressive time series\nestimation, and active learning with exploration.",
          "link": "http://arxiv.org/abs/2107.02266",
          "publishedOn": "2021-07-15T01:59:05.080Z",
          "wordCount": 623,
          "title": "Near-optimal inference in adaptive linear regression. (arXiv:2107.02266v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Truong_L/0/1/0/all/0/1\">Lan V. Truong</a>",
          "description": "This paper estimates free energy, average mutual information, and minimum\nmean square error (MMSE) of a linear model under two assumptions: (1) the\nsource is generated by a Markov chain, (2) the source is generated via a hidden\nMarkov model. Our estimates are based on the replica method in statistical\nphysics. We show that under the posterior mean estimator, the linear model with\nMarkov sources or hidden Markov sources is decoupled into single-input AWGN\nchannels with state information available at both encoder and decoder where the\nstate distribution follows the left Perron-Frobenius eigenvector with unit\nManhattan norm of the stochastic matrix of Markov chains. Numerical results\nshow that the free energies and MSEs obtained via the replica method closely\napproximate to their counterparts achieved by the Metropolis-Hastings algorithm\nor some well-known approximate message passing algorithms in the research\nliterature.",
          "link": "http://arxiv.org/abs/2009.13370",
          "publishedOn": "2021-07-15T01:59:04.915Z",
          "wordCount": 617,
          "title": "Replica Analysis of the Linear Model with Markov or Hidden Markov Signal Priors. (arXiv:2009.13370v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ashukha_A/0/1/0/all/0/1\">Arsenii Ashukha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1\">Andrei Atanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1\">Dmitry Vetrov</a>",
          "description": "Averaging predictions over a set of models -- an ensemble -- is widely used\nto improve predictive performance and uncertainty estimation of deep learning\nmodels. At the same time, many machine learning systems, such as search,\nmatching, and recommendation systems, heavily rely on embeddings.\nUnfortunately, due to misalignment of features of independently trained models,\nembeddings, cannot be improved with a naive deep ensemble like approach. In\nthis work, we look at the ensembling of representations and propose mean\nembeddings with test-time augmentation (MeTTA) simple yet well-performing\nrecipe for ensembling representations. Empirically we demonstrate that MeTTA\nsignificantly boosts the quality of linear evaluation on ImageNet for both\nsupervised and self-supervised models. Even more exciting, we draw connections\nbetween MeTTA, image retrieval, and transformation invariant models. We believe\nthat spreading the success of ensembles to inference higher-quality\nrepresentations is the important step that will open many new applications of\nensembling.",
          "link": "http://arxiv.org/abs/2106.08038",
          "publishedOn": "2021-07-15T01:59:04.856Z",
          "wordCount": 615,
          "title": "Mean Embeddings with Test-Time Data Augmentation for Ensembling of Representations. (arXiv:2106.08038v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08583",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Pokhrel_P/0/1/0/all/0/1\">Pujan Pokhrel</a>",
          "description": "This paper proposes a machine learning method based on the Extra Trees (ET)\nalgorithm for forecasting Significant Wave Heights in oceanic waters. To derive\nmultiple features from the CDIP buoys, which make point measurements, we first\nnowcast various parameters and then forecast them at 30-min intervals. The\nproposed algorithm has Scatter Index (SI), Bias, Correlation Coefficient, Root\nMean Squared Error (RMSE) of 0.130, -0.002, 0.97, and 0.14, respectively, for\none day ahead prediction and 0.110, -0.001, 0.98, and 0.122, respectively, for\n14-day ahead prediction on the testing dataset. While other state-of-the-art\nmethods can only forecast up to 120 hours ahead, we extend it further to 14\ndays. Our proposed setup includes spectral features, hv-block cross-validation,\nand stringent QC criteria. The proposed algorithm performs significantly better\nthan the state-of-the-art methods commonly used for significant wave height\nforecasting for one-day ahead prediction. Moreover, the improved performance of\nthe proposed machine learning method compared to the numerical methods shows\nthat this performance can be extended to even longer periods allowing for early\nprediction of significant wave heights in oceanic waters.",
          "link": "http://arxiv.org/abs/2105.08583",
          "publishedOn": "2021-07-15T01:59:04.842Z",
          "wordCount": 648,
          "title": "Machine Learning in weakly nonlinear systems: A Case study on Significant wave heights. (arXiv:2105.08583v3 [physics.ao-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1806.00421",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Beck_C/0/1/0/all/0/1\">Christian Beck</a>, <a href=\"http://arxiv.org/find/math/1/au:+Becker_S/0/1/0/all/0/1\">Sebastian Becker</a>, <a href=\"http://arxiv.org/find/math/1/au:+Grohs_P/0/1/0/all/0/1\">Philipp Grohs</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jaafari_N/0/1/0/all/0/1\">Nor Jaafari</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jentzen_A/0/1/0/all/0/1\">Arnulf Jentzen</a>",
          "description": "Stochastic differential equations (SDEs) and the Kolmogorov partial\ndifferential equations (PDEs) associated to them have been widely used in\nmodels from engineering, finance, and the natural sciences. In particular, SDEs\nand Kolmogorov PDEs, respectively, are highly employed in models for the\napproximative pricing of financial derivatives. Kolmogorov PDEs and SDEs,\nrespectively, can typically not be solved explicitly and it has been and still\nis an active topic of research to design and analyze numerical methods which\nare able to approximately solve Kolmogorov PDEs and SDEs, respectively. Nearly\nall approximation methods for Kolmogorov PDEs in the literature suffer under\nthe curse of dimensionality or only provide approximations of the solution of\nthe PDE at a single fixed space-time point. In this paper we derive and propose\na numerical approximation method which aims to overcome both of the above\nmentioned drawbacks and intends to deliver a numerical approximation of the\nKolmogorov PDE on an entire region $[a,b]^d$ without suffering from the curse\nof dimensionality. Numerical results on examples including the heat equation,\nthe Black-Scholes model, the stochastic Lorenz equation, and the Heston model\nsuggest that the proposed approximation algorithm is quite effective in high\ndimensions in terms of both accuracy and speed.",
          "link": "http://arxiv.org/abs/1806.00421",
          "publishedOn": "2021-07-15T01:59:04.827Z",
          "wordCount": 685,
          "title": "Solving the Kolmogorov PDE by means of deep learning. (arXiv:1806.00421v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06773",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ding_Y/0/1/0/all/0/1\">Yan Ding</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kim_Y/0/1/0/all/0/1\">Yejin Kim</a>",
          "description": "The evaluation of the BBB penetrating ability of drug molecules is a critical\nstep in brain drug development. Computational prediction based on machine\nlearning has proved to be an efficient way to conduct the evaluation. However,\nperformance of the established models has been limited by their incapability of\ndealing with the interactions between drugs and proteins, which play an\nimportant role in the mechanism behind BBB penetrating behaviors. To address\nthis issue, we employed the relational graph convolutional network (RGCN) to\nhandle the drug-protein (denoted by the encoding gene) relations as well as the\nfeatures of each individual drug. In addition, drug-drug similarity was also\nintroduced to connect structurally similar drugs in the graph. The RGCN model\nwas initially trained without input of any drug features. And the performance\nwas already promising, demonstrating the significant role of the\ndrug-protein/drug-drug relations in the prediction of BBB permeability.\nMoreover, molecular embeddings from a pre-trained knowledge graph were used as\nthe drug features to further enhance the predictive ability of the model.\nFinally, the best performing RGCN model was built with a large number of\nunlabeled drugs integrated into the graph.",
          "link": "http://arxiv.org/abs/2107.06773",
          "publishedOn": "2021-07-15T01:59:04.821Z",
          "wordCount": 628,
          "title": "Relational graph convolutional networks for predicting blood-brain barrier penetration of drug molecules. (arXiv:2107.06773v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Palmes_P/0/1/0/all/0/1\">Paulito P. Palmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kishimoto_A/0/1/0/all/0/1\">Akihiro Kishimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinescu_R/0/1/0/all/0/1\">Radu Marinescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_P/0/1/0/all/0/1\">Parikshit Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daly_E/0/1/0/all/0/1\">Elizabeth Daly</a>",
          "description": "The pipeline optimization problem in machine learning requires simultaneous\noptimization of pipeline structures and parameter adaptation of their elements.\nHaving an elegant way to express these structures can help lessen the\ncomplexity in the management and analysis of their performances together with\nthe different choices of optimization strategies. With these issues in mind, we\ncreated the AutoMLPipeline (AMLP) toolkit which facilitates the creation and\nevaluation of complex machine learning pipeline structures using simple\nexpressions. We use AMLP to find optimal pipeline signatures, datamine them,\nand use these datamined features to speed-up learning and prediction. We\nformulated a two-stage pipeline optimization with surrogate modeling in AMLP\nwhich outperforms other AutoML approaches with a 4-hour time budget in less\nthan 5 minutes of AMLP computation time.",
          "link": "http://arxiv.org/abs/2107.01253",
          "publishedOn": "2021-07-15T01:59:04.805Z",
          "wordCount": 582,
          "title": "Designing Machine Learning Pipeline Toolkit for AutoML Surrogate Modeling Optimization. (arXiv:2107.01253v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jahin_M/0/1/0/all/0/1\">Md. Abrar Jahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krutsylo_A/0/1/0/all/0/1\">Andrii Krutsylo</a>",
          "description": "The research internship at UiT - The Arctic University of Norway was offered\nfor our team being the winner of the 'Smart Roads - Winter Road Maintenance\n2021' Hackathon. The internship commenced on 3 May 2021 and ended on 21 May\n2021 with meetings happening twice each week. In spite of having different\nnationalities and educational backgrounds, we both interns tried to collaborate\nas a team as much as possible. The most alluring part was working on this\nproject made us realize the critical conditions faced by the arctic people,\nwhere it was hard to gain such a unique experience from our residence. We\ndeveloped and implemented several deep learning models to classify the states\n(dry, moist, wet, icy, snowy, slushy). Depending upon the best model, the\nweather forecast app will predict the state taking the Ta, Tsurf, Height,\nSpeed, Water, etc. into consideration. The crucial part was to define a safety\nmetric which is the product of the accident rates based on friction and the\naccident rates based on states. We developed a regressor that will predict the\nsafety metric depending upon the state obtained from the classifier and the\nfriction obtained from the sensor data. A pathfinding algorithm has been\ndesigned using the sensor data, open street map data, weather data.",
          "link": "http://arxiv.org/abs/2107.06755",
          "publishedOn": "2021-07-15T01:59:04.797Z",
          "wordCount": 636,
          "title": "DIT4BEARs Smart Roads Internship. (arXiv:2107.06755v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2003.08938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongge Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boning_D/0/1/0/all/0/1\">Duane Boning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>",
          "description": "A deep reinforcement learning (DRL) agent observes its states through\nobservations, which may contain natural measurement errors or adversarial\nnoises. Since the observations deviate from the true states, they can mislead\nthe agent into making suboptimal actions. Several works have shown this\nvulnerability via adversarial attacks, but existing approaches on improving the\nrobustness of DRL under this setting have limited success and lack for\ntheoretical principles. We show that naively applying existing techniques on\nimproving robustness for classification tasks, like adversarial training, is\nineffective for many RL tasks. We propose the state-adversarial Markov decision\nprocess (SA-MDP) to study the fundamental properties of this problem, and\ndevelop a theoretically principled policy regularization which can be applied\nto a large family of DRL algorithms, including proximal policy optimization\n(PPO), deep deterministic policy gradient (DDPG) and deep Q networks (DQN), for\nboth discrete and continuous action control problems. We significantly improve\nthe robustness of PPO, DDPG and DQN agents under a suite of strong white box\nadversarial attacks, including new attacks of our own. Additionally, we find\nthat a robust policy noticeably improves DRL performance even without an\nadversary in a number of environments. Our code is available at\nhttps://github.com/chenhongge/StateAdvDRL.",
          "link": "http://arxiv.org/abs/2003.08938",
          "publishedOn": "2021-07-15T01:59:04.765Z",
          "wordCount": 740,
          "title": "Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations. (arXiv:2003.08938v7 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11730",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_S/0/1/0/all/0/1\">Sheng Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Sian Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dingwen Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zizhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cappello_F/0/1/0/all/0/1\">Franck Cappello</a>",
          "description": "Error-bounded lossy compression is becoming an indispensable technique for\nthe success of today's scientific projects with vast volumes of data produced\nduring the simulations or instrument data acquisitions. Not only can it\nsignificantly reduce data size, but it also can control the compression errors\nbased on user-specified error bounds. Autoencoder (AE) models have been widely\nused in image compression, but few AE-based compression approaches support\nerror-bounding features, which are highly required by scientific applications.\nTo address this issue, we explore using convolutional autoencoders to improve\nerror-bounded lossy compression for scientific data, with the following three\nkey contributions. (1) We provide an in-depth investigation of the\ncharacteristics of various autoencoder models and develop an error-bounded\nautoencoder-based framework in terms of the SZ model. (2) We optimize the\ncompression quality for main stages in our designed AE-based error-bounded\ncompression framework, fine-tuning the block sizes and latent sizes and also\noptimizing the compression efficiency of latent vectors. (3) We evaluate our\nproposed solution using five real-world scientific datasets and comparing them\nwith six other related works. Experiments show that our solution exhibits a\nvery competitive compression quality from among all the compressors in our\ntests. In absolute terms, it can obtain a much better compression quality (100%\n~ 800% improvement in compression ratio with the same data distortion) compared\nwith SZ2.1 and ZFP in cases with a high compression ratio.",
          "link": "http://arxiv.org/abs/2105.11730",
          "publishedOn": "2021-07-15T01:59:04.751Z",
          "wordCount": 705,
          "title": "Exploring Autoencoder-based Error-bounded Compression for Scientific Data. (arXiv:2105.11730v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mo_Z/0/1/0/all/0/1\">Zhaobin Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1\">Xuan Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1\">Rongye Shi</a>",
          "description": "Car-following behavior has been extensively studied using physics-based\nmodels, such as the Intelligent Driver Model. These models successfully\ninterpret traffic phenomena observed in the real-world but may not fully\ncapture the complex cognitive process of driving. Deep learning models, on the\nother hand, have demonstrated their power in capturing observed traffic\nphenomena but require a large amount of driving data to train. This paper aims\nto develop a family of neural network based car-following models that are\ninformed by physics-based models, which leverage the advantage of both\nphysics-based (being data-efficient and interpretable) and deep learning based\n(being generalizable) models. We design physics-informed deep learning\ncar-following (PIDL-CF) architectures encoded with two popular physics-based\nmodels - IDM and OVM, on which acceleration is predicted for four traffic\nregimes: acceleration, deceleration, cruising, and emergency braking. Two types\nof PIDL-CFM problems are studied, one to predict acceleration only and the\nother to jointly predict acceleration and discover model parameters. We also\ndemonstrate the superior performance of PIDL with the Next Generation\nSIMulation (NGSIM) dataset over baselines, especially when the training data is\nsparse. The results demonstrate the superior performance of neural networks\ninformed by physics over those without. The developed PIDL-CF framework holds\nthe potential for system identification of driving models and for the\ndevelopment of driving-based controls for automated vehicles.",
          "link": "http://arxiv.org/abs/2012.13376",
          "publishedOn": "2021-07-15T01:59:04.732Z",
          "wordCount": 697,
          "title": "A Physics-Informed Deep Learning Paradigm for Car-Following Models. (arXiv:2012.13376v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1\">Kuncahyo Setyo Nugroho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1\">Kuncahyo Setyo Nugroho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1\">Novanto Yudistira</a>",
          "description": "The rise of big data analytics on top of NLP increases the computational\nburden for text processing at scale. The problems faced in NLP are very high\ndimensional text, so it takes a high computation resource. The MapReduce allows\nparallelization of large computations and can improve the efficiency of text\nprocessing. This research aims to study the effect of big data processing on\nNLP tasks based on a deep learning approach. We classify a big text of news\ntopics with fine-tuning BERT used pre-trained models. Five pre-trained models\nwith a different number of parameters were used in this study. To measure the\nefficiency of this method, we compared the performance of the BERT with the\npipelines from Spark NLP. The result shows that BERT without Spark NLP gives\nhigher accuracy compared to BERT with Spark NLP. The accuracy average and\ntraining time of all models using BERT is 0.9187 and 35 minutes while using\nBERT with Spark NLP pipeline is 0.8444 and 9 minutes. The bigger model will\ntake more computation resources and need a longer time to complete the tasks.\nHowever, the accuracy of BERT with Spark NLP only decreased by an average of\n5.7%, while the training time was reduced significantly by 62.9% compared to\nBERT without Spark NLP.",
          "link": "http://arxiv.org/abs/2107.06785",
          "publishedOn": "2021-07-15T01:59:04.696Z",
          "wordCount": 653,
          "title": "Large-Scale News Classification using BERT Language Model: Spark NLP Approach. (arXiv:2107.06785v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06845",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Alawode_B/0/1/0/all/0/1\">Basit O. Alawode</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alfarraj_M/0/1/0/all/0/1\">Motaz Alfarraj</a>",
          "description": "The recent application of deep learning (DL) to various tasks has seen the\nperformance of classical techniques surpassed by their DL-based counterparts.\nAs a result, DL has equally seen application in the removal of noise from\nimages. In particular, the use of deep feed-forward convolutional neural\nnetworks (DnCNNs) has been investigated for denoising. It utilizes advances in\nDL techniques such as deep architecture, residual learning, and batch\nnormalization to achieve better denoising performance when compared with the\nother classical state-of-the-art denoising algorithms. However, its deep\narchitecture resulted in a huge set of trainable parameters. Meta-optimization\nis a training approach of enabling algorithms to learn to train themselves by\nthemselves. Training algorithms using meta-optimizers have been shown to enable\nalgorithms to achieve better performance when compared to the classical\ngradient descent-based training approach. In this work, we investigate the\napplication of the meta-optimization training approach to the DnCNN denoising\nalgorithm to enhance its denoising capability. Our preliminary experiments on\nsimpler algorithms reveal the prospects of utilizing the meta-optimization\ntraining approach towards the enhancement of the DnCNN denoising capability.",
          "link": "http://arxiv.org/abs/2107.06845",
          "publishedOn": "2021-07-15T01:59:04.688Z",
          "wordCount": 621,
          "title": "Meta-Optimization of Deep CNN for Image Denoising Using LSTM. (arXiv:2107.06845v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.14227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Quanming Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>",
          "description": "Negative sampling, which samples negative triplets from non-observed ones in\nknowledge graph (KG), is an essential step in KG embedding. Recently,\ngenerative adversarial network (GAN), has been introduced in negative sampling.\nBy sampling negative triplets with large gradients, these methods avoid the\nproblem of vanishing gradient and thus obtain better performance. However, they\nmake the original model more complex and harder to train. In this paper,\nmotivated by the observation that negative triplets with large gradients are\nimportant but rare, we propose to directly keep track of them with the cache.\nIn this way, our method acts as a \"distilled\" version of previous GAN-based\nmethods, which does not waste training time on additional parameters to fit the\nfull distribution of negative triplets. However, how to sample from and update\nthe cache are two critical questions. We propose to solve these issues by\nautomated machine learning techniques. The automated version also covers\nGAN-based methods as special cases. Theoretical explanation of NSCaching is\nalso provided, justifying the superior over fixed sampling scheme. Besides, we\nfurther extend NSCaching with skip-gram model for graph embedding. Finally,\nextensive experiments show that our method can gain significant improvements on\nvarious KG embedding models and the skip-gram model, and outperforms the\nstate-of-the-art negative sampling methods.",
          "link": "http://arxiv.org/abs/2010.14227",
          "publishedOn": "2021-07-15T01:59:04.682Z",
          "wordCount": 686,
          "title": "Efficient, Simple and Automated Negative Sampling for Knowledge Graph Embedding. (arXiv:2010.14227v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feldman_M/0/1/0/all/0/1\">Moran Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harshaw_C/0/1/0/all/0/1\">Christopher Harshaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karbasi_A/0/1/0/all/0/1\">Amin Karbasi</a>",
          "description": "We present SimultaneousGreedys, a deterministic algorithm for constrained\nsubmodular maximization. At a high level, the algorithm maintains $\\ell$\nsolutions and greedily updates them in a simultaneous fashion.\nSimultaneousGreedys achieves the tightest known approximation guarantees for\nboth $k$-extendible systems and the more general $k$-systems, which are\n$(k+1)^2/k = k + \\mathcal{O}(1)$ and $(1 + \\sqrt{k+2})^2 = k +\n\\mathcal{O}(\\sqrt{k})$, respectively. This is in contrast to previous\nalgorithms, which are designed to provide tight approximation guarantees in one\nsetting, but not both. We also improve the analysis of RepeatedGreedy, showing\nthat it achieves an approximation ratio of $k + \\mathcal{O}(\\sqrt{k})$ for\n$k$-systems when allowed to run for $\\mathcal{O}(\\sqrt{k})$ iterations, an\nimprovement in both the runtime and approximation over previous analyses. We\ndemonstrate that both algorithms may be modified to run in nearly linear time\nwith an arbitrarily small loss in the approximation.\n\nBoth SimultaneousGreedys and RepeatedGreedy are flexible enough to\nincorporate the intersection of $m$ additional knapsack constraints, while\nretaining similar approximation guarantees: both algorithms yield an\napproximation guarantee of roughly $k + 2m + \\mathcal{O}(\\sqrt{k+m})$ for\n$k$-systems and SimultaneousGreedys enjoys an improved approximation guarantee\nof $k+2m + \\mathcal{O}(\\sqrt{m})$ for $k$-extendible systems. To complement our\nalgorithmic contributions, we provide a hardness result which states that no\nalgorithm making polynomially many oracle queries can achieve an approximation\nbetter than $k + 1/2 + \\varepsilon$. We also present SubmodularGreedy.jl, a\nJulia package which implements these algorithms and may be downloaded at\nhttps://github.com/crharshaw/SubmodularGreedy.jl . Finally, we test the\neffectiveness of these algorithms on real datasets.",
          "link": "http://arxiv.org/abs/2009.13998",
          "publishedOn": "2021-07-15T01:59:04.656Z",
          "wordCount": 734,
          "title": "How Do You Want Your Greedy: Simultaneous or Repeated?. (arXiv:2009.13998v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petersen_F/0/1/0/all/0/1\">Felix Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgelt_C/0/1/0/all/0/1\">Christian Borgelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deussen_O/0/1/0/all/0/1\">Oliver Deussen</a>",
          "description": "Sorting and ranking supervision is a method for training neural networks\nend-to-end based on ordering constraints. That is, the ground truth order of\nsets of samples is known, while their absolute values remain unsupervised. For\nthat, we propose differentiable sorting networks by relaxing their pairwise\nconditional swap operations. To address the problems of vanishing gradients and\nextensive blurring that arise with larger numbers of layers, we propose mapping\nactivations to regions with moderate gradients. We consider odd-even as well as\nbitonic sorting networks, which outperform existing relaxations of the sorting\noperation. We show that bitonic sorting networks can achieve stable training on\nlarge input sets of up to 1024 elements.",
          "link": "http://arxiv.org/abs/2105.04019",
          "publishedOn": "2021-07-15T01:59:04.650Z",
          "wordCount": 595,
          "title": "Differentiable Sorting Networks for Scalable Sorting and Ranking Supervision. (arXiv:2105.04019v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reinhold_J/0/1/0/all/0/1\">Jacob C. Reinhold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carass_A/0/1/0/all/0/1\">Aaron Carass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1\">Jerry L. Prince</a>",
          "description": "Precision medicine involves answering counterfactual questions such as \"Would\nthis patient respond better to treatment A or treatment B?\" These types of\nquestions are causal in nature and require the tools of causal inference to be\nanswered, e.g., with a structural causal model (SCM). In this work, we develop\nan SCM that models the interaction between demographic information, disease\ncovariates, and magnetic resonance (MR) images of the brain for people with\nmultiple sclerosis. Inference in the SCM generates counterfactual images that\nshow what an MR image of the brain would look like if demographic or disease\ncovariates are changed. These images can be used for modeling disease\nprogression or used for image processing tasks where controlling for\nconfounders is necessary.",
          "link": "http://arxiv.org/abs/2103.03158",
          "publishedOn": "2021-07-15T01:59:04.644Z",
          "wordCount": 616,
          "title": "A Structural Causal Model for MR Images of Multiple Sclerosis. (arXiv:2103.03158v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shim_J/0/1/0/all/0/1\">Jae-hun Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1\">Kyeongbo Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Suk-Ju Kang</a>",
          "description": "Neural architecture search (NAS), an important branch of automatic machine\nlearning, has become an effective approach to automate the design of deep\nlearning models. However, the major issue in NAS is how to reduce the large\nsearch time imposed by the heavy computational burden. While most recent\napproaches focus on pruning redundant sets or developing new search\nmethodologies, this paper attempts to formulate the problem based on the data\ncuration manner. Our key strategy is to search the architecture using\nsummarized data distribution, i.e., core-set. Typically, many NAS algorithms\nseparate searching and training stages, and the proposed core-set methodology\nis only used in search stage, thus their performance degradation can be\nminimized. In our experiments, we were able to save overall computational time\nfrom 30.8 hours to 3.5 hours, 8.8x reduction, on a single RTX 3090 GPU without\nsacrificing accuracy.",
          "link": "http://arxiv.org/abs/2107.06869",
          "publishedOn": "2021-07-15T01:59:04.638Z",
          "wordCount": 589,
          "title": "Core-set Sampling for Efficient Neural Architecture Search. (arXiv:2107.06869v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tengyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langford_J/0/1/0/all/0/1\">John Langford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mineiro_P/0/1/0/all/0/1\">Paul Mineiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1\">Ida Momennejad</a>",
          "description": "Consider a prosthetic arm, learning to adapt to its user's control signals.\nWe propose Interaction-Grounded Learning for this novel setting, in which a\nlearner's goal is to interact with the environment with no grounding or\nexplicit reward to optimize its policies. Such a problem evades common RL\nsolutions which require an explicit reward. The learning agent observes a\nmultidimensional context vector, takes an action, and then observes a\nmultidimensional feedback vector. This multidimensional feedback vector has no\nexplicit reward information. In order to succeed, the algorithm must learn how\nto evaluate the feedback vector to discover a latent reward signal, with which\nit can ground its policies without supervision. We show that in an\nInteraction-Grounded Learning setting, with certain natural assumptions, a\nlearner can discover the latent reward and ground its policy for successful\ninteraction. We provide theoretical guarantees and a proof-of-concept empirical\nevaluation to demonstrate the effectiveness of our proposed approach.",
          "link": "http://arxiv.org/abs/2106.04887",
          "publishedOn": "2021-07-15T01:59:04.623Z",
          "wordCount": 613,
          "title": "Interaction-Grounded Learning. (arXiv:2106.04887v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wairagkar_M/0/1/0/all/0/1\">Maitreyee Wairagkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villeneuve_E/0/1/0/all/0/1\">Emma Villeneuve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_R/0/1/0/all/0/1\">Rachel King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janko_B/0/1/0/all/0/1\">Balazs Janko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnett_M/0/1/0/all/0/1\">Malcolm Burnett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashburn_A/0/1/0/all/0/1\">Ann Ashburn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_V/0/1/0/all/0/1\">Veena Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sherratt_R/0/1/0/all/0/1\">R. Simon Sherratt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holderbaum_W/0/1/0/all/0/1\">William Holderbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwin_W/0/1/0/all/0/1\">William Harwin</a>",
          "description": "Sit-to-stand transitions are an important part of activities of daily living\nand play a key role in functional mobility in humans. The sit-to-stand movement\nis often affected in older adults due to frailty and in patients with motor\nimpairments such as Parkinson's disease leading to falls. Studying kinematics\nof sit-to-stand transitions can provide insight in assessment, monitoring and\ndeveloping rehabilitation strategies for the affected populations. We propose a\nthree-segment body model for estimating sit-to-stand kinematics using only two\nwearable inertial sensors, placed on the shank and back. Reducing the number of\nsensors to two instead of one per body segment facilitates monitoring and\nclassifying movements over extended periods, making it more comfortable to wear\nwhile reducing the power requirements of sensors. We applied this model on 10\nyounger healthy adults (YH), 12 older healthy adults (OH) and 12 people with\nParkinson's disease (PwP). We have achieved this by incorporating unique\nsit-to-stand classification technique using unsupervised learning in the model\nbased reconstruction of angular kinematics using extended Kalman filter. Our\nproposed model showed that it was possible to successfully estimate thigh\nkinematics despite not measuring the thigh motion with inertial sensor. We\nclassified sit-to-stand transitions, sitting and standing states with the\naccuracies of 98.67%, 94.20% and 91.41% for YH, OH and PwP respectively. We\nhave proposed a novel integrated approach of modelling and classification for\nestimating the body kinematics during sit-to-stand motion and successfully\napplied it on YH, OH and PwP groups.",
          "link": "http://arxiv.org/abs/2107.06859",
          "publishedOn": "2021-07-15T01:59:04.617Z",
          "wordCount": 699,
          "title": "A novel approach for modelling and classifying sit-to-stand kinematics using inertial sensors. (arXiv:2107.06859v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06677",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gutierrez_Estevez_M/0/1/0/all/0/1\">M. A. Gutierrez-Estevez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kasparick_M/0/1/0/all/0/1\">Martin Kasparick</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cavalvante_R/0/1/0/all/0/1\">Renato L. G. Cavalvante</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stanczak_S/0/1/0/all/0/1\">S&#x142;awomir Sta&#x144;czak</a>",
          "description": "Learning any-to-any (A2A) path loss maps, where the objective is the\nreconstruction of path loss between any two given points in a map, might be a\nkey enabler for many applications that rely on device-to-device (D2D)\ncommunication. Such applications include machine-type communications (MTC) or\nvehicle-to-vehicle (V2V) communications. Current approaches for learning A2A\nmaps are either model-based methods, or pure data-driven methods. Model-based\nmethods have the advantage that they can generate reliable estimations with low\ncomputational complexity, but they cannot exploit information coming from data.\nPure data-driven methods can achieve good performance without assuming any\nphysical model, but their complexity and their lack of robustness is not\nacceptable for many applications. In this paper, we propose a novel hybrid\nmodel and data-driven approach that fuses information obtained from datasets\nand models in an online fashion. To that end, we leverage the framework of\nstochastic learning to deal with the sequential arrival of samples and propose\nan online algorithm that alternatively and sequentially minimizes the original\nnon-convex problem. A proof of convergence is presented, along with experiments\nbased firstly on synthetic data, and secondly on a more realistic dataset for\nV2X, with both experiments showing promising results.",
          "link": "http://arxiv.org/abs/2107.06677",
          "publishedOn": "2021-07-15T01:59:04.609Z",
          "wordCount": 650,
          "title": "Hybrid Model and Data Driven Algorithm for Online Learning of Any-to-Any Path Loss Maps. (arXiv:2107.06677v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00120",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lohrenz_T/0/1/0/all/0/1\">Timo Lohrenz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhengyang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fingscheidt_T/0/1/0/all/0/1\">Tim Fingscheidt</a>",
          "description": "Stream fusion, also known as system combination, is a common technique in\nautomatic speech recognition for traditional hybrid hidden Markov model\napproaches, yet mostly unexplored for modern deep neural network end-to-end\nmodel architectures. Here, we investigate various fusion techniques for the\nall-attention-based encoder-decoder architecture known as the transformer,\nstriving to achieve optimal fusion by investigating different fusion levels in\nan example single-microphone setting with fusion of standard magnitude and\nphase features. We introduce a novel multi-encoder learning method that\nperforms a weighted combination of two encoder-decoder multi-head attention\noutputs only during training. Employing then only the magnitude feature encoder\nin inference, we are able to show consistent improvement on Wall Street Journal\n(WSJ) with language model and on Librispeech, without increase in runtime or\nparameters. Combining two such multi-encoder trained models by a simple late\nfusion in inference, we achieve state-of-the-art performance for\ntransformer-based models on WSJ with a significant WER reduction of 19%\nrelative compared to the current benchmark approach.",
          "link": "http://arxiv.org/abs/2104.00120",
          "publishedOn": "2021-07-15T01:59:04.603Z",
          "wordCount": 638,
          "title": "Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition. (arXiv:2104.00120v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08817",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Graf_F/0/1/0/all/0/1\">Florian Graf</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hofer_C/0/1/0/all/0/1\">Christoph D. Hofer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kwitt_R/0/1/0/all/0/1\">Roland Kwitt</a>",
          "description": "Minimizing cross-entropy over the softmax scores of a linear map composed\nwith a high-capacity encoder is arguably the most popular choice for training\nneural networks on supervised learning tasks. However, recent works show that\none can directly optimize the encoder instead, to obtain equally (or even more)\ndiscriminative representations via a supervised variant of a contrastive\nobjective. In this work, we address the question whether there are fundamental\ndifferences in the sought-for representation geometry in the output space of\nthe encoder at minimal loss. Specifically, we prove, under mild assumptions,\nthat both losses attain their minimum once the representations of each class\ncollapse to the vertices of a regular simplex, inscribed in a hypersphere. We\nprovide empirical evidence that this configuration is attained in practice and\nthat reaching a close-to-optimal state typically indicates good generalization\nperformance. Yet, the two losses show remarkably different optimization\nbehavior. The number of iterations required to perfectly fit to data scales\nsuperlinearly with the amount of randomly flipped labels for the supervised\ncontrastive loss. This is in contrast to the approximately linear scaling\npreviously reported for networks trained with cross-entropy.",
          "link": "http://arxiv.org/abs/2102.08817",
          "publishedOn": "2021-07-15T01:59:04.598Z",
          "wordCount": 644,
          "title": "Dissecting Supervised Contrastive Learning. (arXiv:2102.08817v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhaowei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yiwen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "The label noise transition matrix, characterizing the probabilities of a\ntraining instance being wrongly annotated, is crucial to designing popular\nsolutions to learning with noisy labels. Existing works heavily rely on finding\n\"anchor points\" or their approximates, defined as instances belonging to a\nparticular class almost surely. Nonetheless, finding anchor points remains a\nnon-trivial task, and the estimation accuracy is also often throttled by the\nnumber of available anchor points. In this paper, we propose an alternative\noption to the above task. Our main contribution is the discovery of an\nefficient estimation procedure based on a clusterability condition. We prove\nthat with clusterable representations of features, using up to third-order\nconsensuses of noisy labels among neighbor representations is sufficient to\nestimate a unique transition matrix. Compared with methods using anchor points,\nour approach uses substantially more instances and benefits from a much better\nsample complexity. We demonstrate the estimation accuracy and advantages of our\nestimates using both synthetic noisy labels (on CIFAR-10/100) and real\nhuman-level noisy labels (on Clothing1M and our self-collected human-annotated\nCIFAR-10). Our code and human-level noisy CIFAR-10 labels are available at\nhttps://github.com/UCSC-REAL/HOC.",
          "link": "http://arxiv.org/abs/2102.05291",
          "publishedOn": "2021-07-15T01:59:04.592Z",
          "wordCount": 662,
          "title": "Clusterability as an Alternative to Anchor Points When Learning with Noisy Labels. (arXiv:2102.05291v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yinghua Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yuangang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor W.Tsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xin Yao</a>",
          "description": "This paper proposes Differential-Critic Generative Adversarial Network\n(DiCGAN) to learn the distribution of user-desired data when only partial\ninstead of the entire dataset possesses the desired property, which generates\ndesired data that meets user's expectations and can assist in designing\nbiological products with desired properties. Existing approaches select the\ndesired samples first and train regular GANs on the selected samples to derive\nthe user-desired data distribution. However, the selection of the desired data\nrelies on an expert criterion and supervision over the entire dataset. DiCGAN\nintroduces a differential critic that can learn the preference direction from\nthe pairwise preferences, which is amateur knowledge and can be defined on part\nof the training data. The resultant critic guides the generation of the desired\ndata instead of the whole data. Specifically, apart from the Wasserstein GAN\nloss, a ranking loss of the pairwise preferences is defined over the critic. It\nendows the difference of critic values between each pair of samples with the\npairwise preference relation. The higher critic value indicates that the sample\nis preferred by the user. Thus training the generative model for higher critic\nvalues encourages the generation of user-preferred samples. Extensive\nexperiments show that our DiCGAN achieves state-of-the-art performance in\nlearning the user-desired data distributions, especially in the cases of\ninsufficient desired data and limited supervision.",
          "link": "http://arxiv.org/abs/2107.06700",
          "publishedOn": "2021-07-15T01:59:04.558Z",
          "wordCount": 654,
          "title": "Differential-Critic GAN: Generating What You Want by a Cue of Preferences. (arXiv:2107.06700v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Sangmin Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungnyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1\">Jongwoo Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gihun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_S/0/1/0/all/0/1\">Seungjong Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>",
          "description": "This paper proposes a novel contrastive learning framework, coined as\nSelf-Contrastive (SelfCon) Learning, that self-contrasts within multiple\noutputs from the different levels of a network. We confirmed that SelfCon loss\nguarantees the lower bound of mutual information (MI) between the intermediate\nand last representations. Besides, we empirically showed, via various MI\nestimators, that SelfCon loss highly correlates to the increase of MI and\nbetter classification performance. In our experiments, SelfCon surpasses\nsupervised contrastive (SupCon) learning without the need for a multi-viewed\nbatch and with the cheaper computational cost. Especially on ResNet-18, we\nachieved top-1 classification accuracy of 76.45% for the CIFAR-100 dataset,\nwhich is 2.87% and 4.36% higher than SupCon and cross-entropy loss,\nrespectively. We found that mitigating both vanishing gradient and overfitting\nissue makes our method outperform the counterparts.",
          "link": "http://arxiv.org/abs/2106.15499",
          "publishedOn": "2021-07-15T01:59:04.552Z",
          "wordCount": 572,
          "title": "Self-Contrastive Learning. (arXiv:2106.15499v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01926",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Coulombe_P/0/1/0/all/0/1\">Philippe Goulet Coulombe</a>",
          "description": "Random Forest's performance can be matched by a single slow-growing tree\n(SGT), which uses a learning rate to tame CART's greedy algorithm. SGT exploits\nthe view that CART is an extreme case of an iterative weighted least square\nprocedure. Moreover, a unifying view of Boosted Trees (BT) and Random Forests\n(RF) is presented. Greedy ML algorithms' outcomes can be improved using either\n\"slow learning\" or diversification. SGT applies the former to estimate a single\ndeep tree, and Booging (bagging stochastic BT with a high learning rate) uses\nthe latter with additive shallow trees. The performance of this tree ensemble\nquaternity (Booging, BT, SGT, RF) is assessed on simulated and real regression\ntasks.",
          "link": "http://arxiv.org/abs/2103.01926",
          "publishedOn": "2021-07-15T01:59:04.532Z",
          "wordCount": 554,
          "title": "Slow-Growing Trees. (arXiv:2103.01926v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_R/0/1/0/all/0/1\">Reshma Rastogi</a> (nee. Khemchandani), <a href=\"http://arxiv.org/find/cs/1/au:+Pal_A/0/1/0/all/0/1\">Aman Pal</a>",
          "description": "In any learning framework, an expert knowledge always plays a crucial role.\nBut, in the field of machine learning, the knowledge offered by an expert is\nrarely used. Moreover, machine learning algorithms (SVM based) generally use\nhinge loss function which is sensitive towards the noise. Thus, in order to get\nthe advantage from an expert knowledge and to reduce the sensitivity towards\nthe noise, in this paper, we propose privileged information based Twin Pinball\nSupport Vector Machine classifier (Pin-TWSVMPI) where expert's knowledge is in\nthe form of privileged information. The proposed Pin-TWSVMPI incorporates\nprivileged information by using correcting function so as to obtain two\nnonparallel decision hyperplanes. Further, in order to make computations more\nefficient and fast, we use Sequential Minimal Optimization (SMO) technique for\nobtaining the classifier and have also shown its application for Pedestrian\ndetection and Handwritten digit recognition. Further, for UCI datasets, we\nfirst implement a procedure which extracts privileged information from the\nfeatures of the dataset which are then further utilized by Pin-TWSVMPI that\nleads to enhancement in classification accuracy with comparatively lesser\ncomputational time.",
          "link": "http://arxiv.org/abs/2107.06744",
          "publishedOn": "2021-07-15T01:59:04.461Z",
          "wordCount": 612,
          "title": "Efficient Learning of Pinball TWSVM using Privileged Information and its applications. (arXiv:2107.06744v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zheyu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1\">Da-Cheng Juan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaobo Sharon Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>",
          "description": "Emerging device-based Computing-in-memory (CiM) has been proved to be a\npromising candidate for high-energy efficiency deep neural network (DNN)\ncomputations. However, most emerging devices suffer uncertainty issues,\nresulting in a difference between actual data stored and the weight value it is\ndesigned to be. This leads to an accuracy drop from trained models to actually\ndeployed platforms. In this work, we offer a thorough analysis of the effect of\nsuch uncertainties-induced changes in DNN models. To reduce the impact of\ndevice uncertainties, we propose UAE, an uncertainty-aware Neural Architecture\nSearch scheme to identify a DNN model that is both accurate and robust against\ndevice uncertainties.",
          "link": "http://arxiv.org/abs/2107.06871",
          "publishedOn": "2021-07-15T01:59:04.454Z",
          "wordCount": 553,
          "title": "Uncertainty Modeling of Emerging Device-based Computing-in-Memory Neural Accelerators with Application to Neural Architecture Search. (arXiv:2107.06871v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bighashdel_A/0/1/0/all/0/1\">Ariyan Bighashdel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meletis_P/0/1/0/all/0/1\">Panagiotis Meletis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jancura_P/0/1/0/all/0/1\">Pavol Jancura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubbelman_G/0/1/0/all/0/1\">Gijs Dubbelman</a>",
          "description": "This paper presents a deep Inverse Reinforcement Learning (IRL) framework\nthat can learn an a priori unknown number of nonlinear reward functions from\nunlabeled experts' demonstrations. For this purpose, we employ the tools from\nDirichlet processes and propose an adaptive approach to simultaneously account\nfor both complex and unknown number of reward functions. Using the conditional\nmaximum entropy principle, we model the experts' multi-intention behaviors as a\nmixture of latent intention distributions and derive two algorithms to estimate\nthe parameters of the deep reward network along with the number of experts'\nintentions from unlabeled demonstrations. The proposed algorithms are evaluated\non three benchmarks, two of which have been specifically extended in this study\nfor multi-intention IRL, and compared with well-known baselines. We demonstrate\nthrough several experiments the advantages of our algorithms over the existing\napproaches and the benefits of online inferring, rather than fixing beforehand,\nthe number of expert's intentions.",
          "link": "http://arxiv.org/abs/2107.06692",
          "publishedOn": "2021-07-15T01:59:04.448Z",
          "wordCount": 586,
          "title": "Deep Adaptive Multi-Intention Inverse Reinforcement Learning. (arXiv:2107.06692v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06658",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Levine_M/0/1/0/all/0/1\">Matthew E. Levine</a>, <a href=\"http://arxiv.org/find/math/1/au:+Stuart_A/0/1/0/all/0/1\">Andrew M. Stuart</a>",
          "description": "The development of data-informed predictive models for dynamical systems is\nof widespread interest in many disciplines. We present a unifying framework for\nblending mechanistic and machine-learning approaches to identify dynamical\nsystems from data. We compare pure data-driven learning with hybrid models\nwhich incorporate imperfect domain knowledge. We cast the problem in both\ncontinuous- and discrete-time, for problems in which the model error is\nmemoryless and in which it has significant memory, and we compare data-driven\nand hybrid approaches experimentally. Our formulation is agnostic to the chosen\nmachine learning model.\n\nUsing Lorenz '63 and Lorenz '96 Multiscale systems, we find that hybrid\nmethods substantially outperform solely data-driven approaches in terms of data\nhunger, demands for model complexity, and overall predictive performance. We\nalso find that, while a continuous-time framing allows for robustness to\nirregular sampling and desirable domain-interpretability, a discrete-time\nframing can provide similar or better predictive performance, especially when\ndata are undersampled and the vector field cannot be resolved.\n\nWe study model error from the learning theory perspective, defining excess\nrisk and generalization error; for a linear model of the error used to learn\nabout ergodic dynamical systems, both errors are bounded by terms that diminish\nwith the square-root of T. We also illustrate scenarios that benefit from\nmodeling with memory, proving that continuous-time recurrent neural networks\n(RNNs) can, in principle, learn memory-dependent model error and reconstruct\nthe original system arbitrarily well; numerical results depict challenges in\nrepresenting memory by this approach. We also connect RNNs to reservoir\ncomputing and thereby relate the learning of memory-dependent error to recent\nwork on supervised learning between Banach spaces using random features.",
          "link": "http://arxiv.org/abs/2107.06658",
          "publishedOn": "2021-07-15T01:59:04.443Z",
          "wordCount": 711,
          "title": "A Framework for Machine Learning of Model Error in Dynamical Systems. (arXiv:2107.06658v1 [math.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2011.08694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Shohin Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mousavian_A/0/1/0/all/0/1\">Arsalan Mousavian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fishman_A/0/1/0/all/0/1\">Adam Fishman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likhachev_M/0/1/0/all/0/1\">Maxim Likhachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>",
          "description": "Zero-shot execution of unseen robotic tasks is important to allowing robots\nto perform a wide variety of tasks in human environments, but collecting the\namounts of data necessary to train end-to-end policies in the real-world is\noften infeasible. We describe an approach for sim-to-real training that can\naccomplish unseen robotic tasks using models learned in simulation to ground\ncomponents of a simple task planner. We learn a library of parameterized\nskills, along with a set of predicates-based preconditions and termination\nconditions, entirely in simulation. We explore a block-stacking task because it\nhas a clear structure, where multiple skills must be chained together, but our\nmethods are applicable to a wide range of other problems and domains, and can\ntransfer from simulation to the real-world with no fine tuning. The system is\nable to recognize failures and accomplish long-horizon tasks from perceptual\ninput, which is critical for real-world execution. We evaluate our proposed\napproach in both simulation and in the real-world, showing an increase in\nsuccess rate from 91.6% to 98% in simulation and from 10% to 80% success rate\nin the real-world as compared with naive baselines. For experiment videos\nincluding both real-world and simulation, see:\nhttps://www.youtube.com/playlist?list=PL-oD0xHUngeLfQmpngYkGFZarstfPOXqX",
          "link": "http://arxiv.org/abs/2011.08694",
          "publishedOn": "2021-07-15T01:59:04.437Z",
          "wordCount": 681,
          "title": "Reactive Long Horizon Task Execution via Visual Skill and Precondition Models. (arXiv:2011.08694v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsouvalas_V/0/1/0/all/0/1\">Vasileios Tsouvalas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeed_A/0/1/0/all/0/1\">Aaqib Saeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozcelebi_T/0/1/0/all/0/1\">Tanir Ozcelebi</a>",
          "description": "Federated Learning is a distributed machine learning paradigm dealing with\ndecentralized and personal datasets. Since data reside on devices like\nsmartphones and virtual assistants, labeling is entrusted to the clients, or\nlabels are extracted in an automated way. Specifically, in the case of audio\ndata, acquiring semantic annotations can be prohibitively expensive and\ntime-consuming. As a result, an abundance of audio data remains unlabeled and\nunexploited on users' devices. Most existing federated learning approaches\nfocus on supervised learning without harnessing the unlabeled data. In this\nwork, we study the problem of semi-supervised learning of audio models via\nself-training in conjunction with federated learning. We propose FedSTAR to\nexploit large-scale on-device unlabeled data to improve the generalization of\naudio recognition models. We further demonstrate that self-supervised\npre-trained models can accelerate the training of on-device models,\nsignificantly improving convergence to within fewer training rounds. We conduct\nexperiments on diverse public audio classification datasets and investigate the\nperformance of our models under varying percentages of labeled and unlabeled\ndata. Notably, we show that with as little as 3% labeled data available,\nFedSTAR on average can improve the recognition rate by 13.28% compared to the\nfully supervised federated model.",
          "link": "http://arxiv.org/abs/2107.06877",
          "publishedOn": "2021-07-15T01:59:04.431Z",
          "wordCount": 636,
          "title": "Federated Self-Training for Semi-Supervised Audio Recognition. (arXiv:2107.06877v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.05732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deguerre_B/0/1/0/all/0/1\">Benjamin Deguerre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatelain_C/0/1/0/all/0/1\">Clement Chatelain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasso_G/0/1/0/all/0/1\">Gilles Gasso</a>",
          "description": "Object detection in images has reached unprecedented performances. The\nstate-of-the-art methods rely on deep architectures that extract salient\nfeatures and predict bounding boxes enclosing the objects of interest. These\nmethods essentially run on RGB images. However, the RGB images are often\ncompressed by the acquisition devices for storage purpose and transfer\nefficiency. Hence, their decompression is required for object detectors. To\ngain in efficiency, this paper proposes to take advantage of the compressed\nrepresentation of images to carry out object detection usable in constrained\nresources conditions.\n\nSpecifically, we focus on JPEG images and propose a thorough analysis of\ndetection architectures newly designed in regard of the peculiarities of the\nJPEG norm. This leads to a $\\times 1.7$ speed up in comparison with a standard\nRGB-based architecture, while only reducing the detection performance by 5.5%.\nAdditionally, our empirical findings demonstrate that only part of the\ncompressed JPEG information, namely the luminance component, may be required to\nmatch detection accuracy of the full input methods.",
          "link": "http://arxiv.org/abs/2006.05732",
          "publishedOn": "2021-07-15T01:59:04.425Z",
          "wordCount": 645,
          "title": "Object Detection in the DCT Domain: is Luminance the Solution?. (arXiv:2006.05732v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06608",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elkabetz_O/0/1/0/all/0/1\">Omer Elkabetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1\">Nadav Cohen</a>",
          "description": "Existing analyses of optimization in deep learning are either continuous,\nfocusing on (variants of) gradient flow, or discrete, directly treating\n(variants of) gradient descent. Gradient flow is amenable to theoretical\nanalysis, but is stylized and disregards computational efficiency. The extent\nto which it represents gradient descent is an open question in deep learning\ntheory. The current paper studies this question. Viewing gradient descent as an\napproximate numerical solution to the initial value problem of gradient flow,\nwe find that the degree of approximation depends on the curvature along the\nlatter's trajectory. We then show that over deep neural networks with\nhomogeneous activations, gradient flow trajectories enjoy favorable curvature,\nsuggesting they are well approximated by gradient descent. This finding allows\nus to translate an analysis of gradient flow over deep linear neural networks\ninto a guarantee that gradient descent efficiently converges to global minimum\nalmost surely under random initialization. Experiments suggest that over simple\ndeep neural networks, gradient descent with conventional step size is indeed\nclose to the continuous limit. We hypothesize that the theory of gradient flows\nwill be central to unraveling mysteries behind deep learning.",
          "link": "http://arxiv.org/abs/2107.06608",
          "publishedOn": "2021-07-15T01:59:04.408Z",
          "wordCount": 622,
          "title": "Continuous vs. Discrete Optimization of Deep Neural Networks. (arXiv:2107.06608v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Civitarese_D/0/1/0/all/0/1\">Daniel Salles Civitarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szwarcman_D/0/1/0/all/0/1\">Daniela Szwarcman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadrozny_B/0/1/0/all/0/1\">Bianca Zadrozny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watson_C/0/1/0/all/0/1\">Campbell Watson</a>",
          "description": "An impact of climate change is the increase in frequency and intensity of\nextreme precipitation events. However, confidently predicting the likelihood of\nextreme precipitation at seasonal scales remains an outstanding challenge.\nHere, we present an approach to forecasting the quantiles of the maximum daily\nprecipitation in each week up to six months ahead using the temporal fusion\ntransformer (TFT) model. Through experiments in two regions, we compare TFT\npredictions with those of two baselines: climatology and a calibrated ECMWF\nSEAS5 ensemble forecast (S5). Our results show that, in terms of quantile risk\nat six month lead time, the TFT predictions significantly outperform those from\nS5 and show an overall small improvement compared to climatology. The TFT also\nresponds positively to departures from normal that climatology cannot.",
          "link": "http://arxiv.org/abs/2107.06846",
          "publishedOn": "2021-07-15T01:59:04.368Z",
          "wordCount": 563,
          "title": "Extreme Precipitation Seasonal Forecast Using a Transformer Neural Network. (arXiv:2107.06846v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06882",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trabucco_B/0/1/0/all/0/1\">Brandon Trabucco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aviral Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xinyang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Computational design problems arise in a number of settings, from synthetic\nbiology to computer architectures. In this paper, we aim to solve data-driven\nmodel-based optimization (MBO) problems, where the goal is to find a design\ninput that maximizes an unknown objective function provided access to only a\nstatic dataset of prior experiments. Such data-driven optimization procedures\nare the only practical methods in many real-world domains where active data\ncollection is expensive (e.g., when optimizing over proteins) or dangerous\n(e.g., when optimizing over aircraft designs). Typical methods for MBO that\noptimize the design against a learned model suffer from distributional shift:\nit is easy to find a design that \"fools\" the model into predicting a high\nvalue. To overcome this, we propose conservative objective models (COMs), a\nmethod that learns a model of the objective function that lower bounds the\nactual value of the ground-truth objective on out-of-distribution inputs, and\nuses it for optimization. Structurally, COMs resemble adversarial training\nmethods used to overcome adversarial examples. COMs are simple to implement and\noutperform a number of existing methods on a wide range of MBO problems,\nincluding optimizing protein sequences, robot morphologies, neural network\nweights, and superconducting materials.",
          "link": "http://arxiv.org/abs/2107.06882",
          "publishedOn": "2021-07-15T01:59:04.363Z",
          "wordCount": 637,
          "title": "Conservative Objective Models for Effective Offline Model-Based Optimization. (arXiv:2107.06882v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shengjun Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_N/0/1/0/all/0/1\">Niklas Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_B/0/1/0/all/0/1\">Bharatbhushan Shetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitts_B/0/1/0/all/0/1\">Brendan Kitts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gligorijevic_D/0/1/0/all/0/1\">Djordje Gligorijevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gultekin_S/0/1/0/all/0/1\">San Gultekin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_T/0/1/0/all/0/1\">Tingyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianlong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flores_A/0/1/0/all/0/1\">Aaron Flores</a>",
          "description": "Since 2019, most ad exchanges and sell-side platforms (SSPs), in the online\nadvertising industry, shifted from second to first price auctions. Due to the\nfundamental difference between these auctions, demand-side platforms (DSPs)\nhave had to update their bidding strategies to avoid bidding unnecessarily high\nand hence overpaying. Bid shading was proposed to adjust the bid price intended\nfor second-price auctions, in order to balance cost and winning probability in\na first-price auction setup. In this study, we introduce a novel deep\ndistribution network for optimal bidding in both open (non-censored) and closed\n(censored) online first-price auctions. Offline and online A/B testing results\nshow that our algorithm outperforms previous state-of-art algorithms in terms\nof both surplus and effective cost per action (eCPX) metrics. Furthermore, the\nalgorithm is optimized in run-time and has been deployed into VerizonMedia DSP\nas production algorithm, serving hundreds of billions of bid requests per day.\nOnline A/B test shows that advertiser's ROI are improved by +2.4%, +2.4%, and\n+8.6% for impression based (CPM), click based (CPC), and conversion based (CPA)\ncampaigns respectively.",
          "link": "http://arxiv.org/abs/2107.06650",
          "publishedOn": "2021-07-15T01:59:04.356Z",
          "wordCount": 643,
          "title": "An Efficient Deep Distribution Network for Bid Shading in First-Price Auctions. (arXiv:2107.06650v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/1908.03464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiao Wang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tingzhang Zhao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaojun Ye</a> (2) ((1) Department of Computer Science, University of Science and Technology Beijing (2) School of Software, Tsinghua University)",
          "description": "Feature selection, an effective technique for dimensionality reduction, plays\nan important role in many machine learning systems. Supervised knowledge can\nsignificantly improve the performance. However, faced with the rapid growth of\nnewly emerging concepts, existing supervised methods might easily suffer from\nthe scarcity and validity of labeled data for training. In this paper, the\nauthors study the problem of zero-shot feature selection (i.e., building a\nfeature selection model that generalizes well to \"unseen\" concepts with limited\ntraining data of \"seen\" concepts). Specifically, they adopt class-semantic\ndescriptions (i.e., attributes) as supervision for feature selection, so as to\nutilize the supervised knowledge transferred from the seen concepts. For more\nreliable discriminative features, they further propose the\ncenter-characteristic loss which encourages the selected features to capture\nthe central characteristics of seen concepts. Extensive experiments conducted\non various real-world datasets demonstrate the effectiveness of the method.",
          "link": "http://arxiv.org/abs/1908.03464",
          "publishedOn": "2021-07-15T01:59:04.348Z",
          "wordCount": 643,
          "title": "Zero-Shot Feature Selection via Transferring Supervised Knowledge. (arXiv:1908.03464v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dempster_A/0/1/0/all/0/1\">Angus Dempster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_D/0/1/0/all/0/1\">Daniel F. Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1\">Geoffrey I. Webb</a>",
          "description": "Until recently, the most accurate methods for time series classification were\nlimited by high computational complexity. ROCKET achieves state-of-the-art\naccuracy with a fraction of the computational expense of most existing methods\nby transforming input time series using random convolutional kernels, and using\nthe transformed features to train a linear classifier. We reformulate ROCKET\ninto a new method, MINIROCKET, making it up to 75 times faster on larger\ndatasets, and making it almost deterministic (and optionally, with additional\ncomputational expense, fully deterministic), while maintaining essentially the\nsame accuracy. Using this method, it is possible to train and test a classifier\non all of 109 datasets from the UCR archive to state-of-the-art accuracy in\nless than 10 minutes. MINIROCKET is significantly faster than any other method\nof comparable accuracy (including ROCKET), and significantly more accurate than\nany other method of even roughly-similar computational expense. As such, we\nsuggest that MINIROCKET should now be considered and used as the default\nvariant of ROCKET.",
          "link": "http://arxiv.org/abs/2012.08791",
          "publishedOn": "2021-07-15T01:59:04.340Z",
          "wordCount": 641,
          "title": "MINIROCKET: A Very Fast (Almost) Deterministic Transform for Time Series Classification. (arXiv:2012.08791v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klicpera_J/0/1/0/all/0/1\">Johannes Klicpera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lienen_M/0/1/0/all/0/1\">Marten Lienen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "The current best practice for computing optimal transport (OT) is via entropy\nregularization and Sinkhorn iterations. This algorithm runs in quadratic time\nas it requires the full pairwise cost matrix, which is prohibitively expensive\nfor large sets of objects. In this work we propose two effective log-linear\ntime approximations of the cost matrix: First, a sparse approximation based on\nlocality-sensitive hashing (LSH) and, second, a Nystr\\\"om approximation with\nLSH-based sparse corrections, which we call locally corrected Nystr\\\"om (LCN).\nThese approximations enable general log-linear time algorithms for\nentropy-regularized OT that perform well even for the complex, high-dimensional\nspaces common in deep learning. We analyse these approximations theoretically\nand evaluate them experimentally both directly and end-to-end as a component\nfor real-world applications. Using our approximations for unsupervised word\nembedding alignment enables us to speed up a state-of-the-art method by a\nfactor of 3 while also improving the accuracy by 3.1 percentage points without\nany additional model changes. For graph distance regression we propose the\ngraph transport network (GTN), which combines graph neural networks (GNNs) with\nenhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales\nlog-linearly in the number of nodes.",
          "link": "http://arxiv.org/abs/2107.06876",
          "publishedOn": "2021-07-15T01:59:04.324Z",
          "wordCount": 658,
          "title": "Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More. (arXiv:2107.06876v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_A/0/1/0/all/0/1\">Amir Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novin_R/0/1/0/all/0/1\">Roya Sabbagh Novin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merryweather_A/0/1/0/all/0/1\">Andrew Merryweather</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermans_T/0/1/0/all/0/1\">Tucker Hermans</a>",
          "description": "Ergonomics and human comfort are essential concerns in physical human-robot\ninteraction applications. Defining an accurate and easy-to-use ergonomic\nassessment model stands as an important step in providing feedback for postural\ncorrection to improve operator health and comfort. In order to enable efficient\ncomputation, previously proposed automated ergonomic assessment and correction\ntools make approximations or simplifications to gold-standard assessment tools\nused by ergonomists in practice. In order to retain assessment quality, while\nimproving computational considerations, we introduce DULA, a differentiable and\ncontinuous ergonomics model learned to replicate the popular and scientifically\nvalidated RULA assessment. We show that DULA provides assessment comparable to\nRULA while providing computational benefits. We highlight DULA's strength in a\ndemonstration of gradient-based postural optimization for a simulated\nteleoperation task.",
          "link": "http://arxiv.org/abs/2107.06875",
          "publishedOn": "2021-07-15T01:59:04.318Z",
          "wordCount": 569,
          "title": "DULA: A Differentiable Ergonomics Model for Postural Optimization in Physical HRI. (arXiv:2107.06875v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Ashudeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kempe_D/0/1/0/all/0/1\">David Kempe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joachims_T/0/1/0/all/0/1\">Thorsten Joachims</a>",
          "description": "Fairness has emerged as an important consideration in algorithmic\ndecision-making. Unfairness occurs when an agent with higher merit obtains a\nworse outcome than an agent with lower merit. Our central point is that a\nprimary cause of unfairness is uncertainty. A principal or algorithm making\ndecisions never has access to the agents' true merit, and instead uses proxy\nfeatures that only imperfectly predict merit (e.g., GPA, star ratings,\nrecommendation letters). None of these ever fully capture an agent's merit; yet\nexisting approaches have mostly been defining fairness notions directly based\non observed features and outcomes.\n\nOur primary point is that it is more principled to acknowledge and model the\nuncertainty explicitly. The role of observed features is to give rise to a\nposterior distribution of the agents' merits. We use this viewpoint to define a\nnotion of approximate fairness in ranking. We call an algorithm $\\phi$-fair\n(for $\\phi \\in [0,1]$) if it has the following property for all agents $x$ and\nall $k$: if agent $x$ is among the top $k$ agents with respect to merit with\nprobability at least $\\rho$ (according to the posterior merit distribution),\nthen the algorithm places the agent among the top $k$ agents in its ranking\nwith probability at least $\\phi \\rho$.\n\nWe show how to compute rankings that optimally trade off approximate fairness\nagainst utility to the principal. In addition to the theoretical\ncharacterization, we present an empirical analysis of the potential impact of\nthe approach in simulation studies. For real-world validation, we applied the\napproach in the context of a paper recommendation system that we built and\nfielded at a large conference.",
          "link": "http://arxiv.org/abs/2107.06720",
          "publishedOn": "2021-07-15T01:59:04.312Z",
          "wordCount": 708,
          "title": "Fairness in Ranking under Uncertainty. (arXiv:2107.06720v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lixuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_D/0/1/0/all/0/1\">Dario Rossi</a>",
          "description": "The increased success of Deep Learning (DL) has recently sparked large-scale\ndeployment of DL models in many diverse industry segments. Yet, a crucial\nweakness of supervised model is the inherent difficulty in handling\nout-of-distribution samples, i.e., samples belonging to classes that were not\npresented to the model at training time. We propose in this paper a novel way\nto formulate the out-of-distribution detection problem, tailored for DL models.\nOur method does not require fine tuning process on training data, yet is\nsignificantly more accurate than the state of the art for out-of-distribution\ndetection.",
          "link": "http://arxiv.org/abs/2107.06668",
          "publishedOn": "2021-07-15T01:59:04.306Z",
          "wordCount": 529,
          "title": "Thinkback: Task-SpecificOut-of-Distribution Detection. (arXiv:2107.06668v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.13869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lermusiaux_P/0/1/0/all/0/1\">Pierre F.J. Lermusiaux</a>",
          "description": "Complex dynamical systems are used for predictions in many domains. Because\nof computational costs, models are truncated, coarsened, or aggregated. As the\nneglected and unresolved terms become important, the utility of model\npredictions diminishes. We develop a novel, versatile, and rigorous methodology\nto learn non-Markovian closure parameterizations for known-physics/low-fidelity\nmodels using data from high-fidelity simulations. The new \"neural closure\nmodels\" augment low-fidelity models with neural delay differential equations\n(nDDEs), motivated by the Mori-Zwanzig formulation and the inherent delays in\ncomplex dynamical systems. We demonstrate that neural closures efficiently\naccount for truncated modes in reduced-order-models, capture the effects of\nsubgrid-scale processes in coarse models, and augment the simplification of\ncomplex biological and physical-biogeochemical models. We find that using\nnon-Markovian over Markovian closures improves long-term prediction accuracy\nand requires smaller networks. We derive adjoint equations and network\narchitectures needed to efficiently implement the new discrete and distributed\nnDDEs, for any time-integration schemes and allowing nonuniformly-spaced\ntemporal training data. The performance of discrete over distributed delays in\nclosure models is explained using information theory, and we find an optimal\namount of past information for a specified architecture. Finally, we analyze\ncomputational complexity and explain the limited additional cost due to neural\nclosure models.",
          "link": "http://arxiv.org/abs/2012.13869",
          "publishedOn": "2021-07-15T01:59:04.300Z",
          "wordCount": 689,
          "title": "Neural Closure Models for Dynamical Systems. (arXiv:2012.13869v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.08352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_C/0/1/0/all/0/1\">Chen-Chou Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Szu-Wei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wen-Chin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>",
          "description": "Existing objective evaluation metrics for voice conversion (VC) are not\nalways correlated with human perception. Therefore, training VC models with\nsuch criteria may not effectively improve naturalness and similarity of\nconverted speech. In this paper, we propose deep learning-based assessment\nmodels to predict human ratings of converted speech. We adopt the convolutional\nand recurrent neural network models to build a mean opinion score (MOS)\npredictor, termed as MOSNet. The proposed models are tested on large-scale\nlistening test results of the Voice Conversion Challenge (VCC) 2018.\nExperimental results show that the predicted scores of the proposed MOSNet are\nhighly correlated with human MOS ratings at the system level while being fairly\ncorrelated with human MOS ratings at the utterance level. Meanwhile, we have\nmodified MOSNet to predict the similarity scores, and the preliminary results\nshow that the predicted scores are also fairly correlated with human ratings.\nThese results confirm that the proposed models could be used as a computational\nevaluator to measure the MOS of VC systems to reduce the need for expensive\nhuman rating.",
          "link": "http://arxiv.org/abs/1904.08352",
          "publishedOn": "2021-07-15T01:59:04.294Z",
          "wordCount": 665,
          "title": "MOSNet: Deep Learning based Objective Assessment for Voice Conversion. (arXiv:1904.08352v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1\">Kuncahyo Setyo Nugroho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukmadewa_A/0/1/0/all/0/1\">Anantha Yullian Sukmadewa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DW_H/0/1/0/all/0/1\">Haftittah Wuswilahaken DW</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachtiar_F/0/1/0/all/0/1\">Fitra Abdurrachman Bachtiar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1\">Novanto Yudistira</a>",
          "description": "User reviews have an essential role in the success of the developed mobile\napps. User reviews in the textual form are unstructured data, creating a very\nhigh complexity when processed for sentiment analysis. Previous approaches that\nhave been used often ignore the context of reviews. In addition, the relatively\nsmall data makes the model overfitting. A new approach, BERT, has been\nintroduced as a transfer learning model with a pre-trained model that has\npreviously been trained to have a better context representation. This study\nexamines the effectiveness of fine-tuning BERT for sentiment analysis using two\ndifferent pre-trained models. Besides the multilingual pre-trained model, we\nuse the pre-trained model that only has been trained in Indonesian. The dataset\nused is Indonesian user reviews of the ten best apps in 2020 in Google Play\nsites. We also perform hyper-parameter tuning to find the optimum trained\nmodel. Two training data labeling approaches were also tested to determine the\neffectiveness of the model, which is score-based and lexicon-based. The\nexperimental results show that pre-trained models trained in Indonesian have\nbetter average accuracy on lexicon-based data. The pre-trained Indonesian model\nhighest accuracy is 84%, with 25 epochs and a training time of 24 minutes.\nThese results are better than all of the machine learning and multilingual\npre-trained models.",
          "link": "http://arxiv.org/abs/2107.06802",
          "publishedOn": "2021-07-15T01:59:04.278Z",
          "wordCount": 657,
          "title": "BERT Fine-Tuning for Sentiment Analysis on Indonesian Mobile Apps Reviews. (arXiv:2107.06802v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mordvintsev_A/0/1/0/all/0/1\">Alexander Mordvintsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Randazzo_E/0/1/0/all/0/1\">Ettore Randazzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niklasson_E/0/1/0/all/0/1\">Eyvind Niklasson</a>",
          "description": "Reaction-Diffusion (RD) systems provide a computational framework that\ngoverns many pattern formation processes in nature. Current RD system design\npractices boil down to trial-and-error parameter search. We propose a\ndifferentiable optimization method for learning the RD system parameters to\nperform example-based texture synthesis on a 2D plane. We do this by\nrepresenting the RD system as a variant of Neural Cellular Automata and using\ntask-specific differentiable loss functions. RD systems generated by our method\nexhibit robust, non-trivial 'life-like' behavior.",
          "link": "http://arxiv.org/abs/2107.06862",
          "publishedOn": "2021-07-15T01:59:04.272Z",
          "wordCount": 522,
          "title": "Differentiable Programming of Reaction-Diffusion Patterns. (arXiv:2107.06862v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bassily_R/0/1/0/all/0/1\">Raef Bassily</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_C/0/1/0/all/0/1\">Crist&#xf3;bal Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menart_M/0/1/0/all/0/1\">Michael Menart</a>",
          "description": "We study differentially private stochastic optimization in convex and\nnon-convex settings. For the convex case, we focus on the family of non-smooth\ngeneralized linear losses (GLLs). Our algorithm for the $\\ell_2$ setting\nachieves optimal excess population risk in near-linear time, while the best\nknown differentially private algorithms for general convex losses run in\nsuper-linear time. Our algorithm for the $\\ell_1$ setting has nearly-optimal\nexcess population risk $\\tilde{O}\\big(\\sqrt{\\frac{\\log{d}}{n}}\\big)$, and\ncircumvents the dimension dependent lower bound of [AFKT21] for general\nnon-smooth convex losses. In the differentially private non-convex setting, we\nprovide several new algorithms for approximating stationary points of the\npopulation risk. For the $\\ell_1$-case with smooth losses and polyhedral\nconstraint, we provide the first nearly dimension independent rate, $\\tilde\nO\\big(\\frac{\\log^{2/3}{d}}{{n^{1/3}}}\\big)$ in linear time. For the constrained\n$\\ell_2$-case, with smooth losses, we obtain a linear-time algorithm with rate\n$\\tilde O\\big(\\frac{1}{n^{3/10}d^{1/10}}+\\big(\\frac{d}{n^2}\\big)^{1/5}\\big)$.\nFinally, for the $\\ell_2$-case we provide the first method for {\\em non-smooth\nweakly convex} stochastic optimization with rate $\\tilde\nO\\big(\\frac{1}{n^{1/4}}+\\big(\\frac{d}{n^2}\\big)^{1/6}\\big)$ which matches the\nbest existing non-private algorithm when $d= O(\\sqrt{n})$. We also extend all\nour results above for the non-convex $\\ell_2$ setting to the $\\ell_p$ setting,\nwhere $1 < p \\leq 2$, with only polylogarithmic (in the dimension) overhead in\nthe rates.",
          "link": "http://arxiv.org/abs/2107.05585",
          "publishedOn": "2021-07-15T01:59:04.266Z",
          "wordCount": 662,
          "title": "Differentially Private Stochastic Optimization: New Results in Convex and Non-Convex Settings. (arXiv:2107.05585v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Abdul Rafae Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varsanyi_P/0/1/0/all/0/1\">Peter Varsanyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pabreja_R/0/1/0/all/0/1\">Rachit Pabreja</a>",
          "description": "While predictive policing has become increasingly common in assisting with\ndecisions in the criminal justice system, the use of these results is still\ncontroversial. Some software based on deep learning lacks accuracy (e.g., in\nF-1), and importantly many decision processes are not transparent, causing\ndoubt about decision bias, such as perceived racial and age disparities. This\npaper addresses bias issues with post-hoc explanations to provide a trustable\nprediction of whether a person will receive future criminal charges given one's\nprevious criminal records by learning temporal behavior patterns over twenty\nyears. Bi-LSTM relieves the vanishing gradient problem, attentional mechanisms\nallow learning and interpretation of feature importance, and complex-valued\nnetworks inspired quantum physics to facilitate a certain level of transparency\nin modeling the decision process. Our approach shows a consistent and reliable\nprediction precision and recall on a real-life dataset. Our analysis of the\nimportance of each input feature shows the critical causal impact on\ndecision-making, suggesting that criminal histories are statistically\nsignificant factors, while identifiers, such as race and age, are not. Finally,\nour algorithm indicates that a suspect tends to rather than suddenly increase\ncrime severity level over time gradually.",
          "link": "http://arxiv.org/abs/2106.13456",
          "publishedOn": "2021-07-15T01:59:04.254Z",
          "wordCount": 663,
          "title": "Interpreting Criminal Charge Prediction and Its Algorithmic Bias via Quantum-Inspired Complex Valued Networks. (arXiv:2106.13456v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reisser_M/0/1/0/all/0/1\">Matthias Reisser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louizos_C/0/1/0/all/0/1\">Christos Louizos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1\">Efstratios Gavves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1\">Max Welling</a>",
          "description": "Federated learning (FL) has emerged as the predominant approach for\ncollaborative training of neural network models across multiple users, without\nthe need to gather the data at a central location. One of the important\nchallenges in this setting is data heterogeneity, i.e. different users have\ndifferent data characteristics. For this reason, training and using a single\nglobal model might be suboptimal when considering the performance of each of\nthe individual user's data. In this work, we tackle this problem via Federated\nMixture of Experts, FedMix, a framework that allows us to train an ensemble of\nspecialized models. FedMix adaptively selects and trains a user-specific\nselection of the ensemble members. We show that users with similar data\ncharacteristics select the same members and therefore share statistical\nstrength while mitigating the effect of non-i.i.d data. Empirically, we show\nthrough an extensive experimental evaluation that FedMix improves performance\ncompared to using a single global model across a variety of different sources\nof non-i.i.d.-ness.",
          "link": "http://arxiv.org/abs/2107.06724",
          "publishedOn": "2021-07-15T01:59:04.236Z",
          "wordCount": 588,
          "title": "Federated Mixture of Experts. (arXiv:2107.06724v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06675",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ziel_F/0/1/0/all/0/1\">Florian Ziel</a>",
          "description": "The M5 competition uncertainty track aims for probabilistic forecasting of\nsales of thousands of Walmart retail goods. We show that the M5 competition\ndata faces strong overdispersion and sporadic demand, especially zero demand.\nWe discuss resulting modeling issues concerning adequate probabilistic\nforecasting of such count data processes. Unfortunately, the majority of\npopular prediction methods used in the M5 competition (e.g. lightgbm and\nxgboost GBMs) fails to address the data characteristics due to the considered\nobjective functions. The distributional forecasting provides a suitable\nmodeling approach for to the overcome those problems. The GAMLSS framework\nallows flexible probabilistic forecasting using low dimensional distributions.\nWe illustrate, how the GAMLSS approach can be applied for the M5 competition\ndata by modeling the location and scale parameter of various distributions,\ne.g. the negative binomial distribution. Finally, we discuss software packages\nfor distributional modeling and their drawback, like the R package gamlss with\nits package extensions, and (deep) distributional forecasting libraries such as\nTensorFlow Probability.",
          "link": "http://arxiv.org/abs/2107.06675",
          "publishedOn": "2021-07-15T01:59:04.229Z",
          "wordCount": 599,
          "title": "M5 Competition Uncertainty: Overdispersion, distributional forecasting, GAMLSS and beyond. (arXiv:2107.06675v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06762",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Mestre_G/0/1/0/all/0/1\">Gon&#xe7;alo Mestre</a> (1 and 2), <a href=\"http://arxiv.org/find/q-bio/1/au:+Barbulescu_R/0/1/0/all/0/1\">Ruxandra Barbulescu</a> (1), <a href=\"http://arxiv.org/find/q-bio/1/au:+Oliveira_A/0/1/0/all/0/1\">Arlindo L. Oliveira</a> (1 and 2), <a href=\"http://arxiv.org/find/q-bio/1/au:+Silveira_L/0/1/0/all/0/1\">L. Miguel Silveira</a> (1 and 2) ((1) INESC-ID, Rua Alves Redol 9, 1000-029 Lisboa, (2) IST Tecnico Lisboa, Universidade de Lisboa, Av. Rovisco Pais 1, 1049-001 Lisboa)",
          "description": "Given the inner complexity of the human nervous system, insight into the\ndynamics of brain activity can be gained from understanding smaller and simpler\norganisms, such as the nematode C. Elegans. The behavioural and structural\nbiology of these organisms is well-known, making them prime candidates for\nbenchmarking modelling and simulation techniques. In these complex neuronal\ncollections, classical, white-box modelling techniques based on intrinsic\nstructural or behavioural information are either unable to capture the profound\nnonlinearities of the neuronal response to different stimuli or generate\nextremely complex models, which are computationally intractable. In this paper\nwe show how the nervous system of C. Elegans can be modelled and simulated with\ndata-driven models using different neural network architectures. Specifically,\nwe target the use of state of the art recurrent neural networks architectures\nsuch as LSTMs and GRUs and compare these architectures in terms of their\nproperties and their accuracy as well as the complexity of the resulting\nmodels. We show that GRU models with a hidden layer size of 4 units are able to\naccurately reproduce with high accuracy the system's response to very different\nstimuli.",
          "link": "http://arxiv.org/abs/2107.06762",
          "publishedOn": "2021-07-15T01:59:04.223Z",
          "wordCount": 669,
          "title": "Modelling Neuronal Behaviour with Time Series Regression: Recurrent Neural Networks on C. Elegans Data. (arXiv:2107.06762v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2006.02804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1\">Kai Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1\">Xuefei Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1\">Guohao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenhua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shulin Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huazhong Yang</a>",
          "description": "In this work, we propose a low-bit training framework for convolutional\nneural networks, which is built around a novel multi-level scaling (MLS) tensor\nformat. Our framework focuses on reducing the energy consumption of convolution\noperations by quantizing all the convolution operands to low bit-width format.\nSpecifically, we propose the MLS tensor format, in which the element-wise\nbit-width can be largely reduced. Then, we describe the dynamic quantization\nand the low-bit tensor convolution arithmetic to leverage the MLS tensor format\nefficiently. Experiments show that our framework achieves a superior trade-off\nbetween the accuracy and the bit-width than previous low-bit training\nframeworks. For training a variety of models on CIFAR-10, using 1-bit mantissa\nand 2-bit exponent is adequate to keep the accuracy loss within $1\\%$. And on\nlarger datasets like ImageNet, using 4-bit mantissa and 2-bit exponent is\nadequate to keep the accuracy loss within $1\\%$. Through the energy consumption\nsimulation of the computing units, we can estimate that training a variety of\nmodels with our framework could achieve $8.3\\sim10.2\\times$ and\n$1.9\\sim2.3\\times$ higher energy efficiency than training with full-precision\nand 8-bit floating-point arithmetic, respectively.",
          "link": "http://arxiv.org/abs/2006.02804",
          "publishedOn": "2021-07-15T01:59:04.217Z",
          "wordCount": 683,
          "title": "Exploring the Potential of Low-bit Training of Convolutional Neural Networks. (arXiv:2006.02804v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_A/0/1/0/all/0/1\">Akshay L Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Sai Vikas Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1\">Chaitanya Devaguptapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>",
          "description": "Active Learning (AL) techniques aim to minimize the training data required to\ntrain a model for a given task. Pool-based AL techniques start with a small\ninitial labeled pool and then iteratively pick batches of the most informative\nsamples for labeling. Generally, the initial pool is sampled randomly and\nlabeled to seed the AL iterations. While recent studies have focused on\nevaluating the robustness of various query functions in AL, little to no\nattention has been given to the design of the initial labeled pool for deep\nactive learning. Given the recent successes of learning representations in\nself-supervised/unsupervised ways, we study if an intelligently sampled initial\nlabeled pool can improve deep AL performance. We investigate the effect of\nintelligently sampled initial labeled pools, including the use of\nself-supervised and unsupervised strategies, on deep AL methods. The setup,\nhypotheses, methodology, and implementation details were evaluated by peer\nreview before experiments were conducted. Experimental results could not\nconclusively prove that intelligently sampled initial pools are better for AL\nthan random initial pools in the long run, although a Variational\nAutoencoder-based initial pool sampling strategy showed interesting trends that\nmerit deeper investigation.",
          "link": "http://arxiv.org/abs/2011.14696",
          "publishedOn": "2021-07-15T01:59:04.210Z",
          "wordCount": 683,
          "title": "On Initial Pools for Deep Active Learning. (arXiv:2011.14696v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08721",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Pokhrel_P/0/1/0/all/0/1\">Pujan Pokhrel</a>",
          "description": "In this paper, we propose a Light Gradient Boosting (LightGBM) to forecast\ndominant wave periods in oceanic waters. First, we use the data collected from\nCDIP buoys and apply various data filtering methods. The data filtering methods\nallow us to obtain a high-quality dataset for training and validation purposes.\nWe then extract various wave-based features like wave heights, periods,\nskewness, kurtosis, etc., and atmospheric features like humidity, pressure, and\nair temperature for the buoys. Afterward, we train algorithms that use LightGBM\nand Extra Trees through a hv-block cross-validation scheme to forecast dominant\nwave periods for up to 30 days ahead. LightGBM has the R2 score of 0.94, 0.94,\nand 0.94 for 1-day ahead, 15-day ahead, and 30-day ahead prediction. Similarly,\nExtra Trees (ET) has an R2 score of 0.88, 0.86, and 0.85 for 1-day ahead,\n15-day ahead, and 30 day ahead prediction. In case of the test dataset,\nLightGBM has R2 score of 0.94, 0.94, and 0.94 for 1-day ahead, 15-day ahead and\n30-day ahead prediction. ET has R2 score of 0.88, 0.86, and 0.85 for 1-day\nahead, 15-day ahead, and 30-day ahead prediction. A similar R2 score for both\ntraining and the test dataset suggests that the machine learning models\ndeveloped in this paper are robust. Since the LightGBM algorithm outperforms ET\nfor all the windows tested, it is taken as the final algorithm. Note that the\nperformance of both methods does not decrease significantly as the forecast\nhorizon increases. Likewise, the proposed method outperforms the numerical\napproaches included in this paper in the test dataset. For 1 day ahead\nprediction, the proposed algorithm has SI, Bias, CC, and RMSE of 0.09, 0.00,\n0.97, and 1.78 compared to 0.268, 0.40, 0.63, and 2.18 for the European Centre\nfor Medium-range Weather Forecasts (ECMWF) model, which outperforms all the\nother methods in the test dataset.",
          "link": "http://arxiv.org/abs/2105.08721",
          "publishedOn": "2021-07-15T01:59:04.193Z",
          "wordCount": 783,
          "title": "A LightGBM based Forecasting of Dominant Wave Periods in Oceanic Waters. (arXiv:2105.08721v4 [physics.ao-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1\">Anton Obukhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakhuba_M/0/1/0/all/0/1\">Maxim Rakhuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiwu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "We study low-rank parameterizations of weight matrices with embedded spectral\nproperties in the Deep Learning context. The low-rank property leads to\nparameter efficiency and permits taking computational shortcuts when computing\nmappings. Spectral properties are often subject to constraints in optimization\nproblems, leading to better models and stability of optimization. We start by\nlooking at the compact SVD parameterization of weight matrices and identifying\nredundancy sources in the parameterization. We further apply the Tensor Train\n(TT) decomposition to the compact SVD components, and propose a non-redundant\ndifferentiable parameterization of fixed TT-rank tensor manifolds, termed the\nSpectral Tensor Train Parameterization (STTP). We demonstrate the effects of\nneural network compression in the image classification setting and both\ncompression and improved training stability in the generative adversarial\ntraining setting.",
          "link": "http://arxiv.org/abs/2103.04217",
          "publishedOn": "2021-07-15T01:59:04.186Z",
          "wordCount": 609,
          "title": "Spectral Tensor Train Parameterization of Deep Learning Layers. (arXiv:2103.04217v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.03886",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mei_K/0/1/0/all/0/1\">Kai Mei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaochen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajatheva_N/0/1/0/all/0/1\">Nandana Rajatheva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_J/0/1/0/all/0/1\">Jibo Wei</a>",
          "description": "Recently, machine learning-based channel estimation has attracted much\nattention. The performance of machine learning-based estimation has been\nvalidated by simulation experiments. However, little attention has been paid to\nthe theoretical performance analysis. In this paper, we investigate the mean\nsquare error (MSE) performance of machine learning-based estimation. Hypothesis\ntesting is employed to analyze its MSE upper bound. Furthermore, we build a\nstatistical model for hypothesis testing, which holds when the linear learning\nmodule with a low input dimension is used in machine learning-based channel\nestimation, and derive a clear analytical relation between the size of the\ntraining data and performance. Then, we simulate the machine learning-based\nchannel estimation in orthogonal frequency division multiplexing (OFDM) systems\nto verify our analysis results. Finally, the design considerations for the\nsituation where only limited training data is available are discussed. In this\nsituation, our analysis results can be applied to assess the performance and\nsupport the design of machine learning-based channel estimation.",
          "link": "http://arxiv.org/abs/1911.03886",
          "publishedOn": "2021-07-15T01:59:04.178Z",
          "wordCount": 628,
          "title": "Performance Analysis on Machine Learning-Based Channel Estimation. (arXiv:1911.03886v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13275",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Taheri_H/0/1/0/all/0/1\">Hossein Taheri</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pedarsani_R/0/1/0/all/0/1\">Ramtin Pedarsani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thrampoulidis_C/0/1/0/all/0/1\">Christos Thrampoulidis</a>",
          "description": "It has been consistently reported that many machine learning models are\nsusceptible to adversarial attacks i.e., small additive adversarial\nperturbations applied to data points can cause misclassification. Adversarial\ntraining using empirical risk minimization is considered to be the\nstate-of-the-art method for defense against adversarial attacks. Despite being\nsuccessful in practice, several problems in understanding generalization\nperformance of adversarial training remain open. In this paper, we derive\nprecise theoretical predictions for the performance of adversarial training in\nbinary classification. We consider the high-dimensional regime where the\ndimension of data grows with the size of the training data-set at a constant\nratio. Our results provide exact asymptotics for standard and adversarial test\nerrors of the estimators obtained by adversarial training with $\\ell_q$-norm\nbounded perturbations ($q \\ge 1$) for both discriminative binary models and\ngenerative Gaussian-mixture models with correlated features. Furthermore, we\nuse these sharp predictions to uncover several intriguing observations on the\nrole of various parameters including the over-parameterization ratio, the data\nmodel, and the attack budget on the adversarial and standard errors.",
          "link": "http://arxiv.org/abs/2010.13275",
          "publishedOn": "2021-07-15T01:59:04.172Z",
          "wordCount": 649,
          "title": "Asymptotic Behavior of Adversarial Training in Binary Classification. (arXiv:2010.13275v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schubert_I/0/1/0/all/0/1\">Ingmar Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_O/0/1/0/all/0/1\">Ozgur S. Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toussaint_M/0/1/0/all/0/1\">Marc Toussaint</a>",
          "description": "In high-dimensional state spaces, the usefulness of Reinforcement Learning\n(RL) is limited by the problem of exploration. This issue has been addressed\nusing potential-based reward shaping (PB-RS) previously. In the present work,\nwe introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the\nstrict optimality guarantees of PB-RS to a guarantee of preserved long-term\nbehavior. Being less restrictive, FV-RS allows for reward shaping functions\nthat are even better suited for improving the sample efficiency of RL\nalgorithms. In particular, we consider settings in which the agent has access\nto an approximate plan. Here, we use examples of simulated robotic manipulation\ntasks to demonstrate that plan-based FV-RS can indeed significantly improve the\nsample efficiency of RL over plan-based PB-RS.",
          "link": "http://arxiv.org/abs/2107.06661",
          "publishedOn": "2021-07-15T01:59:04.166Z",
          "wordCount": 569,
          "title": "Plan-Based Relaxed Reward Shaping for Goal-Directed Tasks. (arXiv:2107.06661v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tithi_J/0/1/0/all/0/1\">Jesmin Jahan Tithi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrini_F/0/1/0/all/0/1\">Fabrizio Petrini</a>",
          "description": "The Word Movers Distance (WMD) measures the semantic dissimilarity between\ntwo text documents by computing the cost of optimally moving all words of a\nsource/query document to the most similar words of a target document. Computing\nWMD between two documents is costly because it requires solving an optimization\nproblem that costs $O (V^3 \\log(V)) $ where $V$ is the number of unique words\nin the document. Fortunately, WMD can be framed as an Earth Mover's Distance\n(EMD) for which the algorithmic complexity can be reduced to $O(V^2)$ by adding\nan entropy penalty to the optimization problem and solving it using the\nSinkhorn-Knopp algorithm. Additionally, the computation can be made highly\nparallel by computing the WMD of a single query document against multiple\ntarget documents at once, for example by finding whether a given tweet is\nsimilar to any other tweets of a given day.\n\nIn this paper, we first present a shared-memory parallel Sinkhorn-Knopp\nalgorithm to compute the WMD of one document against many other documents by\nadopting the $ O(V^2)$ EMD algorithm. We then algorithmically transform the\noriginal $O(V^2)$ dense compute-heavy version into an equivalent sparse one\nwhich is mapped onto the new Intel Programmable Integrated Unified Memory\nArchitecture (PIUMA) system. The WMD parallel implementation achieves 67x\nspeedup on 96 cores across 4 NUMA sockets of an Intel Cascade Lake system. We\nalso show that PIUMA cores are around 1.2-2.6x faster than Xeon cores on\nSinkhorn-WMD and also provide better strong scaling.",
          "link": "http://arxiv.org/abs/2107.06433",
          "publishedOn": "2021-07-15T01:59:04.149Z",
          "wordCount": 715,
          "title": "A New Parallel Algorithm for Sinkhorn Word-Movers Distance and Its Performance on PIUMA and Xeon CPU. (arXiv:2107.06433v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03374",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mark Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tworek_J/0/1/0/all/0/1\">Jerry Tworek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_H/0/1/0/all/0/1\">Heewoo Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Q/0/1/0/all/0/1\">Qiming Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_H/0/1/0/all/0/1\">Henrique Ponde de Oliveira Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1\">Jared Kaplan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edwards_H/0/1/0/all/0/1\">Harri Edwards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burda_Y/0/1/0/all/0/1\">Yuri Burda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1\">Nicholas Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockman_G/0/1/0/all/0/1\">Greg Brockman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Alex Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1\">Raul Puri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krueger_G/0/1/0/all/0/1\">Gretchen Krueger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrov_M/0/1/0/all/0/1\">Michael Petrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khlaaf_H/0/1/0/all/0/1\">Heidy Khlaaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_G/0/1/0/all/0/1\">Girish Sastry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishkin_P/0/1/0/all/0/1\">Pamela Mishkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_B/0/1/0/all/0/1\">Brooke Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_S/0/1/0/all/0/1\">Scott Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryder_N/0/1/0/all/0/1\">Nick Ryder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlov_M/0/1/0/all/0/1\">Mikhail Pavlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Power_A/0/1/0/all/0/1\">Alethea Power</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaiser_L/0/1/0/all/0/1\">Lukasz Kaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bavarian_M/0/1/0/all/0/1\">Mohammad Bavarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winter_C/0/1/0/all/0/1\">Clemens Winter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tillet_P/0/1/0/all/0/1\">Philippe Tillet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Such_F/0/1/0/all/0/1\">Felipe Petroski Such</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummings_D/0/1/0/all/0/1\">Dave Cummings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plappert_M/0/1/0/all/0/1\">Matthias Plappert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chantzis_F/0/1/0/all/0/1\">Fotios Chantzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_E/0/1/0/all/0/1\">Elizabeth Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herbert_Voss_A/0/1/0/all/0/1\">Ariel Herbert-Voss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guss_W/0/1/0/all/0/1\">William Hebgen Guss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nichol_A/0/1/0/all/0/1\">Alex Nichol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paino_A/0/1/0/all/0/1\">Alex Paino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tezak_N/0/1/0/all/0/1\">Nikolas Tezak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babuschkin_I/0/1/0/all/0/1\">Igor Babuschkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaji_S/0/1/0/all/0/1\">Suchir Balaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shantanu Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunders_W/0/1/0/all/0/1\">William Saunders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hesse_C/0/1/0/all/0/1\">Christopher Hesse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carr_A/0/1/0/all/0/1\">Andrew N. Carr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leike_J/0/1/0/all/0/1\">Jan Leike</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achiam_J/0/1/0/all/0/1\">Josh Achiam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_V/0/1/0/all/0/1\">Vedant Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morikawa_E/0/1/0/all/0/1\">Evan Morikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radford_A/0/1/0/all/0/1\">Alec Radford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knight_M/0/1/0/all/0/1\">Matthew Knight</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brundage_M/0/1/0/all/0/1\">Miles Brundage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murati_M/0/1/0/all/0/1\">Mira Murati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayer_K/0/1/0/all/0/1\">Katie Mayer</a>, et al. (6 additional authors not shown)",
          "description": "We introduce Codex, a GPT language model fine-tuned on publicly available\ncode from GitHub, and study its Python code-writing capabilities. A distinct\nproduction version of Codex powers GitHub Copilot. On HumanEval, a new\nevaluation set we release to measure functional correctness for synthesizing\nprograms from docstrings, our model solves 28.8% of the problems, while GPT-3\nsolves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling\nfrom the model is a surprisingly effective strategy for producing working\nsolutions to difficult prompts. Using this method, we solve 70.2% of our\nproblems with 100 samples per problem. Careful investigation of our model\nreveals its limitations, including difficulty with docstrings describing long\nchains of operations and with binding operations to variables. Finally, we\ndiscuss the potential broader impacts of deploying powerful code generation\ntechnologies, covering safety, security, and economics.",
          "link": "http://arxiv.org/abs/2107.03374",
          "publishedOn": "2021-07-15T01:59:04.143Z",
          "wordCount": 705,
          "title": "Evaluating Large Language Models Trained on Code. (arXiv:2107.03374v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>",
          "description": "Active learning (AL) aims at reducing labeling effort by identifying the most\nvaluable unlabeled data points from a large pool. Traditional AL frameworks\nhave two limitations: First, they perform data selection in a multi-round\nmanner, which is time-consuming and impractical. Second, they usually assume\nthat there are a small amount of labeled data points available in the same\ndomain as the data in the unlabeled pool. Recent work proposes a solution for\none-round active learning based on data utility learning and optimization,\nwhich fixes the first issue but still requires the initially labeled data\npoints in the same domain. In this paper, we propose $\\mathrm{D^2ULO}$ as a\nsolution that solves both issues. Specifically, $\\mathrm{D^2ULO}$ leverages the\nidea of domain adaptation (DA) to train a data utility model which can\neffectively predict the utility for any given unlabeled data in the target\ndomain once labeled. The trained data utility model can then be used to select\nhigh-utility data and at the same time, provide an estimate for the utility of\nthe selected data. Our algorithm does not rely on any feedback from annotators\nin the target domain and hence, can be used to perform zero-round active\nlearning or warm-start existing multi-round active learning strategies. Our\nexperiments show that $\\mathrm{D^2ULO}$ outperforms the existing\nstate-of-the-art AL strategies equipped with domain adaptation over various\ndomain shift settings (e.g., real-to-real data and synthetic-to-real data).\nParticularly, $\\mathrm{D^2ULO}$ are applicable to the scenario where source and\ntarget labels have mismatches, which is not supported by the existing works.",
          "link": "http://arxiv.org/abs/2107.06703",
          "publishedOn": "2021-07-15T01:59:04.135Z",
          "wordCount": 668,
          "title": "Zero-Round Active Learning. (arXiv:2107.06703v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1\">Fay&#xe7;al Ait Aoudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1\">Jakob Hoydis</a>",
          "description": "As communication systems are foreseen to enable new services such as joint\ncommunication and sensing and utilize parts of the sub-THz spectrum, the design\nof novel waveforms that can support these emerging applications becomes\nincreasingly challenging. We present in this work an end-to-end learning\napproach to design waveforms through joint learning of pulse shaping and\nconstellation geometry, together with a neural network (NN)-based receiver.\nOptimization is performed to maximize an achievable information rate, while\nsatisfying constraints on out-of-band emission and power envelope. Our results\nshow that the proposed approach enables up to orders of magnitude smaller\nadjacent channel leakage ratios (ACLRs) with peak-to-average power ratios\n(PAPRs) competitive with traditional filters, without significant loss of\ninformation rate on an additive white Gaussian noise (AWGN) channel, and no\nadditional complexity at the transmitter.",
          "link": "http://arxiv.org/abs/2106.15158",
          "publishedOn": "2021-07-15T01:59:04.130Z",
          "wordCount": 593,
          "title": "End-to-end Waveform Learning Through Joint Optimization of Pulse and Constellation Shaping. (arXiv:2106.15158v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Truong_L/0/1/0/all/0/1\">Lan V. Truong</a>",
          "description": "We establish exact asymptotic expressions for the normalized mutual\ninformation and minimum mean-square-error (MMSE) of sparse linear regression in\nthe sub-linear sparsity regime. Our result is achieved by a generalization of\nthe adaptive interpolation method in Bayesian inference for linear regimes to\nsub-linear ones. A modification of the well-known approximate message passing\nalgorithm to approach the MMSE fundamental limit is also proposed, and its\nstate evolution is rigorously analyzed. Our results show that the traditional\nlinear assumption between the signal dimension and number of observations in\nthe replica and adaptive interpolation methods is not necessary for sparse\nsignals. They also show how to modify the existing well-known AMP algorithms\nfor linear regimes to sub-linear ones.",
          "link": "http://arxiv.org/abs/2101.11156",
          "publishedOn": "2021-07-15T01:59:04.124Z",
          "wordCount": 614,
          "title": "Fundamental limits and algorithms for sparse linear regression with sublinear sparsity. (arXiv:2101.11156v4 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07900",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Yang_C/0/1/0/all/0/1\">Chaoqi Yang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Qian_C/0/1/0/all/0/1\">Cheng Qian</a>, <a href=\"http://arxiv.org/find/math/1/au:+Singh_N/0/1/0/all/0/1\">Navjot Singh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xiao_C/0/1/0/all/0/1\">Cao Xiao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Westover_M/0/1/0/all/0/1\">M Brandon Westover</a>, <a href=\"http://arxiv.org/find/math/1/au:+Solomonik_E/0/1/0/all/0/1\">Edgar Solomonik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>",
          "description": "Tensor decompositions are powerful tools for dimensionality reduction and\nfeature interpretation of multidimensional data such as signals. Existing\ntensor decomposition objectives (e.g., Frobenius norm) are designed for fitting\nraw data under statistical assumptions, which may not align with downstream\nclassification tasks. Also, real-world tensor data are usually high-ordered and\nhave large dimensions with millions or billions of entries. Thus, it is\nexpensive to decompose the whole tensor with traditional algorithms. In\npractice, raw tensor data also contains redundant information while data\naugmentation techniques may be used to smooth out noise in samples. This paper\naddresses the above challenges by proposing augmented tensor decomposition\n(ATD), which effectively incorporates data augmentations to boost downstream\nclassification. To reduce the memory footprint of the decomposition, we propose\na stochastic algorithm that updates the factor matrices in a batch fashion. We\nevaluate ATD on multiple signal datasets. It shows comparable or better\nperformance (e.g., up to 15% in accuracy) over self-supervised and autoencoder\nbaselines with less than 5% of model parameters, achieves 0.6% ~ 1.3% accuracy\ngain over other tensor-based baselines, and reduces the memory footprint by 9X\nwhen compared to standard tensor decomposition algorithms.",
          "link": "http://arxiv.org/abs/2106.07900",
          "publishedOn": "2021-07-15T01:59:04.103Z",
          "wordCount": 657,
          "title": "Augmented Tensor Decomposition with Stochastic Optimization. (arXiv:2106.07900v3 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Richter_C/0/1/0/all/0/1\">Cedric Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wehrheim_H/0/1/0/all/0/1\">Heike Wehrheim</a>",
          "description": "Learning-based bug detectors promise to find bugs in large code bases by\nexploiting natural hints such as names of variables and functions or comments.\nStill, existing techniques tend to underperform when presented with realistic\nbugs. We believe bug detector learning to currently suffer from a lack of\nrealistic defective training examples. In fact, real world bugs are scarce\nwhich has driven existing methods to train on artificially created and mostly\nunrealistic mutants. In this work, we propose a novel contextual mutation\noperator which incorporates knowledge about the mutation context to dynamically\ninject natural and more realistic faults into code. Our approach employs a\nmasked language model to produce a context-dependent distribution over feasible\ntoken replacements. The evaluation shows that sampling from a language model\ndoes not only produce mutants which more accurately represent real bugs but\nalso lead to better performing bug detectors, both on artificial benchmarks and\non real world source code.",
          "link": "http://arxiv.org/abs/2107.06657",
          "publishedOn": "2021-07-15T01:59:04.096Z",
          "wordCount": 587,
          "title": "DeepMutants: Training neural bug detectors with contextual mutations. (arXiv:2107.06657v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_J/0/1/0/all/0/1\">Jeff Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowers_J/0/1/0/all/0/1\">Jeffrey S. Bowers</a>",
          "description": "That shared features between train and test data are required for\ngeneralisation in artificial neural networks has been a common assumption of\nboth proponents and critics of these models. Here, we show that convolutional\narchitectures avoid this limitation by applying them to two well known\nchallenges, based on learning the identity function and learning rules\ngoverning sequences of words. In each case, successful performance on the test\nset requires generalising to features that were not present in the training\ndata, which is typically not feasible for standard connectionist models.\nHowever, our experiments demonstrate that neural networks can succeed on such\nproblems when they incorporate the weight sharing employed by convolutional\narchitectures. In the image processing domain, such architectures are intended\nto reflect the symmetry under spatial translations of the natural world that\nsuch images depict. We discuss the role of symmetry in the two tasks and its\nconnection to generalisation.",
          "link": "http://arxiv.org/abs/2107.06872",
          "publishedOn": "2021-07-15T01:59:04.091Z",
          "wordCount": 606,
          "title": "Generalisation in Neural Networks Does not Require Feature Overlap. (arXiv:2107.06872v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2103.05630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yinan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_B/0/1/0/all/0/1\">Bei Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1\">Guojun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Luchuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1\">Lu Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>",
          "description": "The rapid progress of photorealistic synthesis techniques has reached at a\ncritical point where the boundary between real and manipulated images starts to\nblur. Thus, benchmarking and advancing digital forgery analysis have become a\npressing issue. However, existing face forgery datasets either have limited\ndiversity or only support coarse-grained analysis. To counter this emerging\nthreat, we construct the ForgeryNet dataset, an extremely large face forgery\ndataset with unified annotations in image- and video-level data across four\ntasks: 1) Image Forgery Classification, including two-way (real / fake),\nthree-way (real / fake with identity-replaced forgery approaches / fake with\nidentity-remained forgery approaches), and n-way (real and 15 respective\nforgery approaches) classification. 2) Spatial Forgery Localization, which\nsegments the manipulated area of fake images compared to their corresponding\nsource real images. 3) Video Forgery Classification, which re-defines the\nvideo-level forgery classification with manipulated frames in random positions.\nThis task is important because attackers in real world are free to manipulate\nany target frame. and 4) Temporal Forgery Localization, to localize the\ntemporal segments which are manipulated. ForgeryNet is by far the largest\npublicly available deep face forgery dataset in terms of data-scale (2.9\nmillion images, 221,247 videos), manipulations (7 image-level approaches, 8\nvideo-level approaches), perturbations (36 independent and more mixed\nperturbations) and annotations (6.3 million classification labels, 2.9 million\nmanipulated area annotations and 221,247 temporal forgery segment labels). We\nperform extensive benchmarking and studies of existing face forensics methods\nand obtain several valuable observations.",
          "link": "http://arxiv.org/abs/2103.05630",
          "publishedOn": "2021-07-15T01:59:04.085Z",
          "wordCount": 735,
          "title": "ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis. (arXiv:2103.05630v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vijayan_N/0/1/0/all/0/1\">Nithia Vijayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+A_P/0/1/0/all/0/1\">Prashanth L. A</a>",
          "description": "We propose two policy gradient algorithms for solving the problem of control\nin an off-policy reinforcement learning (RL) context. Both algorithms\nincorporate a smoothed functional (SF) based gradient estimation scheme. The\nfirst algorithm is a straightforward combination of importance sampling-based\noff-policy evaluation with SF-based gradient estimation. The second algorithm,\ninspired by the stochastic variance-reduced gradient (SVRG) algorithm,\nincorporates variance reduction in the update iteration. For both algorithms,\nwe derive non-asymptotic bounds that establish convergence to an approximate\nstationary point. From these results, we infer that the first algorithm\nconverges at a rate that is comparable to the well-known REINFORCE algorithm in\nan off-policy RL context, while the second algorithm exhibits an improved rate\nof convergence.",
          "link": "http://arxiv.org/abs/2101.02137",
          "publishedOn": "2021-07-15T01:59:04.078Z",
          "wordCount": 594,
          "title": "Smoothed functional-based gradient algorithms for off-policy reinforcement learning: A non-asymptotic viewpoint. (arXiv:2101.02137v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhilin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhenzhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1\">Hongtao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_D/0/1/0/all/0/1\">Da Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dagui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guihai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoqiang Zhu</a>",
          "description": "In e-commerce advertising, it is crucial to jointly consider various\nperformance metrics, e.g., user experience, advertiser utility, and platform\nrevenue. Traditional auction mechanisms, such as GSP and VCG auctions, can be\nsuboptimal due to their fixed allocation rules to optimize a single performance\nmetric (e.g., revenue or social welfare). Recently, data-driven auctions,\nlearned directly from auction outcomes to optimize multiple performance\nmetrics, have attracted increasing research interests. However, the procedure\nof auction mechanisms involves various discrete calculation operations, making\nit challenging to be compatible with continuous optimization pipelines in\nmachine learning. In this paper, we design \\underline{D}eep \\underline{N}eural\n\\underline{A}uctions (DNAs) to enable end-to-end auction learning by proposing\na differentiable model to relax the discrete sorting operation, a key component\nin auctions. We optimize the performance metrics by developing deep models to\nefficiently extract contexts from auctions, providing rich features for auction\ndesign. We further integrate the game theoretical conditions within the model\ndesign, to guarantee the stability of the auctions. DNAs have been successfully\ndeployed in the e-commerce advertising system at Taobao. Experimental\nevaluation results on both large-scale data set as well as online A/B test\ndemonstrated that DNAs significantly outperformed other mechanisms widely\nadopted in industry.",
          "link": "http://arxiv.org/abs/2106.03593",
          "publishedOn": "2021-07-15T01:59:04.062Z",
          "wordCount": 714,
          "title": "Neural Auction: End-to-End Learning of Auction Mechanisms for E-Commerce Advertising. (arXiv:2106.03593v2 [cs.GT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kao_S/0/1/0/all/0/1\">Sheng-Chun Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1\">Suvinay Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_G/0/1/0/all/0/1\">Gaurav Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_T/0/1/0/all/0/1\">Tushar Krishna</a>",
          "description": "Attention mechanisms form the backbone of state-of-the-art machine learning\nmodels for a variety of tasks. Deploying them on deep neural network (DNN)\naccelerators, however, is prohibitively challenging especially under long\nsequences. Operators in attention layers exhibit limited reuse and quadratic\ngrowth in memory footprint, leading to severe memory-boundedness. This paper\nintroduces a new attention-tailored dataflow, termed FLAT, which leverages\noperator fusion, loop-nest optimizations, and interleaved execution. It\nincreases the effective memory bandwidth by efficiently utilizing the\nhigh-bandwidth, low-capacity on-chip buffer and thus achieves better run time\nand compute resource utilization. We term FLAT-compatible accelerators ATTACC.\nIn our evaluation, ATTACC achieves 1.94x and 1.76x speedup and 49% and 42% of\nenergy reduction comparing to state-of-the-art edge and cloud accelerators.",
          "link": "http://arxiv.org/abs/2107.06419",
          "publishedOn": "2021-07-15T01:59:04.056Z",
          "wordCount": 549,
          "title": "ATTACC the Quadratic Bottleneck of Attention Layers. (arXiv:2107.06419v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Forouzesh_M/0/1/0/all/0/1\">Mahsa Forouzesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiran_P/0/1/0/all/0/1\">Patrick Thiran</a>",
          "description": "We propose a metric for evaluating the generalization ability of deep neural\nnetworks trained with mini-batch gradient descent. Our metric, called gradient\ndisparity, is the $\\ell_2$ norm distance between the gradient vectors of two\nmini-batches drawn from the training set. It is derived from a probabilistic\nupper bound on the difference between the classification errors over a given\nmini-batch, when the network is trained on this mini-batch and when the network\nis trained on another mini-batch of points sampled from the same dataset. We\nempirically show that gradient disparity is a very promising early-stopping\ncriterion (i) when data is limited, as it uses all the samples for training and\n(ii) when available data has noisy labels, as it signals overfitting better\nthan the validation data. Furthermore, we show in a wide range of experimental\nsettings that gradient disparity is strongly related to the generalization\nerror between the training and test sets, and that it is also very informative\nabout the level of label noise.",
          "link": "http://arxiv.org/abs/2107.06665",
          "publishedOn": "2021-07-15T01:59:04.050Z",
          "wordCount": 592,
          "title": "Disparity Between Batches as a Signal for Early Stopping. (arXiv:2107.06665v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06865",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingkun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yujie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Lei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Faqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jing Pei</a>",
          "description": "Biological spiking neurons with intrinsic dynamics underlie the powerful\nrepresentation and learning capabilities of the brain for processing multimodal\ninformation in complex environments. Despite recent tremendous progress in\nspiking neural networks (SNNs) for handling Euclidean-space tasks, it still\nremains challenging to exploit SNNs in processing non-Euclidean-space data\nrepresented by graph data, mainly due to the lack of effective modeling\nframework and useful training techniques. Here we present a general spike-based\nmodeling framework that enables the direct training of SNNs for graph learning.\nThrough spatial-temporal unfolding for spiking data flows of node features, we\nincorporate graph convolution filters into spiking dynamics and formalize a\nsynergistic learning paradigm. Considering the unique features of spike\nrepresentation and spiking dynamics, we propose a spatial-temporal feature\nnormalization (STFN) technique suitable for SNN to accelerate convergence. We\ninstantiate our methods into two spiking graph models, including graph\nconvolution SNNs and graph attention SNNs, and validate their performance on\nthree node-classification benchmarks, including Cora, Citeseer, and Pubmed. Our\nmodel can achieve comparable performance with the state-of-the-art graph neural\nnetwork (GNN) models with much lower computation costs, demonstrating great\nbenefits for the execution on neuromorphic hardware and prompting neuromorphic\napplications in graphical scenarios.",
          "link": "http://arxiv.org/abs/2107.06865",
          "publishedOn": "2021-07-15T01:59:04.044Z",
          "wordCount": 645,
          "title": "Exploiting Spiking Dynamics with Spatial-temporal Feature Normalization in Graph Learning. (arXiv:2107.06865v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sessa_P/0/1/0/all/0/1\">Pier Giuseppe Sessa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogunovic_I/0/1/0/all/0/1\">Ilija Bogunovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1\">Andreas Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamgarpour_M/0/1/0/all/0/1\">Maryam Kamgarpour</a>",
          "description": "We formulate the novel class of contextual games, a type of repeated games\ndriven by contextual information at each round. By means of kernel-based\nregularity assumptions, we model the correlation between different contexts and\ngame outcomes and propose a novel online (meta) algorithm that exploits such\ncorrelations to minimize the contextual regret of individual players. We define\ngame-theoretic notions of contextual Coarse Correlated Equilibria (c-CCE) and\noptimal contextual welfare for this new class of games and show that c-CCEs and\noptimal welfare can be approached whenever players' contextual regrets vanish.\nFinally, we empirically validate our results in a traffic routing experiment,\nwhere our algorithm leads to better performance and higher welfare compared to\nbaselines that do not exploit the available contextual information or the\ncorrelations present in the game.",
          "link": "http://arxiv.org/abs/2107.06327",
          "publishedOn": "2021-07-15T01:59:04.038Z",
          "wordCount": 579,
          "title": "Contextual Games: Multi-Agent Learning with Side Information. (arXiv:2107.06327v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Duhun Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">Eunjung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1\">Wonjong Rhee</a>",
          "description": "We propose an AID-purifier that can boost the robustness of\nadversarially-trained networks by purifying their inputs. AID-purifier is an\nauxiliary network that works as an add-on to an already trained main\nclassifier. To keep it computationally light, it is trained as a discriminator\nwith a binary cross-entropy loss. To obtain additionally useful information\nfrom the adversarial examples, the architecture design is closely related to\ninformation maximization principles where two layers of the main classification\nnetwork are piped to the auxiliary network. To assist the iterative\noptimization procedure of purification, the auxiliary network is trained with\nAVmixup. AID-purifier can be used together with other purifiers such as\nPixelDefend for an extra enhancement. The overall results indicate that the\nbest performing adversarially-trained networks can be enhanced by the best\nperforming purification networks, where AID-purifier is a competitive candidate\nthat is light and robust.",
          "link": "http://arxiv.org/abs/2107.06456",
          "publishedOn": "2021-07-15T01:59:04.021Z",
          "wordCount": 590,
          "title": "AID-Purifier: A Light Auxiliary Network for Boosting Adversarial Defense. (arXiv:2107.06456v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.04313",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Gibbs_J/0/1/0/all/0/1\">Joe Gibbs</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Gili_K/0/1/0/all/0/1\">Kaitlin Gili</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Holmes_Z/0/1/0/all/0/1\">Zo&#xeb; Holmes</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Commeau_B/0/1/0/all/0/1\">Benjamin Commeau</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Arrasmith_A/0/1/0/all/0/1\">Andrew Arrasmith</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Cincio_L/0/1/0/all/0/1\">Lukasz Cincio</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Coles_P/0/1/0/all/0/1\">Patrick J. Coles</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sornborger_A/0/1/0/all/0/1\">Andrew Sornborger</a>",
          "description": "Moderate-size quantum computers are now publicly accessible over the cloud,\nopening the exciting possibility of performing dynamical simulations of quantum\nsystems. However, while rapidly improving, these devices have short coherence\ntimes, limiting the depth of algorithms that may be successfully implemented.\nHere we demonstrate that, despite these limitations, it is possible to\nimplement long-time, high fidelity simulations on current hardware.\nSpecifically, we simulate an XY-model spin chain on the Rigetti and IBM quantum\ncomputers, maintaining a fidelity of at least 0.9 for over 600 time steps. This\nis a factor of 150 longer than is possible using the iterated Trotter method.\nOur simulations are performed using a new algorithm that we call the fixed\nstate Variational Fast Forwarding (fsVFF) algorithm. This algorithm decreases\nthe circuit depth and width required for a quantum simulation by finding an\napproximate diagonalization of a short time evolution unitary. Crucially, fsVFF\nonly requires finding a diagonalization on the subspace spanned by the initial\nstate, rather than on the total Hilbert space as with previous methods,\nsubstantially reducing the required resources. We further demonstrate the\nviability of fsVFF through large numerical implementations of the algorithm, as\nwell as an analysis of its noise resilience and the scaling of simulation\nerrors.",
          "link": "http://arxiv.org/abs/2102.04313",
          "publishedOn": "2021-07-15T01:59:04.014Z",
          "wordCount": 682,
          "title": "Long-time simulations with high fidelity on quantum hardware. (arXiv:2102.04313v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sokol_K/0/1/0/all/0/1\">Kacper Sokol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flach_P/0/1/0/all/0/1\">Peter Flach</a>",
          "description": "Academic trade requires juggling multiple variants of the same content\npublished in different formats: manuscripts, presentations, posters and\ncomputational notebooks. The need to track versions to accommodate for the\nwrite--review--rebut--revise life-cycle adds another layer of complexity. We\npropose to significantly reduce this burden by maintaining a single source\ndocument in a version-controlled environment (such as git), adding\nfunctionality to generate a collection of output formats popular in academia.\nTo this end, we utilise various open-source tools from the Jupyter scientific\ncomputing ecosystem and operationalise selected software engineering concepts.\nWe offer a proof-of-concept workflow that composes Jupyter Book (an online\ndocument), Jupyter Notebook (a computational narrative) and reveal.js slides\nfrom a single markdown source file. Hosted on GitHub, our approach supports\nchange tracking and versioning, as well as a transparent review process based\non the underlying code issue management infrastructure. An exhibit of our\nworkflow can be previewed at https://so-cool.github.io/you-only-write-thrice/.",
          "link": "http://arxiv.org/abs/2107.06639",
          "publishedOn": "2021-07-15T01:59:04.007Z",
          "wordCount": 614,
          "title": "You Only Write Thrice: Creating Documents, Computational Notebooks and Presentations From a Single Source. (arXiv:2107.06639v1 [cs.PL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nagrecha_K/0/1/0/all/0/1\">Kabir Nagrecha</a>",
          "description": "As deep learning becomes more expensive, both in terms of time and compute,\ninefficiencies in machine learning (ML) training prevent practical usage of\nstate-of-the-art models for most users. The newest model architectures are\nsimply too large to be fit onto a single processor. To address the issue, many\nML practitioners have turned to model parallelism as a method of distributing\nthe computational requirements across several devices. Unfortunately, the\nsequential nature of neural networks causes very low efficiency and device\nutilization in model parallel training jobs. We propose a new form of \"shard\nparallelism\" combining task and model parallelism, then package it into a\nframework we name Hydra. Hydra recasts the problem of model parallelism in the\nmulti-model context to produce a fine-grained parallel workload of independent\nmodel shards, rather than independent models. This new parallel design promises\ndramatic speedups relative to the traditional model parallelism paradigm.",
          "link": "http://arxiv.org/abs/2107.06469",
          "publishedOn": "2021-07-15T01:59:03.992Z",
          "wordCount": 606,
          "title": "Model-Parallel Model Selection for Deep Learning Systems. (arXiv:2107.06469v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2007.06093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albarghouthi_A/0/1/0/all/0/1\">Aws Albarghouthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakriya_G/0/1/0/all/0/1\">Gautam Prakriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>",
          "description": "To verify safety and robustness of neural networks, researchers have\nsuccessfully applied abstract interpretation, primarily using the interval\nabstract domain. In this paper, we study the theoretical power and limits of\nthe interval domain for neural-network verification.\n\nFirst, we introduce the interval universal approximation (IUA) theorem. IUA\nshows that neural networks not only can approximate any continuous function $f$\n(universal approximation) as we have known for decades, but we can find a\nneural network, using any well-behaved activation function, whose interval\nbounds are an arbitrarily close approximation of the set semantics of $f$ (the\nresult of applying $f$ to a set of inputs). We call this notion of\napproximation interval approximation. Our theorem generalizes the recent result\nof Baader et al. (2020) from ReLUs to a rich class of activation functions that\nwe call squashable functions. Additionally, the IUA theorem implies that we can\nalways construct provably robust neural networks under $\\ell_\\infty$-norm using\nalmost any practical activation function.\n\nSecond, we study the computational complexity of constructing neural networks\nthat are amenable to precise interval analysis. This is a crucial question, as\nour constructive proof of IUA is exponential in the size of the approximation\ndomain. We boil this question down to the problem of approximating the range of\na neural network with squashable activation functions. We show that the range\napproximation problem (RA) is a $\\Delta_2$-intermediate problem, which is\nstrictly harder than $\\mathsf{NP}$-complete problems, assuming\n$\\mathsf{coNP}\\not\\subset \\mathsf{NP}$. As a result, IUA is an inherently hard\nproblem: No matter what abstract domain or computational tools we consider to\nachieve interval approximation, there is no efficient construction of such a\nuniversal approximator.",
          "link": "http://arxiv.org/abs/2007.06093",
          "publishedOn": "2021-07-15T01:59:03.984Z",
          "wordCount": 760,
          "title": "Interval Universal Approximation for Neural Networks. (arXiv:2007.06093v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Camargo_A/0/1/0/all/0/1\">Augusto Camargo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_W/0/1/0/all/0/1\">Wesley Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peressim_F/0/1/0/all/0/1\">Felipe Peressim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_A/0/1/0/all/0/1\">Alan Barzilay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finger_M/0/1/0/all/0/1\">Marcelo Finger</a>",
          "description": "In this paper, we studied whether models based on BiLSTM and BERT can predict\nhashtags in Brazilian Portuguese for Ecommerce websites. Hashtags have a\nsizable financial impact on Ecommerce. We processed a corpus of Ecommerce\nreviews as inputs, and predicted hashtags as outputs. We evaluated the results\nusing four quantitative metrics: NIST, BLEU, METEOR and a crowdsourced score. A\nword cloud was used as a qualitative metric. While all computer-generated\nmetrics (NIST, BLEU and METEOR) indicated bad results, the crowdsourced results\nproduced amazing scores. We concluded that the texts predicted by the neural\nnetworks are very promising for use as hashtags for products on Ecommerce\nwebsites. The code for this work is available at\nhttps://github.com/augustocamargo/text-to-hashtag.",
          "link": "http://arxiv.org/abs/2102.00904",
          "publishedOn": "2021-07-15T01:59:03.938Z",
          "wordCount": 573,
          "title": "Text-to-hashtag Generation using Seq2seq Learning. (arXiv:2102.00904v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Svedin_M/0/1/0/all/0/1\">Martin Svedin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Podobas_A/0/1/0/all/0/1\">Artur Podobas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Steven W. D. Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markidis_S/0/1/0/all/0/1\">Stefano Markidis</a>",
          "description": "One of the most promising approaches for data analysis and exploration of\nlarge data sets is Machine Learning techniques that are inspired by brain\nmodels. Such methods use alternative learning rules potentially more\nefficiently than established learning rules. In this work, we focus on the\npotential of brain-inspired ML for exploiting High-Performance Computing (HPC)\nresources to solve ML problems: we discuss the BCPNN and an HPC implementation,\ncalled StreamBrain, its computational cost, suitability to HPC systems. As an\nexample, we use StreamBrain to analyze the Higgs Boson dataset from High Energy\nPhysics and discriminate between background and signal classes in collisions of\nhigh-energy particle colliders. Overall, we reach up to 69.15% accuracy and\n76.4% Area Under the Curve (AUC) performance.",
          "link": "http://arxiv.org/abs/2107.06676",
          "publishedOn": "2021-07-15T01:59:03.932Z",
          "wordCount": 591,
          "title": "Higgs Boson Classification: Brain-inspired BCPNN Learning with StreamBrain. (arXiv:2107.06676v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akarmazyan_S/0/1/0/all/0/1\">Siranush Akarmazyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengaud_E/0/1/0/all/0/1\">Eric Armengaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacco_M/0/1/0/all/0/1\">Manlio Bacco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bravos_G/0/1/0/all/0/1\">George Bravos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calandra_C/0/1/0/all/0/1\">Calogero Calandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_E/0/1/0/all/0/1\">Emanuele Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1\">Antonio Carta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassara_P/0/1/0/all/0/1\">Pietro Cassara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coppola_M/0/1/0/all/0/1\">Massimo Coppola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davalas_C/0/1/0/all/0/1\">Charalampos Davalas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dazzi_P/0/1/0/all/0/1\">Patrizio Dazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Degennaro_M/0/1/0/all/0/1\">Maria Carmela Degennaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarli_D/0/1/0/all/0/1\">Daniele Di Sarli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobaj_J/0/1/0/all/0/1\">J&#xfc;rgen Dobaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallicchio_C/0/1/0/all/0/1\">Claudio Gallicchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girbal_S/0/1/0/all/0/1\">Sylvain Girbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotta_A/0/1/0/all/0/1\">Alberto Gotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groppo_R/0/1/0/all/0/1\">Riccardo Groppo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1\">Vincenzo Lomonaco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macher_G/0/1/0/all/0/1\">Georg Macher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazzei_D/0/1/0/all/0/1\">Daniele Mazzei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mencagli_G/0/1/0/all/0/1\">Gabriele Mencagli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michail_D/0/1/0/all/0/1\">Dimitrios Michail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Micheli_A/0/1/0/all/0/1\">Alessio Micheli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peroglio_R/0/1/0/all/0/1\">Roberta Peroglio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_S/0/1/0/all/0/1\">Salvatore Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potenza_R/0/1/0/all/0/1\">Rosaria Potenza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pourdanesh_F/0/1/0/all/0/1\">Farank Pourdanesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sardianos_C/0/1/0/all/0/1\">Christos Sardianos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tserpes_K/0/1/0/all/0/1\">Konstantinos Tserpes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliabo_F/0/1/0/all/0/1\">Fulvio Tagliab&#xf2;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valtl_J/0/1/0/all/0/1\">Jakob Valtl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varlamis_I/0/1/0/all/0/1\">Iraklis Varlamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veledar_O/0/1/0/all/0/1\">Omar Veledar</a>",
          "description": "This paper discusses the perspective of the H2020 TEACHING project on the\nnext generation of autonomous applications running in a distributed and highly\nheterogeneous environment comprising both virtual and physical resources\nspanning the edge-cloud continuum. TEACHING puts forward a human-centred vision\nleveraging the physiological, emotional, and cognitive state of the users as a\ndriver for the adaptation and optimization of the autonomous applications. It\ndoes so by building a distributed, embedded and federated learning system\ncomplemented by methods and tools to enforce its dependability, security and\nprivacy preservation. The paper discusses the main concepts of the TEACHING\napproach and singles out the main AI-related research challenges associated\nwith it. Further, we provide a discussion of the design choices for the\nTEACHING system to tackle the aforementioned challenges",
          "link": "http://arxiv.org/abs/2107.06543",
          "publishedOn": "2021-07-15T01:59:03.922Z",
          "wordCount": 627,
          "title": "TEACHING -- Trustworthy autonomous cyber-physical applications through human-centred intelligence. (arXiv:2107.06543v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06642",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Luong_M/0/1/0/all/0/1\">Manh Luong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tran_V/0/1/0/all/0/1\">Viet Anh Tran</a>",
          "description": "Voice conversion is a challenging task which transforms the voice\ncharacteristics of a source speaker to a target speaker without changing\nlinguistic content. Recently, there have been many works on many-to-many Voice\nConversion (VC) based on Variational Autoencoder (VAEs) achieving good results,\nhowever, these methods lack the ability to disentangle speaker identity and\nlinguistic content to achieve good performance on unseen speaker scenarios. In\nthis paper, we propose a new method based on feature disentanglement to tackle\nmany to many voice conversion. The method has the capability to disentangle\nspeaker identity and linguistic content from utterances, it can convert from\nmany source speakers to many target speakers with a single autoencoder network.\nMoreover, it naturally deals with the unseen target speaker scenarios. We\nperform both objective and subjective evaluations to show the competitive\nperformance of our proposed method compared with other state-of-the-art models\nin terms of naturalness and target speaker similarity.",
          "link": "http://arxiv.org/abs/2107.06642",
          "publishedOn": "2021-07-15T01:59:03.909Z",
          "wordCount": 594,
          "title": "Many-to-Many Voice Conversion based Feature Disentanglement using Variational Autoencoder. (arXiv:2107.06642v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06615",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Munteanu_A/0/1/0/all/0/1\">Alexander Munteanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omlor_S/0/1/0/all/0/1\">Simon Omlor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1\">David Woodruff</a>",
          "description": "What guarantees are possible for solving logistic regression in one pass over\na data stream? To answer this question, we present the first data oblivious\nsketch for logistic regression. Our sketch can be computed in input sparsity\ntime over a turnstile data stream and reduces the size of a $d$-dimensional\ndata set from $n$ to only $\\operatorname{poly}(\\mu d\\log n)$ weighted points,\nwhere $\\mu$ is a useful parameter which captures the complexity of compressing\nthe data. Solving (weighted) logistic regression on the sketch gives an $O(\\log\nn)$-approximation to the original problem on the full data set. We also show\nhow to obtain an $O(1)$-approximation with slight modifications. Our sketches\nare fast, simple, easy to implement, and our experiments demonstrate their\npracticality.",
          "link": "http://arxiv.org/abs/2107.06615",
          "publishedOn": "2021-07-15T01:59:03.883Z",
          "wordCount": 558,
          "title": "Oblivious sketching for logistic regression. (arXiv:2107.06615v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bartal_Y/0/1/0/all/0/1\">Yair Bartal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fandina_O/0/1/0/all/0/1\">Ora Nova Fandina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsen_K/0/1/0/all/0/1\">Kasper Green Larsen</a>",
          "description": "It is well known that the Johnson-Lindenstrauss dimensionality reduction\nmethod is optimal for worst case distortion. While in practice many other\nmethods and heuristics are used, not much is known in terms of bounds on their\nperformance. The question of whether the JL method is optimal for practical\nmeasures of distortion was recently raised in \\cite{BFN19} (NeurIPS'19). They\nprovided upper bounds on its quality for a wide range of practical measures and\nshowed that indeed these are best possible in many cases. Yet, some of the most\nimportant cases, including the fundamental case of average distortion were left\nopen. In particular, they show that the JL transform has $1+\\epsilon$ average\ndistortion for embedding into $k$-dimensional Euclidean space, where\n$k=O(1/\\eps^2)$, and for more general $q$-norms of distortion, $k =\nO(\\max\\{1/\\eps^2,q/\\eps\\})$, whereas tight lower bounds were established only\nfor large values of $q$ via reduction to the worst case.\n\nIn this paper we prove that these bounds are best possible for any\ndimensionality reduction method, for any $1 \\leq q \\leq O(\\frac{\\log (2\\eps^2\nn)}{\\eps})$ and $\\epsilon \\geq \\frac{1}{\\sqrt{n}}$, where $n$ is the size of\nthe subset of Euclidean space.\n\nOur results imply that the JL method is optimal for various distortion\nmeasures commonly used in practice, such as {\\it stress, energy} and {\\it\nrelative error}. We prove that if any of these measures is bounded by $\\eps$\nthen $k=\\Omega(1/\\eps^2)$, for any $\\epsilon \\geq \\frac{1}{\\sqrt{n}}$, matching\nthe upper bounds of \\cite{BFN19} and extending their tightness results for the\nfull range moment analysis.\n\nOur results may indicate that the JL dimensionality reduction method should\nbe considered more often in practical applications, and the bounds we provide\nfor its quality should be served as a measure for comparison when evaluating\nthe performance of other methods and heuristics.",
          "link": "http://arxiv.org/abs/2107.06626",
          "publishedOn": "2021-07-15T01:59:03.876Z",
          "wordCount": 741,
          "title": "Optimality of the Johnson-Lindenstrauss Dimensionality Reduction for Practical Measures. (arXiv:2107.06626v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>",
          "description": "For most machine learning (ML) tasks, evaluating learning performance on a\ngiven dataset requires intensive computation. On the other hand, the ability to\nefficiently estimate learning performance may benefit a wide spectrum of\napplications, such as active learning, data quality management, and data\nvaluation. Recent empirical studies show that for many common ML models, one\ncan accurately learn a parametric model that predicts learning performance for\nany given input datasets using a small amount of samples. However, the\ntheoretical underpinning of the learnability of such performance prediction\nmodels is still missing. In this work, we develop the first theoretical\nanalysis of the ML performance learning problem. We propose a relaxed notion\nfor submodularity that can well describe the behavior of learning performance\nas a function of input datasets. We give a learning algorithm that achieves a\nconstant-factor approximation under certain assumptions. Further, we give a\nlearning algorithm that achieves arbitrarily small error based on a newly\nderived structural result. We then discuss a natural, important use case of\nlearning performance learning -- data valuation, which is known to suffer\ncomputational challenges due to the requirement of estimating learning\nperformance for many data combinations. We show that performance learning can\nsignificantly improve the accuracy of data valuation.",
          "link": "http://arxiv.org/abs/2107.06336",
          "publishedOn": "2021-07-15T01:59:03.863Z",
          "wordCount": 638,
          "title": "Learnability of Learning Performance and Its Application to Data Valuation. (arXiv:2107.06336v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenqi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Siqin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuhui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>",
          "description": "Recurrent neural networks for language models like long short-term memory\n(LSTM) have been utilized as a tool for modeling and predicting long term\ndynamics of complex stochastic molecular systems. Recently successful examples\non learning slow dynamics by LSTM are given with simulation data of low\ndimensional reaction coordinate. However, in this report we show that the\nfollowing three key factors significantly affect the performance of language\nmodel learning, namely dimensionality of reaction coordinates, temporal\nresolution and state partition. When applying recurrent neural networks to\nmolecular dynamics simulation trajectories of high dimensionality, we find that\nrare events corresponding to the slow dynamics might be obscured by other\nfaster dynamics of the system, and cannot be efficiently learned. Under such\nconditions, we find that coarse graining the conformational space into\nmetastable states and removing recrossing events when estimating transition\nprobabilities between states could greatly help improve the accuracy of slow\ndynamics learning in molecular dynamics. Moreover, we also explore other models\nlike Transformer, which do not show superior performance than LSTM in\novercoming these issues. Therefore, to learn rare events of slow molecular\ndynamics by LSTM and Transformer, it is critical to choose proper temporal\nresolution (i.e., saving intervals of MD simulation trajectories) and state\npartition in high resolution data, since deep neural network models might not\nautomatically disentangle slow dynamics from fast dynamics when both are\npresent in data influencing each other.",
          "link": "http://arxiv.org/abs/2107.06573",
          "publishedOn": "2021-07-15T01:59:03.848Z",
          "wordCount": 680,
          "title": "A Note on Learning Rare Events in Molecular Dynamics using LSTM and Transformer. (arXiv:2107.06573v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "Most existing Vision-and-Language (V&L) models rely on pre-trained visual\nencoders, using a relatively small set of manually-annotated data (as compared\nto web-crawled data), to perceive the visual world. However, it has been\nobserved that large-scale pretraining usually can result in better\ngeneralization performance, e.g., CLIP (Contrastive Language-Image\nPre-training), trained on a massive amount of image-caption pairs, has shown a\nstrong zero-shot capability on various vision tasks. To further study the\nadvantage brought by CLIP, we propose to use CLIP as the visual encoder in\nvarious V&L models in two typical scenarios: 1) plugging CLIP into\ntask-specific fine-tuning; 2) combining CLIP with V&L pre-training and\ntransferring to downstream tasks. We show that CLIP significantly outperforms\nwidely-used visual encoders trained with in-domain annotated data, such as\nBottomUp-TopDown. We achieve competitive or better results on diverse V&L\ntasks, while establishing new state-of-the-art results on Visual Question\nAnswering, Visual Entailment, and V&L Navigation tasks. We release our code at\nhttps://github.com/clip-vil/CLIP-ViL.",
          "link": "http://arxiv.org/abs/2107.06383",
          "publishedOn": "2021-07-15T01:59:03.826Z",
          "wordCount": 618,
          "title": "How Much Can CLIP Benefit Vision-and-Language Tasks?. (arXiv:2107.06383v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roschewitz_D/0/1/0/all/0/1\">David Roschewitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartley_M/0/1/0/all/0/1\">Mary-Anne Hartley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corinzia_L/0/1/0/all/0/1\">Luca Corinzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>",
          "description": "Recently, the ever-growing demand for privacy-oriented machine learning has\nmotivated researchers to develop federated and decentralized learning\ntechniques, allowing individual clients to train models collaboratively without\ndisclosing their private datasets. However, widespread adoption has been\nlimited in domains relying on high levels of user trust, where assessment of\ndata compatibility is essential. In this work, we define and address low\ninteroperability induced by underlying client data inconsistencies in federated\nlearning for tabular data. The proposed method, iFedAvg, builds on federated\naveraging adding local element-wise affine layers to allow for a personalized\nand granular understanding of the collaborative learning process. Thus,\nenabling the detection of outlier datasets in the federation and also learning\nthe compensation for local data distribution shifts without sharing any\noriginal data. We evaluate iFedAvg using several public benchmarks and a\npreviously unstudied collection of real-world datasets from the 2014 - 2016\nWest African Ebola epidemic, jointly forming the largest such dataset in the\nworld. In all evaluations, iFedAvg achieves competitive average performance\nwith negligible overhead. It additionally shows substantial improvement on\noutlier clients, highlighting increased robustness to individual dataset\nshifts. Most importantly, our method provides valuable client-specific insights\nat a fine-grained level to guide interoperable federated learning.",
          "link": "http://arxiv.org/abs/2107.06580",
          "publishedOn": "2021-07-15T01:59:03.817Z",
          "wordCount": 633,
          "title": "IFedAvg: Interpretable Data-Interoperability for Federated Learning. (arXiv:2107.06580v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Orzechowski_P/0/1/0/all/0/1\">Patryk Orzechowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1\">Jason H. Moore</a>",
          "description": "Understanding the strengths and weaknesses of machine learning (ML)\nalgorithms is crucial for determine their scope of application. Here, we\nintroduce the DIverse and GENerative ML Benchmark (DIGEN) - a collection of\nsynthetic datasets for comprehensive, reproducible, and interpretable\nbenchmarking of machine learning algorithms for classification of binary\noutcomes. The DIGEN resource consists of 40 mathematical functions which map\ncontinuous features to discrete endpoints for creating synthetic datasets.\nThese 40 functions were discovered using a heuristic algorithm designed to\nmaximize the diversity of performance among multiple popular machine learning\nalgorithms thus providing a useful test suite for evaluating and comparing new\nmethods. Access to the generative functions facilitates understanding of why a\nmethod performs poorly compared to other algorithms thus providing ideas for\nimprovement. The resource with extensive documentation and analyses is\nopen-source and available on GitHub.",
          "link": "http://arxiv.org/abs/2107.06475",
          "publishedOn": "2021-07-15T01:59:03.790Z",
          "wordCount": 615,
          "title": "Generative and reproducible benchmarks for comprehensive evaluation of machine learning classifiers. (arXiv:2107.06475v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1\">Sungryull Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungtae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jongwook Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seijen_H/0/1/0/all/0/1\">Harm van Seijen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatemi_M/0/1/0/all/0/1\">Mehdi Fatemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>",
          "description": "We propose the k-Shortest-Path (k-SP) constraint: a novel constraint on the\nagent's trajectory that improves the sample efficiency in sparse-reward MDPs.\nWe show that any optimal policy necessarily satisfies the k-SP constraint.\nNotably, the k-SP constraint prevents the policy from exploring state-action\npairs along the non-k-SP trajectories (e.g., going back and forth). However, in\npractice, excluding state-action pairs may hinder the convergence of RL\nalgorithms. To overcome this, we propose a novel cost function that penalizes\nthe policy violating SP constraint, instead of completely excluding it. Our\nnumerical experiment in a tabular RL setting demonstrates that the SP\nconstraint can significantly reduce the trajectory space of policy. As a\nresult, our constraint enables more sample efficient learning by suppressing\nredundant exploration and exploitation. Our experiments on MiniGrid, DeepMind\nLab, Atari, and Fetch show that the proposed method significantly improves\nproximal policy optimization (PPO) and outperforms existing novelty-seeking\nexploration methods including count-based exploration even in continuous\ncontrol tasks, indicating that it improves the sample efficiency by preventing\nthe agent from taking redundant actions.",
          "link": "http://arxiv.org/abs/2107.06405",
          "publishedOn": "2021-07-15T01:59:03.772Z",
          "wordCount": 619,
          "title": "Shortest-Path Constrained Reinforcement Learning for Sparse Reward Tasks. (arXiv:2107.06405v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yihao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_W/0/1/0/all/0/1\">Weikai Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1\">Geguang Pu</a>",
          "description": "High-level representation-guided pixel denoising and adversarial training are\nindependent solutions to enhance the robustness of CNNs against adversarial\nattacks by pre-processing input data and re-training models, respectively. Most\nrecently, adversarial training techniques have been widely studied and improved\nwhile the pixel denoising-based method is getting less attractive. However, it\nis still questionable whether there exists a more advanced pixel\ndenoising-based method and whether the combination of the two solutions\nbenefits each other. To this end, we first comprehensively investigate two\nkinds of pixel denoising methods for adversarial robustness enhancement (i.e.,\nexisting additive-based and unexplored filtering-based methods) under the loss\nfunctions of image-level and semantic-level restorations, respectively, showing\nthat pixel-wise filtering can obtain much higher image quality (e.g., higher\nPSNR) as well as higher robustness (e.g., higher accuracy on adversarial\nexamples) than existing pixel-wise additive-based method. However, we also\nobserve that the robustness results of the filtering-based method rely on the\nperturbation amplitude of adversarial examples used for training. To address\nthis problem, we propose predictive perturbation-aware pixel-wise filtering,\nwhere dual-perturbation filtering and an uncertainty-aware fusion module are\ndesigned and employed to automatically perceive the perturbation amplitude\nduring the training and testing process. The proposed method is termed as\nAdvFilter. Moreover, we combine adversarial pixel denoising methods with three\nadversarial training-based methods, hinting that considering data and models\njointly is able to achieve more robust CNNs. The experiments conduct on\nNeurIPS-2017DEV, SVHN, and CIFAR10 datasets and show the advantages over\nenhancing CNNs' robustness, high generalization to different models, and noise\nlevels.",
          "link": "http://arxiv.org/abs/2107.06501",
          "publishedOn": "2021-07-15T01:59:03.766Z",
          "wordCount": 716,
          "title": "AdvFilter: Predictive Perturbation-aware Filtering against Adversarial Attack via Multi-domain Learning. (arXiv:2107.06501v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Tuan Anh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_K/0/1/0/all/0/1\">Katherine M. Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_L/0/1/0/all/0/1\">Luke Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1\">Kevin Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+N_S/0/1/0/all/0/1\">Siddharth N</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gershman_S/0/1/0/all/0/1\">Samuel J. Gershman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>",
          "description": "Modeling complex phenomena typically involves the use of both discrete and\ncontinuous variables. Such a setting applies across a wide range of problems,\nfrom identifying trends in time-series data to performing effective\ncompositional scene understanding in images. Here, we propose Hybrid Memoised\nWake-Sleep (HMWS), an algorithm for effective inference in such hybrid\ndiscrete-continuous models. Prior approaches to learning suffer as they need to\nperform repeated expensive inner-loop discrete inference. We build on a recent\napproach, Memoised Wake-Sleep (MWS), which alleviates part of the problem by\nmemoising discrete variables, and extend it to allow for a principled and\neffective way to handle continuous variables by learning a separate recognition\nmodel used for importance-sampling based approximate inference and\nmarginalization. We evaluate HMWS in the GP-kernel learning and 3D scene\nunderstanding domains, and show that it outperforms current state-of-the-art\ninference methods.",
          "link": "http://arxiv.org/abs/2107.06393",
          "publishedOn": "2021-07-15T01:59:03.756Z",
          "wordCount": 595,
          "title": "Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface. (arXiv:2107.06393v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06396",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Sengupta_U/0/1/0/all/0/1\">Ushnish Sengupta</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Waxenegger_Wilfing_G/0/1/0/all/0/1\">G&#xfc;nther Waxenegger-Wilfing</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hardi_J/0/1/0/all/0/1\">Justin Hardi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Juniper_M/0/1/0/all/0/1\">Matthew P. Juniper</a>",
          "description": "The 100 MW cryogenic liquid oxygen/hydrogen multi-injector combustor BKD\noperated by the DLR Institute of Space Propulsion is a research platform that\nallows the study of thermoacoustic instabilities under realistic conditions,\nrepresentative of small upper stage rocket engines. We use data from BKD\nexperimental campaigns in which the static chamber pressure and fuel-oxidizer\nratio are varied such that the first tangential mode of the combustor is\nexcited under some conditions. We train an autoregressive Bayesian neural\nnetwork model to forecast the amplitude of the dynamic pressure time series,\ninputting multiple sensor measurements (injector pressure/ temperature\nmeasurements, static chamber pressure, high-frequency dynamic pressure\nmeasurements, high-frequency OH* chemiluminescence measurements) and future\nflow rate control signals. The Bayesian nature of our algorithms allows us to\nwork with a dataset whose size is restricted by the expense of each\nexperimental run, without making overconfident extrapolations. We find that the\nnetworks are able to accurately forecast the evolution of the pressure\namplitude and anticipate instability events on unseen experimental runs 500\nmilliseconds in advance. We compare the predictive accuracy of multiple models\nusing different combinations of sensor inputs. We find that the high-frequency\ndynamic pressure signal is particularly informative. We also use the technique\nof integrated gradients to interpret the influence of different sensor inputs\non the model prediction. The negative log-likelihood of data points in the test\ndataset indicates that predictive uncertainties are well-characterized by our\nBayesian model and simulating a sensor failure event results as expected in a\ndramatic increase in the epistemic component of the uncertainty.",
          "link": "http://arxiv.org/abs/2107.06396",
          "publishedOn": "2021-07-15T01:59:03.748Z",
          "wordCount": 705,
          "title": "Forecasting Thermoacoustic Instabilities in Liquid Propellant Rocket Engines Using Multimodal Bayesian Deep Learning. (arXiv:2107.06396v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Baihe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaixuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham M. Kakade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason D. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1\">Qi Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Runzhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaqi Yang</a>",
          "description": "Deep Reinforcement Learning (RL) powered by neural net approximation of the Q\nfunction has had enormous empirical success. While the theory of RL has\ntraditionally focused on linear function approximation (or eluder dimension)\napproaches, little is known about nonlinear RL with neural net approximations\nof the Q functions. This is the focus of this work, where we study function\napproximation with two-layer neural networks (considering both ReLU and\npolynomial activation functions). Our first result is a computationally and\nstatistically efficient algorithm in the generative model setting under\ncompleteness for two-layer neural networks. Our second result considers this\nsetting but under only realizability of the neural net function class. Here,\nassuming deterministic dynamics, the sample complexity scales linearly in the\nalgebraic dimension. In all cases, our results significantly improve upon what\ncan be attained with linear (or eluder dimension) methods.",
          "link": "http://arxiv.org/abs/2107.06466",
          "publishedOn": "2021-07-15T01:59:03.741Z",
          "wordCount": 583,
          "title": "Going Beyond Linear RL: Sample Efficient Neural Function Approximation. (arXiv:2107.06466v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06473",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Fang_W/0/1/0/all/0/1\">Wenqi Fang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_G/0/1/0/all/0/1\">Guanlin Wu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cao_J/0/1/0/all/0/1\">Jiang Cao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ping_Y/0/1/0/all/0/1\">Yang Ping</a>",
          "description": "Spectral approximation and variational inducing learning for the Gaussian\nprocess are two popular methods to reduce computational complexity. However, in\nprevious research, those methods always tend to adopt the orthonormal basis\nfunctions, such as eigenvectors in the Hilbert space, in the spectrum method,\nor decoupled orthogonal components in the variational framework. In this paper,\ninspired by quantum physics, we introduce a novel basis function, which is\ntunable, local and bounded, to approximate the kernel function in the Gaussian\nprocess. There are two adjustable parameters in these functions, which control\ntheir orthogonality to each other and limit their boundedness. And we conduct\nextensive experiments on open-source datasets to testify its performance.\nCompared to several state-of-the-art methods, it turns out that the proposed\nmethod can obtain satisfactory or even better results, especially with poorly\nchosen kernel functions.",
          "link": "http://arxiv.org/abs/2107.06473",
          "publishedOn": "2021-07-15T01:59:03.734Z",
          "wordCount": 576,
          "title": "Spectrum Gaussian Processes Based On Tunable Basis Functions. (arXiv:2107.06473v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Awalina_A/0/1/0/all/0/1\">Aisyah Awalina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fawaid_J/0/1/0/all/0/1\">Jibran Fawaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krisnabayu_R/0/1/0/all/0/1\">Rifky Yunus Krisnabayu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1\">Novanto Yudistira</a>",
          "description": "Fake news is a problem faced by society in this era. It is not rare for fake\nnews to cause provocation and problem for the people. Indonesia, as a country\nwith the 4th largest population, has a problem in dealing with fake news. More\nthan 30% of rural and urban population are deceived by this fake news problem.\nAs we have been studying, there is only few literatures on preventing the\nspread of fake news in Bahasa Indonesia. So, this research is conducted to\nprevent these problems. The dataset used in this research was obtained from a\nnews portal that identifies fake news, turnbackhoax.id. Using Web Scrapping on\nthis page, we got 1116 data consisting of valid news and fake news. The dataset\ncan be accessed at https://github.com/JibranFawaid/turnbackhoax-dataset. This\ndataset will be combined with other available datasets. The methods used are\nCNN, BiLSTM, Hybrid CNN-BiLSTM, and BERT with Transformer Network. This\nresearch shows that the BERT method with Transformer Network has the best\nresults with an accuracy of up to 90%.",
          "link": "http://arxiv.org/abs/2107.06796",
          "publishedOn": "2021-07-15T01:59:03.710Z",
          "wordCount": 608,
          "title": "Indonesia's Fake News Detection using Transformer Network. (arXiv:2107.06796v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1\">Ibrahim Alabdulmohsin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1\">Larisa Markeeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1\">Daniel Keysers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolstikhin_I/0/1/0/all/0/1\">Ilya Tolstikhin</a>",
          "description": "We introduce a generalization to the lottery ticket hypothesis in which the\nnotion of \"sparsity\" is relaxed by choosing an arbitrary basis in the space of\nparameters. We present evidence that the original results reported for the\ncanonical basis continue to hold in this broader setting. We describe how\nstructured pruning methods, including pruning units or factorizing\nfully-connected layers into products of low-rank matrices, can be cast as\nparticular instances of this \"generalized\" lottery ticket hypothesis. The\ninvestigations reported here are preliminary and are provided to encourage\nfurther research along this direction.",
          "link": "http://arxiv.org/abs/2107.06825",
          "publishedOn": "2021-07-15T01:59:03.703Z",
          "wordCount": 542,
          "title": "A Generalized Lottery Ticket Hypothesis. (arXiv:2107.06825v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dingcheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenjian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuanbo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Wenjie Liang</a>",
          "description": "Accurate capacitance extraction is becoming more important for designing\nintegrated circuits under advanced process technology. The pattern matching\nbased full-chip extraction methodology delivers fast computational speed, but\nsuffers from large error, and tedious efforts on building capacitance models of\nthe increasing structure patterns. In this work, we propose an effective method\nfor building convolutional neural network (CNN) based capacitance models\n(called CNN-Cap) for two-dimensional (2-D) structures in full-chip capacitance\nextraction. With a novel grid-based data representation, the proposed method is\nable to model the pattern with a variable number of conductors, so that largely\nreduce the number of patterns. Based on the ability of ResNet architecture on\ncapturing spatial information and the proposed training skills, the obtained\nCNN-Cap exhibits much better performance over the multilayer perception neural\nnetwork based capacitance model while being more versatile. Extensive\nexperiments on a 55nm and a 15nm process technologies have demonstrated that\nthe error of total capacitance produced with CNN-Cap is always within 1.3% and\nthe error of produced coupling capacitance is less than 10% in over 99.5%\nprobability. CNN-Cap runs more than 4000X faster than 2-D field solver on a GPU\nserver, while it consumes negligible memory compared to the look-up table based\ncapacitance model.",
          "link": "http://arxiv.org/abs/2107.06511",
          "publishedOn": "2021-07-15T01:59:03.695Z",
          "wordCount": 659,
          "title": "CNN-Cap: Effective Convolutional Neural Network Based Capacitance Models for Full-Chip Parasitic Extraction. (arXiv:2107.06511v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1\">Bharadwaj Manda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhaskare_P/0/1/0/all/0/1\">Pranjal Bhaskare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1\">Ramanathan Muthuganapathy</a>",
          "description": "This paper presents a deep learning approach for the classification of\nEngineering (CAD) models using Convolutional Neural Networks (CNNs). Owing to\nthe availability of large annotated datasets and also enough computational\npower in the form of GPUs, many deep learning-based solutions for object\nclassification have been proposed of late, especially in the domain of images\nand graphical models. Nevertheless, very few solutions have been proposed for\nthe task of functional classification of CAD models. Hence, for this research,\nCAD models have been collected from Engineering Shape Benchmark (ESB), National\nDesign Repository (NDR) and augmented with newer models created using a\nmodelling software to form a dataset - 'CADNET'. It is proposed to use a\nresidual network architecture for CADNET, inspired by the popular ResNet. A\nweighted Light Field Descriptor (LFD) scheme is chosen as the method of feature\nextraction, and the generated images are fed as inputs to the CNN. The problem\nof class imbalance in the dataset is addressed using a class weights approach.\nExperiments have been conducted with other signatures such as geodesic distance\netc. using deep networks as well as other network architectures on the CADNET.\nThe LFD-based CNN approach using the proposed network architecture, along with\ngradient boosting yielded the best classification accuracy on CADNET.",
          "link": "http://arxiv.org/abs/2107.06481",
          "publishedOn": "2021-07-15T01:59:03.667Z",
          "wordCount": 671,
          "title": "A Convolutional Neural Network Approach to the Classification of Engineering Models. (arXiv:2107.06481v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sato_M/0/1/0/all/0/1\">Masahiro Sato</a>",
          "description": "Evaluating the causal effect of recommendations is an important objective\nbecause the causal effect on user interactions can directly leads to an\nincrease in sales and user engagement. To select an optimal recommendation\nmodel, it is common to conduct A/B testing to compare model performance.\nHowever, A/B testing of causal effects requires a large number of users, making\nsuch experiments costly and risky. We therefore propose the first interleaving\nmethods that can efficiently compare recommendation models in terms of causal\neffects. In contrast to conventional interleaving methods, we measure the\noutcomes of both items on an interleaved list and items not on the interleaved\nlist, since the causal effect is the difference between outcomes with and\nwithout recommendations. To ensure that the evaluations are unbiased, we either\nselect items with equal probability or weight the outcomes using inverse\npropensity scores. We then verify the unbiasedness and efficiency of online\nevaluation methods through simulated online experiments. The results indicate\nthat our proposed methods are unbiased and that they have superior efficiency\nto A/B testing.",
          "link": "http://arxiv.org/abs/2107.06630",
          "publishedOn": "2021-07-15T01:59:03.658Z",
          "wordCount": 604,
          "title": "Online Evaluation Methods for the Causal Effect of Recommendations. (arXiv:2107.06630v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bogdanovic_M/0/1/0/all/0/1\">Miroslav Bogdanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khadiv_M/0/1/0/all/0/1\">Majid Khadiv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Righetti_L/0/1/0/all/0/1\">Ludovic Righetti</a>",
          "description": "In this work we present a general, two-stage reinforcement learning approach\nfor going from a single demonstration trajectory to a robust policy that can be\ndeployed on hardware without any additional training. The demonstration is used\nin the first stage as a starting point to facilitate initial exploration. In\nthe second stage, the relevant task reward is optimized directly and a policy\nrobust to environment uncertainties is computed. We demonstrate and examine in\ndetail performance and robustness of our approach on highly dynamic hopping and\nbounding tasks on a real quadruped robot.",
          "link": "http://arxiv.org/abs/2107.06629",
          "publishedOn": "2021-07-15T01:59:03.650Z",
          "wordCount": 532,
          "title": "Model-free Reinforcement Learning for Robust Locomotion Using Trajectory Optimization for Exploration. (arXiv:2107.06629v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06618",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bannur_S/0/1/0/all/0/1\">Shruthi Bannur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oktay_O/0/1/0/all/0/1\">Ozan Oktay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bernhardt_M/0/1/0/all/0/1\">Melanie Bernhardt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schwaighofer_A/0/1/0/all/0/1\">Anton Schwaighofer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jena_R/0/1/0/all/0/1\">Rajesh Jena</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nushi_B/0/1/0/all/0/1\">Besmira Nushi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wadhwani_S/0/1/0/all/0/1\">Sharan Wadhwani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nori_A/0/1/0/all/0/1\">Aditya Nori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Natarajan_K/0/1/0/all/0/1\">Kal Natarajan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ashraf_S/0/1/0/all/0/1\">Shazad Ashraf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alvarez_Valle_J/0/1/0/all/0/1\">Javier Alvarez-Valle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Castro_D/0/1/0/all/0/1\">Daniel C. Castro</a>",
          "description": "Chest radiography has been a recommended procedure for patient triaging and\nresource management in intensive care units (ICUs) throughout the COVID-19\npandemic. The machine learning efforts to augment this workflow have been long\nchallenged due to deficiencies in reporting, model evaluation, and failure mode\nanalysis. To address some of those shortcomings, we model radiological features\nwith a human-interpretable class hierarchy that aligns with the radiological\ndecision process. Also, we propose the use of a data-driven error analysis\nmethodology to uncover the blind spots of our model, providing further\ntransparency on its clinical utility. For example, our experiments show that\nmodel failures highly correlate with ICU imaging conditions and with the\ninherent difficulty in distinguishing certain types of radiological features.\nAlso, our hierarchical interpretation and analysis facilitates the comparison\nwith respect to radiologists' findings and inter-variability, which in return\nhelps us to better assess the clinical applicability of models.",
          "link": "http://arxiv.org/abs/2107.06618",
          "publishedOn": "2021-07-15T01:59:03.643Z",
          "wordCount": 673,
          "title": "Hierarchical Analysis of Visual COVID-19 Features from Chest Radiographs. (arXiv:2107.06618v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shaghouri_A/0/1/0/all/0/1\">Anas Al Shaghouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alkhatib_R/0/1/0/all/0/1\">Rami Alkhatib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berjaoui_S/0/1/0/all/0/1\">Samir Berjaoui</a>",
          "description": "Roads are connecting line between different places, and used daily. Roads'\nperiodic maintenance keeps them safe and functional. Detecting and reporting\nthe existence of potholes to responsible departments can help in eliminating\nthem. This study deployed and tested on different deep learning architecture to\ndetect potholes. The images used for training were collected by cellphone\nmounted on the windshield of the car, in addition to many images downloaded\nfrom the internet to increase the size and variability of the database. Second,\nvarious object detection algorithms are employed and compared to detect\npotholes in real-time like SDD-TensorFlow, YOLOv3Darknet53 and YOLOv4Darknet53.\nYOLOv4 achieved the best performance with 81% recall, 85% precision and 85.39%\nmean Average Precision (mAP). The speed of processing was 20 frame per second.\nThe system was able to detect potholes from a range on 100 meters away from the\ncamera. The system can increase the safety of drivers and improve the\nperformance of self-driving cars by detecting pothole time ahead.",
          "link": "http://arxiv.org/abs/2107.06356",
          "publishedOn": "2021-07-15T01:59:03.628Z",
          "wordCount": 606,
          "title": "Real-Time Pothole Detection Using Deep Learning. (arXiv:2107.06356v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosel_F/0/1/0/all/0/1\">Fabian R&#xf6;sel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahrenkrog_Petersen_S/0/1/0/all/0/1\">Stephan A. Fahrenkrog-Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aa_H/0/1/0/all/0/1\">Han van der Aa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weidlich_M/0/1/0/all/0/1\">Matthias Weidlich</a>",
          "description": "To enable process analysis based on an event log without compromising the\nprivacy of individuals involved in process execution, a log may be anonymized.\nSuch anonymization strives to transform a log so that it satisfies provable\nprivacy guarantees, while largely maintaining its utility for process analysis.\nExisting techniques perform anonymization using simple, syntactic measures to\nidentify suitable transformation operations. This way, the semantics of the\nactivities referenced by the events in a trace are neglected, potentially\nleading to transformations in which events of unrelated activities are merged.\nTo avoid this and incorporate the semantics of activities during anonymization,\nwe propose to instead incorporate a distance measure based on feature learning.\nSpecifically, we show how embeddings of events enable the definition of a\ndistance measure for traces to guide event log anonymization. Our experiments\nwith real-world data indicate that anonymization using this measure, compared\nto a syntactic one, yields logs that are closer to the original log in various\ndimensions and, hence, have higher utility for process analysis.",
          "link": "http://arxiv.org/abs/2107.06578",
          "publishedOn": "2021-07-15T01:59:03.611Z",
          "wordCount": 629,
          "title": "A Distance Measure for Privacy-preserving Process Mining based on Feature Learning. (arXiv:2107.06578v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06546",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alishahia_A/0/1/0/all/0/1\">Afra Alishahia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrupala_G/0/1/0/all/0/1\">Grzegorz Chrupa&#x142;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristia_A/0/1/0/all/0/1\">Alejandrina Cristia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higy_B/0/1/0/all/0/1\">Bertrand Higy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavechin_M/0/1/0/all/0/1\">Marvin Lavechin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chen Yu</a>",
          "description": "We present the visually-grounded language modelling track that was introduced\nin the Zero-Resource Speech challenge, 2021 edition, 2nd round. We motivate the\nnew track and discuss participation rules in detail. We also present the two\nbaseline systems that were developed for this track.",
          "link": "http://arxiv.org/abs/2107.06546",
          "publishedOn": "2021-07-15T01:59:03.596Z",
          "wordCount": 501,
          "title": "ZR-2021VG: Zero-Resource Speech Challenge, Visually-Grounded Language Modelling track, 2021 edition. (arXiv:2107.06546v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nystrom_A/0/1/0/all/0/1\">Andrew Nystrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eck_D/0/1/0/all/0/1\">Douglas Eck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>",
          "description": "We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.",
          "link": "http://arxiv.org/abs/2107.06499",
          "publishedOn": "2021-07-15T01:59:03.560Z",
          "wordCount": 571,
          "title": "Deduplicating Training Data Makes Language Models Better. (arXiv:2107.06499v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06428",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Trippe_B/0/1/0/all/0/1\">Brian L. Trippe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Finucane_H/0/1/0/all/0/1\">Hilary K. Finucane</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Broderick_T/0/1/0/all/0/1\">Tamara Broderick</a>",
          "description": "Hierarchical Bayesian methods enable information sharing across multiple\nrelated regression problems. While standard practice is to model regression\nparameters (effects) as (1) exchangeable across datasets and (2) correlated to\ndiffering degrees across covariates, we show that this approach exhibits poor\nstatistical performance when the number of covariates exceeds the number of\ndatasets. For instance, in statistical genetics, we might regress dozens of\ntraits (defining datasets) for thousands of individuals (responses) on up to\nmillions of genetic variants (covariates). When an analyst has more covariates\nthan datasets, we argue that it is often more natural to instead model effects\nas (1) exchangeable across covariates and (2) correlated to differing degrees\nacross datasets. To this end, we propose a hierarchical model expressing our\nalternative perspective. We devise an empirical Bayes estimator for learning\nthe degree of correlation between datasets. We develop theory that demonstrates\nthat our method outperforms the classic approach when the number of covariates\ndominates the number of datasets, and corroborate this result empirically on\nseveral high-dimensional multiple regression and classification problems.",
          "link": "http://arxiv.org/abs/2107.06428",
          "publishedOn": "2021-07-15T01:59:03.550Z",
          "wordCount": 625,
          "title": "For high-dimensional hierarchical models, consider exchangeability of effects across covariates instead of across datasets. (arXiv:2107.06428v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwala_S/0/1/0/all/0/1\">Susama Agarwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dees_B/0/1/0/all/0/1\">Benjamin Dees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gearhart_A/0/1/0/all/0/1\">Andrew Gearhart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowman_C/0/1/0/all/0/1\">Corey Lowman</a>",
          "description": "We study the deformation of the input space by a trained autoencoder via the\nJacobians of the trained weight matrices. In doing so, we prove bounds for the\nmean squared errors for points in the input space, under assumptions regarding\nthe orthogonality of the eigenvectors. We also show that the trace and the\nproduct of the eigenvalues of the Jacobian matrices is a good predictor of the\nMSE on test points. This is a dataset independent means of testing an\nautoencoder's ability to generalize on new input. Namely, no knowledge of the\ndataset on which the network was trained is needed, only the parameters of the\ntrained model.",
          "link": "http://arxiv.org/abs/2107.06386",
          "publishedOn": "2021-07-15T01:59:03.539Z",
          "wordCount": 558,
          "title": "Geometry and Generalization: Eigenvalues as predictors of where a network will fail to generalize. (arXiv:2107.06386v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DAmario_V/0/1/0/all/0/1\">Vanessa D&#x27;Amario</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>",
          "description": "Datasets often contain input dimensions that are unnecessary to predict the\noutput label, e.g. background in object recognition, which lead to more\ntrainable parameters. Deep Neural Networks (DNNs) are robust to increasing the\nnumber of parameters in the hidden layers, but it is unclear whether this holds\ntrue for the input layer. In this letter, we investigate the impact of\nunnecessary input dimensions on a central issue of DNNs: their data efficiency,\nie. the amount of examples needed to achieve certain generalization\nperformance. Our results show that unnecessary input dimensions that are\ntask-unrelated substantially degrade data efficiency. This highlights the need\nfor mechanisms that remove {task-unrelated} dimensions to enable data\nefficiency gains.",
          "link": "http://arxiv.org/abs/2107.06409",
          "publishedOn": "2021-07-15T01:59:03.528Z",
          "wordCount": 554,
          "title": "The Foes of Neural Network's Data Efficiency Among Unnecessary Input Dimensions. (arXiv:2107.06409v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grbic_D/0/1/0/all/0/1\">Djordje Grbic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risi_S/0/1/0/all/0/1\">Sebastian Risi</a>",
          "description": "Random exploration is one of the main mechanisms through which reinforcement\nlearning (RL) finds well-performing policies. However, it can lead to\nundesirable or catastrophic outcomes when learning online in safety-critical\nenvironments. In fact, safe learning is one of the major obstacles towards\nreal-world agents that can learn during deployment. One way of ensuring that\nagents respect hard limitations is to explicitly configure boundaries in which\nthey can operate. While this might work in some cases, we do not always have\nclear a-priori information which states and actions can lead dangerously close\nto hazardous states. Here, we present an approach where an additional policy\ncan override the main policy and offer a safer alternative action. In our\ninstinct-regulated RL (IR^2L) approach, an \"instinctual\" network is trained to\nrecognize undesirable situations, while guarding the learning policy against\nentering them. The instinct network is pre-trained on a single task where it is\nsafe to make mistakes, and transferred to environments in which learning a new\ntask safely is critical. We demonstrate IR^2L in the OpenAI Safety gym domain,\nin which it receives a significantly lower number of safety violations during\ntraining than a baseline RL approach while reaching similar task performance.",
          "link": "http://arxiv.org/abs/2107.06686",
          "publishedOn": "2021-07-15T01:59:03.517Z",
          "wordCount": 644,
          "title": "Safer Reinforcement Learning through Transferable Instinct Networks. (arXiv:2107.06686v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krotov_D/0/1/0/all/0/1\">Dmitry Krotov</a>",
          "description": "Dense Associative Memories or Modern Hopfield Networks have many appealing\nproperties of associative memory. They can do pattern completion, store a large\nnumber of memories, and can be described using a recurrent neural network with\na degree of biological plausibility and rich feedback between the neurons. At\nthe same time, up until now all the models of this class have had only one\nhidden layer, and have only been formulated with densely connected network\narchitectures, two aspects that hinder their machine learning applications.\nThis paper tackles this gap and describes a fully recurrent model of\nassociative memory with an arbitrary large number of layers, some of which can\nbe locally connected (convolutional), and a corresponding energy function that\ndecreases on the dynamical trajectory of the neurons' activations. The memories\nof the full network are dynamically \"assembled\" using primitives encoded in the\nsynaptic weights of the lower layers, with the \"assembling rules\" encoded in\nthe synaptic weights of the higher layers. In addition to the bottom-up\npropagation of information, typical of commonly used feedforward neural\nnetworks, the model described has rich top-down feedback from higher layers\nthat help the lower-layer neurons to decide on their response to the input\nstimuli.",
          "link": "http://arxiv.org/abs/2107.06446",
          "publishedOn": "2021-07-15T01:59:03.494Z",
          "wordCount": 621,
          "title": "Hierarchical Associative Memory. (arXiv:2107.06446v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tavakoli_M/0/1/0/all/0/1\">Mohammadamin Tavakoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldi_P/0/1/0/all/0/1\">Pierre Baldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadowski_P/0/1/0/all/0/1\">Peter Sadowski</a>",
          "description": "In a physical neural system, backpropagation is faced with a number of\nobstacles including: the need for labeled data, the violation of the locality\nlearning principle, the need for symmetric connections, and the lack of\nmodularity. Tourbillon is a new architecture that addresses all these\nlimitations. At its core, it consists of a stack of circular autoencoders\nfollowed by an output layer. The circular autoencoders are trained in\nself-supervised mode by recirculation algorithms and the top layer in\nsupervised mode by stochastic gradient descent, with the option of propagating\nerror information through the entire stack using non-symmetric connections.\nWhile the Tourbillon architecture is meant primarily to address physical\nconstraints, and not to improve current engineering applications of deep\nlearning, we demonstrate its viability on standard benchmark datasets including\nMNIST, Fashion MNIST, and CIFAR10. We show that Tourbillon can achieve\ncomparable performance to models trained with backpropagation and outperform\nmodels that are trained with other physically plausible algorithms, such as\nfeedback alignment.",
          "link": "http://arxiv.org/abs/2107.06424",
          "publishedOn": "2021-07-15T01:59:03.480Z",
          "wordCount": 588,
          "title": "Tourbillon: a Physically Plausible Neural Architecture. (arXiv:2107.06424v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06767",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Racz_M/0/1/0/all/0/1\">Miklos Z. Racz</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sridhar_A/0/1/0/all/0/1\">Anirudh Sridhar</a>",
          "description": "We consider the task of learning latent community structure from multiple\ncorrelated networks. First, we study the problem of learning the latent vertex\ncorrespondence between two edge-correlated stochastic block models, focusing on\nthe regime where the average degree is logarithmic in the number of vertices.\nWe derive the precise information-theoretic threshold for exact recovery: above\nthe threshold there exists an estimator that outputs the true correspondence\nwith probability close to 1, while below it no estimator can recover the true\ncorrespondence with probability bounded away from 0. As an application of our\nresults, we show how one can exactly recover the latent communities using\nmultiple correlated graphs in parameter regimes where it is\ninformation-theoretically impossible to do so using just a single graph.",
          "link": "http://arxiv.org/abs/2107.06767",
          "publishedOn": "2021-07-15T01:59:03.461Z",
          "wordCount": 584,
          "title": "Correlated Stochastic Block Models: Exact Graph Matching with Applications to Recovering Communities. (arXiv:2107.06767v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06534",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Akhtar_Z/0/1/0/all/0/1\">Zeeshan Akhtar</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rajawat_K/0/1/0/all/0/1\">Ketan Rajawat</a>",
          "description": "This paper considers stochastic convex optimization problems with two sets of\nconstraints: (a) deterministic constraints on the domain of the optimization\nvariable, which are difficult to project onto; and (b) deterministic or\nstochastic constraints that admit efficient projection. Problems of this form\narise frequently in the context of semidefinite programming as well as when\nvarious NP-hard problems are solved approximately via semidefinite relaxation.\nSince projection onto the first set of constraints is difficult, it becomes\nnecessary to explore projection-free algorithms, such as the stochastic\nFrank-Wolfe (FW) algorithm. On the other hand, the second set of constraints\ncannot be handled in the same way, and must be incorporated as an indicator\nfunction within the objective function, thereby complicating the application of\nFW methods. Similar problems have been studied before, and solved using\nfirst-order stochastic FW algorithms by applying homotopy and Nesterov's\nsmoothing techniques to the indicator function. This work improves upon these\nexisting results and puts forth momentum-based first-order methods that yield\nimproved convergence rates, at par with the best known rates for problems\nwithout the second set of constraints. Zeroth-order variants of the proposed\nalgorithms are also developed and again improve upon the state-of-the-art rate\nresults. The efficacy of the proposed algorithms is tested on relevant\napplications of sparse matrix estimation, clustering via semidefinite\nrelaxation, and uniform sparsest cut problem.",
          "link": "http://arxiv.org/abs/2107.06534",
          "publishedOn": "2021-07-15T01:59:03.453Z",
          "wordCount": 666,
          "title": "Zeroth and First Order Stochastic Frank-Wolfe Algorithms for Constrained Optimization. (arXiv:2107.06534v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thordsen_E/0/1/0/all/0/1\">Erik Thordsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_E/0/1/0/all/0/1\">Erich Schubert</a>",
          "description": "Many approaches in the field of machine learning and data analysis rely on\nthe assumption that the observed data lies on lower-dimensional manifolds. This\nassumption has been verified empirically for many real data sets. To make use\nof this manifold assumption one generally requires the manifold to be locally\nsampled to a certain density such that features of the manifold can be\nobserved. However, for increasing intrinsic dimensionality of a data set the\nrequired data density introduces the need for very large data sets, resulting\nin one of the many faces of the curse of dimensionality. To combat the\nincreased requirement for local data density we propose a framework to generate\nvirtual data points that faithful to an approximate embedding function\nunderlying the manifold observable in the data.",
          "link": "http://arxiv.org/abs/2107.06566",
          "publishedOn": "2021-07-15T01:59:03.441Z",
          "wordCount": 559,
          "title": "MESS: Manifold Embedding Motivated Super Sampling. (arXiv:2107.06566v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huyuk_A/0/1/0/all/0/1\">Alihan H&#xfc;y&#xfc;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarrett_D/0/1/0/all/0/1\">Daniel Jarrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1\">Mihaela van der Schaar</a>",
          "description": "Understanding an agent's priorities by observing their behavior is critical\nfor transparency and accountability in decision processes, such as in\nhealthcare. While conventional approaches to policy learning almost invariably\nassume stationarity in behavior, this is hardly true in practice: Medical\npractice is constantly evolving, and clinical professionals are constantly\nfine-tuning their priorities. We desire an approach to policy learning that\nprovides (1) interpretable representations of decision-making, accounts for (2)\nnon-stationarity in behavior, as well as operating in an (3) offline manner.\nFirst, we model the behavior of learning agents in terms of contextual bandits,\nand formalize the problem of inverse contextual bandits (ICB). Second, we\npropose two algorithms to tackle ICB, each making varying degrees of\nassumptions regarding the agent's learning strategy. Finally, through both real\nand simulated data for liver transplantations, we illustrate the applicability\nand explainability of our method, as well as validating its accuracy.",
          "link": "http://arxiv.org/abs/2107.06317",
          "publishedOn": "2021-07-15T01:59:03.424Z",
          "wordCount": 583,
          "title": "Inverse Contextual Bandits: Learning How Behavior Evolves over Time. (arXiv:2107.06317v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suneung-Kim/0/1/0/all/0/1\">Suneung-Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Due to the recent outbreak of COVID-19, many classes, exams, and meetings\nhave been conducted non-face-to-face. However, the foundation for video\nconferencing solutions is still insufficient. So this technology has become an\nimportant issue. In particular, these technologies are essential for\nnon-face-to-face testing, and technology dissemination is urgent. In this\npaper, we present a single video conferencing solution using gaze estimation in\npreparation for these problems. Gaze is an important cue for the tasks such as\nanalysis of human behavior. Hence, numerous studies have been proposed to solve\ngaze estimation using deep learning, which is one of the most prominent methods\nup to date. We use these gaze estimation methods to detect abnormal behavior of\nvideo conferencing participants. Our contribution is as follows. i) We find and\napply the optimal network for the gaze estimation method and apply a\nself-supervised method to improve accuracy. ii) For anomaly detection, we\npresent a new dataset that aggregates the values of a new gaze, head pose, etc.\niii) We train newly created data on Multi Layer Perceptron (MLP) models to\ndetect anomaly behavior based on deep learning. We demonstrate the robustness\nof our method through experiments.",
          "link": "http://arxiv.org/abs/2107.06530",
          "publishedOn": "2021-07-15T01:59:03.417Z",
          "wordCount": 685,
          "title": "Detection of Abnormal Behavior with Self-Supervised Gaze Estimation. (arXiv:2107.06530v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_A/0/1/0/all/0/1\">Allen Z. Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1\">Anirudha Majumdar</a>",
          "description": "Our goal is to train control policies that generalize well to unseen\nenvironments. Inspired by the Distributionally Robust Optimization (DRO)\nframework, we propose DRAGEN - Distributionally Robust policy learning via\nAdversarial Generation of ENvironments - for iteratively improving robustness\nof policies to realistic distribution shifts by generating adversarial\nenvironments. The key idea is to learn a generative model for environments\nwhose latent variables capture cost-predictive and realistic variations in\nenvironments. We perform DRO with respect to a Wasserstein ball around the\nempirical distribution of environments by generating realistic adversarial\nenvironments via gradient ascent on the latent space. We demonstrate strong\nOut-of-Distribution (OoD) generalization in simulation for (i) swinging up a\npendulum with onboard vision and (ii) grasping realistic 2D/3D objects.\nGrasping experiments on hardware demonstrate better sim2real performance\ncompared to domain randomization.",
          "link": "http://arxiv.org/abs/2107.06353",
          "publishedOn": "2021-07-15T01:59:03.411Z",
          "wordCount": 561,
          "title": "Distributionally Robust Policy Learning via Adversarial Environment Generation. (arXiv:2107.06353v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongxu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1\">Pavlo Molchanov</a>",
          "description": "Understanding the behavior and vulnerability of pre-trained deep neural\nnetworks (DNNs) can help to improve them. Analysis can be performed via\nreversing the network's flow to generate inputs from internal representations.\nMost existing work relies on priors or data-intensive optimization to invert a\nmodel, yet struggles to scale to deep architectures and complex datasets. This\npaper presents a zero-shot direct model inversion framework that recovers the\ninput to the trained model given only the internal representation. The crux of\nour method is to inverse the DNN in a divide-and-conquer manner while\nre-syncing the inverted layers via cycle-consistency guidance with the help of\nsynthesized data. As a result, we obtain a single feed-forward model capable of\ninversion with a single forward pass without seeing any real data of the\noriginal task. With the proposed approach, we scale zero-shot direct inversion\nto deep architectures and complex datasets. We empirically show that modern\nclassification models on ImageNet can, surprisingly, be inverted, allowing an\napproximate recovery of the original 224x224px images from a representation\nafter more than 20 layers. Moreover, inversion of generators in GANs unveils\nlatent code of a given synthesized face image at 128x128px, which can even, in\nturn, improve defective synthesized images from GANs.",
          "link": "http://arxiv.org/abs/2107.06304",
          "publishedOn": "2021-07-15T01:59:03.404Z",
          "wordCount": 673,
          "title": "Deep Neural Networks are Surprisingly Reversible: A Baseline for Zero-Shot Inversion. (arXiv:2107.06304v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdellatif_A/0/1/0/all/0/1\">Alaa Awad Abdellatif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mhaisen_N/0/1/0/all/0/1\">Naram Mhaisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Amr Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1\">Aiman Erbad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1\">Mohsen Guizani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawy_Z/0/1/0/all/0/1\">Zaher Dawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasreddine_W/0/1/0/all/0/1\">Wassim Nasreddine</a>",
          "description": "Federated learning (FL) is a distributed learning methodology that allows\nmultiple nodes to cooperatively train a deep learning model, without the need\nto share their local data. It is a promising solution for telemonitoring\nsystems that demand intensive data collection, for detection, classification,\nand prediction of future events, from different locations while maintaining a\nstrict privacy constraint. Due to privacy concerns and critical communication\nbottlenecks, it can become impractical to send the FL updated models to a\ncentralized server. Thus, this paper studies the potential of hierarchical FL\nin IoT heterogeneous systems and propose an optimized solution for user\nassignment and resource allocation on multiple edge nodes. In particular, this\nwork focuses on a generic class of machine learning models that are trained\nusing gradient-descent-based schemes while considering the practical\nconstraints of non-uniformly distributed data across different users. We\nevaluate the proposed system using two real-world datasets, and we show that it\noutperforms state-of-the-art FL solutions. In particular, our numerical results\nhighlight the effectiveness of our approach and its ability to provide 4-6%\nincrease in the classification accuracy, with respect to hierarchical FL\nschemes that consider distance-based user assignment. Furthermore, the proposed\napproach could significantly accelerate FL training and reduce communication\noverhead by providing 75-85% reduction in the communication rounds between edge\nnodes and the centralized server, for the same model accuracy.",
          "link": "http://arxiv.org/abs/2107.06548",
          "publishedOn": "2021-07-15T01:59:03.386Z",
          "wordCount": 698,
          "title": "Communication-Efficient Hierarchical Federated Learning for IoT Heterogeneous Systems with Imbalanced Data. (arXiv:2107.06548v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lahtinen_T/0/1/0/all/0/1\">Tuomo Lahtinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turtiainen_H/0/1/0/all/0/1\">Hannu Turtiainen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costin_A/0/1/0/all/0/1\">Andrei Costin</a>",
          "description": "Image annotation and large annotated datasets are crucial parts within the\nComputer Vision and Artificial Intelligence fields.At the same time, it is\nwell-known and acknowledged by the research community that the image annotation\nprocess is challenging, time-consuming and hard to scale. Therefore, the\nresearchers and practitioners are always seeking ways to perform the\nannotations easier, faster, and at higher quality. Even though several widely\nused tools exist and the tools' landscape evolved considerably, most of the\ntools still require intricate technical setups and high levels of technical\nsavviness from its operators and crowdsource contributors.\n\nIn order to address such challenges, we develop and present BRIMA -- a\nflexible and open-source browser extension that allows BRowser-only IMage\nAnnotation at considerably lower overheads. Once added to the browser, it\ninstantly allows the user to annotate images easily and efficiently directly\nfrom the browser without any installation or setup on the client-side. It also\nfeatures cross-browser and cross-platform functionality thus presenting itself\nas a neat tool for researchers within the Computer Vision, Artificial\nIntelligence, and privacy-related fields.",
          "link": "http://arxiv.org/abs/2107.06351",
          "publishedOn": "2021-07-15T01:59:03.380Z",
          "wordCount": 612,
          "title": "BRIMA: low-overhead BRowser-only IMage Annotation tool (Preprint). (arXiv:2107.06351v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozkan_M/0/1/0/all/0/1\">Mehmet Fatih Ozkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yao Ma</a>",
          "description": "Drivers have unique and rich driving behaviors when operating vehicles in\ntraffic. This paper presents a novel driver behavior learning approach that\ncaptures the uniqueness and richness of human driver behavior in realistic\ndriving scenarios. A stochastic inverse reinforcement learning (SIRL) approach\nis proposed to learn a distribution of cost function, which represents the\nrichness of the human driver behavior with a given set of driver-specific\ndemonstrations. Evaluations are conducted on the realistic driving data\ncollected from the 3D driver-in-the-loop driving simulation. The results show\nthat the learned stochastic driver model is capable of expressing the richness\nof the human driving strategies under different realistic driving scenarios.\nCompared to the deterministic baseline driver model, the results reveal that\nthe proposed stochastic driver behavior model can better replicate the driver's\nunique and rich driving strategies in a variety of traffic conditions.",
          "link": "http://arxiv.org/abs/2107.06344",
          "publishedOn": "2021-07-15T01:59:03.329Z",
          "wordCount": 583,
          "title": "Inverse Reinforcement Learning Based Stochastic Driver Behavior Learning. (arXiv:2107.06344v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Theis_J/0/1/0/all/0/1\">Julian Theis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokhtarian_I/0/1/0/all/0/1\">Ilia Mokhtarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darabi_H/0/1/0/all/0/1\">Houshang Darabi</a>",
          "description": "Process mining algorithms discover a process model from an event log. The\nresulting process model is supposed to describe all possible event sequences of\nthe underlying system. Generalization is a process model quality dimension of\ninterest. A generalization metric should quantify the extent to which a process\nmodel represents the observed event sequences contained in the event log and\nthe unobserved event sequences of the system. Most of the available metrics in\nthe literature cannot properly quantify the generalization of a process model.\nA recently published method [1] called Adversarial System Variant Approximation\nleverages Generative Adversarial Networks to approximate the underlying event\nsequence distribution of a system from an event log. While this method\ndemonstrated performance gains over existing methods in measuring the\ngeneralization of process models, its experimental evaluations have been\nperformed under ideal conditions. This paper experimentally investigates the\nperformance of Adversarial System Variant Approximation under non-ideal\nconditions such as biased and limited event logs. Moreover, experiments are\nperformed to investigate the originally proposed sampling hyperparameter value\nof the method on its performance to measure the generalization. The results\nconfirm the need to raise awareness about the working conditions of the\nAdversarial System Variant Approximation method. The outcomes of this paper\nalso serve to initiate future research directions.\n\n[1] Theis, Julian, and Houshang Darabi. \"Adversarial System Variant\nApproximation to Quantify Process Model Generalization.\" IEEE Access 8 (2020):\n194410-194427.",
          "link": "http://arxiv.org/abs/2107.06319",
          "publishedOn": "2021-07-15T01:59:03.321Z",
          "wordCount": 678,
          "title": "On the Performance Analysis of the Adversarial System Variant Approximation Method to Quantify Process Model Generalization. (arXiv:2107.06319v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06281",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Mhiri_I/0/1/0/all/0/1\">Islem Mhiri</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nebli_A/0/1/0/all/0/1\">Ahmed Nebli</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mahjoub_M/0/1/0/all/0/1\">Mohamed Ali Mahjoub</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rekik_I/0/1/0/all/0/1\">Islem Rekik</a>",
          "description": "Brain graph synthesis marked a new era for predicting a target brain graph\nfrom a source one without incurring the high acquisition cost and processing\ntime of neuroimaging data. However, existing multi-modal graph synthesis\nframeworks have several limitations. First, they mainly focus on generating\ngraphs from the same domain (intra-modality), overlooking the rich multimodal\nrepresentations of brain connectivity (inter-modality). Second, they can only\nhandle isomorphic graph generation tasks, limiting their generalizability to\nsynthesizing target graphs with a different node size and topological structure\nfrom those of the source one. More importantly, both target and source domains\nmight have different distributions, which causes a domain fracture between them\n(i.e., distribution misalignment). To address such challenges, we propose an\ninter-modality aligner of non-isomorphic graphs (IMANGraphNet) framework to\ninfer a target graph modality based on a given modality. Our three core\ncontributions lie in (i) predicting a target graph (e.g., functional) from a\nsource graph (e.g., morphological) based on a novel graph generative\nadversarial network (gGAN); (ii) using non-isomorphic graphs for both source\nand target domains with a different number of nodes, edges and structure; and\n(iii) enforcing the predicted target distribution to match that of the ground\ntruth graphs using a graph autoencoder to relax the designed loss oprimization.\nTo handle the unstable behavior of gGAN, we design a new Ground\nTruth-Preserving (GT-P) loss function to guide the generator in learning the\ntopological structure of ground truth brain graphs. Our comprehensive\nexperiments on predicting functional from morphological graphs demonstrate the\noutperformance of IMANGraphNet in comparison with its variants. This can be\nfurther leveraged for integrative and holistic brain mapping in health and\ndisease.",
          "link": "http://arxiv.org/abs/2107.06281",
          "publishedOn": "2021-07-15T01:59:03.285Z",
          "wordCount": 719,
          "title": "Non-isomorphic Inter-modality Graph Alignment and Synthesis for Holistic Brain Mapping. (arXiv:2107.06281v1 [q-bio.NC])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}